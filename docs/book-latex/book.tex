% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Little Book of Algorithms},
  pdfauthor={Duc-Tam Nguyen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Little Book of Algorithms}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 0.1.2}
\author{Duc-Tam Nguyen}
\date{2025-09-11}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Roadmap}\label{roadmap}

The Little Book of Algorithms is a multi-volume project. Each volume has
a clear sequence of chapters, and each chapter has three levels of depth
(L0 beginner intuition, L1 practical techniques, L2 advanced
systems/theory). This roadmap outlines the plan for development and
publication.

\section{Goals}\label{goals}

\begin{itemize}
\tightlist
\item
  Establish a consistent layered structure across all chapters.
\item
  Provide runnable implementations in Python, C, Go, Erlang, and Lean.
\item
  Ensure Quarto build supports HTML, PDF, EPUB, and LaTeX.
\item
  Deliver both pedagogy (L0) and production insights (L2).
\end{itemize}

\section{Volumes}\label{volumes}

\subsection{Volume I - Structures
Linéaires}\label{volume-i---structures-linuxe9aires}

\begin{itemize}
\tightlist
\item
  Chapter 0 - Foundations
\item
  Chapter 1 - Numbers
\item
  Chapter 2 - Arrays
\item
  Chapter 3 - Strings
\item
  Chapter 4 - Linked Lists
\item
  Chapter 5 - Stacks \& Queues
\end{itemize}

\subsection{Volume II - Algorithmes
Fondamentaux}\label{volume-ii---algorithmes-fondamentaux}

\begin{itemize}
\tightlist
\item
  Chapter 6 - Searching
\item
  Chapter 7 - Selection
\item
  Chapter 8 - Sorting
\item
  Chapter 9 - Amortized Analysis
\end{itemize}

\subsection{Volume III - Structures
Hiérarchiques}\label{volume-iii---structures-hiuxe9rarchiques}

\begin{itemize}
\tightlist
\item
  Chapter 10 - Tree Fundamentals
\item
  Chapter 11 - Heaps \& Priority Queues
\item
  Chapter 12 - Binary Search Trees
\item
  Chapter 13 - Balanced Trees \& Ordered Maps
\item
  Chapter 14 - Range Queries
\item
  Chapter 15 - Vector Databases
\end{itemize}

\subsection{Volume IV - Paradigmes
Algorithmiques}\label{volume-iv---paradigmes-algorithmiques}

\begin{itemize}
\tightlist
\item
  Chapter 16 - Divide-and-Conquer
\item
  Chapter 17 - Greedy
\item
  Chapter 18 - Dynamic Programming
\item
  Chapter 19 - Backtracking \& Search
\end{itemize}

\subsection{Volume V - Graphes et
Complexité}\label{volume-v---graphes-et-complexituxe9}

\begin{itemize}
\tightlist
\item
  Chapter 20 - Graph Basics
\item
  Chapter 21 - DAGs \& SCC
\item
  Chapter 22 - Shortest Paths
\item
  Chapter 23 - Flows \& Matchings
\item
  Chapter 24 - Tree Algorithms
\item
  Chapter 25 - Complexity \& Limits
\item
  Chapter 26 - External \& Cache-Oblivious
\item
  Chapter 27 - Probabilistic \& Streaming
\item
  Chapter 28 - Engineering
\end{itemize}

\section{Milestones}\label{milestones}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Complete detailed outlines for all chapters (L0, L1, L2).
\item
  Write draft text for all L0 sections (intuition, analogies, simple
  examples).
\item
  Expand each chapter with L1 content (implementations, correctness
  arguments, exercises).
\item
  Add L2 content (systems insights, proofs, optimizations, advanced
  references).
\item
  Develop and test runnable code in \texttt{src/} across Python, C, Go,
  Erlang, and Lean.
\item
  Integrate diagrams, figures, and visual explanations.
\item
  Finalize Quarto build setup for HTML, PDF, and EPUB.
\item
  Release first public edition (HTML + PDF).
\item
  Add LaTeX build, refine EPUB, and polish cross-references.
\item
  Publish on GitHub Pages and archive DOI.
\item
  Gather feedback, refine explanations, and expand exercises/problem
  sets.
\item
  Long-term: maintain as a living reference with continuous updates and
  companion volumes.
\end{enumerate}

\section{Deliverables}\label{deliverables}

\begin{itemize}
\tightlist
\item
  Quarto project with 29 chapters (00--28).
\item
  Multi-language reference implementations.
\item
  Learning matrix in README for navigation.
\item
  ROADMAP.md (this file) to track progress.
\end{itemize}

\section{Long-term Vision}\label{long-term-vision}

\begin{itemize}
\tightlist
\item
  Maintain the repository as a living reference.
\item
  Extend with exercises, problem sets, and quizzes.
\item
  Build a dependency map across volumes for prerequisites.
\item
  Connect to companion ``Little Book'' series (linear algebra, calculus,
  probability).
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Chapter 1. Numbers}\label{chapter-1.-numbers}

\section{1.1 Representation}\label{representation}

\subsection{1.1 L0. Decimal and Binary
Basics}\label{l0.-decimal-and-binary-basics}

A number representation is a way of writing numbers using symbols and
positional rules. Humans typically use decimal notation, while computers
rely on binary because it aligns with the two-state nature of electronic
circuits. Understanding both systems is the first step in connecting
mathematical intuition with machine computation.

\subsubsection{Numbers in Everyday Life}\label{numbers-in-everyday-life}

Humans work with the decimal system (base 10), which uses digits 0
through 9. Each position in a number has a place value that is a power
of 10.

\[
427 = 4 \times 10^2 + 2 \times 10^1 + 7 \times 10^0
\]

This principle of \emph{positional notation} is the same idea used in
other bases.

\subsubsection{Numbers in Computers}\label{numbers-in-computers}

Computers, however, operate in binary (base 2). A binary digit (bit) can
only be 0 or 1, matching the two stable states of electronic circuits
(off/on). Each binary place value represents a power of 2.

\[
1011_2 = 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0 = 11_{10}
\]

Just like in decimal where \(9 + 1 = 10\), in binary \(1 + 1 = 10_2\).

\subsubsection{Conversion Between Decimal and
Binary}\label{conversion-between-decimal-and-binary}

To convert from decimal to binary, repeatedly divide the number by 2 and
record the remainders. Then read the remainders from bottom to top.

Example: Convert \(42_{10}\) into binary.

\begin{itemize}
\tightlist
\item
  42 ÷ 2 = 21 remainder 0
\item
  21 ÷ 2 = 10 remainder 1
\item
  10 ÷ 2 = 5 remainder 0
\item
  5 ÷ 2 = 2 remainder 1
\item
  2 ÷ 2 = 1 remainder 0
\item
  1 ÷ 2 = 0 remainder 1
\end{itemize}

Reading upward: \(101010_2\).

To convert from binary to decimal, expand into powers of 2 and sum:

\[
101010_2 = 1 \times 2^5 + 0 \times 2^4 + 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 0 \times 2^0 = 42_{10}
\]

\subsubsection{Worked Example (Python)}\label{worked-example-python}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{42}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Decimal:"}\NormalTok{, n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binary :"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(n))   }\CommentTok{\# 0b101010}

\CommentTok{\# binary literal in Python}
\NormalTok{b }\OperatorTok{=} \BaseNTok{0b101010}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binary literal:"}\NormalTok{, b)}

\CommentTok{\# converting binary string to decimal}
\BuiltInTok{print}\NormalTok{(}\StringTok{"From binary \textquotesingle{}1011\textquotesingle{}:"}\NormalTok{, }\BuiltInTok{int}\NormalTok{(}\StringTok{"1011"}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Output:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Decimal:}\NormalTok{ 42}
\ExtensionTok{Binary}\NormalTok{ : 0b101010}
\ExtensionTok{Binary}\NormalTok{ literal: 42}
\ExtensionTok{From}\NormalTok{ binary }\StringTok{\textquotesingle{}1011\textquotesingle{}}\NormalTok{: 11}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters}

\begin{itemize}
\tightlist
\item
  All information inside a computer --- numbers, text, images, programs
  --- reduces to binary representation.
\item
  Decimal and binary conversions are the first bridge between
  human-friendly math and machine-level data.
\item
  Understanding binary is essential for debugging, low-level
  programming, and algorithms that depend on bit operations.
\end{itemize}

\subsubsection{Exercises}\label{exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write the decimal number 19 in binary.
\item
  Convert the binary number \texttt{10101₂} into decimal.
\item
  Show the repeated division steps to convert 27 into binary.
\item
  Verify in Python that \texttt{0b111111} equals 63.
\item
  Explain why computers use binary instead of decimal.
\end{enumerate}

\subsection{1.1 L1. Beyond Binary: Octal, Hex, and Two's
Complement}\label{l1.-beyond-binary-octal-hex-and-twos-complement}

Numbers are not always written in base-10 or even in base-2. For
efficiency and compactness, programmers often use octal (base-8) and
hexadecimal (base-16). At the same time, negative numbers must be
represented reliably; modern computers use two's complement for this
purpose.

\subsubsection{Octal and Hexadecimal}\label{octal-and-hexadecimal}

Octal and hex are simply alternate numeral systems.

\begin{itemize}
\tightlist
\item
  Octal (base 8): digits \texttt{0}--\texttt{7}.
\item
  Hexadecimal (base 16): digits \texttt{0}--\texttt{9} plus
  \texttt{A}--\texttt{F}.
\end{itemize}

Why they matter:

\begin{itemize}
\tightlist
\item
  Hex is concise: one hex digit = 4 binary bits.
\item
  Octal was historically convenient: one octal digit = 3 binary bits
  (useful on early 12-, 24-, or 36-bit machines).
\end{itemize}

For example, the number 42 is written as:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Decimal & Binary & Octal & Hex \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
42 & 101010 & 52 & 2A \\
\end{longtable}

\subsubsection{Two's Complement}\label{twos-complement}

To represent negative numbers, we cannot just ``stick a minus sign'' in
memory. Instead, binary uses two's complement:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a fixed bit-width (say 8 bits).
\item
  For a negative number \texttt{-x}, compute \texttt{2\^{}bits\ -\ x}.
\item
  Store the result as an ordinary binary integer.
\end{enumerate}

Example with 8 bits:

\begin{itemize}
\tightlist
\item
  \texttt{+5} → \texttt{00000101}
\item
  \texttt{-5} → \texttt{11111011}
\item
  \texttt{-1} → \texttt{11111111}
\end{itemize}

Why two's complement is powerful:

\begin{itemize}
\tightlist
\item
  Addition and subtraction ``just work'' with the same circuitry for
  signed and unsigned.
\item
  There is only one representation of zero.
\end{itemize}

\subsubsection{Working Example (Python)}\label{working-example-python}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Decimal 42 in different bases}
\NormalTok{n }\OperatorTok{=} \DecValTok{42}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Decimal:"}\NormalTok{, n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binary :"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(n))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Octal  :"}\NormalTok{, }\BuiltInTok{oct}\NormalTok{(n))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hex    :"}\NormalTok{, }\BuiltInTok{hex}\NormalTok{(n))}

\CommentTok{\# Two\textquotesingle{}s complement for {-}5 in 8 bits}
\KeywordTok{def}\NormalTok{ to\_twos\_complement(x: }\BuiltInTok{int}\NormalTok{, bits: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{8}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{str}\NormalTok{:}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return} \BuiltInTok{format}\NormalTok{(x, }\SpecialStringTok{f"0}\SpecialCharTok{\{}\NormalTok{bits}\SpecialCharTok{\}}\SpecialStringTok{b"}\NormalTok{)}
    \ControlFlowTok{return} \BuiltInTok{format}\NormalTok{((}\DecValTok{1} \OperatorTok{\textless{}\textless{}}\NormalTok{ bits) }\OperatorTok{+}\NormalTok{ x, }\SpecialStringTok{f"0}\SpecialCharTok{\{}\NormalTok{bits}\SpecialCharTok{\}}\SpecialStringTok{b"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"+5:"}\NormalTok{, to\_twos\_complement(}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}5:"}\NormalTok{, to\_twos\_complement(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Output:

\begin{verbatim}
Decimal: 42
Binary : 0b101010
Octal  : 0o52
Hex    : 0x2a
+5: 00000101
-5: 11111011
\end{verbatim}

\subsubsection{Why It Matters}\label{why-it-matters-1}

\begin{itemize}
\tightlist
\item
  Programmer convenience: Hex makes binary compact and human-readable.
\item
  Hardware design: Two's complement ensures arithmetic circuits are
  simple and unified.
\item
  Debugging: Memory dumps, CPU registers, and network packets are
  usually shown in hex.
\end{itemize}

\subsubsection{Exercises}\label{exercises-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convert \texttt{100} into binary, octal, and hex.
\item
  Write \texttt{-7} in 8-bit two's complement.
\item
  Verify that \texttt{0xFF} is equal to 255.
\item
  Parse the bitstring \texttt{"11111001"} as an 8-bit two's complement
  number.
\item
  Explain why engineers prefer two's complement over ``sign-magnitude''
  representation.
\end{enumerate}

\subsection{1.1 L2. Floating-Point and Precision
Issues}\label{l2.-floating-point-and-precision-issues}

Not all numbers are integers. To approximate fractions, scientific
notation, and very large or very small values, computers use
floating-point representation. The de-facto standard is IEEE-754, which
defines how real numbers are encoded, how special values are handled,
and what precision guarantees exist.

\subsubsection{Structure of Floating-Point
Numbers}\label{structure-of-floating-point-numbers}

A floating-point value is composed of three fields:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sign bit (s) --- indicates positive (\texttt{0}) or negative
  (\texttt{1}).
\item
  Exponent (e) --- determines the scale or ``magnitude.''
\item
  Mantissa / significand (m) --- contains the significant digits.
\end{enumerate}

The value is interpreted as:

\[
(-1)^s \times 1.m \times 2^{(e - \text{bias})}
\]

Example: IEEE-754 single precision (32 bits)

\begin{itemize}
\tightlist
\item
  1 sign bit
\item
  8 exponent bits (bias = 127)
\item
  23 mantissa bits
\end{itemize}

\subsubsection{Exact vs Approximate
Representation}\label{exact-vs-approximate-representation}

Some numbers are represented exactly:

\begin{itemize}
\tightlist
\item
  \texttt{1.0} has a clean binary form.
\end{itemize}

Others cannot be represented precisely:

\begin{itemize}
\tightlist
\item
  \texttt{0.1} in decimal is a repeating fraction in binary, so the
  closest approximation is stored.
\end{itemize}

Python example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=} \FloatTok{0.1} \OperatorTok{+} \FloatTok{0.2}
\BuiltInTok{print}\NormalTok{(}\StringTok{"0.1 + 0.2 ="}\NormalTok{, a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal to 0.3?"}\NormalTok{, a }\OperatorTok{==} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Output:

\begin{verbatim}
0.1 + 0.2 = 0.30000000000000004
Equal to 0.3? False
\end{verbatim}

\subsubsection{Special Values}\label{special-values}

IEEE-754 reserves encodings for special cases:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Sign & Exponent & Mantissa & Meaning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0/1 & all 1s & 0 & +∞ / −∞ \\
0/1 & all 1s & nonzero & NaN (Not a Number) \\
0/1 & all 0s & nonzero & Denormals (gradual underflow) \\
\end{longtable}

Examples:

\begin{itemize}
\tightlist
\item
  Division by zero produces infinity: \texttt{1.0\ /\ 0.0\ =\ inf}.
\item
  \texttt{0.0\ /\ 0.0} yields \texttt{NaN}, which propagates in
  computations.
\item
  Denormals allow gradual precision near zero.
\end{itemize}

\subsubsection{Arbitrary Precision}\label{arbitrary-precision}

Languages like Python and libraries like GMP provide arbitrary-precision
arithmetic:

\begin{itemize}
\tightlist
\item
  Integers (\texttt{int}) can grow as large as memory allows.
\item
  Decimal libraries (\texttt{decimal.Decimal} in Python) allow exact
  decimal arithmetic.
\item
  These are slower, but essential for cryptography, symbolic
  computation, and finance.
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Infinity:"}\NormalTok{, }\FloatTok{1.0} \OperatorTok{/} \FloatTok{0.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NaN:"}\NormalTok{, }\FloatTok{0.0} \OperatorTok{/} \FloatTok{0.0}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Is NaN?"}\NormalTok{, math.isnan(}\BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}nan\textquotesingle{}}\NormalTok{)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Is Inf?"}\NormalTok{, math.isinf(}\BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)))}

\CommentTok{\# Arbitrary precision integer}
\NormalTok{big }\OperatorTok{=} \DecValTok{2200}
\BuiltInTok{print}\NormalTok{(}\StringTok{"2200 ="}\NormalTok{, big)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-2}

\begin{itemize}
\tightlist
\item
  Rounding surprises: Many decimal fractions cannot be represented
  exactly.
\item
  Error propagation: Repeated arithmetic may accumulate tiny
  inaccuracies.
\item
  Special values: NaN and infinity must be handled carefully.
\item
  Domain correctness: Cryptography, finance, and symbolic algebra
  require exact precision.
\end{itemize}

\subsubsection{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down the IEEE-754 representation (sign, exponent, mantissa) of
  \texttt{1.0}.
\item
  Explain why \texttt{0.1} is not exactly representable in binary.
\item
  Test in Python whether
  \texttt{float(\textquotesingle{}nan\textquotesingle{})\ ==\ float(\textquotesingle{}nan\textquotesingle{})}.
  What happens, and why?
\item
  Find the smallest positive number you can add to \texttt{1.0} before
  it changes (machine epsilon).
\item
  Why is arbitrary precision slower but critical in some applications?
\end{enumerate}

\section{1.2 Basic Operations}\label{basic-operations}

\subsection{1.2 L0. Addition, Subtraction, Multiplication,
Division}\label{l0.-addition-subtraction-multiplication-division}

An arithmetic operation combines numbers to produce a new number. At
this level we focus on four basics: addition, subtraction,
multiplication, and division---first with decimal intuition, then a peek
at how the same ideas look in binary. Mastering these is essential
before moving to algorithms that build on them.

\subsubsection{Intuition: place value +
carrying/borrowing}\label{intuition-place-value-carryingborrowing}

All four operations are versions of combining place values (ones, tens,
hundreds \ldots; or in binary: ones, twos, fours \ldots).

\begin{itemize}
\tightlist
\item
  Addition: add column by column; if a column exceeds the base, carry 1
  to the next column.
\item
  Subtraction: subtract column by column; if a column is too small,
  borrow 1 from the next column.
\item
  Multiplication: repeated addition; multiply by each digit and shift
  (place value), then add partial results.
\item
  Division: repeated subtraction or sharing; find how many times a
  number ``fits,'' track the remainder.
\end{itemize}

These rules are identical in any base. Only the place values change.

\subsubsection{Decimal examples (by
hand)}\label{decimal-examples-by-hand}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Addition (carry)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{478}
 \ExtensionTok{+}\NormalTok{ 259}
 \ExtensionTok{{-}{-}{-}{-}}
   \ExtensionTok{737}    \ErrorTok{(}\ExtensionTok{8+9=17}\NormalTok{ → write 7, carry 1}\KeywordTok{;} \ExtensionTok{7+5+1=13}\NormalTok{ → write 3, carry 1}\KeywordTok{;} \ExtensionTok{4+2+1=7}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Subtraction (borrow)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{503}
 \ExtensionTok{{-}}\NormalTok{  78}
 \ExtensionTok{{-}{-}{-}{-}}
   \ExtensionTok{425}    \ErrorTok{(}\ExtensionTok{3{-}8}\NormalTok{ borrow → 13{-}8=5}\KeywordTok{;} \ExtensionTok{0}\NormalTok{ became }\AttributeTok{{-}1}\NormalTok{ so borrow from 5 → 9{-}7=2}\KeywordTok{;} \ExtensionTok{4}\NormalTok{ stays 4}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Multiplication (partial sums)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{214}
 \ExtensionTok{×}\NormalTok{   3}
 \ExtensionTok{{-}{-}{-}{-}}
   \ExtensionTok{642}    \ErrorTok{(}\ExtensionTok{214*3}\NormalTok{ = 642}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Long division (quotient + remainder)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
  \ExtensionTok{47}\NormalTok{ ÷ 5 → 9 remainder 2   }\ErrorTok{(}\ExtensionTok{because}\NormalTok{ 5}\PreprocessorTok{*}\NormalTok{9 = 45, leftover 2}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Binary peek (same rules, base
2)}\label{binary-peek-same-rules-base-2}

\begin{itemize}
\item
  Add rules: 0+0=0, 0+1=1, 1+0=1, 1+1=10₂ (write 0, carry 1)
\item
  Subtract rules: 0−0=0, 1−0=1, 1−1=0, 0−1 → borrow (becomes 10₂−1=1,
  borrow 1)
\end{itemize}

Example: \(1011₂ + 0110₂\)

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{1011}
 \ExtensionTok{+}\NormalTok{ 0110}
 \ExtensionTok{{-}{-}{-}{-}{-}{-}}
  \ExtensionTok{10001}   \ErrorTok{(}\ExtensionTok{1+0=1}\KeywordTok{;} \ExtensionTok{1+1=0}\NormalTok{ carry1}\KeywordTok{;} \ExtensionTok{0+1+carry=0}\NormalTok{ carry1}\KeywordTok{;} \ExtensionTok{1+0+carry=0}\NormalTok{ carry1 → carry out}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Worked examples (Python)}\label{worked-examples-python}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Basic arithmetic with integers}
\NormalTok{a, b }\OperatorTok{=} \DecValTok{478}\NormalTok{, }\DecValTok{259}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a+b ="}\NormalTok{, a }\OperatorTok{+}\NormalTok{ b)      }\CommentTok{\# 737}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a{-}b ="}\NormalTok{, a }\OperatorTok{{-}}\NormalTok{ b)      }\CommentTok{\# 219}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a*b ="}\NormalTok{, a }\OperatorTok{*}\NormalTok{ b)      }\CommentTok{\# 123,  478*259 = 123,  ... actually compute:}
\BuiltInTok{print}\NormalTok{(}\StringTok{"47//5 ="}\NormalTok{, }\DecValTok{47} \OperatorTok{//} \DecValTok{5}\NormalTok{)  }\CommentTok{\# integer division {-}\textgreater{} 9}
\BuiltInTok{print}\NormalTok{(}\StringTok{"47\%5  ="}\NormalTok{, }\DecValTok{47} \OperatorTok{\%} \DecValTok{5}\NormalTok{)   }\CommentTok{\# remainder {-}\textgreater{} 2}

\CommentTok{\# Show carry/borrow intuition using binary strings}
\NormalTok{x, y }\OperatorTok{=} \BaseNTok{0b1011}\NormalTok{, }\BaseNTok{0b0110}
\NormalTok{s }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x+y (binary):"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(x), }\StringTok{"+"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(y), }\StringTok{"="}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(s))}

\CommentTok{\# Small helper: manual long division that returns (quotient, remainder)}
\KeywordTok{def}\NormalTok{ long\_divide(n: }\BuiltInTok{int}\NormalTok{, d: }\BuiltInTok{int}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ d }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ZeroDivisionError}\NormalTok{(}\StringTok{"division by zero"}\NormalTok{)}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ n }\OperatorTok{//}\NormalTok{ d}
\NormalTok{    r }\OperatorTok{=}\NormalTok{ n }\OperatorTok{\%}\NormalTok{ d}
    \ControlFlowTok{return}\NormalTok{ q, r}

\BuiltInTok{print}\NormalTok{(}\StringTok{"long\_divide(47,5):"}\NormalTok{, long\_divide(}\DecValTok{47}\NormalTok{, }\DecValTok{5}\NormalTok{))  }\CommentTok{\# (9, 2)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: \texttt{//} is integer division in Python; \texttt{\%} is the
remainder. For now we focus on integers (no decimals).
\end{quote}

\subsubsection{Why it matters}\label{why-it-matters-3}

\begin{itemize}
\tightlist
\item
  Every higher-level algorithm (searching, hashing, cryptography,
  numeric methods) relies on these operations.
\item
  Understanding carry/borrow makes binary arithmetic and bit-level
  reasoning feel natural.
\item
  Knowing integer division and remainder is vital for base conversions,
  hashing (\texttt{mod}), and many algorithmic patterns.
\end{itemize}

\subsubsection{Exercises}\label{exercises-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute by hand, then verify in Python:

  \begin{itemize}
  \tightlist
  \item
    \(326 + 589\)
  \item
    \(704 - 259\)
  \item
    \(38 \times 12\)
  \item
    \(123 \div 7\) (give quotient and remainder)
  \end{itemize}
\item
  In binary, add \(10101₂ + 111₍₂₎\). Show carries.
\item
  Write a short Python snippet that prints the quotient and remainder
  for \texttt{n=200} divided by \texttt{d=23}.
\item
  Convert your remainder into a sentence: ``200 = 23 × (quotient) +
  (remainder)''.
\item
  Challenge: Multiply \(19 \times 23\) by hand using partial sums; then
  check with Python.
\end{enumerate}

\subsection{1.2 L1. Division, Modulo, and
Efficiency}\label{l1.-division-modulo-and-efficiency}

Beyond the simple four arithmetic operations, programmers need to think
about division with remainder, the modulo operator, and how efficient
these operations are on real machines. Addition and subtraction are
almost always ``constant time,'' but division can be slower, and
understanding modulo is essential for algorithms like hashing,
cryptography, and scheduling.

\subsubsection{Integer Division and
Modulo}\label{integer-division-and-modulo}

For integers, division produces both a quotient and a remainder.

\begin{itemize}
\item
  Mathematical definition: for integers \(n, d\) with \(d \neq 0\),

  \[
  n = d \times q + r, \quad 0 \leq r < |d|
  \]

  where \(q\) is the quotient, \(r\) the remainder.
\item
  Programming notation (Python):

  \begin{itemize}
  \tightlist
  \item
    \texttt{n\ //\ d} → quotient
  \item
    \texttt{n\ \%\ d} → remainder
  \end{itemize}
\end{itemize}

Examples:

\begin{itemize}
\tightlist
\item
  \texttt{47\ //\ 5\ =\ 9}, \texttt{47\ \%\ 5\ =\ 2} because
  \(47 = 5 \times 9 + 2\).
\item
  \texttt{23\ //\ 7\ =\ 3}, \texttt{23\ \%\ 7\ =\ 2} because
  \(23 = 7 \times 3 + 2\).
\end{itemize}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
n & d & n // d & n \% d \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
47 & 5 & 9 & 2 \\
23 & 7 & 3 & 2 \\
100 & 9 & 11 & 1 \\
\end{longtable}

\subsubsection{Modulo in Algorithms}\label{modulo-in-algorithms}

The modulo operation is a workhorse in programming:

\begin{itemize}
\item
  Hashing: To map a large integer into a table of size \texttt{m}, use
  \texttt{key\ \%\ m}.
\item
  Cyclic behavior: To loop back after \texttt{7} days in a week:
  \texttt{(day\ +\ shift)\ \%\ 7}.
\item
  Cryptography: Modular arithmetic underlies RSA, Diffie--Hellman, and
  many number-theoretic algorithms.
\end{itemize}

\subsubsection{Efficiency
Considerations}\label{efficiency-considerations}

\begin{itemize}
\tightlist
\item
  Addition and subtraction: generally 1 CPU cycle.
\item
  Multiplication: slightly more expensive, but still fast on modern
  hardware.
\item
  Division and modulo: slower, often an order of magnitude more costly
  than multiplication.
\end{itemize}

Practical tricks:

\begin{itemize}
\item
  If \texttt{d} is a power of two, \texttt{n\ \%\ d} can be computed by
  a bitmask.

  \begin{itemize}
  \tightlist
  \item
    Example: \texttt{n\ \%\ 8\ ==\ n\ \&\ 7} (since 8 = 2³).
  \end{itemize}
\item
  Some compilers automatically optimize modulo when the divisor is
  constant.
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Quotient and remainder}
\NormalTok{n, d }\OperatorTok{=} \DecValTok{47}\NormalTok{, }\DecValTok{5}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Quotient:"}\NormalTok{, n }\OperatorTok{//}\NormalTok{ d)  }\CommentTok{\# 9}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Remainder:"}\NormalTok{, n }\OperatorTok{\%}\NormalTok{ d)  }\CommentTok{\# 2}

\CommentTok{\# Identity check: n == d*q + r}
\NormalTok{q, r }\OperatorTok{=} \BuiltInTok{divmod}\NormalTok{(n, d)  }\CommentTok{\# built{-}in tuple return}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Check:"}\NormalTok{, d}\OperatorTok{*}\NormalTok{q }\OperatorTok{+}\NormalTok{ r }\OperatorTok{==}\NormalTok{ n)}

\CommentTok{\# Modulo for cyclic behavior: days of week}
\NormalTok{days }\OperatorTok{=}\NormalTok{ [}\StringTok{"Mon"}\NormalTok{, }\StringTok{"Tue"}\NormalTok{, }\StringTok{"Wed"}\NormalTok{, }\StringTok{"Thu"}\NormalTok{, }\StringTok{"Fri"}\NormalTok{, }\StringTok{"Sat"}\NormalTok{, }\StringTok{"Sun"}\NormalTok{]}
\NormalTok{start }\OperatorTok{=} \DecValTok{5}  \CommentTok{\# Saturday}
\NormalTok{shift }\OperatorTok{=} \DecValTok{4}
\NormalTok{future\_day }\OperatorTok{=}\NormalTok{ days[(start }\OperatorTok{+}\NormalTok{ shift) }\OperatorTok{\%} \DecValTok{7}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Start Saturday + 4 days ="}\NormalTok{, future\_day)}

\CommentTok{\# Optimization: power{-}of{-}two modulo with bitmask}
\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ [}\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{20}\NormalTok{]:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ \% 8 = }\SpecialCharTok{\{}\NormalTok{n }\OperatorTok{\%} \DecValTok{8}\SpecialCharTok{\}}\SpecialStringTok{, bitmask }\SpecialCharTok{\{}\NormalTok{n }\OperatorTok{\&} \DecValTok{7}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Output:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Quotient:}\NormalTok{ 9}
\ExtensionTok{Remainder:}\NormalTok{ 2}
\ExtensionTok{Check:}\NormalTok{ True}
\ExtensionTok{Start}\NormalTok{ Saturday + 4 days = Wed}
\ExtensionTok{5}\NormalTok{ \% 8 = 5, bitmask 5}
\ExtensionTok{12}\NormalTok{ \% 8 = 4, bitmask 4}
\ExtensionTok{20}\NormalTok{ \% 8 = 4, bitmask 4}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-4}

\begin{itemize}
\tightlist
\item
  Real programs rely heavily on modulo for indexing, hashing, and
  wrap-around logic.
\item
  Division is computationally more expensive; knowing when to replace it
  with bit-level operations improves performance.
\item
  Modular arithmetic introduces a new ``world'' where numbers wrap
  around --- the foundation of many advanced algorithms.
\end{itemize}

\subsubsection{Exercises}\label{exercises-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute by hand and confirm in Python:

  \begin{itemize}
  \tightlist
  \item
    \texttt{100\ //\ 9} and \texttt{100\ \%\ 9}
  \item
    \texttt{123\ //\ 11} and \texttt{123\ \%\ 11}
  \end{itemize}
\item
  Write a function that simulates a clock: given \texttt{hour} and
  \texttt{shift}, return the new hour (24-hour cycle).
\item
  Prove the identity: for any integers \texttt{n} and \texttt{d},

\begin{verbatim}
n == d * (n // d) + (n % d)
\end{verbatim}

  by trying with random values.
\item
  Show how to replace \texttt{n\ \%\ 16} with a bitwise operation. Why
  does it work?
\item
  Challenge: Write a short Python function to check if a number is
  divisible by 7 using only \texttt{\%} and \texttt{//}.
\end{enumerate}

\subsection{1.2 L2. Fast Arithmetic
Algorithms}\label{l2.-fast-arithmetic-algorithms}

When numbers grow large, the naïve methods for multiplication and
division become too slow. On paper, long multiplication takes \(O(n^2)\)
steps for \(n\)-digit numbers. Computers face the same issue:
multiplying two very large integers digit by digit can be expensive.
Fast arithmetic algorithms reduce this cost, using clever
divide-and-conquer techniques or transformations into other domains.

\subsubsection{Multiplication Beyond the School
Method}\label{multiplication-beyond-the-school-method}

Naïve long multiplication

\begin{itemize}
\tightlist
\item
  Treats an \(n\)-digit number as a sequence of digits.
\item
  Each digit of one number multiplies every digit of the other.
\item
  Complexity: \(O(n^2)\).
\item
  Works fine for small integers, but too slow for cryptography or
  big-number libraries.
\end{itemize}

Karatsuba's Algorithm

\begin{itemize}
\item
  Discovered in 1960 by Anatoly Karatsuba.
\item
  Idea: split numbers into halves and reduce multiplications.
\item
  Complexity: \(O(n^{\log_2 3}) \approx O(n^{1.585})\).
\item
  Recursive strategy:

  \begin{itemize}
  \item
    For numbers \(x = x_1 \cdot B^m + x_0\),
    \(y = y_1 \cdot B^m + y_0\).
  \item
    Compute 3 multiplications instead of 4:

    \begin{itemize}
    \tightlist
    \item
      \(z_0 = x_0 y_0\)
    \item
      \(z_2 = x_1 y_1\)
    \item
      \(z_1 = (x_0+x_1)(y_0+y_1) - z_0 - z_2\)
    \end{itemize}
  \item
    Result: \(z_2 \cdot B^{2m} + z_1 \cdot B^m + z_0\).
  \end{itemize}
\end{itemize}

FFT-based Multiplication (Schönhage--Strassen and successors)

\begin{itemize}
\tightlist
\item
  Represent numbers as polynomials of their digits.
\item
  Multiply polynomials efficiently using Fast Fourier Transform.
\item
  Complexity: near \(O(n \log n)\).
\item
  Used in modern big-integer libraries (e.g.~GNU MP, Java's
  \texttt{BigInteger}).
\end{itemize}

\subsubsection{Division Beyond Long
Division}\label{division-beyond-long-division}

\begin{itemize}
\tightlist
\item
  Naïve long division: \(O(n^2)\) for \(n\)-digit dividend.
\item
  Newton's method for reciprocal: approximate \(1/d\) using
  Newton--Raphson iterations, then multiply by \(n\).
\item
  Complexity: tied to multiplication --- if multiplication is fast, so
  is division.
\end{itemize}

\subsubsection{Modular Exponentiation}\label{modular-exponentiation}

Fast arithmetic also matters in modular contexts (cryptography).

\begin{itemize}
\item
  Compute \(a^b \bmod m\) efficiently.
\item
  Square-and-multiply (binary exponentiation):

  \begin{itemize}
  \tightlist
  \item
    Write \(b\) in binary.
  \item
    For each bit: square result, multiply if bit=1.
  \item
    Complexity: \(O(\log b)\) multiplications.
  \end{itemize}
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Naïve multiplication}
\KeywordTok{def}\NormalTok{ naive\_mul(x: }\BuiltInTok{int}\NormalTok{, y: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y  }\CommentTok{\# Python already uses fast methods internally}

\CommentTok{\# Karatsuba multiplication (recursive, simplified)}
\KeywordTok{def}\NormalTok{ karatsuba(x: }\BuiltInTok{int}\NormalTok{, y: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \CommentTok{\# base case}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textless{}} \DecValTok{10} \KeywordTok{or}\NormalTok{ y }\OperatorTok{\textless{}} \DecValTok{10}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y}
    \CommentTok{\# split numbers}
\NormalTok{    n }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(x.bit\_length(), y.bit\_length())}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ n }\OperatorTok{//} \DecValTok{2}
\NormalTok{    high1, low1 }\OperatorTok{=} \BuiltInTok{divmod}\NormalTok{(x, }\DecValTok{1} \OperatorTok{\textless{}\textless{}}\NormalTok{ m)}
\NormalTok{    high2, low2 }\OperatorTok{=} \BuiltInTok{divmod}\NormalTok{(y, }\DecValTok{1} \OperatorTok{\textless{}\textless{}}\NormalTok{ m)}
\NormalTok{    z0 }\OperatorTok{=}\NormalTok{ karatsuba(low1, low2)}
\NormalTok{    z2 }\OperatorTok{=}\NormalTok{ karatsuba(high1, high2)}
\NormalTok{    z1 }\OperatorTok{=}\NormalTok{ karatsuba(low1 }\OperatorTok{+}\NormalTok{ high1, low2 }\OperatorTok{+}\NormalTok{ high2) }\OperatorTok{{-}}\NormalTok{ z0 }\OperatorTok{{-}}\NormalTok{ z2}
    \ControlFlowTok{return}\NormalTok{ (z2 }\OperatorTok{\textless{}\textless{}}\NormalTok{ (}\DecValTok{2}\OperatorTok{*}\NormalTok{m)) }\OperatorTok{+}\NormalTok{ (z1 }\OperatorTok{\textless{}\textless{}}\NormalTok{ m) }\OperatorTok{+}\NormalTok{ z0}

\CommentTok{\# Modular exponentiation (square{-}and{-}multiply)}
\KeywordTok{def}\NormalTok{ modexp(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{, m: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    result }\OperatorTok{=} \DecValTok{1}
\NormalTok{    base }\OperatorTok{=}\NormalTok{ a }\OperatorTok{\%}\NormalTok{ m}
\NormalTok{    exp }\OperatorTok{=}\NormalTok{ b}
    \ControlFlowTok{while}\NormalTok{ exp }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{if}\NormalTok{ exp }\OperatorTok{\&} \DecValTok{1}\NormalTok{:}
\NormalTok{            result }\OperatorTok{=}\NormalTok{ (result }\OperatorTok{*}\NormalTok{ base) }\OperatorTok{\%}\NormalTok{ m}
\NormalTok{        base }\OperatorTok{=}\NormalTok{ (base }\OperatorTok{*}\NormalTok{ base) }\OperatorTok{\%}\NormalTok{ m}
\NormalTok{        exp }\OperatorTok{\textgreater{}\textgreater{}=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ result}

\CommentTok{\# Demo}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Karatsuba(1234, 5678) ="}\NormalTok{, karatsuba(}\DecValTok{1234}\NormalTok{, }\DecValTok{5678}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"pow(7, 128, 13) ="}\NormalTok{, modexp(}\DecValTok{7}\NormalTok{, }\DecValTok{128}\NormalTok{, }\DecValTok{13}\NormalTok{))  }\CommentTok{\# fast modular exponentiation}
\end{Highlighting}
\end{Shaded}

Output:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Karatsuba}\ErrorTok{(}\ExtensionTok{1234,}\NormalTok{ 5678}\KeywordTok{)} \ExtensionTok{=}\NormalTok{ 7006652}
\ExtensionTok{pow}\ErrorTok{(}\ExtensionTok{7,}\NormalTok{ 128, 13}\KeywordTok{)} \ExtensionTok{=}\NormalTok{ 3}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-5}

\begin{itemize}
\tightlist
\item
  Cryptography: RSA requires multiplying and dividing integers with
  thousands of digits.
\item
  Computer algebra systems: symbolic computation depends on fast
  polynomial/integer arithmetic.
\item
  Big data / simulation: arbitrary precision needed when floats are not
  exact.
\end{itemize}

\subsubsection{Exercises}\label{exercises-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Multiply 31415926 × 27182818 using:

  \begin{itemize}
  \tightlist
  \item
    Python's \texttt{*}
  \item
    Your Karatsuba implementation. Compare results.
  \end{itemize}
\item
  Implement \texttt{modexp(a,\ b,\ m)} for \(a=5, b=117, m=19\). Confirm
  with Python's built-in \texttt{pow(a,\ b,\ m)}.
\item
  Explain why Newton's method for division depends on fast
  multiplication.
\item
  Research: what is the current fastest known multiplication algorithm
  for large integers?
\item
  Challenge: Modify Karatsuba to print intermediate
  \texttt{z0,\ z1,\ z2} values for small inputs to visualize the
  recursion.
\end{enumerate}

\section{1.3 Properties}\label{properties}

\subsection{1.3 L0 --- Simple Number
Properties}\label{l0-simple-number-properties}

Numbers have patterns that help us reason about algorithms without heavy
mathematics. At this level we focus on basic properties: even vs odd,
divisibility, and remainders. These ideas show up everywhere---from loop
counters to data structure layouts.

\subsubsection{Even and Odd}\label{even-and-odd}

A number is even if it ends with digit 0, 2, 4, 6, or 8 in decimal, and
odd otherwise.

\begin{itemize}
\item
  In binary, checking parity is even easier: the last bit tells the
  story.

  \begin{itemize}
  \tightlist
  \item
    \texttt{…0} → even
  \item
    \texttt{…1} → odd
  \end{itemize}
\end{itemize}

Example in Python:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ is\_even(n: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{bool}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{ n }\OperatorTok{\%} \DecValTok{2} \OperatorTok{==} \DecValTok{0}

\BuiltInTok{print}\NormalTok{(is\_even(}\DecValTok{10}\NormalTok{))  }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(is\_even(}\DecValTok{7}\NormalTok{))   }\CommentTok{\# False}
\end{Highlighting}
\end{Shaded}

\subsubsection{Divisibility}\label{divisibility}

We often ask: does one number divide another?

\begin{itemize}
\tightlist
\item
  \texttt{a} is divisible by \texttt{b} if there exists some integer
  \texttt{k} with \texttt{a\ =\ b\ *\ k}.
\item
  In code: \texttt{a\ \%\ b\ ==\ 0}.
\end{itemize}

Examples:

\begin{itemize}
\tightlist
\item
  12 is divisible by 3 → \texttt{12\ \%\ 3\ ==\ 0}.
\item
  14 is not divisible by 5 → \texttt{14\ \%\ 5\ ==\ 4}.
\end{itemize}

\subsubsection{Remainders and Modular
Thinking}\label{remainders-and-modular-thinking}

When dividing, the remainder is what's left over.

\begin{itemize}
\item
  Example: \texttt{17\ //\ 5\ =\ 3}, remainder \texttt{2}.
\item
  Modular arithmetic wraps around like a clock:

  \begin{itemize}
  \tightlist
  \item
    \texttt{(17\ \%\ 5)\ =\ 2} → same as ``2 o'clock after going 17
    steps around a 5-hour clock.''
  \end{itemize}
\end{itemize}

This ``wrap-around'' view is central in array indexing, hashing, and
cryptography later on.

\subsubsection{Why It Matters}\label{why-it-matters-6}

\begin{itemize}
\tightlist
\item
  Algorithms: Parity checks decide branching (e.g., even-odd
  optimizations).
\item
  Data structures: Array indices often wrap around using \texttt{\%}.
\item
  Everyday: Calendars cycle days of the week; remainders formalize that.
\end{itemize}

\subsubsection{Exercises}\label{exercises-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function that returns \texttt{"even"} or \texttt{"odd"} for a
  given number.
\item
  Check if 91 is divisible by 7.
\item
  Compute the remainder of 100 divided by 9.
\item
  Use \texttt{\%} to simulate a 7-day week: if today is day 5 (Saturday)
  and you add 10 days, what day is it?
\item
  Find the last digit of \texttt{2\^{}15} without computing the full
  number (hint: check the remainder mod 10).
\end{enumerate}

\subsection{1.3 L1 --- Classical Number Theory
Tools}\label{l1-classical-number-theory-tools}

Beyond simple parity and divisibility, algorithms often need deeper
number properties. At this level we introduce a few ``toolkit'' ideas
from elementary number theory: greatest common divisor (GCD), least
common multiple (LCM), and modular arithmetic identities. These are
lightweight but powerful concepts that show up in algorithm design,
cryptography, and optimization.

\subsubsection{Greatest Common Divisor
(GCD)}\label{greatest-common-divisor-gcd}

The GCD of two numbers is the largest number that divides both.

\begin{itemize}
\tightlist
\item
  Example: \texttt{gcd(20,\ 14)\ =\ 2}.
\item
  Why useful: GCD simplifies fractions, ensures ratios are reduced, and
  appears in algorithm correctness proofs.
\end{itemize}

Euclid's Algorithm: Instead of trial division, we can compute GCD
quickly:

\begin{verbatim}
gcd(a, b) = gcd(b, a % b)
\end{verbatim}

This repeats until \texttt{b\ =\ 0}, at which point \texttt{a} is the
answer.

Python example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gcd(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \ControlFlowTok{while}\NormalTok{ b:}
\NormalTok{        a, b }\OperatorTok{=}\NormalTok{ b, a }\OperatorTok{\%}\NormalTok{ b}
    \ControlFlowTok{return}\NormalTok{ a}

\BuiltInTok{print}\NormalTok{(gcd(}\DecValTok{20}\NormalTok{, }\DecValTok{14}\NormalTok{))  }\CommentTok{\# 2}
\end{Highlighting}
\end{Shaded}

\subsubsection{Least Common Multiple
(LCM)}\label{least-common-multiple-lcm}

The LCM of two numbers is the smallest positive number divisible by
both.

\begin{itemize}
\item
  Example: \texttt{lcm(12,\ 18)\ =\ 36}.
\item
  Connection to GCD:

\begin{verbatim}
lcm(a, b) = (a * b) // gcd(a, b)
\end{verbatim}
\end{itemize}

This is useful in scheduling, periodic tasks, and synchronization
problems.

\subsubsection{Modular Arithmetic
Identities}\label{modular-arithmetic-identities}

Remainders behave predictably under operations:

\begin{itemize}
\tightlist
\item
  Addition:
  \texttt{(a\ +\ b)\ \%\ m\ =\ ((a\ \%\ m)\ +\ (b\ \%\ m))\ \%\ m}
\item
  Multiplication:
  \texttt{(a\ *\ b)\ \%\ m\ =\ ((a\ \%\ m)\ *\ (b\ \%\ m))\ \%\ m}
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  \texttt{(123\ +\ 456)\ \%\ 7\ =\ (123\ \%\ 7\ +\ 456\ \%\ 7)\ \%\ 7}
\item
  This property lets us work with small remainders instead of huge
  numbers, key in cryptography and hashing.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-7}

\begin{itemize}
\tightlist
\item
  Algorithms: GCD ensures efficiency in fraction reduction, graph
  algorithms, and number-theoretic algorithms.
\item
  Systems: LCM models periodicity, e.g., aligning CPU scheduling
  intervals.
\item
  Cryptography: Modular arithmetic underpins secure communication (RSA,
  Diffie-Hellman).
\item
  Practical programming: Modular identities simplify computations with
  limited ranges (hash tables, cyclic arrays).
\end{itemize}

\subsubsection{Exercises}\label{exercises-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \texttt{gcd(252,\ 198)} by hand using Euclid's algorithm.
\item
  Write a function that returns the LCM of two numbers. Test it on (12,
  18).
\item
  Show that \texttt{(37\ +\ 85)\ \%\ 12} equals
  \texttt{((37\ \%\ 12)\ +\ (85\ \%\ 12))\ \%\ 12}.
\item
  Reduce the fraction \texttt{84/126} using GCD.
\item
  Find the smallest day \texttt{d} such that \texttt{d} is a multiple of
  both 12 and 18 (hint: LCM).
\end{enumerate}

\subsection{1.3 L2 --- Advanced Number Theory in
Algorithms}\label{l2-advanced-number-theory-in-algorithms}

At this level, we move beyond everyday divisibility and Euclid's
algorithm. Modern algorithms frequently rely on deep number theory to
achieve efficiency. Topics such as modular inverses, Euler's totient
function, and primality tests are crucial foundations for cryptography,
randomized algorithms, and competitive programming.

\subsubsection{Modular Inverses}\label{modular-inverses}

The modular inverse of a number \texttt{a} (mod \texttt{m}) is an
integer \texttt{x} such that:

\begin{verbatim}
(a * x) % m = 1
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Example: the inverse of 3 modulo 7 is 5, because
  \texttt{(3*5)\ \%\ 7\ =\ 15\ \%\ 7\ =\ 1}.
\item
  Existence: an inverse exists if and only if \texttt{gcd(a,\ m)\ =\ 1}.
\item
  Computation: using the Extended Euclidean Algorithm.
\end{itemize}

This is the backbone of modular division and is heavily used in
cryptography (RSA), hash functions, and matrix inverses mod \texttt{p}.

\subsubsection{Euler's Totient Function
(φ)}\label{eulers-totient-function-ux3c6}

The function \texttt{φ(n)} counts the number of integers between 1 and
\texttt{n} that are coprime to \texttt{n}.

\begin{itemize}
\item
  Example: \texttt{φ(9)\ =\ 6} because \{1, 2, 4, 5, 7, 8\} are coprime
  to 9.
\item
  Key property (Euler's theorem):

\begin{verbatim}
a^φ(n) ≡ 1 (mod n)     if gcd(a, n) = 1
\end{verbatim}
\item
  Special case: Fermat's Little Theorem --- for prime \texttt{p},

\begin{verbatim}
a^(p-1) ≡ 1 (mod p)
\end{verbatim}
\end{itemize}

This result is central in modular exponentiation and cryptosystems like
RSA.

\subsubsection{Primality Testing}\label{primality-testing}

Determining if a number is prime is easy for small inputs but hard for
large ones. Efficient algorithms are essential:

\begin{itemize}
\tightlist
\item
  Trial division: works only for small \texttt{n}.
\item
  Fermat primality test: uses Fermat's Little Theorem to detect
  composites, but can be fooled by Carmichael numbers.
\item
  Miller--Rabin test: a probabilistic algorithm widely used in practice
  (cryptographic key generation).
\item
  AKS primality test: a deterministic polynomial-time method
  (theoretical importance).
\end{itemize}

Example intuition:

\begin{itemize}
\tightlist
\item
  For large \texttt{n}, we don't check all divisors; we test properties
  of \texttt{a\^{}k\ mod\ n} for random bases \texttt{a}.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-8}

\begin{itemize}
\tightlist
\item
  Cryptography: Public-key systems depend on modular inverses, Euler's
  theorem, and large primes.
\item
  Algorithms: Modular inverses simplify solving equations in modular
  arithmetic (e.g., Chinese Remainder Theorem applications).
\item
  Practical Computing: Randomized primality tests (like Miller--Rabin)
  balance correctness and efficiency in real-world systems.
\end{itemize}

\subsubsection{Exercises}\label{exercises-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the modular inverse of 7 modulo 13.
\item
  Compute φ(10) and verify Euler's theorem for \texttt{a\ =\ 3}.
\item
  Use Fermat's test to check whether 341 is prime. (Hint: try
  \texttt{a\ =\ 2}.)
\item
  Implement modular inverse using the Extended Euclidean Algorithm.
\item
  Research: why do cryptographic protocols prefer Miller--Rabin over
  AKS, even though AKS is deterministic?
\end{enumerate}

\section{1.4 Overflow \& Precision}\label{overflow-precision}

\subsection{1.4 L0 - When Numbers Get Too Big or Too
Small}\label{l0---when-numbers-get-too-big-or-too-small}

Numbers inside a computer are stored with a fixed number of bits. This
means they can only represent values up to a certain limit. If a
calculation produces a result larger than this limit, the value ``wraps
around,'' much like the digits on an odometer rolling over after 999 to
000. This phenomenon is called overflow. Similarly, computers often
cannot represent all decimal fractions exactly, leading to tiny errors
called precision loss.

\subsubsection{Deep Dive}\label{deep-dive}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Integer Overflow

  \begin{itemize}
  \tightlist
  \item
    A computer uses a fixed number of bits (commonly 8, 16, 32, or 64)
    to store integers.
  \item
    An 8-bit unsigned integer can represent values from 0 to 255. Adding
    1 to 255 causes the value to wrap back to 0.
  \item
    Signed integers use \emph{two's complement} representation. For an
    8-bit signed integer, the range is −128 to +127. Adding 1 to 127
    makes it overflow to −128.
  \end{itemize}

  Example in binary:

\begin{verbatim}
11111111₂ (255) + 1 = 00000000₂ (0)
01111111₂ (+127) + 1 = 10000000₂ (−128)
\end{verbatim}
\item
  Floating-Point Precision

  \begin{itemize}
  \tightlist
  \item
    Decimal fractions like 0.1 cannot always be represented exactly in
    binary.
  \item
    As a result, calculations may accumulate tiny errors.
  \item
    For example, repeatedly adding 0.1 may not exactly equal 1.0 due to
    precision limits.
  \end{itemize}
\end{enumerate}

\subsubsection{Example}\label{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Integer overflow simulation with 8{-}bit values}
\KeywordTok{def}\NormalTok{ add\_8bit(a, b):}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ (a }\OperatorTok{+}\NormalTok{ b) }\OperatorTok{\%} \DecValTok{256}  \CommentTok{\# simulate wraparound}
    \ControlFlowTok{return}\NormalTok{ result}

\BuiltInTok{print}\NormalTok{(add\_8bit(}\DecValTok{250}\NormalTok{, }\DecValTok{10}\NormalTok{))   }\CommentTok{\# 260 wraps to 4}
\BuiltInTok{print}\NormalTok{(add\_8bit(}\DecValTok{255}\NormalTok{, }\DecValTok{1}\NormalTok{))    }\CommentTok{\# wraps to 0}

\CommentTok{\# Floating{-}point precision issue}
\NormalTok{x }\OperatorTok{=} \FloatTok{0.1} \OperatorTok{+} \FloatTok{0.2}
\BuiltInTok{print}\NormalTok{(x)           }\CommentTok{\# Expected 0.3, but gives 0.30000000000000004}
\BuiltInTok{print}\NormalTok{(x }\OperatorTok{==} \FloatTok{0.3}\NormalTok{)    }\CommentTok{\# False}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-9}

\begin{itemize}
\tightlist
\item
  Unexpected results: A calculation may suddenly produce a negative
  number or wrap around to zero.
\item
  Real-world impact:

  \begin{itemize}
  \tightlist
  \item
    Video games may show scores jumping strangely if counters overflow.
  \item
    Banking or financial systems must avoid losing cents due to
    floating-point errors.
  \item
    Engineers and scientists rely on careful handling of precision to
    ensure correct simulations.
  \end{itemize}
\item
  Foundation for algorithms: Understanding overflow and precision
  prepares you for later topics like hashing, cryptography, and
  numerical analysis.
\end{itemize}

\subsubsection{Exercises}\label{exercises-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a 4-bit unsigned integer system. What happens if you start at
  14 and keep adding 1?
\item
  In Python, try adding \texttt{0.1} to itself ten times. Does it equal
  exactly \texttt{1.0}? Why or why not?
\item
  Write a function that checks if an 8-bit signed integer addition would
  overflow.
\item
  Imagine you are programming a digital clock that uses 2 digits for
  minutes (00--59). What happens when the value goes from 59 to 60? How
  would you handle this?
\end{enumerate}

\subsection{1.4 L1 - Detecting and Managing Overflow in Real
Programs}\label{l1---detecting-and-managing-overflow-in-real-programs}

Computers don't do math in the abstract. Integers live in fixed-width
registers; floats follow IEEE-754. Robust software accounts for these
limits up front: choose the right representation, detect overflow, and
compare floats safely. The following sections explain how these issues
show up in practice and how to design around them.

\subsubsection{Deep Dive}\label{deep-dive-1}

\paragraph{1) Integer arithmetic in
practice}\label{integer-arithmetic-in-practice}

Fixed width means wraparound at \(2^{n}\). Unsigned wrap is modular
arithmetic; signed overflow (two's complement) can flip signs.
Developers often discover this the hard way when a counter suddenly goes
negative or wraps back to zero in production logs.

Bit width \& ranges This table reminds us of the hard limits baked into
hardware. Once the range is exceeded, the value doesn't ``grow
bigger''---it wraps.

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6067}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3371}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Width
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Signed range
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsigned range
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
32 & −2,147,483,648 \ldots{} 2,147,483,647 & 0 \ldots{} 4,294,967,295 \\
64 & −9,223,372,036,854,775,808 \ldots{} 9,223,372,036,854,775,807 & 0
\ldots{} 18,446,744,073,709,551,615 \\
\end{longtable}

Overflow semantics by language Each language makes slightly different
promises. This matters if you're writing cross-language services or
reading binary data across APIs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2201}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2956}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4151}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Language
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Signed overflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsigned overflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
C/C++ & UB (undefined) & Modular wrap & Use
UBSan/\texttt{-fsanitize=undefined}; widen types or check before add. \\
Rust & Traps in debug; defined APIs & \texttt{wrapping\_add},
\texttt{checked\_add}, \texttt{saturating\_add} & Make intent
explicit. \\
Java/Kotlin & Wraps (two's complement) & N/A (only signed types) & Use
\texttt{Math.addExact} to trap. \\
C\# & Wraps by default; \texttt{checked} to trap &
\texttt{checked}/\texttt{unchecked} blocks & \texttt{decimal} type for
money. \\
Python & Arbitrary precision & Arbitrary precision & Simulates fixed
width if needed. \\
\end{longtable}

A quick lesson: ``wrap'' may be safe in crypto or hashing, but it's
usually a bug in counters or indices. Always decide what you want up
front.

\paragraph{2) Floating-point you can depend
on}\label{floating-point-you-can-depend-on}

IEEE-754 doubles have \textasciitilde15--16 decimal digits and huge
dynamic range, but not exact decimal fractions. Think of floats as
\emph{convenient approximations}. They are perfect for physics
simulations, but brittle when used to represent cents in a bank account.

Where precision is lost These examples show why ``0.1 + 0.2 != 0.3''
isn't a joke---it's a direct consequence of binary storage.

\begin{itemize}
\tightlist
\item
  Scale mismatch: \(1\text{e}16 + 1 = 1\text{e}16\). The tiny
  \texttt{+1} gets lost.
\item
  Cancellation: subtracting nearly equal numbers deletes significant
  digits.
\item
  Decimal fractions (\texttt{0.1}) are repeating in binary.
\end{itemize}

Comparing floats Never compare with \texttt{==}. Instead, use a mixed
relative + absolute check:

\[
|x-y| \le \max(\text{rel}\cdot\max(|x|,|y|),\ \text{abs})
\]

This makes comparisons robust whether you're near zero or far away.

Rounding modes (when you explicitly care) Most of the time you don't
think about rounding---hardware defaults to ``round to nearest, ties to
even.'' But when writing financial systems or interval arithmetic, you
want to control it.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4396}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5604}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Round to nearest, ties to even (default) & General numeric work;
minimizes bias \\
Toward 0 / ±∞ & Bounds, interval arithmetic, conservative estimates \\
\end{longtable}

Having explicit rounding modes is like having a steering wheel---you
don't always turn, but you're glad it's there when the road curves.

Summation strategies The order of addition matters for floats. These
options give you a menu of accuracy vs.~speed.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
When to use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Naïve left-to-right & Worst & Low & Never for sensitive sums \\
Pairwise / tree & Better & Med & Parallel reductions, ``good
default'' \\
Kahan (compensated) & Best & Higher & Financial-ish aggregates, small
vectors \\
\end{longtable}

You don't need Kahan everywhere, but knowing it exists keeps you from
blaming ``mystery bugs'' on hardware.

Representation choices Sometimes the best answer is to avoid floats
entirely. Money is the classic example.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2235}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7765}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Use case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Recommended representation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Currency, invoicing & Fixed-point (e.g., cents as \texttt{int64}) or
\texttt{decimal}/\texttt{BigDecimal} \\
Scientific compute & \texttt{float64}, compensated sums, stable
algorithms \\
IDs, counters & \texttt{uint64}/\texttt{int64}, detect overflow on
boundaries \\
\end{longtable}

\subsubsection{Code (Python---portable
patterns)}\label{code-pythonportable-patterns}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 32{-}bit checked add (raises on overflow)}
\KeywordTok{def}\NormalTok{ add\_i32\_checked(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    s }\OperatorTok{=}\NormalTok{ a }\OperatorTok{+}\NormalTok{ b}
    \ControlFlowTok{if}\NormalTok{ s }\OperatorTok{\textless{}} \OperatorTok{{-}}\DecValTok{2\_147\_483\_648} \KeywordTok{or}\NormalTok{ s }\OperatorTok{\textgreater{}} \DecValTok{2\_147\_483\_647}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{OverflowError}\NormalTok{(}\StringTok{"int32 overflow"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ s}

\CommentTok{\# Simulate 32{-}bit wrap (intentional modular arithmetic)}
\KeywordTok{def}\NormalTok{ add\_i32\_wrapping(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    s }\OperatorTok{=}\NormalTok{ (a }\OperatorTok{+}\NormalTok{ b) }\OperatorTok{\&} \BaseNTok{0xFFFFFFFF}
    \ControlFlowTok{return}\NormalTok{ s }\OperatorTok{{-}} \BaseNTok{0x100000000} \ControlFlowTok{if}\NormalTok{ s }\OperatorTok{\&} \BaseNTok{0x80000000} \ControlFlowTok{else}\NormalTok{ s}

\CommentTok{\# Relative+absolute epsilon float compare}
\KeywordTok{def}\NormalTok{ almost\_equal(x: }\BuiltInTok{float}\NormalTok{, y: }\BuiltInTok{float}\NormalTok{, rel}\OperatorTok{=}\FloatTok{1e{-}12}\NormalTok{, abs\_}\OperatorTok{=}\FloatTok{1e{-}12}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{bool}\NormalTok{:}
    \ControlFlowTok{return} \BuiltInTok{abs}\NormalTok{(x }\OperatorTok{{-}}\NormalTok{ y) }\OperatorTok{\textless{}=} \BuiltInTok{max}\NormalTok{(rel }\OperatorTok{*} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{abs}\NormalTok{(x), }\BuiltInTok{abs}\NormalTok{(y)), abs\_)}

\CommentTok{\# Kahan (compensated) summation}
\KeywordTok{def}\NormalTok{ kahan\_sum(xs):}
\NormalTok{    s }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{    c }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs:}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{{-}}\NormalTok{ c}
\NormalTok{        t }\OperatorTok{=}\NormalTok{ s }\OperatorTok{+}\NormalTok{ y}
\NormalTok{        c }\OperatorTok{=}\NormalTok{ (t }\OperatorTok{{-}}\NormalTok{ s) }\OperatorTok{{-}}\NormalTok{ y}
\NormalTok{        s }\OperatorTok{=}\NormalTok{ t}
    \ControlFlowTok{return}\NormalTok{ s}

\CommentTok{\# Fixed{-}point cents (safe for \textasciitilde{}±9e16 cents with int64)}
\KeywordTok{def}\NormalTok{ dollars\_to\_cents(d: }\BuiltInTok{str}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    whole, \_, frac }\OperatorTok{=}\NormalTok{ d.partition(}\StringTok{"."}\NormalTok{)}
\NormalTok{    frac }\OperatorTok{=}\NormalTok{ (frac }\OperatorTok{+} \StringTok{"00"}\NormalTok{)[:}\DecValTok{2}\NormalTok{]}
    \ControlFlowTok{return} \BuiltInTok{int}\NormalTok{(whole) }\OperatorTok{*} \DecValTok{100} \OperatorTok{+} \BuiltInTok{int}\NormalTok{(frac)}

\KeywordTok{def}\NormalTok{ cents\_to\_dollars(c: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{str}\NormalTok{:}
\NormalTok{    sign }\OperatorTok{=} \StringTok{"{-}"} \ControlFlowTok{if}\NormalTok{ c }\OperatorTok{\textless{}} \DecValTok{0} \ControlFlowTok{else} \StringTok{""}
\NormalTok{    c }\OperatorTok{=} \BuiltInTok{abs}\NormalTok{(c)}
    \ControlFlowTok{return} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{sign}\SpecialCharTok{\}\{}\NormalTok{c}\OperatorTok{//}\DecValTok{100}\SpecialCharTok{\}}\SpecialStringTok{.}\SpecialCharTok{\{}\NormalTok{c}\OperatorTok{\%}\DecValTok{100}\SpecialCharTok{:02d\}}\SpecialStringTok{"}
\end{Highlighting}
\end{Shaded}

These examples are in Python for clarity, but the same ideas exist in
every major language.

\subsubsection{Why it matters}\label{why-it-matters-10}

\begin{itemize}
\tightlist
\item
  Reliability: Silent wrap or float drift becomes data corruption under
  load or over time.
\item
  Interoperability: Services in different languages disagree on
  overflow; define and document your contracts.
\item
  Reproducibility: Deterministic numerics (same inputs → same bits)
  depend on summation order, rounding, and libraries.
\item
  Security: UB-triggered overflows can turn into exploitable states.
\end{itemize}

This is why ``it worked on my laptop'' is not enough. You want to be
sure it works on every platform, every time.

\subsubsection{Exercises}\label{exercises-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Overflow policy: For a metrics pipeline, decide where to use
  \texttt{checked}, \texttt{wrapping}, and \texttt{saturating}
  addition---and justify each with failure modes.
\item
  ULP probe: Find the smallest \(\epsilon\) such that
  \texttt{1.0\ +\ ε\ !=\ 1.0} in your language; explain how it relates
  to machine epsilon.
\item
  Summation bake-off: Sum the first 1M terms of the harmonic series with
  naïve, pairwise, and Kahan methods; compare results and timings.
\item
  Fixed-point ledger: Implement deposit/transfer/withdraw using
  \texttt{int64} cents; prove no rounding loss for two-decimal
  currencies.
\item
  Boundary tests: Write property tests that \texttt{add\_i32\_checked}
  raises on \texttt{\{INT\_MAX,1\}} and \texttt{\{INT\_MIN,-1\}}, and
  equals modular add where documented.
\item
  Cross-lang contract: Specify a JSON schema for monetary amounts and
  counters that avoids float types; include examples and edge cases.
\end{enumerate}

Great --- let's rework 1.4 Overflow \& Precision (L2) into a friendlier
deep dive, using the same pattern: structured sections, tables, and
added ``bridge'' sentences that guide the reader through complex,
low-level material. This version should be dense enough to teach
internals, but smooth enough to read without feeling like a spec sheet.

\subsection{1.4 L2. Under the Hood}\label{l2.-under-the-hood}

At the lowest level, overflow and precision aren't abstract
concepts---they are consequences of how CPUs, compilers, and libraries
actually implement arithmetic. Understanding these details makes
debugging easier and gives you control over performance,
reproducibility, and correctness.

\subsubsection{Deep Dive}\label{deep-dive-2}

\paragraph{1) Hardware semantics}\label{hardware-semantics}

CPUs implement integer and floating-point arithmetic directly in
silicon. When the result doesn't fit, different flags or traps are
triggered.

\begin{itemize}
\tightlist
\item
  Status flags (integers): Most architectures (x86, ARM, RISC-V) set
  overflow, carry, and zero flags after arithmetic. These flags drive
  branch instructions like \texttt{jo} (``jump if overflow'').
\item
  Floating-point control: The FPU or SIMD unit maintains exception flags
  (inexact, overflow, underflow, invalid, divide-by-zero). These rarely
  trap by default; they silently set flags until explicitly checked.
\end{itemize}

Architectural view

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0606}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2525}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Arch
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Integer overflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
FP behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Developer hooks
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
x86-64 & Wraparound in 2's complement; OF/CF bits set & IEEE-754; flags
in MXCSR & \texttt{jo}/\texttt{jno}, \texttt{fenv.h} \\
ARM64 & Wraparound; NZCV flags & IEEE-754; exception bits & condition
codes, \texttt{feset*} \\
RISC-V & Wraparound; OV/CF optional & IEEE-754; status regs & CSRs, trap
handlers \\
\end{longtable}

Knowing what the CPU does lets you choose: rely on hardware wrap, trap
explicitly, or add software checks.

\paragraph{2) Compiler and language
layers}\label{compiler-and-language-layers}

Even if hardware sets flags, your language may ignore them. Compilers
often optimize based on the language spec.

\begin{itemize}
\tightlist
\item
  C/C++: Signed overflow is \emph{undefined behavior}---the optimizer
  assumes it never happens, which can remove safety checks you thought
  were there.
\item
  Rust: Catches overflow in debug builds, then forces you to pick:
  \texttt{checked\_add}, \texttt{wrapping\_add}, or
  \texttt{saturating\_add}.
\item
  JVM languages (Java, Kotlin, Scala): Always wrap, hiding UB but
  forcing you to detect overflow yourself.
\item
  .NET (C\#, F\#): Defaults to wrapping; you can enable \texttt{checked}
  contexts to trap.
\item
  Python: Emulates unbounded integers, but sometimes simulates C-like
  behavior for low-level modules.
\end{itemize}

These choices aren't arbitrary---they reflect trade-offs between speed,
safety, and backward compatibility.

\paragraph{3) Precision management in floating
point}\label{precision-management-in-floating-point}

Floating-point has more than just rounding errors. Engineers deal with
gradual underflow, denormals, and fused operations.

\begin{itemize}
\tightlist
\item
  Subnormals: Numbers smaller than \textasciitilde2.2e-308 in double
  precision become ``denormalized,'' losing precision but extending the
  range toward zero. Many CPUs handle these slowly.
\item
  Flush-to-zero: Some systems skip subnormals entirely, treating them as
  zero to boost speed. Great for graphics; risky for scientific code.
\item
  FMA (fused multiply-add): Computes \texttt{(a*b\ +\ c)} with one
  rounding, often improving precision and speed. However, it can break
  reproducibility across machines that do/don't use FMA.
\end{itemize}

Precision events

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2973}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5811}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Event
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What happens
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why it matters
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Overflow & Becomes ±Inf & Detectable via \texttt{isinf}, often safe \\
Underflow & Becomes 0 or subnormal & Performance hit, possible accuracy
loss \\
Inexact & Result rounded & Happens constantly; only matters if
flagged \\
Invalid & NaN produced & Division 0/0, sqrt(-1), etc. \\
\end{longtable}

When performance bugs show up in HPC or ML code, denormals and FMAs are
often the hidden cause.

\paragraph{4) Debugging and testing
tools}\label{debugging-and-testing-tools}

Low-level correctness requires instrumentation. Fortunately, toolchains
give you options.

\begin{itemize}
\tightlist
\item
  Sanitizers: \texttt{-fsanitize=undefined} (Clang/GCC) traps on signed
  overflow.
\item
  Valgrind / perf counters: Can catch denormal slowdowns.
\item
  Unit-test utilities: Rust's \texttt{assert\_eq!(checked\_add(…))},
  Python's \texttt{math.isclose}, Java's \texttt{BigDecimal} reference
  checks.
\item
  Reproducibility flags: \texttt{-ffast-math} (fast but
  non-deterministic), vs.~\texttt{-frounding-math} (strict).
\end{itemize}

Testing with multiple compilers and settings reveals assumptions you
didn't know you had.

\paragraph{5) Strategies in production
systems}\label{strategies-in-production-systems}

When deploying real systems, you pick policies that match domain needs.

\begin{itemize}
\tightlist
\item
  Databases: Use \texttt{DECIMAL(p,s)} to store fixed-point, preventing
  float drift in sums.
\item
  Financial systems: Explicit fixed-point types (cents as
  \texttt{int64}) + saturating logic on overflow.
\item
  Graphics / ML: Accept \texttt{float32} imprecision; gain throughput
  with fused ops and flush-to-zero.
\item
  Low-level kernels: Exploit modular wraparound deliberately for hash
  tables, checksums, and crypto.
\end{itemize}

Policy menu

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2464}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7536}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Money transfers & Fixed-point, saturating arithmetic \\
Physics sim & \texttt{float64}, stable integrators, compensated
summation \\
Hashing / RNG & Embrace wraparound modular math \\
Critical counters & \texttt{uint64} with explicit overflow trap \\
\end{longtable}

Thinking in policies avoids one-off hacks. Document ``why'' once, then
apply consistently.

\subsubsection{Code Examples}\label{code-examples}

C (wrap vs check)

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdint.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}stdbool.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}limits.h\textgreater{}}

\DataTypeTok{bool}\NormalTok{ add\_checked\_i32}\OperatorTok{(}\DataTypeTok{int32\_t}\NormalTok{ a}\OperatorTok{,} \DataTypeTok{int32\_t}\NormalTok{ b}\OperatorTok{,} \DataTypeTok{int32\_t} \OperatorTok{*}\NormalTok{out}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{if} \OperatorTok{((}\NormalTok{b }\OperatorTok{\textgreater{}} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ a }\OperatorTok{\textgreater{}}\NormalTok{ INT32\_MAX }\OperatorTok{{-}}\NormalTok{ b}\OperatorTok{)} \OperatorTok{||}
        \OperatorTok{(}\NormalTok{b }\OperatorTok{\textless{}} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ a }\OperatorTok{\textless{}}\NormalTok{ INT32\_MIN }\OperatorTok{{-}}\NormalTok{ b}\OperatorTok{))} \OperatorTok{\{}
        \ControlFlowTok{return} \KeywordTok{false}\OperatorTok{;} \CommentTok{// overflow}
    \OperatorTok{\}}
    \OperatorTok{*}\NormalTok{out }\OperatorTok{=}\NormalTok{ a }\OperatorTok{+}\NormalTok{ b}\OperatorTok{;}
    \ControlFlowTok{return} \KeywordTok{true}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Rust (explicit intent)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fn}\NormalTok{ demo() }\OperatorTok{\{}
    \KeywordTok{let}\NormalTok{ x}\OperatorTok{:} \DataTypeTok{i32} \OperatorTok{=} \DataTypeTok{i32}\PreprocessorTok{::}\ConstantTok{MAX}\OperatorTok{;}
    \PreprocessorTok{println!}\NormalTok{(}\StringTok{"\{:?\}"}\OperatorTok{,}\NormalTok{ x}\OperatorTok{.}\NormalTok{wrapping\_add(}\DecValTok{1}\NormalTok{))}\OperatorTok{;}   \CommentTok{// wrap}
    \PreprocessorTok{println!}\NormalTok{(}\StringTok{"\{:?\}"}\OperatorTok{,}\NormalTok{ x}\OperatorTok{.}\NormalTok{checked\_add(}\DecValTok{1}\NormalTok{))}\OperatorTok{;}    \CommentTok{// None}
    \PreprocessorTok{println!}\NormalTok{(}\StringTok{"\{:?\}"}\OperatorTok{,}\NormalTok{ x}\OperatorTok{.}\NormalTok{saturating\_add(}\DecValTok{1}\NormalTok{))}\OperatorTok{;} \CommentTok{// clamp}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Python (reproducibility check)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\KeywordTok{def}\NormalTok{ ulp\_diff(a: }\BuiltInTok{float}\NormalTok{, b: }\BuiltInTok{float}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \CommentTok{\# Compares floats in terms of ULPs}
    \ImportTok{import}\NormalTok{ struct}
\NormalTok{    ai }\OperatorTok{=}\NormalTok{ struct.unpack(}\StringTok{\textquotesingle{}!q\textquotesingle{}}\NormalTok{, struct.pack(}\StringTok{\textquotesingle{}!d\textquotesingle{}}\NormalTok{, a))[}\DecValTok{0}\NormalTok{]}
\NormalTok{    bi }\OperatorTok{=}\NormalTok{ struct.unpack(}\StringTok{\textquotesingle{}!q\textquotesingle{}}\NormalTok{, struct.pack(}\StringTok{\textquotesingle{}!d\textquotesingle{}}\NormalTok{, b))[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{return} \BuiltInTok{abs}\NormalTok{(ai }\OperatorTok{{-}}\NormalTok{ bi)}

\BuiltInTok{print}\NormalTok{(ulp\_diff(}\FloatTok{1.0}\NormalTok{, math.nextafter(}\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{)))  }\CommentTok{\# 1}
\end{Highlighting}
\end{Shaded}

These snippets show how different languages force you to state your
policy, rather than relying on ``whatever the hardware does.''

\subsubsection{Why it matters}\label{why-it-matters-11}

\begin{itemize}
\tightlist
\item
  Performance: Understanding denormals and FMAs can save orders of
  magnitude in compute-heavy workloads.
\item
  Correctness: Database money columns or counters in billing systems can
  silently corrupt without fixed-point or overflow checks.
\item
  Portability: Code that relies on UB may ``work'' on GCC Linux but fail
  on Clang macOS.
\item
  Security: Integer overflow bugs (e.g., buffer length miscalculation)
  remain a classic vulnerability class.
\end{itemize}

In short, overflow and precision are not ``just math''---they are
systems-level contracts that must be understood and enforced.

\subsubsection{Exercises}\label{exercises-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compiler behavior: Write a C function that overflows
  \texttt{int32\_t}. Compile with and without
  \texttt{-fsanitize=undefined}. What changes?
\item
  FMA investigation: Run a dot-product with and without
  \texttt{-ffast-math}. Measure result differences across compilers.
\item
  Denormal trap: Construct a loop multiplying by \texttt{1e-308}. Time
  it with flush-to-zero enabled vs disabled.
\item
  Policy design: For an in-memory database, define rules for counters,
  timestamps, and currency columns. Which use wrapping, which use
  fixed-point, which trap?
\item
  Cross-language test: Implement \texttt{add\_checked\_i32} in C, Rust,
  and Python. Run edge-case inputs (\texttt{INT\_MAX},
  \texttt{INT\_MIN}). Compare semantics.
\item
  ULP meter: Write a function in your language to compute ULP distance
  between two floats. Use it to compare rounding differences between
  platforms.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Chapter 2. Arrays}\label{chapter-2.-arrays}

\section{2.1 Static Arrays}\label{static-arrays}

\subsection{2.2 L0 --- Arrays That Grow}\label{l0-arrays-that-grow}

A dynamic array is like a container that can expand and shrink as
needed. Unlike static arrays, which must know their size in advance, a
dynamic array adapts as elements are added or removed. You can think of
it as a bookshelf where new shelves appear automatically when space runs
out. The underlying idea is simple: keep the benefits of fast
index-based access, while adding flexibility to change the size.

\subsubsection{Deep Dive}\label{deep-dive-3}

A dynamic array begins with a fixed amount of space called its capacity.
When the number of elements (the length) exceeds this capacity, the
array grows. This is usually done by allocating a new, larger block of
memory and copying the old elements into it. After this, new elements
can be added until the new capacity is filled, at which point the
process repeats.

Despite this resizing process, the key properties remain:

\begin{itemize}
\tightlist
\item
  Fast access and update: Elements can still be reached instantly using
  an index.
\item
  Append flexibility: New elements can be added at the end without
  worrying about fixed size.
\item
  Occasional resizing cost: Most appends are quick, but when resizing
  happens, it takes longer because all elements must be copied.
\end{itemize}

The performance picture is intuitive:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3247}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4935}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time Complexity (Typical)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Index maps directly to position \\
Update element & O(1) & Replace value in place \\
Append element & O(1) amortized & Occasionally O(n) when resizing
occurs \\
Pop element & O(1) & Remove from end \\
Insert/Delete & O(n) & Elements must be shifted \\
\end{longtable}

Dynamic arrays therefore trade predictability for flexibility. The
occasional slow operation is outweighed by the ability to grow and
shrink on demand, which makes them useful for most real-world tasks
where the number of elements is not known in advance.

\subsubsection{Worked Example}\label{worked-example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a dynamic array using Python\textquotesingle{}s built{-}in list}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Append elements (array grows automatically)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    arr.append((i }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{10}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after appending:"}\NormalTok{, arr)}

\CommentTok{\# Access and update elements}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}
\NormalTok{arr[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated array:"}\NormalTok{, arr)}

\CommentTok{\# Remove last element}
\NormalTok{last }\OperatorTok{=}\NormalTok{ arr.pop()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Removed element:"}\NormalTok{, last)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after pop:"}\NormalTok{, arr)}

\CommentTok{\# Traverse array}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{arr[i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This short program shows how a dynamic array in Python resizes
automatically with \texttt{append} and shrinks with \texttt{pop}. Access
and updates remain instant, while resizing happens invisibly when more
space is needed.

\subsubsection{Why it matters}\label{why-it-matters-12}

Dynamic arrays combine efficiency and flexibility. They allow programs
to handle unknown or changing amounts of data without predefining sizes.
They form the backbone of lists in high-level languages, balancing
performance with usability. They also illustrate the idea of amortized
cost: most operations are fast, but occasional expensive operations are
averaged out over time.

\subsubsection{Exercises}\label{exercises-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array and append numbers 1 through 10. Print the final
  array.
\item
  Replace the 3rd element with a new value.
\item
  Remove the last two elements and print the result.
\item
  Write a procedure that traverses a dynamic array and computes the
  average of its elements.
\item
  Explain why appending one element might sometimes be much slower than
  appending another, even though both look the same in code.
\end{enumerate}

\subsection{2.1 L1 --- Static Arrays in
Practice}\label{l1-static-arrays-in-practice}

Static arrays are one of the simplest and most reliable ways of storing
data. They are defined as collections of elements laid out in a
fixed-size, contiguous block of memory. Unlike dynamic arrays, their
size is determined at creation and cannot be changed later. This
property makes them predictable, efficient, and easy to reason about,
but also less flexible when dealing with varying amounts of data.

\subsubsection{Deep Dive}\label{deep-dive-4}

At the heart of static arrays is their memory layout. When an array is
created, the program reserves a continuous region of memory large enough
to hold all its elements. Each element is stored right next to the
previous one. This design allows very fast access because the position
of any element can be computed directly:

\begin{verbatim}
address_of(arr[i]) = base_address + (i × element_size)
\end{verbatim}

No searching or scanning is required, only simple arithmetic. This is
why reading or writing to an element at a given index is considered O(1)
--- constant time regardless of the array size.

The trade-offs emerge when considering insertion or deletion. Because
elements are tightly packed, inserting a new element in the middle
requires shifting all the subsequent elements by one position. Deleting
works the same way in reverse. These operations are therefore O(n),
linear in the size of the array.

The cost summary is straightforward:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Time Complexity & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Direct index calculation \\
Update element & O(1) & Replace in place \\
Traverse & O(n) & Visit each element once \\
Insert/Delete & O(n) & Shifting elements required \\
\end{longtable}

\paragraph{Trade-offs.}\label{trade-offs.}

Static arrays excel when you know the size in advance. They guarantee
fast access and compact memory usage because there is no overhead for
resizing or metadata. However, they lack flexibility. If the array is
too small, you must allocate a larger one and copy all elements over. If
it is too large, memory is wasted. This is why languages like Python
provide dynamic lists by default, while static arrays are used in
performance-critical or resource-constrained contexts.

\paragraph{Use cases.}\label{use-cases.}

\begin{itemize}
\tightlist
\item
  Buffers: Fixed-size areas for network packets or hardware input.
\item
  Lookup tables: Precomputed constants or small ranges of values (e.g.,
  ASCII character tables).
\item
  Static configuration data: Tables known at compile-time, where
  resizing is unnecessary.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.}

Programmers must be careful of two common issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Out-of-bounds errors: Trying to access an index outside the valid
  range, leading to exceptions (in safe languages) or undefined behavior
  (in low-level languages).
\item
  Sizing problems: Underestimating leads to crashes, overestimating
  leads to wasted memory.
\end{enumerate}

Static arrays are common in many programming environments. In Python,
the \texttt{array} module provides a fixed-type sequence that behaves
more like a C-style array. Libraries like NumPy also provide fixed-shape
arrays that offer efficient memory usage and fast computations. In C and
C++, arrays are part of the language itself, and they form the
foundation of higher-level containers like \texttt{std::vector}.

\subsubsection{Worked Example}\label{worked-example-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ array}

\CommentTok{\# Create a static array of integers (type \textquotesingle{}i\textquotesingle{} = signed int)}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ array.array(}\StringTok{\textquotesingle{}i\textquotesingle{}}\NormalTok{, [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Fill the array with values}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
\NormalTok{    arr[i] }\OperatorTok{=}\NormalTok{ (i }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{10}

\CommentTok{\# Access and update elements}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}
\NormalTok{arr[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}

\CommentTok{\# Traverse the array}
\BuiltInTok{print}\NormalTok{(}\StringTok{"All elements:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{arr[i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Demonstrating the limitation: trying to insert beyond capacity}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    arr.insert(}\DecValTok{5}\NormalTok{, }\DecValTok{60}\NormalTok{)  }\CommentTok{\# This technically works in Python\textquotesingle{}s array, but resizes internally}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Inserted new element:"}\NormalTok{, arr)}
\ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Error inserting into static array:"}\NormalTok{, e)}
\end{Highlighting}
\end{Shaded}

This code illustrates the strengths and weaknesses of static arrays.
Access and updates are immediate, and traversal is simple. But the
notion of a ``fixed size'' means that insertion and deletion are costly
or, in some languages, unsupported.

\subsubsection{Why it matters}\label{why-it-matters-13}

Static arrays are the building blocks of data structures. They teach the
trade-off between speed and flexibility. They remind us that memory is
finite and that how data is laid out in memory directly impacts
performance. Whether writing Python code, using NumPy, or implementing
algorithms in C, understanding static arrays makes it easier to reason
about cost, predict behavior, and avoid common errors.

\subsubsection{Exercises}\label{exercises-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array of size 8 and fill it with even numbers from 2 to 16.
  Then access the 4th element directly.
\item
  Update the middle element of a fixed-size array with a new value.
\item
  Write a procedure to traverse an array and find the maximum element.
\item
  Explain why inserting a new value into the beginning of a static array
  requires shifting every other element.
\item
  Give two examples of real-world systems where fixed-size arrays are a
  natural fit.
\end{enumerate}

\subsection{2.1 L2 --- Static Arrays and the System
Beneath}\label{l2-static-arrays-and-the-system-beneath}

Static arrays are more than just a collection of values; they are a
direct window into how computers store and access data. At the advanced
level, understanding static arrays means looking at memory models, cache
behavior, compiler optimizations, and the role of arrays in operating
systems and production libraries. This perspective is critical for
building high-performance software and for avoiding subtle, system-level
bugs.

\subsubsection{Deep Dive}\label{deep-dive-5}

At the lowest level, a static array is a contiguous block of memory.
When an array is declared, the compiler calculates the required size as
\texttt{length\ ×\ element\_size} and reserves that many bytes. Each
element is addressed by simple arithmetic:

\begin{verbatim}
address_of(arr[i]) = base_address + (i × element_size)
\end{verbatim}

This is why access and updates are constant time. The difference between
static arrays and dynamically allocated ones often comes down to where
the memory lives. Arrays declared inside a function may live on the
stack, offering fast allocation and automatic cleanup. Larger arrays or
arrays whose size isn't known at compile time are allocated on the heap,
requiring runtime management via calls such as \texttt{malloc} and
\texttt{free}.

The cache hierarchy makes arrays especially efficient. Because elements
are contiguous, accessing \texttt{arr{[}i{]}} loads not just one element
but also its neighbors into a cache line (often 64 bytes). This
property, known as spatial locality, means that scanning through an
array is very fast. Prefetchers in modern CPUs exploit this by pulling
in upcoming cache lines before they are needed. However, irregular
access patterns (e.g., striding by 17) can defeat prefetching and lead
to performance drops.

Alignment and padding further influence performance. On most systems,
integers must start at addresses divisible by 4, and doubles at
addresses divisible by 8. If the compiler cannot guarantee alignment, it
may add padding bytes to enforce it. Misaligned accesses can cause
slowdowns or even hardware faults on strict architectures.

Different programming languages expose these behaviors differently. In
C, a declaration like \texttt{int\ arr{[}10{]};} on the stack creates
exactly 40 bytes on a 32-bit system. In contrast,
\texttt{malloc(10\ *\ sizeof(int))} allocates memory on the heap. In
C++, \texttt{std::array\textless{}int,\ 10\textgreater{}} is a safer
wrapper around C arrays, while
\texttt{std::vector\textless{}int\textgreater{}} adds resizing at the
cost of indirection and metadata. In Fortran and NumPy, multidimensional
arrays can be stored in column-major order rather than row-major, which
changes how indices map to addresses and affects iteration performance.

The operating system kernel makes heavy use of static arrays. For
example, Linux defines fixed-size arrays in structures like
\texttt{task\_struct} for file descriptors, and uses arrays in page
tables for managing memory mappings. Static arrays provide
predictability and remove the need for runtime memory allocation in
performance-critical or security-sensitive code.

From a performance profiling standpoint, arrays reveal fundamental
trade-offs. Shifting elements during insertion or deletion requires
copying bytes across memory, and the cost grows linearly with the number
of elements. Compilers attempt to optimize loops over arrays with
vectorization, turning element-wise operations into SIMD instructions.
They may also apply loop unrolling or bounds-check elimination (BCE)
when it can be proven that indices remain safe.

Static arrays also carry risks. In C and C++, accessing out-of-bounds
memory leads to undefined behavior, often exploited in buffer overflow
attacks. Languages like Java or Python mitigate this with runtime bounds
checks, but at the expense of some performance.

At this level, static arrays should be seen not only as a data structure
but as a fundamental contract between code, compiler, and hardware.

Worked Example (C)

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdio.h\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \CommentTok{// Static array of 8 integers allocated on the stack}
    \DataTypeTok{int}\NormalTok{ arr}\OperatorTok{[}\DecValTok{8}\OperatorTok{];}

    \CommentTok{// Initialize array}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{8}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        arr}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=} \OperatorTok{(}\NormalTok{i }\OperatorTok{+} \DecValTok{1}\OperatorTok{)} \OperatorTok{*} \DecValTok{10}\OperatorTok{;}
    \OperatorTok{\}}

    \CommentTok{// Access and update element}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Element at index 3: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{[}\DecValTok{3}\OperatorTok{]);}
\NormalTok{    arr}\OperatorTok{[}\DecValTok{3}\OperatorTok{]} \OperatorTok{=} \DecValTok{99}\OperatorTok{;}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Updated element at index 3: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{[}\DecValTok{3}\OperatorTok{]);}

    \CommentTok{// Traverse with cache{-}friendly pattern}
    \DataTypeTok{int}\NormalTok{ sum }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{8}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        sum }\OperatorTok{+=}\NormalTok{ arr}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \OperatorTok{\}}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Sum of array: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ sum}\OperatorTok{);}

    \CommentTok{// Dangerous: Uncommenting would cause undefined behavior}
    \CommentTok{// printf("\%d\textbackslash{}n", arr[10]);}

    \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This C program demonstrates how static arrays live on the stack, how
indexing works, and why out-of-bounds access is dangerous. On real
hardware, iterating sequentially benefits from spatial locality, making
the traversal very fast compared to random access.

\subsubsection{Why it matters}\label{why-it-matters-14}

Static arrays are the substrate upon which much of computing is built.
They are simple in abstraction but complex in practice, touching
compilers, operating systems, and hardware. Understanding them is
essential for:

\begin{itemize}
\tightlist
\item
  Writing cache-friendly and high-performance code.
\item
  Avoiding security vulnerabilities like buffer overflows.
\item
  Appreciating why higher-level data structures behave the way they do.
\item
  Building intuition for memory layout, alignment, and the interaction
  between code and the CPU.
\end{itemize}

Arrays are not just ``collections of values'' --- they are the
foundation of efficient data processing.

\subsubsection{Exercises}\label{exercises-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In C, declare a static array of size 16 and measure how long it takes
  to sum its elements sequentially versus accessing them in steps of 4.
  Explain the performance difference.
\item
  Explain why iterating over a 2D array row by row is faster in C than
  column by column.
\item
  Consider a struct with mixed types (e.g., \texttt{char}, \texttt{int},
  \texttt{double}). Predict where padding bytes will be inserted if
  placed inside an array.
\item
  Research and describe how the Linux kernel uses static arrays in
  managing processes or memory.
\item
  Demonstrate with code how accessing beyond the end of a static array
  in C can cause undefined behavior, and explain why this is a serious
  risk in system programming.
\end{enumerate}

\section{2.2 Dynamic Arrays}\label{dynamic-arrays}

\subsection{2.2 L0 --- Arrays That Grow}\label{l0-arrays-that-grow-1}

A dynamic array is like a container that can expand and shrink as
needed. Unlike static arrays, which must know their size in advance, a
dynamic array adapts as elements are added or removed. You can think of
it as a bookshelf where new shelves appear automatically when space runs
out. The underlying idea is simple: keep the benefits of fast
index-based access, while adding flexibility to change the size.

\subsubsection{Deep Dive}\label{deep-dive-6}

A dynamic array begins with a fixed amount of space called its capacity.
When the number of elements (the length) exceeds this capacity, the
array grows. This is usually done by allocating a new, larger block of
memory and copying the old elements into it. After this, new elements
can be added until the new capacity is filled, at which point the
process repeats.

Despite this resizing process, the key properties remain:

\begin{itemize}
\tightlist
\item
  Fast access and update: Elements can still be reached instantly using
  an index.
\item
  Append flexibility: New elements can be added at the end without
  worrying about fixed size.
\item
  Occasional resizing cost: Most appends are quick, but when resizing
  happens, it takes longer because all elements must be copied.
\end{itemize}

The performance picture is intuitive:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3247}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4935}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time Complexity (Typical)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Index maps directly to position \\
Update element & O(1) & Replace value in place \\
Append element & O(1) amortized & Occasionally O(n) when resizing
occurs \\
Pop element & O(1) & Remove from end \\
Insert/Delete & O(n) & Elements must be shifted \\
\end{longtable}

Dynamic arrays therefore trade predictability for flexibility. The
occasional slow operation is outweighed by the ability to grow and
shrink on demand, which makes them useful for most real-world tasks
where the number of elements is not known in advance.

\subsubsection{Worked Example}\label{worked-example-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a dynamic array using Python\textquotesingle{}s built{-}in list}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Append elements (array grows automatically)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    arr.append((i }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{10}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after appending:"}\NormalTok{, arr)}

\CommentTok{\# Access and update elements}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}
\NormalTok{arr[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated array:"}\NormalTok{, arr)}

\CommentTok{\# Remove last element}
\NormalTok{last }\OperatorTok{=}\NormalTok{ arr.pop()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Removed element:"}\NormalTok{, last)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after pop:"}\NormalTok{, arr)}

\CommentTok{\# Traverse array}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{arr[i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This short program shows how a dynamic array in Python resizes
automatically with \texttt{append} and shrinks with \texttt{pop}. Access
and updates remain instant, while resizing happens invisibly when more
space is needed.

\subsubsection{Why it matters}\label{why-it-matters-15}

Dynamic arrays combine efficiency and flexibility. They allow programs
to handle unknown or changing amounts of data without predefining sizes.
They form the backbone of lists in high-level languages, balancing
performance with usability. They also illustrate the idea of amortized
cost: most operations are fast, but occasional expensive operations are
averaged out over time.

\subsubsection{Exercises}\label{exercises-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array and append numbers 1 through 10. Print the final
  array.
\item
  Replace the 3rd element with a new value.
\item
  Remove the last two elements and print the result.
\item
  Write a procedure that traverses a dynamic array and computes the
  average of its elements.
\item
  Explain why appending one element might sometimes be much slower than
  appending another, even though both look the same in code.
\end{enumerate}

\subsection{2.2 L1 --- Dynamic Arrays in
Practice}\label{l1-dynamic-arrays-in-practice}

Dynamic arrays extend the idea of static arrays by making size flexible.
They allow adding or removing elements without knowing the total number
in advance. Under the hood, this flexibility is achieved through careful
memory management: the array is stored in a contiguous block, but when
more space is needed, a larger block is allocated, and all elements are
copied over. This mechanism balances speed with adaptability and is the
reason why dynamic arrays are the default sequence type in many
languages.

\subsubsection{Deep Dive}\label{deep-dive-7}

A dynamic array starts with a certain capacity, often larger than the
initial number of elements. When the number of stored elements exceeds
capacity, the array is resized. The common strategy is to double the
capacity. For example, an array of capacity 4 that becomes full will
reallocate to capacity 8. All existing elements are copied into the new
block, and the old memory is freed.

This strategy makes appending efficient on average. While an individual
resize costs O(n) because of the copying, most appends are O(1). Across
a long sequence of operations, the total cost averages out --- this is
called amortized analysis.

Dynamic arrays retain the key advantages of static arrays:

\begin{itemize}
\tightlist
\item
  Contiguous storage means fast random access with \texttt{O(1)} time.
\item
  Updates are also \texttt{O(1)} because they overwrite existing slots.
\end{itemize}

The challenges appear with other operations:

\begin{itemize}
\tightlist
\item
  Insertions or deletions in the middle require shifting elements,
  making them O(n).
\item
  Resizing events create temporary latency spikes, especially when
  arrays are large.
\end{itemize}

A clear summary:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Time Complexity & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Direct index calculation \\
Update element & O(1) & Replace value in place \\
Append element & O(1) amortized & Occasional O(n) when resizing \\
Pop element & O(1) & Remove from end \\
Insert/Delete & O(n) & Shifting elements required \\
\end{longtable}

\paragraph{Trade-offs.}\label{trade-offs.-1}

Dynamic arrays sacrifice predictability for convenience. Resizing causes
performance spikes, but the doubling strategy keeps the average cost
low. Over-allocation wastes some memory, but it reduces the frequency of
resizes. The key is that this trade-off is usually favorable in
practice.

\paragraph{Use cases.}\label{use-cases.-1}

Dynamic arrays are well-suited for:

\begin{itemize}
\tightlist
\item
  Lists whose size is not known in advance.
\item
  Workloads dominated by appending and reading values.
\item
  General-purpose data structures in high-level programming languages.
\end{itemize}

\paragraph{Language implementations.}\label{language-implementations.}

\begin{itemize}
\tightlist
\item
  Python: \texttt{list} is a dynamic array, using an over-allocation
  strategy to reduce frequent resizes.
\item
  C++: \texttt{std::vector} doubles its capacity when needed,
  invalidating pointers/references after reallocation.
\item
  Java: \texttt{ArrayList} grows by about 1.5× when full, trading memory
  efficiency for fewer copies.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-1}

\begin{itemize}
\tightlist
\item
  In languages with pointers or references, resizes can invalidate
  existing references.
\item
  Large arrays may cause noticeable latency during reallocation.
\item
  Middle insertions and deletions remain inefficient compared to linked
  structures.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrate dynamic array behavior using Python\textquotesingle{}s list}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Append elements to trigger resizing internally}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{12}\NormalTok{):}
\NormalTok{    arr.append(i)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Appended }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{, length = }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(arr)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Access and update}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 5:"}\NormalTok{, arr[}\DecValTok{5}\NormalTok{])}
\NormalTok{arr[}\DecValTok{5}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated element at index 5:"}\NormalTok{, arr[}\DecValTok{5}\NormalTok{])}

\CommentTok{\# Insert in the middle (expensive operation)}
\NormalTok{arr.insert(}\DecValTok{6}\NormalTok{, }\DecValTok{123}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after middle insert:"}\NormalTok{, arr)}

\CommentTok{\# Pop elements}
\NormalTok{arr.pop()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after pop:"}\NormalTok{, arr)}
\end{Highlighting}
\end{Shaded}

This example illustrates appending, updating, inserting, and popping.
While Python hides the resizing, the cost is there: occasionally the
list must allocate more space and copy its contents.

\subsubsection{Why it matters}\label{why-it-matters-16}

Dynamic arrays balance flexibility and performance. They demonstrate the
principle of amortized complexity, showing how expensive operations can
be smoothed out over time. They also highlight trade-offs between memory
usage and speed. Understanding them explains why high-level lists
perform well in everyday coding but also where they can fail under
stress.

\subsubsection{Exercises}\label{exercises-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a dynamic array and append the numbers 1 to 20. Measure how
  many times resizing would have occurred if the growth factor were 2.
\item
  Insert an element into the middle of a large array and explain why
  this operation is slower than appending at the end.
\item
  Write a procedure to remove all odd numbers from a dynamic array.
\item
  Compare Python's \texttt{list}, Java's \texttt{ArrayList}, and C++'s
  \texttt{std::vector} in terms of growth strategy.
\item
  Explain why references to elements of a \texttt{std::vector} may
  become invalid after resizing.
\end{enumerate}

\subsection{2.2 L2 --- Dynamic Arrays Under the
Hood}\label{l2-dynamic-arrays-under-the-hood}

Dynamic arrays reveal how high-level flexibility is built on top of
low-level memory management. While they appear as resizable containers,
underneath they are carefully engineered to balance performance, memory
efficiency, and safety. Understanding their internals sheds light on
allocators, cache behavior, and the risks of pointer invalidation.

\subsubsection{Deep Dive}\label{deep-dive-8}

Dynamic arrays rely on heap allocation. When first created, they reserve
a contiguous memory block with some capacity. As elements are appended
and the array fills, the implementation must allocate a new, larger
block, copy all existing elements, and free the old block.

Most implementations use a geometric growth strategy, often doubling the
capacity when space runs out. Some use a factor smaller than two, such
as 1.5×, to reduce memory waste. The trade-off is between speed and
efficiency:

\begin{itemize}
\tightlist
\item
  Larger growth factors reduce the number of costly reallocations.
\item
  Smaller growth factors waste less memory but increase resize
  frequency.
\end{itemize}

This leads to an amortized O(1) cost for append. Each resize is
expensive, but they happen infrequently enough that the average cost
remains constant across many operations.

However, resizes have side effects:

\begin{itemize}
\tightlist
\item
  Pointer invalidation: In C++ \texttt{std::vector}, any reference,
  pointer, or iterator into the old memory becomes invalid after
  reallocation.
\item
  Latency spikes: Copying thousands or millions of elements in one step
  can stall a program, especially in real-time or low-latency systems.
\item
  Allocator fragmentation: Repeated growth and shrink cycles can
  fragment the heap, reducing performance in long-running systems.
\end{itemize}

Cache efficiency is one of the strengths of dynamic arrays. Because
elements are stored contiguously, traversals are cache-friendly, and
prefetchers can load entire blocks into cache lines. But reallocations
can disrupt locality temporarily, as the array may move to a new region
of memory.

Different languages implement dynamic arrays with variations:

\begin{itemize}
\tightlist
\item
  Python lists use over-allocation with a small growth factor
  (\textasciitilde12.5\% to 25\% extra). This minimizes wasted memory
  while keeping amortized costs stable.
\item
  C++ \texttt{std::vector} typically doubles its capacity when needed.
  Developers can call \texttt{reserve()} to preallocate memory and avoid
  repeated reallocations.
\item
  Java \texttt{ArrayList} grows by \textasciitilde1.5×, balancing heap
  usage with resize frequency.
\end{itemize}

Dynamic arrays also face risks:

\begin{itemize}
\tightlist
\item
  If resizing logic is incorrect, buffer overflows may occur.
\item
  Attackers can exploit repeated growth/shrink cycles to cause
  denial-of-service via frequent allocations.
\item
  Very large allocations can fail outright if memory is exhausted.
\end{itemize}

From a profiling perspective, workloads matter. Append-heavy patterns
perform extremely well due to amortization. Insert-heavy or
middle-delete workloads perform poorly because of element shifting.
Allocator-aware optimizations, like pre-reserving capacity, can
dramatically improve performance.

\subsubsection{Worked Example (C++)}\label{worked-example-c}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}iostream\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}vector\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \BuiltInTok{std::}\NormalTok{vector}\OperatorTok{\textless{}}\DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ v}\OperatorTok{;}
\NormalTok{    v}\OperatorTok{.}\NormalTok{reserve}\OperatorTok{(}\DecValTok{4}\OperatorTok{);}  \CommentTok{// reserve space for 4 elements to reduce reallocations}

    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{12}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        v}\OperatorTok{.}\NormalTok{push\_back}\OperatorTok{(}\NormalTok{i }\OperatorTok{*} \DecValTok{10}\OperatorTok{);}
        \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Appended "} \OperatorTok{\textless{}\textless{}}\NormalTok{ i}\OperatorTok{*}\DecValTok{10}
                  \OperatorTok{\textless{}\textless{}} \StringTok{", size = "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{.}\NormalTok{size}\OperatorTok{()}
                  \OperatorTok{\textless{}\textless{}} \StringTok{", capacity = "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{.}\NormalTok{capacity}\OperatorTok{()} \OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}
    \OperatorTok{\}}

    \CommentTok{// Access and update}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Element at index 5: "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}
\NormalTok{    v}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{=} \DecValTok{99}\OperatorTok{;}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Updated element at index 5: "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}

    \CommentTok{// Demonstrate invalidation risk}
    \DataTypeTok{int}\OperatorTok{*}\NormalTok{ ptr }\OperatorTok{=} \OperatorTok{\&}\NormalTok{v}\OperatorTok{[}\DecValTok{0}\OperatorTok{];}
\NormalTok{    v}\OperatorTok{.}\NormalTok{push\_back}\OperatorTok{(}\DecValTok{12345}\OperatorTok{);} \CommentTok{// may reallocate and move data}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Old pointer may now be invalid: "} \OperatorTok{\textless{}\textless{}} \OperatorTok{*}\NormalTok{ptr }\OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;} \CommentTok{// UB if reallocated}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This program shows how \texttt{std::vector} manages capacity. The output
reveals how capacity grows as more elements are appended. The pointer
invalidation example highlights a subtle but critical risk: after a
resize, old addresses into the array are no longer safe.

\subsubsection{Why it matters}\label{why-it-matters-17}

Dynamic arrays expose the tension between abstraction and reality. They
appear simple, but internally they touch almost every layer of the
system: heap allocators, caches, compiler optimizations, and safety
checks. They are essential for understanding how high-level languages
achieve both usability and performance, and they illustrate real-world
engineering trade-offs between speed, memory, and safety.

\subsubsection{Exercises}\label{exercises-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In C++, measure the capacity growth of a
  \texttt{std::vector\textless{}int\textgreater{}} as you append 1,000
  elements. Plot size vs capacity.
\item
  Explain why a program that repeatedly appends and deletes elements
  might fragment the heap over time.
\item
  Compare the growth strategies of Python \texttt{list}, C++
  \texttt{std::vector}, and Java \texttt{ArrayList}. Which wastes more
  memory? Which minimizes resize cost?
\item
  Write a program that appends 1 million integers to a dynamic array and
  then times the traversal. Compare it with inserting 1 million integers
  at the beginning.
\item
  Show how \texttt{reserve()} in \texttt{std::vector} or
  \texttt{ensureCapacity()} in Java \texttt{ArrayList} can eliminate
  costly reallocation spikes.
\end{enumerate}

\section{2.3 Slices \& Views}\label{slices-views}

\subsection{2.3 L0 --- Looking Through a
Window}\label{l0-looking-through-a-window}

A slice or view is a way to look at part of an array without creating a
new one. Instead of copying data, a slice points to the same underlying
elements, just with its own start and end boundaries. This makes working
with subarrays fast and memory-efficient. You can think of a slice as a
window into a longer row of boxes, showing only the portion you care
about.

\subsubsection{Deep Dive}\label{deep-dive-9}

When you take a slice, you don't get a new array filled with copied
elements. Instead, you get a new ``view'' that remembers where in the
original array it starts and stops. This is useful because:

\begin{itemize}
\tightlist
\item
  No copying means creating a slice is very fast.
\item
  Shared storage means changes in the slice also affect the original
  array (in languages like Go, Rust, or NumPy).
\item
  Reduced scope means you can focus on a part of the array without
  carrying the entire structure.
\end{itemize}

Key properties of slices:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They refer to the same memory as the original array.
\item
  They have their own length (number of elements visible).
\item
  They may also carry a capacity, which limits how far they can expand
  into the original array.
\end{enumerate}

In Python, list slicing (\texttt{arr{[}2:5{]}}) creates a new list with
copies of the elements. This is not a true view. By contrast, NumPy
arrays, Go slices, and Rust slices provide real views --- updates to the
slice affect the original array.

A summary:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & Slice/View & New Array (Copy) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Memory usage & Shares existing storage & Allocates new storage \\
Creation cost & O(1) & O(n) for copied elements \\
Updates & Affect original array & Independent \\
Safety & Risk of aliasing issues & No shared changes \\
\end{longtable}

Slices are especially valuable when working with large datasets, where
copying would be too expensive.

\subsubsection{Worked Example}\label{worked-example-4}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Python slicing creates a copy, but useful to illustrate concept}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{]}

\CommentTok{\# Slice of middle part}
\NormalTok{sub }\OperatorTok{=}\NormalTok{ arr[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original array:"}\NormalTok{, arr)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Slice (copy in Python):"}\NormalTok{, sub)}

\CommentTok{\# Modifying the slice does not affect the original (Python behavior)}
\NormalTok{sub[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Modified slice:"}\NormalTok{, sub)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original array unchanged:"}\NormalTok{, arr)}

\CommentTok{\# In contrast, NumPy arrays behave like true views}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{arr\_np }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{])}
\NormalTok{sub\_np }\OperatorTok{=}\NormalTok{ arr\_np[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{sub\_np[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy slice reflects back:"}\NormalTok{, arr\_np)}
\end{Highlighting}
\end{Shaded}

This example shows the difference: Python lists create a copy, while
NumPy slices act as views and affect the original.

\subsubsection{Why it matters}\label{why-it-matters-18}

Slices let you work with subsets of data without wasting memory or time
copying. They are critical in systems and scientific computing where
performance matters. They also highlight the idea of aliasing: when two
names refer to the same data. Understanding slices teaches you when
changes propagate and when they don't, which helps avoid surprising
bugs.

\subsubsection{Exercises}\label{exercises-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array of 10 numbers. Take a slice of the middle 5 elements
  and print them.
\item
  Update the first element in your slice and describe what happens to
  the original array in your chosen language.
\item
  Compare slicing behavior in Python and NumPy: which one copies, which
  one shares?
\item
  Explain why slicing a very large dataset is more efficient than
  copying it.
\item
  Think of a real-world analogy where two people share the same resource
  but only see part of it. How does this relate to slices?
\end{enumerate}

\subsection{2.3 L1 --- Slices in Practice}\label{l1-slices-in-practice}

Slices provide a practical way to work with subarrays efficiently.
Instead of copying data into a new structure, a slice acts as a
lightweight reference to part of an existing array. This gives
programmers flexibility to manipulate sections of data without paying
the cost of duplication, while still preserving the familiar indexing
model of arrays.

\subsubsection{Deep Dive}\label{deep-dive-10}

At the implementation level, a slice is typically represented by a small
structure that stores:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A pointer to the first element in the slice.
\item
  The slice's length (how many elements it can access).
\item
  Optionally, its capacity (how far the slice can grow into the backing
  array).
\end{enumerate}

Indexing into a slice works just like indexing into an array:

\begin{verbatim}
slice[i] → base_address + i × element_size
\end{verbatim}

The complexity model stays consistent:

\begin{itemize}
\tightlist
\item
  Slice creation: O(1) when implemented as a view, O(n) if the language
  copies elements.
\item
  Access/update: O(1), just like arrays.
\item
  Traversal: O(k), proportional to the slice's length.
\end{itemize}

This design makes slices efficient but introduces trade-offs. With true
views, the slice and the original array share memory. Updates made
through one are visible through the other. This can be extremely useful
but also dangerous, as it introduces the possibility of unintended side
effects. Languages that prioritize safety (like Python lists) avoid this
by returning a copy instead of a view.

The balance is clear:

\begin{itemize}
\tightlist
\item
  Views (Go, Rust, NumPy): fast and memory-efficient, but require
  discipline to avoid aliasing bugs.
\item
  Copies (Python lists): safer, but slower and more memory-intensive for
  large arrays.
\end{itemize}

A summary of behaviors:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1591}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1591}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Language/Library
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Slice Behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shared Updates
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Go & View & Yes & Backed by \texttt{(ptr,\ len,\ cap)} triple \\
Rust & View & Yes & Safe with borrow checker (mutable/immutable) \\
Python list & Copy & No & Safer but memory-expensive \\
NumPy array & View & Yes & Basis of efficient scientific computing \\
C/C++ & Manual pointer & Yes & No built-in slice type; must manage
manually \\
\end{longtable}

\paragraph{Use cases.}\label{use-cases.-2}

\begin{itemize}
\tightlist
\item
  Processing large datasets in segments without copying.
\item
  Implementing algorithms like sliding windows, partitions, or
  block-based iteration.
\item
  Sharing views of arrays across functions for modular design without
  allocating new memory.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-2}

\begin{itemize}
\tightlist
\item
  In languages with views, careless updates can corrupt the original
  array unexpectedly.
\item
  In Go and C++, extending a slice/view beyond its capacity causes
  runtime errors or undefined behavior.
\item
  In Python, forgetting that slices are copies can lead to performance
  issues in large-scale workloads.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-5}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrating copy slices vs view slices in Python and NumPy}

\CommentTok{\# Python list slicing creates a copy}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}
\NormalTok{sub }\OperatorTok{=}\NormalTok{ arr[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{sub[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Python original:"}\NormalTok{, arr)  }\CommentTok{\# unchanged}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Python slice (copy):"}\NormalTok{, sub)}

\CommentTok{\# NumPy slicing creates a view}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{arr\_np }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{sub\_np }\OperatorTok{=}\NormalTok{ arr\_np[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{sub\_np[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy original (affected):"}\NormalTok{, arr\_np)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy slice (view):"}\NormalTok{, sub\_np)}
\end{Highlighting}
\end{Shaded}

This example shows the key difference: Python lists copy, while NumPy
provides true views. The choice reflects different design priorities:
safety in Python's core data structures versus performance in numerical
computing.

\subsubsection{Why it matters}\label{why-it-matters-19}

Slices make programs more efficient and expressive. They eliminate
unnecessary copying, speed up algorithms that work on subranges, and
support modular programming by passing references instead of duplicating
data. At the same time, they expose important design trade-offs between
safety and performance. Understanding slices provides insight into how
modern languages manage memory efficiently while protecting against
common errors.

\subsubsection{Exercises}\label{exercises-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In Go, create an array of 10 elements and take a slice of the middle
  5. Update the slice and observe the effect on the array.
\item
  In Python, slice a list of 1 million numbers and explain the
  performance cost compared to slicing a NumPy array of the same size.
\item
  Write a procedure that accepts a slice and doubles each element. Test
  with both a copy-based language (Python lists) and a view-based
  language (NumPy or Go).
\item
  Explain why passing slices to functions is more memory-efficient than
  passing entire arrays.
\item
  Discuss a scenario where slice aliasing could lead to unintended bugs
  in a large program.
\end{enumerate}

\subsection{2.3 L2 --- Slices and Views in
Systems}\label{l2-slices-and-views-in-systems}

Slices are not just convenient programming shortcuts; they represent a
powerful abstraction that ties language semantics to hardware realities.
At this level, slices expose details about memory layout, lifetime, and
compiler optimizations. They are central to performance-critical systems
because they allow efficient access to subsets of data without copying,
while also demanding careful handling to avoid aliasing bugs and unsafe
memory access.

\subsubsection{Deep Dive}\label{deep-dive-11}

A slice is typically represented internally as a triple:

\begin{itemize}
\tightlist
\item
  A pointer to the first element,
\item
  A length describing how many elements are visible,
\item
  A capacity showing how far the slice may extend into the backing
  array.
\end{itemize}

Indexing into a slice is still O(1), but the compiler inserts bounds
checks to prevent invalid access. In performance-sensitive code,
compilers often apply bounds-check elimination (BCE) when they can prove
that loop indices remain within safe limits. This allows slices to
combine safety with near-native performance.

Slices are non-owning references. They do not manage memory themselves
but instead depend on the underlying array. In languages like Rust, the
borrow checker enforces lifetimes to prevent dangling slices. In C and
C++, however, programmers must manually ensure that the backing array
outlives the slice, or risk undefined behavior.

Because slices share memory, they introduce aliasing. Multiple slices
can point to overlapping regions of the same array. This can lead to
subtle bugs if two parts of a program update the same region
concurrently. In multithreaded contexts, mutable aliasing without
synchronization can cause data races. Some systems adopt copy-on-write
strategies to reduce risks, but this adds overhead.

From a performance perspective, slices preserve contiguity, which is
ideal for cache locality and prefetching. Sequential traversal is
cache-friendly, but strided access (e.g., every 3rd element) can defeat
hardware prefetchers, reducing efficiency. Languages like NumPy exploit
strides explicitly, enabling both dense and sparse-like views without
copying.

Language designs differ in how they handle slices:

\begin{itemize}
\tightlist
\item
  Go uses \texttt{(ptr,\ len,\ cap)}. Appending to a slice may allocate
  a new array if capacity is exceeded, silently detaching it from the
  original backing storage.
\item
  Rust distinguishes \texttt{\&{[}T{]}} for immutable and
  \texttt{\&mut\ {[}T{]}} for mutable slices, with the compiler
  enforcing safe borrowing rules.
\item
  C/C++ provide no built-in slice type, so developers rely on raw
  pointers and manual length tracking. This is flexible but error-prone.
\item
  NumPy supports advanced slicing: views with strides, broadcasting
  rules, and multidimensional slices for scientific computing.
\end{itemize}

Compilers also optimize slice-heavy code:

\begin{itemize}
\tightlist
\item
  Vectorization transforms element-wise loops into SIMD instructions
  when slices are contiguous.
\item
  Escape analysis determines whether slices can stay stack-allocated or
  must be promoted to the heap.
\end{itemize}

System-level use cases highlight the importance of slices:

\begin{itemize}
\tightlist
\item
  Zero-copy I/O: network and file system buffers are exposed as slices
  into larger memory regions.
\item
  Memory-mapped files: slices map directly to disk pages, enabling
  efficient processing of large datasets.
\item
  GPU programming: CUDA and OpenCL kernels operate on slices of device
  memory, avoiding transfers.
\end{itemize}

These applications show why slices are not just a programming
convenience but a core tool for bridging high-level logic with low-level
performance.

\subsubsection{Worked Example (Go)}\label{worked-example-go}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{package}\NormalTok{ main}

\KeywordTok{import} \StringTok{"fmt"}

\KeywordTok{func}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
\NormalTok{    arr }\OperatorTok{:=} \OperatorTok{[}\DecValTok{6}\OperatorTok{]}\DataTypeTok{int}\OperatorTok{\{}\DecValTok{10}\OperatorTok{,} \DecValTok{20}\OperatorTok{,} \DecValTok{30}\OperatorTok{,} \DecValTok{40}\OperatorTok{,} \DecValTok{50}\OperatorTok{,} \DecValTok{60}\OperatorTok{\}}
\NormalTok{    s }\OperatorTok{:=}\NormalTok{ arr}\OperatorTok{[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\OperatorTok{]} \CommentTok{// slice referencing elements 20, 30, 40}

\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Original array:"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{)}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Slice view:"}\OperatorTok{,}\NormalTok{ s}\OperatorTok{)}

    \CommentTok{// Update through slice}
\NormalTok{    s}\OperatorTok{[}\DecValTok{0}\OperatorTok{]} \OperatorTok{=} \DecValTok{99}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"After update via slice, array:"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{)}

    \CommentTok{// Demonstrate capacity}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Slice length:"}\OperatorTok{,} \BuiltInTok{len}\OperatorTok{(}\NormalTok{s}\OperatorTok{),} \StringTok{"capacity:"}\OperatorTok{,} \BuiltInTok{cap}\OperatorTok{(}\NormalTok{s}\OperatorTok{))}

    \CommentTok{// Appending beyond slice capacity reallocates}
\NormalTok{    s }\OperatorTok{=} \BuiltInTok{append}\OperatorTok{(}\NormalTok{s}\OperatorTok{,} \DecValTok{70}\OperatorTok{,} \DecValTok{80}\OperatorTok{)}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Slice after append:"}\OperatorTok{,}\NormalTok{ s}\OperatorTok{)}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Array after append (unchanged):"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{)}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This example illustrates Go's slice model. The slice \texttt{s}
initially shares storage with \texttt{arr}. Updates propagate to the
array. However, when appending exceeds the slice's capacity, Go
allocates a new backing array, breaking the link with the original. This
behavior is efficient but can surprise developers if not understood.

\subsubsection{Why it matters}\label{why-it-matters-20}

Slices embody key system concepts: pointer arithmetic, memory ownership,
cache locality, and aliasing. They explain how languages achieve
zero-copy abstractions while balancing safety and performance. They also
highlight risks such as dangling references and silent reallocations.
Mastery of slices is essential for building efficient algorithms,
avoiding memory errors, and reasoning about system-level performance.

\subsubsection{Exercises}\label{exercises-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In Go, create an array of 8 integers and take overlapping slices.
  Modify one slice and observe effects on the other. Explain why this
  happens.
\item
  In Rust, attempt to create two mutable slices of the same array
  region. Explain why the borrow checker rejects it.
\item
  In C, simulate a slice using a pointer and a length. Show what happens
  if the backing array is freed while the slice is still in use.
\item
  In NumPy, create a 2D array and take a strided slice (every second
  row). Explain why performance is worse than contiguous slicing.
\item
  Compare how Python, Go, and Rust enforce (or fail to enforce) safety
  when working with slices.
\end{enumerate}

\section{2.4 Multidimensional Arrays}\label{multidimensional-arrays}

\subsection{2.4 L0 --- Tables and Grids}\label{l0-tables-and-grids}

A multidimensional array is an extension of the simple array idea.
Instead of storing data in a single row, a multidimensional array
organizes elements in a grid, table, or cube. The most common example is
a two-dimensional array, which looks like a table with rows and columns.
Each position in the grid is identified by two coordinates: one for the
row and one for the column. This structure is useful for representing
spreadsheets, images, game boards, and mathematical matrices.

\subsubsection{Deep Dive}\label{deep-dive-12}

You can think of a multidimensional array as an array of arrays. A
two-dimensional array is a list where each element is itself another
list. For example, a 3×3 table contains 3 rows, each of which has 3
columns. Accessing an element requires specifying both coordinates:
\texttt{arr{[}row{]}{[}col{]}}.

Even though we visualize multidimensional arrays as grids, in memory
they are still stored as a single continuous sequence. To find an
element, the program computes its position using a formula. In a 2D
array with \texttt{n} columns, the element at \texttt{(row,\ col)} is
located at:

\begin{verbatim}
index = row × n + col
\end{verbatim}

This mapping allows direct access in constant time, just like with 1D
arrays.

Common operations are:

\begin{itemize}
\tightlist
\item
  Creation: decide dimensions and initialize with values.
\item
  Access: specify row and column to retrieve an element.
\item
  Update: change the value at a given coordinate.
\item
  Traversal: visit elements row by row or column by column.
\end{itemize}

A quick summary:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Description & Cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & Get value at (row, col) & O(1) \\
Update element & Replace value at (row, col) & O(1) \\
Traverse array & Visit all elements & O(n×m) \\
\end{longtable}

Multidimensional arrays introduce an important detail: traversal order.
In many languages (like C and Python's NumPy), arrays are stored in
row-major order, which means all elements of the first row are laid out
contiguously, then the second row, and so on. Others, like Fortran, use
column-major order. This difference affects performance in more advanced
topics, but at this level, the key idea is that access is still fast and
predictable.

\subsubsection{Worked Example}\label{worked-example-6}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a 2D array (3x3 table) using list of lists}
\NormalTok{table }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{],}
\NormalTok{    [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{],}
\NormalTok{    [}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{]}
\NormalTok{]}

\CommentTok{\# Access element in second row, third column}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at (1, 2):"}\NormalTok{, table[}\DecValTok{1}\NormalTok{][}\DecValTok{2}\NormalTok{])  }\CommentTok{\# prints 6}

\CommentTok{\# Update element}
\NormalTok{table[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated table:"}\NormalTok{, table)}

\CommentTok{\# Traverse row by row}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Row traversal:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ table:}
    \ControlFlowTok{for}\NormalTok{ val }\KeywordTok{in}\NormalTok{ row:}
        \BuiltInTok{print}\NormalTok{(val, end}\OperatorTok{=}\StringTok{" "}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This example shows how to build and use a 2D array in Python. It looks
like a table, with easy access via coordinates.

\subsubsection{Why it matters}\label{why-it-matters-21}

Multidimensional arrays provide a natural way to represent structured
data like matrices, grids, and images. They allow algorithms to work
directly with two-dimensional or higher-dimensional information without
flattening everything into one long row. This makes programs easier to
write, read, and reason about.

\subsubsection{Exercises}\label{exercises-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a 3×3 array with numbers 1 through 9 and print it in a table
  format.
\item
  Access the element at row 2, column 3 and describe how you found it.
\item
  Change the center element of a 3×3 array to 0.
\item
  Write a loop to compute the sum of all values in a 4×4 array.
\item
  Explain why accessing \texttt{(row,\ col)} in a 2D array is still O(1)
  even though the data is stored in a single sequence in memory.
\end{enumerate}

\subsection{2.4 L1 --- Multidimensional Arrays in
Practice}\label{l1-multidimensional-arrays-in-practice}

Multidimensional arrays are powerful because they extend the linear
model of arrays into grids, tables, and higher dimensions. At a
practical level, they are still stored in memory as a flattened linear
block. What changes is the indexing formula: instead of a single index,
we use multiple coordinates that the system translates into one offset.

\subsubsection{Deep Dive}\label{deep-dive-13}

The most common form is a 2D array. In memory, the elements are laid out
row by row (row-major) or column by column (column-major).

\begin{itemize}
\tightlist
\item
  Row-major (C, NumPy default): elements of each row are contiguous.
\item
  Column-major (Fortran, MATLAB): elements of each column are
  contiguous.
\end{itemize}

For a 2D array with \texttt{num\_cols} columns, the element at
\texttt{(row,\ col)} in row-major order is located at:

\begin{verbatim}
index = row × num_cols + col
\end{verbatim}

For column-major order with \texttt{num\_rows} rows, the formula is:

\begin{verbatim}
index = col × num_rows + row
\end{verbatim}

This distinction matters when traversing. Accessing elements in the
memory's natural order (row by row for row-major, column by column for
column-major) is cache-friendly. Traversing in the opposite order forces
the program to jump around in memory, leading to slower performance.

Extending to 3D and higher is straightforward. For a 3D array with
\texttt{(layers,\ rows,\ cols)} in row-major order:

\begin{verbatim}
index = layer × (rows × cols) + row × cols + col
\end{verbatim}

Complexity remains consistent:

\begin{itemize}
\tightlist
\item
  Access/update: O(1) using index calculation.
\item
  Traversal: O(n × m) for 2D, O(n × m × k) for 3D.
\end{itemize}

Trade-offs:

\begin{itemize}
\tightlist
\item
  Contiguous multidimensional arrays provide excellent performance for
  predictable workloads (e.g., matrix operations).
\item
  Resizing is costly because the entire block must be reallocated.
\item
  Jagged arrays (arrays of arrays) provide flexibility but lose memory
  contiguity, reducing cache performance.
\end{itemize}

Use cases:

\begin{itemize}
\tightlist
\item
  Storing images (pixels as grids).
\item
  Mathematical matrices in scientific computing.
\item
  Game boards and maps.
\item
  Tables in database-like structures.
\end{itemize}

Different languages implement multidimensional arrays differently:

\begin{itemize}
\tightlist
\item
  Python lists: nested lists simulate 2D arrays but are jagged and
  fragmented in memory.
\item
  NumPy: provides true multidimensional arrays stored contiguously in
  row-major (default) or column-major order.
\item
  C/C++: support both contiguous multidimensional arrays
  (\texttt{int\ arr{[}rows{]}{[}cols{]};}) and pointer-based arrays of
  arrays.
\item
  Java: uses arrays of arrays (jagged by default).
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-7}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Comparing list of lists vs NumPy arrays}
\CommentTok{\# List of lists (jagged)}
\NormalTok{table }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{],}
\NormalTok{    [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{],}
\NormalTok{    [}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{]}
\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at (2, 1):"}\NormalTok{, table[}\DecValTok{2}\NormalTok{][}\DecValTok{1}\NormalTok{])  }\CommentTok{\# 8}

\CommentTok{\# NumPy array (true contiguous 2D array)}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{],[}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at (2, 1):"}\NormalTok{, matrix[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])  }\CommentTok{\# 8}

\CommentTok{\# Traversal in row{-}major order}
\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(matrix.shape[}\DecValTok{0}\NormalTok{]):}
    \ControlFlowTok{for}\NormalTok{ col }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(matrix.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{        val }\OperatorTok{=}\NormalTok{ matrix[row, col]  }\CommentTok{\# efficient in NumPy}
\end{Highlighting}
\end{Shaded}

The Python list-of-lists behaves like a table, but each row may live
separately in memory. NumPy, on the other hand, stores data
contiguously, enabling much faster iteration and vectorized operations.

\subsubsection{Why it matters}\label{why-it-matters-22}

Multidimensional arrays are central to real-world applications, from
graphics and simulations to data science and machine learning. They
highlight how physical memory layout (row-major vs column-major)
interacts with algorithm design. Understanding them allows developers to
choose between safety, flexibility, and performance, depending on the
problem.

\subsubsection{Exercises}\label{exercises-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a procedure to sum all values in a 5×5 array by traversing row
  by row.
\item
  For a 3×3 NumPy array, access element \texttt{(2,1)} and explain how
  its memory index is calculated in row-major order.
\item
  Create a jagged array (rows of different lengths) in Python. Show how
  traversal differs from a true 2D array.
\item
  Explain why traversing a NumPy array by rows is faster than by
  columns.
\item
  Write a formula for computing the linear index of \texttt{(i,j,k)} in
  a 3D array stored in row-major order.
\end{enumerate}

\subsection{2.4 L2 --- Multidimensional Arrays and System
Realities}\label{l2-multidimensional-arrays-and-system-realities}

Multidimensional arrays are not only a logical abstraction but also a
system-level structure that interacts with memory layout, caches, and
compilers. At this level, understanding how they are stored, accessed,
and optimized is essential for building high-performance code in
scientific computing, graphics, and data-intensive systems.

\subsubsection{Deep Dive}\label{deep-dive-14}

A multidimensional array is stored either as a contiguous linear block
or as an array of pointers (jagged array). In the contiguous layout,
elements follow one another in memory according to a linearization
formula. In row-major order (C, NumPy), a 2D element at
\texttt{(row,\ col)} is:

\begin{verbatim}
index = row × num_cols + col
\end{verbatim}

In column-major order (Fortran, MATLAB), the formula is:

\begin{verbatim}
index = col × num_rows + row
\end{verbatim}

This difference has deep performance consequences. In row-major layout,
traversing row by row is cache-friendly because consecutive elements are
contiguous. Traversing column by column introduces large strides, which
can cause cache and TLB misses. In column-major arrays, the reverse
holds true.

\paragraph{Cache and performance.}\label{cache-and-performance.}

When an array is traversed sequentially in its natural memory order,
cache lines are used efficiently and hardware prefetchers work well.
Strided access, such as reading every k-th column in a row-major layout,
prevents prefetchers from predicting the access pattern and leads to
performance drops. For large arrays, this can mean the difference
between processing gigabytes per second and megabytes per second.

\paragraph{Alignment and padding.}\label{alignment-and-padding.}

Compilers and libraries often align rows to cache line or SIMD vector
boundaries. For example, a 64-byte cache line may cause padding to be
inserted so that each row begins on a boundary. In parallel systems,
this prevents false sharing when multiple threads process different
rows. However, padding increases memory footprint.

\paragraph{Language-level
differences.}\label{language-level-differences.}

\begin{itemize}
\tightlist
\item
  C/C++: contiguous 2D arrays (\texttt{int\ arr{[}rows{]}{[}cols{]}})
  guarantee row-major layout. Jagged arrays (array of pointers)
  sacrifice locality but allow uneven row sizes.
\item
  Fortran/MATLAB: column-major ordering dominates scientific computing,
  influencing algorithms in BLAS and LAPACK.
\item
  NumPy: stores strides explicitly, enabling flexible slicing and
  arbitrary views. Strided slices can represent transposed matrices
  without copying.
\end{itemize}

\paragraph{Optimizations.}\label{optimizations.}

\begin{itemize}
\tightlist
\item
  Loop tiling/blocking: partition loops into smaller blocks that fit
  into cache, maximizing reuse.
\item
  SIMD-friendly layouts: structure-of-arrays (SoA) improves
  vectorization compared to array-of-structures (AoS).
\item
  Matrix multiplication kernels: carefully designed to exploit cache
  hierarchy, prefetching, and SIMD registers.
\end{itemize}

\paragraph{System-level use cases.}\label{system-level-use-cases.}

\begin{itemize}
\tightlist
\item
  Image processing: images stored as row-major arrays, with pixels in
  contiguous scanlines. Efficient filters process them row by row.
\item
  GPU computing: memory coalescing requires threads in a warp to access
  contiguous memory regions; array layout directly affects throughput.
\item
  Databases: columnar storage uses column-major arrays, enabling fast
  scans and aggregation queries.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-3}

\begin{itemize}
\tightlist
\item
  Traversing in the ``wrong'' order can cause performance cliffs.
\item
  Large index calculations may overflow if not handled carefully.
\item
  Porting algorithms between row-major and column-major languages can
  introduce subtle bugs.
\end{itemize}

Profiling. Practical analysis involves comparing traversal patterns,
cache miss rates, and vectorization efficiency. Modern compilers can
eliminate redundant bounds checks and auto-vectorize well-structured
loops, but poor layout or order can block these optimizations.

\subsubsection{Worked Example (C)}\label{worked-example-c-1}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdio.h\textgreater{}}
\PreprocessorTok{\#define ROWS }\DecValTok{4}
\PreprocessorTok{\#define COLS }\DecValTok{4}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \DataTypeTok{int}\NormalTok{ arr}\OperatorTok{[}\NormalTok{ROWS}\OperatorTok{][}\NormalTok{COLS}\OperatorTok{];}

    \CommentTok{// Fill the array}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ r }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ r }\OperatorTok{\textless{}}\NormalTok{ ROWS}\OperatorTok{;}\NormalTok{ r}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ c }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ c }\OperatorTok{\textless{}}\NormalTok{ COLS}\OperatorTok{;}\NormalTok{ c}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            arr}\OperatorTok{[}\NormalTok{r}\OperatorTok{][}\NormalTok{c}\OperatorTok{]} \OperatorTok{=}\NormalTok{ r }\OperatorTok{*}\NormalTok{ COLS }\OperatorTok{+}\NormalTok{ c}\OperatorTok{;}
        \OperatorTok{\}}
    \OperatorTok{\}}

    \CommentTok{// Row{-}major traversal (cache{-}friendly in C)}
    \DataTypeTok{int}\NormalTok{ sum\_row }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ r }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ r }\OperatorTok{\textless{}}\NormalTok{ ROWS}\OperatorTok{;}\NormalTok{ r}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ c }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ c }\OperatorTok{\textless{}}\NormalTok{ COLS}\OperatorTok{;}\NormalTok{ c}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            sum\_row }\OperatorTok{+=}\NormalTok{ arr}\OperatorTok{[}\NormalTok{r}\OperatorTok{][}\NormalTok{c}\OperatorTok{];}
        \OperatorTok{\}}
    \OperatorTok{\}}

    \CommentTok{// Column traversal (less efficient in row{-}major)}
    \DataTypeTok{int}\NormalTok{ sum\_col }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ c }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ c }\OperatorTok{\textless{}}\NormalTok{ COLS}\OperatorTok{;}\NormalTok{ c}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ r }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ r }\OperatorTok{\textless{}}\NormalTok{ ROWS}\OperatorTok{;}\NormalTok{ r}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            sum\_col }\OperatorTok{+=}\NormalTok{ arr}\OperatorTok{[}\NormalTok{r}\OperatorTok{][}\NormalTok{c}\OperatorTok{];}
        \OperatorTok{\}}
    \OperatorTok{\}}

\NormalTok{    printf}\OperatorTok{(}\StringTok{"Row traversal sum: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ sum\_row}\OperatorTok{);}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Column traversal sum: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ sum\_col}\OperatorTok{);}
    \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This program highlights traversal order. On large arrays, row-major
traversal is much faster in C because of cache-friendly memory access,
while column traversal may cause frequent cache misses.

\subsubsection{Why it matters}\label{why-it-matters-23}

Multidimensional arrays sit at the heart of performance-critical
applications. Their memory layout determines how well algorithms
interact with CPU caches, vector units, and GPUs. Understanding
row-major vs column-major, stride penalties, and cache-aware traversal
allows developers to write software that scales from toy programs to
high-performance computing systems.

\subsubsection{Exercises}\label{exercises-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In C, create a 1000×1000 matrix and measure the time difference
  between row-major and column traversal. Explain the results.
\item
  In NumPy, take a 2D array and transpose it. Use \texttt{.strides} to
  confirm that the transposed array is a view, not a copy.
\item
  Write the linear index formula for a 4D array \texttt{(a,b,c,d)} in
  row-major order.
\item
  Explain how false sharing could occur when two threads update adjacent
  rows of a large array.
\item
  Compare the impact of row-major vs column-major layout in matrix
  multiplication performance.
\end{enumerate}

\section{2.5 Sparse Arrays}\label{sparse-arrays}

\subsection{2.5 L0 --- Sparse Arrays as Empty Parking
Lots}\label{l0-sparse-arrays-as-empty-parking-lots}

A sparse array is a way of storing data when most of the positions are
empty. Instead of recording every slot like in a dense array, a sparse
array only remembers the places that hold actual values. You can think
of a huge parking lot with only a few cars parked: a dense array writes
down every spot, empty or not, while a sparse array just writes down the
locations of the cars.

\subsubsection{Deep Dive}\label{deep-dive-15}

Dense arrays are straightforward: every position has a value, even if it
is zero or unused. This makes access simple and fast, but wastes memory
if most positions are empty. Sparse arrays solve this by storing only
the useful entries.

There are many ways to represent a sparse array:

\begin{itemize}
\tightlist
\item
  Dictionary/Map: store index → value pairs, ignoring empty slots.
\item
  Coordinate list (COO): keep two lists, one for indices and one for
  values.
\item
  Run-length encoding: store stretches of empty values as counts,
  followed by the next filled value.
\end{itemize}

The key idea is to save memory at the cost of more complex indexing.
Access is no longer just arithmetic (\texttt{arr{[}i{]}}) but requires
looking up in the chosen structure.

Comparison:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4857}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Access Speed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Good For
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dense array & High & O(1) & Data with many filled elements \\
Sparse (map) & Low & O(1) average & Few filled elements, random
access \\
Sparse (list) & Very low & O(n) & Very small number of entries \\
\end{longtable}

\subsubsection{Worked Example}\label{worked-example-8}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Dense representation: wastes memory for mostly empty data}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{20}
\NormalTok{dense[}\DecValTok{3}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\NormalTok{dense[}\DecValTok{15}\NormalTok{] }\OperatorTok{=} \DecValTok{25}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dense array:"}\NormalTok{, dense)}

\CommentTok{\# Sparse representation using dictionary}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ \{}\DecValTok{3}\NormalTok{: }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{: }\DecValTok{25}\NormalTok{\}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sparse array:"}\NormalTok{, sparse)}

\CommentTok{\# Access value}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at index 3:"}\NormalTok{, sparse.get(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at index 7:"}\NormalTok{, sparse.get(}\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{))  }\CommentTok{\# default to 0 for missing}
\end{Highlighting}
\end{Shaded}

This shows how a sparse dictionary only records the positions that
matter, while the dense version allocates space for all 20 slots.

\subsubsection{Why it matters}\label{why-it-matters-24}

Sparse arrays are crucial when working with large data where most
entries are empty. They save memory and make it possible to process huge
datasets that would not fit into memory as dense arrays. They also
appear in real-world systems like machine learning (feature vectors),
scientific computing (matrices with few non-zero entries), and search
engines (posting lists).

\subsubsection{Exercises}\label{exercises-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a sparse array of size 1000 with only 3 non-zero values at
  indices 2, 500, and 999.
\item
  Write a procedure to count the number of non-empty values in a sparse
  array.
\item
  Access an index that does not exist in the sparse array and explain
  what should be returned.
\item
  Compare the memory used by a dense array of 1000 zeros and a sparse
  representation with 3 values.
\item
  Think of a real-world example (outside programming) where recording
  only the ``non-empty'' spots is more efficient than listing
  everything.
\end{enumerate}

\subsection{2.5 L1 --- Sparse Arrays in
Practice}\label{l1-sparse-arrays-in-practice}

Sparse arrays become important when dealing with very large datasets
where only a few positions hold non-zero values. Instead of allocating
memory for every element, practical implementations use compact
structures to track only the occupied indices. This saves memory, but
requires trade-offs in access speed and update complexity.

\subsubsection{Deep Dive}\label{deep-dive-16}

There are several practical ways to represent sparse arrays:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dictionary/Hash Map

  \begin{itemize}
  \tightlist
  \item
    Store index → value pairs.
  \item
    Very fast random access and updates (average O(1)).
  \item
    Memory overhead is higher because of hash structures.
  \end{itemize}
\item
  Coordinate List (COO)

  \begin{itemize}
  \tightlist
  \item
    Keep two parallel arrays: one for indices, one for values.
  \item
    Compact, easy to construct, but access is O(n).
  \item
    Good for static data with few updates.
  \end{itemize}
\item
  Compressed Sparse Row (CSR) / Compressed Sparse Column (CSC)

  \begin{itemize}
  \tightlist
  \item
    Widely used for sparse matrices.
  \item
    Use three arrays: values, column indices, and row pointers (or vice
    versa).
  \item
    Extremely efficient for matrix-vector operations.
  \item
    Poor at dynamic updates, since compression must be rebuilt.
  \end{itemize}
\item
  Run-Length Encoding (RLE)

  \begin{itemize}
  \tightlist
  \item
    Store runs of zeros as counts, followed by non-zero entries.
  \item
    Best for sequences with long stretches of emptiness.
  \end{itemize}
\end{enumerate}

A comparison:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1020}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1633}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3469}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3878}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Access Speed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best For
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dictionary & Higher per-entry & O(1) avg & Dynamic updates,
unpredictable indices \\
COO & Very low & O(n) & Static, small sparse sets \\
CSR/CSC & Compact & O(1) row scan, O(log n) col lookup & Linear algebra,
scientific computing \\
RLE & Very compact & Sequential O(n), random slower & Time-series with
long zero runs \\
\end{longtable}

\paragraph{Trade-offs:}\label{trade-offs}

\begin{itemize}
\tightlist
\item
  Dense arrays are fast but waste memory.
\item
  Sparse arrays save memory but access/update complexity varies.
\item
  Choice of structure depends on workload (frequent random access vs
  batch computation).
\end{itemize}

\paragraph{Use cases:}\label{use-cases}

\begin{itemize}
\tightlist
\item
  Machine learning: sparse feature vectors in text classification or
  recommender systems.
\item
  Graph algorithms: adjacency matrices for sparse graphs.
\item
  Search engines: inverted index posting lists.
\item
  Scientific computing: storing large sparse matrices for simulations.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-9}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sparse array using Python dictionary}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ \{}\DecValTok{2}\NormalTok{: }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{: }\DecValTok{50}\NormalTok{, }\DecValTok{999}\NormalTok{: }\DecValTok{7}\NormalTok{\}}

\CommentTok{\# Accessing}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at 100:"}\NormalTok{, sparse.get(}\DecValTok{100}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at 3 (missing):"}\NormalTok{, sparse.get(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\CommentTok{\# Inserting new value}
\NormalTok{sparse[}\DecValTok{500}\NormalTok{] }\OperatorTok{=} \DecValTok{42}

\CommentTok{\# Traversing non{-}empty values}
\ControlFlowTok{for}\NormalTok{ idx, val }\KeywordTok{in}\NormalTok{ sparse.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{idx}\SpecialCharTok{\}}\SpecialStringTok{ → }\SpecialCharTok{\{}\NormalTok{val}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For dense vs sparse comparison:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{1000}
\NormalTok{dense[}\DecValTok{2}\NormalTok{], dense[}\DecValTok{100}\NormalTok{], dense[}\DecValTok{999}\NormalTok{] }\OperatorTok{=} \DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{7}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dense uses 1000 slots, sparse uses"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(sparse), }\StringTok{"entries"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-25}

Sparse arrays strike a balance between memory efficiency and
performance. They let you work with massive datasets that would
otherwise be impossible to store in memory. They also demonstrate the
importance of choosing the right representation for the problem: a
dictionary for dynamic updates, CSR for scientific kernels, or RLE for
compressed logs.

\subsubsection{Exercises}\label{exercises-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a sparse array of length 1000 with values at indices 2, 100,
  and 999 using:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    a dictionary, and
  \item
    two parallel lists (indices, values).
  \end{enumerate}
\item
  Write a procedure that traverses only non-empty entries and prints
  them.
\item
  Explain why inserting a value in CSR format is more expensive than in
  a dictionary-based representation.
\item
  Compare memory usage of a dense array of length 1000 with only 5
  non-zero entries against its sparse dictionary form.
\item
  Give two real-world scenarios where CSR is preferable to
  dictionary-based sparse arrays.
\end{enumerate}

\subsection{2.5 L2 --- Sparse Arrays and Compressed Layouts in
Systems}\label{l2-sparse-arrays-and-compressed-layouts-in-systems}

Sparse arrays are not only about saving memory; they embody deep design
choices about compression, cache use, and hardware acceleration. At this
level, the question is not ``should I store zeros or not,'' but ``which
representation balances memory, access speed, and computational
efficiency for the workload?''

\subsubsection{Deep Dive}\label{deep-dive-17}

Several compressed storage formats exist, each tuned to different needs:

\begin{itemize}
\tightlist
\item
  COO (Coordinate List): Store parallel arrays for row indices, column
  indices, and values. Flexible and simple, but inefficient for repeated
  access because lookups require scanning.
\item
  CSR (Compressed Sparse Row): Use three arrays: \texttt{values},
  \texttt{col\_indices}, and \texttt{row\_ptr} to mark boundaries.
  Accessing all elements of a row is O(1), while finding a specific
  column in a row is O(log n) or linear. Excellent for sparse
  matrix-vector multiplication (SpMV).
\item
  CSC (Compressed Sparse Column): Similar to CSR, but optimized for
  column operations.
\item
  DIA (Diagonal): Only store diagonals in banded matrices. Extremely
  memory-efficient for PDE solvers.
\item
  ELL (Ellpack/Itpack): Store each row padded to the same length,
  enabling SIMD and GPU vectorization. Works well when rows have similar
  numbers of nonzeros.
\item
  HYB (Hybrid, CUDA): Combines ELL for regular rows and COO for
  irregular cases. Used in GPU-accelerated sparse libraries.
\end{itemize}

\paragraph{Performance and
Complexity.}\label{performance-and-complexity.}

\begin{itemize}
\tightlist
\item
  Dictionaries/maps: O(1) average access, but higher overhead per entry.
\item
  COO: O(n) lookups, better for incremental construction.
\item
  CSR/CSC: excellent for batch operations, poor for insertions.
\item
  ELL/DIA: high throughput on SIMD/GPU hardware but inflexible.
\end{itemize}

Sparse matrix-vector multiplication (SpMV) illustrates trade-offs. With
CSR:

\begin{verbatim}
y[row] = Σ values[k] * x[col_indices[k]]  
\end{verbatim}

where \texttt{row\_ptr} guides which elements belong to each row. The
cost is proportional to the number of nonzeros, but performance is
limited by memory bandwidth and irregular access to \texttt{x}.

\paragraph{Cache and alignment.}\label{cache-and-alignment.}

Compressed formats improve locality for sequential access but introduce
irregular memory access patterns when multiplying or searching. Strided
iteration can align with cache lines, but pointer-heavy layouts fragment
memory. Padding (in ELL) improves SIMD alignment but wastes space.

\paragraph{Language and library
implementations.}\label{language-and-library-implementations.}

\begin{itemize}
\tightlist
\item
  Python SciPy: \texttt{csr\_matrix}, \texttt{csc\_matrix},
  \texttt{coo\_matrix}, \texttt{dia\_matrix}.
\item
  C++: Eigen and Armadillo expose CSR and CSC; Intel MKL provides highly
  optimized kernels.
\item
  CUDA/cuSPARSE: Hybrid ELL + COO kernels tuned for GPUs.
\end{itemize}

\paragraph{System-level use cases.}\label{system-level-use-cases.-1}

\begin{itemize}
\tightlist
\item
  Large-scale PDE solvers and finite element methods.
\item
  Graph algorithms (PageRank, shortest paths) using sparse adjacency
  matrices.
\item
  Inverted indices in search engines (postings lists).
\item
  Feature vectors in machine learning (bag-of-words, recommender
  systems).
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-4}

\begin{itemize}
\tightlist
\item
  Insertion is expensive in compressed formats (requires shifting or
  rebuilding).
\item
  Converting between formats (e.g., COO ↔ CSR) can dominate runtime if
  done repeatedly.
\item
  A poor choice of format (e.g., using ELL for irregular sparsity) can
  waste memory or block vectorization.
\end{itemize}

\paragraph{Optimization and
profiling.}\label{optimization-and-profiling.}

\begin{itemize}
\tightlist
\item
  Benchmark SpMV across formats and measure achieved bandwidth.
\item
  Profile cache misses and TLB behavior in irregular workloads.
\item
  On GPUs, measure coalesced vs scattered memory access to judge format
  suitability.
\end{itemize}

\paragraph{Worked Example (Python with
SciPy)}\label{worked-example-python-with-scipy}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.sparse }\ImportTok{import}\NormalTok{ csr\_matrix}

\CommentTok{\# Dense 5x5 with many zeros}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{]}
\NormalTok{])}

\CommentTok{\# Convert to CSR}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ csr\_matrix(dense)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"CSR data array:"}\NormalTok{, sparse.data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"CSR indices:"}\NormalTok{, sparse.indices)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"CSR indptr:"}\NormalTok{, sparse.indptr)}

\CommentTok{\# Sparse matrix{-}vector multiplication}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ sparse }\OperatorTok{@}\NormalTok{ x}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Result of SpMV:"}\NormalTok{, y)}
\end{Highlighting}
\end{Shaded}

This example shows how a dense matrix with many zeros can be stored
efficiently in CSR. Only nonzeros are stored, and SpMV avoids
unnecessary multiplications.

\subsubsection{Why it matters}\label{why-it-matters-26}

Sparse array formats are the backbone of scientific computing, machine
learning, and search engines. Choosing the right format determines
whether a computation runs in seconds or hours. At scale, cache
efficiency, memory bandwidth, and vectorization potential matter as much
as algorithmic complexity. Sparse arrays teach the critical lesson that
representation is performance.

\subsubsection{Exercises}\label{exercises-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement COO and CSR representations of the same sparse matrix and
  compare memory usage.
\item
  Write a small CSR-based SpMV routine and measure its speed against a
  dense implementation.
\item
  Explain why ELL format is efficient on GPUs but wasteful on highly
  irregular graphs.
\item
  In SciPy, convert a \texttt{csr\_matrix} to \texttt{csc\_matrix} and
  back. Measure the cost for large matrices.
\item
  Given a graph with 1M nodes and 10M edges, explain why adjacency lists
  and CSR are more practical than dense matrices.
\end{enumerate}

\section{2.6 Prefix Sums \& Scans}\label{prefix-sums-scans}

\subsection{2.6 L0 --- Running Totals}\label{l0-running-totals}

A prefix sum, also called a scan, is a way of turning a sequence into
running totals. Instead of just producing one final sum, we produce an
array where each position shows the sum of all earlier elements. It is
like keeping a receipt tape at the checkout: each item is added in
order, and you see the growing total after each step.

\subsubsection{Deep Dive}\label{deep-dive-18}

Prefix sums are simple but powerful. Given an array
\texttt{{[}a0,\ a1,\ a2,\ …,\ an-1{]}}, the prefix sum array
\texttt{{[}p0,\ p1,\ p2,\ …,\ pn-1{]}} is defined as:

\begin{itemize}
\item
  Inclusive scan:

\begin{verbatim}
pi = a0 + a1 + … + ai
\end{verbatim}
\item
  Exclusive scan:

\begin{verbatim}
pi = a0 + a1 + … + ai-1
\end{verbatim}

  (with p0 = 0 by convention).
\end{itemize}

Example with array \texttt{{[}1,\ 2,\ 3,\ 4{]}}:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Index & Original & Inclusive & Exclusive \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1 & 1 & 0 \\
1 & 2 & 3 & 1 \\
2 & 3 & 6 & 3 \\
3 & 4 & 10 & 6 \\
\end{longtable}

Prefix sums are built in a single pass, left to right. This is O(n) in
time, requiring an extra array of length n to store results.

Once constructed, prefix sums allow fast range queries. For any subarray
between indices \texttt{i} and \texttt{j}, the sum is:

\begin{verbatim}
sum(i..j) = prefix[j] - prefix[i-1]
\end{verbatim}

This reduces what would be O(n) work into O(1) time per query.

Prefix sums also generalize beyond addition: they can be built with
multiplication, min, max, or any associative operation.

\subsubsection{Worked Example}\label{worked-example-10}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}

\CommentTok{\# Inclusive prefix sum}
\NormalTok{inclusive }\OperatorTok{=}\NormalTok{ []}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    inclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Inclusive scan:"}\NormalTok{, inclusive)}

\CommentTok{\# Exclusive prefix sum}
\NormalTok{exclusive }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{]}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    exclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Exclusive scan:"}\NormalTok{, exclusive)}

\CommentTok{\# Range query using prefix sums}
\NormalTok{i, j }\OperatorTok{=} \DecValTok{1}\NormalTok{, }\DecValTok{3}  \CommentTok{\# sum from index 1 to 3 (2+3+4)}
\NormalTok{range\_sum }\OperatorTok{=}\NormalTok{ inclusive[j] }\OperatorTok{{-}}\NormalTok{ (inclusive[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Range sum (1..3):"}\NormalTok{, range\_sum)}
\end{Highlighting}
\end{Shaded}

This program shows inclusive and exclusive scans, and how to use them to
answer range queries quickly.

\subsubsection{Why it matters}\label{why-it-matters-27}

Prefix sums transform repeated work into reusable results. They make
range queries efficient, reduce algorithmic complexity, and appear in
countless applications: histograms, text processing, probability
distributions, and parallel computing. They also introduce the idea of
trading extra storage for faster queries, a common algorithmic
technique.

\subsubsection{Exercises}\label{exercises-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the prefix sum of \texttt{{[}1,\ 2,\ 3,\ 4,\ 5{]}} by hand.
\item
  Show the difference between inclusive and exclusive prefix sums for
  \texttt{{[}5,\ 10,\ 15{]}}.
\item
  Use a prefix sum to find the sum of elements from index 2 to 4 in
  \texttt{{[}3,\ 6,\ 9,\ 12,\ 15{]}}.
\item
  Given a prefix sum array \texttt{{[}2,\ 5,\ 9,\ 14{]}}, reconstruct
  the original array.
\item
  Explain why prefix sums are more efficient than computing each
  subarray sum from scratch when handling many queries.
\end{enumerate}

\subsection{2.6 L1 --- Prefix Sums in
Practice}\label{l1-prefix-sums-in-practice}

Prefix sums are a versatile tool for speeding up algorithms that involve
repeated range queries. Instead of recalculating sums over and over, we
preprocess the array once to create cumulative totals. This
preprocessing costs O(n), but it allows each query to be answered in
O(1).

\subsubsection{Deep Dive}\label{deep-dive-19}

A prefix sum array is built by scanning the original array from left to
right:

\begin{verbatim}
prefix[i] = prefix[i-1] + arr[i]
\end{verbatim}

This produces the inclusive scan. The exclusive scan shifts everything
rightward, leaving prefix{[}0{]} = 0 and excluding the current element.

The choice between inclusive and exclusive depends on application:

\begin{itemize}
\tightlist
\item
  Inclusive is easier for direct cumulative totals.
\item
  Exclusive is more natural when answering range queries.
\end{itemize}

Once built, prefix sums enable efficient operations:

\begin{itemize}
\tightlist
\item
  Range queries:
  \texttt{sum(i..j)\ =\ prefix{[}j{]}\ -\ prefix{[}i-1{]}}.
\item
  Reconstruction: the original array can be recovered with
  \texttt{arr{[}i{]}\ =\ prefix{[}i{]}\ -\ prefix{[}i-1{]}}.
\item
  Generalization: the same idea works for multiplication (cumulative
  product), logical OR/AND, or even min/max. The key requirement is that
  the operation is associative.
\end{itemize}

\paragraph{Trade-offs:}\label{trade-offs-1}

\begin{itemize}
\tightlist
\item
  Building prefix sums requires O(n) extra memory.
\item
  If only a few queries are needed, recomputing directly may be simpler.
\item
  For many queries, the preprocessing overhead is worthwhile.
\end{itemize}

\paragraph{Use cases:}\label{use-cases-1}

\begin{itemize}
\tightlist
\item
  Fast range-sum queries in databases or competitive programming.
\item
  Cumulative frequencies in histograms.
\item
  Substring analysis in text algorithms (e.g., number of vowels in a
  range).
\item
  Probability and statistics: cumulative distribution functions.
\end{itemize}

\paragraph{Language implementations:}\label{language-implementations}

\begin{itemize}
\tightlist
\item
  Python: \texttt{itertools.accumulate}, \texttt{numpy.cumsum}.
\item
  C++: \texttt{std::partial\_sum} from
  \texttt{\textless{}numeric\textgreater{}}.
\item
  Java: custom loop, or stream reductions.
\end{itemize}

\paragraph{Pitfalls:}\label{pitfalls}

\begin{itemize}
\tightlist
\item
  Confusing inclusive vs exclusive scans often leads to off-by-one
  errors.
\item
  For large datasets, cumulative sums may overflow fixed-width integers.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-11}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ itertools}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{]}

\CommentTok{\# Inclusive prefix sum using Python loop}
\NormalTok{inclusive }\OperatorTok{=}\NormalTok{ []}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    inclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Inclusive prefix sum:"}\NormalTok{, inclusive)}

\CommentTok{\# Exclusive prefix sum}
\NormalTok{exclusive }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{]}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    exclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Exclusive prefix sum:"}\NormalTok{, exclusive)}

\CommentTok{\# NumPy cumsum (inclusive)}
\NormalTok{np\_inclusive }\OperatorTok{=}\NormalTok{ np.cumsum(arr)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy inclusive scan:"}\NormalTok{, np\_inclusive)}

\CommentTok{\# Range query using prefix sums}
\NormalTok{i, j }\OperatorTok{=} \DecValTok{1}\NormalTok{, }\DecValTok{3}  \CommentTok{\# indices 1..3 → 4+6+8}
\NormalTok{range\_sum }\OperatorTok{=}\NormalTok{ inclusive[j] }\OperatorTok{{-}}\NormalTok{ (inclusive[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Range sum (1..3):"}\NormalTok{, range\_sum)}

\CommentTok{\# Recover original array from prefix sums}
\NormalTok{reconstructed }\OperatorTok{=}\NormalTok{ [inclusive[}\DecValTok{0}\NormalTok{]] }\OperatorTok{+}\NormalTok{ [inclusive[i] }\OperatorTok{{-}}\NormalTok{ inclusive[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(inclusive))]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reconstructed array:"}\NormalTok{, reconstructed)}
\end{Highlighting}
\end{Shaded}

This example demonstrates building prefix sums by hand, using built-in
libraries, answering queries, and reconstructing the original array.

\subsubsection{Why it matters}\label{why-it-matters-28}

Prefix sums reduce repeated work into reusable results. They transform
O(n) queries into O(1), making algorithms faster and more scalable. They
are a foundational idea in algorithm design, connecting to histograms,
distributions, and dynamic programming.

\subsubsection{Exercises}\label{exercises-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build both inclusive and exclusive prefix sums for
  \texttt{{[}5,\ 10,\ 15,\ 20{]}}.
\item
  Use prefix sums to compute the sum of elements from index 2 to 4 in
  \texttt{{[}1,\ 3,\ 5,\ 7,\ 9{]}}.
\item
  Given a prefix sum array \texttt{{[}3,\ 8,\ 15,\ 24{]}}, reconstruct
  the original array.
\item
  Write a procedure that computes cumulative products (scan with
  multiplication).
\item
  Explain why prefix sums are more useful when answering hundreds of
  queries instead of just one.
\end{enumerate}

\subsection{2.6 L2 --- Prefix Sums and Parallel
Scans}\label{l2-prefix-sums-and-parallel-scans}

Prefix sums seem simple, but at scale they become a central systems
primitive. They serve as the backbone of parallel algorithms, GPU
kernels, and high-performance libraries. At this level, the focus shifts
from ``what is a prefix sum'' to ``how can we compute it efficiently
across thousands of cores, with minimal synchronization and maximal
throughput?''

\subsubsection{Deep Dive}\label{deep-dive-20}

Sequential algorithm. The simple prefix sum is O(n):

\begin{verbatim}
prefix[0] = arr[0]
for i in 1..n-1:
    prefix[i] = prefix[i-1] + arr[i]
\end{verbatim}

Efficient for single-threaded contexts, but inherently sequential
because each value depends on the one before it.

Parallel algorithms. Two key approaches dominate:

\begin{itemize}
\item
  Hillis--Steele scan (1986):

  \begin{itemize}
  \tightlist
  \item
    Iterative doubling method.
  \item
    At step k, each thread adds the value from 2\^{}k positions behind.
  \item
    O(n log n) work, O(log n) depth. Simple but not work-efficient.
  \end{itemize}
\item
  Blelloch scan (1990):

  \begin{itemize}
  \item
    Work-efficient, O(n) total operations, O(log n) depth.
  \item
    Two phases:

    \begin{itemize}
    \tightlist
    \item
      Up-sweep (reduce): build a tree of partial sums.
    \item
      Down-sweep: propagate sums back down to compute prefix results.
    \end{itemize}
  \item
    Widely used in GPU libraries.
  \end{itemize}
\end{itemize}

\paragraph{Hardware performance.}\label{hardware-performance.}

\begin{itemize}
\tightlist
\item
  Cache-aware scans: memory locality matters for large arrays. Blocking
  and tiling reduce cache misses.
\item
  SIMD vectorization: multiple prefix elements are computed in parallel
  inside CPU vector registers.
\item
  GPUs: scans are implemented at warp and block levels, with CUDA
  providing primitives like \texttt{thrust::inclusive\_scan}. Warp
  shuffles (\texttt{\_\_shfl\_up\_sync}) allow efficient intra-warp
  scans without shared memory.
\end{itemize}

\paragraph{Memory and
synchronization.}\label{memory-and-synchronization.}

\begin{itemize}
\tightlist
\item
  In-place scans reduce memory use but complicate parallelization.
\item
  Exclusive vs inclusive variants require careful handling of initial
  values.
\item
  Synchronization overhead and false sharing are common risks in
  multithreaded CPU scans.
\item
  Distributed scans (MPI) require combining partial results from each
  node, then adjusting local scans with offsets.
\end{itemize}

\paragraph{Libraries and
implementations.}\label{libraries-and-implementations.}

\begin{itemize}
\tightlist
\item
  C++ TBB: \texttt{parallel\_scan} supports both exclusive and
  inclusive.
\item
  CUDA Thrust: \texttt{inclusive\_scan}, \texttt{exclusive\_scan} for
  GPU workloads.
\item
  OpenMP: provides \texttt{\#pragma\ omp\ parallel\ for\ reduction} but
  true scans require more explicit handling.
\item
  MPI: \texttt{MPI\_Scan} and \texttt{MPI\_Exscan} provide distributed
  prefix sums.
\end{itemize}

\paragraph{System-level use cases.}\label{system-level-use-cases.-2}

\begin{itemize}
\tightlist
\item
  Parallel histogramming: count frequencies in parallel, prefix sums to
  compute cumulative counts.
\item
  Radix sort: scans partition data into buckets efficiently.
\item
  Stream compaction: filter elements while maintaining order.
\item
  GPU memory allocation: prefix sums assign disjoint output positions to
  threads.
\item
  Database indexing: scans help build offsets for columnar data storage.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-5}

\begin{itemize}
\tightlist
\item
  Race conditions when threads update overlapping memory.
\item
  Load imbalance in irregular workloads (e.g., skewed distributions).
\item
  Wrong handling of inclusive vs exclusive leads to subtle bugs in
  partitioning algorithms.
\end{itemize}

\paragraph{Profiling and
optimization.}\label{profiling-and-optimization.}

\begin{itemize}
\tightlist
\item
  Benchmark sequential vs parallel scan on arrays of size 10\^{}6 or
  10\^{}9.
\item
  Compare scalability with 2, 4, 8, \ldots{} cores.
\item
  Measure GPU kernel efficiency at warp, block, and grid levels.
\end{itemize}

\subsubsection{Worked Example (CUDA
Thrust)}\label{worked-example-cuda-thrust}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}thrust/device\_vector.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}thrust/scan.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}iostream\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{device\_vector}\OperatorTok{\textless{}}\DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ data}\OperatorTok{\{}\DecValTok{1}\OperatorTok{,} \DecValTok{2}\OperatorTok{,} \DecValTok{3}\OperatorTok{,} \DecValTok{4}\OperatorTok{,} \DecValTok{5}\OperatorTok{\};}

    \CommentTok{// Inclusive scan}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{inclusive\_scan}\OperatorTok{(}\NormalTok{data}\OperatorTok{.}\NormalTok{begin}\OperatorTok{(),}\NormalTok{ data}\OperatorTok{.}\NormalTok{end}\OperatorTok{(),}\NormalTok{ data}\OperatorTok{.}\NormalTok{begin}\OperatorTok{());}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Inclusive scan: "}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ x }\OperatorTok{:}\NormalTok{ data}\OperatorTok{)} \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}}\NormalTok{ x }\OperatorTok{\textless{}\textless{}} \StringTok{" "}\OperatorTok{;}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}

    \CommentTok{// Exclusive scan}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{device\_vector}\OperatorTok{\textless{}}\DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ data2}\OperatorTok{\{}\DecValTok{1}\OperatorTok{,} \DecValTok{2}\OperatorTok{,} \DecValTok{3}\OperatorTok{,} \DecValTok{4}\OperatorTok{,} \DecValTok{5}\OperatorTok{\};}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{exclusive\_scan}\OperatorTok{(}\NormalTok{data2}\OperatorTok{.}\NormalTok{begin}\OperatorTok{(),}\NormalTok{ data2}\OperatorTok{.}\NormalTok{end}\OperatorTok{(),}\NormalTok{ data2}\OperatorTok{.}\NormalTok{begin}\OperatorTok{(),} \DecValTok{0}\OperatorTok{);}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Exclusive scan: "}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ x }\OperatorTok{:}\NormalTok{ data2}\OperatorTok{)} \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}}\NormalTok{ x }\OperatorTok{\textless{}\textless{}} \StringTok{" "}\OperatorTok{;}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This program offloads prefix sum computation to the GPU. With thousands
of threads, even huge arrays can be scanned in milliseconds.

\subsubsection{Why it matters}\label{why-it-matters-29}

Prefix sums are a textbook example of how a simple algorithm scales into
a building block of parallel computing. They are used in compilers,
graphics, search engines, and machine learning systems. They show how
rethinking algorithms for hardware (CPU caches, SIMD, GPUs, distributed
clusters) leads to new designs.

\subsubsection{Exercises}\label{exercises-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement the Hillis--Steele scan for an array of length 16 and show
  each step.
\item
  Implement the Blelloch scan in pseudocode and explain how the up-sweep
  and down-sweep phases work.
\item
  Benchmark a sequential prefix sum vs an OpenMP parallel scan on
  10\^{}7 elements.
\item
  In CUDA, implement an exclusive scan at the warp level using shuffle
  instructions.
\item
  Explain how prefix sums are used in stream compaction (removing zeros
  from an array while preserving order).
\end{enumerate}

\section{Deep Dive}\label{deep-dive-21}

\subsubsection{2.1 Static Arrays}\label{static-arrays-1}

\begin{itemize}
\tightlist
\item
  Memory alignment and padding in C and assembly.
\item
  Array indexing formulas compiled into machine code.
\item
  Page tables and kernel use of fixed-size arrays
  (\texttt{task\_struct}, \texttt{inode}).
\item
  Vectorization of loops over static arrays (SSE/AVX).
\item
  Bounds checking elimination in high-level languages.
\end{itemize}

\subsubsection{2.2 Dynamic Arrays}\label{dynamic-arrays-1}

\begin{itemize}
\tightlist
\item
  Growth factor experiments: doubling vs 1.5× vs incremental.
\item
  Profiling Python's list growth strategy (measure capacity jumps).
\item
  Amortized vs worst-case complexity: proofs with actual benchmarks.
\item
  Reallocation latency spikes in low-latency systems.
\item
  Comparing \texttt{std::vector::reserve} vs default growth.
\item
  Memory fragmentation in long-running programs.
\end{itemize}

\subsubsection{2.3 Slices \& Views}\label{slices-views-1}

\begin{itemize}
\tightlist
\item
  Slice metadata structure in Go (\texttt{ptr,\ len,\ cap}).
\item
  Rust borrow checker rules for \texttt{\&{[}T{]}} vs
  \texttt{\&mut\ {[}T{]}}.
\item
  NumPy stride tricks: transpose as a view, not a copy.
\item
  Performance gap: traversing contiguous vs strided slices.
\item
  Cache/TLB impact of strided access (e.g., step=16).
\item
  False sharing when two threads use overlapping slices.
\end{itemize}

\subsubsection{2.4 Multidimensional
Arrays}\label{multidimensional-arrays-1}

\begin{itemize}
\tightlist
\item
  Row-major vs column-major benchmarks: traverse order timing.
\item
  Linear index formulas for N-dimensional arrays.
\item
  Loop tiling/blocking for matrix multiplication.
\item
  Structure of Arrays (SoA) vs Array of Structures (AoS).
\item
  False sharing and padding in multi-threaded traversal.
\item
  BLAS/LAPACK optimizations and cache-aware kernels.
\item
  GPU coalesced memory access in 2D/3D arrays.
\end{itemize}

\subsubsection{2.5 Sparse Arrays \& Compressed
Layouts}\label{sparse-arrays-compressed-layouts}

\begin{itemize}
\tightlist
\item
  COO, CSR, CSC: hands-on with memory footprint and iteration cost.
\item
  Comparing dictionary-based vs CSR-based sparse vectors.
\item
  Parallel SpMV benchmarks on CPU vs GPU.
\item
  DIA and ELL formats: why they shine in structured sparsity.
\item
  Hybrid GPU formats (HYB: ELL + COO).
\item
  Search engine inverted indices as sparse structures.
\item
  Sparse arrays in ML: bag-of-words and embeddings.
\end{itemize}

\subsubsection{2.6 Prefix Sums \& Scans}\label{prefix-sums-scans-1}

\begin{itemize}
\tightlist
\item
  Inclusive vs exclusive scans: correctness pitfalls.
\item
  Hillis--Steele vs Blelloch scans: step count vs work efficiency.
\item
  Cache-friendly prefix sums on CPUs (blocked scans).
\item
  SIMD prefix sum using AVX intrinsics.
\item
  CUDA warp shuffle scans (\texttt{\_\_shfl\_up\_sync}).
\item
  MPI distributed scans across clusters.
\item
  Stream compaction via prefix sums (remove zeros in O(n)).
\item
  Radix sort built from parallel scans.
\end{itemize}

\section{LAB}\label{lab}

\subsubsection{2.1 Static Arrays}\label{static-arrays-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Implement fixed-size arrays in C and Python, compare
  access/update speeds.
\item
  LAB 2: Explore how static arrays are used in Linux kernel
  (\texttt{task\_struct}, page tables).
\item
  LAB 3: Disassemble a simple loop over a static array and inspect the
  generated assembly.
\item
  LAB 4: Benchmark cache effects: sequential vs random access in a large
  static array.
\end{itemize}

\subsubsection{2.2 Dynamic Arrays}\label{dynamic-arrays-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Implement your own dynamic array in C (with doubling strategy).
\item
  LAB 2: Benchmark Python's \texttt{list} growth by tracking capacity
  changes while appending.
\item
  LAB 3: Compare growth factors: doubling vs 1.5× vs fixed increments.
\item
  LAB 4: Stress test reallocation cost by appending millions of
  elements, measure latency spikes.
\item
  LAB 5: Use \texttt{std::vector::reserve} in C++ and compare
  performance vs default growth.
\end{itemize}

\subsubsection{2.3 Slices \& Views}\label{slices-views-2}

\begin{itemize}
\tightlist
\item
  LAB 1: In Go, experiment with slice creation, capacity, and append ---
  observe when new arrays are allocated.
\item
  LAB 2: In Rust, create overlapping slices and see how the borrow
  checker enforces safety.
\item
  LAB 3: In Python, compare slicing a list vs slicing a NumPy array ---
  demonstrate copy vs view behavior.
\item
  LAB 4: Benchmark stride slicing in NumPy (\texttt{arr{[}::16{]}}) and
  explain performance drop.
\item
  LAB 5: Demonstrate aliasing bugs when two slices share the same
  underlying array.
\end{itemize}

\subsubsection{2.4 Multidimensional
Arrays}\label{multidimensional-arrays-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Write code to traverse a 1000×1000 array row by row vs column
  by column, measure performance.
\item
  LAB 2: Implement your own 2D array in C using both contiguous memory
  and array-of-pointers, compare speed.
\item
  LAB 3: Use NumPy to confirm row-major order with \texttt{.strides},
  then create a column-major array and compare.
\item
  LAB 4: Implement a tiled matrix multiplication in C/NumPy and measure
  cache improvement.
\item
  LAB 5: Experiment with SoA vs AoS layouts for a struct of 3 floats
  (x,y,z). Measure iteration performance.
\end{itemize}

\subsubsection{2.5 Sparse Arrays \& Compressed
Layouts}\label{sparse-arrays-compressed-layouts-1}

\begin{itemize}
\tightlist
\item
  LAB 1: Implement sparse arrays with Python dict vs dense lists,
  compare memory usage.
\item
  LAB 2: Build COO and CSR representations for the same matrix, print
  memory layout.
\item
  LAB 3: Benchmark dense vs CSR matrix-vector multiplication.
\item
  LAB 4: Use SciPy's \texttt{csr\_matrix} and \texttt{csc\_matrix}, run
  queries, compare performance.
\item
  LAB 5: Implement a simple search engine inverted index as a sparse
  array of word→docID list.
\end{itemize}

\subsubsection{2.6 Prefix Sums \& Scans}\label{prefix-sums-scans-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Write inclusive and exclusive prefix sums in Python.
\item
  LAB 2: Benchmark prefix sums for answering 1000 range queries vs naive
  summation.
\item
  LAB 3: Implement Blelloch scan in C/NumPy and visualize the
  up-sweep/down-sweep steps.
\item
  LAB 4: Implement prefix sums on GPU (CUDA/Thrust), compare speed to
  CPU.
\item
  LAB 5: Use prefix sums for stream compaction: remove zeros from an
  array while preserving order.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Chapter 3. Strings}\label{chapter-3.-strings}

\section{3.1 Representation}\label{representation-1}

\subsection{3.1 L0 --- Necklace of
Characters}\label{l0-necklace-of-characters}

A string is a sequence of characters. Each character can be a letter,
number, symbol, or even whitespace. Strings are used to represent text,
from simple names and messages to entire documents. In programming, a
string is usually enclosed in quotes to tell the computer where it
begins and ends. Strings are fundamental because almost every program
interacts with text: input from users, output on screens, or data stored
in files.

\subsubsection{Deep Dive}\label{deep-dive-22}

Strings look simple but have a few important properties that beginners
must understand.

In many languages such as Python, strings cannot be changed after
creation. If a program ``changes'' a string, what really happens is the
creation of a new one. This is why appending repeatedly can be costly.

Each character in a string has a position, starting at 0. Accessing is
direct: \texttt{word{[}0{]}} gives the first character. Slicing extracts
portions: \texttt{word{[}1:4{]}} creates a substring.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
String & Index Positions \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{"HELLO"} & H=0, E=1, L=2, L=3, O=4 \\
\end{longtable}

Negative indices count backward: \texttt{word{[}-1{]}} gives the last
character.

Strings can be combined or repeated. Concatenation uses the \texttt{+}
operator; repetition uses \texttt{*}. Example: \texttt{"Hi"\ +\ "!"} →
\texttt{"Hi!"}, \texttt{"Na"\ *\ 4} → \texttt{"NaNaNaNa"}.

Quotes (\texttt{\textquotesingle{}} or \texttt{"}) mark strings. Escape
characters handle special cases: \texttt{"\textbackslash{}n"} is
newline, \texttt{"\textbackslash{}t"} is tab. Triple quotes allow
multi-line text.

Computers store strings as numbers. ASCII is a small table of 128
characters (A--Z, digits, symbols). Unicode is a universal table
covering all writing systems. Modern programs use Unicode (usually UTF-8
encoding), allowing text like \texttt{"こんにちは"} or \texttt{"😊"}.

\subsubsection{Worked Example (Python)}\label{worked-example-python-4}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Defining different kinds of strings}
\NormalTok{s1 }\OperatorTok{=} \StringTok{"Hello"}
\NormalTok{s2 }\OperatorTok{=} \StringTok{\textquotesingle{}World\textquotesingle{}}
\NormalTok{s3 }\OperatorTok{=} \StringTok{"""This is }
\StringTok{a multi{-}line string."""}

\CommentTok{\# Indexing}
\BuiltInTok{print}\NormalTok{(s1[}\DecValTok{0}\NormalTok{])      }\CommentTok{\# H}
\BuiltInTok{print}\NormalTok{(s1[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])     }\CommentTok{\# o}

\CommentTok{\# Slicing}
\BuiltInTok{print}\NormalTok{(s1[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{])    }\CommentTok{\# ell}

\CommentTok{\# Concatenation and repetition}
\NormalTok{greeting }\OperatorTok{=}\NormalTok{ s1 }\OperatorTok{+} \StringTok{" "} \OperatorTok{+}\NormalTok{ s2}
\BuiltInTok{print}\NormalTok{(greeting)   }\CommentTok{\# Hello World}

\NormalTok{laugh }\OperatorTok{=} \StringTok{"Ha"} \OperatorTok{*} \DecValTok{3}
\BuiltInTok{print}\NormalTok{(laugh)      }\CommentTok{\# HaHaHa}

\CommentTok{\# Escapes}
\NormalTok{quote }\OperatorTok{=} \StringTok{"She said: }\CharTok{\textbackslash{}"}\StringTok{Yes!}\CharTok{\textbackslash{}"}\StringTok{"}
\BuiltInTok{print}\NormalTok{(quote)      }\CommentTok{\# She said: "Yes!"}

\CommentTok{\# Unicode characters}
\NormalTok{emoji }\OperatorTok{=} \StringTok{"Smile 😊"}
\BuiltInTok{print}\NormalTok{(emoji)      }\CommentTok{\# Smile 😊}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-30}

Strings are everywhere. They store names, labels, and text messages.
They carry commands and configurations in programs. They are the
backbone of web pages, logs, and data files. Understanding immutability
avoids performance mistakes. Knowing slicing and concatenation makes
manipulation easier. Being aware of Unicode prevents bugs in
multilingual applications. Strings look simple but are a core
abstraction every developer must master.

\subsubsection{Exercises}\label{exercises-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the word \texttt{"COMPUTER"} and print the first, middle, and
  last character.
\item
  Slice the string \texttt{"PROGRAMMING"} to extract \texttt{"GRAM"}.
\item
  Concatenate \texttt{"DATA"} and \texttt{"BASE"} into one string
  without using \texttt{+}.
\item
  Repeat the string \texttt{"Na"} four times, then add
  \texttt{"\ Batman!"} at the end.
\item
  Create a multi-line string containing your name, age, and favorite
  color.
\item
  Find the difference between \texttt{word{[}0{]}} and
  \texttt{word{[}-1{]}} in \texttt{"PYTHON"}.
\item
  Write a string that contains both single and double quotes.
\item
  Show that \texttt{"abc"\ *\ 0} results in an empty string.
\item
  Print the Unicode string for \texttt{"😊"} alongside a normal word.
\item
  Explain in one sentence why immutability matters when working with
  strings.
\end{enumerate}

\subsection{3.1 L1 --- Encodings \&
Efficiency}\label{l1-encodings-efficiency}

Strings in modern programming languages are more than just ``text.''
They are data structures designed with specific trade-offs in memory
layout, immutability, and performance. At this level, understanding how
strings behave internally allows programmers to write faster, safer, and
more maintainable code.

\subsubsection{Overview/Definition}\label{overviewdefinition}

A string is immutable in many languages, meaning once created, it cannot
be changed. This design improves safety and makes strings easier to
reason about, but it also affects performance. Operations like
concatenation, slicing, and encoding conversions create new strings
under the hood. Intermediate programmers must understand how these costs
accumulate, when interning optimizes memory usage, and how encodings
like UTF-8 and UTF-16 impact representation.

\subsubsection{Deep Dive}\label{deep-dive-23}

Strings cannot be modified in place in Python, Java, or Go. If
\texttt{"hello"} is changed to \texttt{"hallo"}, the original object
remains untouched, and a new one is created. This has two main
consequences:

\begin{itemize}
\tightlist
\item
  Safe sharing across code without fear of accidental modification.
\item
  Extra memory and CPU costs when building new strings repeatedly.
\end{itemize}

Languages often reuse identical string values. For example, Python keeps
a single \texttt{"foo"} literal in memory if used multiple times. This
is called \emph{interning}.

\begin{itemize}
\tightlist
\item
  Automatic interning happens for identifiers and some literals.
\item
  Manual interning (\texttt{sys.intern()}) can reduce memory in
  applications with repeated keys (like symbol tables or parsers).
\end{itemize}

Strings store characters differently across languages:

\begin{itemize}
\item
  UTF-8: Variable-width, 1--4 bytes per character. Compact for
  ASCII-heavy text.
\item
  UTF-16: Mostly 2 bytes per character, but uses surrogate pairs for
  others.
\item
  UTF-32: Fixed 4 bytes per character, simple but wasteful.

  \begin{longtable}[]{@{}lll@{}}
  \toprule\noalign{}
  Encoding & Example Text \texttt{"A😊"} & Storage Size \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  ASCII & not representable & --- \\
  UTF-8 & \texttt{0x41\ 0xF0\ 0x9F\ 0x98\ 0x8A} & 5 bytes \\
  UTF-16 & \texttt{0x0041\ 0xD83D\ 0xDE0A} & 6 bytes \\
  UTF-32 & \texttt{0x00000041\ 0x0001F60A} & 8 bytes \\
  \end{longtable}

  This matters because operations like slicing or indexing depend on how
  characters are encoded.
\end{itemize}

Concatenation Costs

\begin{itemize}
\tightlist
\item
  In Python, \texttt{a\ +\ b} builds a new string by copying both
  inputs.
\item
  Repeated concatenation inside loops leads to quadratic performance.
\item
  Solution: accumulate pieces in a list and join once.
\end{itemize}

Example of inefficient vs efficient approach:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inefficient}
\NormalTok{result }\OperatorTok{=} \StringTok{""}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{):}
\NormalTok{    result }\OperatorTok{+=} \BuiltInTok{str}\NormalTok{(i)}

\CommentTok{\# Efficient}
\NormalTok{parts }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{):}
\NormalTok{    parts.append(}\BuiltInTok{str}\NormalTok{(i))}
\NormalTok{result }\OperatorTok{=} \StringTok{""}\NormalTok{.join(parts)}
\end{Highlighting}
\end{Shaded}

Slicing and Substrings

\begin{itemize}
\tightlist
\item
  In Python, \texttt{s{[}2:5{]}} creates a new string, copying data.
\item
  In Java before version 7u6, substrings shared the same underlying
  array (risking memory leaks if a small substring referenced a huge
  array). After 7u6, substring copies data to avoid this issue.
\item
  In C++, \texttt{std::string\_view} allows non-owning references to
  substrings without copying.
\end{itemize}

Encoding and Decoding

\begin{itemize}
\tightlist
\item
  Strings are abstract characters; at some point, they must become
  bytes.
\item
  \texttt{.encode()} turns a Python \texttt{str} into bytes using a
  given encoding.
\item
  \texttt{.decode()} reverses the process.
\item
  Pitfalls: decoding with the wrong encoding leads to errors or
  corrupted text.
\end{itemize}

Performance Pitfalls in Practice

\begin{itemize}
\tightlist
\item
  Parsing logs or JSON with millions of lines can cause memory spikes if
  string copies accumulate.
\item
  Unicode introduces complexity: slicing \texttt{"😊"} may split it in
  half if treated as bytes instead of characters.
\item
  Benchmarking is essential when dealing with high-throughput text
  pipelines.
\end{itemize}

Cross-Language Comparisons

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0696}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2957}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6348}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Language
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Python & Unicode (PEP 393 flexible storage) & Optimized for
compactness. \\
Java & UTF-16, immutable \texttt{String} & Uses \texttt{StringBuilder}
for mutable ops. \\
C++ & \texttt{std::string}, \texttt{string\_view} & SSO (small string
optimization) avoids heap allocation for short strings. \\
Go & UTF-8 encoded immutable slices & \texttt{string} is a read-only
\texttt{{[}{]}byte}. \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-5}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Interning demonstration}
\ImportTok{import}\NormalTok{ sys}

\NormalTok{a }\OperatorTok{=} \StringTok{"hello"}
\NormalTok{b }\OperatorTok{=} \StringTok{"hello"}
\BuiltInTok{print}\NormalTok{(a }\KeywordTok{is}\NormalTok{ b)  }\CommentTok{\# True, both refer to the same interned literal}

\CommentTok{\# Manual interning}
\NormalTok{x }\OperatorTok{=}\NormalTok{ sys.}\BuiltInTok{intern}\NormalTok{(}\StringTok{"repeated\_key"}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ sys.}\BuiltInTok{intern}\NormalTok{(}\StringTok{"repeated\_key"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(x }\KeywordTok{is}\NormalTok{ y)  }\CommentTok{\# True}

\CommentTok{\# Encoding and decoding}
\NormalTok{text }\OperatorTok{=} \StringTok{"Café"}
\NormalTok{encoded }\OperatorTok{=}\NormalTok{ text.encode(}\StringTok{"utf{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(encoded)  }\CommentTok{\# b\textquotesingle{}Caf\textbackslash{}xc3\textbackslash{}xa9\textquotesingle{}}
\NormalTok{decoded }\OperatorTok{=}\NormalTok{ encoded.decode(}\StringTok{"utf{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(decoded)  }\CommentTok{\# Café}

\CommentTok{\# Concatenation efficiency}
\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"alpha"}\NormalTok{, }\StringTok{"beta"}\NormalTok{, }\StringTok{"gamma"}\NormalTok{]}
\NormalTok{joined }\OperatorTok{=} \StringTok{"{-}"}\NormalTok{.join(words)}
\BuiltInTok{print}\NormalTok{(joined)  }\CommentTok{\# alpha{-}beta{-}gamma}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-31}

Intermediate-level understanding of string representation prevents
subtle but costly mistakes. Knowing that strings are immutable avoids
slow concatenation loops. Interning saves memory in large-scale
text-heavy systems. Choosing the right encoding prevents bugs when
handling international text. Understanding substring behavior helps
avoid hidden memory leaks. These details directly impact performance,
correctness, and reliability in production systems.

\subsubsection{Exercises}\label{exercises-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write code that shows the difference in performance between
  concatenating strings in a loop and using \texttt{join}.
\item
  Demonstrate string interning: show two identical literals referencing
  the same object.
\item
  Encode and decode a string containing accented characters using UTF-8.
\item
  Take a long string and slice it into two halves; explain why slicing
  creates a new object in Python.
\item
  Compare memory usage between UTF-8 and UTF-16 for a text that contains
  only ASCII characters.
\item
  Investigate what happens when decoding a UTF-8 byte sequence with
  ASCII encoding.
\item
  Show why \texttt{"😊"{[}0{]}} in Python gives a character but in UTF-8
  byte arrays it spans multiple bytes.
\item
  Explain the difference between Java \texttt{String} and
  \texttt{StringBuilder} in terms of mutability.
\item
  Write code that demonstrates the effect of \texttt{sys.intern()} on
  repeated keys.
\item
  Research and explain how \texttt{std::string\_view} in C++ avoids
  copies.
\end{enumerate}

\subsection{3.1 L2 --- Internals \& Systems}\label{l2-internals-systems}

Strings are central to modern software, but their internal design
depends heavily on language, runtime, and operating system. At this
level, understanding low-level representation, OS interaction, and
hardware acceleration becomes critical.

Strings are immutable in most high-level languages, but under the hood
they are arrays of bytes or characters managed by the runtime. Different
languages make different trade-offs: memory layout, encoding choice, and
substring handling. The operating system provides system calls for
moving string data between memory and devices. Hardware accelerates
common operations like copying and comparison. Production libraries
extend functionality for correctness and speed.

\subsubsection{Deep Dive}\label{deep-dive-24}

\paragraph{Low-Level Representations}\label{low-level-representations}

C strings are arrays of \texttt{char} terminated by
\texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}}. A
missing terminator causes buffer overflows. C++ \texttt{std::string}
stores length and data, with small string optimization (SSO) to keep
short strings inline. Python uses PEP 393: compact Unicode with 1, 2, or
4 bytes per character depending on the highest code point. Java
\texttt{String} is UTF-16, compressed if all characters fit in Latin-1.
Go strings are immutable slices of bytes; Rust distinguishes
\texttt{String} (owned) and \texttt{\&str} (borrowed).

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Language & Representation & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
C & \texttt{char\ *} +
\texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}} & Simple
but unsafe. \\
C++ & \texttt{std::string}, SSO & Short strings inline. \\
Python & Flexible Unicode & Adapts width per string. \\
Java & UTF-16, compressed & Immutable. \\
Go & UTF-8, slice view & Immutable. \\
Rust & \texttt{String} / \texttt{\&str} & UTF-8 guaranteed. \\
\end{longtable}

\paragraph{OS-Level Considerations}\label{os-level-considerations}

Strings occupy stack or heap memory. Large strings cause fragmentation.
Garbage-collected languages like Java and Python reclaim unused strings
automatically. System calls (\texttt{read}, \texttt{write}) exchange raw
bytes with the OS. In C, forgetting
\texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}} causes
overruns. At boundaries (sockets, files), encoding mismatches are a
common source of bugs.

\paragraph{Hardware-Level
Considerations}\label{hardware-level-considerations}

Performance depends on cache alignment. A string aligned to cache lines
is scanned faster. Libraries like \texttt{glibc} use SIMD instructions
for \texttt{memcpy}, \texttt{memcmp}, and \texttt{strlen}, processing
multiple bytes per CPU instruction. Large-scale search and comparison
rely on vectorized instructions. Misaligned memory leads to extra CPU
cycles.

\paragraph{Advanced Encoding Issues}\label{advanced-encoding-issues}

Unicode normalization ensures two strings that look identical are stored
in the same form. NFC and NFD differ in how accents are represented.
UTF-16 surrogate pairs represent code points beyond \texttt{0xFFFF}.
Grapheme clusters (like ``👨‍👩‍👧‍👦'') span multiple code points but behave as
one character for users. Security issues arise: Unicode confusables
trick users (\texttt{а} in Cyrillic vs \texttt{a} in Latin), and null
characters may bypass string checks in C.

\paragraph{Production Libraries \&
Techniques}\label{production-libraries-techniques}

ICU provides collation, normalization, and locale-sensitive operations.
RE2 avoids regex backtracking vulnerabilities. Hyperscan accelerates
string matching using SIMD. String views or slices in C++ and Rust
enable zero-copy substrings.

\paragraph{Performance Engineering}\label{performance-engineering}

Concatenation cost is linear in total length. Substrings may leak memory
if they reference a large parent buffer (Java pre-7u6). Copy semantics
avoid leaks but cost memory. Benchmarking is necessary to choose the
right trade-off. Immutability simplifies reasoning but requires builders
(\texttt{StringBuilder}, \texttt{bytes.Buffer}) for efficiency.

\subsubsection{Worked Example (Python)}\label{worked-example-python-6}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}
\ImportTok{import}\NormalTok{ unicodedata}
\ImportTok{import}\NormalTok{ time}

\CommentTok{\# {-}{-}{-} Representation {-}{-}{-}}
\NormalTok{ascii\_text }\OperatorTok{=} \StringTok{"Hello"}
\NormalTok{unicode\_text }\OperatorTok{=} \StringTok{"こんにちは😊"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ASCII:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(ascii\_text), }\StringTok{"chars,"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(ascii\_text.encode(}\StringTok{"utf{-}8"}\NormalTok{)), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Unicode:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(unicode\_text), }\StringTok{"chars,"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(unicode\_text.encode(}\StringTok{"utf{-}8"}\NormalTok{)), }\StringTok{"bytes"}\NormalTok{)}

\CommentTok{\# {-}{-}{-} Encoding/Decoding {-}{-}{-}}
\NormalTok{data }\OperatorTok{=} \StringTok{"Café"}
\NormalTok{utf8\_bytes }\OperatorTok{=}\NormalTok{ data.encode(}\StringTok{"utf{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}8 bytes:"}\NormalTok{, utf8\_bytes)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Decoded back:"}\NormalTok{, utf8\_bytes.decode(}\StringTok{"utf{-}8"}\NormalTok{))}

\CommentTok{\# {-}{-}{-} Interning {-}{-}{-}}
\NormalTok{a }\OperatorTok{=} \StringTok{"hello"}
\NormalTok{b }\OperatorTok{=} \StringTok{"hello"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a is b:"}\NormalTok{, a }\KeywordTok{is}\NormalTok{ b)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ sys.}\BuiltInTok{intern}\NormalTok{(}\StringTok{"repeated\_key"}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ sys.}\BuiltInTok{intern}\NormalTok{(}\StringTok{"repeated\_key"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x is y:"}\NormalTok{, x }\KeywordTok{is}\NormalTok{ y)}

\CommentTok{\# {-}{-}{-} Concatenation benchmark {-}{-}{-}}
\NormalTok{N }\OperatorTok{=} \DecValTok{20000}
\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{s }\OperatorTok{=} \StringTok{""}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
\NormalTok{    s }\OperatorTok{+=} \StringTok{"x"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive concat time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{parts }\OperatorTok{=}\NormalTok{ [}\StringTok{"x"} \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N)]}
\NormalTok{s }\OperatorTok{=} \StringTok{""}\NormalTok{.join(parts)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Join concat time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\CommentTok{\# {-}{-}{-} Unicode normalization {-}{-}{-}}
\NormalTok{s1 }\OperatorTok{=} \StringTok{"café"}          \CommentTok{\# composed}
\NormalTok{s2 }\OperatorTok{=} \StringTok{"cafe}\CharTok{\textbackslash{}u0301}\StringTok{"}    \CommentTok{\# decomposed}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Raw equal:"}\NormalTok{, s1 }\OperatorTok{==}\NormalTok{ s2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NFC equal:"}\NormalTok{, unicodedata.normalize(}\StringTok{"NFC"}\NormalTok{, s1) }\OperatorTok{==}\NormalTok{ unicodedata.normalize(}\StringTok{"NFC"}\NormalTok{, s2))}

\CommentTok{\# {-}{-}{-} Surrogate pairs \& grapheme clusters {-}{-}{-}}
\NormalTok{smile }\OperatorTok{=} \StringTok{"😊"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}16 units:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(smile.encode(}\StringTok{"utf{-}16"}\NormalTok{)) }\OperatorTok{//} \DecValTok{2}\NormalTok{)}
\NormalTok{family }\OperatorTok{=} \StringTok{"👨‍👩‍👧‍👦"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Family code points:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(family), }\StringTok{"rendered:"}\NormalTok{, family)}

\CommentTok{\# {-}{-}{-} Unicode confusables {-}{-}{-}}
\NormalTok{latin\_a }\OperatorTok{=} \StringTok{"a"}
\NormalTok{cyrillic\_a }\OperatorTok{=} \StringTok{"а"}  \CommentTok{\# visually similar}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Latin a == Cyrillic a:"}\NormalTok{, latin\_a }\OperatorTok{==}\NormalTok{ cyrillic\_a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Latin ord:"}\NormalTok{, }\BuiltInTok{ord}\NormalTok{(latin\_a), }\StringTok{"Cyrillic ord:"}\NormalTok{, }\BuiltInTok{ord}\NormalTok{(cyrillic\_a))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-32}

Knowing internals prevents bugs and security issues. Buffer overflows
from missing terminators break systems. Misunderstood Unicode can cause
errors in databases or user interfaces. Cache alignment and SIMD
accelerate processing of huge text datasets. Normalization avoids
mismatched text values. Confusables create attack vectors. Production
systems depend on engineers understanding these low-level details.

\subsubsection{Exercises}\label{exercises-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain why a string with a missing terminator can cause reading
  beyond its memory.
\item
  Compare how many bytes the string \texttt{"Hello"} uses in UTF-8,
  UTF-16, and UTF-32.
\item
  Show two visually identical strings that are not equal because of
  different Unicode code points.
\item
  Demonstrate normalization making two unequal strings equal.
\item
  Measure and compare performance between naive concatenation and
  buffered concatenation.
\item
  Explain how surrogate pairs represent characters beyond
  \texttt{0xFFFF}.
\item
  Construct a grapheme cluster (like a family emoji) and count its code
  points versus user-perceived characters.
\item
  Show how substring references can create memory leaks if the original
  large string is kept alive.
\item
  Investigate how cache alignment could affect the speed of scanning a
  very large string.
\item
  Design a test case that reveals a security risk using Unicode
  confusables.
\end{enumerate}

\section{3.2 Operations}\label{operations}

\subsection{3.2 L0 --- Everyday String
Manipulations}\label{l0-everyday-string-manipulations}

Strings are not only stored but also actively used and modified.
Beginners often start with everyday tasks: changing text to uppercase,
trimming spaces, searching for words, and building sentences. These
operations are simple yet powerful, and they form the foundation of text
processing in any program.

String operations are ways to manipulate or examine text. They include
changing case, removing whitespace, searching for substrings, splitting
sentences into words, joining words into sentences, and formatting
messages. These actions are essential for handling user input,
displaying results, and working with text files.

\subsubsection{Deep Dive}\label{deep-dive-25}

\paragraph{Case Conversion}\label{case-conversion}

Text often needs to be converted. Uppercase makes words stand out
(\texttt{"hello"\ →\ "HELLO"}). Lowercase is useful for comparisons
(\texttt{"Yes"} vs \texttt{"yes"}). Case conversion helps normalize text
before processing.

\paragraph{Trimming Whitespace}\label{trimming-whitespace}

Whitespace before or after text causes bugs when comparing values.
\texttt{"\ hello\ "} is not the same as \texttt{"hello"}. Trimming
removes unwanted spaces, tabs, or newlines.

\paragraph{Replacing Substrings}\label{replacing-substrings}

Sometimes one part of text must change.
\texttt{"cat".replace("c",\ "h")} becomes \texttt{"hat"}. Replacing
works for single characters or words.

\paragraph{Searching in Strings}\label{searching-in-strings}

Finding a substring answers whether \texttt{"dog"} appears inside
\texttt{"hotdog"}. Many languages return the index where the substring
begins. If not found, they return a special value (like \texttt{-1} or
\texttt{None}).

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Example & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Contains & \texttt{"hello"\ in\ "say\ hello"} & True \\
Index & \texttt{"abc".find("b")} & 1 \\
Not found & \texttt{"abc".find("z")} & -1 (or equivalent) \\
\end{longtable}

\paragraph{Splitting and Joining}\label{splitting-and-joining}

Splitting breaks text into parts: \texttt{"one\ two\ three"} →
\texttt{{[}"one",\ "two",\ "three"{]}}. Joining does the reverse:
\texttt{{[}"a","b","c"{]}} → \texttt{"a-b-c"}. They are inverse
operations.

\paragraph{Formatting}\label{formatting}

Combining variables with text creates messages.
\texttt{"Hello\ "\ +\ name} works, but formatting systems are clearer:
\texttt{"Hello,\ \{name\}!"}. Formatting ensures readable and consistent
text output.

\subsubsection{Worked Example (Python)}\label{worked-example-python-7}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Case conversion}
\NormalTok{word }\OperatorTok{=} \StringTok{"Hello"}
\BuiltInTok{print}\NormalTok{(word.upper())   }\CommentTok{\# HELLO}
\BuiltInTok{print}\NormalTok{(word.lower())   }\CommentTok{\# hello}

\CommentTok{\# Trimming whitespace}
\NormalTok{text }\OperatorTok{=} \StringTok{"   data  "}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Before:"}\NormalTok{, }\BuiltInTok{repr}\NormalTok{(text))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"After:"}\NormalTok{, }\BuiltInTok{repr}\NormalTok{(text.strip()))}

\CommentTok{\# Replacing substrings}
\NormalTok{animal }\OperatorTok{=} \StringTok{"cat"}
\BuiltInTok{print}\NormalTok{(animal.replace(}\StringTok{"c"}\NormalTok{, }\StringTok{"h"}\NormalTok{))  }\CommentTok{\# hat}

\CommentTok{\# Searching}
\NormalTok{sentence }\OperatorTok{=} \StringTok{"the quick brown fox"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"fox"} \KeywordTok{in}\NormalTok{ sentence)        }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(sentence.find(}\StringTok{"quick"}\NormalTok{))   }\CommentTok{\# 4}
\BuiltInTok{print}\NormalTok{(sentence.find(}\StringTok{"dog"}\NormalTok{))     }\CommentTok{\# {-}1}

\CommentTok{\# Splitting and joining}
\NormalTok{line }\OperatorTok{=} \StringTok{"one two three"}
\NormalTok{parts }\OperatorTok{=}\NormalTok{ line.split(}\StringTok{" "}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(parts)  }\CommentTok{\# [\textquotesingle{}one\textquotesingle{}, \textquotesingle{}two\textquotesingle{}, \textquotesingle{}three\textquotesingle{}]}
\NormalTok{joined }\OperatorTok{=} \StringTok{"{-}"}\NormalTok{.join(parts)}
\BuiltInTok{print}\NormalTok{(joined)  }\CommentTok{\# one{-}two{-}three}

\CommentTok{\# Formatting}
\NormalTok{name }\OperatorTok{=} \StringTok{"Alice"}
\NormalTok{age }\OperatorTok{=} \DecValTok{30}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"My name is }\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{, I am }\SpecialCharTok{\{}\NormalTok{age}\SpecialCharTok{\}}\SpecialStringTok{ years old."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-33}

Text is everywhere: names, messages, configuration files, logs. Programs
must read, search, and reshape strings constantly. Converting case
enables reliable comparisons. Trimming prevents subtle errors with extra
spaces. Splitting and joining support structured data handling.
Formatting improves readability for users. These basic operations turn
raw text into structured, usable information.

\subsubsection{Exercises}\label{exercises-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convert the string \texttt{"python"} to uppercase and lowercase.
\item
  Remove extra spaces from \texttt{"\ \ \ hello\ world\ \ \ "}.
\item
  Replace \texttt{"dog"} with \texttt{"cat"} in the sentence
  \texttt{"the\ dog\ barked"}.
\item
  Check if \texttt{"apple"} is inside \texttt{"pineapple"}.
\item
  Find the position of \texttt{"moon"} in \texttt{"the\ moon\ rises"}.
\item
  Split \texttt{"red,green,blue"} into parts and then join them back
  with \texttt{"\textbar{}"}.
\item
  Count how many times the word \texttt{"the"} appears in
  \texttt{"the\ cat\ and\ the\ dog"}.
\item
  Format a string that says \texttt{"My\ favorite\ number\ is\ X"},
  where \texttt{X} is any integer.
\item
  Combine a list of words into a sentence with spaces in between.
\item
  Explain in one sentence why trimming whitespace is important when
  processing user input.
\end{enumerate}

Do you want me to continue with 3.2 L1 in the same structured style,
adding intermediate concepts like regex basics and efficient searching?

\subsection{3.2 L1 --- Patterns \& Practical
Tricks}\label{l1-patterns-practical-tricks}

Strings are not only for simple manipulations. In real programs, text
must be searched, parsed, and transformed efficiently. At this level,
the focus shifts from basic usage to techniques that handle larger
inputs, avoid performance traps, and apply structured text processing.

Intermediate string operations expand beyond uppercase, trimming, and
splitting. They involve efficient searching, counting, and pattern
matching. Regular expressions become a tool for flexible matching.
Developers also need to parse structured data such as CSV and JSON,
where correctness and performance both matter.

\subsubsection{Deep Dive}\label{deep-dive-26}

\paragraph{Efficient Searching}\label{efficient-searching}

Most languages provide methods like \texttt{find}, \texttt{index},
\texttt{startswith}, and \texttt{endswith}. These run in linear time but
are implemented with optimizations in native code. Checking prefixes and
suffixes avoids scanning the entire string.

\paragraph{Counting and Replacing}\label{counting-and-replacing}

Counting how many times a substring occurs is essential for statistics
or validation. Replacing substrings can transform logs or templates.
Efficiency matters: repeated replacements in large text can be costly.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Example Input & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Startswith & \texttt{"hello".startswith("he")} & True \\
Endswith & \texttt{"world".endswith("ld")} & True \\
Count substring & \texttt{"banana".count("na")} & 2 \\
Replace & \texttt{"2025-09".replace("-",\ "/")} & \texttt{"2025/09"} \\
\end{longtable}

\paragraph{Regular Expressions (Regex)}\label{regular-expressions-regex}

Regex allows powerful pattern matching with symbols:

\begin{itemize}
\tightlist
\item
  \texttt{.} matches any character.
\item
  \texttt{*} means repeat zero or more times.
\item
  \texttt{+} means one or more times.
\item
  \texttt{{[}abc{]}} matches any of the listed characters.
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  Pattern \texttt{{[}0-9{]}+} matches any sequence of digits.
\item
  Pattern
  \texttt{\textbackslash{}w+@\textbackslash{}w+\textbackslash{}.\textbackslash{}w+}
  matches simple email addresses.
\end{itemize}

Regex engines vary: some use backtracking (flexible but can be slow),
others use DFA/NFA (linear-time but less expressive).

\paragraph{Parsing Structured Text}\label{parsing-structured-text}

Many formats (CSV, JSON, logs) are line-based text. Splitting on commas
or spaces is not enough for robust parsing, but for simple cases
splitting and trimming work. Intermediate developers should know the
limitations of naive parsing and when to use libraries.

\paragraph{Performance Pitfalls}\label{performance-pitfalls}

Naive string concatenation inside loops can be quadratic in cost.
Multiple find/replace calls on large inputs can also degrade
performance. The best practice is to buffer results or use streaming
parsers.

\subsubsection{Worked Example (Python)}\label{worked-example-python-8}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}

\NormalTok{text }\OperatorTok{=} \StringTok{"user: alice, email: alice@example.com; user: bob, email: bob@example.org"}

\CommentTok{\# Efficient searching}
\BuiltInTok{print}\NormalTok{(text.startswith(}\StringTok{"user"}\NormalTok{))      }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(text.endswith(}\StringTok{"org"}\NormalTok{))         }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(text.find(}\StringTok{"bob"}\NormalTok{))             }\CommentTok{\# position of \textquotesingle{}bob\textquotesingle{}}

\CommentTok{\# Counting and replacing}
\NormalTok{fruit }\OperatorTok{=} \StringTok{"banana"}
\BuiltInTok{print}\NormalTok{(fruit.count(}\StringTok{"na"}\NormalTok{))            }\CommentTok{\# 2}
\BuiltInTok{print}\NormalTok{(fruit.replace(}\StringTok{"na"}\NormalTok{, }\StringTok{"NA"}\NormalTok{))    }\CommentTok{\# baNANA}

\CommentTok{\# Regex: find all email addresses}
\NormalTok{emails }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{@}\DecValTok{\textbackslash{}w}\OperatorTok{+}\CharTok{\textbackslash{}.}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(emails)  }\CommentTok{\# [\textquotesingle{}alice@example.com\textquotesingle{}, \textquotesingle{}bob@example.org\textquotesingle{}]}

\CommentTok{\# Regex: extract all numbers from a string}
\NormalTok{data }\OperatorTok{=} \StringTok{"Order 123, item 456, total 789"}
\NormalTok{numbers }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\PreprocessorTok{[0{-}9]}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, data)}
\BuiltInTok{print}\NormalTok{(numbers)  }\CommentTok{\# [\textquotesingle{}123\textquotesingle{}, \textquotesingle{}456\textquotesingle{}, \textquotesingle{}789\textquotesingle{}]}

\CommentTok{\# Parsing structured text (simple CSV line)}
\NormalTok{line }\OperatorTok{=} \StringTok{"apple, banana, cherry"}
\NormalTok{parts }\OperatorTok{=}\NormalTok{ [p.strip() }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ line.split(}\StringTok{","}\NormalTok{)]}
\BuiltInTok{print}\NormalTok{(parts)  }\CommentTok{\# [\textquotesingle{}apple\textquotesingle{}, \textquotesingle{}banana\textquotesingle{}, \textquotesingle{}cherry\textquotesingle{}]}

\CommentTok{\# Performance demo: avoid naive concatenation}
\NormalTok{N }\OperatorTok{=} \DecValTok{10000}
\CommentTok{\# Slow}
\NormalTok{s }\OperatorTok{=} \StringTok{""}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
\NormalTok{    s }\OperatorTok{+=} \StringTok{"x"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(s))}

\CommentTok{\# Fast}
\NormalTok{s }\OperatorTok{=} \StringTok{""}\NormalTok{.join([}\StringTok{"x"} \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N)])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Join length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(s))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-34}

Intermediate operations turn raw text into structured data. Searching
and counting enable validation and analysis. Regex provides flexible
pattern extraction, but misuse can harm performance. Parsing prepares
text for further computation. Avoiding pitfalls like naive concatenation
ensures scalable code. These techniques are essential for handling logs,
configurations, user inputs, and datasets in real-world applications.

\subsubsection{Exercises (Easy → Hard)}\label{exercises-easy-hard}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check if the string \texttt{"algorithm"} starts with \texttt{"algo"}
  and ends with \texttt{"rithm"}.
\item
  Count how many times \texttt{"the"} appears in
  \texttt{"the\ cat\ and\ the\ dog\ and\ the\ theater"}.
\item
  Replace every \texttt{"-"} in \texttt{"2025-09-10"} with \texttt{"/"}.
\item
  Extract all numbers from the text
  \texttt{"room\ 101,\ floor\ 5,\ building\ 42"}.
\item
  Write a regex to match any word that starts with \texttt{"a"} and ends
  with \texttt{"e"}.
\item
  Parse \texttt{"red,\ green,\ blue\ ,\ yellow"} into a clean list of
  color names without spaces.
\item
  Demonstrate why repeated concatenation in a loop is slower than using
  a buffer or join.
\item
  Write a regex to match simple phone numbers like
  \texttt{"123-456-7890"}.
\item
  Explain why splitting CSV by commas fails if values contain quotes
  (e.g., \texttt{"apple,\ "banana,grape",\ cherry"}).
\item
  Design a regex that extracts domain names from email addresses.
\end{enumerate}

\subsection{3.2 L2 --- Algorithms \&
Systems}\label{l2-algorithms-systems}

Advanced string operations rely on specialized algorithms and system
support. Searching and matching must be efficient on gigabytes of text.
Regex engines balance flexibility with safety. Compilers tokenize source
code into symbols using string scanning. And at the system boundary,
encoding and locale awareness become critical for correct communication.

\subsubsection{Deep Dive}\label{deep-dive-27}

\paragraph{String Search Algorithms}\label{string-search-algorithms}

The naive search scans one character at a time, which is simple but
inefficient for large text.

\begin{itemize}
\tightlist
\item
  Knuth--Morris--Pratt (KMP) preprocesses the pattern to avoid redundant
  comparisons.
\item
  Boyer--Moore skips ahead using knowledge of mismatched characters,
  performing sublinear searches in practice.
\item
  These algorithms form the backbone of text editors, search utilities,
  and compilers.
\end{itemize}

\paragraph{Regex Engine Internals}\label{regex-engine-internals}

Regex engines are either backtracking-based (Perl, Python) or
automata-based (RE2, Rust).

\begin{itemize}
\tightlist
\item
  Backtracking engines support rich features but risk exponential
  slowdowns.
\item
  DFA/NFA-based engines guarantee linear-time execution but restrict
  advanced patterns.
\item
  JIT compilation of regex (like in Java or .NET) generates machine code
  for speed.
\end{itemize}

\paragraph{Tokenization and Lexical
Analysis}\label{tokenization-and-lexical-analysis}

Compilers and interpreters must scan source code into tokens. This uses
deterministic finite automata (DFAs) built from regex-like rules. For
example, recognizing identifiers, keywords, and numbers all rely on
string scanning.

\paragraph{Unicode-Aware Searching}\label{unicode-aware-searching}

Searching in Unicode text requires more than simple byte comparison.
Case folding (making case-insensitive comparisons) differs by locale.
Grapheme clusters must be considered as user-visible units, not code
points.

\paragraph{System Boundaries}\label{system-boundaries}

Strings cross boundaries when written to files, sockets, or system
calls. Encodings must match between sender and receiver. Misalignment
causes corrupted output or runtime errors. Databases and network APIs
rely on normalized, correctly encoded strings.

\paragraph{Performance Considerations}\label{performance-considerations}

Substring extraction may copy data (safe but costly) or reference parent
buffers (fast but risky). SIMD instructions accelerate scanning large
strings in modern CPUs. Benchmarks show naive operations become
bottlenecks at scale, requiring optimized algorithms.

\subsubsection{Worked Example (Python)}\label{worked-example-python-9}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}
\ImportTok{import}\NormalTok{ time}

\CommentTok{\# {-}{-}{-} Naive search {-}{-}{-}}
\KeywordTok{def}\NormalTok{ naive\_search(text, pattern):}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(text) }\OperatorTok{{-}} \BuiltInTok{len}\NormalTok{(pattern) }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
        \ControlFlowTok{if}\NormalTok{ text[i:i}\OperatorTok{+}\BuiltInTok{len}\NormalTok{(pattern)] }\OperatorTok{==}\NormalTok{ pattern:}
            \ControlFlowTok{return}\NormalTok{ i}
    \ControlFlowTok{return} \OperatorTok{{-}}\DecValTok{1}

\CommentTok{\# {-}{-}{-} KMP search {-}{-}{-}}
\KeywordTok{def}\NormalTok{ kmp\_table(pattern):}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \BuiltInTok{len}\NormalTok{(pattern)}
\NormalTok{    j }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(pattern)):}
        \ControlFlowTok{while}\NormalTok{ j }\OperatorTok{\textgreater{}} \DecValTok{0} \KeywordTok{and}\NormalTok{ pattern[i] }\OperatorTok{!=}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{=}\NormalTok{ table[j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ pattern[i] }\OperatorTok{==}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{+=} \DecValTok{1}
\NormalTok{            table[i] }\OperatorTok{=}\NormalTok{ j}
    \ControlFlowTok{return}\NormalTok{ table}

\KeywordTok{def}\NormalTok{ kmp\_search(text, pattern):}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ kmp\_table(pattern)}
\NormalTok{    j }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(text)):}
        \ControlFlowTok{while}\NormalTok{ j }\OperatorTok{\textgreater{}} \DecValTok{0} \KeywordTok{and}\NormalTok{ text[i] }\OperatorTok{!=}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{=}\NormalTok{ table[j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ text[i] }\OperatorTok{==}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ j }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(pattern):}
            \ControlFlowTok{return}\NormalTok{ i }\OperatorTok{{-}}\NormalTok{ j }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return} \OperatorTok{{-}}\DecValTok{1}

\CommentTok{\# {-}{-}{-} Performance test {-}{-}{-}}
\NormalTok{text }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{100000} \OperatorTok{+} \StringTok{"b"}
\NormalTok{pattern }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{1000} \OperatorTok{+} \StringTok{"b"}

\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive:"}\NormalTok{, naive\_search(text, pattern))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"KMP:"}\NormalTok{, kmp\_search(text, pattern))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"KMP time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\CommentTok{\# {-}{-}{-} Regex engines {-}{-}{-}}
\NormalTok{data }\OperatorTok{=} \StringTok{"user: alice@example.com id: 42"}
\NormalTok{emails }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{@}\DecValTok{\textbackslash{}w}\OperatorTok{+}\CharTok{\textbackslash{}.}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Regex found:"}\NormalTok{, emails)}

\CommentTok{\# {-}{-}{-} Unicode case folding {-}{-}{-}}
\NormalTok{s1 }\OperatorTok{=} \StringTok{"Straße"}
\NormalTok{s2 }\OperatorTok{=} \StringTok{"STRASSE"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Casefold equal:"}\NormalTok{, s1.casefold() }\OperatorTok{==}\NormalTok{ s2.casefold())}

\CommentTok{\# {-}{-}{-} Grapheme cluster awareness {-}{-}{-}}
\NormalTok{family }\OperatorTok{=} \StringTok{"👨‍👩‍👧‍👦"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Code points:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(family))     }\CommentTok{\# length in code points}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Rendered:"}\NormalTok{, family)             }\CommentTok{\# perceived as one emoji}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-35}

At scale, naive methods fail. Efficient algorithms like KMP and
Boyer--Moore make search practical in editors, search engines, and
compilers. Regex engines must be chosen carefully: flexible backtracking
engines can crash under malicious input, while DFA-based engines provide
safety. Unicode awareness ensures correctness in multilingual systems.
At system boundaries, encoding mismatches corrupt data. Performance
engineering determines whether systems handle megabytes or terabytes of
text reliably.

\subsubsection{Exercises}\label{exercises-34}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Describe how the naive substring search works and why it can be
  inefficient.
\item
  Compare the number of steps needed to search \texttt{"aaaaab"} in
  \texttt{"a"*1000\ +\ "b"} using naive vs KMP.
\item
  Explain the key idea behind Boyer--Moore that makes it faster than
  naive search.
\item
  Show why some regex patterns can lead to exponential backtracking.
\item
  Write a regex that matches valid identifiers (letters, digits,
  underscores, not starting with a digit).
\item
  Explain how compilers use finite automata to tokenize code.
\item
  Compare string equality with and without Unicode case folding
  (\texttt{"Straße"} vs \texttt{"STRASSE"}).
\item
  Show how a grapheme cluster (like family emoji) differs from code
  point count.
\item
  Explain why substring references in some languages (e.g., Java before
  7u6) could cause memory leaks.
\item
  Design a benchmark plan to measure the impact of SIMD acceleration on
  string scanning.
\end{enumerate}

\section{3.3 Comparison}\label{comparison}

\subsection{3.3 L0 --- Equality \& Ordering}\label{l0-equality-ordering}

Comparing strings is one of the simplest but most important operations
in programming. Equality checks determine if two pieces of text are the
same, while ordering lets us sort words alphabetically. These operations
are intuitive but have precise rules that beginners must understand.

\subsubsection{Overview/Definition}\label{overviewdefinition-1}

Two strings are equal if they contain exactly the same sequence of
characters in the same order. Ordering compares strings
lexicographically, meaning character by character from left to right,
similar to how words are sorted in a dictionary. These rules allow
programs to check conditions, filter text, and sort lists.

\subsubsection{Deep Dive}\label{deep-dive-28}

\paragraph{Equality}\label{equality}

Equality is strict: \texttt{"cat"} equals \texttt{"cat"} but not
\texttt{"Cat"}. Whitespace matters too, so \texttt{"hello\ "} is not the
same as \texttt{"hello"}. Computers check equality by comparing each
character in order until a difference is found or both end.

\paragraph{Inequality}\label{inequality}

If two strings differ in length or characters, they are not equal. The
result is simply \texttt{true} or \texttt{false}.

Lexicographic Ordering Ordering compares based on the numerical values
of characters. \texttt{"apple"} comes before \texttt{"banana"} because
\texttt{\textquotesingle{}a\textquotesingle{}} is less than
\texttt{\textquotesingle{}b\textquotesingle{}}. If the first characters
are equal, the comparison moves to the next.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Comparison & Example & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Equal & \texttt{"cat"\ ==\ "cat"} & True \\
Case-sensitive & \texttt{"cat"\ ==\ "Cat"} & False \\
Shorter vs longer & \texttt{"car"\ \textless{}\ "cart"} & True \\
Alphabetical & \texttt{"apple"\ \textless{}\ "banana"} & True \\
\end{longtable}

\paragraph{Sorting}\label{sorting}

Sorting a list of strings uses lexicographic rules. The result is the
same as dictionary order in English when restricted to simple lowercase
letters.

\subsubsection{Worked Example (Python)}\label{worked-example-python-10}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Equality}
\BuiltInTok{print}\NormalTok{(}\StringTok{"cat"} \OperatorTok{==} \StringTok{"cat"}\NormalTok{)     }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(}\StringTok{"cat"} \OperatorTok{==} \StringTok{"Cat"}\NormalTok{)     }\CommentTok{\# False}
\BuiltInTok{print}\NormalTok{(}\StringTok{"hello "} \OperatorTok{==} \StringTok{"hello"}\NormalTok{)}\CommentTok{\# False}

\CommentTok{\# Inequality}
\BuiltInTok{print}\NormalTok{(}\StringTok{"dog"} \OperatorTok{!=} \StringTok{"cat"}\NormalTok{)     }\CommentTok{\# True}

\CommentTok{\# Lexicographic ordering}
\BuiltInTok{print}\NormalTok{(}\StringTok{"apple"} \OperatorTok{\textless{}} \StringTok{"banana"}\NormalTok{) }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(}\StringTok{"zebra"} \OperatorTok{\textgreater{}} \StringTok{"yak"}\NormalTok{)    }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(}\StringTok{"car"} \OperatorTok{\textless{}} \StringTok{"cart"}\NormalTok{)     }\CommentTok{\# True}

\CommentTok{\# Sorting a list}
\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"banana"}\NormalTok{, }\StringTok{"apple"}\NormalTok{, }\StringTok{"cherry"}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{sorted}\NormalTok{(words))      }\CommentTok{\# [\textquotesingle{}apple\textquotesingle{}, \textquotesingle{}banana\textquotesingle{}, \textquotesingle{}cherry\textquotesingle{}]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-36}

Equality and ordering are the foundation of text handling. Programs must
compare passwords, search for keywords, or sort lists of names. Without
correct comparison, search engines would mis-rank results and databases
would fail to find matches. Even small differences like uppercase vs
lowercase or extra spaces can change outcomes. Understanding these rules
prevents errors in everyday programming.

\subsubsection{Exercises}\label{exercises-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check if \texttt{"dog"} equals \texttt{"Dog"}.
\item
  Compare \texttt{"sun"} and \texttt{"moon"} and decide which comes
  first alphabetically.
\item
  Show why \texttt{"car"} is less than \texttt{"cart"}.
\item
  Sort the list \texttt{{[}"pear",\ "apple",\ "orange"{]}} in dictionary
  order.
\item
  Explain why \texttt{"hello\ "} and \texttt{"hello"} are not equal.
\item
  Compare \texttt{"Zoo"} and \texttt{"apple"} and explain why uppercase
  letters can affect order.
\item
  Demonstrate that two strings must have the same length and characters
  to be equal.
\item
  Given a list of names, describe how to remove duplicates using string
  equality.
\item
  Explain what happens if two strings are equal up to the length of the
  shorter one (e.g., \texttt{"abc"} vs \texttt{"abcd"}).
\item
  Design a procedure to sort a list of mixed short and long strings
  alphabetically.
\end{enumerate}

\subsection{3.3 L1 --- Collation \& Locales}\label{l1-collation-locales}

Comparing strings gets more complicated once you move beyond simple
equality and dictionary order. Real-world applications must deal with
case sensitivity, cultural rules, and mixed content like numbers and
letters. At this level, programmers learn practical techniques for
handling comparisons in everyday systems.

Collation is the set of rules that define how strings are compared and
sorted. While lexicographic order works for plain ASCII, different
languages and contexts require more sophisticated rules. For example,
\texttt{"ä"} may be treated as equal to \texttt{"a"} in one locale but
as a separate letter in another. Intermediate-level comparison also
introduces the idea of ignoring case and whitespace when appropriate.

\subsubsection{Deep Dive}\label{deep-dive-29}

\paragraph{Case Sensitivity}\label{case-sensitivity}

By default, string comparison is case-sensitive: \texttt{"cat"} ≠
\texttt{"Cat"}. Case-insensitive comparison requires converting both
sides to the same case or using a locale-aware function.

\paragraph{Locales and Cultural Rules}\label{locales-and-cultural-rules}

Different languages define alphabetical order differently.

\begin{itemize}
\tightlist
\item
  In German, \texttt{"ä"} often sorts like \texttt{"ae"}.
\item
  In Swedish, \texttt{"ä"} is a separate letter after \texttt{"z"}.
\item
  In English, \texttt{"apple"\ \textless{}\ "Banana"} because uppercase
  and lowercase letters use different numeric codes, but in many
  systems, case-insensitive comparison is preferred.
\end{itemize}

\paragraph{Numbers in Strings}\label{numbers-in-strings}

Lexicographic order can be confusing with numbers. \texttt{"file10"}
comes before \texttt{"file2"} because \texttt{"1"} sorts before
\texttt{"2"}. Natural sorting treats numbers as whole values so
\texttt{"file2"\ \textless{}\ "file10"}.

\paragraph{Databases and Collation}\label{databases-and-collation}

Databases define collations to decide how text is compared and sorted. A
collation can be case-sensitive (\texttt{CS}) or case-insensitive
(\texttt{CI}). It can also be accent-sensitive (\texttt{AS}) or
accent-insensitive (\texttt{AI}). This ensures consistent results across
queries.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Comparison Type & Example & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Case-sensitive & \texttt{"cat"\ ==\ "Cat"} & False \\
Case-insensitive & \texttt{"cat"\ \textasciitilde{}\ "Cat"} & True \\
Locale (German) & \texttt{"äpfel"\ vs\ "apfel"} & Equal or ordered
together \\
Natural sorting & \texttt{"file2"\ \textless{}\ "file10"} & True \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-11}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ locale}
\ImportTok{import}\NormalTok{ re}

\CommentTok{\# Case{-}sensitive vs case{-}insensitive}
\BuiltInTok{print}\NormalTok{(}\StringTok{"cat"} \OperatorTok{==} \StringTok{"Cat"}\NormalTok{)            }\CommentTok{\# False}
\BuiltInTok{print}\NormalTok{(}\StringTok{"cat"}\NormalTok{.lower() }\OperatorTok{==} \StringTok{"Cat"}\NormalTok{.lower())  }\CommentTok{\# True}

\CommentTok{\# Locale{-}aware comparison}
\NormalTok{locale.setlocale(locale.LC\_COLLATE, }\StringTok{"de\_DE.UTF{-}8"}\NormalTok{)  }\CommentTok{\# German}
\BuiltInTok{print}\NormalTok{(locale.strcoll(}\StringTok{"äpfel"}\NormalTok{, }\StringTok{"apfel"}\NormalTok{))  }\CommentTok{\# May treat ä \textasciitilde{} ae}

\NormalTok{locale.setlocale(locale.LC\_COLLATE, }\StringTok{"sv\_SE.UTF{-}8"}\NormalTok{)  }\CommentTok{\# Swedish}
\BuiltInTok{print}\NormalTok{(locale.strcoll(}\StringTok{"ä"}\NormalTok{, }\StringTok{"z"}\NormalTok{))  }\CommentTok{\# ä sorted after z}

\CommentTok{\# Natural sorting with numbers}
\NormalTok{files }\OperatorTok{=}\NormalTok{ [}\StringTok{"file2"}\NormalTok{, }\StringTok{"file10"}\NormalTok{, }\StringTok{"file1"}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{sorted}\NormalTok{(files))   }\CommentTok{\# Lexicographic: [\textquotesingle{}file1\textquotesingle{}, \textquotesingle{}file10\textquotesingle{}, \textquotesingle{}file2\textquotesingle{}]}
\NormalTok{files.sort(key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: [}\BuiltInTok{int}\NormalTok{(n) }\ControlFlowTok{if}\NormalTok{ n.isdigit() }\ControlFlowTok{else}\NormalTok{ n }\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ re.split(}\VerbatimStringTok{r\textquotesingle{}}\KeywordTok{(}\DecValTok{\textbackslash{}d}\OperatorTok{+}\KeywordTok{)}\VerbatimStringTok{\textquotesingle{}}\NormalTok{, x)])}
\BuiltInTok{print}\NormalTok{(files)           }\CommentTok{\# Natural sort: [\textquotesingle{}file1\textquotesingle{}, \textquotesingle{}file2\textquotesingle{}, \textquotesingle{}file10\textquotesingle{}]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-37}

Collation and locale rules affect almost every application that
processes human text. Sorting names in a contact list, comparing file
names, or filtering database queries all rely on comparison rules.
Incorrect handling leads to confusing results for users, especially in
multilingual contexts. Natural sorting improves readability when working
with numbered data. Database collations ensure consistency in queries
and reports.

\subsubsection{Exercises}\label{exercises-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare \texttt{"cat"} and \texttt{"Cat"} both case-sensitive and
  case-insensitive.
\item
  Explain why \texttt{"file10"} comes before \texttt{"file2"} in
  lexicographic order.
\item
  Show how converting both strings to lowercase helps with
  case-insensitive comparison.
\item
  Describe how \texttt{"ä"} is treated differently in German vs Swedish
  ordering.
\item
  Given a list of filenames (\texttt{file1}, \texttt{file2},
  \texttt{file10}), sort them in natural order.
\item
  Explain how a database collation can make \texttt{"résumé"} equal to
  \texttt{"resume"}.
\item
  Show how case-insensitive and accent-insensitive comparisons change
  the equality of \texttt{"café"} and \texttt{"CAFE"}.
\item
  Create a rule set for sorting words that mix numbers and letters, like
  \texttt{"a2"}, \texttt{"a10"}, \texttt{"a3"}.
\item
  Demonstrate how incorrect collation could mis-sort a list of
  international names.
\item
  Design an algorithm for locale-aware string comparison that handles
  both case and accent differences.
\end{enumerate}

\subsection{3.3 L2 --- Deep Comparisons}\label{l2-deep-comparisons}

At the advanced level, comparing strings requires knowledge of Unicode,
normalization, system-level optimizations, and specialized libraries.
Equality and ordering are no longer trivial: they must handle multiple
scripts, cultural rules, and performance constraints at scale.

Deep comparison of strings involves more than checking characters one by
one. Unicode introduces multiple ways to represent the same text.
Collation rules vary across locales. Large-scale systems need efficient
comparison functions that work correctly with international text while
remaining fast enough for billions of operations.

\subsubsection{Deep Dive}\label{deep-dive-30}

\paragraph{Unicode Normalization}\label{unicode-normalization}

The same character can be represented in different ways. For example,
\texttt{"é"} can be stored as a single code point (\texttt{U+00E9}) or
as \texttt{\textquotesingle{}e\textquotesingle{}} plus a combining
accent (\texttt{U+0065\ U+0301}). Normalization ensures consistent
representation. NFC (composed) and NFD (decomposed) are the most common
forms.

\paragraph{Case Folding}\label{case-folding}

Case-insensitive comparisons must use case folding, which is more
complex than simply converting to lowercase. For instance, the German
\texttt{"ß"} becomes \texttt{"ss"} when folded.

\paragraph{Collation with ICU}\label{collation-with-icu}

The International Components for Unicode (ICU) library provides robust
comparison and sorting. It supports locale-sensitive rules, accent
sensitivity, and custom collation. For example, \texttt{"résumé"} may
equal \texttt{"resume"} in accent-insensitive mode.

\paragraph{System-Level
Implementations}\label{system-level-implementations}

At low levels, comparison often reduces to optimized memory functions.
Functions like \texttt{memcmp} compare bytes quickly, using CPU
instructions that process multiple bytes at once. Short-circuit
evaluation stops at the first mismatch. Cache alignment improves
throughput when scanning long strings.

\paragraph{Security Concerns}\label{security-concerns}

Unicode confusables (characters that look alike, such as Latin
\texttt{"a"} and Cyrillic \texttt{"а"}) can fool comparison routines if
normalization is not enforced. Attackers exploit this in phishing and
injection attacks.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3421}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Challenge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Solution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Multiple representations & \texttt{"é"} vs
\texttt{"e\textbackslash{}u0301"} & Normalize (NFC/NFD) \\
Case folding & \texttt{"Straße"} vs \texttt{"STRASSE"} & Unicode-aware
case folding \\
Collation differences & \texttt{"ä"} in German vs Swedish & ICU locale
rules \\
Confusables & \texttt{"a"} vs \texttt{"а"} & Security checks, mapping \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-12}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ unicodedata}
\ImportTok{import}\NormalTok{ locale}

\CommentTok{\# Unicode normalization}
\NormalTok{s1 }\OperatorTok{=} \StringTok{"café"}          \CommentTok{\# composed}
\NormalTok{s2 }\OperatorTok{=} \StringTok{"cafe}\CharTok{\textbackslash{}u0301}\StringTok{"}    \CommentTok{\# decomposed}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Raw equal:"}\NormalTok{, s1 }\OperatorTok{==}\NormalTok{ s2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NFC equal:"}\NormalTok{, unicodedata.normalize(}\StringTok{"NFC"}\NormalTok{, s1) }\OperatorTok{==}\NormalTok{ unicodedata.normalize(}\StringTok{"NFC"}\NormalTok{, s2))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NFD equal:"}\NormalTok{, unicodedata.normalize(}\StringTok{"NFD"}\NormalTok{, s1) }\OperatorTok{==}\NormalTok{ unicodedata.normalize(}\StringTok{"NFD"}\NormalTok{, s2))}

\CommentTok{\# Case folding}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ß"}\NormalTok{.casefold() }\OperatorTok{==} \StringTok{"ss"}\NormalTok{)   }\CommentTok{\# True}

\CommentTok{\# Locale{-}sensitive ordering}
\NormalTok{locale.setlocale(locale.LC\_COLLATE, }\StringTok{"fr\_FR.UTF{-}8"}\NormalTok{)  }\CommentTok{\# French}
\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"école"}\NormalTok{, }\StringTok{"eclair"}\NormalTok{, }\StringTok{"étude"}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{sorted}\NormalTok{(words, key}\OperatorTok{=}\NormalTok{locale.strxfrm))  }\CommentTok{\# Locale{-}aware sort}

\CommentTok{\# Confusables}
\NormalTok{latin\_a }\OperatorTok{=} \StringTok{"a"}
\NormalTok{cyrillic\_a }\OperatorTok{=} \StringTok{"а"}  \CommentTok{\# visually similar}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal?"}\NormalTok{, latin\_a }\OperatorTok{==}\NormalTok{ cyrillic\_a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Latin:"}\NormalTok{, }\BuiltInTok{ord}\NormalTok{(latin\_a), }\StringTok{"Cyrillic:"}\NormalTok{, }\BuiltInTok{ord}\NormalTok{(cyrillic\_a))}

\CommentTok{\# System{-}level demonstration (byte comparison)}
\NormalTok{b1 }\OperatorTok{=} \StringTok{b"apple"}
\NormalTok{b2 }\OperatorTok{=} \StringTok{b"apricot"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"memcmp style:"}\NormalTok{, (b1 }\OperatorTok{\textgreater{}}\NormalTok{ b2) }\OperatorTok{{-}}\NormalTok{ (b1 }\OperatorTok{\textless{}}\NormalTok{ b2))  }\CommentTok{\# {-}1, 0, or 1}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-38}

Deep string comparison underpins databases, search engines, and
operating systems. Normalization ensures data consistency across storage
and transfer. Case folding avoids incorrect matches in multilingual
systems. ICU collation makes applications usable across cultures.
Optimized byte comparisons keep performance acceptable at scale.
Security requires detecting confusables to prevent spoofing. Without
these techniques, global systems would be unreliable and vulnerable.

\subsubsection{Exercises}\label{exercises-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Give an example where two visually identical strings are unequal
  because of different Unicode encodings.
\item
  Normalize two strings with accents and show they become equal.
\item
  Explain why \texttt{"ß"} must be compared to \texttt{"ss"} in
  case-insensitive matching.
\item
  Sort the words \texttt{"école"}, \texttt{"eclair"}, \texttt{"étude"}
  using French collation rules.
\item
  Show why \texttt{"a"} (Latin) and \texttt{"а"} (Cyrillic) are
  dangerous in string comparison.
\item
  Describe how \texttt{memcmp} works at the system level to compare two
  byte sequences.
\item
  Explain how cache alignment improves performance when comparing long
  strings.
\item
  Demonstrate the difference between accent-sensitive and
  accent-insensitive comparison with \texttt{"café"} vs \texttt{"cafe"}.
\item
  Propose a method to detect and mitigate Unicode confusables in user
  input.
\item
  Design a benchmark to compare normalized vs non-normalized string
  comparison in a large dataset.
\end{enumerate}

\section{3.4 Patterns}\label{patterns}

\subsection{3.4 L0 --- Simple Search}\label{l0-simple-search}

Strings are often used to find and replace text. Beginners need to
understand how to look for substrings, replace parts of text, and use
simple wildcards. These operations are the first step toward more
advanced pattern matching.

\subsubsection{Overview/Definition}\label{overviewdefinition-2}

A search operation checks if a smaller string exists inside a larger
one. If found, it returns the position or a confirmation. Replace
operations swap one substring for another. Wildcards act as
placeholders, matching characters in flexible ways. These basics allow
programs to search, clean, and transform text.

\subsubsection{Deep Dive}\label{deep-dive-31}

\paragraph{Finding Substrings}\label{finding-substrings}

The simplest search asks: does \texttt{"dog"} appear inside
\texttt{"hotdog"}? Most systems return the starting index of the match.
If not found, a special value is returned.

\paragraph{Replacing Text}\label{replacing-text}

Replacement takes an old value and swaps it with a new one.
\texttt{"the\ sky\ is\ blue"} with \texttt{"blue"\ →\ "red"} becomes
\texttt{"the\ sky\ is\ red"}. Replacement is useful for cleaning or
formatting text.

\paragraph{Wildcards (Intuitive View)}\label{wildcards-intuitive-view}

Wildcards are simple symbols that represent ``any character.'' A
\texttt{*} can mean ``any sequence,'' and a \texttt{?} can mean ``one
character.'' Example: pattern \texttt{"ca*"} matches \texttt{"cat"},
\texttt{"car"}, and \texttt{"castle"}. Wildcards are simpler than full
regular expressions but build the same intuition.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Example Input & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Contains & \texttt{"cat"} in \texttt{"concatenate"} & True \\
Find index & \texttt{"hello".find("lo")} & 3 \\
Replace & \texttt{"2025-09-10".replace("-",\ "/")} &
\texttt{"2025/09/10"} \\
Wildcard & Pattern \texttt{"ca*"} vs \texttt{"castle"} & Match \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-13}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Searching substrings}
\NormalTok{sentence }\OperatorTok{=} \StringTok{"the quick brown fox"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"quick"} \KeywordTok{in}\NormalTok{ sentence)       }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(sentence.find(}\StringTok{"fox"}\NormalTok{))      }\CommentTok{\# 16}
\BuiltInTok{print}\NormalTok{(sentence.find(}\StringTok{"dog"}\NormalTok{))      }\CommentTok{\# {-}1 (not found)}

\CommentTok{\# Replacing text}
\NormalTok{date }\OperatorTok{=} \StringTok{"2025{-}09{-}10"}
\BuiltInTok{print}\NormalTok{(date.replace(}\StringTok{"{-}"}\NormalTok{, }\StringTok{"/"}\NormalTok{))    }\CommentTok{\# 2025/09/10}

\CommentTok{\# Simple wildcard matching (manual example)}
\KeywordTok{def}\NormalTok{ matches(pattern, text):}
    \ControlFlowTok{if}\NormalTok{ pattern }\OperatorTok{==} \StringTok{"*"}\NormalTok{:  }\CommentTok{\# match everything}
        \ControlFlowTok{return} \VariableTok{True}
    \ControlFlowTok{if}\NormalTok{ pattern.endswith(}\StringTok{"*"}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ text.startswith(pattern[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
    \ControlFlowTok{if}\NormalTok{ pattern.startswith(}\StringTok{"*"}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ text.endswith(pattern[}\DecValTok{1}\NormalTok{:])}
    \ControlFlowTok{return}\NormalTok{ pattern }\OperatorTok{==}\NormalTok{ text}

\BuiltInTok{print}\NormalTok{(matches(}\StringTok{"ca*"}\NormalTok{, }\StringTok{"cat"}\NormalTok{))      }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(matches(}\StringTok{"ca*"}\NormalTok{, }\StringTok{"castle"}\NormalTok{))   }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(matches(}\StringTok{"*ing"}\NormalTok{, }\StringTok{"running"}\NormalTok{)) }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(matches(}\StringTok{"*at"}\NormalTok{, }\StringTok{"cat"}\NormalTok{))      }\CommentTok{\# True}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-39}

Searching and replacing text is one of the most common tasks in
programming. From finding a word in a document, to cleaning up user
input, to formatting data for storage, these operations are essential.
Wildcards introduce the idea of flexible matching, which prepares
learners for regular expressions and more advanced pattern recognition.

\subsubsection{Exercises}\label{exercises-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check if the word \texttt{"sun"} appears in the sentence
  \texttt{"the\ sun\ sets"}.
\item
  Find the position of \texttt{"moon"} in \texttt{"the\ moon\ rises"}.
\item
  Replace \texttt{"morning"} with \texttt{"evening"} in
  \texttt{"good\ morning"}.
\item
  Show that searching for \texttt{"dog"} in \texttt{"hotdog"} succeeds.
\item
  Replace all dashes in \texttt{"2025-09-10"} with slashes.
\item
  Write a rule with \texttt{*} that matches both \texttt{"cat"} and
  \texttt{"castle"}.
\item
  Create a wildcard pattern that matches any word ending in
  \texttt{"ing"}.
\item
  Explain why a failed search must return a special value (not just
  \texttt{0}).
\item
  Demonstrate how searching for \texttt{"a"} in \texttt{"banana"} should
  return multiple matches.
\item
  Describe how wildcards can be extended into full regular expressions.
\end{enumerate}

\subsection{3.4 L1 --- Regular Expressions in
Practice}\label{l1-regular-expressions-in-practice}

At the intermediate level, string pattern matching expands from simple
wildcards to regular expressions (regex). Regex provides a concise and
powerful way to describe patterns in text. It is widely used in
programming for searching, extracting, and validating structured data.

A regular expression is a sequence of symbols that defines a search
pattern. Unlike simple wildcards, regex can express detailed rules such
as ``find all numbers,'' ``match words ending in \texttt{ing},'' or
``validate an email address.'' Regex is supported in almost every major
programming language.

\subsubsection{Deep Dive}\label{deep-dive-32}

\paragraph{Core Symbols}\label{core-symbols}

\begin{itemize}
\tightlist
\item
  \texttt{.} matches any single character.
\item
  \texttt{*} means zero or more repetitions.
\item
  \texttt{+} means one or more repetitions.
\item
  \texttt{?} means zero or one occurrence.
\item
  \texttt{{[}abc{]}} matches one character from the set \texttt{a},
  \texttt{b}, or \texttt{c}.
\item
  \texttt{{[}0-9{]}} matches any digit.
\item
  \texttt{\^{}} matches the beginning of a string; \texttt{\$} matches
  the end.
\end{itemize}

\paragraph{Examples of Patterns}\label{examples-of-patterns}

\begin{itemize}
\tightlist
\item
  \texttt{{[}0-9{]}+} matches sequences of digits like \texttt{"123"}.
\item
  \texttt{\textbackslash{}w+} matches words made of letters, digits, and
  underscores.
\item
  \texttt{\textbackslash{}w+@\textbackslash{}w+\textbackslash{}.\textbackslash{}w+}
  matches simple email addresses.
\item
  \texttt{\^{}Hello} matches any string starting with \texttt{"Hello"}.
\end{itemize}

\paragraph{Greedy vs Lazy Matching}\label{greedy-vs-lazy-matching}

Regex quantifiers (\texttt{*}, \texttt{+}) are greedy by default: they
try to match as much as possible. Adding \texttt{?} makes them lazy,
matching the smallest possible part.

\begin{itemize}
\tightlist
\item
  Greedy: pattern \texttt{".*"} on \texttt{"abc"} → \texttt{"abc"}.
\item
  Lazy: pattern \texttt{".*?"} on \texttt{"abc"} → \texttt{"a"}.
\end{itemize}

\paragraph{Practical Uses}\label{practical-uses}

Regex is used for:

\begin{itemize}
\tightlist
\item
  Extracting structured data from logs.
\item
  Validating input like emails or phone numbers.
\item
  Searching and replacing complex text patterns.
\item
  Parsing configuration files or text protocols.
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-14}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}

\NormalTok{text }\OperatorTok{=} \StringTok{"Order 123, item 456, email: alice@example.com"}

\CommentTok{\# Match digits}
\NormalTok{numbers }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\PreprocessorTok{[0{-}9]}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Numbers:"}\NormalTok{, numbers)  }\CommentTok{\# [\textquotesingle{}123\textquotesingle{}, \textquotesingle{}456\textquotesingle{}]}

\CommentTok{\# Match words}
\NormalTok{words }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Words:"}\NormalTok{, words)}

\CommentTok{\# Match email addresses}
\NormalTok{emails }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{@}\DecValTok{\textbackslash{}w}\OperatorTok{+}\CharTok{\textbackslash{}.}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Emails:"}\NormalTok{, emails)}

\CommentTok{\# Anchors}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{bool}\NormalTok{(re.match(}\VerbatimStringTok{r"}\DecValTok{\^{}}\VerbatimStringTok{Order"}\NormalTok{, text)))   }\CommentTok{\# True (starts with Order)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{bool}\NormalTok{(re.search(}\VerbatimStringTok{r"com}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, text)))   }\CommentTok{\# True (ends with com)}

\CommentTok{\# Greedy vs lazy}
\NormalTok{sample }\OperatorTok{=} \StringTok{"\textless{}tag\textgreater{}content\textless{}/tag\textgreater{}\textless{}tag\textgreater{}other\textless{}/tag\textgreater{}"}
\NormalTok{greedy }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{*}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, sample)}
\NormalTok{lazy   }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{*?}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, sample)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy:"}\NormalTok{, greedy)  }\CommentTok{\# [\textquotesingle{}\textless{}tag\textgreater{}content\textless{}/tag\textgreater{}\textless{}tag\textgreater{}other\textless{}/tag\textgreater{}\textquotesingle{}]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Lazy:"}\NormalTok{, lazy)      }\CommentTok{\# [\textquotesingle{}\textless{}tag\textgreater{}content\textless{}/tag\textgreater{}\textquotesingle{}, \textquotesingle{}\textless{}tag\textgreater{}other\textless{}/tag\textgreater{}\textquotesingle{}]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-40}

Regex provides a compact language for describing patterns in text.
Without it, tasks like extracting emails, validating formats, or parsing
logs would require many lines of manual code. Regex is also standardized
across languages, so learning it once provides a universal tool.
However, careless use can create unreadable code or performance
problems, so disciplined practice is essential.

\subsubsection{Exercises}\label{exercises-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a regex that matches any sequence of digits.
\item
  Match all words in \texttt{"The\ quick\ brown\ fox"}.
\item
  Find all substrings that start with \texttt{"a"} and end with
  \texttt{"e"}.
\item
  Match an email address of the form \texttt{"name@domain.com"}.
\item
  Match any string that begins with \texttt{"http://"} or
  \texttt{"https://"}.
\item
  Extract all numbers from \texttt{"Invoice:\ \#123,\ \#456,\ \#789"}.
\item
  Demonstrate the difference between greedy and lazy matching with
  \texttt{"\textless{}tag\textgreater{}text\textless{}/tag\textgreater{}\textless{}tag\textgreater{}more\textless{}/tag\textgreater{}"}.
\item
  Write a regex to validate phone numbers like \texttt{"123-456-7890"}.
\item
  Explain why using regex to parse complex formats (like HTML) can be
  unreliable.
\item
  Create a regex that matches words containing exactly three vowels.
\end{enumerate}

\subsection{3.4 L2 --- Engines \&
Optimizations}\label{l2-engines-optimizations}

Regular expressions are powerful, but at scale their performance and
safety depend on the design of the engine. Advanced systems need regex
engines that are predictable, fast, and safe against denial-of-service
attacks. At this level, the focus is on internals, optimizations, and
production libraries.

Regex engines are implementations of pattern matching. Some use
backtracking, exploring many paths but risking exponential slowdowns.
Others use deterministic finite automata (DFA) or non-deterministic
automata (NFA), which guarantee linear runtime but reduce flexibility.
Modern libraries combine these with JIT compilation or SIMD acceleration
for speed. Production systems must choose the right engine for
correctness and reliability.

\subsubsection{Deep Dive}\label{deep-dive-33}

\paragraph{Engine Architectures}\label{engine-architectures}

Backtracking engines (Perl, Python, PCRE) allow advanced features like
backreferences but may run in exponential time on crafted inputs.
DFA/NFA engines (RE2, Rust, grep) convert patterns to automata that run
in linear time. JIT-based engines (Java, .NET) compile regex into
machine code for faster execution.

\paragraph{Greedy vs Lazy at Engine
Level}\label{greedy-vs-lazy-at-engine-level}

Quantifiers like \texttt{*} and \texttt{+} are greedy by default.
Engines track multiple possibilities with stacks or state machines. Lazy
quantifiers stop early, but the engine still evaluates alternatives
internally.

\paragraph{Performance Optimizations}\label{performance-optimizations}

Compiled regex patterns are faster than compiling on every use. Anchors
(\texttt{\^{}}, \texttt{\$}) and character classes (\texttt{{[}0-9{]}})
reduce search space. Avoiding pathological expressions like
\texttt{(a+)+} prevents catastrophic backtracking. SIMD instructions
scan text in chunks, speeding up low-level matching.

\paragraph{Advanced Libraries}\label{advanced-libraries}

\begin{itemize}
\tightlist
\item
  RE2 (Google): guarantees linear runtime, disallows constructs that
  cause exponential behavior.
\item
  Hyperscan (Intel): supports multi-pattern regex with SIMD
  acceleration, used in intrusion detection.
\item
  ICU: supports Unicode-aware regex and locale-sensitive operations.
\end{itemize}

\paragraph{System-Level Integration}\label{system-level-integration}

Regex engines are embedded in compilers, text editors, search tools, and
security systems. Multi-pattern matching is essential for scanning logs,
detecting spam, and monitoring network traffic.

\paragraph{Security Concerns}\label{security-concerns-1}

Poorly written regex can create ReDoS (Regex Denial of Service).
Attackers send inputs that trigger exponential backtracking, freezing
systems. For example, pattern \texttt{(a+)+b} against
\texttt{"aaaaaaa…"} takes excessive time. Safe libraries (RE2,
Hyperscan) prevent this by design.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1463}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4268}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4268}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Engine Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weaknesses
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Backtracking & Full regex features, flexible & Risk of exponential
runtime (ReDoS) \\
DFA/NFA & Linear runtime, predictable & No backreferences, fewer
features \\
JIT Hybrid & Very fast, compiled to machine code & Still vulnerable to
bad patterns \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-15}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re, time}

\CommentTok{\# Backtracking danger: catastrophic regex}
\NormalTok{pattern }\OperatorTok{=}\NormalTok{ re.}\BuiltInTok{compile}\NormalTok{(}\VerbatimStringTok{r"}\KeywordTok{(}\VerbatimStringTok{a}\OperatorTok{+}\KeywordTok{)}\OperatorTok{+}\VerbatimStringTok{b"}\NormalTok{)}
\NormalTok{text }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{25} \OperatorTok{+} \StringTok{"b"}   \CommentTok{\# works}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Match:"}\NormalTok{, }\BuiltInTok{bool}\NormalTok{(pattern.match(text)))}

\CommentTok{\# Problematic input: missing final \textquotesingle{}b\textquotesingle{}}
\NormalTok{text\_bad }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{25}     \CommentTok{\# exponential backtracking}
\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Bad match:"}\NormalTok{, }\BuiltInTok{bool}\NormalTok{(pattern.match(text\_bad)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{), }\StringTok{"seconds"}\NormalTok{)}

\CommentTok{\# Greedy vs lazy}
\NormalTok{html }\OperatorTok{=} \StringTok{"\textless{}tag\textgreater{}first\textless{}/tag\textgreater{}\textless{}tag\textgreater{}second\textless{}/tag\textgreater{}"}
\NormalTok{greedy }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{*}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, html)}
\NormalTok{lazy   }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{*?}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, html)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy:"}\NormalTok{, greedy)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Lazy:"}\NormalTok{, lazy)}

\CommentTok{\# Precompiled pattern efficiency}
\NormalTok{words }\OperatorTok{=} \StringTok{"error warning info debug error warning"}\NormalTok{.split()}
\NormalTok{p }\OperatorTok{=}\NormalTok{ re.}\BuiltInTok{compile}\NormalTok{(}\VerbatimStringTok{r"error}\ControlFlowTok{|}\VerbatimStringTok{warning"}\NormalTok{)}
\NormalTok{matches }\OperatorTok{=}\NormalTok{ [w }\ControlFlowTok{for}\NormalTok{ w }\KeywordTok{in}\NormalTok{ words }\ControlFlowTok{if}\NormalTok{ p.match(w)]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Matches:"}\NormalTok{, matches)}

\CommentTok{\# Unicode{-}aware regex}
\NormalTok{text }\OperatorTok{=} \StringTok{"café resume résumé"}
\NormalTok{utf8\_words }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}8 words:"}\NormalTok{, utf8\_words)}

\CommentTok{\# Simulated natural sorting with regex}
\NormalTok{files }\OperatorTok{=}\NormalTok{ [}\StringTok{"file1"}\NormalTok{, }\StringTok{"file10"}\NormalTok{, }\StringTok{"file2"}\NormalTok{]}
\NormalTok{splitter }\OperatorTok{=}\NormalTok{ re.}\BuiltInTok{compile}\NormalTok{(}\VerbatimStringTok{r\textquotesingle{}}\KeywordTok{(}\DecValTok{\textbackslash{}d}\OperatorTok{+}\KeywordTok{)}\VerbatimStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{files.sort(key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: [}\BuiltInTok{int}\NormalTok{(n) }\ControlFlowTok{if}\NormalTok{ n.isdigit() }\ControlFlowTok{else}\NormalTok{ n }\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ splitter.split(x)])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Natural sort:"}\NormalTok{, files)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-41}

Regex is a double-edged sword. It compresses complex logic into concise
expressions, but the wrong engine or pattern can cripple systems.
Backtracking engines offer flexibility but can be exploited. DFA-based
engines guarantee performance but restrict features. Production
libraries like RE2 and Hyperscan provide safe, optimized solutions for
real workloads. Security, speed, and predictability all depend on
understanding regex internals.

\subsubsection{Exercises}\label{exercises-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain the difference between greedy and lazy matching.
\item
  Show why \texttt{(a+)+b} can cause problems on inputs like
  \texttt{"aaaaaa"}.
\item
  Compare backtracking and DFA approaches to regex.
\item
  Write a regex to match numbers at the start of a string
  (\texttt{\^{}123}).
\item
  Explain why precompiling a regex can improve performance.
\item
  Describe how RE2 avoids catastrophic backtracking.
\item
  Show why regex should not be used for parsing HTML.
\item
  Design a regex for matching IPv4 addresses and explain performance
  concerns.
\item
  Compare the runtime of regex matching with and without SIMD
  acceleration in theory.
\item
  Propose a strategy to defend against Regex Denial of Service in a
  production system.
\end{enumerate}

\section{3.5 Applications}\label{applications}

\subsection{3.5 L0 --- Everyday Uses}\label{l0-everyday-uses}

Strings are used in almost every program. Beginners usually encounter
them when printing messages, saving text to files, or checking simple
inputs. These small applications show how strings connect code to
real-world tasks.

Everyday string applications include formatting output, working with
text files, validating simple input, and combining small pieces of data
into readable results. These tasks look simple but form the foundation
of real-world software, from command-line tools to web apps.

\subsubsection{Deep Dive}\label{deep-dive-34}

\paragraph{Text Formatting}\label{text-formatting}

Programs must often display results with both words and numbers.
Concatenation creates readable sentences: \texttt{"Hello,\ "\ +\ name}.
Many languages also support templates: \texttt{"Hello,\ \{name\}"}.

\paragraph{File Reading Basics}\label{file-reading-basics}

A file is just text stored on disk. Reading one line returns a string.
Splitting turns the line into words. Writing strings saves output.
Example: reading \texttt{"Alice\ 25"} and splitting it into a name and
an age.

\paragraph{Input Validation (Simple)}\label{input-validation-simple}

Not all inputs are valid. Checking that a string is not empty prevents
errors. Minimum length ensures requirements are met (like passwords).
Character checks confirm only letters or digits are entered.

\paragraph{Basic Data Handling}\label{basic-data-handling}

Strings often need simple processing: counting characters, trimming
spaces, or extracting parts. \texttt{"John\ Doe"} can be split into
\texttt{"John"} and \texttt{"Doe"}. Small pieces of text are often
joined into a readable result.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Everyday Task & Example Input & Example Output \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Greeting & \texttt{"Alice"} & \texttt{"Hello,\ Alice!"} \\
Trim spaces & \texttt{"\ hello\ "} & \texttt{"hello"} \\
Count characters & \texttt{"banana"} & \texttt{6} \\
Save and load message & \texttt{"Note:\ call\ Bob"} & Written and
reloaded \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-16}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Formatting text}
\NormalTok{name }\OperatorTok{=} \StringTok{"Alice"}
\NormalTok{age }\OperatorTok{=} \DecValTok{25}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Hello, }\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{! You are }\SpecialCharTok{\{}\NormalTok{age}\SpecialCharTok{\}}\SpecialStringTok{ years old."}\NormalTok{)}

\CommentTok{\# File basics (write then read)}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"note.txt"}\NormalTok{, }\StringTok{"w"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    f.write(}\StringTok{"Remember to buy milk"}\NormalTok{)}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"note.txt"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    content }\OperatorTok{=}\NormalTok{ f.read()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"File content:"}\NormalTok{, content)}

\CommentTok{\# Input validation}
\NormalTok{user\_input }\OperatorTok{=} \StringTok{"   hello123   "}
\NormalTok{cleaned }\OperatorTok{=}\NormalTok{ user\_input.strip()}
\ControlFlowTok{if}\NormalTok{ cleaned }\KeywordTok{and}\NormalTok{ cleaned.isalnum():}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Valid input:"}\NormalTok{, cleaned)}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Invalid input"}\NormalTok{)}

\CommentTok{\# Data handling}
\NormalTok{full\_name }\OperatorTok{=} \StringTok{"John Doe"}
\NormalTok{first, last }\OperatorTok{=}\NormalTok{ full\_name.split()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"First name:"}\NormalTok{, first)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Last name:"}\NormalTok{, last)}

\CommentTok{\# Joining pieces}
\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"data"}\NormalTok{, }\StringTok{"structures"}\NormalTok{, }\StringTok{"are"}\NormalTok{, }\StringTok{"fun"}\NormalTok{]}
\NormalTok{sentence }\OperatorTok{=} \StringTok{" "}\NormalTok{.join(words)}
\BuiltInTok{print}\NormalTok{(sentence)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-42}

Everyday string handling bridges user interaction and program logic.
Without formatting, programs would only output raw numbers. Without
validation, programs would accept broken or unsafe input. Without file
reading and writing, no program could save notes or logs. These basics
show how strings connect software to people and data.

\subsubsection{Exercises}\label{exercises-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print \texttt{"Hello,\ NAME!"} using a variable for the name.
\item
  Count the number of characters in \texttt{"banana"}.
\item
  Save the string \texttt{"study\ algorithms"} to a file, then read it
  back.
\item
  Remove extra spaces from \texttt{"\ \ \ world\ \ \ "} and print the
  cleaned string.
\item
  Check if \texttt{"abc123"} contains only letters and digits.
\item
  Split \texttt{"Alice\ Johnson"} into first and last names.
\item
  Join \texttt{{[}"red",\ "green",\ "blue"{]}} into
  \texttt{"red,green,blue"}.
\item
  Create a greeting that includes both a name and an age.
\item
  Validate that a password is at least 8 characters long.
\item
  Write a program that asks for a note, saves it to a file, and then
  reloads and prints it.
\end{enumerate}

\subsection{3.5 L1 --- Engineering
Contexts}\label{l1-engineering-contexts}

As programs grow, string handling shifts from small tasks to structured
data and system integration. Strings become the glue for configurations,
logs, and filtering. These applications demand clarity, consistency, and
efficiency.

\subsubsection{Overview/Definition}\label{overviewdefinition-3}

Strings in engineering contexts are used to represent structured
formats, record system activity, and enable searching and filtering.
They provide a common medium for both humans and machines, balancing
readability with machine-parsable structure.

\subsubsection{Deep Dive}\label{deep-dive-35}

\paragraph{Configuration Formats}\label{configuration-formats}

Configuration files define how systems run. Common formats like JSON,
YAML, or INI are all string-based. Parsing these strings gives
structured values. For example, \texttt{"timeout":\ 30} in JSON is a
string representation of a setting, later converted into a number.

\paragraph{Logs and Monitoring}\label{logs-and-monitoring}

Systems record events as text logs. A log line might be
\texttt{"2025-09-10\ 12:00:01\ ERROR\ Connection\ failed"}. These
strings are later parsed for monitoring and debugging. Consistency in
string format is critical.

\paragraph{Searching and Filtering}\label{searching-and-filtering}

Large systems must sift through text quickly. Searching log files for
\texttt{"ERROR"} highlights problems. Filtering configuration values
based on keywords is another common task. Case-insensitive matching or
regex-based extraction makes this robust.

\paragraph{Data Exchange}\label{data-exchange}

APIs and network protocols often transmit structured text. JSON payloads
are strings at the boundary between systems. Converting between string
representation and structured data is a common responsibility of
engineering code.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1688}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4416}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3896}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Context
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example String
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Configuration & \texttt{\{"host":"localhost","port":8080\}} & System
settings \\
Log line & \texttt{2025-09-10\ ERROR\ Disk\ full} & Monitoring \&
debugging \\
Search/filter & Find \texttt{"ERROR"} in a log file & Identify issues \\
API payload & \texttt{\{"user":"alice","id":42\}} & Data exchange
between services \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-17}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ json}
\ImportTok{import}\NormalTok{ re}

\CommentTok{\# Configuration parsing (JSON as string)}
\NormalTok{config\_str }\OperatorTok{=} \StringTok{\textquotesingle{}\{"host":"localhost","port":8080\}\textquotesingle{}}
\NormalTok{config }\OperatorTok{=}\NormalTok{ json.loads(config\_str)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Host:"}\NormalTok{, config[}\StringTok{"host"}\NormalTok{], }\StringTok{"Port:"}\NormalTok{, config[}\StringTok{"port"}\NormalTok{])}

\CommentTok{\# Logging and filtering}
\NormalTok{logs }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"2025{-}09{-}10 12:00:01 INFO  Server started"}\NormalTok{,}
    \StringTok{"2025{-}09{-}10 12:01:22 ERROR Connection failed"}\NormalTok{,}
    \StringTok{"2025{-}09{-}10 12:02:45 WARN  Disk almost full"}
\NormalTok{]}
\NormalTok{errors }\OperatorTok{=}\NormalTok{ [line }\ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in}\NormalTok{ logs }\ControlFlowTok{if} \StringTok{"ERROR"} \KeywordTok{in}\NormalTok{ line]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Error logs:"}\NormalTok{, errors)}

\CommentTok{\# Case{-}insensitive search}
\NormalTok{query }\OperatorTok{=} \StringTok{"warn"}
\NormalTok{matches }\OperatorTok{=}\NormalTok{ [line }\ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in}\NormalTok{ logs }\ControlFlowTok{if}\NormalTok{ query.lower() }\KeywordTok{in}\NormalTok{ line.lower()]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Warnings:"}\NormalTok{, matches)}

\CommentTok{\# Extracting structured data from logs with regex}
\NormalTok{pattern }\OperatorTok{=}\NormalTok{ re.}\BuiltInTok{compile}\NormalTok{(}\VerbatimStringTok{r"}\KeywordTok{(}\DecValTok{\textbackslash{}d}\OperatorTok{\{4\}}\VerbatimStringTok{{-}}\DecValTok{\textbackslash{}d}\OperatorTok{\{2\}}\VerbatimStringTok{{-}}\DecValTok{\textbackslash{}d}\OperatorTok{\{2\}}\KeywordTok{)}\DecValTok{\textbackslash{}s}\OperatorTok{+}\DecValTok{\textbackslash{}S}\OperatorTok{+}\DecValTok{\textbackslash{}s}\OperatorTok{+}\KeywordTok{(}\VerbatimStringTok{ERROR}\ControlFlowTok{|}\VerbatimStringTok{WARN}\ControlFlowTok{|}\VerbatimStringTok{INFO}\KeywordTok{)}\VerbatimStringTok{"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ log }\KeywordTok{in}\NormalTok{ logs:}
\NormalTok{    match }\OperatorTok{=}\NormalTok{ pattern.search(log)}
    \ControlFlowTok{if}\NormalTok{ match:}
\NormalTok{        date, level }\OperatorTok{=}\NormalTok{ match.groups()}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Date: }\SpecialCharTok{\{}\NormalTok{date}\SpecialCharTok{\}}\SpecialStringTok{, Level: }\SpecialCharTok{\{}\NormalTok{level}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-43}

Strings in configurations allow flexible system tuning without
recompiling. Logs provide the primary record of system behavior and
failures. Searching and filtering turn raw logs into actionable
insights. APIs and protocols rely on structured strings to communicate.
Mastery of these applications makes code more reliable and systems more
transparent.

\subsubsection{Exercises}\label{exercises-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Parse a JSON string that contains a username and print the value.
\item
  Find all log lines that contain \texttt{"ERROR"}.
\item
  Perform a case-insensitive search for \texttt{"warn"} in a list of log
  lines.
\item
  Extract the date from a log entry of the form
  \texttt{"YYYY-MM-DD\ ..."}.
\item
  Parse a configuration string to extract host and port values.
\item
  Write a filter that selects only \texttt{"INFO"} level log lines.
\item
  Convert a dictionary of settings into a JSON string.
\item
  Explain why consistent formatting in logs is important for monitoring
  systems.
\item
  Create a regex that extracts both the date and log level from a log
  entry.
\item
  Design a simple log query: given a list of log strings and a keyword,
  return all lines that match the keyword.
\end{enumerate}

\subsection{3.5 L2 --- Large-Scale
Systems}\label{l2-large-scale-systems}

When systems handle massive amounts of text, strings are no longer just
for messages, logs, or configs. They become the raw material of search
engines, data compression, natural language processing, and security
enforcement. At this scale, efficiency, correctness, and safety are
critical.

Strings in large-scale systems must be stored compactly, searched
efficiently, processed for meaning, and sanitized for safety.
Specialized algorithms and libraries turn raw text into structured,
searchable, and secure data.

\subsubsection{Deep Dive}\label{deep-dive-36}

\paragraph{Search Engines}\label{search-engines}

Full-text search systems use inverted indexes: instead of storing text
linearly, they map each word to the documents it appears in. Searching
\texttt{"cat"} means looking up \texttt{"cat"} in the index rather than
scanning every file. Ranking algorithms (like TF-IDF or BM25) then score
relevance.

\paragraph{Data Compression}\label{data-compression}

Strings consume storage. Compression algorithms like Huffman coding,
gzip, or dictionary-based schemes reduce size. For example,
\texttt{"banana\ banana\ banana"} can be compressed by storing
\texttt{"banana"} once and pointing to it three times. Compression
reduces storage costs and speeds up network transfer.

\paragraph{Natural Language Processing
(NLP)}\label{natural-language-processing-nlp}

Raw strings must be tokenized (split into words), normalized (case
folding, stemming), and sometimes embedded into vectors.
\texttt{"running"} → \texttt{"run"}. Preprocessing ensures machine
learning models treat text consistently.

\paragraph{Security}\label{security}

Strings are also attack vectors. SQL injection, XSS, and command
injection happen when untrusted strings are executed as code. Escaping
or sanitizing input prevents this. Unicode tricks (confusables, hidden
nulls) are used in phishing and bypass attacks. Secure systems enforce
normalization and validation before processing.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1733}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4933}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Domain
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
String Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Techniques
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Search engine & Find text in billions of docs & Inverted index,
ranking \\
Compression & Reduce text storage and transfer cost & Huffman, gzip,
dictionary \\
NLP & Prepare text for analysis & Tokenization, stemming \\
Security & Prevent malicious string usage & Escaping, validation \\
\end{longtable}

\subsubsection{Worked Example (Python)}\label{worked-example-python-18}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}
\ImportTok{import}\NormalTok{ zlib}
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ defaultdict}

\CommentTok{\# {-}{-}{-} Search Engine Simulation {-}{-}{-}}
\NormalTok{documents }\OperatorTok{=}\NormalTok{ \{}
    \DecValTok{1}\NormalTok{: }\StringTok{"the quick brown fox"}\NormalTok{,}
    \DecValTok{2}\NormalTok{: }\StringTok{"the lazy dog"}\NormalTok{,}
    \DecValTok{3}\NormalTok{: }\StringTok{"the fox jumped over the dog"}
\NormalTok{\}}

\CommentTok{\# Build inverted index}
\NormalTok{index }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{list}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ doc\_id, text }\KeywordTok{in}\NormalTok{ documents.items():}
    \ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ text.split():}
\NormalTok{        index[word].append(doc\_id)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Index for \textquotesingle{}fox\textquotesingle{}:"}\NormalTok{, index[}\StringTok{"fox"}\NormalTok{])  }\CommentTok{\# [1, 3]}

\CommentTok{\# {-}{-}{-} Compression Example {-}{-}{-}}
\NormalTok{text }\OperatorTok{=} \StringTok{"banana banana banana"}
\NormalTok{compressed }\OperatorTok{=}\NormalTok{ zlib.compress(text.encode(}\StringTok{"utf{-}8"}\NormalTok{))}
\NormalTok{decompressed }\OperatorTok{=}\NormalTok{ zlib.decompress(compressed).decode(}\StringTok{"utf{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(text), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Compressed:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(compressed), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Decompressed:"}\NormalTok{, decompressed)}

\CommentTok{\# {-}{-}{-} NLP Tokenization \& Normalization {-}{-}{-}}
\NormalTok{raw }\OperatorTok{=} \StringTok{"Running runs runner"}
\NormalTok{tokens }\OperatorTok{=}\NormalTok{ raw.lower().split()}
\NormalTok{stems }\OperatorTok{=}\NormalTok{ [re.sub(}\VerbatimStringTok{r"}\KeywordTok{(}\VerbatimStringTok{ing}\ControlFlowTok{|}\VerbatimStringTok{s}\ControlFlowTok{|}\VerbatimStringTok{er}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, }\StringTok{""}\NormalTok{, t) }\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in}\NormalTok{ tokens]  }\CommentTok{\# naive stemming}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Tokens:"}\NormalTok{, tokens)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Stems:"}\NormalTok{, stems)}

\CommentTok{\# {-}{-}{-} Security Example {-}{-}{-}}
\NormalTok{unsafe\_input }\OperatorTok{=} \StringTok{"\textquotesingle{}; DROP TABLE users; {-}{-}"}
\NormalTok{safe\_query }\OperatorTok{=} \StringTok{"SELECT * FROM accounts WHERE name = ?"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Unsafe input:"}\NormalTok{, unsafe\_input)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Safe query template:"}\NormalTok{, safe\_query)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-44}

Large-scale string handling powers web search, messaging apps,
compression in files and networks, and text-based AI systems. Without
inverted indexes, search engines would be unusably slow. Without
compression, storage and bandwidth would be wasteful. Without
normalization, machine learning models would misinterpret text. Without
security checks, attackers could exploit untrusted strings to damage
systems. Strings at scale are the backbone of modern computing.

\subsubsection{Exercises}\label{exercises-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain how an inverted index speeds up searching compared to scanning
  all documents.
\item
  Show how compression reduces storage for repetitive strings like
  \texttt{"abc\ abc\ abc"}.
\item
  Tokenize \texttt{"Cats\ are\ running"} into lowercase words.
\item
  Normalize \texttt{"Running"} into its root form \texttt{"run"}.
\item
  Describe why \texttt{"café"} and \texttt{"cafe"} should be normalized
  before search.
\item
  Give an example of SQL injection using a string input.
\item
  Explain how escaping special characters prevents injection attacks.
\item
  Compare the storage savings between raw and compressed text for a long
  repeated string.
\item
  Propose how to detect Unicode confusables in user names.
\item
  Design a small experiment to measure search speed with and without an
  inverted index.
\end{enumerate}

\section{Deep Dive}\label{deep-dive-37}

\subsection{3.1 Representation}\label{representation-2}

\begin{itemize}
\item
  L0:

  \begin{itemize}
  \tightlist
  \item
    Immutability in everyday languages (intuitive).
  \item
    Indexing and slicing basics.
  \item
    Concatenation and repetition.
  \item
    ASCII vs Unicode (conceptual).
  \end{itemize}
\item
  L1:

  \begin{itemize}
  \tightlist
  \item
    Immutability and its performance costs.
  \item
    String interning and memory reuse.
  \item
    Encoding differences: UTF-8, UTF-16, UTF-32.
  \item
    Slicing behavior across languages (Python vs Java vs C++).
  \item
    Efficient concatenation (\texttt{join}, builders, buffers).
  \end{itemize}
\item
  L2:

  \begin{itemize}
  \tightlist
  \item
    Language-specific internals: C \texttt{char*}, C++
    \texttt{std::string} + SSO, Python PEP 393, Java \texttt{String}, Go
    slices, Rust \texttt{String}/\texttt{\&str}.
  \item
    OS-level considerations: heap vs stack, fragmentation, system call
    boundaries.
  \item
    Hardware-level: cache alignment, SIMD optimization in libc.
  \item
    Unicode normalization (NFC, NFD, case folding).
  \item
    Security issues: confusables, null injection.
  \item
    ICU, RE2, Hyperscan as production-grade libraries.
  \end{itemize}
\end{itemize}

\subsection{3.2 Operations}\label{operations-1}

\begin{itemize}
\item
  L0:

  \begin{itemize}
  \tightlist
  \item
    Case conversion (upper, lower).
  \item
    Trimming whitespace.
  \item
    Basic search (\texttt{find}, \texttt{in}).
  \item
    Splitting and joining.
  \item
    Simple string formatting.
  \end{itemize}
\item
  L1:

  \begin{itemize}
  \tightlist
  \item
    Efficient searching (\texttt{startswith}, \texttt{endswith}).
  \item
    Counting substrings and replacing.
  \item
    Regular expression basics (\texttt{.}, \texttt{*}, \texttt{+},
    \texttt{{[}{]}}).
  \item
    Parsing structured text (CSV, JSON).
  \item
    Performance pitfalls (naive concatenation, multiple scans).
  \end{itemize}
\item
  L2:

  \begin{itemize}
  \tightlist
  \item
    String search algorithms: Naive, KMP, Boyer--Moore.
  \item
    Regex engine internals (backtracking vs DFA/NFA).
  \item
    Tokenization in compilers (lexical analysis with automata).
  \item
    Unicode-aware searching and case folding.
  \item
    System boundary handling (sockets, files, encodings).
  \item
    SIMD-accelerated substring search.
  \end{itemize}
\end{itemize}

\subsection{3.3 Comparison}\label{comparison-1}

\begin{itemize}
\item
  L0:

  \begin{itemize}
  \tightlist
  \item
    Equality checks (case-sensitive, whitespace).
  \item
    Lexicographic ordering.
  \item
    Sorting by dictionary order.
  \end{itemize}
\item
  L1:

  \begin{itemize}
  \tightlist
  \item
    Case-insensitive comparisons.
  \item
    Locale-aware collation (German vs Swedish).
  \item
    Natural sorting with numbers (\texttt{file2} \textless{}
    \texttt{file10}).
  \item
    Database collations (CI/CS, AI/AS).
  \end{itemize}
\item
  L2:

  \begin{itemize}
  \tightlist
  \item
    Unicode normalization for equality.
  \item
    Case folding vs lowercase.
  \item
    ICU collation internals.
  \item
    System-level optimizations (memcmp, short-circuit).
  \item
    Cache-aware comparison for large text.
  \item
    Detecting and handling Unicode confusables.
  \end{itemize}
\end{itemize}

\subsection{3.4 Patterns}\label{patterns-1}

\begin{itemize}
\item
  L0:

  \begin{itemize}
  \tightlist
  \item
    Simple substring search.
  \item
    Replace operations.
  \item
    Wildcards (\texttt{*}, \texttt{?}) as intuitive pattern matching.
  \end{itemize}
\item
  L1:

  \begin{itemize}
  \tightlist
  \item
    Regex syntax: quantifiers, anchors, character classes.
  \item
    Greedy vs lazy matching.
  \item
    Regex for input validation (emails, phones).
  \item
    Extracting structured data from logs.
  \end{itemize}
\item
  L2:

  \begin{itemize}
  \tightlist
  \item
    Regex engine architectures: backtracking, DFA/NFA, hybrid JIT.
  \item
    Performance optimizations (precompiled regex, anchors, SIMD).
  \item
    Production libraries: RE2, Hyperscan, ICU.
  \item
    System integration: compilers, search tools, IDS.
  \item
    Security: catastrophic backtracking, ReDoS.
  \end{itemize}
\end{itemize}

\subsection{3.5 Applications}\label{applications-1}

\begin{itemize}
\item
  L0:

  \begin{itemize}
  \tightlist
  \item
    Simple greetings and message formatting.
  \item
    Reading and writing small text files.
  \item
    Input validation (non-empty, length, alphanumeric).
  \item
    Joining/extracting small strings.
  \end{itemize}
\item
  L1:

  \begin{itemize}
  \tightlist
  \item
    Configurations (INI, JSON, YAML).
  \item
    Logs and monitoring pipelines.
  \item
    Searching and filtering text data.
  \item
    APIs and network payloads as strings.
  \end{itemize}
\item
  L2:

  \begin{itemize}
  \tightlist
  \item
    Search engines with inverted indexes.
  \item
    Text compression (Huffman, gzip, dictionary).
  \item
    Natural language preprocessing (tokenization, stemming).
  \item
    Security concerns: injections, normalization, Unicode attacks.
  \item
    High-scale performance engineering for string-heavy systems.
  \end{itemize}
\end{itemize}

\section{LAB}\label{lab-1}

\subsection{3.1 Representation}\label{representation-3}

\begin{itemize}
\item
  LAB 1: String Encodings in Practice

  \begin{itemize}
  \tightlist
  \item
    Represent the same text (\texttt{"A😊"}) in UTF-8, UTF-16, and
    UTF-32.
  \item
    Measure memory size in each encoding.
  \item
    Show how slicing works differently in each.
  \end{itemize}
\item
  LAB 2: String Interning and Memory Efficiency

  \begin{itemize}
  \tightlist
  \item
    Create repeated strings with and without interning.
  \item
    Measure memory usage.
  \item
    Discuss practical use cases (symbol tables, compilers).
  \end{itemize}
\item
  LAB 3: Kernel-Level String Operations

  \begin{itemize}
  \tightlist
  \item
    Explore C \texttt{char*} strings with \texttt{strlen} and buffer
    overflows.
  \item
    Compare to higher-level language safety (Python, Java).
  \end{itemize}
\end{itemize}

\subsection{3.2 Operations}\label{operations-2}

\begin{itemize}
\item
  LAB 4: Concatenation Cost Benchmark

  \begin{itemize}
  \tightlist
  \item
    Compare loop concatenation (\texttt{+}) vs buffered approaches
    (\texttt{join}, builders).
  \item
    Measure time for small, medium, and large inputs.
  \end{itemize}
\item
  LAB 5: Regex Basics Explorer

  \begin{itemize}
  \tightlist
  \item
    Implement small regex patterns (\texttt{{[}0-9{]}+},
    \texttt{\textbackslash{}w+}) across different languages.
  \item
    Show differences in syntax and output.
  \end{itemize}
\item
  LAB 6: CSV Parsing Pitfalls

  \begin{itemize}
  \tightlist
  \item
    Try splitting \texttt{"apple,"banana,grape",cherry"} naively.
  \item
    Show incorrect results.
  \item
    Contrast with proper CSV parsers.
  \end{itemize}
\end{itemize}

\subsection{3.3 Comparison}\label{comparison-2}

\begin{itemize}
\item
  LAB 7: Locale-Aware Sorting

  \begin{itemize}
  \tightlist
  \item
    Sort a list with \texttt{"ä"}, \texttt{"z"}, \texttt{"apple"}.
  \item
    Show German vs Swedish locale differences.
  \end{itemize}
\item
  LAB 8: Unicode Normalization Demo

  \begin{itemize}
  \tightlist
  \item
    Compare \texttt{"café"} vs \texttt{"cafe\textbackslash{}u0301"}.
  \item
    Show why normalization (NFC, NFD) matters for equality.
  \end{itemize}
\item
  LAB 9: Confusable Characters Security Check

  \begin{itemize}
  \tightlist
  \item
    Compare Latin \texttt{"a"} vs Cyrillic \texttt{"а"}.
  \item
    Build a detector for confusable Unicode characters.
  \end{itemize}
\end{itemize}

\subsection{3.4 Patterns}\label{patterns-2}

\begin{itemize}
\item
  LAB 10: Substring Search Algorithms

  \begin{itemize}
  \tightlist
  \item
    Implement Naive, KMP, and Boyer--Moore search.
  \item
    Benchmark them on long repetitive text.
  \end{itemize}
\item
  LAB 11: Regex Engine Catastrophe

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{(a+)+b} on long input without a \texttt{b}.
  \item
    Measure how exponential backtracking freezes the engine.
  \item
    Contrast with RE2 (linear-time safe).
  \end{itemize}
\item
  LAB 12: Greedy vs Lazy Matching

  \begin{itemize}
  \tightlist
  \item
    Run regex
    \texttt{"\textless{}tag\textgreater{}.*\textless{}/tag\textgreater{}"}
    vs
    \texttt{"\textless{}tag\textgreater{}.*?\textless{}/tag\textgreater{}"}
    on HTML-like text.
  \item
    Show difference between greedy and lazy quantifiers.
  \end{itemize}
\end{itemize}

\subsection{3.5 Applications}\label{applications-2}

\begin{itemize}
\item
  LAB 13: Build a Mini Inverted Index

  \begin{itemize}
  \tightlist
  \item
    Take a small set of documents.
  \item
    Build an index mapping words to documents.
  \item
    Query the index for \texttt{"fox"}, \texttt{"dog"}, etc.
  \end{itemize}
\item
  LAB 14: Compression of Repetitive Strings

  \begin{itemize}
  \tightlist
  \item
    Implement a simple dictionary compressor.
  \item
    Compare original vs compressed sizes for
    \texttt{"banana\ banana\ banana"}.
  \end{itemize}
\item
  LAB 15: Injection Attack Simulation

  \begin{itemize}
  \tightlist
  \item
    Build a simple query string with unsanitized input.
  \item
    Show how malicious input
    (\texttt{"\textquotesingle{};\ DROP\ TABLE\ users;\ -\/-"}) alters
    behavior.
  \item
    Fix it with escaping or parameter binding.
  \end{itemize}
\end{itemize}

\section{LAB 1: String Encodings in
Practice}\label{lab-1-string-encodings-in-practice}

\subsection{Goal}\label{goal}

Understand how different encodings (UTF-8, UTF-16, UTF-32) represent the
same text. Learn how memory size changes depending on encoding, and why
this matters for storage and transmission.

\subsection{Setup}\label{setup}

You can use any language that provides access to string encodings
(Python, Go, C, Java). Python is a good choice because its
\texttt{encode()} method supports multiple encodings.

Text sample:

\begin{verbatim}
"A😊"
\end{verbatim}

This combines a simple ASCII character (\texttt{A}) with a multi-byte
emoji (\texttt{😊}).

\subsection{Step-by-Step}\label{step-by-step}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Represent the string in memory

  \begin{itemize}
  \tightlist
  \item
    Create the string \texttt{"A😊"}.
  \item
    Print its length in characters.
  \item
    Print its length in bytes when encoded in UTF-8, UTF-16, and UTF-32.
  \end{itemize}
\item
  Inspect the raw byte sequences

  \begin{itemize}
  \tightlist
  \item
    Encode the string in each format.
  \item
    Print the resulting byte arrays.
  \item
    Write them in hexadecimal for clarity.
  \end{itemize}
\item
  Compare differences

  \begin{itemize}
  \tightlist
  \item
    Notice that UTF-8 is variable-length (1 byte for \texttt{A}, 4 bytes
    for \texttt{😊}).
  \item
    UTF-16 uses 2 bytes for \texttt{A}, 4 bytes for \texttt{😊}
    (surrogate pair).
  \item
    UTF-32 uses 4 bytes for both.
  \end{itemize}
\item
  Reflect on trade-offs

  \begin{itemize}
  \tightlist
  \item
    UTF-8 is compact for ASCII-heavy text (English).
  \item
    UTF-16 is a balance for languages with many non-ASCII characters.
  \item
    UTF-32 is simple but wastes space.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OperatorTok{=} \StringTok{"A😊"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Characters:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(text))}

\CommentTok{\# UTF{-}8}
\NormalTok{utf8\_bytes }\OperatorTok{=}\NormalTok{ text.encode(}\StringTok{"utf{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}8 length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(utf8\_bytes), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}8 bytes:"}\NormalTok{, utf8\_bytes.}\BuiltInTok{hex}\NormalTok{())}

\CommentTok{\# UTF{-}16}
\NormalTok{utf16\_bytes }\OperatorTok{=}\NormalTok{ text.encode(}\StringTok{"utf{-}16"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}16 length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(utf16\_bytes), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}16 bytes:"}\NormalTok{, utf16\_bytes.}\BuiltInTok{hex}\NormalTok{())}

\CommentTok{\# UTF{-}32}
\NormalTok{utf32\_bytes }\OperatorTok{=}\NormalTok{ text.encode(}\StringTok{"utf{-}32"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}32 length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(utf32\_bytes), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"UTF{-}32 bytes:"}\NormalTok{, utf32\_bytes.}\BuiltInTok{hex}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Expected output (approximate, may differ by platform endianness):

\begin{verbatim}
Characters: 2
UTF-8 length: 5 bytes
UTF-8 bytes: 41f09f988a
UTF-16 length: 6 bytes
UTF-16 bytes: fffe41003dd89a
UTF-32 length: 8 bytes
UTF-32 bytes: fffe0000410001f60a
\end{verbatim}

\subsection{Expected Results}\label{expected-results}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Encoding & Bytes for \texttt{"A"} & Bytes for \texttt{"😊"} & Total
Bytes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
UTF-8 & 1 & 4 & 5 \\
UTF-16 & 2 & 4 (surrogate) & 6 \\
UTF-32 & 4 & 4 & 8 \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-45}

\begin{itemize}
\tightlist
\item
  Encoding affects storage cost and speed.
\item
  A file with millions of ASCII characters will be 2× bigger in UTF-16
  than UTF-8.
\item
  Emojis or non-Latin scripts can make UTF-8 larger than UTF-16.
\item
  Choosing the wrong encoding can waste storage, slow down transmission,
  or even corrupt data if misinterpreted.
\end{itemize}

\subsection{Exercises}\label{exercises-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Encode the string \texttt{"Hello"} in UTF-8, UTF-16, UTF-32. Compare
  sizes.
\item
  Repeat with \texttt{"こんにちは"} (Japanese). Which encoding is most
  efficient?
\item
  Inspect the raw bytes of \texttt{"💡"} and explain why it requires
  surrogate pairs in UTF-16.
\item
  Write a program that takes any string and reports its size in multiple
  encodings.
\item
  Explain why UTF-32 is rarely used for storage despite its simplicity.
\end{enumerate}

\section{LAB 2: String Interning and Memory
Efficiency}\label{lab-2-string-interning-and-memory-efficiency}

\subsection{Goal}\label{goal-1}

Understand how string interning reduces memory usage by reusing
identical string objects. Learn when interning happens automatically,
when it must be explicit, and why it matters in systems with many
repeated strings.

\subsection{Setup}\label{setup-1}

Use a language that supports string interning (Python, Java, C\#, Go
with symbol tables, etc.). We'll use Python because it demonstrates both
automatic interning (for short literals) and manual interning
(\texttt{sys.intern()}).

Test dataset: a large list of repeated keys (e.g., \texttt{"user123"},
\texttt{"error"}, \texttt{"warning"}).

\subsection{Step-by-Step}\label{step-by-step-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Automatic Interning of Literals

  \begin{itemize}
  \tightlist
  \item
    Define two identical string literals.
  \item
    Compare them with \texttt{==} (value equality) and \texttt{is}
    (identity).
  \item
    Observe that they may point to the same object in memory.
  \end{itemize}
\item
  Non-Interned Strings

  \begin{itemize}
  \tightlist
  \item
    Build identical strings at runtime (e.g., \texttt{"ab"\ +\ "cd"}).
  \item
    Check identity with \texttt{is}.
  \item
    They may not be the same object, even if values match.
  \end{itemize}
\item
  Explicit Interning

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{sys.intern()} on repeated dynamic strings.
  \item
    Compare memory addresses before and after.
  \end{itemize}
\item
  Memory Benchmark

  \begin{itemize}
  \tightlist
  \item
    Create a list of repeated values (e.g., 100k \texttt{"error"}
    strings).
  \item
    Measure memory usage with and without interning.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Interning saves memory by keeping one copy of repeated values.
  \item
    It speeds up dictionary lookups (symbols, keywords, tokens).
  \item
    But it increases memory if used on unique or rarely repeated
    strings.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sys}

\CommentTok{\# 1. Automatic interning}
\NormalTok{a }\OperatorTok{=} \StringTok{"hello"}
\NormalTok{b }\OperatorTok{=} \StringTok{"hello"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a == b:"}\NormalTok{, a }\OperatorTok{==}\NormalTok{ b)   }\CommentTok{\# True (values equal)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a is b:"}\NormalTok{, a }\KeywordTok{is}\NormalTok{ b)   }\CommentTok{\# True (same object due to interning)}

\CommentTok{\# 2. Non{-}interned strings}
\NormalTok{x }\OperatorTok{=} \StringTok{""}\NormalTok{.join([}\StringTok{"he"}\NormalTok{, }\StringTok{"llo"}\NormalTok{])}
\NormalTok{y }\OperatorTok{=} \StringTok{""}\NormalTok{.join([}\StringTok{"he"}\NormalTok{, }\StringTok{"llo"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x == y:"}\NormalTok{, x }\OperatorTok{==}\NormalTok{ y)   }\CommentTok{\# True (values equal)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x is y:"}\NormalTok{, x }\KeywordTok{is}\NormalTok{ y)   }\CommentTok{\# False (different objects)}

\CommentTok{\# 3. Manual interning}
\NormalTok{x\_interned }\OperatorTok{=}\NormalTok{ sys.}\BuiltInTok{intern}\NormalTok{(x)}
\NormalTok{y\_interned }\OperatorTok{=}\NormalTok{ sys.}\BuiltInTok{intern}\NormalTok{(y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x\_interned is y\_interned:"}\NormalTok{, x\_interned }\KeywordTok{is}\NormalTok{ y\_interned)  }\CommentTok{\# True}

\CommentTok{\# 4. Memory efficiency demo}
\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"error"}\NormalTok{] }\OperatorTok{*} \DecValTok{100000}
\NormalTok{unique\_words }\OperatorTok{=}\NormalTok{ [sys.}\BuiltInTok{intern}\NormalTok{(}\StringTok{"error"}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{100000}\NormalTok{)]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normal list length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(words))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Interned list length:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(unique\_words))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normal memory ids (first 3):"}\NormalTok{, [}\BuiltInTok{id}\NormalTok{(w) }\ControlFlowTok{for}\NormalTok{ w }\KeywordTok{in}\NormalTok{ words[:}\DecValTok{3}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Interned memory ids (first 3):"}\NormalTok{, [}\BuiltInTok{id}\NormalTok{(w) }\ControlFlowTok{for}\NormalTok{ w }\KeywordTok{in}\NormalTok{ unique\_words[:}\DecValTok{3}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-1}

\begin{itemize}
\tightlist
\item
  String literals often share the same memory address.
\item
  Dynamically built identical strings are separate objects unless
  interned.
\item
  Interned strings reuse the same memory, reducing duplication.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Identity (\texttt{is})
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory effect
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{"hello"} vs \texttt{"hello"} & True & Interned automatically. \\
\texttt{"".join({[}"he","llo"{]})} & False & Different objects in
memory. \\
\texttt{sys.intern()} applied & True & Both point to one shared
object. \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-46}

\begin{itemize}
\tightlist
\item
  Compilers and interpreters use interning for keywords, identifiers,
  and symbols.
\item
  Databases and search engines save memory when handling millions of
  repeated strings.
\item
  Performance improves because identity comparison (\texttt{is}) is
  faster than value comparison.
\item
  Overuse of interning on unique strings wastes memory and CPU cycles.
\end{itemize}

\subsection{Exercises}\label{exercises-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create two identical literals and prove they point to the same memory
  location.
\item
  Build two identical strings dynamically and show they are different
  objects.
\item
  Apply interning to the dynamic strings and prove they now share
  memory.
\item
  Create 10,000 repeated \texttt{"apple"} strings with and without
  interning. Compare identity checks.
\item
  Explain why interning improves dictionary lookups.
\item
  Show an example where interning increases memory use (hint: many
  unique strings).
\item
  Compare interned vs non-interned strings in terms of lookup speed in a
  dictionary of 100k keys.
\item
  Describe how interning is used in language runtimes (e.g., symbol
  tables in compilers).
\item
  Propose a scenario in large-scale systems (like log processing) where
  interning saves significant memory.
\item
  Explain why interning must be combined with garbage collection to
  avoid memory leaks.
\end{enumerate}

\section{LAB 3: Kernel-Level String
Operations}\label{lab-3-kernel-level-string-operations}

\subsection{Goal}\label{goal-2}

Explore how strings are represented at the C level using \texttt{char*}.
Learn how null terminators
(\texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}}) define
string boundaries, how buffer overflows occur, and why higher-level
languages add safety.

\subsection{Setup}\label{setup-2}

This lab uses C (or C-like pseudocode). It can be run with \texttt{gcc}
or \texttt{clang}. We'll demonstrate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How C stores strings in memory.
\item
  What happens when the null terminator is missing.
\item
  Why functions like \texttt{strcpy} can cause buffer overflows.
\end{enumerate}

\subsection{Step-by-Step}\label{step-by-step-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  String Representation in C

  \begin{itemize}
  \tightlist
  \item
    Create a C string with \texttt{char\ s{[}{]}\ =\ "hello";}.
  \item
    Print its characters in a loop.
  \item
    Show that memory includes
    \texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}} at
    the end.
  \end{itemize}
\item
  Effect of Null Terminator

  \begin{itemize}
  \tightlist
  \item
    Create a buffer without
    \texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}}.
  \item
    Pass it to \texttt{printf("\%s")}.
  \item
    Observe how the function reads past the intended end, printing
    garbage or crashing.
  \end{itemize}
\item
  Buffer Overflow Risk

  \begin{itemize}
  \tightlist
  \item
    Create a small buffer (e.g., 5 bytes).
  \item
    Copy a larger string into it with \texttt{strcpy}.
  \item
    Show that memory beyond the buffer is overwritten.
  \end{itemize}
\item
  Safer Alternatives

  \begin{itemize}
  \tightlist
  \item
    Replace \texttt{strcpy} with \texttt{strncpy} or \texttt{snprintf}.
  \item
    Show how they limit copies and prevent overflows.
  \end{itemize}
\end{enumerate}

\subsection{Example (C)}\label{example-c}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdio.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}string.h\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \CommentTok{// 1. String representation}
    \DataTypeTok{char}\NormalTok{ s}\OperatorTok{[]} \OperatorTok{=} \StringTok{"hello"}\OperatorTok{;}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"String: }\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ s}\OperatorTok{);}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \KeywordTok{sizeof}\OperatorTok{(}\NormalTok{s}\OperatorTok{);}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        printf}\OperatorTok{(}\StringTok{"Byte }\SpecialCharTok{\%d}\StringTok{: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ i}\OperatorTok{,}\NormalTok{ s}\OperatorTok{[}\NormalTok{i}\OperatorTok{]);}
    \OperatorTok{\}}

    \CommentTok{// 2. Missing null terminator}
    \DataTypeTok{char}\NormalTok{ bad}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{=} \OperatorTok{\{}\CharTok{\textquotesingle{}h\textquotesingle{}}\OperatorTok{,}\CharTok{\textquotesingle{}e\textquotesingle{}}\OperatorTok{,}\CharTok{\textquotesingle{}l\textquotesingle{}}\OperatorTok{,}\CharTok{\textquotesingle{}l\textquotesingle{}}\OperatorTok{,}\CharTok{\textquotesingle{}o\textquotesingle{}}\OperatorTok{\};} \CommentTok{// no \textquotesingle{}\textbackslash{}0\textquotesingle{}}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Bad string (no null): }\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ bad}\OperatorTok{);} \CommentTok{// prints garbage}

    \CommentTok{// 3. Buffer overflow}
    \DataTypeTok{char}\NormalTok{ small}\OperatorTok{[}\DecValTok{5}\OperatorTok{];}
\NormalTok{    strcpy}\OperatorTok{(}\NormalTok{small}\OperatorTok{,} \StringTok{"overflow!"}\OperatorTok{);} \CommentTok{// too long, unsafe}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Overflowed string: }\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ small}\OperatorTok{);}

    \CommentTok{// 4. Safe alternative}
    \DataTypeTok{char}\NormalTok{ safe}\OperatorTok{[}\DecValTok{5}\OperatorTok{];}
\NormalTok{    strncpy}\OperatorTok{(}\NormalTok{safe}\OperatorTok{,} \StringTok{"safe"}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\NormalTok{safe}\OperatorTok{)} \OperatorTok{{-}} \DecValTok{1}\OperatorTok{);}
\NormalTok{    safe}\OperatorTok{[}\KeywordTok{sizeof}\OperatorTok{(}\NormalTok{safe}\OperatorTok{)} \OperatorTok{{-}} \DecValTok{1}\OperatorTok{]} \OperatorTok{=} \CharTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}0}\CharTok{\textquotesingle{}}\OperatorTok{;} \CommentTok{// ensure null terminator}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Safe string: }\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ safe}\OperatorTok{);}

    \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The string \texttt{"hello"} occupies 6 bytes: 5 characters + 1 null
  terminator.
\item
  Without
  \texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}},
  \texttt{printf} continues reading memory until it finds a random null.
\item
  \texttt{strcpy} into a small buffer overwrites adjacent memory,
  causing corruption or crashes.
\item
  \texttt{strncpy} keeps the buffer safe by limiting copied characters.
\end{enumerate}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Case & Behavior \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
With \texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}} &
Prints correctly. \\
Without \texttt{\textquotesingle{}\textbackslash{}0\textquotesingle{}} &
Prints garbage or crashes. \\
Overflow with \texttt{strcpy} & Memory corruption. \\
Safe with \texttt{strncpy} & Correct, bounded copy. \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-47}

\begin{itemize}
\tightlist
\item
  C-level strings are unsafe by default. The programmer must manage
  termination and bounds.
\item
  Buffer overflows are a leading cause of security vulnerabilities.
\item
  Kernel code, device drivers, and embedded systems often rely on raw C
  strings.
\item
  Higher-level languages (Python, Java, Go, Rust) wrap strings in safe
  abstractions, preventing these bugs.
\end{itemize}

\subsection{Exercises}\label{exercises-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a C string and print its bytes, showing the null terminator.
\item
  Remove the null terminator from a string and observe the output.
\item
  Copy \texttt{"abcdefgh"} into a buffer of size 5 and note the result.
\item
  Rewrite the unsafe copy using \texttt{strncpy} or \texttt{snprintf}.
\item
  Explain why \texttt{strncpy} may leave out the null terminator if not
  handled carefully.
\item
  Investigate what happens if you forget to set
  \texttt{safe{[}sizeof(safe)-1{]}\ =\ \textquotesingle{}\textbackslash{}0\textquotesingle{}}.
\item
  Compare memory usage of \texttt{"hello"} vs \texttt{"hello\ world"}.
\item
  Explain why C strings make functions like \texttt{strlen} O(n) instead
  of O(1).
\item
  Show how buffer overflows can alter unrelated variables in memory.
\item
  Discuss how Rust or Go prevents these issues by design.
\end{enumerate}

\section{LAB 4: Concatenation Cost
Benchmark}\label{lab-4-concatenation-cost-benchmark}

\subsection{Goal}\label{goal-3}

Measure the cost of string concatenation when building large strings.
Understand why naive approaches (\texttt{+} in loops) are inefficient,
and how buffered approaches (\texttt{join}, builders, buffers) improve
performance.

\subsection{Setup}\label{setup-3}

This lab can be done in Python, Java, Go, or C++. Each has different
idioms:

\begin{itemize}
\tightlist
\item
  Python → \texttt{+} vs \texttt{"".join(list)}
\item
  Java → \texttt{+} vs \texttt{StringBuilder}
\item
  Go → \texttt{+=} vs \texttt{strings.Builder}
\item
  C++ → \texttt{+} vs \texttt{std::ostringstream}
\end{itemize}

We'll use Python as the demonstration language.

\subsection{Step-by-Step}\label{step-by-step-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Naive Concatenation in Loop

  \begin{itemize}
  \tightlist
  \item
    Start with an empty string.
  \item
    Append characters one by one with \texttt{+}.
  \item
    Measure execution time.
  \end{itemize}
\item
  Buffered Concatenation with List + Join

  \begin{itemize}
  \tightlist
  \item
    Store characters in a list.
  \item
    Use \texttt{"".join(list)} to build the final string once.
  \item
    Measure execution time.
  \end{itemize}
\item
  Compare Results

  \begin{itemize}
  \tightlist
  \item
    Show how loop concatenation scales poorly (O(n²)).
  \item
    Show how buffered concatenation scales linearly (O(n)).
  \end{itemize}
\item
  Experiment with Sizes

  \begin{itemize}
  \tightlist
  \item
    Run benchmarks for N = 10³, 10⁴, 10⁵ characters.
  \item
    Record execution times.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Naive concatenation reallocates and copies strings repeatedly.
  \item
    Buffered methods allocate memory once and fill it efficiently.
  \item
    Similar patterns exist across languages.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-2}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ time}

\KeywordTok{def}\NormalTok{ naive\_concat(n):}
\NormalTok{    s }\OperatorTok{=} \StringTok{""}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        s }\OperatorTok{+=} \StringTok{"x"}
    \ControlFlowTok{return}\NormalTok{ s}

\KeywordTok{def}\NormalTok{ join\_concat(n):}
\NormalTok{    parts }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        parts.append(}\StringTok{"x"}\NormalTok{)}
    \ControlFlowTok{return} \StringTok{""}\NormalTok{.join(parts)}

\ControlFlowTok{for}\NormalTok{ N }\KeywordTok{in}\NormalTok{ [}\DecValTok{1000}\NormalTok{, }\DecValTok{10000}\NormalTok{, }\DecValTok{50000}\NormalTok{]:}
\NormalTok{    start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{    naive\_concat(N)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Naive concat N=}\SpecialCharTok{\{}\NormalTok{N}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{s"}\NormalTok{)}

\NormalTok{    start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{    join\_concat(N)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Join concat N=}\SpecialCharTok{\{}\NormalTok{N}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(time.time() }\OperatorTok{{-}}\NormalTok{ start, }\DecValTok{4}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{s"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"{-}{-}{-}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-3}

\begin{itemize}
\tightlist
\item
  For small N (1000), both methods are similar.
\item
  For larger N (50k+), naive concatenation slows dramatically.
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
N & Naive \texttt{+} (s) & Join (s) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1,000 & \textasciitilde0.002 & \textasciitilde0.001 \\
10,000 & \textasciitilde0.15 & \textasciitilde0.01 \\
50,000 & \textasciitilde3.0+ & \textasciitilde0.05 \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-48}

\begin{itemize}
\tightlist
\item
  Text-heavy applications (logs, report generation, data pipelines) must
  build large strings efficiently.
\item
  Using the wrong method can multiply runtime by 100× or more.
\item
  High-level languages hide memory allocation, but understanding
  efficiency prevents hidden bottlenecks.
\item
  Systems languages (C++, Rust, Go) provide explicit builders to avoid
  repeated allocations.
\end{itemize}

\subsection{Exercises}\label{exercises-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Benchmark string concatenation with \texttt{+} for N = 1000, 10,000,
  and 100,000.
\item
  Benchmark buffered concatenation (\texttt{join},
  \texttt{StringBuilder}, or equivalent).
\item
  Plot execution time against N for both methods.
\item
  Explain why naive concatenation is O(n²).
\item
  Demonstrate that small strings (N \textless{} 100) show no noticeable
  difference.
\item
  Compare Python \texttt{join} with Go \texttt{strings.Builder}.
\item
  Create a function that takes a list of words and joins them with
  spaces efficiently.
\item
  Show how repeated concatenation can impact log processing in a
  simulated workload.
\item
  Extend the benchmark to include Unicode characters like \texttt{"😊"}.
\item
  Propose an optimization strategy for building multi-megabyte strings
  in a web server.
\end{enumerate}

\section{LAB 5: Regex Basics
Explorer}\label{lab-5-regex-basics-explorer}

\subsection{Goal}\label{goal-4}

Learn how regular expressions (regex) describe text patterns. Practice
matching digits, words, and simple patterns across strings. Compare
regex behavior across different languages, noting syntax similarities
and differences.

\subsection{Setup}\label{setup-4}

Regex exists in almost every modern language.

\begin{itemize}
\tightlist
\item
  Python → \texttt{re} module
\item
  Java → \texttt{Pattern} / \texttt{Matcher}
\item
  Go → \texttt{regexp} package
\item
  C++ → \texttt{\textless{}regex\textgreater{}}
\item
  JavaScript → \texttt{/pattern/} literals
\end{itemize}

We'll demonstrate in Python for clarity, but the concepts transfer
directly.

\subsection{Step-by-Step}\label{step-by-step-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Basic Digit Matching

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{{[}0-9{]}+} to match sequences of digits.
  \item
    Extract numbers from a sentence.
  \end{itemize}
\item
  Word Matching

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{\textbackslash{}w+} to capture words (letters, digits,
    underscores).
  \item
    Show how it splits a sentence into tokens.
  \end{itemize}
\item
  Anchors

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{\^{}pattern} to match the start of a string.
  \item
    Use \texttt{pattern\$} to match the end.
  \end{itemize}
\item
  Character Classes

  \begin{itemize}
  \tightlist
  \item
    \texttt{{[}aeiou{]}} matches vowels.
  \item
    \texttt{{[}A-Z{]}} matches uppercase letters.
  \end{itemize}
\item
  Practical Example: Email

  \begin{itemize}
  \tightlist
  \item
    Use
    \texttt{\textbackslash{}w+@\textbackslash{}w+\textbackslash{}.\textbackslash{}w+}
    to match simple emails.
  \item
    Note limitations (not RFC-complete, but useful).
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-3}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}

\NormalTok{text }\OperatorTok{=} \StringTok{"User Alice has email alice@example.com and id 12345."}

\CommentTok{\# 1. Digits}
\NormalTok{digits }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\PreprocessorTok{[0{-}9]}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Digits:"}\NormalTok{, digits)  }\CommentTok{\# [\textquotesingle{}12345\textquotesingle{}]}

\CommentTok{\# 2. Words}
\NormalTok{words }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Words:"}\NormalTok{, words)   }\CommentTok{\# [\textquotesingle{}User\textquotesingle{}, \textquotesingle{}Alice\textquotesingle{}, \textquotesingle{}has\textquotesingle{}, ...]}

\CommentTok{\# 3. Anchors}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{bool}\NormalTok{(re.match(}\VerbatimStringTok{r"User"}\NormalTok{, text)))   }\CommentTok{\# True (starts with "User")}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{bool}\NormalTok{(re.search(}\VerbatimStringTok{r"12345}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, text))) }\CommentTok{\# True (ends with "12345.")}

\CommentTok{\# 4. Character classes}
\NormalTok{vowels }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\PreprocessorTok{[aeiou]}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Vowels:"}\NormalTok{, vowels)}

\CommentTok{\# 5. Email pattern}
\NormalTok{emails }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{@}\DecValTok{\textbackslash{}w}\OperatorTok{+}\CharTok{\textbackslash{}.}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Emails:"}\NormalTok{, emails)}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-4}

\begin{itemize}
\tightlist
\item
  \texttt{{[}0-9{]}+} extracts \texttt{"12345"}.
\item
  \texttt{\textbackslash{}w+} splits the sentence into words.
\item
  \texttt{\^{}User} matches the beginning.
\item
  \texttt{{[}aeiou{]}} finds vowels.
\item
  \texttt{\textbackslash{}w+@\textbackslash{}w+\textbackslash{}.\textbackslash{}w+}
  extracts \texttt{"alice@example.com"}.
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Pattern & Meaning & Example Match \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{{[}0-9{]}+} & One or more digits & \texttt{"12345"} \\
\texttt{\textbackslash{}w+} & Word characters & \texttt{"Alice"} \\
\texttt{\^{}User} & Must start with \texttt{"User"} &
\texttt{"User\ Alice..."} \\
\texttt{{[}aeiou{]}} & Any single vowel & \texttt{"a",\ "e",\ "i"} \\
\texttt{\textbackslash{}w+@\textbackslash{}w+\textbackslash{}.\textbackslash{}w+}
& Simple email pattern & \texttt{"alice@example.com"} \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-49}

Regex is a universal tool for working with text: extracting data,
validating input, or parsing logs. It is concise and expressive,
replacing dozens of manual string operations. Learning the basics of
regex builds the foundation for advanced pattern matching and text
processing in any language.

\subsection{Exercises}\label{exercises-48}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a regex to match all digits in \texttt{"room\ 101\ floor\ 5"}.
\item
  Extract all words from \texttt{"The\ quick\ brown\ fox"}.
\item
  Match any string starting with \texttt{"Hello"}.
\item
  Match all vowels in \texttt{"banana"}.
\item
  Extract all email addresses from
  \texttt{"bob@mail.com,\ alice@test.org"}.
\item
  Write a regex that matches only uppercase words.
\item
  Find all three-letter words in \texttt{"the\ cat\ sat\ on\ the\ mat"}.
\item
  Show how \texttt{\^{}} and \texttt{\$} can be used to validate a
  string that must start and end with digits.
\item
  Explain why
  \texttt{\textbackslash{}w+@\textbackslash{}w+\textbackslash{}.\textbackslash{}w+}
  is not sufficient for real email validation.
\item
  Write a regex that finds all words ending in \texttt{"ing"} from a
  sentence.
\end{enumerate}

\section{LAB 6: CSV Parsing Pitfalls}\label{lab-6-csv-parsing-pitfalls}

\subsection{Goal}\label{goal-5}

Show why naive string splitting fails on CSV data. Demonstrate common
pitfalls like quoted fields and embedded commas. Learn why dedicated CSV
parsers are needed for reliability.

\subsection{Setup}\label{setup-5}

CSV (Comma-Separated Values) looks simple but has rules:

\begin{itemize}
\tightlist
\item
  Fields can be quoted with \texttt{"}
\item
  Quoted fields can contain commas
\item
  Quotes inside fields are escaped as \texttt{""}
\end{itemize}

We'll use Python, since it has both naive string methods and a built-in
\texttt{csv} module.

\subsection{Step-by-Step}\label{step-by-step-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Naive Split Works Sometimes

  \begin{itemize}
  \tightlist
  \item
    Take \texttt{"apple,banana,cherry"}.
  \item
    Split by \texttt{,}.
  \item
    Works fine.
  \end{itemize}
\item
  Problem: Quoted Fields

  \begin{itemize}
  \tightlist
  \item
    Take \texttt{"apple,"banana,grape",cherry"}.
  \item
    Split by \texttt{,}.
  \item
    See how \texttt{"banana,grape"} is broken incorrectly.
  \end{itemize}
\item
  Problem: Escaped Quotes

  \begin{itemize}
  \tightlist
  \item
    Take \texttt{"apple,"he\ said\ ""hi""",cherry"}.
  \item
    Naive split breaks the field and does not handle quotes.
  \end{itemize}
\item
  Correct Parsing with CSV Module

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{csv.reader} to handle quotes and escapes.
  \item
    Show correct results.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Naive string splitting fails whenever quotes or embedded commas
    exist.
  \item
    Correct parsers follow RFC 4180 rules.
  \item
    Production code must use libraries, not ad hoc splitting.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-4}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ csv}

\CommentTok{\# 1. Naive split (works)}
\NormalTok{line1 }\OperatorTok{=} \StringTok{"apple,banana,cherry"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive split:"}\NormalTok{, line1.split(}\StringTok{","}\NormalTok{))  }\CommentTok{\# [\textquotesingle{}apple\textquotesingle{}, \textquotesingle{}banana\textquotesingle{}, \textquotesingle{}cherry\textquotesingle{}]}

\CommentTok{\# 2. Quoted fields with commas}
\NormalTok{line2 }\OperatorTok{=} \StringTok{\textquotesingle{}apple,"banana,grape",cherry\textquotesingle{}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive split:"}\NormalTok{, line2.split(}\StringTok{","}\NormalTok{))  }\CommentTok{\# [\textquotesingle{}apple\textquotesingle{}, \textquotesingle{}"banana\textquotesingle{}, \textquotesingle{}grape"\textquotesingle{}, \textquotesingle{}cherry\textquotesingle{}]}

\CommentTok{\# 3. Quoted fields with escaped quotes}
\NormalTok{line3 }\OperatorTok{=} \StringTok{\textquotesingle{}apple,"he said ""hi""",cherry\textquotesingle{}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive split:"}\NormalTok{, line3.split(}\StringTok{","}\NormalTok{))  }\CommentTok{\# [\textquotesingle{}apple\textquotesingle{}, \textquotesingle{}"he said ""hi"""\textquotesingle{}, \textquotesingle{}cherry\textquotesingle{}]}

\CommentTok{\# 4. Correct parsing}
\ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in}\NormalTok{ [line1, line2, line3]:}
\NormalTok{    parsed }\OperatorTok{=} \BuiltInTok{next}\NormalTok{(csv.reader([line]))}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"CSV parser:"}\NormalTok{, parsed)}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-5}

\begin{itemize}
\tightlist
\item
  Naive split fails on quoted fields.
\item
  CSV parser handles commas and quotes correctly.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2952}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Input String
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Naive Split
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CSV Parser
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{apple,banana,cherry} &
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\textquotesingle{}banana\textquotesingle{},\textquotesingle{}cherry\textquotesingle{}{]}}
&
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\textquotesingle{}banana\textquotesingle{},\textquotesingle{}cherry\textquotesingle{}{]}} \\
\texttt{apple,"banana,grape",cherry} &
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\textquotesingle{}"banana\textquotesingle{},\textquotesingle{}grape"\textquotesingle{},\textquotesingle{}cherry\textquotesingle{}{]}}
&
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\textquotesingle{}banana,grape\textquotesingle{},\textquotesingle{}cherry\textquotesingle{}{]}} \\
\texttt{apple,"he\ said\ ""hi""",cherry} &
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\textquotesingle{}"he\ said\ ""hi"""\textquotesingle{},\textquotesingle{}cherry\textquotesingle{}{]}}
&
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\textquotesingle{}he\ said\ "hi"\textquotesingle{},\textquotesingle{}cherry\textquotesingle{}{]}} \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-50}

CSV looks simple but is deceptively tricky. Many production bugs come
from naive parsing that breaks when encountering quotes or embedded
commas. Using proper libraries ensures reliability across systems.
Understanding these pitfalls prevents data corruption in analytics, ETL
pipelines, and configuration parsing.

\subsection{Exercises}\label{exercises-49}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write code that naively splits \texttt{"a,b,c"} and verify the output.
\item
  Split \texttt{"a,"b,c",d"} naively and explain what goes wrong.
\item
  Parse the same line using a CSV library and compare results.
\item
  Show how \texttt{"a,"he\ said\ ""ok""",c"} is incorrectly parsed by
  naive splitting.
\item
  Write a regex that attempts to parse CSV and explain its limits.
\item
  Compare parsing speed of naive split vs \texttt{csv.reader}.
\item
  Explain why using regex for full CSV parsing is impractical.
\item
  Show how a CSV parser handles an empty field (\texttt{"a,,c"}).
\item
  Create a CSV with newline characters inside quotes and parse it
  correctly.
\item
  Describe why standards like RFC 4180 exist for CSV and what problems
  they solve.
\end{enumerate}

\section{LAB 7: Locale-Aware Sorting}\label{lab-7-locale-aware-sorting}

\subsection{Goal}\label{goal-6}

Understand how different locales affect string sorting. See why
\texttt{"ä"} sorts differently in German vs Swedish, and how case
sensitivity influences order. Learn why collation rules matter in
real-world systems like databases and search engines.

\subsection{Setup}\label{setup-6}

Use a language that supports locale-aware collation:

\begin{itemize}
\tightlist
\item
  Python → \texttt{locale} module
\item
  Java → \texttt{Collator}
\item
  C++ → \texttt{\textless{}locale\textgreater{}} with \texttt{collate}
\item
  Databases → \texttt{COLLATE} clauses
\end{itemize}

We'll demonstrate with Python.

\subsection{Step-by-Step}\label{step-by-step-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Default Sorting (ASCII/Unicode order)

  \begin{itemize}
  \tightlist
  \item
    Sort a list of words:
    \texttt{{[}"apple",\ "Banana",\ "äpfel",\ "zebra"{]}}.
  \item
    Observe that uppercase letters and special characters do not sort as
    expected for dictionary order.
  \end{itemize}
\item
  German Collation

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{"de\_DE.UTF-8"} locale.
  \item
    In German, \texttt{"ä"} is often treated like \texttt{"ae"}.
  \item
    Sort the same list and observe differences.
  \end{itemize}
\item
  Swedish Collation

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{"sv\_SE.UTF-8"} locale.
  \item
    In Swedish, \texttt{"ä"} is a separate letter that comes after
    \texttt{"z"}.
  \item
    Compare results with German sorting.
  \end{itemize}
\item
  Case Sensitivity

  \begin{itemize}
  \tightlist
  \item
    Compare \texttt{"apple"} vs \texttt{"Apple"}.
  \item
    Show how some locales ignore case, others do not.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Locale defines sorting rules for a culture or language.
  \item
    Incorrect collation leads to confusing search results or misordered
    lists.
  \item
    Databases use collations to enforce consistency.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-5}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ locale}

\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"apple"}\NormalTok{, }\StringTok{"Banana"}\NormalTok{, }\StringTok{"äpfel"}\NormalTok{, }\StringTok{"zebra"}\NormalTok{]}

\CommentTok{\# 1. Default sorting}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Default:"}\NormalTok{, }\BuiltInTok{sorted}\NormalTok{(words))}

\CommentTok{\# 2. German collation}
\NormalTok{locale.setlocale(locale.LC\_COLLATE, }\StringTok{"de\_DE.UTF{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"German:"}\NormalTok{, }\BuiltInTok{sorted}\NormalTok{(words, key}\OperatorTok{=}\NormalTok{locale.strxfrm))}

\CommentTok{\# 3. Swedish collation}
\NormalTok{locale.setlocale(locale.LC\_COLLATE, }\StringTok{"sv\_SE.UTF{-}8"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Swedish:"}\NormalTok{, }\BuiltInTok{sorted}\NormalTok{(words, key}\OperatorTok{=}\NormalTok{locale.strxfrm))}

\CommentTok{\# 4. Case{-}insensitive comparison (normalize before sort)}
\NormalTok{case\_insensitive }\OperatorTok{=} \BuiltInTok{sorted}\NormalTok{(words, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ w: w.lower())}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Case{-}insensitive:"}\NormalTok{, case\_insensitive)}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-6}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Locale & Sorted Output \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Default (Unicode) &
\texttt{{[}\textquotesingle{}Banana\textquotesingle{},\ \textquotesingle{}apple\textquotesingle{},\ \textquotesingle{}zebra\textquotesingle{},\ \textquotesingle{}äpfel\textquotesingle{}{]}} \\
German (de\_DE) &
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\ \textquotesingle{}äpfel\textquotesingle{},\ \textquotesingle{}Banana\textquotesingle{},\ \textquotesingle{}zebra\textquotesingle{}{]}} \\
Swedish (sv\_SE) &
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\ \textquotesingle{}Banana\textquotesingle{},\ \textquotesingle{}zebra\textquotesingle{},\ \textquotesingle{}äpfel\textquotesingle{}{]}} \\
Case-insensitive &
\texttt{{[}\textquotesingle{}apple\textquotesingle{},\ \textquotesingle{}äpfel\textquotesingle{},\ \textquotesingle{}Banana\textquotesingle{},\ \textquotesingle{}zebra\textquotesingle{}{]}} \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-51}

\begin{itemize}
\tightlist
\item
  Multilingual systems must handle cultural sorting correctly.
\item
  \texttt{"ä"} is \texttt{"ae"} in German, but a distinct letter after
  \texttt{"z"} in Swedish.
\item
  Case sensitivity can confuse users if \texttt{"Apple"} and
  \texttt{"apple"} appear far apart.
\item
  Databases and search engines must define collation rules to ensure
  consistency.
\end{itemize}

\subsection{Exercises}\label{exercises-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sort \texttt{{[}"Zoo",\ "apple",\ "Äpfel"{]}} in default order.
\item
  Sort the same list in German locale.
\item
  Sort the same list in Swedish locale.
\item
  Explain why results differ between German and Swedish.
\item
  Write code that performs case-insensitive sorting.
\item
  Compare how \texttt{"Résumé"} and \texttt{"resume"} sort under
  accent-sensitive vs accent-insensitive collation.
\item
  Demonstrate natural sorting for file names:
  \texttt{{[}"file1",\ "file10",\ "file2"{]}}.
\item
  Explain how incorrect collation could break alphabetical order in a
  contact list.
\item
  Research and list collations available in your system/database.
\item
  Propose a default collation strategy for a global web application.
\end{enumerate}

\section{LAB 8: Unicode Normalization
Demo}\label{lab-8-unicode-normalization-demo}

\subsection{Goal}\label{goal-7}

Show how visually identical strings can have different underlying
Unicode encodings. Learn to use normalization (NFC, NFD) to make
comparisons reliable across systems.

\subsection{Setup}\label{setup-7}

Unicode allows multiple representations of the same text:

\begin{itemize}
\tightlist
\item
  Precomposed characters (\texttt{é} as U+00E9)
\item
  Decomposed sequences (\texttt{e} + U+0301 combining accent)
\end{itemize}

We'll use Python's \texttt{unicodedata} module to demonstrate.

\subsection{Step-by-Step}\label{step-by-step-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create Equivalent Strings

  \begin{itemize}
  \tightlist
  \item
    Define \texttt{s1\ =\ "café"} (precomposed).
  \item
    Define \texttt{s2\ =\ "cafe\textbackslash{}u0301"} (decomposed).
  \item
    Print both --- they look the same.
  \end{itemize}
\item
  Compare Without Normalization

  \begin{itemize}
  \tightlist
  \item
    Check \texttt{s1\ ==\ s2}.
  \item
    Result is \texttt{False}, even though they appear identical.
  \end{itemize}
\item
  Normalize Strings

  \begin{itemize}
  \tightlist
  \item
    Apply NFC (Normalization Form Composed).
  \item
    Apply NFD (Normalization Form Decomposed).
  \item
    Compare results again.
  \end{itemize}
\item
  Check Lengths and Code Points

  \begin{itemize}
  \tightlist
  \item
    Compare lengths of \texttt{s1} and \texttt{s2}.
  \item
    Print their Unicode code points.
  \item
    Show how precomposed has 1 code point for \texttt{é}, decomposed has
    2.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Normalization ensures consistent storage and comparison.
  \item
    Databases and search engines often normalize text before indexing.
  \item
    Without it, identical-looking words may be treated as different.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-6}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ unicodedata}

\CommentTok{\# 1. Create equivalent strings}
\NormalTok{s1 }\OperatorTok{=} \StringTok{"café"}           \CommentTok{\# precomposed}
\NormalTok{s2 }\OperatorTok{=} \StringTok{"cafe}\CharTok{\textbackslash{}u0301}\StringTok{"}     \CommentTok{\# decomposed}

\BuiltInTok{print}\NormalTok{(}\StringTok{"s1:"}\NormalTok{, s1)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"s2:"}\NormalTok{, s2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal raw?:"}\NormalTok{, s1 }\OperatorTok{==}\NormalTok{ s2)}

\CommentTok{\# 2. Normalize}
\NormalTok{nfc1 }\OperatorTok{=}\NormalTok{ unicodedata.normalize(}\StringTok{"NFC"}\NormalTok{, s1)}
\NormalTok{nfc2 }\OperatorTok{=}\NormalTok{ unicodedata.normalize(}\StringTok{"NFC"}\NormalTok{, s2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal NFC?:"}\NormalTok{, nfc1 }\OperatorTok{==}\NormalTok{ nfc2)}

\NormalTok{nfd1 }\OperatorTok{=}\NormalTok{ unicodedata.normalize(}\StringTok{"NFD"}\NormalTok{, s1)}
\NormalTok{nfd2 }\OperatorTok{=}\NormalTok{ unicodedata.normalize(}\StringTok{"NFD"}\NormalTok{, s2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal NFD?:"}\NormalTok{, nfd1 }\OperatorTok{==}\NormalTok{ nfd2)}

\CommentTok{\# 3. Inspect lengths and code points}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Length s1:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(s1))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Length s2:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(s2))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Code points s1:"}\NormalTok{, [}\BuiltInTok{hex}\NormalTok{(}\BuiltInTok{ord}\NormalTok{(c)) }\ControlFlowTok{for}\NormalTok{ c }\KeywordTok{in}\NormalTok{ s1])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Code points s2:"}\NormalTok{, [}\BuiltInTok{hex}\NormalTok{(}\BuiltInTok{ord}\NormalTok{(c)) }\ControlFlowTok{for}\NormalTok{ c }\KeywordTok{in}\NormalTok{ s2])}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-7}

\begin{itemize}
\item
  \texttt{s1\ ==\ s2} → False (different underlying encoding).
\item
  After normalization (NFC or NFD), they compare equal.
\item
  Lengths differ (\texttt{len(s1)\ ==\ 4}, \texttt{len(s2)\ ==\ 5}).
\item
  Code points differ:

  \begin{itemize}
  \tightlist
  \item
    \texttt{s1}: \texttt{{[}0x63,\ 0x61,\ 0x66,\ 0xe9{]}}
  \item
    \texttt{s2}: \texttt{{[}0x63,\ 0x61,\ 0x66,\ 0x65,\ 0x301{]}}
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1017}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2034}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2542}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1017}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3390}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
String
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Visible Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Encoding Style
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Code Points
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
s1 & café & Precomposed NFC & 4 & c a f é (U+00E9) \\
s2 & café & Decomposed NFD & 5 & c a f e + ´ (U+0301) \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-52}

\begin{itemize}
\tightlist
\item
  Identical-looking text can fail equality checks if not normalized.
\item
  File systems, databases, and APIs may use different normalization
  forms.
\item
  Search engines normalize text to avoid duplicate results.
\item
  Security-sensitive systems must normalize input to avoid spoofing or
  bypass attacks.
\end{itemize}

\subsection{Exercises}\label{exercises-51}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare \texttt{"résumé"} written with precomposed vs decomposed
  accents.
\item
  Show how string lengths differ between NFC and NFD forms.
\item
  Write code to normalize a list of user inputs before storing them in a
  database.
\item
  Explain why \texttt{"é"} (U+00E9) is not equal to
  \texttt{"e"\ +\ U+0301}.
\item
  Demonstrate normalization on Japanese text (hiragana + diacritics).
\item
  Show how failing to normalize could cause duplicate entries in a
  dictionary.
\item
  Investigate what normalization form macOS uses by default for
  filenames.
\item
  Explain why normalization is critical in international domain names.
\item
  Create a function that reports whether two strings are canonically
  equivalent.
\item
  Design a normalization step for a search engine pipeline.
\end{enumerate}

\section{LAB 9: Confusable Characters Security
Check}\label{lab-9-confusable-characters-security-check}

\subsection{Goal}\label{goal-8}

Demonstrate how visually identical characters can come from different
scripts (Latin, Cyrillic, Greek). Show how this can fool string
comparisons, and implement a basic detector for such confusables.

\subsection{Setup}\label{setup-8}

Attackers exploit Unicode confusables (e.g., Latin \texttt{"a"} vs
Cyrillic \texttt{"а"}) to trick users or bypass validation. This is
called a homoglyph attack. Examples:

\begin{itemize}
\tightlist
\item
  \texttt{"paypal.com"} vs \texttt{"раураl.com"} (Cyrillic \texttt{р}
  and \texttt{а}).
\item
  \texttt{"admin"} vs \texttt{"аdmin"} (Cyrillic \texttt{а}).
\end{itemize}

We'll use Python for demonstration (\texttt{unicodedata} module).

\subsection{Step-by-Step}\label{step-by-step-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create Confusable Strings

  \begin{itemize}
  \tightlist
  \item
    Define \texttt{latin\_a\ =\ "a"} and \texttt{cyrillic\_a\ =\ "а"}
    (U+0430).
  \item
    Print them side by side.
  \item
    Compare with \texttt{==} to show they are different.
  \end{itemize}
\item
  Inspect Code Points

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{ord()} to print Unicode code points.
  \item
    Show the numeric difference despite identical appearance.
  \end{itemize}
\item
  Confusable Detection

  \begin{itemize}
  \tightlist
  \item
    Write a function that checks if characters belong to different
    Unicode blocks.
  \item
    Flag potential confusables.
  \end{itemize}
\item
  Practical Example: Fake Domain

  \begin{itemize}
  \tightlist
  \item
    Construct \texttt{"раураl.com"} (Cyrillic letters).
  \item
    Compare it to \texttt{"paypal.com"}.
  \item
    Show why naive string equality misses the difference.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \item
    Homoglyph attacks target login systems, domains, and usernames.
  \item
    Defenses:

    \begin{itemize}
    \tightlist
    \item
      Normalize + restrict allowed scripts.
    \item
      Use libraries like
      \href{https://www.unicode.org/Public/security/latest/confusables.txt}{Unicode
      confusables.txt}.
    \item
      Visual warnings in browsers.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-7}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ unicodedata}

\CommentTok{\# 1. Confusable characters}
\NormalTok{latin\_a }\OperatorTok{=} \StringTok{"a"}
\NormalTok{cyrillic\_a }\OperatorTok{=} \StringTok{"а"}  \CommentTok{\# U+0430}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Latin a:"}\NormalTok{, latin\_a, }\StringTok{"Cyrillic a:"}\NormalTok{, cyrillic\_a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal?:"}\NormalTok{, latin\_a }\OperatorTok{==}\NormalTok{ cyrillic\_a)}

\CommentTok{\# 2. Inspect code points}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Latin a code point:"}\NormalTok{, }\BuiltInTok{hex}\NormalTok{(}\BuiltInTok{ord}\NormalTok{(latin\_a)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Cyrillic a code point:"}\NormalTok{, }\BuiltInTok{hex}\NormalTok{(}\BuiltInTok{ord}\NormalTok{(cyrillic\_a)))}

\CommentTok{\# 3. Simple detector}
\KeywordTok{def}\NormalTok{ detect\_confusables(text):}
\NormalTok{    scripts }\OperatorTok{=}\NormalTok{ \{\}}
    \ControlFlowTok{for}\NormalTok{ ch }\KeywordTok{in}\NormalTok{ text:}
\NormalTok{        name }\OperatorTok{=}\NormalTok{ unicodedata.name(ch, }\StringTok{""}\NormalTok{)}
        \ControlFlowTok{if} \StringTok{"CYRILLIC"} \KeywordTok{in}\NormalTok{ name:}
\NormalTok{            scripts[ch] }\OperatorTok{=} \StringTok{"Cyrillic"}
        \ControlFlowTok{elif} \StringTok{"LATIN"} \KeywordTok{in}\NormalTok{ name:}
\NormalTok{            scripts[ch] }\OperatorTok{=} \StringTok{"Latin"}
        \ControlFlowTok{elif} \StringTok{"GREEK"} \KeywordTok{in}\NormalTok{ name:}
\NormalTok{            scripts[ch] }\OperatorTok{=} \StringTok{"Greek"}
    \ControlFlowTok{return}\NormalTok{ scripts}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Detection:"}\NormalTok{, detect\_confusables(}\StringTok{"раураl"}\NormalTok{))}

\CommentTok{\# 4. Fake domain example}
\NormalTok{real }\OperatorTok{=} \StringTok{"paypal.com"}
\NormalTok{fake }\OperatorTok{=} \StringTok{"раураl.com"}  \CommentTok{\# with Cyrillic \textquotesingle{}р\textquotesingle{} and \textquotesingle{}а\textquotesingle{}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Real domain:"}\NormalTok{, real)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Fake domain:"}\NormalTok{, fake)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal?:"}\NormalTok{, real }\OperatorTok{==}\NormalTok{ fake)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Fake detection:"}\NormalTok{, detect\_confusables(fake))}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-8}

\begin{itemize}
\item
  \texttt{"a"} vs \texttt{"а"} look the same but are unequal.
\item
  Code points differ:

  \begin{itemize}
  \tightlist
  \item
    Latin \texttt{a} → \texttt{U+0061}
  \item
    Cyrillic \texttt{а} → \texttt{U+0430}
  \end{itemize}
\item
  Detection function flags script differences.
\item
  \texttt{"paypal.com"} ≠ \texttt{"раураl.com"}, but naive eyes might
  miss it.
\end{itemize}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Character & Visual & Unicode & Script \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{a} & a & U+0061 & Latin \\
\texttt{а} & a & U+0430 & Cyrillic \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-53}

\begin{itemize}
\tightlist
\item
  Security systems that ignore Unicode tricks are vulnerable to phishing
  and spoofing.
\item
  Fake domains, usernames, or commands can bypass filters.
\item
  Normalization does not fix confusables --- script restriction or
  mapping is required.
\item
  Modern browsers warn users about domains mixing scripts.
\end{itemize}

\subsection{Exercises}\label{exercises-52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare \texttt{"pаypal"} (with Cyrillic \texttt{а}) to
  \texttt{"paypal"}.
\item
  Print Unicode code points of \texttt{"o"} vs Cyrillic \texttt{"о"}.
\item
  Extend the \texttt{detect\_confusables} function to flag Greek letters
  too.
\item
  Create a fake username using mixed Latin and Cyrillic.
\item
  Show why normalization (NFC/NFD) does not fix confusables.
\item
  Research Unicode \texttt{confusables.txt} and find at least 5 common
  homoglyphs.
\item
  Propose a rule to allow only one script (Latin or Cyrillic) per
  string.
\item
  Write a program to detect mixed-script domains.
\item
  Explain how browsers handle homoglyph domains.
\item
  Design a security policy for usernames to prevent confusable attacks.
\end{enumerate}

\section{LAB 10: Substring Search
Algorithms}\label{lab-10-substring-search-algorithms}

\subsection{Goal}\label{goal-9}

Compare different substring search algorithms --- Naive,
Knuth--Morris--Pratt (KMP), and Boyer--Moore (BM). Learn how they work,
when they perform well, and measure their efficiency on long texts.

\subsection{Setup}\label{setup-9}

Substring search asks: \emph{Does pattern \texttt{P} occur in text
\texttt{T}?}

\begin{itemize}
\tightlist
\item
  Naive algorithm: check every position → O(n·m).
\item
  KMP: preprocess pattern to skip comparisons → O(n + m).
\item
  Boyer--Moore: skip ahead using heuristics → sublinear in practice.
\end{itemize}

We'll implement these in Python for clarity.

\subsection{Step-by-Step}\label{step-by-step-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Naive Algorithm

  \begin{itemize}
  \tightlist
  \item
    For each position in text, compare pattern character by character.
  \item
    Slow on long inputs with repeated matches.
  \end{itemize}
\item
  KMP Algorithm

  \begin{itemize}
  \tightlist
  \item
    Build a failure function (longest prefix-suffix table).
  \item
    Skip ahead when mismatch occurs.
  \item
    Guarantees linear time.
  \end{itemize}
\item
  Boyer--Moore Algorithm (Bad Character Rule)

  \begin{itemize}
  \tightlist
  \item
    Start matching from the end of the pattern.
  \item
    On mismatch, shift based on last occurrence of character.
  \item
    Very fast in practice, especially for large alphabets.
  \end{itemize}
\item
  Benchmark on Large Input

  \begin{itemize}
  \tightlist
  \item
    Generate a text of size \textasciitilde100,000 characters.
  \item
    Search for a small pattern.
  \item
    Measure runtimes.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Naive is simple but inefficient.
  \item
    KMP is optimal for worst case.
  \item
    Boyer--Moore is fastest in practice for natural language.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-8}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ time}

\CommentTok{\# 1. Naive}
\KeywordTok{def}\NormalTok{ naive\_search(text, pattern):}
\NormalTok{    n, m }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(text), }\BuiltInTok{len}\NormalTok{(pattern)}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n }\OperatorTok{{-}}\NormalTok{ m }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
        \ControlFlowTok{if}\NormalTok{ text[i:i}\OperatorTok{+}\NormalTok{m] }\OperatorTok{==}\NormalTok{ pattern:}
            \ControlFlowTok{return}\NormalTok{ i}
    \ControlFlowTok{return} \OperatorTok{{-}}\DecValTok{1}

\CommentTok{\# 2. KMP}
\KeywordTok{def}\NormalTok{ kmp\_table(pattern):}
\NormalTok{    m }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(pattern)}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*}\NormalTok{ m}
\NormalTok{    j }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, m):}
        \ControlFlowTok{while}\NormalTok{ j }\OperatorTok{\textgreater{}} \DecValTok{0} \KeywordTok{and}\NormalTok{ pattern[i] }\OperatorTok{!=}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{=}\NormalTok{ table[j }\OperatorTok{{-}} \DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ pattern[i] }\OperatorTok{==}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{+=} \DecValTok{1}
\NormalTok{            table[i] }\OperatorTok{=}\NormalTok{ j}
    \ControlFlowTok{return}\NormalTok{ table}

\KeywordTok{def}\NormalTok{ kmp\_search(text, pattern):}
\NormalTok{    n, m }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(text), }\BuiltInTok{len}\NormalTok{(pattern)}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ kmp\_table(pattern)}
\NormalTok{    j }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
        \ControlFlowTok{while}\NormalTok{ j }\OperatorTok{\textgreater{}} \DecValTok{0} \KeywordTok{and}\NormalTok{ text[i] }\OperatorTok{!=}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{=}\NormalTok{ table[j }\OperatorTok{{-}} \DecValTok{1}\NormalTok{]}
        \ControlFlowTok{if}\NormalTok{ text[i] }\OperatorTok{==}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{+=} \DecValTok{1}
            \ControlFlowTok{if}\NormalTok{ j }\OperatorTok{==}\NormalTok{ m:}
                \ControlFlowTok{return}\NormalTok{ i }\OperatorTok{{-}}\NormalTok{ m }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return} \OperatorTok{{-}}\DecValTok{1}

\CommentTok{\# 3. Boyer–Moore (bad character heuristic)}
\KeywordTok{def}\NormalTok{ bm\_table(pattern):}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ \{\}}
    \ControlFlowTok{for}\NormalTok{ i, c }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(pattern):}
\NormalTok{        table[c] }\OperatorTok{=}\NormalTok{ i}
    \ControlFlowTok{return}\NormalTok{ table}

\KeywordTok{def}\NormalTok{ bm\_search(text, pattern):}
\NormalTok{    n, m }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(text), }\BuiltInTok{len}\NormalTok{(pattern)}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ bm\_table(pattern)}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{while}\NormalTok{ i }\OperatorTok{\textless{}=}\NormalTok{ n }\OperatorTok{{-}}\NormalTok{ m:}
\NormalTok{        j }\OperatorTok{=}\NormalTok{ m }\OperatorTok{{-}} \DecValTok{1}
        \ControlFlowTok{while}\NormalTok{ j }\OperatorTok{\textgreater{}=} \DecValTok{0} \KeywordTok{and}\NormalTok{ text[i }\OperatorTok{+}\NormalTok{ j] }\OperatorTok{==}\NormalTok{ pattern[j]:}
\NormalTok{            j }\OperatorTok{{-}=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ j }\OperatorTok{\textless{}} \DecValTok{0}\NormalTok{:}
            \ControlFlowTok{return}\NormalTok{ i}
\NormalTok{        shift }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{1}\NormalTok{, j }\OperatorTok{{-}}\NormalTok{ table.get(text[i }\OperatorTok{+}\NormalTok{ j], }\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{        i }\OperatorTok{+=}\NormalTok{ shift}
    \ControlFlowTok{return} \OperatorTok{{-}}\DecValTok{1}

\CommentTok{\# Benchmark}
\NormalTok{text }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{100000} \OperatorTok{+} \StringTok{"b"}
\NormalTok{pattern }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{10} \OperatorTok{+} \StringTok{"b"}

\ControlFlowTok{for}\NormalTok{ name, func }\KeywordTok{in}\NormalTok{ [(}\StringTok{"Naive"}\NormalTok{, naive\_search), (}\StringTok{"KMP"}\NormalTok{, kmp\_search), (}\StringTok{"Boyer–Moore"}\NormalTok{, bm\_search)]:}
\NormalTok{    start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{    pos }\OperatorTok{=}\NormalTok{ func(text, pattern)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{ found at }\SpecialCharTok{\{}\NormalTok{pos}\SpecialCharTok{\}}\SpecialStringTok{, time }\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(time.time()}\OperatorTok{{-}}\NormalTok{start,}\DecValTok{4}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{s"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-9}

\begin{itemize}
\tightlist
\item
  Naive: Slow for large repetitive text.
\item
  KMP: Linear time, handles worst case well.
\item
  Boyer--Moore: Fastest for natural text (skips ahead aggressively).
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1486}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2027}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1892}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4595}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Worst Case Time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best Case Time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Naive & O(n·m) & O(n) & Simple, bad for long patterns \\
KMP & O(n + m) & O(n + m) & Guaranteed linear time \\
Boyer--Moore & O(n·m) worst & Sublinear avg & Very fast in practice on
real text \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-54}

\begin{itemize}
\tightlist
\item
  Search engines, text editors, and databases rely on efficient
  substring search.
\item
  Naive methods break down at scale.
\item
  KMP shows theory guiding better algorithms.
\item
  Boyer--Moore demonstrates practical engineering that beats naive even
  further.
\end{itemize}

\subsection{Exercises}\label{exercises-53}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement naive substring search in any language.
\item
  Search for \texttt{"abc"} in \texttt{"aaaabc"} and explain why naive
  repeats work.
\item
  Build the prefix table for \texttt{"ababaca"} in KMP.
\item
  Explain how KMP avoids re-checking characters.
\item
  Implement Boyer--Moore with the bad character rule.
\item
  Show why Boyer--Moore jumps ahead more than one character on
  mismatches.
\item
  Benchmark naive vs KMP vs BM on 1MB text.
\item
  Compare results for random text vs repetitive text.
\item
  Explain why Boyer--Moore worst-case is still O(n·m).
\item
  Propose a hybrid search strategy for DNA sequences (small alphabet,
  long text).
\end{enumerate}

\section{LAB 11: Regex Engine Catastrophe
(ReDoS)}\label{lab-11-regex-engine-catastrophe-redos}

\subsection{Goal}\label{goal-10}

See how certain regex patterns cause catastrophic backtracking and turn
tiny inputs into huge runtimes (Regex Denial of Service---ReDoS). Learn
safe alternatives and mitigation strategies.

\subsection{Setup}\label{setup-10}

Any language with a backtracking engine will reproduce the issue
(Python/PCRE/Perl/JavaScript). An automata-based engine (e.g., RE2,
Rust's default) won't exhibit catastrophic behavior for the same
patterns.

Test patterns that are notorious:

\begin{itemize}
\tightlist
\item
  Nested, overlapping quantifiers: \texttt{(a+)+b}
\item
  Ambiguous alternation with stars: \texttt{(a\textbar{}aa)+b}
\item
  Catastrophic HTMLish: \texttt{(\textless{}.+\textgreater{})+}
\end{itemize}

\subsection{Step-by-Step}\label{step-by-step-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reproduce a Fast Match

  \begin{itemize}
  \tightlist
  \item
    Pattern: \texttt{(a+)+b}
  \item
    Input: \texttt{"a"*25\ +\ "b"}
  \item
    Expect: quick success (engine finds the trailing \texttt{b}).
  \end{itemize}
\item
  Trigger Catastrophe

  \begin{itemize}
  \tightlist
  \item
    Same pattern: \texttt{(a+)+b}
  \item
    Input: \texttt{"a"*25} (no \texttt{b})
  \item
    Expect: very slow---engine explores exponential backtracking paths.
  \end{itemize}
\item
  Measure Impact

  \begin{itemize}
  \tightlist
  \item
    Time both inputs.
  \item
    Increase input length stepwise (e.g., 15, 20, 25, 30 \texttt{a}s).
  \item
    Observe runtime explosion for the no-\texttt{b} case.
  \end{itemize}
\item
  Try Another Problematic Pattern

  \begin{itemize}
  \tightlist
  \item
    Pattern: \texttt{(a\textbar{}aa)+b}
  \item
    Inputs: \texttt{"a"*N+"b"} (fast) and \texttt{"a"*N} (slow).
  \item
    Note that overlapping alternatives also trigger exponential blowups.
  \end{itemize}
\item
  Mitigate

  \begin{itemize}
  \item
    Refactor to unambiguous patterns:

    \begin{itemize}
    \tightlist
    \item
      Replace \texttt{(a+)+b} with \texttt{a+\ b}-style constructions
      that avoid nested quantifiers, or use possessive/atomic
      quantifiers if supported (e.g., \texttt{a++b},
      \texttt{(?\textgreater{}a+)b}).
    \end{itemize}
  \item
    Constrain with anchors and character classes to reduce search space.
  \item
    Precompile and timeout: set execution time limits (if supported).
  \item
    Use safe engines (RE2/Hyperscan) for untrusted input.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-9}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re, time}

\KeywordTok{def}\NormalTok{ timed\_match(pattern, text, flags}\OperatorTok{=}\DecValTok{0}\NormalTok{):}
\NormalTok{    start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{    ok }\OperatorTok{=} \BuiltInTok{bool}\NormalTok{(re.match(pattern, text, flags))}
\NormalTok{    dur }\OperatorTok{=}\NormalTok{ time.time() }\OperatorTok{{-}}\NormalTok{ start}
    \ControlFlowTok{return}\NormalTok{ ok, dur}

\CommentTok{\# 1) Fast match: pattern matches because of trailing \textquotesingle{}b\textquotesingle{}}
\NormalTok{pat }\OperatorTok{=} \VerbatimStringTok{r"}\KeywordTok{(}\VerbatimStringTok{a}\OperatorTok{+}\KeywordTok{)}\OperatorTok{+}\VerbatimStringTok{b"}
\NormalTok{fast\_text }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{25} \OperatorTok{+} \StringTok{"b"}
\NormalTok{ok, dur }\OperatorTok{=}\NormalTok{ timed\_match(pat, fast\_text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"FAST   {-}\textgreater{} match:"}\NormalTok{, ok, }\StringTok{"time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(dur, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\CommentTok{\# 2) Catastrophic: same pattern, missing the final \textquotesingle{}b\textquotesingle{}}
\NormalTok{slow\_text }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*} \DecValTok{25}
\NormalTok{ok, dur }\OperatorTok{=}\NormalTok{ timed\_match(pat, slow\_text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"SLOW   {-}\textgreater{} match:"}\NormalTok{, ok, }\StringTok{"time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(dur, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\CommentTok{\# 3) Scale up to see blowup}
\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ [}\DecValTok{20}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{28}\NormalTok{]:}
\NormalTok{    txt }\OperatorTok{=} \StringTok{"a"} \OperatorTok{*}\NormalTok{ n}
\NormalTok{    ok, dur }\OperatorTok{=}\NormalTok{ timed\_match(pat, txt)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"N=}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{:\textless{}2\}}\SpecialStringTok{ {-}\textgreater{} match:}\SpecialCharTok{\{}\NormalTok{ok}\SpecialCharTok{\}}\SpecialStringTok{ time:}\SpecialCharTok{\{}\BuiltInTok{round}\NormalTok{(dur, }\DecValTok{4}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{ s"}\NormalTok{)}

\CommentTok{\# 4) Another problematic pattern}
\NormalTok{pat2 }\OperatorTok{=} \VerbatimStringTok{r"}\KeywordTok{(}\VerbatimStringTok{a}\ControlFlowTok{|}\VerbatimStringTok{aa}\KeywordTok{)}\OperatorTok{+}\VerbatimStringTok{b"}
\NormalTok{ok, dur }\OperatorTok{=}\NormalTok{ timed\_match(pat2, }\StringTok{"a"} \OperatorTok{*} \DecValTok{25} \OperatorTok{+} \StringTok{"b"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ALT OK {-}\textgreater{} match:"}\NormalTok{, ok, }\StringTok{"time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(dur, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\NormalTok{ok, dur }\OperatorTok{=}\NormalTok{ timed\_match(pat2, }\StringTok{"a"} \OperatorTok{*} \DecValTok{25}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"ALT SLOW{-}\textgreater{} match:"}\NormalTok{, ok, }\StringTok{"time:"}\NormalTok{, }\BuiltInTok{round}\NormalTok{(dur, }\DecValTok{4}\NormalTok{), }\StringTok{"s"}\NormalTok{)}

\CommentTok{\# 5) Safer alternative using atomic/possessive (if your engine supports it)}
\CommentTok{\# Python\textquotesingle{}s \textquotesingle{}re\textquotesingle{} does not support atomic groups/possessive quantifiers.}
\CommentTok{\# Pseudocode examples for other engines:}
\CommentTok{\#   {-} PCRE/Java: r"(?\textgreater{}a+)b" or r"a++b"}
\CommentTok{\# For Python, restructure logic instead of relying on engine features.}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-10}

\begin{itemize}
\tightlist
\item
  With trailing \texttt{b}, matches are fast.
\item
  Without \texttt{b}, runtime grows rapidly as \texttt{N} increases.
\item
  Overlapping alternations show the same pattern of slowdown.
\item
  Atomic/possessive quantifiers (in engines that support them) remove
  backtracking and restore predictable performance.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1412}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2118}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3294}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1882}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Engine Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{(a+)+b} & \texttt{a…ab} & Backtracking & Fast & \\
\texttt{(a+)+b} & \texttt{a…a} & Backtracking & Catastrophic (very slow)
& \\
`(a & aa)+b` & \texttt{a…ab} & Backtracking & Fast \\
`(a & aa)+b` & \texttt{a…a} & Backtracking & Catastrophic \\
\texttt{(?\textgreater{}a+)b} & \texttt{a…a?} & Atomic (PCRE/Java) &
Predictable, no catastrophe & \\
\texttt{a++b} & \texttt{a…a?} & Possessive & Predictable & \\
RE2 version & any of above & DFA/NFA (safe) & Linear-time, no
catastrophe & \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-55}

\begin{itemize}
\tightlist
\item
  Untrusted input + fragile regex = DoS vector.
\item
  Web apps, API gateways, and log pipelines often apply regex to
  attacker-controlled text.
\item
  The fix is architectural (pick safe engines) and design-oriented
  (write non-ambiguous patterns, set timeouts).
\end{itemize}

\subsection{Exercises}\label{exercises-54}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain why \texttt{(a+)+b} is dangerous on inputs composed only of
  \texttt{a}s.
\item
  Show a timing table for \texttt{(a+)+b} against \texttt{a\^{}N} for N
  = 10, 15, 20, 25.
\item
  Construct another catastrophic pattern using overlapping alternation
  (e.g., \texttt{(ab\textbar{}a)*b}).
\item
  Rewrite \texttt{(a\textbar{}aa)+b} into a pattern that avoids
  ambiguity while matching the same language.
\item
  Describe how atomic groups or possessive quantifiers prevent
  backtracking.
\item
  Propose engine-agnostic mitigations: anchoring, limiting input length,
  pre-validation.
\item
  Design a test harness that detects suspicious regex (runtime or
  backtracking depth spikes).
\item
  Compare behavior of the same pattern in a backtracking engine vs a
  linear-time engine (e.g., RE2).
\item
  For a production route-matching regex, list safeguards to prevent
  ReDoS in a web server.
\item
  Outline a policy for your organization: when to allow
  backreferences/lookbehinds; when to mandate RE2-class engines.
\end{enumerate}

\section{LAB 12: Greedy vs Lazy
Matching}\label{lab-12-greedy-vs-lazy-matching}

\subsection{Goal}\label{goal-11}

See how regex quantifiers (\texttt{*}, \texttt{+}, \texttt{?}) behave in
greedy mode (match as much as possible) versus lazy mode (match as
little as possible). Learn when greedy matching leads to over-capture
and how lazy mode fixes it.

\subsection{Setup}\label{setup-11}

Regex engines default to greedy quantifiers:

\begin{itemize}
\tightlist
\item
  \texttt{.*} matches the longest possible string.
\item
  Adding \texttt{?} makes them lazy: \texttt{.*?} matches the shortest
  possible string.
\end{itemize}

We'll use Python's \texttt{re} module.

\subsection{Step-by-Step}\label{step-by-step-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simple Greedy Match

  \begin{itemize}
  \tightlist
  \item
    Pattern:
    \texttt{\textless{}tag\textgreater{}.*\textless{}/tag\textgreater{}}
  \item
    Input:
    \texttt{"\textless{}tag\textgreater{}one\textless{}/tag\textgreater{}\textless{}tag\textgreater{}two\textless{}/tag\textgreater{}"}
  \item
    Greedy captures from the first \texttt{\textless{}tag\textgreater{}}
    to the last \texttt{\textless{}/tag\textgreater{}}, swallowing too
    much.
  \end{itemize}
\item
  Switch to Lazy

  \begin{itemize}
  \tightlist
  \item
    Pattern:
    \texttt{\textless{}tag\textgreater{}.*?\textless{}/tag\textgreater{}}
  \item
    Same input.
  \item
    Lazy captures each
    \texttt{\textless{}tag\textgreater{}…\textless{}/tag\textgreater{}}
    pair separately, as expected.
  \end{itemize}
\item
  Experiment with Plus Quantifier

  \begin{itemize}
  \tightlist
  \item
    Compare
    \texttt{\textless{}tag\textgreater{}.+\textless{}/tag\textgreater{}}
    vs
    \texttt{\textless{}tag\textgreater{}.+?\textless{}/tag\textgreater{}}.
  \item
    \texttt{+} requires at least one character, \texttt{*} allows zero.
  \end{itemize}
\item
  Realistic Example: HTML-ish Parsing

  \begin{itemize}
  \tightlist
  \item
    Input:
    \texttt{"\textless{}b\textgreater{}bold\textless{}/b\textgreater{}\textless{}i\textgreater{}italic\textless{}/i\textgreater{}"}.
  \item
    Greedy regex: \texttt{\textless{}.*\textgreater{}} → captures
    everything between first \texttt{\textless{}} and last
    \texttt{\textgreater{}}.
  \item
    Lazy regex: \texttt{\textless{}.*?\textgreater{}} → captures
    \texttt{\textless{}b\textgreater{}},
    \texttt{\textless{}/b\textgreater{}},
    \texttt{\textless{}i\textgreater{}},
    \texttt{\textless{}/i\textgreater{}} separately.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Greedy matching is fine when only one block exists.
  \item
    Lazy matching is safer for repeated structures.
  \item
    Regex is not a full parser; overuse leads to fragile code.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-10}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}

\NormalTok{text }\OperatorTok{=} \StringTok{"\textless{}tag\textgreater{}one\textless{}/tag\textgreater{}\textless{}tag\textgreater{}two\textless{}/tag\textgreater{}"}

\CommentTok{\# 1. Greedy match}
\NormalTok{greedy }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{*}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy:"}\NormalTok{, greedy)}

\CommentTok{\# 2. Lazy match}
\NormalTok{lazy }\OperatorTok{=}\NormalTok{ re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{*?}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Lazy:"}\NormalTok{, lazy)}

\CommentTok{\# 3. Greedy vs lazy with plus quantifier}
\NormalTok{sample }\OperatorTok{=} \StringTok{"\textless{}tag\textgreater{}a\textless{}/tag\textgreater{}\textless{}tag\textgreater{}b\textless{}/tag\textgreater{}"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy +:"}\NormalTok{, re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{+}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, sample))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Lazy +?:"}\NormalTok{, re.findall(}\VerbatimStringTok{r"\textless{}tag\textgreater{}}\DecValTok{.}\OperatorTok{+?}\VerbatimStringTok{\textless{}/tag\textgreater{}"}\NormalTok{, sample))}

\CommentTok{\# 4. HTML{-}ish example}
\NormalTok{html }\OperatorTok{=} \StringTok{"\textless{}b\textgreater{}bold\textless{}/b\textgreater{}\textless{}i\textgreater{}italic\textless{}/i\textgreater{}"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Greedy HTML:"}\NormalTok{, re.findall(}\VerbatimStringTok{r"\textless{}}\DecValTok{.}\OperatorTok{*}\VerbatimStringTok{\textgreater{}"}\NormalTok{, html))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Lazy HTML:"}\NormalTok{, re.findall(}\VerbatimStringTok{r"\textless{}}\DecValTok{.}\OperatorTok{*?}\VerbatimStringTok{\textgreater{}"}\NormalTok{, html))}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-11}

\begin{itemize}
\tightlist
\item
  Greedy consumes everything between first and last match.
\item
  Lazy matches each block separately.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4524}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Output
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{\textless{}tag\textgreater{}.*\textless{}/tag\textgreater{}} &
\texttt{\textless{}tag\textgreater{}one\textless{}/tag\textgreater{}\textless{}tag\textgreater{}two\textless{}/tag\textgreater{}}
&
\texttt{{[}\textquotesingle{}\textless{}tag\textgreater{}one\textless{}/tag\textgreater{}\textless{}tag\textgreater{}two\textless{}/tag\textgreater{}\textquotesingle{}{]}} \\
\texttt{\textless{}tag\textgreater{}.*?\textless{}/tag\textgreater{}} &
\texttt{\textless{}tag\textgreater{}one\textless{}/tag\textgreater{}\textless{}tag\textgreater{}two\textless{}/tag\textgreater{}}
&
\texttt{{[}\textquotesingle{}\textless{}tag\textgreater{}one\textless{}/tag\textgreater{}\textquotesingle{},\ \textquotesingle{}\textless{}tag\textgreater{}two\textless{}/tag\textgreater{}\textquotesingle{}{]}} \\
\texttt{\textless{}.*\textgreater{}} (greedy) &
\texttt{\textless{}b\textgreater{}bold\textless{}/b\textgreater{}\textless{}i\textgreater{}italic\textless{}/i\textgreater{}}
&
\texttt{{[}\textquotesingle{}\textless{}b\textgreater{}bold\textless{}/b\textgreater{}\textless{}i\textgreater{}italic\textless{}/i\textgreater{}\textquotesingle{}{]}} \\
\texttt{\textless{}.*?\textgreater{}} (lazy) &
\texttt{\textless{}b\textgreater{}bold\textless{}/b\textgreater{}\textless{}i\textgreater{}italic\textless{}/i\textgreater{}}
&
\texttt{{[}\textquotesingle{}\textless{}b\textgreater{}\textquotesingle{},\ \textquotesingle{}\textless{}/b\textgreater{}\textquotesingle{},\ \textquotesingle{}\textless{}i\textgreater{}\textquotesingle{},\ \textquotesingle{}\textless{}/i\textgreater{}\textquotesingle{}{]}} \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-56}

\begin{itemize}
\tightlist
\item
  Greedy vs lazy matching changes how much text is captured.
\item
  Incorrect choice can swallow entire documents or miss intended
  matches.
\item
  Useful in log parsing, HTML scraping, and template matching.
\item
  Shows why regex is powerful but also error-prone when used without
  care.
\end{itemize}

\subsection{Exercises}\label{exercises-55}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a regex to capture
  \texttt{\textless{}tag\textgreater{}…\textless{}/tag\textgreater{}} in
  \texttt{"\textless{}tag\textgreater{}a\textless{}/tag\textgreater{}\textless{}tag\textgreater{}b\textless{}/tag\textgreater{}"}.
  Compare greedy vs lazy.
\item
  Show why \texttt{\textless{}.*\textgreater{}} over-captures in
  \texttt{"\textless{}p\textgreater{}hi\textless{}/p\textgreater{}\textless{}p\textgreater{}bye\textless{}/p\textgreater{}"}.
\item
  Modify \texttt{\textless{}.*\textgreater{}} into
  \texttt{\textless{}.*?\textgreater{}} and explain the difference.
\item
  Use \texttt{.+?} to capture non-empty blocks. Show how it differs from
  \texttt{.*?}.
\item
  Write a regex that extracts
  \texttt{\textless{}b\textgreater{}…\textless{}/b\textgreater{}} text
  only.
\item
  Compare results of
  \texttt{\textless{}div\textgreater{}.*\textless{}/div\textgreater{}}
  on a file with nested \texttt{\textless{}div\textgreater{}} tags.
\item
  Explain why lazy quantifiers may still produce unexpected results in
  deeply nested structures.
\item
  Benchmark \texttt{.*} vs \texttt{.*?} on a 1MB HTML file.
\item
  Show how greedy vs lazy affects performance (number of backtracking
  steps).
\item
  Explain why full HTML parsing should not rely solely on regex.
\end{enumerate}

\section{LAB 13: Build a Mini Inverted
Index}\label{lab-13-build-a-mini-inverted-index}

\subsection{Goal}\label{goal-12}

Implement a simplified inverted index, the core data structure behind
search engines. Learn how to map words to the documents they appear in,
then use the index to answer queries quickly.

\subsection{Setup}\label{setup-12}

An inverted index stores entries like:

\begin{verbatim}
"dog" → [doc1, doc3]  
"cat" → [doc2]  
\end{verbatim}

Instead of scanning every document, you look up the word in the index.
We'll implement this in Python, but the idea applies to any language or
system.

\subsection{Step-by-Step}\label{step-by-step-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Prepare Documents

  \begin{itemize}
  \item
    Create a small collection of texts, each with an ID.
  \item
    Example:

    \begin{itemize}
    \tightlist
    \item
      \texttt{1:\ "the\ quick\ brown\ fox"}
    \item
      \texttt{2:\ "the\ lazy\ dog"}
    \item
      \texttt{3:\ "the\ fox\ jumped\ over\ the\ dog"}
    \end{itemize}
  \end{itemize}
\item
  Tokenize Documents

  \begin{itemize}
  \tightlist
  \item
    Split text into lowercase words.
  \item
    Remove punctuation (simplified tokenizer).
  \end{itemize}
\item
  Build Inverted Index

  \begin{itemize}
  \tightlist
  \item
    For each word, append the document ID to its list.
  \item
    Use a dictionary (map) of word → set of doc IDs.
  \end{itemize}
\item
  Query the Index

  \begin{itemize}
  \tightlist
  \item
    Look up a word and return all documents containing it.
  \item
    Extend to multi-word queries by intersecting sets.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Inverted index allows fast search compared to scanning.
  \item
    Real search engines add ranking (TF-IDF, BM25), phrase search, and
    indexing optimizations.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-11}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ defaultdict}
\ImportTok{import}\NormalTok{ re}

\CommentTok{\# 1. Documents}
\NormalTok{docs }\OperatorTok{=}\NormalTok{ \{}
    \DecValTok{1}\NormalTok{: }\StringTok{"the quick brown fox"}\NormalTok{,}
    \DecValTok{2}\NormalTok{: }\StringTok{"the lazy dog"}\NormalTok{,}
    \DecValTok{3}\NormalTok{: }\StringTok{"the fox jumped over the dog"}
\NormalTok{\}}

\CommentTok{\# 2. Tokenize}
\KeywordTok{def}\NormalTok{ tokenize(text):}
    \ControlFlowTok{return}\NormalTok{ re.findall(}\VerbatimStringTok{r"}\DecValTok{\textbackslash{}w}\OperatorTok{+}\VerbatimStringTok{"}\NormalTok{, text.lower())}

\CommentTok{\# 3. Build inverted index}
\NormalTok{index }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{set}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ doc\_id, text }\KeywordTok{in}\NormalTok{ docs.items():}
    \ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ tokenize(text):}
\NormalTok{        index[word].add(doc\_id)}

\CommentTok{\# 4. Query}
\KeywordTok{def}\NormalTok{ search(word):}
    \ControlFlowTok{return}\NormalTok{ index.get(word.lower(), }\BuiltInTok{set}\NormalTok{())}

\KeywordTok{def}\NormalTok{ search\_multi(words):}
\NormalTok{    sets }\OperatorTok{=}\NormalTok{ [search(w) }\ControlFlowTok{for}\NormalTok{ w }\KeywordTok{in}\NormalTok{ words]}
    \ControlFlowTok{return} \BuiltInTok{set}\NormalTok{.intersection(}\OperatorTok{*}\NormalTok{sets) }\ControlFlowTok{if}\NormalTok{ sets }\ControlFlowTok{else} \BuiltInTok{set}\NormalTok{()}

\CommentTok{\# Example queries}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Index for \textquotesingle{}fox\textquotesingle{}:"}\NormalTok{, index[}\StringTok{"fox"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Search \textquotesingle{}dog\textquotesingle{}:"}\NormalTok{, search(}\StringTok{"dog"}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Search \textquotesingle{}fox AND dog\textquotesingle{}:"}\NormalTok{, search\_multi([}\StringTok{"fox"}\NormalTok{, }\StringTok{"dog"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-12}

\begin{itemize}
\tightlist
\item
  Index for \texttt{"fox"} → \texttt{\{1,\ 3\}}
\item
  Search \texttt{"dog"} → \texttt{\{2,\ 3\}}
\item
  Search \texttt{"fox\ AND\ dog"} → \texttt{\{3\}}
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Word & Document IDs \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
the & \{1, 2, 3\} \\
quick & \{1\} \\
brown & \{1\} \\
fox & \{1, 3\} \\
lazy & \{2\} \\
dog & \{2, 3\} \\
jumped & \{3\} \\
over & \{3\} \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-57}

\begin{itemize}
\tightlist
\item
  This is the foundation of Google, Lucene, Elasticsearch and every
  major search system.
\item
  Inverted indexes make keyword search efficient.
\item
  They scale from a few documents to billions.
\item
  Concepts here connect to IR (Information Retrieval) theory.
\end{itemize}

\subsection{Exercises}\label{exercises-56}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build an inverted index for 5 custom sentences.
\item
  Search for a single word and list document IDs.
\item
  Implement AND search: return docs containing all words.
\item
  Implement OR search: return docs containing at least one word.
\item
  Extend the tokenizer to remove stop words like \texttt{"the"},
  \texttt{"and"}.
\item
  Modify the index to count word frequency in each document.
\item
  Add a simple ranking: prefer documents with more matches.
\item
  Test the system with queries on \texttt{"cat"} when no doc contains
  \texttt{"cat"}.
\item
  Explain why inverted indexes are better than scanning.
\item
  Design how you would scale this to 1 million documents.
\end{enumerate}

\section{LAB 14: Compression of Repetitive
Strings}\label{lab-14-compression-of-repetitive-strings}

\subsection{Goal}\label{goal-13}

Understand how string compression reduces storage by exploiting
repetition. Learn simple dictionary-based compression, compare sizes
with and without compression, and see how algorithms like gzip or
Huffman coding achieve savings.

\subsection{Setup}\label{setup-13}

We'll simulate a simple compression scheme:

\begin{itemize}
\tightlist
\item
  Naive storage: keep the string as-is.
\item
  Dictionary-based compression: replace repeated substrings with
  references.
\item
  Compare results with Python's \texttt{zlib} (which implements DEFLATE,
  combining LZ77 + Huffman coding).
\end{itemize}

Test input:

\begin{verbatim}
"banana banana banana"
\end{verbatim}

\subsection{Step-by-Step}\label{step-by-step-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Uncompressed Storage

  \begin{itemize}
  \tightlist
  \item
    Measure length of original string in bytes.
  \end{itemize}
\item
  Naive Dictionary Compression

  \begin{itemize}
  \tightlist
  \item
    Store \texttt{"banana"} once.
  \item
    Represent repeated \texttt{"banana"}s as references.
  \item
    Calculate compressed size (dictionary + references).
  \end{itemize}
\item
  zlib Compression

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{zlib.compress()} to compress.
  \item
    Compare compressed size with original.
  \end{itemize}
\item
  Test with Different Inputs

  \begin{itemize}
  \tightlist
  \item
    Highly repetitive: \texttt{"abc\ abc\ abc\ abc"} → compresses well.
  \item
    Random: \texttt{"qwertyuiop"} → compresses poorly.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    Compression works best when redundancy exists.
  \item
    Dictionary + entropy coding are the backbone of text and file
    compression.
  \item
    Trade-off: compression saves space but costs CPU time.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-12}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ zlib}

\CommentTok{\# 1. Original string}
\NormalTok{text }\OperatorTok{=} \StringTok{"banana banana banana"}
\NormalTok{original\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(text.encode(}\StringTok{"utf{-}8"}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original text:"}\NormalTok{, text)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original size:"}\NormalTok{, original\_size, }\StringTok{"bytes"}\NormalTok{)}

\CommentTok{\# 2. Naive dictionary compression simulation}
\NormalTok{dictionary }\OperatorTok{=}\NormalTok{ \{}\StringTok{"banana"}\NormalTok{: }\DecValTok{1}\NormalTok{\}}
\NormalTok{tokens }\OperatorTok{=}\NormalTok{ [dictionary.get(word, word) }\ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ text.split()]}
\NormalTok{compressed\_repr }\OperatorTok{=}\NormalTok{ (dictionary, tokens)}
\NormalTok{simulated\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(}\StringTok{"banana"}\NormalTok{) }\OperatorTok{+} \BuiltInTok{len}\NormalTok{(tokens)  }\CommentTok{\# rough size}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Naive simulated compressed size:"}\NormalTok{, simulated\_size, }\StringTok{"bytes"}\NormalTok{)}

\CommentTok{\# 3. zlib compression}
\NormalTok{compressed }\OperatorTok{=}\NormalTok{ zlib.compress(text.encode(}\StringTok{"utf{-}8"}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"zlib compressed size:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(compressed), }\StringTok{"bytes"}\NormalTok{)}

\CommentTok{\# 4. Compare with random string}
\NormalTok{random\_text }\OperatorTok{=} \StringTok{"qwertyuiop"}
\NormalTok{compressed\_random }\OperatorTok{=}\NormalTok{ zlib.compress(random\_text.encode(}\StringTok{"utf{-}8"}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Random original size:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(random\_text), }\StringTok{"bytes"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Random compressed size:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(compressed\_random), }\StringTok{"bytes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-13}

\begin{itemize}
\tightlist
\item
  Original: \textasciitilde20 bytes.
\item
  Naive dictionary scheme: fewer bytes than original.
\item
  zlib: compressed size significantly smaller (e.g., 20 → 17).
\item
  Random text: compressed size ≈ original (sometimes slightly larger due
  to overhead).
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Input & Original Size & Compressed Size \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{"banana\ banana\ banana"} & 20 bytes & \textasciitilde17
bytes \\
\texttt{"abc\ abc\ abc\ abc"} & 15 bytes & \textasciitilde13 bytes \\
\texttt{"qwertyuiop"} & 10 bytes & \textasciitilde12 bytes (overhead) \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-58}

\begin{itemize}
\tightlist
\item
  Compression underpins storage (zip files, databases, backups) and
  transmission (HTTP gzip, messaging).
\item
  Saves bandwidth and storage by removing redundancy.
\item
  Shows trade-offs: repetitive text compresses well, random text does
  not.
\item
  Foundation for deeper algorithms: Huffman coding, LZ77, arithmetic
  coding.
\end{itemize}

\subsection{Exercises}\label{exercises-57}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute original and compressed sizes of
  \texttt{"hello\ hello\ hello"}.
\item
  Compare compression ratio for \texttt{"aaaaa"} vs \texttt{"abcde"}.
\item
  Write a function that stores unique words in a dictionary and replaces
  repeats with indexes.
\item
  Extend your dictionary to store \texttt{"banana"} as key \texttt{1}
  and \texttt{"apple"} as key \texttt{2}. Compress
  \texttt{"banana\ apple\ banana"}.
\item
  Measure zlib compression ratio for a large string of repeated
  \texttt{"test"}.
\item
  Generate random strings of length 100 and test compression. What
  happens?
\item
  Explain why compression sometimes increases size on short random
  inputs.
\item
  Compare zlib compression with \texttt{bz2} or \texttt{lzma} in Python.
\item
  Explain how compression is used in network protocols like HTTP/2.
\item
  Propose how compression could be applied in search engine indexes
  (from LAB 13).
\end{enumerate}

\section{LAB 15: Injection Attack
Simulation}\label{lab-15-injection-attack-simulation}

\subsection{Goal}\label{goal-14}

Demonstrate how insecure string handling in queries can lead to
injection attacks (e.g., SQL injection). Show how malicious inputs
manipulate string-based queries, and how parameterized queries prevent
this.

\subsection{Setup}\label{setup-14}

Any system that builds queries with string concatenation is vulnerable.
We'll simulate SQL injection in Python:

\begin{itemize}
\tightlist
\item
  Unsafe example: concatenating user input directly into a query string.
\item
  Safe example: using parameterized queries (\texttt{?} or
  \texttt{\%s}).
\end{itemize}

We'll use \texttt{sqlite3} for demonstration, since it's lightweight and
built into Python.

\subsection{Step-by-Step}\label{step-by-step-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a Sample Database

  \begin{itemize}
  \tightlist
  \item
    Table: \texttt{accounts(name\ TEXT,\ balance\ INT)}.
  \item
    Insert rows: \texttt{"Alice",\ 100}, \texttt{"Bob",\ 50}.
  \end{itemize}
\item
  Unsafe Query

  \begin{itemize}
  \item
    Take user input for name.
  \item
    Build query with string concatenation:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ accounts }\KeywordTok{WHERE}\NormalTok{ name }\OperatorTok{=} \StringTok{\textquotesingle{}USER\_INPUT\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}
  \item
    Input \texttt{"Alice"} → works fine.
  \item
    Input
    \texttt{"\textquotesingle{}\ OR\ \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}
    → query returns all rows (attack).
  \end{itemize}
\item
  Simulated SQL Injection Attack

  \begin{itemize}
  \tightlist
  \item
    Input \texttt{";\ DROP\ TABLE\ accounts;\ -\/-"} → destructive
    query.
  \item
    In real systems, this would delete the table.
  \end{itemize}
\item
  Safe Query with Parameters

  \begin{itemize}
  \tightlist
  \item
    Use
    \texttt{cursor.execute("SELECT\ *\ FROM\ accounts\ WHERE\ name\ =\ ?",\ (user\_input,))}.
  \item
    Input
    \texttt{"\textquotesingle{}\ OR\ \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}
    → safely treated as a literal string, no injection.
  \end{itemize}
\item
  Discussion

  \begin{itemize}
  \tightlist
  \item
    String concatenation = dangerous.
  \item
    Parameterization = safe.
  \item
    This applies to SQL, shell commands, HTML, and beyond.
  \end{itemize}
\end{enumerate}

\subsection{Example (Python)}\label{example-python-13}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sqlite3}

\CommentTok{\# 1. Setup}
\NormalTok{conn }\OperatorTok{=}\NormalTok{ sqlite3.}\ExtensionTok{connect}\NormalTok{(}\StringTok{":memory:"}\NormalTok{)}
\NormalTok{c }\OperatorTok{=}\NormalTok{ conn.cursor()}
\NormalTok{c.execute(}\StringTok{"CREATE TABLE accounts (name TEXT, balance INT)"}\NormalTok{)}
\NormalTok{c.executemany(}\StringTok{"INSERT INTO accounts VALUES (?, ?)"}\NormalTok{, [(}\StringTok{"Alice"}\NormalTok{, }\DecValTok{100}\NormalTok{), (}\StringTok{"Bob"}\NormalTok{, }\DecValTok{50}\NormalTok{)])}
\NormalTok{conn.commit()}

\CommentTok{\# 2. Unsafe query}
\KeywordTok{def}\NormalTok{ unsafe\_query(user\_input):}
\NormalTok{    query }\OperatorTok{=} \SpecialStringTok{f"SELECT * FROM accounts WHERE name = \textquotesingle{}}\SpecialCharTok{\{}\NormalTok{user\_input}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}"}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Unsafe query:"}\NormalTok{, query)}
    \ControlFlowTok{return} \BuiltInTok{list}\NormalTok{(c.execute(query))}

\CommentTok{\# Safe query}
\KeywordTok{def}\NormalTok{ safe\_query(user\_input):}
\NormalTok{    query }\OperatorTok{=} \StringTok{"SELECT * FROM accounts WHERE name = ?"}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Safe query:"}\NormalTok{, query)}
    \ControlFlowTok{return} \BuiltInTok{list}\NormalTok{(c.execute(query, (user\_input,)))}

\CommentTok{\# Normal input}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Normal unsafe:"}\NormalTok{, unsafe\_query(}\StringTok{"Alice"}\NormalTok{))}

\CommentTok{\# Injection attack}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Injection unsafe:"}\NormalTok{, unsafe\_query(}\StringTok{"\textquotesingle{} OR \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}\NormalTok{))}

\CommentTok{\# Safe version prevents injection}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Safe version:"}\NormalTok{, safe\_query(}\StringTok{"\textquotesingle{} OR \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsection{Expected Results}\label{expected-results-14}

\begin{itemize}
\tightlist
\item
  Normal input \texttt{"Alice"} works fine.
\item
  Injection input
  \texttt{"\textquotesingle{}\ OR\ \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}
  returns all rows in unsafe query.
\item
  Safe query treats it literally → no injection.
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Input & Unsafe Query Result & Safe Query Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{"Alice"} & \texttt{{[}("Alice",\ 100){]}} &
\texttt{{[}("Alice",\ 100){]}} \\
\texttt{"\textquotesingle{}\ OR\ \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}
& \texttt{{[}("Alice",100),("Bob",50){]}} & \texttt{{[}{]}} (no
match) \\
\end{longtable}

\subsection{Why it matters}\label{why-it-matters-59}

\begin{itemize}
\tightlist
\item
  SQL injection is one of the most critical web vulnerabilities (OWASP
  Top 10).
\item
  Attackers can dump data, bypass authentication, or destroy databases.
\item
  The problem is not SQL itself but string misuse.
\item
  Fix: always use parameterized queries or ORMs with safe bindings.
\end{itemize}

\subsection{Exercises}\label{exercises-58}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a query vulnerable to injection and test with \texttt{"Alice"}.
\item
  Inject
  \texttt{"\textquotesingle{}\ OR\ \textquotesingle{}1\textquotesingle{}=\textquotesingle{}1"}
  and explain why it returns all rows.
\item
  Try input \texttt{";\ DROP\ TABLE\ accounts;\ -\/-"} and explain what
  would happen in a real DB.
\item
  Rewrite your code to use parameterized queries.
\item
  Test injection again with safe queries and verify no effect.
\item
  Extend this idea to shell commands (\texttt{os.system}) and simulate
  command injection.
\item
  Research one real-world breach caused by SQL injection.
\item
  Explain why escaping input manually (e.g., replacing quotes) is
  insufficient.
\item
  Design input validation rules that complement parameterization.
\item
  Propose a security guideline for handling strings in queries for your
  organization.
\end{enumerate}




\end{document}
