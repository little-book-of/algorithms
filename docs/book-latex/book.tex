% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Little Book of Algorithms},
  pdfauthor={Duc-Tam Nguyen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Little Book of Algorithms}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 0.1.1}
\author{Duc-Tam Nguyen}
\date{2025-09-11}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Roadmap}\label{roadmap}

The Little Book of Algorithms is a multi-volume project. Each volume has
a clear sequence of chapters, and each chapter has three levels of depth
(L0 beginner intuition, L1 practical techniques, L2 advanced
systems/theory). This roadmap outlines the plan for development and
publication.

\section{Goals}\label{goals}

\begin{itemize}
\tightlist
\item
  Establish a consistent layered structure across all chapters.
\item
  Provide runnable implementations in Python, C, Go, Erlang, and Lean.
\item
  Ensure Quarto build supports HTML, PDF, EPUB, and LaTeX.
\item
  Deliver both pedagogy (L0) and production insights (L2).
\end{itemize}

\section{Volumes}\label{volumes}

\subsection{Volume I - Structures
Linéaires}\label{volume-i---structures-linuxe9aires}

\begin{itemize}
\tightlist
\item
  Chapter 0 - Foundations
\item
  Chapter 1 - Numbers
\item
  Chapter 2 - Arrays
\item
  Chapter 3 - Strings
\item
  Chapter 4 - Linked Lists
\item
  Chapter 5 - Stacks \& Queues
\end{itemize}

\subsection{Volume II - Algorithmes
Fondamentaux}\label{volume-ii---algorithmes-fondamentaux}

\begin{itemize}
\tightlist
\item
  Chapter 6 - Searching
\item
  Chapter 7 - Selection
\item
  Chapter 8 - Sorting
\item
  Chapter 9 - Amortized Analysis
\end{itemize}

\subsection{Volume III - Structures
Hiérarchiques}\label{volume-iii---structures-hiuxe9rarchiques}

\begin{itemize}
\tightlist
\item
  Chapter 10 - Tree Fundamentals
\item
  Chapter 11 - Heaps \& Priority Queues
\item
  Chapter 12 - Binary Search Trees
\item
  Chapter 13 - Balanced Trees \& Ordered Maps
\item
  Chapter 14 - Range Queries
\item
  Chapter 15 - Vector Databases
\end{itemize}

\subsection{Volume IV - Paradigmes
Algorithmiques}\label{volume-iv---paradigmes-algorithmiques}

\begin{itemize}
\tightlist
\item
  Chapter 16 - Divide-and-Conquer
\item
  Chapter 17 - Greedy
\item
  Chapter 18 - Dynamic Programming
\item
  Chapter 19 - Backtracking \& Search
\end{itemize}

\subsection{Volume V - Graphes et
Complexité}\label{volume-v---graphes-et-complexituxe9}

\begin{itemize}
\tightlist
\item
  Chapter 20 - Graph Basics
\item
  Chapter 21 - DAGs \& SCC
\item
  Chapter 22 - Shortest Paths
\item
  Chapter 23 - Flows \& Matchings
\item
  Chapter 24 - Tree Algorithms
\item
  Chapter 25 - Complexity \& Limits
\item
  Chapter 26 - External \& Cache-Oblivious
\item
  Chapter 27 - Probabilistic \& Streaming
\item
  Chapter 28 - Engineering
\end{itemize}

\section{Milestones}\label{milestones}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Complete detailed outlines for all chapters (L0, L1, L2).
\item
  Write draft text for all L0 sections (intuition, analogies, simple
  examples).
\item
  Expand each chapter with L1 content (implementations, correctness
  arguments, exercises).
\item
  Add L2 content (systems insights, proofs, optimizations, advanced
  references).
\item
  Develop and test runnable code in \texttt{src/} across Python, C, Go,
  Erlang, and Lean.
\item
  Integrate diagrams, figures, and visual explanations.
\item
  Finalize Quarto build setup for HTML, PDF, and EPUB.
\item
  Release first public edition (HTML + PDF).
\item
  Add LaTeX build, refine EPUB, and polish cross-references.
\item
  Publish on GitHub Pages and archive DOI.
\item
  Gather feedback, refine explanations, and expand exercises/problem
  sets.
\item
  Long-term: maintain as a living reference with continuous updates and
  companion volumes.
\end{enumerate}

\section{Deliverables}\label{deliverables}

\begin{itemize}
\tightlist
\item
  Quarto project with 29 chapters (00--28).
\item
  Multi-language reference implementations.
\item
  Learning matrix in README for navigation.
\item
  ROADMAP.md (this file) to track progress.
\end{itemize}

\section{Long-term Vision}\label{long-term-vision}

\begin{itemize}
\tightlist
\item
  Maintain the repository as a living reference.
\item
  Extend with exercises, problem sets, and quizzes.
\item
  Build a dependency map across volumes for prerequisites.
\item
  Connect to companion ``Little Book'' series (linear algebra, calculus,
  probability).
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Chapter 1. Numbers}\label{chapter-1.-numbers}

\section{1.1 Representation}\label{representation}

\subsection{1.1 L0. Decimal and Binary
Basics}\label{l0.-decimal-and-binary-basics}

A number representation is a way of writing numbers using symbols and
positional rules. Humans typically use decimal notation, while computers
rely on binary because it aligns with the two-state nature of electronic
circuits. Understanding both systems is the first step in connecting
mathematical intuition with machine computation.

\subsubsection{Numbers in Everyday Life}\label{numbers-in-everyday-life}

Humans work with the decimal system (base 10), which uses digits 0
through 9. Each position in a number has a place value that is a power
of 10.

\[
427 = 4 \times 10^2 + 2 \times 10^1 + 7 \times 10^0
\]

This principle of \emph{positional notation} is the same idea used in
other bases.

\subsubsection{Numbers in Computers}\label{numbers-in-computers}

Computers, however, operate in binary (base 2). A binary digit (bit) can
only be 0 or 1, matching the two stable states of electronic circuits
(off/on). Each binary place value represents a power of 2.

\[
1011_2 = 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0 = 11_{10}
\]

Just like in decimal where \(9 + 1 = 10\), in binary \(1 + 1 = 10_2\).

\subsubsection{Conversion Between Decimal and
Binary}\label{conversion-between-decimal-and-binary}

To convert from decimal to binary, repeatedly divide the number by 2 and
record the remainders. Then read the remainders from bottom to top.

Example: Convert \(42_{10}\) into binary.

\begin{itemize}
\tightlist
\item
  42 ÷ 2 = 21 remainder 0
\item
  21 ÷ 2 = 10 remainder 1
\item
  10 ÷ 2 = 5 remainder 0
\item
  5 ÷ 2 = 2 remainder 1
\item
  2 ÷ 2 = 1 remainder 0
\item
  1 ÷ 2 = 0 remainder 1
\end{itemize}

Reading upward: \(101010_2\).

To convert from binary to decimal, expand into powers of 2 and sum:

\[
101010_2 = 1 \times 2^5 + 0 \times 2^4 + 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 0 \times 2^0 = 42_{10}
\]

\subsubsection{Worked Example (Python)}\label{worked-example-python}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{42}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Decimal:"}\NormalTok{, n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binary :"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(n))   }\CommentTok{\# 0b101010}

\CommentTok{\# binary literal in Python}
\NormalTok{b }\OperatorTok{=} \BaseNTok{0b101010}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binary literal:"}\NormalTok{, b)}

\CommentTok{\# converting binary string to decimal}
\BuiltInTok{print}\NormalTok{(}\StringTok{"From binary \textquotesingle{}1011\textquotesingle{}:"}\NormalTok{, }\BuiltInTok{int}\NormalTok{(}\StringTok{"1011"}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Output:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Decimal:}\NormalTok{ 42}
\ExtensionTok{Binary}\NormalTok{ : 0b101010}
\ExtensionTok{Binary}\NormalTok{ literal: 42}
\ExtensionTok{From}\NormalTok{ binary }\StringTok{\textquotesingle{}1011\textquotesingle{}}\NormalTok{: 11}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters}

\begin{itemize}
\tightlist
\item
  All information inside a computer --- numbers, text, images, programs
  --- reduces to binary representation.
\item
  Decimal and binary conversions are the first bridge between
  human-friendly math and machine-level data.
\item
  Understanding binary is essential for debugging, low-level
  programming, and algorithms that depend on bit operations.
\end{itemize}

\subsubsection{Exercises}\label{exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write the decimal number 19 in binary.
\item
  Convert the binary number \texttt{10101₂} into decimal.
\item
  Show the repeated division steps to convert 27 into binary.
\item
  Verify in Python that \texttt{0b111111} equals 63.
\item
  Explain why computers use binary instead of decimal.
\end{enumerate}

\subsection{1.1 L1. Beyond Binary: Octal, Hex, and Two's
Complement}\label{l1.-beyond-binary-octal-hex-and-twos-complement}

Numbers are not always written in base-10 or even in base-2. For
efficiency and compactness, programmers often use octal (base-8) and
hexadecimal (base-16). At the same time, negative numbers must be
represented reliably; modern computers use two's complement for this
purpose.

\subsubsection{Octal and Hexadecimal}\label{octal-and-hexadecimal}

Octal and hex are simply alternate numeral systems.

\begin{itemize}
\tightlist
\item
  Octal (base 8): digits \texttt{0}--\texttt{7}.
\item
  Hexadecimal (base 16): digits \texttt{0}--\texttt{9} plus
  \texttt{A}--\texttt{F}.
\end{itemize}

Why they matter:

\begin{itemize}
\tightlist
\item
  Hex is concise: one hex digit = 4 binary bits.
\item
  Octal was historically convenient: one octal digit = 3 binary bits
  (useful on early 12-, 24-, or 36-bit machines).
\end{itemize}

For example, the number 42 is written as:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Decimal & Binary & Octal & Hex \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
42 & 101010 & 52 & 2A \\
\end{longtable}

\subsubsection{Two's Complement}\label{twos-complement}

To represent negative numbers, we cannot just ``stick a minus sign'' in
memory. Instead, binary uses two's complement:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a fixed bit-width (say 8 bits).
\item
  For a negative number \texttt{-x}, compute \texttt{2\^{}bits\ -\ x}.
\item
  Store the result as an ordinary binary integer.
\end{enumerate}

Example with 8 bits:

\begin{itemize}
\tightlist
\item
  \texttt{+5} → \texttt{00000101}
\item
  \texttt{-5} → \texttt{11111011}
\item
  \texttt{-1} → \texttt{11111111}
\end{itemize}

Why two's complement is powerful:

\begin{itemize}
\tightlist
\item
  Addition and subtraction ``just work'' with the same circuitry for
  signed and unsigned.
\item
  There is only one representation of zero.
\end{itemize}

\subsubsection{Working Example (Python)}\label{working-example-python}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Decimal 42 in different bases}
\NormalTok{n }\OperatorTok{=} \DecValTok{42}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Decimal:"}\NormalTok{, n)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Binary :"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(n))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Octal  :"}\NormalTok{, }\BuiltInTok{oct}\NormalTok{(n))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Hex    :"}\NormalTok{, }\BuiltInTok{hex}\NormalTok{(n))}

\CommentTok{\# Two\textquotesingle{}s complement for {-}5 in 8 bits}
\KeywordTok{def}\NormalTok{ to\_twos\_complement(x: }\BuiltInTok{int}\NormalTok{, bits: }\BuiltInTok{int} \OperatorTok{=} \DecValTok{8}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{str}\NormalTok{:}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return} \BuiltInTok{format}\NormalTok{(x, }\SpecialStringTok{f"0}\SpecialCharTok{\{}\NormalTok{bits}\SpecialCharTok{\}}\SpecialStringTok{b"}\NormalTok{)}
    \ControlFlowTok{return} \BuiltInTok{format}\NormalTok{((}\DecValTok{1} \OperatorTok{\textless{}\textless{}}\NormalTok{ bits) }\OperatorTok{+}\NormalTok{ x, }\SpecialStringTok{f"0}\SpecialCharTok{\{}\NormalTok{bits}\SpecialCharTok{\}}\SpecialStringTok{b"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"+5:"}\NormalTok{, to\_twos\_complement(}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}5:"}\NormalTok{, to\_twos\_complement(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Output:

\begin{verbatim}
Decimal: 42
Binary : 0b101010
Octal  : 0o52
Hex    : 0x2a
+5: 00000101
-5: 11111011
\end{verbatim}

\subsubsection{Why It Matters}\label{why-it-matters-1}

\begin{itemize}
\tightlist
\item
  Programmer convenience: Hex makes binary compact and human-readable.
\item
  Hardware design: Two's complement ensures arithmetic circuits are
  simple and unified.
\item
  Debugging: Memory dumps, CPU registers, and network packets are
  usually shown in hex.
\end{itemize}

\subsubsection{Exercises}\label{exercises-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convert \texttt{100} into binary, octal, and hex.
\item
  Write \texttt{-7} in 8-bit two's complement.
\item
  Verify that \texttt{0xFF} is equal to 255.
\item
  Parse the bitstring \texttt{"11111001"} as an 8-bit two's complement
  number.
\item
  Explain why engineers prefer two's complement over ``sign-magnitude''
  representation.
\end{enumerate}

\subsection{1.1 L2. Floating-Point and Precision
Issues}\label{l2.-floating-point-and-precision-issues}

Not all numbers are integers. To approximate fractions, scientific
notation, and very large or very small values, computers use
floating-point representation. The de-facto standard is IEEE-754, which
defines how real numbers are encoded, how special values are handled,
and what precision guarantees exist.

\subsubsection{Structure of Floating-Point
Numbers}\label{structure-of-floating-point-numbers}

A floating-point value is composed of three fields:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sign bit (s) --- indicates positive (\texttt{0}) or negative
  (\texttt{1}).
\item
  Exponent (e) --- determines the scale or ``magnitude.''
\item
  Mantissa / significand (m) --- contains the significant digits.
\end{enumerate}

The value is interpreted as:

\[
(-1)^s \times 1.m \times 2^{(e - \text{bias})}
\]

Example: IEEE-754 single precision (32 bits)

\begin{itemize}
\tightlist
\item
  1 sign bit
\item
  8 exponent bits (bias = 127)
\item
  23 mantissa bits
\end{itemize}

\subsubsection{Exact vs Approximate
Representation}\label{exact-vs-approximate-representation}

Some numbers are represented exactly:

\begin{itemize}
\tightlist
\item
  \texttt{1.0} has a clean binary form.
\end{itemize}

Others cannot be represented precisely:

\begin{itemize}
\tightlist
\item
  \texttt{0.1} in decimal is a repeating fraction in binary, so the
  closest approximation is stored.
\end{itemize}

Python example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=} \FloatTok{0.1} \OperatorTok{+} \FloatTok{0.2}
\BuiltInTok{print}\NormalTok{(}\StringTok{"0.1 + 0.2 ="}\NormalTok{, a)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Equal to 0.3?"}\NormalTok{, a }\OperatorTok{==} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Output:

\begin{verbatim}
0.1 + 0.2 = 0.30000000000000004
Equal to 0.3? False
\end{verbatim}

\subsubsection{Special Values}\label{special-values}

IEEE-754 reserves encodings for special cases:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Sign & Exponent & Mantissa & Meaning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0/1 & all 1s & 0 & +∞ / −∞ \\
0/1 & all 1s & nonzero & NaN (Not a Number) \\
0/1 & all 0s & nonzero & Denormals (gradual underflow) \\
\end{longtable}

Examples:

\begin{itemize}
\tightlist
\item
  Division by zero produces infinity: \texttt{1.0\ /\ 0.0\ =\ inf}.
\item
  \texttt{0.0\ /\ 0.0} yields \texttt{NaN}, which propagates in
  computations.
\item
  Denormals allow gradual precision near zero.
\end{itemize}

\subsubsection{Arbitrary Precision}\label{arbitrary-precision}

Languages like Python and libraries like GMP provide arbitrary-precision
arithmetic:

\begin{itemize}
\tightlist
\item
  Integers (\texttt{int}) can grow as large as memory allows.
\item
  Decimal libraries (\texttt{decimal.Decimal} in Python) allow exact
  decimal arithmetic.
\item
  These are slower, but essential for cryptography, symbolic
  computation, and finance.
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Infinity:"}\NormalTok{, }\FloatTok{1.0} \OperatorTok{/} \FloatTok{0.0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NaN:"}\NormalTok{, }\FloatTok{0.0} \OperatorTok{/} \FloatTok{0.0}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Is NaN?"}\NormalTok{, math.isnan(}\BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}nan\textquotesingle{}}\NormalTok{)))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Is Inf?"}\NormalTok{, math.isinf(}\BuiltInTok{float}\NormalTok{(}\StringTok{\textquotesingle{}inf\textquotesingle{}}\NormalTok{)))}

\CommentTok{\# Arbitrary precision integer}
\NormalTok{big }\OperatorTok{=} \DecValTok{2200}
\BuiltInTok{print}\NormalTok{(}\StringTok{"2200 ="}\NormalTok{, big)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-2}

\begin{itemize}
\tightlist
\item
  Rounding surprises: Many decimal fractions cannot be represented
  exactly.
\item
  Error propagation: Repeated arithmetic may accumulate tiny
  inaccuracies.
\item
  Special values: NaN and infinity must be handled carefully.
\item
  Domain correctness: Cryptography, finance, and symbolic algebra
  require exact precision.
\end{itemize}

\subsubsection{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down the IEEE-754 representation (sign, exponent, mantissa) of
  \texttt{1.0}.
\item
  Explain why \texttt{0.1} is not exactly representable in binary.
\item
  Test in Python whether
  \texttt{float(\textquotesingle{}nan\textquotesingle{})\ ==\ float(\textquotesingle{}nan\textquotesingle{})}.
  What happens, and why?
\item
  Find the smallest positive number you can add to \texttt{1.0} before
  it changes (machine epsilon).
\item
  Why is arbitrary precision slower but critical in some applications?
\end{enumerate}

\section{1.2 Basic Operations}\label{basic-operations}

\subsection{1.2 L0. Addition, Subtraction, Multiplication,
Division}\label{l0.-addition-subtraction-multiplication-division}

An arithmetic operation combines numbers to produce a new number. At
this level we focus on four basics: addition, subtraction,
multiplication, and division---first with decimal intuition, then a peek
at how the same ideas look in binary. Mastering these is essential
before moving to algorithms that build on them.

\subsubsection{Intuition: place value +
carrying/borrowing}\label{intuition-place-value-carryingborrowing}

All four operations are versions of combining place values (ones, tens,
hundreds \ldots; or in binary: ones, twos, fours \ldots).

\begin{itemize}
\tightlist
\item
  Addition: add column by column; if a column exceeds the base, carry 1
  to the next column.
\item
  Subtraction: subtract column by column; if a column is too small,
  borrow 1 from the next column.
\item
  Multiplication: repeated addition; multiply by each digit and shift
  (place value), then add partial results.
\item
  Division: repeated subtraction or sharing; find how many times a
  number ``fits,'' track the remainder.
\end{itemize}

These rules are identical in any base. Only the place values change.

\subsubsection{Decimal examples (by
hand)}\label{decimal-examples-by-hand}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Addition (carry)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{478}
 \ExtensionTok{+}\NormalTok{ 259}
 \ExtensionTok{{-}{-}{-}{-}}
   \ExtensionTok{737}    \ErrorTok{(}\ExtensionTok{8+9=17}\NormalTok{ → write 7, carry 1}\KeywordTok{;} \ExtensionTok{7+5+1=13}\NormalTok{ → write 3, carry 1}\KeywordTok{;} \ExtensionTok{4+2+1=7}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Subtraction (borrow)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{503}
 \ExtensionTok{{-}}\NormalTok{  78}
 \ExtensionTok{{-}{-}{-}{-}}
   \ExtensionTok{425}    \ErrorTok{(}\ExtensionTok{3{-}8}\NormalTok{ borrow → 13{-}8=5}\KeywordTok{;} \ExtensionTok{0}\NormalTok{ became }\AttributeTok{{-}1}\NormalTok{ so borrow from 5 → 9{-}7=2}\KeywordTok{;} \ExtensionTok{4}\NormalTok{ stays 4}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Multiplication (partial sums)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{214}
 \ExtensionTok{×}\NormalTok{   3}
 \ExtensionTok{{-}{-}{-}{-}}
   \ExtensionTok{642}    \ErrorTok{(}\ExtensionTok{214*3}\NormalTok{ = 642}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Long division (quotient + remainder)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
  \ExtensionTok{47}\NormalTok{ ÷ 5 → 9 remainder 2   }\ErrorTok{(}\ExtensionTok{because}\NormalTok{ 5}\PreprocessorTok{*}\NormalTok{9 = 45, leftover 2}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Binary peek (same rules, base
2)}\label{binary-peek-same-rules-base-2}

\begin{itemize}
\item
  Add rules: 0+0=0, 0+1=1, 1+0=1, 1+1=10₂ (write 0, carry 1)
\item
  Subtract rules: 0−0=0, 1−0=1, 1−1=0, 0−1 → borrow (becomes 10₂−1=1,
  borrow 1)
\end{itemize}

Example: \(1011₂ + 0110₂\)

\begin{Shaded}
\begin{Highlighting}[]
   \ExtensionTok{1011}
 \ExtensionTok{+}\NormalTok{ 0110}
 \ExtensionTok{{-}{-}{-}{-}{-}{-}}
  \ExtensionTok{10001}   \ErrorTok{(}\ExtensionTok{1+0=1}\KeywordTok{;} \ExtensionTok{1+1=0}\NormalTok{ carry1}\KeywordTok{;} \ExtensionTok{0+1+carry=0}\NormalTok{ carry1}\KeywordTok{;} \ExtensionTok{1+0+carry=0}\NormalTok{ carry1 → carry out}\KeywordTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Worked examples (Python)}\label{worked-examples-python}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Basic arithmetic with integers}
\NormalTok{a, b }\OperatorTok{=} \DecValTok{478}\NormalTok{, }\DecValTok{259}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a+b ="}\NormalTok{, a }\OperatorTok{+}\NormalTok{ b)      }\CommentTok{\# 737}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a{-}b ="}\NormalTok{, a }\OperatorTok{{-}}\NormalTok{ b)      }\CommentTok{\# 219}
\BuiltInTok{print}\NormalTok{(}\StringTok{"a*b ="}\NormalTok{, a }\OperatorTok{*}\NormalTok{ b)      }\CommentTok{\# 123,  478*259 = 123,  ... actually compute:}
\BuiltInTok{print}\NormalTok{(}\StringTok{"47//5 ="}\NormalTok{, }\DecValTok{47} \OperatorTok{//} \DecValTok{5}\NormalTok{)  }\CommentTok{\# integer division {-}\textgreater{} 9}
\BuiltInTok{print}\NormalTok{(}\StringTok{"47\%5  ="}\NormalTok{, }\DecValTok{47} \OperatorTok{\%} \DecValTok{5}\NormalTok{)   }\CommentTok{\# remainder {-}\textgreater{} 2}

\CommentTok{\# Show carry/borrow intuition using binary strings}
\NormalTok{x, y }\OperatorTok{=} \BaseNTok{0b1011}\NormalTok{, }\BaseNTok{0b0110}
\NormalTok{s }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
\BuiltInTok{print}\NormalTok{(}\StringTok{"x+y (binary):"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(x), }\StringTok{"+"}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(y), }\StringTok{"="}\NormalTok{, }\BuiltInTok{bin}\NormalTok{(s))}

\CommentTok{\# Small helper: manual long division that returns (quotient, remainder)}
\KeywordTok{def}\NormalTok{ long\_divide(n: }\BuiltInTok{int}\NormalTok{, d: }\BuiltInTok{int}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ d }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ZeroDivisionError}\NormalTok{(}\StringTok{"division by zero"}\NormalTok{)}
\NormalTok{    q }\OperatorTok{=}\NormalTok{ n }\OperatorTok{//}\NormalTok{ d}
\NormalTok{    r }\OperatorTok{=}\NormalTok{ n }\OperatorTok{\%}\NormalTok{ d}
    \ControlFlowTok{return}\NormalTok{ q, r}

\BuiltInTok{print}\NormalTok{(}\StringTok{"long\_divide(47,5):"}\NormalTok{, long\_divide(}\DecValTok{47}\NormalTok{, }\DecValTok{5}\NormalTok{))  }\CommentTok{\# (9, 2)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: \texttt{//} is integer division in Python; \texttt{\%} is the
remainder. For now we focus on integers (no decimals).
\end{quote}

\subsubsection{Why it matters}\label{why-it-matters-3}

\begin{itemize}
\tightlist
\item
  Every higher-level algorithm (searching, hashing, cryptography,
  numeric methods) relies on these operations.
\item
  Understanding carry/borrow makes binary arithmetic and bit-level
  reasoning feel natural.
\item
  Knowing integer division and remainder is vital for base conversions,
  hashing (\texttt{mod}), and many algorithmic patterns.
\end{itemize}

\subsubsection{Exercises}\label{exercises-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute by hand, then verify in Python:

  \begin{itemize}
  \tightlist
  \item
    \(326 + 589\)
  \item
    \(704 - 259\)
  \item
    \(38 \times 12\)
  \item
    \(123 \div 7\) (give quotient and remainder)
  \end{itemize}
\item
  In binary, add \(10101₂ + 111₍₂₎\). Show carries.
\item
  Write a short Python snippet that prints the quotient and remainder
  for \texttt{n=200} divided by \texttt{d=23}.
\item
  Convert your remainder into a sentence: ``200 = 23 × (quotient) +
  (remainder)''.
\item
  Challenge: Multiply \(19 \times 23\) by hand using partial sums; then
  check with Python.
\end{enumerate}

\subsection{1.2 L1. Division, Modulo, and
Efficiency}\label{l1.-division-modulo-and-efficiency}

Beyond the simple four arithmetic operations, programmers need to think
about division with remainder, the modulo operator, and how efficient
these operations are on real machines. Addition and subtraction are
almost always ``constant time,'' but division can be slower, and
understanding modulo is essential for algorithms like hashing,
cryptography, and scheduling.

\subsubsection{Integer Division and
Modulo}\label{integer-division-and-modulo}

For integers, division produces both a quotient and a remainder.

\begin{itemize}
\item
  Mathematical definition: for integers \(n, d\) with \(d \neq 0\),

  \[
  n = d \times q + r, \quad 0 \leq r < |d|
  \]

  where \(q\) is the quotient, \(r\) the remainder.
\item
  Programming notation (Python):

  \begin{itemize}
  \tightlist
  \item
    \texttt{n\ //\ d} → quotient
  \item
    \texttt{n\ \%\ d} → remainder
  \end{itemize}
\end{itemize}

Examples:

\begin{itemize}
\tightlist
\item
  \texttt{47\ //\ 5\ =\ 9}, \texttt{47\ \%\ 5\ =\ 2} because
  \(47 = 5 \times 9 + 2\).
\item
  \texttt{23\ //\ 7\ =\ 3}, \texttt{23\ \%\ 7\ =\ 2} because
  \(23 = 7 \times 3 + 2\).
\end{itemize}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
n & d & n // d & n \% d \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
47 & 5 & 9 & 2 \\
23 & 7 & 3 & 2 \\
100 & 9 & 11 & 1 \\
\end{longtable}

\subsubsection{Modulo in Algorithms}\label{modulo-in-algorithms}

The modulo operation is a workhorse in programming:

\begin{itemize}
\item
  Hashing: To map a large integer into a table of size \texttt{m}, use
  \texttt{key\ \%\ m}.
\item
  Cyclic behavior: To loop back after \texttt{7} days in a week:
  \texttt{(day\ +\ shift)\ \%\ 7}.
\item
  Cryptography: Modular arithmetic underlies RSA, Diffie--Hellman, and
  many number-theoretic algorithms.
\end{itemize}

\subsubsection{Efficiency
Considerations}\label{efficiency-considerations}

\begin{itemize}
\tightlist
\item
  Addition and subtraction: generally 1 CPU cycle.
\item
  Multiplication: slightly more expensive, but still fast on modern
  hardware.
\item
  Division and modulo: slower, often an order of magnitude more costly
  than multiplication.
\end{itemize}

Practical tricks:

\begin{itemize}
\item
  If \texttt{d} is a power of two, \texttt{n\ \%\ d} can be computed by
  a bitmask.

  \begin{itemize}
  \tightlist
  \item
    Example: \texttt{n\ \%\ 8\ ==\ n\ \&\ 7} (since 8 = 2³).
  \end{itemize}
\item
  Some compilers automatically optimize modulo when the divisor is
  constant.
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Quotient and remainder}
\NormalTok{n, d }\OperatorTok{=} \DecValTok{47}\NormalTok{, }\DecValTok{5}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Quotient:"}\NormalTok{, n }\OperatorTok{//}\NormalTok{ d)  }\CommentTok{\# 9}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Remainder:"}\NormalTok{, n }\OperatorTok{\%}\NormalTok{ d)  }\CommentTok{\# 2}

\CommentTok{\# Identity check: n == d*q + r}
\NormalTok{q, r }\OperatorTok{=} \BuiltInTok{divmod}\NormalTok{(n, d)  }\CommentTok{\# built{-}in tuple return}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Check:"}\NormalTok{, d}\OperatorTok{*}\NormalTok{q }\OperatorTok{+}\NormalTok{ r }\OperatorTok{==}\NormalTok{ n)}

\CommentTok{\# Modulo for cyclic behavior: days of week}
\NormalTok{days }\OperatorTok{=}\NormalTok{ [}\StringTok{"Mon"}\NormalTok{, }\StringTok{"Tue"}\NormalTok{, }\StringTok{"Wed"}\NormalTok{, }\StringTok{"Thu"}\NormalTok{, }\StringTok{"Fri"}\NormalTok{, }\StringTok{"Sat"}\NormalTok{, }\StringTok{"Sun"}\NormalTok{]}
\NormalTok{start }\OperatorTok{=} \DecValTok{5}  \CommentTok{\# Saturday}
\NormalTok{shift }\OperatorTok{=} \DecValTok{4}
\NormalTok{future\_day }\OperatorTok{=}\NormalTok{ days[(start }\OperatorTok{+}\NormalTok{ shift) }\OperatorTok{\%} \DecValTok{7}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Start Saturday + 4 days ="}\NormalTok{, future\_day)}

\CommentTok{\# Optimization: power{-}of{-}two modulo with bitmask}
\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in}\NormalTok{ [}\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{20}\NormalTok{]:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{n}\SpecialCharTok{\}}\SpecialStringTok{ \% 8 = }\SpecialCharTok{\{}\NormalTok{n }\OperatorTok{\%} \DecValTok{8}\SpecialCharTok{\}}\SpecialStringTok{, bitmask }\SpecialCharTok{\{}\NormalTok{n }\OperatorTok{\&} \DecValTok{7}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Output:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Quotient:}\NormalTok{ 9}
\ExtensionTok{Remainder:}\NormalTok{ 2}
\ExtensionTok{Check:}\NormalTok{ True}
\ExtensionTok{Start}\NormalTok{ Saturday + 4 days = Wed}
\ExtensionTok{5}\NormalTok{ \% 8 = 5, bitmask 5}
\ExtensionTok{12}\NormalTok{ \% 8 = 4, bitmask 4}
\ExtensionTok{20}\NormalTok{ \% 8 = 4, bitmask 4}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-4}

\begin{itemize}
\tightlist
\item
  Real programs rely heavily on modulo for indexing, hashing, and
  wrap-around logic.
\item
  Division is computationally more expensive; knowing when to replace it
  with bit-level operations improves performance.
\item
  Modular arithmetic introduces a new ``world'' where numbers wrap
  around --- the foundation of many advanced algorithms.
\end{itemize}

\subsubsection{Exercises}\label{exercises-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute by hand and confirm in Python:

  \begin{itemize}
  \tightlist
  \item
    \texttt{100\ //\ 9} and \texttt{100\ \%\ 9}
  \item
    \texttt{123\ //\ 11} and \texttt{123\ \%\ 11}
  \end{itemize}
\item
  Write a function that simulates a clock: given \texttt{hour} and
  \texttt{shift}, return the new hour (24-hour cycle).
\item
  Prove the identity: for any integers \texttt{n} and \texttt{d},

\begin{verbatim}
n == d * (n // d) + (n % d)
\end{verbatim}

  by trying with random values.
\item
  Show how to replace \texttt{n\ \%\ 16} with a bitwise operation. Why
  does it work?
\item
  Challenge: Write a short Python function to check if a number is
  divisible by 7 using only \texttt{\%} and \texttt{//}.
\end{enumerate}

\subsection{1.2 L2. Fast Arithmetic
Algorithms}\label{l2.-fast-arithmetic-algorithms}

When numbers grow large, the naïve methods for multiplication and
division become too slow. On paper, long multiplication takes \(O(n^2)\)
steps for \(n\)-digit numbers. Computers face the same issue:
multiplying two very large integers digit by digit can be expensive.
Fast arithmetic algorithms reduce this cost, using clever
divide-and-conquer techniques or transformations into other domains.

\subsubsection{Multiplication Beyond the School
Method}\label{multiplication-beyond-the-school-method}

Naïve long multiplication

\begin{itemize}
\tightlist
\item
  Treats an \(n\)-digit number as a sequence of digits.
\item
  Each digit of one number multiplies every digit of the other.
\item
  Complexity: \(O(n^2)\).
\item
  Works fine for small integers, but too slow for cryptography or
  big-number libraries.
\end{itemize}

Karatsuba's Algorithm

\begin{itemize}
\item
  Discovered in 1960 by Anatoly Karatsuba.
\item
  Idea: split numbers into halves and reduce multiplications.
\item
  Complexity: \(O(n^{\log_2 3}) \approx O(n^{1.585})\).
\item
  Recursive strategy:

  \begin{itemize}
  \item
    For numbers \(x = x_1 \cdot B^m + x_0\),
    \(y = y_1 \cdot B^m + y_0\).
  \item
    Compute 3 multiplications instead of 4:

    \begin{itemize}
    \tightlist
    \item
      \(z_0 = x_0 y_0\)
    \item
      \(z_2 = x_1 y_1\)
    \item
      \(z_1 = (x_0+x_1)(y_0+y_1) - z_0 - z_2\)
    \end{itemize}
  \item
    Result: \(z_2 \cdot B^{2m} + z_1 \cdot B^m + z_0\).
  \end{itemize}
\end{itemize}

FFT-based Multiplication (Schönhage--Strassen and successors)

\begin{itemize}
\tightlist
\item
  Represent numbers as polynomials of their digits.
\item
  Multiply polynomials efficiently using Fast Fourier Transform.
\item
  Complexity: near \(O(n \log n)\).
\item
  Used in modern big-integer libraries (e.g.~GNU MP, Java's
  \texttt{BigInteger}).
\end{itemize}

\subsubsection{Division Beyond Long
Division}\label{division-beyond-long-division}

\begin{itemize}
\tightlist
\item
  Naïve long division: \(O(n^2)\) for \(n\)-digit dividend.
\item
  Newton's method for reciprocal: approximate \(1/d\) using
  Newton--Raphson iterations, then multiply by \(n\).
\item
  Complexity: tied to multiplication --- if multiplication is fast, so
  is division.
\end{itemize}

\subsubsection{Modular Exponentiation}\label{modular-exponentiation}

Fast arithmetic also matters in modular contexts (cryptography).

\begin{itemize}
\item
  Compute \(a^b \bmod m\) efficiently.
\item
  Square-and-multiply (binary exponentiation):

  \begin{itemize}
  \tightlist
  \item
    Write \(b\) in binary.
  \item
    For each bit: square result, multiply if bit=1.
  \item
    Complexity: \(O(\log b)\) multiplications.
  \end{itemize}
\end{itemize}

\subsubsection{Worked Example (Python)}\label{worked-example-python-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Naïve multiplication}
\KeywordTok{def}\NormalTok{ naive\_mul(x: }\BuiltInTok{int}\NormalTok{, y: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y  }\CommentTok{\# Python already uses fast methods internally}

\CommentTok{\# Karatsuba multiplication (recursive, simplified)}
\KeywordTok{def}\NormalTok{ karatsuba(x: }\BuiltInTok{int}\NormalTok{, y: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \CommentTok{\# base case}
    \ControlFlowTok{if}\NormalTok{ x }\OperatorTok{\textless{}} \DecValTok{10} \KeywordTok{or}\NormalTok{ y }\OperatorTok{\textless{}} \DecValTok{10}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y}
    \CommentTok{\# split numbers}
\NormalTok{    n }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(x.bit\_length(), y.bit\_length())}
\NormalTok{    m }\OperatorTok{=}\NormalTok{ n }\OperatorTok{//} \DecValTok{2}
\NormalTok{    high1, low1 }\OperatorTok{=} \BuiltInTok{divmod}\NormalTok{(x, }\DecValTok{1} \OperatorTok{\textless{}\textless{}}\NormalTok{ m)}
\NormalTok{    high2, low2 }\OperatorTok{=} \BuiltInTok{divmod}\NormalTok{(y, }\DecValTok{1} \OperatorTok{\textless{}\textless{}}\NormalTok{ m)}
\NormalTok{    z0 }\OperatorTok{=}\NormalTok{ karatsuba(low1, low2)}
\NormalTok{    z2 }\OperatorTok{=}\NormalTok{ karatsuba(high1, high2)}
\NormalTok{    z1 }\OperatorTok{=}\NormalTok{ karatsuba(low1 }\OperatorTok{+}\NormalTok{ high1, low2 }\OperatorTok{+}\NormalTok{ high2) }\OperatorTok{{-}}\NormalTok{ z0 }\OperatorTok{{-}}\NormalTok{ z2}
    \ControlFlowTok{return}\NormalTok{ (z2 }\OperatorTok{\textless{}\textless{}}\NormalTok{ (}\DecValTok{2}\OperatorTok{*}\NormalTok{m)) }\OperatorTok{+}\NormalTok{ (z1 }\OperatorTok{\textless{}\textless{}}\NormalTok{ m) }\OperatorTok{+}\NormalTok{ z0}

\CommentTok{\# Modular exponentiation (square{-}and{-}multiply)}
\KeywordTok{def}\NormalTok{ modexp(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{, m: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    result }\OperatorTok{=} \DecValTok{1}
\NormalTok{    base }\OperatorTok{=}\NormalTok{ a }\OperatorTok{\%}\NormalTok{ m}
\NormalTok{    exp }\OperatorTok{=}\NormalTok{ b}
    \ControlFlowTok{while}\NormalTok{ exp }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{if}\NormalTok{ exp }\OperatorTok{\&} \DecValTok{1}\NormalTok{:}
\NormalTok{            result }\OperatorTok{=}\NormalTok{ (result }\OperatorTok{*}\NormalTok{ base) }\OperatorTok{\%}\NormalTok{ m}
\NormalTok{        base }\OperatorTok{=}\NormalTok{ (base }\OperatorTok{*}\NormalTok{ base) }\OperatorTok{\%}\NormalTok{ m}
\NormalTok{        exp }\OperatorTok{\textgreater{}\textgreater{}=} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ result}

\CommentTok{\# Demo}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Karatsuba(1234, 5678) ="}\NormalTok{, karatsuba(}\DecValTok{1234}\NormalTok{, }\DecValTok{5678}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"pow(7, 128, 13) ="}\NormalTok{, modexp(}\DecValTok{7}\NormalTok{, }\DecValTok{128}\NormalTok{, }\DecValTok{13}\NormalTok{))  }\CommentTok{\# fast modular exponentiation}
\end{Highlighting}
\end{Shaded}

Output:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Karatsuba}\ErrorTok{(}\ExtensionTok{1234,}\NormalTok{ 5678}\KeywordTok{)} \ExtensionTok{=}\NormalTok{ 7006652}
\ExtensionTok{pow}\ErrorTok{(}\ExtensionTok{7,}\NormalTok{ 128, 13}\KeywordTok{)} \ExtensionTok{=}\NormalTok{ 3}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-5}

\begin{itemize}
\tightlist
\item
  Cryptography: RSA requires multiplying and dividing integers with
  thousands of digits.
\item
  Computer algebra systems: symbolic computation depends on fast
  polynomial/integer arithmetic.
\item
  Big data / simulation: arbitrary precision needed when floats are not
  exact.
\end{itemize}

\subsubsection{Exercises}\label{exercises-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Multiply 31415926 × 27182818 using:

  \begin{itemize}
  \tightlist
  \item
    Python's \texttt{*}
  \item
    Your Karatsuba implementation. Compare results.
  \end{itemize}
\item
  Implement \texttt{modexp(a,\ b,\ m)} for \(a=5, b=117, m=19\). Confirm
  with Python's built-in \texttt{pow(a,\ b,\ m)}.
\item
  Explain why Newton's method for division depends on fast
  multiplication.
\item
  Research: what is the current fastest known multiplication algorithm
  for large integers?
\item
  Challenge: Modify Karatsuba to print intermediate
  \texttt{z0,\ z1,\ z2} values for small inputs to visualize the
  recursion.
\end{enumerate}

\section{1.3 Properties}\label{properties}

\subsection{1.3 L0 --- Simple Number
Properties}\label{l0-simple-number-properties}

Numbers have patterns that help us reason about algorithms without heavy
mathematics. At this level we focus on basic properties: even vs odd,
divisibility, and remainders. These ideas show up everywhere---from loop
counters to data structure layouts.

\subsubsection{Even and Odd}\label{even-and-odd}

A number is even if it ends with digit 0, 2, 4, 6, or 8 in decimal, and
odd otherwise.

\begin{itemize}
\item
  In binary, checking parity is even easier: the last bit tells the
  story.

  \begin{itemize}
  \tightlist
  \item
    \texttt{…0} → even
  \item
    \texttt{…1} → odd
  \end{itemize}
\end{itemize}

Example in Python:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ is\_even(n: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{bool}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{ n }\OperatorTok{\%} \DecValTok{2} \OperatorTok{==} \DecValTok{0}

\BuiltInTok{print}\NormalTok{(is\_even(}\DecValTok{10}\NormalTok{))  }\CommentTok{\# True}
\BuiltInTok{print}\NormalTok{(is\_even(}\DecValTok{7}\NormalTok{))   }\CommentTok{\# False}
\end{Highlighting}
\end{Shaded}

\subsubsection{Divisibility}\label{divisibility}

We often ask: does one number divide another?

\begin{itemize}
\tightlist
\item
  \texttt{a} is divisible by \texttt{b} if there exists some integer
  \texttt{k} with \texttt{a\ =\ b\ *\ k}.
\item
  In code: \texttt{a\ \%\ b\ ==\ 0}.
\end{itemize}

Examples:

\begin{itemize}
\tightlist
\item
  12 is divisible by 3 → \texttt{12\ \%\ 3\ ==\ 0}.
\item
  14 is not divisible by 5 → \texttt{14\ \%\ 5\ ==\ 4}.
\end{itemize}

\subsubsection{Remainders and Modular
Thinking}\label{remainders-and-modular-thinking}

When dividing, the remainder is what's left over.

\begin{itemize}
\item
  Example: \texttt{17\ //\ 5\ =\ 3}, remainder \texttt{2}.
\item
  Modular arithmetic wraps around like a clock:

  \begin{itemize}
  \tightlist
  \item
    \texttt{(17\ \%\ 5)\ =\ 2} → same as ``2 o'clock after going 17
    steps around a 5-hour clock.''
  \end{itemize}
\end{itemize}

This ``wrap-around'' view is central in array indexing, hashing, and
cryptography later on.

\subsubsection{Why It Matters}\label{why-it-matters-6}

\begin{itemize}
\tightlist
\item
  Algorithms: Parity checks decide branching (e.g., even-odd
  optimizations).
\item
  Data structures: Array indices often wrap around using \texttt{\%}.
\item
  Everyday: Calendars cycle days of the week; remainders formalize that.
\end{itemize}

\subsubsection{Exercises}\label{exercises-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function that returns \texttt{"even"} or \texttt{"odd"} for a
  given number.
\item
  Check if 91 is divisible by 7.
\item
  Compute the remainder of 100 divided by 9.
\item
  Use \texttt{\%} to simulate a 7-day week: if today is day 5 (Saturday)
  and you add 10 days, what day is it?
\item
  Find the last digit of \texttt{2\^{}15} without computing the full
  number (hint: check the remainder mod 10).
\end{enumerate}

\subsection{1.3 L1 --- Classical Number Theory
Tools}\label{l1-classical-number-theory-tools}

Beyond simple parity and divisibility, algorithms often need deeper
number properties. At this level we introduce a few ``toolkit'' ideas
from elementary number theory: greatest common divisor (GCD), least
common multiple (LCM), and modular arithmetic identities. These are
lightweight but powerful concepts that show up in algorithm design,
cryptography, and optimization.

\subsubsection{Greatest Common Divisor
(GCD)}\label{greatest-common-divisor-gcd}

The GCD of two numbers is the largest number that divides both.

\begin{itemize}
\tightlist
\item
  Example: \texttt{gcd(20,\ 14)\ =\ 2}.
\item
  Why useful: GCD simplifies fractions, ensures ratios are reduced, and
  appears in algorithm correctness proofs.
\end{itemize}

Euclid's Algorithm: Instead of trial division, we can compute GCD
quickly:

\begin{verbatim}
gcd(a, b) = gcd(b, a % b)
\end{verbatim}

This repeats until \texttt{b\ =\ 0}, at which point \texttt{a} is the
answer.

Python example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gcd(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \ControlFlowTok{while}\NormalTok{ b:}
\NormalTok{        a, b }\OperatorTok{=}\NormalTok{ b, a }\OperatorTok{\%}\NormalTok{ b}
    \ControlFlowTok{return}\NormalTok{ a}

\BuiltInTok{print}\NormalTok{(gcd(}\DecValTok{20}\NormalTok{, }\DecValTok{14}\NormalTok{))  }\CommentTok{\# 2}
\end{Highlighting}
\end{Shaded}

\subsubsection{Least Common Multiple
(LCM)}\label{least-common-multiple-lcm}

The LCM of two numbers is the smallest positive number divisible by
both.

\begin{itemize}
\item
  Example: \texttt{lcm(12,\ 18)\ =\ 36}.
\item
  Connection to GCD:

\begin{verbatim}
lcm(a, b) = (a * b) // gcd(a, b)
\end{verbatim}
\end{itemize}

This is useful in scheduling, periodic tasks, and synchronization
problems.

\subsubsection{Modular Arithmetic
Identities}\label{modular-arithmetic-identities}

Remainders behave predictably under operations:

\begin{itemize}
\tightlist
\item
  Addition:
  \texttt{(a\ +\ b)\ \%\ m\ =\ ((a\ \%\ m)\ +\ (b\ \%\ m))\ \%\ m}
\item
  Multiplication:
  \texttt{(a\ *\ b)\ \%\ m\ =\ ((a\ \%\ m)\ *\ (b\ \%\ m))\ \%\ m}
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  \texttt{(123\ +\ 456)\ \%\ 7\ =\ (123\ \%\ 7\ +\ 456\ \%\ 7)\ \%\ 7}
\item
  This property lets us work with small remainders instead of huge
  numbers, key in cryptography and hashing.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-7}

\begin{itemize}
\tightlist
\item
  Algorithms: GCD ensures efficiency in fraction reduction, graph
  algorithms, and number-theoretic algorithms.
\item
  Systems: LCM models periodicity, e.g., aligning CPU scheduling
  intervals.
\item
  Cryptography: Modular arithmetic underpins secure communication (RSA,
  Diffie-Hellman).
\item
  Practical programming: Modular identities simplify computations with
  limited ranges (hash tables, cyclic arrays).
\end{itemize}

\subsubsection{Exercises}\label{exercises-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \texttt{gcd(252,\ 198)} by hand using Euclid's algorithm.
\item
  Write a function that returns the LCM of two numbers. Test it on (12,
  18).
\item
  Show that \texttt{(37\ +\ 85)\ \%\ 12} equals
  \texttt{((37\ \%\ 12)\ +\ (85\ \%\ 12))\ \%\ 12}.
\item
  Reduce the fraction \texttt{84/126} using GCD.
\item
  Find the smallest day \texttt{d} such that \texttt{d} is a multiple of
  both 12 and 18 (hint: LCM).
\end{enumerate}

\subsection{1.3 L2 --- Advanced Number Theory in
Algorithms}\label{l2-advanced-number-theory-in-algorithms}

At this level, we move beyond everyday divisibility and Euclid's
algorithm. Modern algorithms frequently rely on deep number theory to
achieve efficiency. Topics such as modular inverses, Euler's totient
function, and primality tests are crucial foundations for cryptography,
randomized algorithms, and competitive programming.

\subsubsection{Modular Inverses}\label{modular-inverses}

The modular inverse of a number \texttt{a} (mod \texttt{m}) is an
integer \texttt{x} such that:

\begin{verbatim}
(a * x) % m = 1
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Example: the inverse of 3 modulo 7 is 5, because
  \texttt{(3*5)\ \%\ 7\ =\ 15\ \%\ 7\ =\ 1}.
\item
  Existence: an inverse exists if and only if \texttt{gcd(a,\ m)\ =\ 1}.
\item
  Computation: using the Extended Euclidean Algorithm.
\end{itemize}

This is the backbone of modular division and is heavily used in
cryptography (RSA), hash functions, and matrix inverses mod \texttt{p}.

\subsubsection{Euler's Totient Function
(φ)}\label{eulers-totient-function-ux3c6}

The function \texttt{φ(n)} counts the number of integers between 1 and
\texttt{n} that are coprime to \texttt{n}.

\begin{itemize}
\item
  Example: \texttt{φ(9)\ =\ 6} because \{1, 2, 4, 5, 7, 8\} are coprime
  to 9.
\item
  Key property (Euler's theorem):

\begin{verbatim}
a^φ(n) ≡ 1 (mod n)     if gcd(a, n) = 1
\end{verbatim}
\item
  Special case: Fermat's Little Theorem --- for prime \texttt{p},

\begin{verbatim}
a^(p-1) ≡ 1 (mod p)
\end{verbatim}
\end{itemize}

This result is central in modular exponentiation and cryptosystems like
RSA.

\subsubsection{Primality Testing}\label{primality-testing}

Determining if a number is prime is easy for small inputs but hard for
large ones. Efficient algorithms are essential:

\begin{itemize}
\tightlist
\item
  Trial division: works only for small \texttt{n}.
\item
  Fermat primality test: uses Fermat's Little Theorem to detect
  composites, but can be fooled by Carmichael numbers.
\item
  Miller--Rabin test: a probabilistic algorithm widely used in practice
  (cryptographic key generation).
\item
  AKS primality test: a deterministic polynomial-time method
  (theoretical importance).
\end{itemize}

Example intuition:

\begin{itemize}
\tightlist
\item
  For large \texttt{n}, we don't check all divisors; we test properties
  of \texttt{a\^{}k\ mod\ n} for random bases \texttt{a}.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-8}

\begin{itemize}
\tightlist
\item
  Cryptography: Public-key systems depend on modular inverses, Euler's
  theorem, and large primes.
\item
  Algorithms: Modular inverses simplify solving equations in modular
  arithmetic (e.g., Chinese Remainder Theorem applications).
\item
  Practical Computing: Randomized primality tests (like Miller--Rabin)
  balance correctness and efficiency in real-world systems.
\end{itemize}

\subsubsection{Exercises}\label{exercises-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the modular inverse of 7 modulo 13.
\item
  Compute φ(10) and verify Euler's theorem for \texttt{a\ =\ 3}.
\item
  Use Fermat's test to check whether 341 is prime. (Hint: try
  \texttt{a\ =\ 2}.)
\item
  Implement modular inverse using the Extended Euclidean Algorithm.
\item
  Research: why do cryptographic protocols prefer Miller--Rabin over
  AKS, even though AKS is deterministic?
\end{enumerate}

\section{1.4 Overflow \& Precision}\label{overflow-precision}

\subsection{1.4 L0 - When Numbers Get Too Big or Too
Small}\label{l0---when-numbers-get-too-big-or-too-small}

Numbers inside a computer are stored with a fixed number of bits. This
means they can only represent values up to a certain limit. If a
calculation produces a result larger than this limit, the value ``wraps
around,'' much like the digits on an odometer rolling over after 999 to
000. This phenomenon is called overflow. Similarly, computers often
cannot represent all decimal fractions exactly, leading to tiny errors
called precision loss.

\subsubsection{Deep Dive}\label{deep-dive}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Integer Overflow

  \begin{itemize}
  \tightlist
  \item
    A computer uses a fixed number of bits (commonly 8, 16, 32, or 64)
    to store integers.
  \item
    An 8-bit unsigned integer can represent values from 0 to 255. Adding
    1 to 255 causes the value to wrap back to 0.
  \item
    Signed integers use \emph{two's complement} representation. For an
    8-bit signed integer, the range is −128 to +127. Adding 1 to 127
    makes it overflow to −128.
  \end{itemize}

  Example in binary:

\begin{verbatim}
11111111₂ (255) + 1 = 00000000₂ (0)
01111111₂ (+127) + 1 = 10000000₂ (−128)
\end{verbatim}
\item
  Floating-Point Precision

  \begin{itemize}
  \tightlist
  \item
    Decimal fractions like 0.1 cannot always be represented exactly in
    binary.
  \item
    As a result, calculations may accumulate tiny errors.
  \item
    For example, repeatedly adding 0.1 may not exactly equal 1.0 due to
    precision limits.
  \end{itemize}
\end{enumerate}

\subsubsection{Example}\label{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Integer overflow simulation with 8{-}bit values}
\KeywordTok{def}\NormalTok{ add\_8bit(a, b):}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ (a }\OperatorTok{+}\NormalTok{ b) }\OperatorTok{\%} \DecValTok{256}  \CommentTok{\# simulate wraparound}
    \ControlFlowTok{return}\NormalTok{ result}

\BuiltInTok{print}\NormalTok{(add\_8bit(}\DecValTok{250}\NormalTok{, }\DecValTok{10}\NormalTok{))   }\CommentTok{\# 260 wraps to 4}
\BuiltInTok{print}\NormalTok{(add\_8bit(}\DecValTok{255}\NormalTok{, }\DecValTok{1}\NormalTok{))    }\CommentTok{\# wraps to 0}

\CommentTok{\# Floating{-}point precision issue}
\NormalTok{x }\OperatorTok{=} \FloatTok{0.1} \OperatorTok{+} \FloatTok{0.2}
\BuiltInTok{print}\NormalTok{(x)           }\CommentTok{\# Expected 0.3, but gives 0.30000000000000004}
\BuiltInTok{print}\NormalTok{(x }\OperatorTok{==} \FloatTok{0.3}\NormalTok{)    }\CommentTok{\# False}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-9}

\begin{itemize}
\tightlist
\item
  Unexpected results: A calculation may suddenly produce a negative
  number or wrap around to zero.
\item
  Real-world impact:

  \begin{itemize}
  \tightlist
  \item
    Video games may show scores jumping strangely if counters overflow.
  \item
    Banking or financial systems must avoid losing cents due to
    floating-point errors.
  \item
    Engineers and scientists rely on careful handling of precision to
    ensure correct simulations.
  \end{itemize}
\item
  Foundation for algorithms: Understanding overflow and precision
  prepares you for later topics like hashing, cryptography, and
  numerical analysis.
\end{itemize}

\subsubsection{Exercises}\label{exercises-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a 4-bit unsigned integer system. What happens if you start at
  14 and keep adding 1?
\item
  In Python, try adding \texttt{0.1} to itself ten times. Does it equal
  exactly \texttt{1.0}? Why or why not?
\item
  Write a function that checks if an 8-bit signed integer addition would
  overflow.
\item
  Imagine you are programming a digital clock that uses 2 digits for
  minutes (00--59). What happens when the value goes from 59 to 60? How
  would you handle this?
\end{enumerate}

\subsection{1.4 L1 - Detecting and Managing Overflow in Real
Programs}\label{l1---detecting-and-managing-overflow-in-real-programs}

Computers don't do math in the abstract. Integers live in fixed-width
registers; floats follow IEEE-754. Robust software accounts for these
limits up front: choose the right representation, detect overflow, and
compare floats safely. The following sections explain how these issues
show up in practice and how to design around them.

\subsubsection{Deep Dive}\label{deep-dive-1}

\paragraph{1) Integer arithmetic in
practice}\label{integer-arithmetic-in-practice}

Fixed width means wraparound at \(2^{n}\). Unsigned wrap is modular
arithmetic; signed overflow (two's complement) can flip signs.
Developers often discover this the hard way when a counter suddenly goes
negative or wraps back to zero in production logs.

Bit width \& ranges This table reminds us of the hard limits baked into
hardware. Once the range is exceeded, the value doesn't ``grow
bigger''---it wraps.

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0562}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6067}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3371}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Width
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Signed range
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsigned range
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
32 & −2,147,483,648 \ldots{} 2,147,483,647 & 0 \ldots{} 4,294,967,295 \\
64 & −9,223,372,036,854,775,808 \ldots{} 9,223,372,036,854,775,807 & 0
\ldots{} 18,446,744,073,709,551,615 \\
\end{longtable}

Overflow semantics by language Each language makes slightly different
promises. This matters if you're writing cross-language services or
reading binary data across APIs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2201}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2956}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4151}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Language
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Signed overflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsigned overflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
C/C++ & UB (undefined) & Modular wrap & Use
UBSan/\texttt{-fsanitize=undefined}; widen types or check before add. \\
Rust & Traps in debug; defined APIs & \texttt{wrapping\_add},
\texttt{checked\_add}, \texttt{saturating\_add} & Make intent
explicit. \\
Java/Kotlin & Wraps (two's complement) & N/A (only signed types) & Use
\texttt{Math.addExact} to trap. \\
C\# & Wraps by default; \texttt{checked} to trap &
\texttt{checked}/\texttt{unchecked} blocks & \texttt{decimal} type for
money. \\
Python & Arbitrary precision & Arbitrary precision & Simulates fixed
width if needed. \\
\end{longtable}

A quick lesson: ``wrap'' may be safe in crypto or hashing, but it's
usually a bug in counters or indices. Always decide what you want up
front.

\paragraph{2) Floating-point you can depend
on}\label{floating-point-you-can-depend-on}

IEEE-754 doubles have \textasciitilde15--16 decimal digits and huge
dynamic range, but not exact decimal fractions. Think of floats as
\emph{convenient approximations}. They are perfect for physics
simulations, but brittle when used to represent cents in a bank account.

Where precision is lost These examples show why ``0.1 + 0.2 != 0.3''
isn't a joke---it's a direct consequence of binary storage.

\begin{itemize}
\tightlist
\item
  Scale mismatch: \(1\text{e}16 + 1 = 1\text{e}16\). The tiny
  \texttt{+1} gets lost.
\item
  Cancellation: subtracting nearly equal numbers deletes significant
  digits.
\item
  Decimal fractions (\texttt{0.1}) are repeating in binary.
\end{itemize}

Comparing floats Never compare with \texttt{==}. Instead, use a mixed
relative + absolute check:

\[
|x-y| \le \max(\text{rel}\cdot\max(|x|,|y|),\ \text{abs})
\]

This makes comparisons robust whether you're near zero or far away.

Rounding modes (when you explicitly care) Most of the time you don't
think about rounding---hardware defaults to ``round to nearest, ties to
even.'' But when writing financial systems or interval arithmetic, you
want to control it.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4396}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5604}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mode
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Round to nearest, ties to even (default) & General numeric work;
minimizes bias \\
Toward 0 / ±∞ & Bounds, interval arithmetic, conservative estimates \\
\end{longtable}

Having explicit rounding modes is like having a steering wheel---you
don't always turn, but you're glad it's there when the road curves.

Summation strategies The order of addition matters for floats. These
options give you a menu of accuracy vs.~speed.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
When to use
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Naïve left-to-right & Worst & Low & Never for sensitive sums \\
Pairwise / tree & Better & Med & Parallel reductions, ``good
default'' \\
Kahan (compensated) & Best & Higher & Financial-ish aggregates, small
vectors \\
\end{longtable}

You don't need Kahan everywhere, but knowing it exists keeps you from
blaming ``mystery bugs'' on hardware.

Representation choices Sometimes the best answer is to avoid floats
entirely. Money is the classic example.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2235}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7765}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Use case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Recommended representation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Currency, invoicing & Fixed-point (e.g., cents as \texttt{int64}) or
\texttt{decimal}/\texttt{BigDecimal} \\
Scientific compute & \texttt{float64}, compensated sums, stable
algorithms \\
IDs, counters & \texttt{uint64}/\texttt{int64}, detect overflow on
boundaries \\
\end{longtable}

\subsubsection{Code (Python---portable
patterns)}\label{code-pythonportable-patterns}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 32{-}bit checked add (raises on overflow)}
\KeywordTok{def}\NormalTok{ add\_i32\_checked(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    s }\OperatorTok{=}\NormalTok{ a }\OperatorTok{+}\NormalTok{ b}
    \ControlFlowTok{if}\NormalTok{ s }\OperatorTok{\textless{}} \OperatorTok{{-}}\DecValTok{2\_147\_483\_648} \KeywordTok{or}\NormalTok{ s }\OperatorTok{\textgreater{}} \DecValTok{2\_147\_483\_647}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{OverflowError}\NormalTok{(}\StringTok{"int32 overflow"}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ s}

\CommentTok{\# Simulate 32{-}bit wrap (intentional modular arithmetic)}
\KeywordTok{def}\NormalTok{ add\_i32\_wrapping(a: }\BuiltInTok{int}\NormalTok{, b: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    s }\OperatorTok{=}\NormalTok{ (a }\OperatorTok{+}\NormalTok{ b) }\OperatorTok{\&} \BaseNTok{0xFFFFFFFF}
    \ControlFlowTok{return}\NormalTok{ s }\OperatorTok{{-}} \BaseNTok{0x100000000} \ControlFlowTok{if}\NormalTok{ s }\OperatorTok{\&} \BaseNTok{0x80000000} \ControlFlowTok{else}\NormalTok{ s}

\CommentTok{\# Relative+absolute epsilon float compare}
\KeywordTok{def}\NormalTok{ almost\_equal(x: }\BuiltInTok{float}\NormalTok{, y: }\BuiltInTok{float}\NormalTok{, rel}\OperatorTok{=}\FloatTok{1e{-}12}\NormalTok{, abs\_}\OperatorTok{=}\FloatTok{1e{-}12}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{bool}\NormalTok{:}
    \ControlFlowTok{return} \BuiltInTok{abs}\NormalTok{(x }\OperatorTok{{-}}\NormalTok{ y) }\OperatorTok{\textless{}=} \BuiltInTok{max}\NormalTok{(rel }\OperatorTok{*} \BuiltInTok{max}\NormalTok{(}\BuiltInTok{abs}\NormalTok{(x), }\BuiltInTok{abs}\NormalTok{(y)), abs\_)}

\CommentTok{\# Kahan (compensated) summation}
\KeywordTok{def}\NormalTok{ kahan\_sum(xs):}
\NormalTok{    s }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{    c }\OperatorTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs:}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{{-}}\NormalTok{ c}
\NormalTok{        t }\OperatorTok{=}\NormalTok{ s }\OperatorTok{+}\NormalTok{ y}
\NormalTok{        c }\OperatorTok{=}\NormalTok{ (t }\OperatorTok{{-}}\NormalTok{ s) }\OperatorTok{{-}}\NormalTok{ y}
\NormalTok{        s }\OperatorTok{=}\NormalTok{ t}
    \ControlFlowTok{return}\NormalTok{ s}

\CommentTok{\# Fixed{-}point cents (safe for \textasciitilde{}±9e16 cents with int64)}
\KeywordTok{def}\NormalTok{ dollars\_to\_cents(d: }\BuiltInTok{str}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
\NormalTok{    whole, \_, frac }\OperatorTok{=}\NormalTok{ d.partition(}\StringTok{"."}\NormalTok{)}
\NormalTok{    frac }\OperatorTok{=}\NormalTok{ (frac }\OperatorTok{+} \StringTok{"00"}\NormalTok{)[:}\DecValTok{2}\NormalTok{]}
    \ControlFlowTok{return} \BuiltInTok{int}\NormalTok{(whole) }\OperatorTok{*} \DecValTok{100} \OperatorTok{+} \BuiltInTok{int}\NormalTok{(frac)}

\KeywordTok{def}\NormalTok{ cents\_to\_dollars(c: }\BuiltInTok{int}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{str}\NormalTok{:}
\NormalTok{    sign }\OperatorTok{=} \StringTok{"{-}"} \ControlFlowTok{if}\NormalTok{ c }\OperatorTok{\textless{}} \DecValTok{0} \ControlFlowTok{else} \StringTok{""}
\NormalTok{    c }\OperatorTok{=} \BuiltInTok{abs}\NormalTok{(c)}
    \ControlFlowTok{return} \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{sign}\SpecialCharTok{\}\{}\NormalTok{c}\OperatorTok{//}\DecValTok{100}\SpecialCharTok{\}}\SpecialStringTok{.}\SpecialCharTok{\{}\NormalTok{c}\OperatorTok{\%}\DecValTok{100}\SpecialCharTok{:02d\}}\SpecialStringTok{"}
\end{Highlighting}
\end{Shaded}

These examples are in Python for clarity, but the same ideas exist in
every major language.

\subsubsection{Why it matters}\label{why-it-matters-10}

\begin{itemize}
\tightlist
\item
  Reliability: Silent wrap or float drift becomes data corruption under
  load or over time.
\item
  Interoperability: Services in different languages disagree on
  overflow; define and document your contracts.
\item
  Reproducibility: Deterministic numerics (same inputs → same bits)
  depend on summation order, rounding, and libraries.
\item
  Security: UB-triggered overflows can turn into exploitable states.
\end{itemize}

This is why ``it worked on my laptop'' is not enough. You want to be
sure it works on every platform, every time.

\subsubsection{Exercises}\label{exercises-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Overflow policy: For a metrics pipeline, decide where to use
  \texttt{checked}, \texttt{wrapping}, and \texttt{saturating}
  addition---and justify each with failure modes.
\item
  ULP probe: Find the smallest \(\epsilon\) such that
  \texttt{1.0\ +\ ε\ !=\ 1.0} in your language; explain how it relates
  to machine epsilon.
\item
  Summation bake-off: Sum the first 1M terms of the harmonic series with
  naïve, pairwise, and Kahan methods; compare results and timings.
\item
  Fixed-point ledger: Implement deposit/transfer/withdraw using
  \texttt{int64} cents; prove no rounding loss for two-decimal
  currencies.
\item
  Boundary tests: Write property tests that \texttt{add\_i32\_checked}
  raises on \texttt{\{INT\_MAX,1\}} and \texttt{\{INT\_MIN,-1\}}, and
  equals modular add where documented.
\item
  Cross-lang contract: Specify a JSON schema for monetary amounts and
  counters that avoids float types; include examples and edge cases.
\end{enumerate}

Great --- let's rework 1.4 Overflow \& Precision (L2) into a friendlier
deep dive, using the same pattern: structured sections, tables, and
added ``bridge'' sentences that guide the reader through complex,
low-level material. This version should be dense enough to teach
internals, but smooth enough to read without feeling like a spec sheet.

\subsection{1.4 L2. Under the Hood}\label{l2.-under-the-hood}

At the lowest level, overflow and precision aren't abstract
concepts---they are consequences of how CPUs, compilers, and libraries
actually implement arithmetic. Understanding these details makes
debugging easier and gives you control over performance,
reproducibility, and correctness.

\subsubsection{Deep Dive}\label{deep-dive-2}

\paragraph{1) Hardware semantics}\label{hardware-semantics}

CPUs implement integer and floating-point arithmetic directly in
silicon. When the result doesn't fit, different flags or traps are
triggered.

\begin{itemize}
\tightlist
\item
  Status flags (integers): Most architectures (x86, ARM, RISC-V) set
  overflow, carry, and zero flags after arithmetic. These flags drive
  branch instructions like \texttt{jo} (``jump if overflow'').
\item
  Floating-point control: The FPU or SIMD unit maintains exception flags
  (inexact, overflow, underflow, invalid, divide-by-zero). These rarely
  trap by default; they silently set flags until explicitly checked.
\end{itemize}

Architectural view

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0606}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2424}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2525}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Arch
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Integer overflow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
FP behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Developer hooks
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
x86-64 & Wraparound in 2's complement; OF/CF bits set & IEEE-754; flags
in MXCSR & \texttt{jo}/\texttt{jno}, \texttt{fenv.h} \\
ARM64 & Wraparound; NZCV flags & IEEE-754; exception bits & condition
codes, \texttt{feset*} \\
RISC-V & Wraparound; OV/CF optional & IEEE-754; status regs & CSRs, trap
handlers \\
\end{longtable}

Knowing what the CPU does lets you choose: rely on hardware wrap, trap
explicitly, or add software checks.

\paragraph{2) Compiler and language
layers}\label{compiler-and-language-layers}

Even if hardware sets flags, your language may ignore them. Compilers
often optimize based on the language spec.

\begin{itemize}
\tightlist
\item
  C/C++: Signed overflow is \emph{undefined behavior}---the optimizer
  assumes it never happens, which can remove safety checks you thought
  were there.
\item
  Rust: Catches overflow in debug builds, then forces you to pick:
  \texttt{checked\_add}, \texttt{wrapping\_add}, or
  \texttt{saturating\_add}.
\item
  JVM languages (Java, Kotlin, Scala): Always wrap, hiding UB but
  forcing you to detect overflow yourself.
\item
  .NET (C\#, F\#): Defaults to wrapping; you can enable \texttt{checked}
  contexts to trap.
\item
  Python: Emulates unbounded integers, but sometimes simulates C-like
  behavior for low-level modules.
\end{itemize}

These choices aren't arbitrary---they reflect trade-offs between speed,
safety, and backward compatibility.

\paragraph{3) Precision management in floating
point}\label{precision-management-in-floating-point}

Floating-point has more than just rounding errors. Engineers deal with
gradual underflow, denormals, and fused operations.

\begin{itemize}
\tightlist
\item
  Subnormals: Numbers smaller than \textasciitilde2.2e-308 in double
  precision become ``denormalized,'' losing precision but extending the
  range toward zero. Many CPUs handle these slowly.
\item
  Flush-to-zero: Some systems skip subnormals entirely, treating them as
  zero to boost speed. Great for graphics; risky for scientific code.
\item
  FMA (fused multiply-add): Computes \texttt{(a*b\ +\ c)} with one
  rounding, often improving precision and speed. However, it can break
  reproducibility across machines that do/don't use FMA.
\end{itemize}

Precision events

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1216}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2973}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5811}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Event
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What happens
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why it matters
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Overflow & Becomes ±Inf & Detectable via \texttt{isinf}, often safe \\
Underflow & Becomes 0 or subnormal & Performance hit, possible accuracy
loss \\
Inexact & Result rounded & Happens constantly; only matters if
flagged \\
Invalid & NaN produced & Division 0/0, sqrt(-1), etc. \\
\end{longtable}

When performance bugs show up in HPC or ML code, denormals and FMAs are
often the hidden cause.

\paragraph{4) Debugging and testing
tools}\label{debugging-and-testing-tools}

Low-level correctness requires instrumentation. Fortunately, toolchains
give you options.

\begin{itemize}
\tightlist
\item
  Sanitizers: \texttt{-fsanitize=undefined} (Clang/GCC) traps on signed
  overflow.
\item
  Valgrind / perf counters: Can catch denormal slowdowns.
\item
  Unit-test utilities: Rust's \texttt{assert\_eq!(checked\_add(…))},
  Python's \texttt{math.isclose}, Java's \texttt{BigDecimal} reference
  checks.
\item
  Reproducibility flags: \texttt{-ffast-math} (fast but
  non-deterministic), vs.~\texttt{-frounding-math} (strict).
\end{itemize}

Testing with multiple compilers and settings reveals assumptions you
didn't know you had.

\paragraph{5) Strategies in production
systems}\label{strategies-in-production-systems}

When deploying real systems, you pick policies that match domain needs.

\begin{itemize}
\tightlist
\item
  Databases: Use \texttt{DECIMAL(p,s)} to store fixed-point, preventing
  float drift in sums.
\item
  Financial systems: Explicit fixed-point types (cents as
  \texttt{int64}) + saturating logic on overflow.
\item
  Graphics / ML: Accept \texttt{float32} imprecision; gain throughput
  with fused ops and flush-to-zero.
\item
  Low-level kernels: Exploit modular wraparound deliberately for hash
  tables, checksums, and crypto.
\end{itemize}

Policy menu

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2464}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7536}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Money transfers & Fixed-point, saturating arithmetic \\
Physics sim & \texttt{float64}, stable integrators, compensated
summation \\
Hashing / RNG & Embrace wraparound modular math \\
Critical counters & \texttt{uint64} with explicit overflow trap \\
\end{longtable}

Thinking in policies avoids one-off hacks. Document ``why'' once, then
apply consistently.

\subsubsection{Code Examples}\label{code-examples}

C (wrap vs check)

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdint.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}stdbool.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}limits.h\textgreater{}}

\DataTypeTok{bool}\NormalTok{ add\_checked\_i32}\OperatorTok{(}\DataTypeTok{int32\_t}\NormalTok{ a}\OperatorTok{,} \DataTypeTok{int32\_t}\NormalTok{ b}\OperatorTok{,} \DataTypeTok{int32\_t} \OperatorTok{*}\NormalTok{out}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{if} \OperatorTok{((}\NormalTok{b }\OperatorTok{\textgreater{}} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ a }\OperatorTok{\textgreater{}}\NormalTok{ INT32\_MAX }\OperatorTok{{-}}\NormalTok{ b}\OperatorTok{)} \OperatorTok{||}
        \OperatorTok{(}\NormalTok{b }\OperatorTok{\textless{}} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ a }\OperatorTok{\textless{}}\NormalTok{ INT32\_MIN }\OperatorTok{{-}}\NormalTok{ b}\OperatorTok{))} \OperatorTok{\{}
        \ControlFlowTok{return} \KeywordTok{false}\OperatorTok{;} \CommentTok{// overflow}
    \OperatorTok{\}}
    \OperatorTok{*}\NormalTok{out }\OperatorTok{=}\NormalTok{ a }\OperatorTok{+}\NormalTok{ b}\OperatorTok{;}
    \ControlFlowTok{return} \KeywordTok{true}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Rust (explicit intent)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fn}\NormalTok{ demo() }\OperatorTok{\{}
    \KeywordTok{let}\NormalTok{ x}\OperatorTok{:} \DataTypeTok{i32} \OperatorTok{=} \DataTypeTok{i32}\PreprocessorTok{::}\ConstantTok{MAX}\OperatorTok{;}
    \PreprocessorTok{println!}\NormalTok{(}\StringTok{"\{:?\}"}\OperatorTok{,}\NormalTok{ x}\OperatorTok{.}\NormalTok{wrapping\_add(}\DecValTok{1}\NormalTok{))}\OperatorTok{;}   \CommentTok{// wrap}
    \PreprocessorTok{println!}\NormalTok{(}\StringTok{"\{:?\}"}\OperatorTok{,}\NormalTok{ x}\OperatorTok{.}\NormalTok{checked\_add(}\DecValTok{1}\NormalTok{))}\OperatorTok{;}    \CommentTok{// None}
    \PreprocessorTok{println!}\NormalTok{(}\StringTok{"\{:?\}"}\OperatorTok{,}\NormalTok{ x}\OperatorTok{.}\NormalTok{saturating\_add(}\DecValTok{1}\NormalTok{))}\OperatorTok{;} \CommentTok{// clamp}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Python (reproducibility check)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}

\KeywordTok{def}\NormalTok{ ulp\_diff(a: }\BuiltInTok{float}\NormalTok{, b: }\BuiltInTok{float}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \CommentTok{\# Compares floats in terms of ULPs}
    \ImportTok{import}\NormalTok{ struct}
\NormalTok{    ai }\OperatorTok{=}\NormalTok{ struct.unpack(}\StringTok{\textquotesingle{}!q\textquotesingle{}}\NormalTok{, struct.pack(}\StringTok{\textquotesingle{}!d\textquotesingle{}}\NormalTok{, a))[}\DecValTok{0}\NormalTok{]}
\NormalTok{    bi }\OperatorTok{=}\NormalTok{ struct.unpack(}\StringTok{\textquotesingle{}!q\textquotesingle{}}\NormalTok{, struct.pack(}\StringTok{\textquotesingle{}!d\textquotesingle{}}\NormalTok{, b))[}\DecValTok{0}\NormalTok{]}
    \ControlFlowTok{return} \BuiltInTok{abs}\NormalTok{(ai }\OperatorTok{{-}}\NormalTok{ bi)}

\BuiltInTok{print}\NormalTok{(ulp\_diff(}\FloatTok{1.0}\NormalTok{, math.nextafter(}\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{)))  }\CommentTok{\# 1}
\end{Highlighting}
\end{Shaded}

These snippets show how different languages force you to state your
policy, rather than relying on ``whatever the hardware does.''

\subsubsection{Why it matters}\label{why-it-matters-11}

\begin{itemize}
\tightlist
\item
  Performance: Understanding denormals and FMAs can save orders of
  magnitude in compute-heavy workloads.
\item
  Correctness: Database money columns or counters in billing systems can
  silently corrupt without fixed-point or overflow checks.
\item
  Portability: Code that relies on UB may ``work'' on GCC Linux but fail
  on Clang macOS.
\item
  Security: Integer overflow bugs (e.g., buffer length miscalculation)
  remain a classic vulnerability class.
\end{itemize}

In short, overflow and precision are not ``just math''---they are
systems-level contracts that must be understood and enforced.

\subsubsection{Exercises}\label{exercises-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compiler behavior: Write a C function that overflows
  \texttt{int32\_t}. Compile with and without
  \texttt{-fsanitize=undefined}. What changes?
\item
  FMA investigation: Run a dot-product with and without
  \texttt{-ffast-math}. Measure result differences across compilers.
\item
  Denormal trap: Construct a loop multiplying by \texttt{1e-308}. Time
  it with flush-to-zero enabled vs disabled.
\item
  Policy design: For an in-memory database, define rules for counters,
  timestamps, and currency columns. Which use wrapping, which use
  fixed-point, which trap?
\item
  Cross-language test: Implement \texttt{add\_checked\_i32} in C, Rust,
  and Python. Run edge-case inputs (\texttt{INT\_MAX},
  \texttt{INT\_MIN}). Compare semantics.
\item
  ULP meter: Write a function in your language to compute ULP distance
  between two floats. Use it to compare rounding differences between
  platforms.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Chapter 2. Arrays}\label{chapter-2.-arrays}

\section{2.1 Static Arrays}\label{static-arrays}

\subsection{2.2 L0 --- Arrays That Grow}\label{l0-arrays-that-grow}

A dynamic array is like a container that can expand and shrink as
needed. Unlike static arrays, which must know their size in advance, a
dynamic array adapts as elements are added or removed. You can think of
it as a bookshelf where new shelves appear automatically when space runs
out. The underlying idea is simple: keep the benefits of fast
index-based access, while adding flexibility to change the size.

\subsubsection{Deep Dive}\label{deep-dive-3}

A dynamic array begins with a fixed amount of space called its capacity.
When the number of elements (the length) exceeds this capacity, the
array grows. This is usually done by allocating a new, larger block of
memory and copying the old elements into it. After this, new elements
can be added until the new capacity is filled, at which point the
process repeats.

Despite this resizing process, the key properties remain:

\begin{itemize}
\tightlist
\item
  Fast access and update: Elements can still be reached instantly using
  an index.
\item
  Append flexibility: New elements can be added at the end without
  worrying about fixed size.
\item
  Occasional resizing cost: Most appends are quick, but when resizing
  happens, it takes longer because all elements must be copied.
\end{itemize}

The performance picture is intuitive:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3247}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4935}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time Complexity (Typical)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Index maps directly to position \\
Update element & O(1) & Replace value in place \\
Append element & O(1) amortized & Occasionally O(n) when resizing
occurs \\
Pop element & O(1) & Remove from end \\
Insert/Delete & O(n) & Elements must be shifted \\
\end{longtable}

Dynamic arrays therefore trade predictability for flexibility. The
occasional slow operation is outweighed by the ability to grow and
shrink on demand, which makes them useful for most real-world tasks
where the number of elements is not known in advance.

\subsubsection{Worked Example}\label{worked-example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a dynamic array using Python\textquotesingle{}s built{-}in list}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Append elements (array grows automatically)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    arr.append((i }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{10}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after appending:"}\NormalTok{, arr)}

\CommentTok{\# Access and update elements}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}
\NormalTok{arr[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated array:"}\NormalTok{, arr)}

\CommentTok{\# Remove last element}
\NormalTok{last }\OperatorTok{=}\NormalTok{ arr.pop()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Removed element:"}\NormalTok{, last)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after pop:"}\NormalTok{, arr)}

\CommentTok{\# Traverse array}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{arr[i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This short program shows how a dynamic array in Python resizes
automatically with \texttt{append} and shrinks with \texttt{pop}. Access
and updates remain instant, while resizing happens invisibly when more
space is needed.

\subsubsection{Why it matters}\label{why-it-matters-12}

Dynamic arrays combine efficiency and flexibility. They allow programs
to handle unknown or changing amounts of data without predefining sizes.
They form the backbone of lists in high-level languages, balancing
performance with usability. They also illustrate the idea of amortized
cost: most operations are fast, but occasional expensive operations are
averaged out over time.

\subsubsection{Exercises}\label{exercises-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array and append numbers 1 through 10. Print the final
  array.
\item
  Replace the 3rd element with a new value.
\item
  Remove the last two elements and print the result.
\item
  Write a procedure that traverses a dynamic array and computes the
  average of its elements.
\item
  Explain why appending one element might sometimes be much slower than
  appending another, even though both look the same in code.
\end{enumerate}

\subsection{2.1 L1 --- Static Arrays in
Practice}\label{l1-static-arrays-in-practice}

Static arrays are one of the simplest and most reliable ways of storing
data. They are defined as collections of elements laid out in a
fixed-size, contiguous block of memory. Unlike dynamic arrays, their
size is determined at creation and cannot be changed later. This
property makes them predictable, efficient, and easy to reason about,
but also less flexible when dealing with varying amounts of data.

\subsubsection{Deep Dive}\label{deep-dive-4}

At the heart of static arrays is their memory layout. When an array is
created, the program reserves a continuous region of memory large enough
to hold all its elements. Each element is stored right next to the
previous one. This design allows very fast access because the position
of any element can be computed directly:

\begin{verbatim}
address_of(arr[i]) = base_address + (i × element_size)
\end{verbatim}

No searching or scanning is required, only simple arithmetic. This is
why reading or writing to an element at a given index is considered O(1)
--- constant time regardless of the array size.

The trade-offs emerge when considering insertion or deletion. Because
elements are tightly packed, inserting a new element in the middle
requires shifting all the subsequent elements by one position. Deleting
works the same way in reverse. These operations are therefore O(n),
linear in the size of the array.

The cost summary is straightforward:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Time Complexity & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Direct index calculation \\
Update element & O(1) & Replace in place \\
Traverse & O(n) & Visit each element once \\
Insert/Delete & O(n) & Shifting elements required \\
\end{longtable}

\paragraph{Trade-offs.}\label{trade-offs.}

Static arrays excel when you know the size in advance. They guarantee
fast access and compact memory usage because there is no overhead for
resizing or metadata. However, they lack flexibility. If the array is
too small, you must allocate a larger one and copy all elements over. If
it is too large, memory is wasted. This is why languages like Python
provide dynamic lists by default, while static arrays are used in
performance-critical or resource-constrained contexts.

\paragraph{Use cases.}\label{use-cases.}

\begin{itemize}
\tightlist
\item
  Buffers: Fixed-size areas for network packets or hardware input.
\item
  Lookup tables: Precomputed constants or small ranges of values (e.g.,
  ASCII character tables).
\item
  Static configuration data: Tables known at compile-time, where
  resizing is unnecessary.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.}

Programmers must be careful of two common issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Out-of-bounds errors: Trying to access an index outside the valid
  range, leading to exceptions (in safe languages) or undefined behavior
  (in low-level languages).
\item
  Sizing problems: Underestimating leads to crashes, overestimating
  leads to wasted memory.
\end{enumerate}

Static arrays are common in many programming environments. In Python,
the \texttt{array} module provides a fixed-type sequence that behaves
more like a C-style array. Libraries like NumPy also provide fixed-shape
arrays that offer efficient memory usage and fast computations. In C and
C++, arrays are part of the language itself, and they form the
foundation of higher-level containers like \texttt{std::vector}.

\subsubsection{Worked Example}\label{worked-example-1}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ array}

\CommentTok{\# Create a static array of integers (type \textquotesingle{}i\textquotesingle{} = signed int)}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ array.array(}\StringTok{\textquotesingle{}i\textquotesingle{}}\NormalTok{, [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Fill the array with values}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
\NormalTok{    arr[i] }\OperatorTok{=}\NormalTok{ (i }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{10}

\CommentTok{\# Access and update elements}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}
\NormalTok{arr[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}

\CommentTok{\# Traverse the array}
\BuiltInTok{print}\NormalTok{(}\StringTok{"All elements:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{arr[i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Demonstrating the limitation: trying to insert beyond capacity}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    arr.insert(}\DecValTok{5}\NormalTok{, }\DecValTok{60}\NormalTok{)  }\CommentTok{\# This technically works in Python\textquotesingle{}s array, but resizes internally}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Inserted new element:"}\NormalTok{, arr)}
\ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Error inserting into static array:"}\NormalTok{, e)}
\end{Highlighting}
\end{Shaded}

This code illustrates the strengths and weaknesses of static arrays.
Access and updates are immediate, and traversal is simple. But the
notion of a ``fixed size'' means that insertion and deletion are costly
or, in some languages, unsupported.

\subsubsection{Why it matters}\label{why-it-matters-13}

Static arrays are the building blocks of data structures. They teach the
trade-off between speed and flexibility. They remind us that memory is
finite and that how data is laid out in memory directly impacts
performance. Whether writing Python code, using NumPy, or implementing
algorithms in C, understanding static arrays makes it easier to reason
about cost, predict behavior, and avoid common errors.

\subsubsection{Exercises}\label{exercises-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array of size 8 and fill it with even numbers from 2 to 16.
  Then access the 4th element directly.
\item
  Update the middle element of a fixed-size array with a new value.
\item
  Write a procedure to traverse an array and find the maximum element.
\item
  Explain why inserting a new value into the beginning of a static array
  requires shifting every other element.
\item
  Give two examples of real-world systems where fixed-size arrays are a
  natural fit.
\end{enumerate}

\subsection{2.1 L2 --- Static Arrays and the System
Beneath}\label{l2-static-arrays-and-the-system-beneath}

Static arrays are more than just a collection of values; they are a
direct window into how computers store and access data. At the advanced
level, understanding static arrays means looking at memory models, cache
behavior, compiler optimizations, and the role of arrays in operating
systems and production libraries. This perspective is critical for
building high-performance software and for avoiding subtle, system-level
bugs.

\subsubsection{Deep Dive}\label{deep-dive-5}

At the lowest level, a static array is a contiguous block of memory.
When an array is declared, the compiler calculates the required size as
\texttt{length\ ×\ element\_size} and reserves that many bytes. Each
element is addressed by simple arithmetic:

\begin{verbatim}
address_of(arr[i]) = base_address + (i × element_size)
\end{verbatim}

This is why access and updates are constant time. The difference between
static arrays and dynamically allocated ones often comes down to where
the memory lives. Arrays declared inside a function may live on the
stack, offering fast allocation and automatic cleanup. Larger arrays or
arrays whose size isn't known at compile time are allocated on the heap,
requiring runtime management via calls such as \texttt{malloc} and
\texttt{free}.

The cache hierarchy makes arrays especially efficient. Because elements
are contiguous, accessing \texttt{arr{[}i{]}} loads not just one element
but also its neighbors into a cache line (often 64 bytes). This
property, known as spatial locality, means that scanning through an
array is very fast. Prefetchers in modern CPUs exploit this by pulling
in upcoming cache lines before they are needed. However, irregular
access patterns (e.g., striding by 17) can defeat prefetching and lead
to performance drops.

Alignment and padding further influence performance. On most systems,
integers must start at addresses divisible by 4, and doubles at
addresses divisible by 8. If the compiler cannot guarantee alignment, it
may add padding bytes to enforce it. Misaligned accesses can cause
slowdowns or even hardware faults on strict architectures.

Different programming languages expose these behaviors differently. In
C, a declaration like \texttt{int\ arr{[}10{]};} on the stack creates
exactly 40 bytes on a 32-bit system. In contrast,
\texttt{malloc(10\ *\ sizeof(int))} allocates memory on the heap. In
C++, \texttt{std::array\textless{}int,\ 10\textgreater{}} is a safer
wrapper around C arrays, while
\texttt{std::vector\textless{}int\textgreater{}} adds resizing at the
cost of indirection and metadata. In Fortran and NumPy, multidimensional
arrays can be stored in column-major order rather than row-major, which
changes how indices map to addresses and affects iteration performance.

The operating system kernel makes heavy use of static arrays. For
example, Linux defines fixed-size arrays in structures like
\texttt{task\_struct} for file descriptors, and uses arrays in page
tables for managing memory mappings. Static arrays provide
predictability and remove the need for runtime memory allocation in
performance-critical or security-sensitive code.

From a performance profiling standpoint, arrays reveal fundamental
trade-offs. Shifting elements during insertion or deletion requires
copying bytes across memory, and the cost grows linearly with the number
of elements. Compilers attempt to optimize loops over arrays with
vectorization, turning element-wise operations into SIMD instructions.
They may also apply loop unrolling or bounds-check elimination (BCE)
when it can be proven that indices remain safe.

Static arrays also carry risks. In C and C++, accessing out-of-bounds
memory leads to undefined behavior, often exploited in buffer overflow
attacks. Languages like Java or Python mitigate this with runtime bounds
checks, but at the expense of some performance.

At this level, static arrays should be seen not only as a data structure
but as a fundamental contract between code, compiler, and hardware.

Worked Example (C)

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdio.h\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \CommentTok{// Static array of 8 integers allocated on the stack}
    \DataTypeTok{int}\NormalTok{ arr}\OperatorTok{[}\DecValTok{8}\OperatorTok{];}

    \CommentTok{// Initialize array}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{8}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        arr}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=} \OperatorTok{(}\NormalTok{i }\OperatorTok{+} \DecValTok{1}\OperatorTok{)} \OperatorTok{*} \DecValTok{10}\OperatorTok{;}
    \OperatorTok{\}}

    \CommentTok{// Access and update element}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Element at index 3: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{[}\DecValTok{3}\OperatorTok{]);}
\NormalTok{    arr}\OperatorTok{[}\DecValTok{3}\OperatorTok{]} \OperatorTok{=} \DecValTok{99}\OperatorTok{;}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Updated element at index 3: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{[}\DecValTok{3}\OperatorTok{]);}

    \CommentTok{// Traverse with cache{-}friendly pattern}
    \DataTypeTok{int}\NormalTok{ sum }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{8}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        sum }\OperatorTok{+=}\NormalTok{ arr}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \OperatorTok{\}}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Sum of array: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ sum}\OperatorTok{);}

    \CommentTok{// Dangerous: Uncommenting would cause undefined behavior}
    \CommentTok{// printf("\%d\textbackslash{}n", arr[10]);}

    \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This C program demonstrates how static arrays live on the stack, how
indexing works, and why out-of-bounds access is dangerous. On real
hardware, iterating sequentially benefits from spatial locality, making
the traversal very fast compared to random access.

\subsubsection{Why it matters}\label{why-it-matters-14}

Static arrays are the substrate upon which much of computing is built.
They are simple in abstraction but complex in practice, touching
compilers, operating systems, and hardware. Understanding them is
essential for:

\begin{itemize}
\tightlist
\item
  Writing cache-friendly and high-performance code.
\item
  Avoiding security vulnerabilities like buffer overflows.
\item
  Appreciating why higher-level data structures behave the way they do.
\item
  Building intuition for memory layout, alignment, and the interaction
  between code and the CPU.
\end{itemize}

Arrays are not just ``collections of values'' --- they are the
foundation of efficient data processing.

\subsubsection{Exercises}\label{exercises-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In C, declare a static array of size 16 and measure how long it takes
  to sum its elements sequentially versus accessing them in steps of 4.
  Explain the performance difference.
\item
  Explain why iterating over a 2D array row by row is faster in C than
  column by column.
\item
  Consider a struct with mixed types (e.g., \texttt{char}, \texttt{int},
  \texttt{double}). Predict where padding bytes will be inserted if
  placed inside an array.
\item
  Research and describe how the Linux kernel uses static arrays in
  managing processes or memory.
\item
  Demonstrate with code how accessing beyond the end of a static array
  in C can cause undefined behavior, and explain why this is a serious
  risk in system programming.
\end{enumerate}

\section{2.2 Dynamic Arrays}\label{dynamic-arrays}

\subsection{2.2 L0 --- Arrays That Grow}\label{l0-arrays-that-grow-1}

A dynamic array is like a container that can expand and shrink as
needed. Unlike static arrays, which must know their size in advance, a
dynamic array adapts as elements are added or removed. You can think of
it as a bookshelf where new shelves appear automatically when space runs
out. The underlying idea is simple: keep the benefits of fast
index-based access, while adding flexibility to change the size.

\subsubsection{Deep Dive}\label{deep-dive-6}

A dynamic array begins with a fixed amount of space called its capacity.
When the number of elements (the length) exceeds this capacity, the
array grows. This is usually done by allocating a new, larger block of
memory and copying the old elements into it. After this, new elements
can be added until the new capacity is filled, at which point the
process repeats.

Despite this resizing process, the key properties remain:

\begin{itemize}
\tightlist
\item
  Fast access and update: Elements can still be reached instantly using
  an index.
\item
  Append flexibility: New elements can be added at the end without
  worrying about fixed size.
\item
  Occasional resizing cost: Most appends are quick, but when resizing
  happens, it takes longer because all elements must be copied.
\end{itemize}

The performance picture is intuitive:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3247}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4935}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time Complexity (Typical)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Index maps directly to position \\
Update element & O(1) & Replace value in place \\
Append element & O(1) amortized & Occasionally O(n) when resizing
occurs \\
Pop element & O(1) & Remove from end \\
Insert/Delete & O(n) & Elements must be shifted \\
\end{longtable}

Dynamic arrays therefore trade predictability for flexibility. The
occasional slow operation is outweighed by the ability to grow and
shrink on demand, which makes them useful for most real-world tasks
where the number of elements is not known in advance.

\subsubsection{Worked Example}\label{worked-example-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a dynamic array using Python\textquotesingle{}s built{-}in list}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Append elements (array grows automatically)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
\NormalTok{    arr.append((i }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{10}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after appending:"}\NormalTok{, arr)}

\CommentTok{\# Access and update elements}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 2:"}\NormalTok{, arr[}\DecValTok{2}\NormalTok{])}
\NormalTok{arr[}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated array:"}\NormalTok{, arr)}

\CommentTok{\# Remove last element}
\NormalTok{last }\OperatorTok{=}\NormalTok{ arr.pop()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Removed element:"}\NormalTok{, last)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after pop:"}\NormalTok{, arr)}

\CommentTok{\# Traverse array}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(arr)):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{arr[i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This short program shows how a dynamic array in Python resizes
automatically with \texttt{append} and shrinks with \texttt{pop}. Access
and updates remain instant, while resizing happens invisibly when more
space is needed.

\subsubsection{Why it matters}\label{why-it-matters-15}

Dynamic arrays combine efficiency and flexibility. They allow programs
to handle unknown or changing amounts of data without predefining sizes.
They form the backbone of lists in high-level languages, balancing
performance with usability. They also illustrate the idea of amortized
cost: most operations are fast, but occasional expensive operations are
averaged out over time.

\subsubsection{Exercises}\label{exercises-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array and append numbers 1 through 10. Print the final
  array.
\item
  Replace the 3rd element with a new value.
\item
  Remove the last two elements and print the result.
\item
  Write a procedure that traverses a dynamic array and computes the
  average of its elements.
\item
  Explain why appending one element might sometimes be much slower than
  appending another, even though both look the same in code.
\end{enumerate}

\subsection{2.2 L1 --- Dynamic Arrays in
Practice}\label{l1-dynamic-arrays-in-practice}

Dynamic arrays extend the idea of static arrays by making size flexible.
They allow adding or removing elements without knowing the total number
in advance. Under the hood, this flexibility is achieved through careful
memory management: the array is stored in a contiguous block, but when
more space is needed, a larger block is allocated, and all elements are
copied over. This mechanism balances speed with adaptability and is the
reason why dynamic arrays are the default sequence type in many
languages.

\subsubsection{Deep Dive}\label{deep-dive-7}

A dynamic array starts with a certain capacity, often larger than the
initial number of elements. When the number of stored elements exceeds
capacity, the array is resized. The common strategy is to double the
capacity. For example, an array of capacity 4 that becomes full will
reallocate to capacity 8. All existing elements are copied into the new
block, and the old memory is freed.

This strategy makes appending efficient on average. While an individual
resize costs O(n) because of the copying, most appends are O(1). Across
a long sequence of operations, the total cost averages out --- this is
called amortized analysis.

Dynamic arrays retain the key advantages of static arrays:

\begin{itemize}
\tightlist
\item
  Contiguous storage means fast random access with \texttt{O(1)} time.
\item
  Updates are also \texttt{O(1)} because they overwrite existing slots.
\end{itemize}

The challenges appear with other operations:

\begin{itemize}
\tightlist
\item
  Insertions or deletions in the middle require shifting elements,
  making them O(n).
\item
  Resizing events create temporary latency spikes, especially when
  arrays are large.
\end{itemize}

A clear summary:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Time Complexity & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & O(1) & Direct index calculation \\
Update element & O(1) & Replace value in place \\
Append element & O(1) amortized & Occasional O(n) when resizing \\
Pop element & O(1) & Remove from end \\
Insert/Delete & O(n) & Shifting elements required \\
\end{longtable}

\paragraph{Trade-offs.}\label{trade-offs.-1}

Dynamic arrays sacrifice predictability for convenience. Resizing causes
performance spikes, but the doubling strategy keeps the average cost
low. Over-allocation wastes some memory, but it reduces the frequency of
resizes. The key is that this trade-off is usually favorable in
practice.

\paragraph{Use cases.}\label{use-cases.-1}

Dynamic arrays are well-suited for:

\begin{itemize}
\tightlist
\item
  Lists whose size is not known in advance.
\item
  Workloads dominated by appending and reading values.
\item
  General-purpose data structures in high-level programming languages.
\end{itemize}

\paragraph{Language implementations.}\label{language-implementations.}

\begin{itemize}
\tightlist
\item
  Python: \texttt{list} is a dynamic array, using an over-allocation
  strategy to reduce frequent resizes.
\item
  C++: \texttt{std::vector} doubles its capacity when needed,
  invalidating pointers/references after reallocation.
\item
  Java: \texttt{ArrayList} grows by about 1.5× when full, trading memory
  efficiency for fewer copies.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-1}

\begin{itemize}
\tightlist
\item
  In languages with pointers or references, resizes can invalidate
  existing references.
\item
  Large arrays may cause noticeable latency during reallocation.
\item
  Middle insertions and deletions remain inefficient compared to linked
  structures.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrate dynamic array behavior using Python\textquotesingle{}s list}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Append elements to trigger resizing internally}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{12}\NormalTok{):}
\NormalTok{    arr.append(i)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Appended }\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{\}}\SpecialStringTok{, length = }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(arr)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# Access and update}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at index 5:"}\NormalTok{, arr[}\DecValTok{5}\NormalTok{])}
\NormalTok{arr[}\DecValTok{5}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated element at index 5:"}\NormalTok{, arr[}\DecValTok{5}\NormalTok{])}

\CommentTok{\# Insert in the middle (expensive operation)}
\NormalTok{arr.insert(}\DecValTok{6}\NormalTok{, }\DecValTok{123}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after middle insert:"}\NormalTok{, arr)}

\CommentTok{\# Pop elements}
\NormalTok{arr.pop()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Array after pop:"}\NormalTok{, arr)}
\end{Highlighting}
\end{Shaded}

This example illustrates appending, updating, inserting, and popping.
While Python hides the resizing, the cost is there: occasionally the
list must allocate more space and copy its contents.

\subsubsection{Why it matters}\label{why-it-matters-16}

Dynamic arrays balance flexibility and performance. They demonstrate the
principle of amortized complexity, showing how expensive operations can
be smoothed out over time. They also highlight trade-offs between memory
usage and speed. Understanding them explains why high-level lists
perform well in everyday coding but also where they can fail under
stress.

\subsubsection{Exercises}\label{exercises-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a dynamic array and append the numbers 1 to 20. Measure how
  many times resizing would have occurred if the growth factor were 2.
\item
  Insert an element into the middle of a large array and explain why
  this operation is slower than appending at the end.
\item
  Write a procedure to remove all odd numbers from a dynamic array.
\item
  Compare Python's \texttt{list}, Java's \texttt{ArrayList}, and C++'s
  \texttt{std::vector} in terms of growth strategy.
\item
  Explain why references to elements of a \texttt{std::vector} may
  become invalid after resizing.
\end{enumerate}

\subsection{2.2 L2 --- Dynamic Arrays Under the
Hood}\label{l2-dynamic-arrays-under-the-hood}

Dynamic arrays reveal how high-level flexibility is built on top of
low-level memory management. While they appear as resizable containers,
underneath they are carefully engineered to balance performance, memory
efficiency, and safety. Understanding their internals sheds light on
allocators, cache behavior, and the risks of pointer invalidation.

\subsubsection{Deep Dive}\label{deep-dive-8}

Dynamic arrays rely on heap allocation. When first created, they reserve
a contiguous memory block with some capacity. As elements are appended
and the array fills, the implementation must allocate a new, larger
block, copy all existing elements, and free the old block.

Most implementations use a geometric growth strategy, often doubling the
capacity when space runs out. Some use a factor smaller than two, such
as 1.5×, to reduce memory waste. The trade-off is between speed and
efficiency:

\begin{itemize}
\tightlist
\item
  Larger growth factors reduce the number of costly reallocations.
\item
  Smaller growth factors waste less memory but increase resize
  frequency.
\end{itemize}

This leads to an amortized O(1) cost for append. Each resize is
expensive, but they happen infrequently enough that the average cost
remains constant across many operations.

However, resizes have side effects:

\begin{itemize}
\tightlist
\item
  Pointer invalidation: In C++ \texttt{std::vector}, any reference,
  pointer, or iterator into the old memory becomes invalid after
  reallocation.
\item
  Latency spikes: Copying thousands or millions of elements in one step
  can stall a program, especially in real-time or low-latency systems.
\item
  Allocator fragmentation: Repeated growth and shrink cycles can
  fragment the heap, reducing performance in long-running systems.
\end{itemize}

Cache efficiency is one of the strengths of dynamic arrays. Because
elements are stored contiguously, traversals are cache-friendly, and
prefetchers can load entire blocks into cache lines. But reallocations
can disrupt locality temporarily, as the array may move to a new region
of memory.

Different languages implement dynamic arrays with variations:

\begin{itemize}
\tightlist
\item
  Python lists use over-allocation with a small growth factor
  (\textasciitilde12.5\% to 25\% extra). This minimizes wasted memory
  while keeping amortized costs stable.
\item
  C++ \texttt{std::vector} typically doubles its capacity when needed.
  Developers can call \texttt{reserve()} to preallocate memory and avoid
  repeated reallocations.
\item
  Java \texttt{ArrayList} grows by \textasciitilde1.5×, balancing heap
  usage with resize frequency.
\end{itemize}

Dynamic arrays also face risks:

\begin{itemize}
\tightlist
\item
  If resizing logic is incorrect, buffer overflows may occur.
\item
  Attackers can exploit repeated growth/shrink cycles to cause
  denial-of-service via frequent allocations.
\item
  Very large allocations can fail outright if memory is exhausted.
\end{itemize}

From a profiling perspective, workloads matter. Append-heavy patterns
perform extremely well due to amortization. Insert-heavy or
middle-delete workloads perform poorly because of element shifting.
Allocator-aware optimizations, like pre-reserving capacity, can
dramatically improve performance.

\subsubsection{Worked Example (C++)}\label{worked-example-c}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}iostream\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}vector\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \BuiltInTok{std::}\NormalTok{vector}\OperatorTok{\textless{}}\DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ v}\OperatorTok{;}
\NormalTok{    v}\OperatorTok{.}\NormalTok{reserve}\OperatorTok{(}\DecValTok{4}\OperatorTok{);}  \CommentTok{// reserve space for 4 elements to reduce reallocations}

    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{12}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        v}\OperatorTok{.}\NormalTok{push\_back}\OperatorTok{(}\NormalTok{i }\OperatorTok{*} \DecValTok{10}\OperatorTok{);}
        \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Appended "} \OperatorTok{\textless{}\textless{}}\NormalTok{ i}\OperatorTok{*}\DecValTok{10}
                  \OperatorTok{\textless{}\textless{}} \StringTok{", size = "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{.}\NormalTok{size}\OperatorTok{()}
                  \OperatorTok{\textless{}\textless{}} \StringTok{", capacity = "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{.}\NormalTok{capacity}\OperatorTok{()} \OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}
    \OperatorTok{\}}

    \CommentTok{// Access and update}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Element at index 5: "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}
\NormalTok{    v}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{=} \DecValTok{99}\OperatorTok{;}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Updated element at index 5: "} \OperatorTok{\textless{}\textless{}}\NormalTok{ v}\OperatorTok{[}\DecValTok{5}\OperatorTok{]} \OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}

    \CommentTok{// Demonstrate invalidation risk}
    \DataTypeTok{int}\OperatorTok{*}\NormalTok{ ptr }\OperatorTok{=} \OperatorTok{\&}\NormalTok{v}\OperatorTok{[}\DecValTok{0}\OperatorTok{];}
\NormalTok{    v}\OperatorTok{.}\NormalTok{push\_back}\OperatorTok{(}\DecValTok{12345}\OperatorTok{);} \CommentTok{// may reallocate and move data}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Old pointer may now be invalid: "} \OperatorTok{\textless{}\textless{}} \OperatorTok{*}\NormalTok{ptr }\OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;} \CommentTok{// UB if reallocated}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This program shows how \texttt{std::vector} manages capacity. The output
reveals how capacity grows as more elements are appended. The pointer
invalidation example highlights a subtle but critical risk: after a
resize, old addresses into the array are no longer safe.

\subsubsection{Why it matters}\label{why-it-matters-17}

Dynamic arrays expose the tension between abstraction and reality. They
appear simple, but internally they touch almost every layer of the
system: heap allocators, caches, compiler optimizations, and safety
checks. They are essential for understanding how high-level languages
achieve both usability and performance, and they illustrate real-world
engineering trade-offs between speed, memory, and safety.

\subsubsection{Exercises}\label{exercises-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In C++, measure the capacity growth of a
  \texttt{std::vector\textless{}int\textgreater{}} as you append 1,000
  elements. Plot size vs capacity.
\item
  Explain why a program that repeatedly appends and deletes elements
  might fragment the heap over time.
\item
  Compare the growth strategies of Python \texttt{list}, C++
  \texttt{std::vector}, and Java \texttt{ArrayList}. Which wastes more
  memory? Which minimizes resize cost?
\item
  Write a program that appends 1 million integers to a dynamic array and
  then times the traversal. Compare it with inserting 1 million integers
  at the beginning.
\item
  Show how \texttt{reserve()} in \texttt{std::vector} or
  \texttt{ensureCapacity()} in Java \texttt{ArrayList} can eliminate
  costly reallocation spikes.
\end{enumerate}

\section{2.3 Slices \& Views}\label{slices-views}

\subsection{2.3 L0 --- Looking Through a
Window}\label{l0-looking-through-a-window}

A slice or view is a way to look at part of an array without creating a
new one. Instead of copying data, a slice points to the same underlying
elements, just with its own start and end boundaries. This makes working
with subarrays fast and memory-efficient. You can think of a slice as a
window into a longer row of boxes, showing only the portion you care
about.

\subsubsection{Deep Dive}\label{deep-dive-9}

When you take a slice, you don't get a new array filled with copied
elements. Instead, you get a new ``view'' that remembers where in the
original array it starts and stops. This is useful because:

\begin{itemize}
\tightlist
\item
  No copying means creating a slice is very fast.
\item
  Shared storage means changes in the slice also affect the original
  array (in languages like Go, Rust, or NumPy).
\item
  Reduced scope means you can focus on a part of the array without
  carrying the entire structure.
\end{itemize}

Key properties of slices:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They refer to the same memory as the original array.
\item
  They have their own length (number of elements visible).
\item
  They may also carry a capacity, which limits how far they can expand
  into the original array.
\end{enumerate}

In Python, list slicing (\texttt{arr{[}2:5{]}}) creates a new list with
copies of the elements. This is not a true view. By contrast, NumPy
arrays, Go slices, and Rust slices provide real views --- updates to the
slice affect the original array.

A summary:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & Slice/View & New Array (Copy) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Memory usage & Shares existing storage & Allocates new storage \\
Creation cost & O(1) & O(n) for copied elements \\
Updates & Affect original array & Independent \\
Safety & Risk of aliasing issues & No shared changes \\
\end{longtable}

Slices are especially valuable when working with large datasets, where
copying would be too expensive.

\subsubsection{Worked Example}\label{worked-example-4}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Python slicing creates a copy, but useful to illustrate concept}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{]}

\CommentTok{\# Slice of middle part}
\NormalTok{sub }\OperatorTok{=}\NormalTok{ arr[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original array:"}\NormalTok{, arr)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Slice (copy in Python):"}\NormalTok{, sub)}

\CommentTok{\# Modifying the slice does not affect the original (Python behavior)}
\NormalTok{sub[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Modified slice:"}\NormalTok{, sub)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Original array unchanged:"}\NormalTok{, arr)}

\CommentTok{\# In contrast, NumPy arrays behave like true views}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{arr\_np }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{])}
\NormalTok{sub\_np }\OperatorTok{=}\NormalTok{ arr\_np[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{sub\_np[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy slice reflects back:"}\NormalTok{, arr\_np)}
\end{Highlighting}
\end{Shaded}

This example shows the difference: Python lists create a copy, while
NumPy slices act as views and affect the original.

\subsubsection{Why it matters}\label{why-it-matters-18}

Slices let you work with subsets of data without wasting memory or time
copying. They are critical in systems and scientific computing where
performance matters. They also highlight the idea of aliasing: when two
names refer to the same data. Understanding slices teaches you when
changes propagate and when they don't, which helps avoid surprising
bugs.

\subsubsection{Exercises}\label{exercises-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an array of 10 numbers. Take a slice of the middle 5 elements
  and print them.
\item
  Update the first element in your slice and describe what happens to
  the original array in your chosen language.
\item
  Compare slicing behavior in Python and NumPy: which one copies, which
  one shares?
\item
  Explain why slicing a very large dataset is more efficient than
  copying it.
\item
  Think of a real-world analogy where two people share the same resource
  but only see part of it. How does this relate to slices?
\end{enumerate}

\subsection{2.3 L1 --- Slices in Practice}\label{l1-slices-in-practice}

Slices provide a practical way to work with subarrays efficiently.
Instead of copying data into a new structure, a slice acts as a
lightweight reference to part of an existing array. This gives
programmers flexibility to manipulate sections of data without paying
the cost of duplication, while still preserving the familiar indexing
model of arrays.

\subsubsection{Deep Dive}\label{deep-dive-10}

At the implementation level, a slice is typically represented by a small
structure that stores:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A pointer to the first element in the slice.
\item
  The slice's length (how many elements it can access).
\item
  Optionally, its capacity (how far the slice can grow into the backing
  array).
\end{enumerate}

Indexing into a slice works just like indexing into an array:

\begin{verbatim}
slice[i] → base_address + i × element_size
\end{verbatim}

The complexity model stays consistent:

\begin{itemize}
\tightlist
\item
  Slice creation: O(1) when implemented as a view, O(n) if the language
  copies elements.
\item
  Access/update: O(1), just like arrays.
\item
  Traversal: O(k), proportional to the slice's length.
\end{itemize}

This design makes slices efficient but introduces trade-offs. With true
views, the slice and the original array share memory. Updates made
through one are visible through the other. This can be extremely useful
but also dangerous, as it introduces the possibility of unintended side
effects. Languages that prioritize safety (like Python lists) avoid this
by returning a copy instead of a view.

The balance is clear:

\begin{itemize}
\tightlist
\item
  Views (Go, Rust, NumPy): fast and memory-efficient, but require
  discipline to avoid aliasing bugs.
\item
  Copies (Python lists): safer, but slower and more memory-intensive for
  large arrays.
\end{itemize}

A summary of behaviors:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1591}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1591}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Language/Library
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Slice Behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shared Updates
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Go & View & Yes & Backed by \texttt{(ptr,\ len,\ cap)} triple \\
Rust & View & Yes & Safe with borrow checker (mutable/immutable) \\
Python list & Copy & No & Safer but memory-expensive \\
NumPy array & View & Yes & Basis of efficient scientific computing \\
C/C++ & Manual pointer & Yes & No built-in slice type; must manage
manually \\
\end{longtable}

\paragraph{Use cases.}\label{use-cases.-2}

\begin{itemize}
\tightlist
\item
  Processing large datasets in segments without copying.
\item
  Implementing algorithms like sliding windows, partitions, or
  block-based iteration.
\item
  Sharing views of arrays across functions for modular design without
  allocating new memory.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-2}

\begin{itemize}
\tightlist
\item
  In languages with views, careless updates can corrupt the original
  array unexpectedly.
\item
  In Go and C++, extending a slice/view beyond its capacity causes
  runtime errors or undefined behavior.
\item
  In Python, forgetting that slices are copies can lead to performance
  issues in large-scale workloads.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-5}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrating copy slices vs view slices in Python and NumPy}

\CommentTok{\# Python list slicing creates a copy}
\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}
\NormalTok{sub }\OperatorTok{=}\NormalTok{ arr[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{sub[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Python original:"}\NormalTok{, arr)  }\CommentTok{\# unchanged}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Python slice (copy):"}\NormalTok{, sub)}

\CommentTok{\# NumPy slicing creates a view}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{arr\_np }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{sub\_np }\OperatorTok{=}\NormalTok{ arr\_np[}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\NormalTok{sub\_np[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy original (affected):"}\NormalTok{, arr\_np)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy slice (view):"}\NormalTok{, sub\_np)}
\end{Highlighting}
\end{Shaded}

This example shows the key difference: Python lists copy, while NumPy
provides true views. The choice reflects different design priorities:
safety in Python's core data structures versus performance in numerical
computing.

\subsubsection{Why it matters}\label{why-it-matters-19}

Slices make programs more efficient and expressive. They eliminate
unnecessary copying, speed up algorithms that work on subranges, and
support modular programming by passing references instead of duplicating
data. At the same time, they expose important design trade-offs between
safety and performance. Understanding slices provides insight into how
modern languages manage memory efficiently while protecting against
common errors.

\subsubsection{Exercises}\label{exercises-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In Go, create an array of 10 elements and take a slice of the middle
  5. Update the slice and observe the effect on the array.
\item
  In Python, slice a list of 1 million numbers and explain the
  performance cost compared to slicing a NumPy array of the same size.
\item
  Write a procedure that accepts a slice and doubles each element. Test
  with both a copy-based language (Python lists) and a view-based
  language (NumPy or Go).
\item
  Explain why passing slices to functions is more memory-efficient than
  passing entire arrays.
\item
  Discuss a scenario where slice aliasing could lead to unintended bugs
  in a large program.
\end{enumerate}

\subsection{2.3 L2 --- Slices and Views in
Systems}\label{l2-slices-and-views-in-systems}

Slices are not just convenient programming shortcuts; they represent a
powerful abstraction that ties language semantics to hardware realities.
At this level, slices expose details about memory layout, lifetime, and
compiler optimizations. They are central to performance-critical systems
because they allow efficient access to subsets of data without copying,
while also demanding careful handling to avoid aliasing bugs and unsafe
memory access.

\subsubsection{Deep Dive}\label{deep-dive-11}

A slice is typically represented internally as a triple:

\begin{itemize}
\tightlist
\item
  A pointer to the first element,
\item
  A length describing how many elements are visible,
\item
  A capacity showing how far the slice may extend into the backing
  array.
\end{itemize}

Indexing into a slice is still O(1), but the compiler inserts bounds
checks to prevent invalid access. In performance-sensitive code,
compilers often apply bounds-check elimination (BCE) when they can prove
that loop indices remain within safe limits. This allows slices to
combine safety with near-native performance.

Slices are non-owning references. They do not manage memory themselves
but instead depend on the underlying array. In languages like Rust, the
borrow checker enforces lifetimes to prevent dangling slices. In C and
C++, however, programmers must manually ensure that the backing array
outlives the slice, or risk undefined behavior.

Because slices share memory, they introduce aliasing. Multiple slices
can point to overlapping regions of the same array. This can lead to
subtle bugs if two parts of a program update the same region
concurrently. In multithreaded contexts, mutable aliasing without
synchronization can cause data races. Some systems adopt copy-on-write
strategies to reduce risks, but this adds overhead.

From a performance perspective, slices preserve contiguity, which is
ideal for cache locality and prefetching. Sequential traversal is
cache-friendly, but strided access (e.g., every 3rd element) can defeat
hardware prefetchers, reducing efficiency. Languages like NumPy exploit
strides explicitly, enabling both dense and sparse-like views without
copying.

Language designs differ in how they handle slices:

\begin{itemize}
\tightlist
\item
  Go uses \texttt{(ptr,\ len,\ cap)}. Appending to a slice may allocate
  a new array if capacity is exceeded, silently detaching it from the
  original backing storage.
\item
  Rust distinguishes \texttt{\&{[}T{]}} for immutable and
  \texttt{\&mut\ {[}T{]}} for mutable slices, with the compiler
  enforcing safe borrowing rules.
\item
  C/C++ provide no built-in slice type, so developers rely on raw
  pointers and manual length tracking. This is flexible but error-prone.
\item
  NumPy supports advanced slicing: views with strides, broadcasting
  rules, and multidimensional slices for scientific computing.
\end{itemize}

Compilers also optimize slice-heavy code:

\begin{itemize}
\tightlist
\item
  Vectorization transforms element-wise loops into SIMD instructions
  when slices are contiguous.
\item
  Escape analysis determines whether slices can stay stack-allocated or
  must be promoted to the heap.
\end{itemize}

System-level use cases highlight the importance of slices:

\begin{itemize}
\tightlist
\item
  Zero-copy I/O: network and file system buffers are exposed as slices
  into larger memory regions.
\item
  Memory-mapped files: slices map directly to disk pages, enabling
  efficient processing of large datasets.
\item
  GPU programming: CUDA and OpenCL kernels operate on slices of device
  memory, avoiding transfers.
\end{itemize}

These applications show why slices are not just a programming
convenience but a core tool for bridging high-level logic with low-level
performance.

\subsubsection{Worked Example (Go)}\label{worked-example-go}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{package}\NormalTok{ main}

\KeywordTok{import} \StringTok{"fmt"}

\KeywordTok{func}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
\NormalTok{    arr }\OperatorTok{:=} \OperatorTok{[}\DecValTok{6}\OperatorTok{]}\DataTypeTok{int}\OperatorTok{\{}\DecValTok{10}\OperatorTok{,} \DecValTok{20}\OperatorTok{,} \DecValTok{30}\OperatorTok{,} \DecValTok{40}\OperatorTok{,} \DecValTok{50}\OperatorTok{,} \DecValTok{60}\OperatorTok{\}}
\NormalTok{    s }\OperatorTok{:=}\NormalTok{ arr}\OperatorTok{[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\OperatorTok{]} \CommentTok{// slice referencing elements 20, 30, 40}

\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Original array:"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{)}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Slice view:"}\OperatorTok{,}\NormalTok{ s}\OperatorTok{)}

    \CommentTok{// Update through slice}
\NormalTok{    s}\OperatorTok{[}\DecValTok{0}\OperatorTok{]} \OperatorTok{=} \DecValTok{99}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"After update via slice, array:"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{)}

    \CommentTok{// Demonstrate capacity}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Slice length:"}\OperatorTok{,} \BuiltInTok{len}\OperatorTok{(}\NormalTok{s}\OperatorTok{),} \StringTok{"capacity:"}\OperatorTok{,} \BuiltInTok{cap}\OperatorTok{(}\NormalTok{s}\OperatorTok{))}

    \CommentTok{// Appending beyond slice capacity reallocates}
\NormalTok{    s }\OperatorTok{=} \BuiltInTok{append}\OperatorTok{(}\NormalTok{s}\OperatorTok{,} \DecValTok{70}\OperatorTok{,} \DecValTok{80}\OperatorTok{)}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Slice after append:"}\OperatorTok{,}\NormalTok{ s}\OperatorTok{)}
\NormalTok{    fmt}\OperatorTok{.}\NormalTok{Println}\OperatorTok{(}\StringTok{"Array after append (unchanged):"}\OperatorTok{,}\NormalTok{ arr}\OperatorTok{)}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This example illustrates Go's slice model. The slice \texttt{s}
initially shares storage with \texttt{arr}. Updates propagate to the
array. However, when appending exceeds the slice's capacity, Go
allocates a new backing array, breaking the link with the original. This
behavior is efficient but can surprise developers if not understood.

\subsubsection{Why it matters}\label{why-it-matters-20}

Slices embody key system concepts: pointer arithmetic, memory ownership,
cache locality, and aliasing. They explain how languages achieve
zero-copy abstractions while balancing safety and performance. They also
highlight risks such as dangling references and silent reallocations.
Mastery of slices is essential for building efficient algorithms,
avoiding memory errors, and reasoning about system-level performance.

\subsubsection{Exercises}\label{exercises-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In Go, create an array of 8 integers and take overlapping slices.
  Modify one slice and observe effects on the other. Explain why this
  happens.
\item
  In Rust, attempt to create two mutable slices of the same array
  region. Explain why the borrow checker rejects it.
\item
  In C, simulate a slice using a pointer and a length. Show what happens
  if the backing array is freed while the slice is still in use.
\item
  In NumPy, create a 2D array and take a strided slice (every second
  row). Explain why performance is worse than contiguous slicing.
\item
  Compare how Python, Go, and Rust enforce (or fail to enforce) safety
  when working with slices.
\end{enumerate}

\section{2.4 Multidimensional Arrays}\label{multidimensional-arrays}

\subsection{2.4 L0 --- Tables and Grids}\label{l0-tables-and-grids}

A multidimensional array is an extension of the simple array idea.
Instead of storing data in a single row, a multidimensional array
organizes elements in a grid, table, or cube. The most common example is
a two-dimensional array, which looks like a table with rows and columns.
Each position in the grid is identified by two coordinates: one for the
row and one for the column. This structure is useful for representing
spreadsheets, images, game boards, and mathematical matrices.

\subsubsection{Deep Dive}\label{deep-dive-12}

You can think of a multidimensional array as an array of arrays. A
two-dimensional array is a list where each element is itself another
list. For example, a 3×3 table contains 3 rows, each of which has 3
columns. Accessing an element requires specifying both coordinates:
\texttt{arr{[}row{]}{[}col{]}}.

Even though we visualize multidimensional arrays as grids, in memory
they are still stored as a single continuous sequence. To find an
element, the program computes its position using a formula. In a 2D
array with \texttt{n} columns, the element at \texttt{(row,\ col)} is
located at:

\begin{verbatim}
index = row × n + col
\end{verbatim}

This mapping allows direct access in constant time, just like with 1D
arrays.

Common operations are:

\begin{itemize}
\tightlist
\item
  Creation: decide dimensions and initialize with values.
\item
  Access: specify row and column to retrieve an element.
\item
  Update: change the value at a given coordinate.
\item
  Traversal: visit elements row by row or column by column.
\end{itemize}

A quick summary:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Operation & Description & Cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Access element & Get value at (row, col) & O(1) \\
Update element & Replace value at (row, col) & O(1) \\
Traverse array & Visit all elements & O(n×m) \\
\end{longtable}

Multidimensional arrays introduce an important detail: traversal order.
In many languages (like C and Python's NumPy), arrays are stored in
row-major order, which means all elements of the first row are laid out
contiguously, then the second row, and so on. Others, like Fortran, use
column-major order. This difference affects performance in more advanced
topics, but at this level, the key idea is that access is still fast and
predictable.

\subsubsection{Worked Example}\label{worked-example-6}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a 2D array (3x3 table) using list of lists}
\NormalTok{table }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{],}
\NormalTok{    [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{],}
\NormalTok{    [}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{]}
\NormalTok{]}

\CommentTok{\# Access element in second row, third column}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at (1, 2):"}\NormalTok{, table[}\DecValTok{1}\NormalTok{][}\DecValTok{2}\NormalTok{])  }\CommentTok{\# prints 6}

\CommentTok{\# Update element}
\NormalTok{table[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{99}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Updated table:"}\NormalTok{, table)}

\CommentTok{\# Traverse row by row}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Row traversal:"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ table:}
    \ControlFlowTok{for}\NormalTok{ val }\KeywordTok{in}\NormalTok{ row:}
        \BuiltInTok{print}\NormalTok{(val, end}\OperatorTok{=}\StringTok{" "}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This example shows how to build and use a 2D array in Python. It looks
like a table, with easy access via coordinates.

\subsubsection{Why it matters}\label{why-it-matters-21}

Multidimensional arrays provide a natural way to represent structured
data like matrices, grids, and images. They allow algorithms to work
directly with two-dimensional or higher-dimensional information without
flattening everything into one long row. This makes programs easier to
write, read, and reason about.

\subsubsection{Exercises}\label{exercises-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a 3×3 array with numbers 1 through 9 and print it in a table
  format.
\item
  Access the element at row 2, column 3 and describe how you found it.
\item
  Change the center element of a 3×3 array to 0.
\item
  Write a loop to compute the sum of all values in a 4×4 array.
\item
  Explain why accessing \texttt{(row,\ col)} in a 2D array is still O(1)
  even though the data is stored in a single sequence in memory.
\end{enumerate}

\subsection{2.4 L1 --- Multidimensional Arrays in
Practice}\label{l1-multidimensional-arrays-in-practice}

Multidimensional arrays are powerful because they extend the linear
model of arrays into grids, tables, and higher dimensions. At a
practical level, they are still stored in memory as a flattened linear
block. What changes is the indexing formula: instead of a single index,
we use multiple coordinates that the system translates into one offset.

\subsubsection{Deep Dive}\label{deep-dive-13}

The most common form is a 2D array. In memory, the elements are laid out
row by row (row-major) or column by column (column-major).

\begin{itemize}
\tightlist
\item
  Row-major (C, NumPy default): elements of each row are contiguous.
\item
  Column-major (Fortran, MATLAB): elements of each column are
  contiguous.
\end{itemize}

For a 2D array with \texttt{num\_cols} columns, the element at
\texttt{(row,\ col)} in row-major order is located at:

\begin{verbatim}
index = row × num_cols + col
\end{verbatim}

For column-major order with \texttt{num\_rows} rows, the formula is:

\begin{verbatim}
index = col × num_rows + row
\end{verbatim}

This distinction matters when traversing. Accessing elements in the
memory's natural order (row by row for row-major, column by column for
column-major) is cache-friendly. Traversing in the opposite order forces
the program to jump around in memory, leading to slower performance.

Extending to 3D and higher is straightforward. For a 3D array with
\texttt{(layers,\ rows,\ cols)} in row-major order:

\begin{verbatim}
index = layer × (rows × cols) + row × cols + col
\end{verbatim}

Complexity remains consistent:

\begin{itemize}
\tightlist
\item
  Access/update: O(1) using index calculation.
\item
  Traversal: O(n × m) for 2D, O(n × m × k) for 3D.
\end{itemize}

Trade-offs:

\begin{itemize}
\tightlist
\item
  Contiguous multidimensional arrays provide excellent performance for
  predictable workloads (e.g., matrix operations).
\item
  Resizing is costly because the entire block must be reallocated.
\item
  Jagged arrays (arrays of arrays) provide flexibility but lose memory
  contiguity, reducing cache performance.
\end{itemize}

Use cases:

\begin{itemize}
\tightlist
\item
  Storing images (pixels as grids).
\item
  Mathematical matrices in scientific computing.
\item
  Game boards and maps.
\item
  Tables in database-like structures.
\end{itemize}

Different languages implement multidimensional arrays differently:

\begin{itemize}
\tightlist
\item
  Python lists: nested lists simulate 2D arrays but are jagged and
  fragmented in memory.
\item
  NumPy: provides true multidimensional arrays stored contiguously in
  row-major (default) or column-major order.
\item
  C/C++: support both contiguous multidimensional arrays
  (\texttt{int\ arr{[}rows{]}{[}cols{]};}) and pointer-based arrays of
  arrays.
\item
  Java: uses arrays of arrays (jagged by default).
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-7}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Comparing list of lists vs NumPy arrays}
\CommentTok{\# List of lists (jagged)}
\NormalTok{table }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{],}
\NormalTok{    [}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{],}
\NormalTok{    [}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{]}
\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at (2, 1):"}\NormalTok{, table[}\DecValTok{2}\NormalTok{][}\DecValTok{1}\NormalTok{])  }\CommentTok{\# 8}

\CommentTok{\# NumPy array (true contiguous 2D array)}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{matrix }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{],[}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{],[}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Element at (2, 1):"}\NormalTok{, matrix[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])  }\CommentTok{\# 8}

\CommentTok{\# Traversal in row{-}major order}
\ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(matrix.shape[}\DecValTok{0}\NormalTok{]):}
    \ControlFlowTok{for}\NormalTok{ col }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(matrix.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{        val }\OperatorTok{=}\NormalTok{ matrix[row, col]  }\CommentTok{\# efficient in NumPy}
\end{Highlighting}
\end{Shaded}

The Python list-of-lists behaves like a table, but each row may live
separately in memory. NumPy, on the other hand, stores data
contiguously, enabling much faster iteration and vectorized operations.

\subsubsection{Why it matters}\label{why-it-matters-22}

Multidimensional arrays are central to real-world applications, from
graphics and simulations to data science and machine learning. They
highlight how physical memory layout (row-major vs column-major)
interacts with algorithm design. Understanding them allows developers to
choose between safety, flexibility, and performance, depending on the
problem.

\subsubsection{Exercises}\label{exercises-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a procedure to sum all values in a 5×5 array by traversing row
  by row.
\item
  For a 3×3 NumPy array, access element \texttt{(2,1)} and explain how
  its memory index is calculated in row-major order.
\item
  Create a jagged array (rows of different lengths) in Python. Show how
  traversal differs from a true 2D array.
\item
  Explain why traversing a NumPy array by rows is faster than by
  columns.
\item
  Write a formula for computing the linear index of \texttt{(i,j,k)} in
  a 3D array stored in row-major order.
\end{enumerate}

\subsection{2.4 L2 --- Multidimensional Arrays and System
Realities}\label{l2-multidimensional-arrays-and-system-realities}

Multidimensional arrays are not only a logical abstraction but also a
system-level structure that interacts with memory layout, caches, and
compilers. At this level, understanding how they are stored, accessed,
and optimized is essential for building high-performance code in
scientific computing, graphics, and data-intensive systems.

\subsubsection{Deep Dive}\label{deep-dive-14}

A multidimensional array is stored either as a contiguous linear block
or as an array of pointers (jagged array). In the contiguous layout,
elements follow one another in memory according to a linearization
formula. In row-major order (C, NumPy), a 2D element at
\texttt{(row,\ col)} is:

\begin{verbatim}
index = row × num_cols + col
\end{verbatim}

In column-major order (Fortran, MATLAB), the formula is:

\begin{verbatim}
index = col × num_rows + row
\end{verbatim}

This difference has deep performance consequences. In row-major layout,
traversing row by row is cache-friendly because consecutive elements are
contiguous. Traversing column by column introduces large strides, which
can cause cache and TLB misses. In column-major arrays, the reverse
holds true.

\paragraph{Cache and performance.}\label{cache-and-performance.}

When an array is traversed sequentially in its natural memory order,
cache lines are used efficiently and hardware prefetchers work well.
Strided access, such as reading every k-th column in a row-major layout,
prevents prefetchers from predicting the access pattern and leads to
performance drops. For large arrays, this can mean the difference
between processing gigabytes per second and megabytes per second.

\paragraph{Alignment and padding.}\label{alignment-and-padding.}

Compilers and libraries often align rows to cache line or SIMD vector
boundaries. For example, a 64-byte cache line may cause padding to be
inserted so that each row begins on a boundary. In parallel systems,
this prevents false sharing when multiple threads process different
rows. However, padding increases memory footprint.

\paragraph{Language-level
differences.}\label{language-level-differences.}

\begin{itemize}
\tightlist
\item
  C/C++: contiguous 2D arrays (\texttt{int\ arr{[}rows{]}{[}cols{]}})
  guarantee row-major layout. Jagged arrays (array of pointers)
  sacrifice locality but allow uneven row sizes.
\item
  Fortran/MATLAB: column-major ordering dominates scientific computing,
  influencing algorithms in BLAS and LAPACK.
\item
  NumPy: stores strides explicitly, enabling flexible slicing and
  arbitrary views. Strided slices can represent transposed matrices
  without copying.
\end{itemize}

\paragraph{Optimizations.}\label{optimizations.}

\begin{itemize}
\tightlist
\item
  Loop tiling/blocking: partition loops into smaller blocks that fit
  into cache, maximizing reuse.
\item
  SIMD-friendly layouts: structure-of-arrays (SoA) improves
  vectorization compared to array-of-structures (AoS).
\item
  Matrix multiplication kernels: carefully designed to exploit cache
  hierarchy, prefetching, and SIMD registers.
\end{itemize}

\paragraph{System-level use cases.}\label{system-level-use-cases.}

\begin{itemize}
\tightlist
\item
  Image processing: images stored as row-major arrays, with pixels in
  contiguous scanlines. Efficient filters process them row by row.
\item
  GPU computing: memory coalescing requires threads in a warp to access
  contiguous memory regions; array layout directly affects throughput.
\item
  Databases: columnar storage uses column-major arrays, enabling fast
  scans and aggregation queries.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-3}

\begin{itemize}
\tightlist
\item
  Traversing in the ``wrong'' order can cause performance cliffs.
\item
  Large index calculations may overflow if not handled carefully.
\item
  Porting algorithms between row-major and column-major languages can
  introduce subtle bugs.
\end{itemize}

Profiling. Practical analysis involves comparing traversal patterns,
cache miss rates, and vectorization efficiency. Modern compilers can
eliminate redundant bounds checks and auto-vectorize well-structured
loops, but poor layout or order can block these optimizations.

\subsubsection{Worked Example (C)}\label{worked-example-c-1}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}stdio.h\textgreater{}}
\PreprocessorTok{\#define ROWS }\DecValTok{4}
\PreprocessorTok{\#define COLS }\DecValTok{4}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
    \DataTypeTok{int}\NormalTok{ arr}\OperatorTok{[}\NormalTok{ROWS}\OperatorTok{][}\NormalTok{COLS}\OperatorTok{];}

    \CommentTok{// Fill the array}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ r }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ r }\OperatorTok{\textless{}}\NormalTok{ ROWS}\OperatorTok{;}\NormalTok{ r}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ c }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ c }\OperatorTok{\textless{}}\NormalTok{ COLS}\OperatorTok{;}\NormalTok{ c}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            arr}\OperatorTok{[}\NormalTok{r}\OperatorTok{][}\NormalTok{c}\OperatorTok{]} \OperatorTok{=}\NormalTok{ r }\OperatorTok{*}\NormalTok{ COLS }\OperatorTok{+}\NormalTok{ c}\OperatorTok{;}
        \OperatorTok{\}}
    \OperatorTok{\}}

    \CommentTok{// Row{-}major traversal (cache{-}friendly in C)}
    \DataTypeTok{int}\NormalTok{ sum\_row }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ r }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ r }\OperatorTok{\textless{}}\NormalTok{ ROWS}\OperatorTok{;}\NormalTok{ r}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ c }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ c }\OperatorTok{\textless{}}\NormalTok{ COLS}\OperatorTok{;}\NormalTok{ c}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            sum\_row }\OperatorTok{+=}\NormalTok{ arr}\OperatorTok{[}\NormalTok{r}\OperatorTok{][}\NormalTok{c}\OperatorTok{];}
        \OperatorTok{\}}
    \OperatorTok{\}}

    \CommentTok{// Column traversal (less efficient in row{-}major)}
    \DataTypeTok{int}\NormalTok{ sum\_col }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ c }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ c }\OperatorTok{\textless{}}\NormalTok{ COLS}\OperatorTok{;}\NormalTok{ c}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ r }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ r }\OperatorTok{\textless{}}\NormalTok{ ROWS}\OperatorTok{;}\NormalTok{ r}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            sum\_col }\OperatorTok{+=}\NormalTok{ arr}\OperatorTok{[}\NormalTok{r}\OperatorTok{][}\NormalTok{c}\OperatorTok{];}
        \OperatorTok{\}}
    \OperatorTok{\}}

\NormalTok{    printf}\OperatorTok{(}\StringTok{"Row traversal sum: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ sum\_row}\OperatorTok{);}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Column traversal sum: }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ sum\_col}\OperatorTok{);}
    \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This program highlights traversal order. On large arrays, row-major
traversal is much faster in C because of cache-friendly memory access,
while column traversal may cause frequent cache misses.

\subsubsection{Why it matters}\label{why-it-matters-23}

Multidimensional arrays sit at the heart of performance-critical
applications. Their memory layout determines how well algorithms
interact with CPU caches, vector units, and GPUs. Understanding
row-major vs column-major, stride penalties, and cache-aware traversal
allows developers to write software that scales from toy programs to
high-performance computing systems.

\subsubsection{Exercises}\label{exercises-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In C, create a 1000×1000 matrix and measure the time difference
  between row-major and column traversal. Explain the results.
\item
  In NumPy, take a 2D array and transpose it. Use \texttt{.strides} to
  confirm that the transposed array is a view, not a copy.
\item
  Write the linear index formula for a 4D array \texttt{(a,b,c,d)} in
  row-major order.
\item
  Explain how false sharing could occur when two threads update adjacent
  rows of a large array.
\item
  Compare the impact of row-major vs column-major layout in matrix
  multiplication performance.
\end{enumerate}

\section{2.5 Sparse Arrays}\label{sparse-arrays}

\subsection{2.5 L0 --- Sparse Arrays as Empty Parking
Lots}\label{l0-sparse-arrays-as-empty-parking-lots}

A sparse array is a way of storing data when most of the positions are
empty. Instead of recording every slot like in a dense array, a sparse
array only remembers the places that hold actual values. You can think
of a huge parking lot with only a few cars parked: a dense array writes
down every spot, empty or not, while a sparse array just writes down the
locations of the cars.

\subsubsection{Deep Dive}\label{deep-dive-15}

Dense arrays are straightforward: every position has a value, even if it
is zero or unused. This makes access simple and fast, but wastes memory
if most positions are empty. Sparse arrays solve this by storing only
the useful entries.

There are many ways to represent a sparse array:

\begin{itemize}
\tightlist
\item
  Dictionary/Map: store index → value pairs, ignoring empty slots.
\item
  Coordinate list (COO): keep two lists, one for indices and one for
  values.
\item
  Run-length encoding: store stretches of empty values as counts,
  followed by the next filled value.
\end{itemize}

The key idea is to save memory at the cost of more complex indexing.
Access is no longer just arithmetic (\texttt{arr{[}i{]}}) but requires
looking up in the chosen structure.

Comparison:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4857}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Representation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Access Speed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Good For
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dense array & High & O(1) & Data with many filled elements \\
Sparse (map) & Low & O(1) average & Few filled elements, random
access \\
Sparse (list) & Very low & O(n) & Very small number of entries \\
\end{longtable}

\subsubsection{Worked Example}\label{worked-example-8}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Dense representation: wastes memory for mostly empty data}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{20}
\NormalTok{dense[}\DecValTok{3}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\NormalTok{dense[}\DecValTok{15}\NormalTok{] }\OperatorTok{=} \DecValTok{25}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dense array:"}\NormalTok{, dense)}

\CommentTok{\# Sparse representation using dictionary}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ \{}\DecValTok{3}\NormalTok{: }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{: }\DecValTok{25}\NormalTok{\}}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sparse array:"}\NormalTok{, sparse)}

\CommentTok{\# Access value}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at index 3:"}\NormalTok{, sparse.get(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at index 7:"}\NormalTok{, sparse.get(}\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{))  }\CommentTok{\# default to 0 for missing}
\end{Highlighting}
\end{Shaded}

This shows how a sparse dictionary only records the positions that
matter, while the dense version allocates space for all 20 slots.

\subsubsection{Why it matters}\label{why-it-matters-24}

Sparse arrays are crucial when working with large data where most
entries are empty. They save memory and make it possible to process huge
datasets that would not fit into memory as dense arrays. They also
appear in real-world systems like machine learning (feature vectors),
scientific computing (matrices with few non-zero entries), and search
engines (posting lists).

\subsubsection{Exercises}\label{exercises-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a sparse array of size 1000 with only 3 non-zero values at
  indices 2, 500, and 999.
\item
  Write a procedure to count the number of non-empty values in a sparse
  array.
\item
  Access an index that does not exist in the sparse array and explain
  what should be returned.
\item
  Compare the memory used by a dense array of 1000 zeros and a sparse
  representation with 3 values.
\item
  Think of a real-world example (outside programming) where recording
  only the ``non-empty'' spots is more efficient than listing
  everything.
\end{enumerate}

\subsection{2.5 L1 --- Sparse Arrays in
Practice}\label{l1-sparse-arrays-in-practice}

Sparse arrays become important when dealing with very large datasets
where only a few positions hold non-zero values. Instead of allocating
memory for every element, practical implementations use compact
structures to track only the occupied indices. This saves memory, but
requires trade-offs in access speed and update complexity.

\subsubsection{Deep Dive}\label{deep-dive-16}

There are several practical ways to represent sparse arrays:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dictionary/Hash Map

  \begin{itemize}
  \tightlist
  \item
    Store index → value pairs.
  \item
    Very fast random access and updates (average O(1)).
  \item
    Memory overhead is higher because of hash structures.
  \end{itemize}
\item
  Coordinate List (COO)

  \begin{itemize}
  \tightlist
  \item
    Keep two parallel arrays: one for indices, one for values.
  \item
    Compact, easy to construct, but access is O(n).
  \item
    Good for static data with few updates.
  \end{itemize}
\item
  Compressed Sparse Row (CSR) / Compressed Sparse Column (CSC)

  \begin{itemize}
  \tightlist
  \item
    Widely used for sparse matrices.
  \item
    Use three arrays: values, column indices, and row pointers (or vice
    versa).
  \item
    Extremely efficient for matrix-vector operations.
  \item
    Poor at dynamic updates, since compression must be rebuilt.
  \end{itemize}
\item
  Run-Length Encoding (RLE)

  \begin{itemize}
  \tightlist
  \item
    Store runs of zeros as counts, followed by non-zero entries.
  \item
    Best for sequences with long stretches of emptiness.
  \end{itemize}
\end{enumerate}

A comparison:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1020}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1633}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3469}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3878}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory Use
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Access Speed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best For
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dictionary & Higher per-entry & O(1) avg & Dynamic updates,
unpredictable indices \\
COO & Very low & O(n) & Static, small sparse sets \\
CSR/CSC & Compact & O(1) row scan, O(log n) col lookup & Linear algebra,
scientific computing \\
RLE & Very compact & Sequential O(n), random slower & Time-series with
long zero runs \\
\end{longtable}

\paragraph{Trade-offs:}\label{trade-offs}

\begin{itemize}
\tightlist
\item
  Dense arrays are fast but waste memory.
\item
  Sparse arrays save memory but access/update complexity varies.
\item
  Choice of structure depends on workload (frequent random access vs
  batch computation).
\end{itemize}

\paragraph{Use cases:}\label{use-cases}

\begin{itemize}
\tightlist
\item
  Machine learning: sparse feature vectors in text classification or
  recommender systems.
\item
  Graph algorithms: adjacency matrices for sparse graphs.
\item
  Search engines: inverted index posting lists.
\item
  Scientific computing: storing large sparse matrices for simulations.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-9}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sparse array using Python dictionary}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ \{}\DecValTok{2}\NormalTok{: }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{: }\DecValTok{50}\NormalTok{, }\DecValTok{999}\NormalTok{: }\DecValTok{7}\NormalTok{\}}

\CommentTok{\# Accessing}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at 100:"}\NormalTok{, sparse.get(}\DecValTok{100}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Value at 3 (missing):"}\NormalTok{, sparse.get(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\CommentTok{\# Inserting new value}
\NormalTok{sparse[}\DecValTok{500}\NormalTok{] }\OperatorTok{=} \DecValTok{42}

\CommentTok{\# Traversing non{-}empty values}
\ControlFlowTok{for}\NormalTok{ idx, val }\KeywordTok{in}\NormalTok{ sparse.items():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Index }\SpecialCharTok{\{}\NormalTok{idx}\SpecialCharTok{\}}\SpecialStringTok{ → }\SpecialCharTok{\{}\NormalTok{val}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For dense vs sparse comparison:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{] }\OperatorTok{*} \DecValTok{1000}
\NormalTok{dense[}\DecValTok{2}\NormalTok{], dense[}\DecValTok{100}\NormalTok{], dense[}\DecValTok{999}\NormalTok{] }\OperatorTok{=} \DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{7}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Dense uses 1000 slots, sparse uses"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(sparse), }\StringTok{"entries"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why it matters}\label{why-it-matters-25}

Sparse arrays strike a balance between memory efficiency and
performance. They let you work with massive datasets that would
otherwise be impossible to store in memory. They also demonstrate the
importance of choosing the right representation for the problem: a
dictionary for dynamic updates, CSR for scientific kernels, or RLE for
compressed logs.

\subsubsection{Exercises}\label{exercises-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Represent a sparse array of length 1000 with values at indices 2, 100,
  and 999 using:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    a dictionary, and
  \item
    two parallel lists (indices, values).
  \end{enumerate}
\item
  Write a procedure that traverses only non-empty entries and prints
  them.
\item
  Explain why inserting a value in CSR format is more expensive than in
  a dictionary-based representation.
\item
  Compare memory usage of a dense array of length 1000 with only 5
  non-zero entries against its sparse dictionary form.
\item
  Give two real-world scenarios where CSR is preferable to
  dictionary-based sparse arrays.
\end{enumerate}

\subsection{2.5 L2 --- Sparse Arrays and Compressed Layouts in
Systems}\label{l2-sparse-arrays-and-compressed-layouts-in-systems}

Sparse arrays are not only about saving memory; they embody deep design
choices about compression, cache use, and hardware acceleration. At this
level, the question is not ``should I store zeros or not,'' but ``which
representation balances memory, access speed, and computational
efficiency for the workload?''

\subsubsection{Deep Dive}\label{deep-dive-17}

Several compressed storage formats exist, each tuned to different needs:

\begin{itemize}
\tightlist
\item
  COO (Coordinate List): Store parallel arrays for row indices, column
  indices, and values. Flexible and simple, but inefficient for repeated
  access because lookups require scanning.
\item
  CSR (Compressed Sparse Row): Use three arrays: \texttt{values},
  \texttt{col\_indices}, and \texttt{row\_ptr} to mark boundaries.
  Accessing all elements of a row is O(1), while finding a specific
  column in a row is O(log n) or linear. Excellent for sparse
  matrix-vector multiplication (SpMV).
\item
  CSC (Compressed Sparse Column): Similar to CSR, but optimized for
  column operations.
\item
  DIA (Diagonal): Only store diagonals in banded matrices. Extremely
  memory-efficient for PDE solvers.
\item
  ELL (Ellpack/Itpack): Store each row padded to the same length,
  enabling SIMD and GPU vectorization. Works well when rows have similar
  numbers of nonzeros.
\item
  HYB (Hybrid, CUDA): Combines ELL for regular rows and COO for
  irregular cases. Used in GPU-accelerated sparse libraries.
\end{itemize}

\paragraph{Performance and
Complexity.}\label{performance-and-complexity.}

\begin{itemize}
\tightlist
\item
  Dictionaries/maps: O(1) average access, but higher overhead per entry.
\item
  COO: O(n) lookups, better for incremental construction.
\item
  CSR/CSC: excellent for batch operations, poor for insertions.
\item
  ELL/DIA: high throughput on SIMD/GPU hardware but inflexible.
\end{itemize}

Sparse matrix-vector multiplication (SpMV) illustrates trade-offs. With
CSR:

\begin{verbatim}
y[row] = Σ values[k] * x[col_indices[k]]  
\end{verbatim}

where \texttt{row\_ptr} guides which elements belong to each row. The
cost is proportional to the number of nonzeros, but performance is
limited by memory bandwidth and irregular access to \texttt{x}.

\paragraph{Cache and alignment.}\label{cache-and-alignment.}

Compressed formats improve locality for sequential access but introduce
irregular memory access patterns when multiplying or searching. Strided
iteration can align with cache lines, but pointer-heavy layouts fragment
memory. Padding (in ELL) improves SIMD alignment but wastes space.

\paragraph{Language and library
implementations.}\label{language-and-library-implementations.}

\begin{itemize}
\tightlist
\item
  Python SciPy: \texttt{csr\_matrix}, \texttt{csc\_matrix},
  \texttt{coo\_matrix}, \texttt{dia\_matrix}.
\item
  C++: Eigen and Armadillo expose CSR and CSC; Intel MKL provides highly
  optimized kernels.
\item
  CUDA/cuSPARSE: Hybrid ELL + COO kernels tuned for GPUs.
\end{itemize}

\paragraph{System-level use cases.}\label{system-level-use-cases.-1}

\begin{itemize}
\tightlist
\item
  Large-scale PDE solvers and finite element methods.
\item
  Graph algorithms (PageRank, shortest paths) using sparse adjacency
  matrices.
\item
  Inverted indices in search engines (postings lists).
\item
  Feature vectors in machine learning (bag-of-words, recommender
  systems).
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-4}

\begin{itemize}
\tightlist
\item
  Insertion is expensive in compressed formats (requires shifting or
  rebuilding).
\item
  Converting between formats (e.g., COO ↔ CSR) can dominate runtime if
  done repeatedly.
\item
  A poor choice of format (e.g., using ELL for irregular sparsity) can
  waste memory or block vectorization.
\end{itemize}

\paragraph{Optimization and
profiling.}\label{optimization-and-profiling.}

\begin{itemize}
\tightlist
\item
  Benchmark SpMV across formats and measure achieved bandwidth.
\item
  Profile cache misses and TLB behavior in irregular workloads.
\item
  On GPUs, measure coalesced vs scattered memory access to judge format
  suitability.
\end{itemize}

\paragraph{Worked Example (Python with
SciPy)}\label{worked-example-python-with-scipy}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.sparse }\ImportTok{import}\NormalTok{ csr\_matrix}

\CommentTok{\# Dense 5x5 with many zeros}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{    [}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{],}
\NormalTok{    [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{]}
\NormalTok{])}

\CommentTok{\# Convert to CSR}
\NormalTok{sparse }\OperatorTok{=}\NormalTok{ csr\_matrix(dense)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"CSR data array:"}\NormalTok{, sparse.data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"CSR indices:"}\NormalTok{, sparse.indices)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"CSR indptr:"}\NormalTok{, sparse.indptr)}

\CommentTok{\# Sparse matrix{-}vector multiplication}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ sparse }\OperatorTok{@}\NormalTok{ x}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Result of SpMV:"}\NormalTok{, y)}
\end{Highlighting}
\end{Shaded}

This example shows how a dense matrix with many zeros can be stored
efficiently in CSR. Only nonzeros are stored, and SpMV avoids
unnecessary multiplications.

\subsubsection{Why it matters}\label{why-it-matters-26}

Sparse array formats are the backbone of scientific computing, machine
learning, and search engines. Choosing the right format determines
whether a computation runs in seconds or hours. At scale, cache
efficiency, memory bandwidth, and vectorization potential matter as much
as algorithmic complexity. Sparse arrays teach the critical lesson that
representation is performance.

\subsubsection{Exercises}\label{exercises-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement COO and CSR representations of the same sparse matrix and
  compare memory usage.
\item
  Write a small CSR-based SpMV routine and measure its speed against a
  dense implementation.
\item
  Explain why ELL format is efficient on GPUs but wasteful on highly
  irregular graphs.
\item
  In SciPy, convert a \texttt{csr\_matrix} to \texttt{csc\_matrix} and
  back. Measure the cost for large matrices.
\item
  Given a graph with 1M nodes and 10M edges, explain why adjacency lists
  and CSR are more practical than dense matrices.
\end{enumerate}

\section{2.6 Prefix Sums \& Scans}\label{prefix-sums-scans}

\subsection{2.6 L0 --- Running Totals}\label{l0-running-totals}

A prefix sum, also called a scan, is a way of turning a sequence into
running totals. Instead of just producing one final sum, we produce an
array where each position shows the sum of all earlier elements. It is
like keeping a receipt tape at the checkout: each item is added in
order, and you see the growing total after each step.

\subsubsection{Deep Dive}\label{deep-dive-18}

Prefix sums are simple but powerful. Given an array
\texttt{{[}a0,\ a1,\ a2,\ …,\ an-1{]}}, the prefix sum array
\texttt{{[}p0,\ p1,\ p2,\ …,\ pn-1{]}} is defined as:

\begin{itemize}
\item
  Inclusive scan:

\begin{verbatim}
pi = a0 + a1 + … + ai
\end{verbatim}
\item
  Exclusive scan:

\begin{verbatim}
pi = a0 + a1 + … + ai-1
\end{verbatim}

  (with p0 = 0 by convention).
\end{itemize}

Example with array \texttt{{[}1,\ 2,\ 3,\ 4{]}}:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Index & Original & Inclusive & Exclusive \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1 & 1 & 0 \\
1 & 2 & 3 & 1 \\
2 & 3 & 6 & 3 \\
3 & 4 & 10 & 6 \\
\end{longtable}

Prefix sums are built in a single pass, left to right. This is O(n) in
time, requiring an extra array of length n to store results.

Once constructed, prefix sums allow fast range queries. For any subarray
between indices \texttt{i} and \texttt{j}, the sum is:

\begin{verbatim}
sum(i..j) = prefix[j] - prefix[i-1]
\end{verbatim}

This reduces what would be O(n) work into O(1) time per query.

Prefix sums also generalize beyond addition: they can be built with
multiplication, min, max, or any associative operation.

\subsubsection{Worked Example}\label{worked-example-10}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}

\CommentTok{\# Inclusive prefix sum}
\NormalTok{inclusive }\OperatorTok{=}\NormalTok{ []}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    inclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Inclusive scan:"}\NormalTok{, inclusive)}

\CommentTok{\# Exclusive prefix sum}
\NormalTok{exclusive }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{]}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    exclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Exclusive scan:"}\NormalTok{, exclusive)}

\CommentTok{\# Range query using prefix sums}
\NormalTok{i, j }\OperatorTok{=} \DecValTok{1}\NormalTok{, }\DecValTok{3}  \CommentTok{\# sum from index 1 to 3 (2+3+4)}
\NormalTok{range\_sum }\OperatorTok{=}\NormalTok{ inclusive[j] }\OperatorTok{{-}}\NormalTok{ (inclusive[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Range sum (1..3):"}\NormalTok{, range\_sum)}
\end{Highlighting}
\end{Shaded}

This program shows inclusive and exclusive scans, and how to use them to
answer range queries quickly.

\subsubsection{Why it matters}\label{why-it-matters-27}

Prefix sums transform repeated work into reusable results. They make
range queries efficient, reduce algorithmic complexity, and appear in
countless applications: histograms, text processing, probability
distributions, and parallel computing. They also introduce the idea of
trading extra storage for faster queries, a common algorithmic
technique.

\subsubsection{Exercises}\label{exercises-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the prefix sum of \texttt{{[}1,\ 2,\ 3,\ 4,\ 5{]}} by hand.
\item
  Show the difference between inclusive and exclusive prefix sums for
  \texttt{{[}5,\ 10,\ 15{]}}.
\item
  Use a prefix sum to find the sum of elements from index 2 to 4 in
  \texttt{{[}3,\ 6,\ 9,\ 12,\ 15{]}}.
\item
  Given a prefix sum array \texttt{{[}2,\ 5,\ 9,\ 14{]}}, reconstruct
  the original array.
\item
  Explain why prefix sums are more efficient than computing each
  subarray sum from scratch when handling many queries.
\end{enumerate}

\subsection{2.6 L1 --- Prefix Sums in
Practice}\label{l1-prefix-sums-in-practice}

Prefix sums are a versatile tool for speeding up algorithms that involve
repeated range queries. Instead of recalculating sums over and over, we
preprocess the array once to create cumulative totals. This
preprocessing costs O(n), but it allows each query to be answered in
O(1).

\subsubsection{Deep Dive}\label{deep-dive-19}

A prefix sum array is built by scanning the original array from left to
right:

\begin{verbatim}
prefix[i] = prefix[i-1] + arr[i]
\end{verbatim}

This produces the inclusive scan. The exclusive scan shifts everything
rightward, leaving prefix{[}0{]} = 0 and excluding the current element.

The choice between inclusive and exclusive depends on application:

\begin{itemize}
\tightlist
\item
  Inclusive is easier for direct cumulative totals.
\item
  Exclusive is more natural when answering range queries.
\end{itemize}

Once built, prefix sums enable efficient operations:

\begin{itemize}
\tightlist
\item
  Range queries:
  \texttt{sum(i..j)\ =\ prefix{[}j{]}\ -\ prefix{[}i-1{]}}.
\item
  Reconstruction: the original array can be recovered with
  \texttt{arr{[}i{]}\ =\ prefix{[}i{]}\ -\ prefix{[}i-1{]}}.
\item
  Generalization: the same idea works for multiplication (cumulative
  product), logical OR/AND, or even min/max. The key requirement is that
  the operation is associative.
\end{itemize}

\paragraph{Trade-offs:}\label{trade-offs-1}

\begin{itemize}
\tightlist
\item
  Building prefix sums requires O(n) extra memory.
\item
  If only a few queries are needed, recomputing directly may be simpler.
\item
  For many queries, the preprocessing overhead is worthwhile.
\end{itemize}

\paragraph{Use cases:}\label{use-cases-1}

\begin{itemize}
\tightlist
\item
  Fast range-sum queries in databases or competitive programming.
\item
  Cumulative frequencies in histograms.
\item
  Substring analysis in text algorithms (e.g., number of vowels in a
  range).
\item
  Probability and statistics: cumulative distribution functions.
\end{itemize}

\paragraph{Language implementations:}\label{language-implementations}

\begin{itemize}
\tightlist
\item
  Python: \texttt{itertools.accumulate}, \texttt{numpy.cumsum}.
\item
  C++: \texttt{std::partial\_sum} from
  \texttt{\textless{}numeric\textgreater{}}.
\item
  Java: custom loop, or stream reductions.
\end{itemize}

\paragraph{Pitfalls:}\label{pitfalls}

\begin{itemize}
\tightlist
\item
  Confusing inclusive vs exclusive scans often leads to off-by-one
  errors.
\item
  For large datasets, cumulative sums may overflow fixed-width integers.
\end{itemize}

\subsubsection{Worked Example}\label{worked-example-11}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ itertools}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{arr }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{]}

\CommentTok{\# Inclusive prefix sum using Python loop}
\NormalTok{inclusive }\OperatorTok{=}\NormalTok{ []}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    inclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Inclusive prefix sum:"}\NormalTok{, inclusive)}

\CommentTok{\# Exclusive prefix sum}
\NormalTok{exclusive }\OperatorTok{=}\NormalTok{ [}\DecValTok{0}\NormalTok{]}
\NormalTok{running }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ arr[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]:}
\NormalTok{    running }\OperatorTok{+=}\NormalTok{ x}
\NormalTok{    exclusive.append(running)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Exclusive prefix sum:"}\NormalTok{, exclusive)}

\CommentTok{\# NumPy cumsum (inclusive)}
\NormalTok{np\_inclusive }\OperatorTok{=}\NormalTok{ np.cumsum(arr)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"NumPy inclusive scan:"}\NormalTok{, np\_inclusive)}

\CommentTok{\# Range query using prefix sums}
\NormalTok{i, j }\OperatorTok{=} \DecValTok{1}\NormalTok{, }\DecValTok{3}  \CommentTok{\# indices 1..3 → 4+6+8}
\NormalTok{range\_sum }\OperatorTok{=}\NormalTok{ inclusive[j] }\OperatorTok{{-}}\NormalTok{ (inclusive[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Range sum (1..3):"}\NormalTok{, range\_sum)}

\CommentTok{\# Recover original array from prefix sums}
\NormalTok{reconstructed }\OperatorTok{=}\NormalTok{ [inclusive[}\DecValTok{0}\NormalTok{]] }\OperatorTok{+}\NormalTok{ [inclusive[i] }\OperatorTok{{-}}\NormalTok{ inclusive[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(inclusive))]}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Reconstructed array:"}\NormalTok{, reconstructed)}
\end{Highlighting}
\end{Shaded}

This example demonstrates building prefix sums by hand, using built-in
libraries, answering queries, and reconstructing the original array.

\subsubsection{Why it matters}\label{why-it-matters-28}

Prefix sums reduce repeated work into reusable results. They transform
O(n) queries into O(1), making algorithms faster and more scalable. They
are a foundational idea in algorithm design, connecting to histograms,
distributions, and dynamic programming.

\subsubsection{Exercises}\label{exercises-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build both inclusive and exclusive prefix sums for
  \texttt{{[}5,\ 10,\ 15,\ 20{]}}.
\item
  Use prefix sums to compute the sum of elements from index 2 to 4 in
  \texttt{{[}1,\ 3,\ 5,\ 7,\ 9{]}}.
\item
  Given a prefix sum array \texttt{{[}3,\ 8,\ 15,\ 24{]}}, reconstruct
  the original array.
\item
  Write a procedure that computes cumulative products (scan with
  multiplication).
\item
  Explain why prefix sums are more useful when answering hundreds of
  queries instead of just one.
\end{enumerate}

\subsection{2.6 L2 --- Prefix Sums and Parallel
Scans}\label{l2-prefix-sums-and-parallel-scans}

Prefix sums seem simple, but at scale they become a central systems
primitive. They serve as the backbone of parallel algorithms, GPU
kernels, and high-performance libraries. At this level, the focus shifts
from ``what is a prefix sum'' to ``how can we compute it efficiently
across thousands of cores, with minimal synchronization and maximal
throughput?''

\subsubsection{Deep Dive}\label{deep-dive-20}

Sequential algorithm. The simple prefix sum is O(n):

\begin{verbatim}
prefix[0] = arr[0]
for i in 1..n-1:
    prefix[i] = prefix[i-1] + arr[i]
\end{verbatim}

Efficient for single-threaded contexts, but inherently sequential
because each value depends on the one before it.

Parallel algorithms. Two key approaches dominate:

\begin{itemize}
\item
  Hillis--Steele scan (1986):

  \begin{itemize}
  \tightlist
  \item
    Iterative doubling method.
  \item
    At step k, each thread adds the value from 2\^{}k positions behind.
  \item
    O(n log n) work, O(log n) depth. Simple but not work-efficient.
  \end{itemize}
\item
  Blelloch scan (1990):

  \begin{itemize}
  \item
    Work-efficient, O(n) total operations, O(log n) depth.
  \item
    Two phases:

    \begin{itemize}
    \tightlist
    \item
      Up-sweep (reduce): build a tree of partial sums.
    \item
      Down-sweep: propagate sums back down to compute prefix results.
    \end{itemize}
  \item
    Widely used in GPU libraries.
  \end{itemize}
\end{itemize}

\paragraph{Hardware performance.}\label{hardware-performance.}

\begin{itemize}
\tightlist
\item
  Cache-aware scans: memory locality matters for large arrays. Blocking
  and tiling reduce cache misses.
\item
  SIMD vectorization: multiple prefix elements are computed in parallel
  inside CPU vector registers.
\item
  GPUs: scans are implemented at warp and block levels, with CUDA
  providing primitives like \texttt{thrust::inclusive\_scan}. Warp
  shuffles (\texttt{\_\_shfl\_up\_sync}) allow efficient intra-warp
  scans without shared memory.
\end{itemize}

\paragraph{Memory and
synchronization.}\label{memory-and-synchronization.}

\begin{itemize}
\tightlist
\item
  In-place scans reduce memory use but complicate parallelization.
\item
  Exclusive vs inclusive variants require careful handling of initial
  values.
\item
  Synchronization overhead and false sharing are common risks in
  multithreaded CPU scans.
\item
  Distributed scans (MPI) require combining partial results from each
  node, then adjusting local scans with offsets.
\end{itemize}

\paragraph{Libraries and
implementations.}\label{libraries-and-implementations.}

\begin{itemize}
\tightlist
\item
  C++ TBB: \texttt{parallel\_scan} supports both exclusive and
  inclusive.
\item
  CUDA Thrust: \texttt{inclusive\_scan}, \texttt{exclusive\_scan} for
  GPU workloads.
\item
  OpenMP: provides \texttt{\#pragma\ omp\ parallel\ for\ reduction} but
  true scans require more explicit handling.
\item
  MPI: \texttt{MPI\_Scan} and \texttt{MPI\_Exscan} provide distributed
  prefix sums.
\end{itemize}

\paragraph{System-level use cases.}\label{system-level-use-cases.-2}

\begin{itemize}
\tightlist
\item
  Parallel histogramming: count frequencies in parallel, prefix sums to
  compute cumulative counts.
\item
  Radix sort: scans partition data into buckets efficiently.
\item
  Stream compaction: filter elements while maintaining order.
\item
  GPU memory allocation: prefix sums assign disjoint output positions to
  threads.
\item
  Database indexing: scans help build offsets for columnar data storage.
\end{itemize}

\paragraph{Pitfalls.}\label{pitfalls.-5}

\begin{itemize}
\tightlist
\item
  Race conditions when threads update overlapping memory.
\item
  Load imbalance in irregular workloads (e.g., skewed distributions).
\item
  Wrong handling of inclusive vs exclusive leads to subtle bugs in
  partitioning algorithms.
\end{itemize}

\paragraph{Profiling and
optimization.}\label{profiling-and-optimization.}

\begin{itemize}
\tightlist
\item
  Benchmark sequential vs parallel scan on arrays of size 10\^{}6 or
  10\^{}9.
\item
  Compare scalability with 2, 4, 8, \ldots{} cores.
\item
  Measure GPU kernel efficiency at warp, block, and grid levels.
\end{itemize}

\subsubsection{Worked Example (CUDA
Thrust)}\label{worked-example-cuda-thrust}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#include }\ImportTok{\textless{}thrust/device\_vector.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}thrust/scan.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}iostream\textgreater{}}

\DataTypeTok{int}\NormalTok{ main}\OperatorTok{()} \OperatorTok{\{}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{device\_vector}\OperatorTok{\textless{}}\DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ data}\OperatorTok{\{}\DecValTok{1}\OperatorTok{,} \DecValTok{2}\OperatorTok{,} \DecValTok{3}\OperatorTok{,} \DecValTok{4}\OperatorTok{,} \DecValTok{5}\OperatorTok{\};}

    \CommentTok{// Inclusive scan}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{inclusive\_scan}\OperatorTok{(}\NormalTok{data}\OperatorTok{.}\NormalTok{begin}\OperatorTok{(),}\NormalTok{ data}\OperatorTok{.}\NormalTok{end}\OperatorTok{(),}\NormalTok{ data}\OperatorTok{.}\NormalTok{begin}\OperatorTok{());}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Inclusive scan: "}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ x }\OperatorTok{:}\NormalTok{ data}\OperatorTok{)} \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}}\NormalTok{ x }\OperatorTok{\textless{}\textless{}} \StringTok{" "}\OperatorTok{;}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}

    \CommentTok{// Exclusive scan}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{device\_vector}\OperatorTok{\textless{}}\DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ data2}\OperatorTok{\{}\DecValTok{1}\OperatorTok{,} \DecValTok{2}\OperatorTok{,} \DecValTok{3}\OperatorTok{,} \DecValTok{4}\OperatorTok{,} \DecValTok{5}\OperatorTok{\};}
\NormalTok{    thrust}\OperatorTok{::}\NormalTok{exclusive\_scan}\OperatorTok{(}\NormalTok{data2}\OperatorTok{.}\NormalTok{begin}\OperatorTok{(),}\NormalTok{ data2}\OperatorTok{.}\NormalTok{end}\OperatorTok{(),}\NormalTok{ data2}\OperatorTok{.}\NormalTok{begin}\OperatorTok{(),} \DecValTok{0}\OperatorTok{);}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \StringTok{"Exclusive scan: "}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ x }\OperatorTok{:}\NormalTok{ data2}\OperatorTok{)} \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}}\NormalTok{ x }\OperatorTok{\textless{}\textless{}} \StringTok{" "}\OperatorTok{;}
    \BuiltInTok{std::}\NormalTok{cout }\OperatorTok{\textless{}\textless{}} \BuiltInTok{std::}\NormalTok{endl}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This program offloads prefix sum computation to the GPU. With thousands
of threads, even huge arrays can be scanned in milliseconds.

\subsubsection{Why it matters}\label{why-it-matters-29}

Prefix sums are a textbook example of how a simple algorithm scales into
a building block of parallel computing. They are used in compilers,
graphics, search engines, and machine learning systems. They show how
rethinking algorithms for hardware (CPU caches, SIMD, GPUs, distributed
clusters) leads to new designs.

\subsubsection{Exercises}\label{exercises-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement the Hillis--Steele scan for an array of length 16 and show
  each step.
\item
  Implement the Blelloch scan in pseudocode and explain how the up-sweep
  and down-sweep phases work.
\item
  Benchmark a sequential prefix sum vs an OpenMP parallel scan on
  10\^{}7 elements.
\item
  In CUDA, implement an exclusive scan at the warp level using shuffle
  instructions.
\item
  Explain how prefix sums are used in stream compaction (removing zeros
  from an array while preserving order).
\end{enumerate}

\section{Deep Dive}\label{deep-dive-21}

\subsubsection{2.1 Static Arrays}\label{static-arrays-1}

\begin{itemize}
\tightlist
\item
  Memory alignment and padding in C and assembly.
\item
  Array indexing formulas compiled into machine code.
\item
  Page tables and kernel use of fixed-size arrays
  (\texttt{task\_struct}, \texttt{inode}).
\item
  Vectorization of loops over static arrays (SSE/AVX).
\item
  Bounds checking elimination in high-level languages.
\end{itemize}

\subsubsection{2.2 Dynamic Arrays}\label{dynamic-arrays-1}

\begin{itemize}
\tightlist
\item
  Growth factor experiments: doubling vs 1.5× vs incremental.
\item
  Profiling Python's list growth strategy (measure capacity jumps).
\item
  Amortized vs worst-case complexity: proofs with actual benchmarks.
\item
  Reallocation latency spikes in low-latency systems.
\item
  Comparing \texttt{std::vector::reserve} vs default growth.
\item
  Memory fragmentation in long-running programs.
\end{itemize}

\subsubsection{2.3 Slices \& Views}\label{slices-views-1}

\begin{itemize}
\tightlist
\item
  Slice metadata structure in Go (\texttt{ptr,\ len,\ cap}).
\item
  Rust borrow checker rules for \texttt{\&{[}T{]}} vs
  \texttt{\&mut\ {[}T{]}}.
\item
  NumPy stride tricks: transpose as a view, not a copy.
\item
  Performance gap: traversing contiguous vs strided slices.
\item
  Cache/TLB impact of strided access (e.g., step=16).
\item
  False sharing when two threads use overlapping slices.
\end{itemize}

\subsubsection{2.4 Multidimensional
Arrays}\label{multidimensional-arrays-1}

\begin{itemize}
\tightlist
\item
  Row-major vs column-major benchmarks: traverse order timing.
\item
  Linear index formulas for N-dimensional arrays.
\item
  Loop tiling/blocking for matrix multiplication.
\item
  Structure of Arrays (SoA) vs Array of Structures (AoS).
\item
  False sharing and padding in multi-threaded traversal.
\item
  BLAS/LAPACK optimizations and cache-aware kernels.
\item
  GPU coalesced memory access in 2D/3D arrays.
\end{itemize}

\subsubsection{2.5 Sparse Arrays \& Compressed
Layouts}\label{sparse-arrays-compressed-layouts}

\begin{itemize}
\tightlist
\item
  COO, CSR, CSC: hands-on with memory footprint and iteration cost.
\item
  Comparing dictionary-based vs CSR-based sparse vectors.
\item
  Parallel SpMV benchmarks on CPU vs GPU.
\item
  DIA and ELL formats: why they shine in structured sparsity.
\item
  Hybrid GPU formats (HYB: ELL + COO).
\item
  Search engine inverted indices as sparse structures.
\item
  Sparse arrays in ML: bag-of-words and embeddings.
\end{itemize}

\subsubsection{2.6 Prefix Sums \& Scans}\label{prefix-sums-scans-1}

\begin{itemize}
\tightlist
\item
  Inclusive vs exclusive scans: correctness pitfalls.
\item
  Hillis--Steele vs Blelloch scans: step count vs work efficiency.
\item
  Cache-friendly prefix sums on CPUs (blocked scans).
\item
  SIMD prefix sum using AVX intrinsics.
\item
  CUDA warp shuffle scans (\texttt{\_\_shfl\_up\_sync}).
\item
  MPI distributed scans across clusters.
\item
  Stream compaction via prefix sums (remove zeros in O(n)).
\item
  Radix sort built from parallel scans.
\end{itemize}

\section{LAB}\label{lab}

\subsubsection{2.1 Static Arrays}\label{static-arrays-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Implement fixed-size arrays in C and Python, compare
  access/update speeds.
\item
  LAB 2: Explore how static arrays are used in Linux kernel
  (\texttt{task\_struct}, page tables).
\item
  LAB 3: Disassemble a simple loop over a static array and inspect the
  generated assembly.
\item
  LAB 4: Benchmark cache effects: sequential vs random access in a large
  static array.
\end{itemize}

\subsubsection{2.2 Dynamic Arrays}\label{dynamic-arrays-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Implement your own dynamic array in C (with doubling strategy).
\item
  LAB 2: Benchmark Python's \texttt{list} growth by tracking capacity
  changes while appending.
\item
  LAB 3: Compare growth factors: doubling vs 1.5× vs fixed increments.
\item
  LAB 4: Stress test reallocation cost by appending millions of
  elements, measure latency spikes.
\item
  LAB 5: Use \texttt{std::vector::reserve} in C++ and compare
  performance vs default growth.
\end{itemize}

\subsubsection{2.3 Slices \& Views}\label{slices-views-2}

\begin{itemize}
\tightlist
\item
  LAB 1: In Go, experiment with slice creation, capacity, and append ---
  observe when new arrays are allocated.
\item
  LAB 2: In Rust, create overlapping slices and see how the borrow
  checker enforces safety.
\item
  LAB 3: In Python, compare slicing a list vs slicing a NumPy array ---
  demonstrate copy vs view behavior.
\item
  LAB 4: Benchmark stride slicing in NumPy (\texttt{arr{[}::16{]}}) and
  explain performance drop.
\item
  LAB 5: Demonstrate aliasing bugs when two slices share the same
  underlying array.
\end{itemize}

\subsubsection{2.4 Multidimensional
Arrays}\label{multidimensional-arrays-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Write code to traverse a 1000×1000 array row by row vs column
  by column, measure performance.
\item
  LAB 2: Implement your own 2D array in C using both contiguous memory
  and array-of-pointers, compare speed.
\item
  LAB 3: Use NumPy to confirm row-major order with \texttt{.strides},
  then create a column-major array and compare.
\item
  LAB 4: Implement a tiled matrix multiplication in C/NumPy and measure
  cache improvement.
\item
  LAB 5: Experiment with SoA vs AoS layouts for a struct of 3 floats
  (x,y,z). Measure iteration performance.
\end{itemize}

\subsubsection{2.5 Sparse Arrays \& Compressed
Layouts}\label{sparse-arrays-compressed-layouts-1}

\begin{itemize}
\tightlist
\item
  LAB 1: Implement sparse arrays with Python dict vs dense lists,
  compare memory usage.
\item
  LAB 2: Build COO and CSR representations for the same matrix, print
  memory layout.
\item
  LAB 3: Benchmark dense vs CSR matrix-vector multiplication.
\item
  LAB 4: Use SciPy's \texttt{csr\_matrix} and \texttt{csc\_matrix}, run
  queries, compare performance.
\item
  LAB 5: Implement a simple search engine inverted index as a sparse
  array of word→docID list.
\end{itemize}

\subsubsection{2.6 Prefix Sums \& Scans}\label{prefix-sums-scans-2}

\begin{itemize}
\tightlist
\item
  LAB 1: Write inclusive and exclusive prefix sums in Python.
\item
  LAB 2: Benchmark prefix sums for answering 1000 range queries vs naive
  summation.
\item
  LAB 3: Implement Blelloch scan in C/NumPy and visualize the
  up-sweep/down-sweep steps.
\item
  LAB 4: Implement prefix sums on GPU (CUDA/Thrust), compare speed to
  CPU.
\item
  LAB 5: Use prefix sums for stream compaction: remove zeros from an
  array while preserving order.
\end{itemize}




\end{document}
