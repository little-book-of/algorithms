<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 10. AI, ML and Optimization – The Little Book of Algorithms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../books/en-us/list-9.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-1fe81d0376b2c50856e68e651e390326.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-27c261d06b905028a18691de25d09dde.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../books/en-us/list-10.html"><span class="chapter-title">Chapter 10. AI, ML and Optimization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">The Little Book of Algorithms</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Content</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/cheatsheet.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Cheatsheet</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/book.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/plan.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">The Plan</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 1. Foundations of Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 2. Sorting and searching</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 3. Data Structure in Action</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 4. Graph Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 5. Dynamic Programming</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 6. Mathematics for Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 7. Strings and Text Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 8. Geometry, Graphics and Spatial Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 9. Systems, Databases and Distributed Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-us/list-10.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Chapter 10. AI, ML and Optimization</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#section-91.-classical-ml" id="toc-section-91.-classical-ml" class="nav-link active" data-scroll-target="#section-91.-classical-ml">Section 91. Classical ML</a>
  <ul class="collapse">
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering">901. k-Means Clustering</a></li>
  <li><a href="#k-medoids-pam" id="toc-k-medoids-pam" class="nav-link" data-scroll-target="#k-medoids-pam">902. k-Medoids (PAM)</a></li>
  <li><a href="#gaussian-mixture-model-em" id="toc-gaussian-mixture-model-em" class="nav-link" data-scroll-target="#gaussian-mixture-model-em">903. Gaussian Mixture Model (EM)</a></li>
  <li><a href="#naive-bayes-classifier" id="toc-naive-bayes-classifier" class="nav-link" data-scroll-target="#naive-bayes-classifier">904. Naive Bayes Classifier</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">905. Logistic Regression</a></li>
  <li><a href="#perceptron" id="toc-perceptron" class="nav-link" data-scroll-target="#perceptron">906. Perceptron</a></li>
  <li><a href="#decision-tree-cart" id="toc-decision-tree-cart" class="nav-link" data-scroll-target="#decision-tree-cart">907. Decision Tree (CART)</a></li>
  <li><a href="#id3-algorithm" id="toc-id3-algorithm" class="nav-link" data-scroll-target="#id3-algorithm">908. ID3 Algorithm</a></li>
  <li><a href="#k-nearest-neighbors-knn" id="toc-k-nearest-neighbors-knn" class="nav-link" data-scroll-target="#k-nearest-neighbors-knn">909. k-Nearest Neighbors (kNN)</a></li>
  <li><a href="#linear-discriminant-analysis-lda" id="toc-linear-discriminant-analysis-lda" class="nav-link" data-scroll-target="#linear-discriminant-analysis-lda">910. Linear Discriminant Analysis (LDA)</a></li>
  </ul></li>
  <li><a href="#section-92.-ensemble-methods" id="toc-section-92.-ensemble-methods" class="nav-link" data-scroll-target="#section-92.-ensemble-methods">Section 92. Ensemble Methods</a>
  <ul class="collapse">
  <li><a href="#bagging-bootstrap-aggregation" id="toc-bagging-bootstrap-aggregation" class="nav-link" data-scroll-target="#bagging-bootstrap-aggregation">911. Bagging (Bootstrap Aggregation)</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">912. Random Forest</a></li>
  <li><a href="#adaboost-adaptive-boosting" id="toc-adaboost-adaptive-boosting" class="nav-link" data-scroll-target="#adaboost-adaptive-boosting">913. AdaBoost (Adaptive Boosting)</a></li>
  <li><a href="#gradient-boosting" id="toc-gradient-boosting" class="nav-link" data-scroll-target="#gradient-boosting">914. Gradient Boosting</a></li>
  <li><a href="#xgboost-extreme-gradient-boosting" id="toc-xgboost-extreme-gradient-boosting" class="nav-link" data-scroll-target="#xgboost-extreme-gradient-boosting">915. XGBoost (Extreme Gradient Boosting)</a></li>
  <li><a href="#lightgbm-light-gradient-boosting-machine" id="toc-lightgbm-light-gradient-boosting-machine" class="nav-link" data-scroll-target="#lightgbm-light-gradient-boosting-machine">916. LightGBM (Light Gradient Boosting Machine)</a></li>
  <li><a href="#catboost-categorical-boosting" id="toc-catboost-categorical-boosting" class="nav-link" data-scroll-target="#catboost-categorical-boosting">917. CatBoost (Categorical Boosting)</a></li>
  <li><a href="#stacking-stacked-generalization" id="toc-stacking-stacked-generalization" class="nav-link" data-scroll-target="#stacking-stacked-generalization">918. Stacking (Stacked Generalization)</a></li>
  <li><a href="#voting-classifier" id="toc-voting-classifier" class="nav-link" data-scroll-target="#voting-classifier">919. Voting Classifier</a></li>
  <li><a href="#snapshot-ensemble" id="toc-snapshot-ensemble" class="nav-link" data-scroll-target="#snapshot-ensemble">920. Snapshot Ensemble</a></li>
  </ul></li>
  <li><a href="#section-93.-gradient-methods" id="toc-section-93.-gradient-methods" class="nav-link" data-scroll-target="#section-93.-gradient-methods">Section 93. Gradient Methods</a>
  <ul class="collapse">
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">921. Gradient Descent</a></li>
  <li><a href="#stochastic-gradient-descent-sgd" id="toc-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent-sgd">922. Stochastic Gradient Descent (SGD)</a></li>
  <li><a href="#mini-batch-stochastic-gradient-descent-mini-batch-sgd" id="toc-mini-batch-stochastic-gradient-descent-mini-batch-sgd" class="nav-link" data-scroll-target="#mini-batch-stochastic-gradient-descent-mini-batch-sgd">923. Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD)</a></li>
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum">924. Momentum</a></li>
  <li><a href="#nesterov-accelerated-gradient-nag" id="toc-nesterov-accelerated-gradient-nag" class="nav-link" data-scroll-target="#nesterov-accelerated-gradient-nag">925. Nesterov Accelerated Gradient (NAG)</a></li>
  <li><a href="#adagrad-adaptive-gradient" id="toc-adagrad-adaptive-gradient" class="nav-link" data-scroll-target="#adagrad-adaptive-gradient">926. AdaGrad (Adaptive Gradient)</a></li>
  <li><a href="#rmsprop-root-mean-square-propagation" id="toc-rmsprop-root-mean-square-propagation" class="nav-link" data-scroll-target="#rmsprop-root-mean-square-propagation">927. RMSProp (Root Mean Square Propagation)</a></li>
  <li><a href="#adam-adaptive-moment-estimation" id="toc-adam-adaptive-moment-estimation" class="nav-link" data-scroll-target="#adam-adaptive-moment-estimation">928. Adam (Adaptive Moment Estimation)</a></li>
  <li><a href="#adamw-adam-with-decoupled-weight-decay" id="toc-adamw-adam-with-decoupled-weight-decay" class="nav-link" data-scroll-target="#adamw-adam-with-decoupled-weight-decay">929. AdamW (Adam with Decoupled Weight Decay)</a></li>
  <li><a href="#l-bfgs-limited-memory-broydenfletchergoldfarbshanno" id="toc-l-bfgs-limited-memory-broydenfletchergoldfarbshanno" class="nav-link" data-scroll-target="#l-bfgs-limited-memory-broydenfletchergoldfarbshanno">930. L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)</a></li>
  </ul></li>
  <li><a href="#section-94.-deep-learning" id="toc-section-94.-deep-learning" class="nav-link" data-scroll-target="#section-94.-deep-learning">Section 94. Deep Learning</a>
  <ul class="collapse">
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">931. Backpropagation</a></li>
  <li><a href="#xavier-he-initialization" id="toc-xavier-he-initialization" class="nav-link" data-scroll-target="#xavier-he-initialization">932. Xavier / He Initialization</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">933. Dropout</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">934. Batch Normalization</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization">935. Layer Normalization</a></li>
  <li><a href="#gradient-clipping" id="toc-gradient-clipping" class="nav-link" data-scroll-target="#gradient-clipping">936. Gradient Clipping</a></li>
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping">937. Early Stopping</a></li>
  <li><a href="#weight-decay" id="toc-weight-decay" class="nav-link" data-scroll-target="#weight-decay">938. Weight Decay</a></li>
  <li><a href="#learning-rate-scheduling" id="toc-learning-rate-scheduling" class="nav-link" data-scroll-target="#learning-rate-scheduling">939. Learning Rate Scheduling</a></li>
  <li><a href="#residual-connections" id="toc-residual-connections" class="nav-link" data-scroll-target="#residual-connections">940. Residual Connections</a></li>
  </ul></li>
  <li><a href="#section-95.-sequence-models" id="toc-section-95.-sequence-models" class="nav-link" data-scroll-target="#section-95.-sequence-models">Section 95. Sequence Models</a>
  <ul class="collapse">
  <li><a href="#hidden-markov-model-forwardbackward-algorithm" id="toc-hidden-markov-model-forwardbackward-algorithm" class="nav-link" data-scroll-target="#hidden-markov-model-forwardbackward-algorithm">941. Hidden Markov Model (Forward–Backward Algorithm)</a></li>
  <li><a href="#viterbi-algorithm" id="toc-viterbi-algorithm" class="nav-link" data-scroll-target="#viterbi-algorithm">942. Viterbi Algorithm</a></li>
  <li><a href="#baumwelch-algorithm" id="toc-baumwelch-algorithm" class="nav-link" data-scroll-target="#baumwelch-algorithm">943. Baum–Welch Algorithm</a></li>
  <li><a href="#beam-search" id="toc-beam-search" class="nav-link" data-scroll-target="#beam-search">944. Beam Search</a></li>
  <li><a href="#greedy-decoding" id="toc-greedy-decoding" class="nav-link" data-scroll-target="#greedy-decoding">945. Greedy Decoding</a></li>
  <li><a href="#connectionist-temporal-classification-ctc" id="toc-connectionist-temporal-classification-ctc" class="nav-link" data-scroll-target="#connectionist-temporal-classification-ctc">946. Connectionist Temporal Classification (CTC)</a></li>
  <li><a href="#attention-mechanism" id="toc-attention-mechanism" class="nav-link" data-scroll-target="#attention-mechanism">947. Attention Mechanism</a></li>
  <li><a href="#transformer-decoder" id="toc-transformer-decoder" class="nav-link" data-scroll-target="#transformer-decoder">948. Transformer Decoder</a></li>
  <li><a href="#seq2seq-with-attention" id="toc-seq2seq-with-attention" class="nav-link" data-scroll-target="#seq2seq-with-attention">949. Seq2Seq with Attention</a></li>
  <li><a href="#pointer-network" id="toc-pointer-network" class="nav-link" data-scroll-target="#pointer-network">950. Pointer Network</a></li>
  </ul></li>
  <li><a href="#section-96.-metaheuristics" id="toc-section-96.-metaheuristics" class="nav-link" data-scroll-target="#section-96.-metaheuristics">Section 96. Metaheuristics</a>
  <ul class="collapse">
  <li><a href="#genetic-algorithm-ga" id="toc-genetic-algorithm-ga" class="nav-link" data-scroll-target="#genetic-algorithm-ga">951. Genetic Algorithm (GA)</a></li>
  <li><a href="#simulated-annealing-sa" id="toc-simulated-annealing-sa" class="nav-link" data-scroll-target="#simulated-annealing-sa">952. Simulated Annealing (SA)</a></li>
  <li><a href="#tabu-search" id="toc-tabu-search" class="nav-link" data-scroll-target="#tabu-search">953. Tabu Search</a></li>
  <li><a href="#particle-swarm-optimization-pso" id="toc-particle-swarm-optimization-pso" class="nav-link" data-scroll-target="#particle-swarm-optimization-pso">954. Particle Swarm Optimization (PSO)</a></li>
  <li><a href="#ant-colony-optimization-aco" id="toc-ant-colony-optimization-aco" class="nav-link" data-scroll-target="#ant-colony-optimization-aco">955. Ant Colony Optimization (ACO)</a></li>
  <li><a href="#differential-evolution-de" id="toc-differential-evolution-de" class="nav-link" data-scroll-target="#differential-evolution-de">956. Differential Evolution (DE)</a></li>
  <li><a href="#harmony-search" id="toc-harmony-search" class="nav-link" data-scroll-target="#harmony-search">957. Harmony Search</a></li>
  <li><a href="#firefly-algorithm" id="toc-firefly-algorithm" class="nav-link" data-scroll-target="#firefly-algorithm">958. Firefly Algorithm</a></li>
  <li><a href="#bee-colony-optimization" id="toc-bee-colony-optimization" class="nav-link" data-scroll-target="#bee-colony-optimization">959. Bee Colony Optimization</a></li>
  <li><a href="#hill-climbing" id="toc-hill-climbing" class="nav-link" data-scroll-target="#hill-climbing">960. Hill Climbing</a></li>
  </ul></li>
  <li><a href="#section-97.-reinforcement-learning" id="toc-section-97.-reinforcement-learning" class="nav-link" data-scroll-target="#section-97.-reinforcement-learning">Section 97. Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-control" id="toc-monte-carlo-control" class="nav-link" data-scroll-target="#monte-carlo-control">961. Monte Carlo Control</a></li>
  <li><a href="#temporal-difference-td-learning" id="toc-temporal-difference-td-learning" class="nav-link" data-scroll-target="#temporal-difference-td-learning">962. Temporal Difference (TD) Learning</a></li>
  <li><a href="#sarsa-on-policy-temporal-difference-learning" id="toc-sarsa-on-policy-temporal-difference-learning" class="nav-link" data-scroll-target="#sarsa-on-policy-temporal-difference-learning">963. SARSA (On-Policy Temporal Difference Learning)</a></li>
  <li><a href="#q-learning-off-policy-temporal-difference-control" id="toc-q-learning-off-policy-temporal-difference-control" class="nav-link" data-scroll-target="#q-learning-off-policy-temporal-difference-control">964. Q-Learning (Off-Policy Temporal Difference Control)</a></li>
  <li><a href="#double-q-learning" id="toc-double-q-learning" class="nav-link" data-scroll-target="#double-q-learning">965. Double Q-Learning</a></li>
  <li><a href="#deep-q-network-dqn" id="toc-deep-q-network-dqn" class="nav-link" data-scroll-target="#deep-q-network-dqn">966. Deep Q-Network (DQN)</a></li>
  <li><a href="#reinforce-policy-gradient-by-sampling" id="toc-reinforce-policy-gradient-by-sampling" class="nav-link" data-scroll-target="#reinforce-policy-gradient-by-sampling">967. REINFORCE (Policy Gradient by Sampling)</a></li>
  <li><a href="#actorcritic-value-guided-policy-update" id="toc-actorcritic-value-guided-policy-update" class="nav-link" data-scroll-target="#actorcritic-value-guided-policy-update">968. Actor–Critic (Value-Guided Policy Update)</a></li>
  <li><a href="#ppo-proximal-policy-optimization" id="toc-ppo-proximal-policy-optimization" class="nav-link" data-scroll-target="#ppo-proximal-policy-optimization">969. PPO (Proximal Policy Optimization)</a></li>
  <li><a href="#ddpg-sac-continuous-action-reinforcement-learning" id="toc-ddpg-sac-continuous-action-reinforcement-learning" class="nav-link" data-scroll-target="#ddpg-sac-continuous-action-reinforcement-learning">970. DDPG / SAC (Continuous Action Reinforcement Learning)</a></li>
  <li><a href="#deep-deterministic-policy-gradient-ddpg" id="toc-deep-deterministic-policy-gradient-ddpg" class="nav-link" data-scroll-target="#deep-deterministic-policy-gradient-ddpg">1. Deep Deterministic Policy Gradient (DDPG)</a></li>
  <li><a href="#soft-actorcritic-sac" id="toc-soft-actorcritic-sac" class="nav-link" data-scroll-target="#soft-actorcritic-sac">2. Soft Actor–Critic (SAC)</a></li>
  </ul></li>
  <li><a href="#section-98.-approximation-and-online-algorithms" id="toc-section-98.-approximation-and-online-algorithms" class="nav-link" data-scroll-target="#section-98.-approximation-and-online-algorithms">Section 98. Approximation and Online Algorithms</a>
  <ul class="collapse">
  <li><a href="#greedy-set-cover-ln-n-approximation" id="toc-greedy-set-cover-ln-n-approximation" class="nav-link" data-scroll-target="#greedy-set-cover-ln-n-approximation">971. Greedy Set Cover (ln n-Approximation)</a></li>
  <li><a href="#vertex-cover-approximation-double-matching-heuristic" id="toc-vertex-cover-approximation-double-matching-heuristic" class="nav-link" data-scroll-target="#vertex-cover-approximation-double-matching-heuristic">972. Vertex Cover Approximation (Double-Matching Heuristic)</a></li>
  <li><a href="#traveling-salesman-approximation-mst-based-2-approximation" id="toc-traveling-salesman-approximation-mst-based-2-approximation" class="nav-link" data-scroll-target="#traveling-salesman-approximation-mst-based-2-approximation">973. Traveling Salesman Approximation (MST-based 2-Approximation)</a></li>
  <li><a href="#k-center-approximation-farthest-point-heuristic" id="toc-k-center-approximation-farthest-point-heuristic" class="nav-link" data-scroll-target="#k-center-approximation-farthest-point-heuristic">974. k-Center Approximation (Farthest-Point Heuristic)</a></li>
  <li><a href="#online-paging-lru-least-recently-used" id="toc-online-paging-lru-least-recently-used" class="nav-link" data-scroll-target="#online-paging-lru-least-recently-used">975. Online Paging (LRU – Least Recently Used)</a></li>
  <li><a href="#online-matching-ranking-algorithm" id="toc-online-matching-ranking-algorithm" class="nav-link" data-scroll-target="#online-matching-ranking-algorithm">976. Online Matching (Ranking Algorithm)</a></li>
  <li><a href="#online-knapsack-ratio-based-acceptance" id="toc-online-knapsack-ratio-based-acceptance" class="nav-link" data-scroll-target="#online-knapsack-ratio-based-acceptance">977. Online Knapsack (Ratio-Based Acceptance)</a></li>
  <li><a href="#competitive-ratio-evaluation" id="toc-competitive-ratio-evaluation" class="nav-link" data-scroll-target="#competitive-ratio-evaluation">978. Competitive Ratio Evaluation</a></li>
  <li><a href="#ptas-and-fptas-schemes-polynomial-time-approximation" id="toc-ptas-and-fptas-schemes-polynomial-time-approximation" class="nav-link" data-scroll-target="#ptas-and-fptas-schemes-polynomial-time-approximation">979. PTAS and FPTAS Schemes (Polynomial-Time Approximation)</a></li>
  <li><a href="#primaldual-method-approximate-combinatorial-optimization" id="toc-primaldual-method-approximate-combinatorial-optimization" class="nav-link" data-scroll-target="#primaldual-method-approximate-combinatorial-optimization">980. Primal–Dual Method (Approximate Combinatorial Optimization)</a></li>
  </ul></li>
  <li><a href="#section-99.-fairness-causal-inference-and-robust-optimization" id="toc-section-99.-fairness-causal-inference-and-robust-optimization" class="nav-link" data-scroll-target="#section-99.-fairness-causal-inference-and-robust-optimization">Section 99. Fairness, Causal Inference, and Robust Optimization</a>
  <ul class="collapse">
  <li><a href="#reweighting-for-fairness" id="toc-reweighting-for-fairness" class="nav-link" data-scroll-target="#reweighting-for-fairness">981. Reweighting for Fairness</a></li>
  <li><a href="#demographic-parity-constraint" id="toc-demographic-parity-constraint" class="nav-link" data-scroll-target="#demographic-parity-constraint">982. Demographic Parity Constraint</a></li>
  <li><a href="#equalized-odds" id="toc-equalized-odds" class="nav-link" data-scroll-target="#equalized-odds">983. Equalized Odds</a></li>
  <li><a href="#adversarial-debiasing" id="toc-adversarial-debiasing" class="nav-link" data-scroll-target="#adversarial-debiasing">984. Adversarial Debiasing</a></li>
  <li><a href="#causal-dag-discovery" id="toc-causal-dag-discovery" class="nav-link" data-scroll-target="#causal-dag-discovery">985. Causal DAG Discovery</a></li>
  <li><a href="#propensity-score-matching" id="toc-propensity-score-matching" class="nav-link" data-scroll-target="#propensity-score-matching">986. Propensity Score Matching</a></li>
  <li><a href="#instrumental-variable-estimation" id="toc-instrumental-variable-estimation" class="nav-link" data-scroll-target="#instrumental-variable-estimation">987. Instrumental Variable Estimation</a></li>
  <li><a href="#robust-optimization" id="toc-robust-optimization" class="nav-link" data-scroll-target="#robust-optimization">988. Robust Optimization</a></li>
  <li><a href="#distributionally-robust-optimization" id="toc-distributionally-robust-optimization" class="nav-link" data-scroll-target="#distributionally-robust-optimization">989. Distributionally Robust Optimization</a></li>
  <li><a href="#counterfactual-fairness" id="toc-counterfactual-fairness" class="nav-link" data-scroll-target="#counterfactual-fairness">990. Counterfactual Fairness</a></li>
  </ul></li>
  <li><a href="#section-100.-ai-planning-search-and-learning-systems" id="toc-section-100.-ai-planning-search-and-learning-systems" class="nav-link" data-scroll-target="#section-100.-ai-planning-search-and-learning-systems">Section 100. AI Planning, Search and Learning Systems</a>
  <ul class="collapse">
  <li><a href="#breadth-first-search-bfs" id="toc-breadth-first-search-bfs" class="nav-link" data-scroll-target="#breadth-first-search-bfs">991. Breadth-First Search (BFS)</a></li>
  <li><a href="#depth-first-search-dfs" id="toc-depth-first-search-dfs" class="nav-link" data-scroll-target="#depth-first-search-dfs">992. Depth-First Search (DFS)</a></li>
  <li><a href="#a-search" id="toc-a-search" class="nav-link" data-scroll-target="#a-search">993. A* Search</a></li>
  <li><a href="#iterative-deepening-a-ida" id="toc-iterative-deepening-a-ida" class="nav-link" data-scroll-target="#iterative-deepening-a-ida">994. Iterative Deepening A* (IDA*)</a></li>
  <li><a href="#uniform-cost-search-ucs" id="toc-uniform-cost-search-ucs" class="nav-link" data-scroll-target="#uniform-cost-search-ucs">995. Uniform Cost Search (UCS)</a></li>
  <li><a href="#monte-carlo-tree-search-mcts" id="toc-monte-carlo-tree-search-mcts" class="nav-link" data-scroll-target="#monte-carlo-tree-search-mcts">996. Monte Carlo Tree Search (MCTS)</a></li>
  <li><a href="#minimax-algorithm" id="toc-minimax-algorithm" class="nav-link" data-scroll-target="#minimax-algorithm">997. Minimax Algorithm</a></li>
  <li><a href="#alphabeta-pruning" id="toc-alphabeta-pruning" class="nav-link" data-scroll-target="#alphabeta-pruning">998. Alpha–Beta Pruning</a></li>
  <li><a href="#strips-planning" id="toc-strips-planning" class="nav-link" data-scroll-target="#strips-planning">999. STRIPS Planning</a></li>
  <li><a href="#hierarchical-task-network-htn-planning" id="toc-hierarchical-task-network-htn-planning" class="nav-link" data-scroll-target="#hierarchical-task-network-htn-planning">1000. Hierarchical Task Network (HTN) Planning</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 10. AI, ML and Optimization</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="section-91.-classical-ml" class="level1">
<h1>Section 91. Classical ML</h1>
<section id="k-means-clustering" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering">901. k-Means Clustering</h3>
<p>k-Means clustering is one of the simplest and most popular unsupervised learning algorithms. It groups data into <em>k</em> clusters so that each point belongs to the cluster with the nearest centroid. You can think of it as finding “gravity centers” that pull similar points together.</p>
<section id="what-problem-are-we-solving" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving">What Problem Are We Solving?</h4>
<p>We often have raw data without labels and want to discover natural groupings.<br>
k-Means answers: <em>“Which points belong together?”</em></p>
<p>Given:</p>
<ul>
<li>A dataset of points <span class="math inline">\(X = \{x_1, x_2, \dots, x_n\}\)</span></li>
<li>A target number of clusters <span class="math inline">\(k\)</span></li>
</ul>
<p>We want to find <span class="math inline">\(k\)</span> centroids <span class="math inline">\(\mu_1, \mu_2, \dots, \mu_k\)</span> that minimize total squared distance:</p>
<p><span class="math display">\[
\text{Objective: } \min_{\mu} \sum_{i=1}^{n} \lVert x_i - \mu_{c(i)} \rVert^2
\]</span></p>
<p>where <span class="math inline">\(c(i)\)</span> is the cluster assignment of point <span class="math inline">\(x_i\)</span>.</p>
</section>
<section id="how-does-it-work-plain-language" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language">How Does It Work (Plain Language)?</h4>
<p>Think of <em>k</em> seeds dropped into your data. Each point chooses its nearest seed. Then the seeds move to the center of their assigned points. Repeat until the seeds stop moving. That’s k-Means.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td>Pick <em>k</em> centroids (random or smart init)</td>
</tr>
<tr class="even">
<td>2</td>
<td>Assign step</td>
<td>Assign each point to nearest centroid</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update step</td>
<td>Move each centroid to mean of its points</td>
</tr>
<tr class="even">
<td>4</td>
<td>Repeat</td>
<td>Until centroids stop changing (converge)</td>
</tr>
</tbody>
</table>
<p>Example (2 clusters):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 32%">
<col style="width: 32%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Centroid 1</th>
<th>Centroid 2</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>Random points</td>
<td>Random points</td>
<td>Start anywhere</td>
</tr>
<tr class="even">
<td>1</td>
<td>Move to mean of cluster</td>
<td>Move to mean of cluster</td>
<td>Clusters adjust</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Minimal change</td>
<td>Minimal change</td>
<td>Converged</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-easy-versions" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kmeans(X, k, iters<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> X[np.random.choice(n, k, replace<span class="op">=</span><span class="va">False</span>)]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> np.linalg.norm(X[:, <span class="va">None</span>] <span class="op">-</span> centroids[<span class="va">None</span>, :], axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> np.argmin(distances, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        new_centroids <span class="op">=</span> np.array([X[labels <span class="op">==</span> j].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k)])</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.allclose(centroids, new_centroids):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> new_centroids</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> centroids, labels</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Simple pseudocode-style structure</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span>iter <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> iter <span class="op">&lt;</span> MAX_ITER<span class="op">;</span> iter<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    assign_points_to_nearest_centroid<span class="op">();</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    recompute_centroids_as_means<span class="op">();</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>centroids_converged<span class="op">())</span> <span class="cf">break</span><span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters">Why It Matters</h4>
<ul>
<li>Reveals hidden structure in unlabeled data.</li>
<li>Foundation for clustering, image compression, and vector quantization.</li>
<li>Builds intuition for iterative optimization (EM, Lloyd’s algorithm).</li>
<li>Runs fast, easy to implement, widely used as a baseline.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works">A Gentle Proof (Why It Works)</h4>
<p>Each iteration decreases the sum of squared distances.</p>
<ul>
<li>Assign step: choosing the nearest centroid never increases cost.</li>
<li>Update step: moving to the mean minimizes squared error.</li>
</ul>
<p>Since there are finitely many clusterings, the algorithm must converge (though not necessarily to the <em>global</em> optimum).</p>
<p>Objective function: <span class="math display">\[
J = \sum_{i=1}^n | x_i - \mu_{c(i)} |^2
\]</span></p>
<p>At each step: ( J_{} J_{} )</p>
</section>
<section id="try-it-yourself" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself">Try It Yourself</h4>
<ol type="1">
<li>Run on 2D points: ((1,1), (1.5,2), (5,8), (8,8)) with (k=2).</li>
<li>Try random vs.&nbsp;k-Means++ initialization.</li>
<li>Plot clusters and centroids after convergence.</li>
<li>Increase (k): what happens to cluster sizes?</li>
<li>Compare with hierarchical clustering results.</li>
</ol>
</section>
<section id="test-cases" class="level4">
<h4 class="anchored" data-anchor-id="test-cases">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>k</th>
<th>Expected Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4 points in 2 groups</td>
<td>2</td>
<td>Splits into 2 clusters</td>
</tr>
<tr class="even">
<td>All points in a line</td>
<td>3</td>
<td>Divides into segments</td>
</tr>
<tr class="odd">
<td>Identical points</td>
<td>2</td>
<td>Both centroids converge to same point</td>
</tr>
<tr class="even">
<td>Random cloud</td>
<td>3</td>
<td>Forms roughly equal partitions</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity" class="level4">
<h4 class="anchored" data-anchor-id="complexity">Complexity</h4>
<ul>
<li>Time: ( O(n k d t) ) (n points, k clusters, d dimensions, t iterations)</li>
<li>Space: ( O(n + k) )</li>
</ul>
<p>k-Means clustering is your lens for structure, a simple loop of “assign and update” that uncovers patterns where none were labeled.</p>
</section>
</section>
<section id="k-medoids-pam" class="level3">
<h3 class="anchored" data-anchor-id="k-medoids-pam">902. k-Medoids (PAM)</h3>
<p>k-Medoids clustering is like k-Means, but instead of using the <em>mean</em> of points as a center, it chooses actual data points (medoids) as the representatives of clusters. This makes it more robust to outliers and non-Euclidean distances.</p>
<section id="what-problem-are-we-solving-1" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-1">What Problem Are We Solving?</h4>
<p>Sometimes the “mean” of points doesn’t make sense, especially when:</p>
<ul>
<li>The data has outliers that distort averages.</li>
<li>Distances are not Euclidean (e.g.&nbsp;edit distance, Manhattan).</li>
<li>We want cluster centers to be <em>real points</em> in the dataset.</li>
</ul>
<p>k-Medoids solves this by picking <em>actual</em> examples as centers (medoids) to minimize total dissimilarity:</p>
<p><span class="math display">\[
\text{Objective: } \min_{M} \sum_{i=1}^n d(x_i, m_{c(i)})
\]</span></p>
<p>where ( M = {m_1, , m_k} ) are medoids, and ( d(,) ) is any distance metric.</p>
</section>
<section id="how-does-it-work-plain-language-1" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-1">How Does It Work (Plain Language)?</h4>
<p>Think of k-Medoids as “find the most central representative.” Each cluster picks one of its points that minimizes total distance to all others.</p>
<p>Step-by-step (PAM: Partitioning Around Medoids):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 19%">
<col style="width: 74%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td>Pick <em>k</em> random points as medoids</td>
</tr>
<tr class="even">
<td>2</td>
<td>Assign</td>
<td>Assign each point to nearest medoid</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Swap</td>
<td>For each non-medoid point, try swapping with a medoid</td>
</tr>
<tr class="even">
<td>4</td>
<td>Evaluate</td>
<td>If swap reduces total cost, accept it</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until no better swap found (converged)</td>
</tr>
</tbody>
</table>
<p>Example (k=2):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Iteration</th>
<th>Medoids</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>Randomly select 2 points</td>
<td>Start anywhere</td>
</tr>
<tr class="even">
<td>1</td>
<td>Reassign clusters, test swaps</td>
<td>Reduce total distance</td>
</tr>
<tr class="odd">
<td>2</td>
<td>No better swaps</td>
<td>Stop</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-easy-versions-1" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-1">Tiny Code (Easy Versions)</h4>
<p>Python (Simplified PAM)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pam(X, k, dist_fn):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    medoids <span class="op">=</span> np.random.choice(n, k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assign points to nearest medoid</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> np.array([[dist_fn(X[i], X[m]) <span class="cf">for</span> m <span class="kw">in</span> medoids] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> np.argmin(distances, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Try swapping</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        improved <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="kw">in</span> medoids: <span class="cf">continue</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> m <span class="kw">in</span> medoids:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                new_medoids <span class="op">=</span> medoids.copy()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>                new_medoids[new_medoids <span class="op">==</span> m] <span class="op">=</span> i</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                new_cost <span class="op">=</span> <span class="bu">sum</span>(dist_fn(X[j], X[new_medoids[labels[j]]]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>                old_cost <span class="op">=</span> <span class="bu">sum</span>(dist_fn(X[j], X[medoids[labels[j]]]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> new_cost <span class="op">&lt;</span> old_cost:</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>                    medoids <span class="op">=</span> new_medoids</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>                    improved <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> improved:</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> medoids, labels</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-1" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-1">Why It Matters</h4>
<ul>
<li>Robust to noise and outliers, medoids aren’t pulled by extremes.</li>
<li>Works with any distance metric (Euclidean, cosine, edit distance).</li>
<li>Used in bioinformatics, text clustering, and anomaly detection.</li>
<li>A great conceptual bridge from k-Means to more flexible clustering.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-1" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-1">A Gentle Proof (Why It Works)</h4>
<p>Each swap is guaranteed to not increase total cost: <span class="math display">\[
J = \sum_{i=1}^{n} d(x_i, m_{c(i)})
\]</span> Since there’s a finite number of possible medoid sets, and each iteration strictly improves or preserves cost, the algorithm converges to a local minimum.</p>
<p>Unlike k-Means, no averaging is required, only distance comparisons.</p>
</section>
<section id="try-it-yourself-1" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-1">Try It Yourself</h4>
<ol type="1">
<li>Run k-Medoids with Manhattan distance on 2D points.</li>
<li>Add an outlier, see if medoids resist drift.</li>
<li>Compare results with k-Means (same <em>k</em>).</li>
<li>Test on string data (e.g.&nbsp;Levenshtein distance).</li>
<li>Visualize medoids as “chosen representatives”.</li>
</ol>
</section>
<section id="test-cases-1" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-1">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 1%">
<col style="width: 15%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>k</th>
<th>Metric</th>
<th>Expected Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Points with outlier</td>
<td>2</td>
<td>Euclidean</td>
<td>Medoid ignores outlier</td>
</tr>
<tr class="even">
<td>Strings (“cat”, “bat”, “rat”)</td>
<td>2</td>
<td>Edit distance</td>
<td>Clusters by similarity</td>
</tr>
<tr class="odd">
<td>Line of points</td>
<td>3</td>
<td>Manhattan</td>
<td>Centers are actual data points</td>
</tr>
<tr class="even">
<td>Random scatter</td>
<td>2</td>
<td>Euclidean</td>
<td>Partitions like k-Means but medoid-based</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-1" class="level4">
<h4 class="anchored" data-anchor-id="complexity-1">Complexity</h4>
<ul>
<li>Time: ( O(k (n-k)^2) ) (PAM)</li>
<li>Space: ( O(n) )</li>
</ul>
<p>k-Medoids clustering finds <em>real exemplars</em>, not averages, a method that stays true to your data and stands firm against outliers.</p>
</section>
</section>
<section id="gaussian-mixture-model-em" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-mixture-model-em">903. Gaussian Mixture Model (EM)</h3>
<p>Gaussian Mixture Models (GMMs) take clustering to the probabilistic world. Instead of assigning each point to a single cluster, they let every point <em>belong to multiple clusters with different probabilities</em>. The model assumes data is generated from a mix of several Gaussian distributions.</p>
<section id="what-problem-are-we-solving-2" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-2">What Problem Are We Solving?</h4>
<p>Sometimes clusters overlap or have fuzzy boundaries. Hard assignments (like in k-Means) can be misleading. We want soft clustering, each data point has a probability of belonging to each cluster.</p>
<p>Given:</p>
<ul>
<li>Data points ( X = {x_1, x_2, , x_n} )</li>
<li>A chosen number of components ( k )</li>
</ul>
<p>We model: <span class="math display">\[
p(x) = \sum_{j=1}^k \pi_j , \mathcal{N}(x \mid \mu_j, \Sigma_j)
\]</span> where:</p>
<ul>
<li>( _j ): weight (mixture proportion)</li>
<li>( _j ): mean of component ( j )</li>
<li>( _j ): covariance of component ( j )</li>
</ul>
</section>
<section id="how-does-it-work-plain-language-2" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-2">How Does It Work (Plain Language)?</h4>
<p>Think of GMM as “soft k-Means.” Each point is not assigned to one cluster, but gets fractional membership, “70% cluster A, 30% cluster B.”</p>
<p>We use the Expectation–Maximization (EM) algorithm to estimate parameters.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 31%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td>Pick ( _j, _j, _j )</td>
</tr>
<tr class="even">
<td>2</td>
<td>E-step (Expectation)</td>
<td>Compute membership probabilities (responsibilities)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>M-step (Maximization)</td>
<td>Update parameters using weighted averages</td>
</tr>
<tr class="even">
<td>4</td>
<td>Repeat</td>
<td>Until convergence (log-likelihood stabilizes)</td>
</tr>
</tbody>
</table>
<p>E-step formula: <span class="math display">\[
\gamma_{ij} = \frac{\pi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}{\sum_{l=1}^k \pi_l \mathcal{N}(x_i \mid \mu_l, \Sigma_l)}
\]</span></p>
<p>M-step updates: <span class="math display">\[
\mu_j = \frac{\sum_i \gamma_{ij} x_i}{\sum_i \gamma_{ij}}, \quad
\Sigma_j = \frac{\sum_i \gamma_{ij} (x_i - \mu_j)(x_i - \mu_j)^T}{\sum_i \gamma_{ij}}, \quad
\pi_j = \frac{1}{n} \sum_i \gamma_{ij}
\]</span></p>
</section>
<section id="tiny-code-easy-versions-2" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-2">Tiny Code (Easy Versions)</h4>
<p>Python (with NumPy, minimal EM)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gmm_em(X, k, iters<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> X.shape</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">0</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> X[np.random.choice(n, k, replace<span class="op">=</span><span class="va">False</span>)]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> [np.eye(d)] <span class="op">*</span> k</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> np.ones(k) <span class="op">/</span> k</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># E-step</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        gamma <span class="op">=</span> np.zeros((n, k))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            gamma[:, j] <span class="op">=</span> pi[j] <span class="op">*</span> multivariate_normal.pdf(X, mu[j], sigma[j])</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        gamma <span class="op">/=</span> gamma.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># M-step</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        Nk <span class="op">=</span> gamma.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            mu[j] <span class="op">=</span> (gamma[:, j][:, <span class="va">None</span>] <span class="op">*</span> X).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> Nk[j]</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            x_centered <span class="op">=</span> X <span class="op">-</span> mu[j]</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            sigma[j] <span class="op">=</span> (gamma[:, j][:, <span class="va">None</span>, <span class="va">None</span>] <span class="op">*</span> </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                        np.einsum(<span class="st">'ni,nj-&gt;nij'</span>, x_centered, x_centered)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> Nk[j]</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            pi[j] <span class="op">=</span> Nk[j] <span class="op">/</span> n</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu, sigma, pi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-2" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-2">Why It Matters</h4>
<ul>
<li>Handles overlapping clusters naturally.</li>
<li>Produces probabilistic assignments instead of hard labels.</li>
<li>Supports elliptical (not just spherical) clusters via covariance.</li>
<li>Foundation for EM algorithm, soft clustering, and latent variable models.</li>
<li>Used in speech recognition, image segmentation, and density estimation.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-2" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-2">A Gentle Proof (Why It Works)</h4>
<p>EM alternates between:</p>
<ul>
<li>E-step: Estimate hidden variables (responsibilities).</li>
<li>M-step: Maximize likelihood given responsibilities.</li>
</ul>
<p>Each iteration does not decrease the data log-likelihood: <span class="math display">\[
\log p(X \mid \theta) = \sum_{i=1}^{n} \log \sum_{j=1}^{k} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)
\]</span> Hence, convergence is guaranteed (though possibly to a local maximum).</p>
</section>
<section id="try-it-yourself-2" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-2">Try It Yourself</h4>
<ol type="1">
<li>Cluster 2D points with overlapping Gaussians.</li>
<li>Visualize ellipses ((_j)) to see cluster shapes.</li>
<li>Compare GMM clusters vs.&nbsp;k-Means on same data.</li>
<li>Change (k): see underfitting vs.&nbsp;overfitting.</li>
<li>Add small noise, GMM handles it better than k-Means.</li>
</ol>
</section>
<section id="test-cases-2" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-2">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>k</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2D blobs (overlap)</td>
<td>2</td>
<td>Smooth boundaries</td>
</tr>
<tr class="even">
<td>Non-spherical clusters</td>
<td>3</td>
<td>Elliptical shapes</td>
</tr>
<tr class="odd">
<td>Well-separated data</td>
<td>2</td>
<td>Matches k-Means</td>
</tr>
<tr class="even">
<td>Single Gaussian</td>
<td>1</td>
<td>Learns mean and covariance</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-2" class="level4">
<h4 class="anchored" data-anchor-id="complexity-2">Complexity</h4>
<ul>
<li>Time: ( O(nkd) ) per iteration</li>
<li>Space: ( O(nk) ) (responsibilities)</li>
</ul>
<p>Gaussian Mixture Models blend geometry with probability, instead of forcing points into boxes, they let them <em>belong</em> where they most likely fit.</p>
</section>
</section>
<section id="naive-bayes-classifier" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes-classifier">904. Naive Bayes Classifier</h3>
<p>Naive Bayes is a simple yet powerful probabilistic classifier based on Bayes’ theorem with a bold assumption: all features are conditionally independent given the class. Despite the “naive” assumption, it works surprisingly well across text, spam filtering, and many real-world tasks.</p>
<section id="what-problem-are-we-solving-3" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-3">What Problem Are We Solving?</h4>
<p>We want to predict a class label from a set of features, using probabilities rather than distances or hyperplanes.</p>
<p>Given:</p>
<ul>
<li>Training data ((x_i, y_i))</li>
<li>Features (x = (x_1, x_2, , x_d))</li>
<li>Labels (y )</li>
</ul>
<p>We want: <span class="math display">\[
\hat{y} = \arg\max_y P(y \mid x)
\]</span></p>
<p>By Bayes’ theorem: <span class="math display">\[
P(y \mid x) = \frac{P(x \mid y) P(y)}{P(x)}
\]</span></p>
<p>Since (P(x)) is constant across classes: <span class="math display">\[
\hat{y} = \arg\max_y P(y) , P(x \mid y)
\]</span></p>
<p>With feature independence: <span class="math display">\[
P(x \mid y) = \prod_{j=1}^d P(x_j \mid y)
\]</span></p>
<p>So we only need per-feature class conditionals.</p>
</section>
<section id="how-does-it-work-plain-language-3" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-3">How Does It Work (Plain Language)?</h4>
<p>Naive Bayes looks at each feature separately, multiplies how likely each one is under a class, and picks the class with the biggest combined probability.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 16%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Count</td>
<td>Estimate (P(y)) = class frequency</td>
</tr>
<tr class="even">
<td>2</td>
<td>Estimate</td>
<td>For each feature, estimate (P(x_j y))</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Predict</td>
<td>For a new example, compute (P(y) _j P(x_j y))</td>
</tr>
<tr class="even">
<td>4</td>
<td>Select</td>
<td>Choose class with highest probability</td>
</tr>
</tbody>
</table>
<p>Example (spam filtering):</p>
<ul>
<li>Class: spam / not spam</li>
<li>Features: words like “buy”, “free”, “click”</li>
<li>Each word adds weight to spam probability.</li>
</ul>
</section>
<section id="tiny-code-easy-versions-3" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-3">Tiny Code (Easy Versions)</h4>
<p>Python (Discrete Naive Bayes)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, Counter</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NaiveBayes:</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_counts <span class="op">=</span> Counter()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_counts <span class="op">=</span> defaultdict(Counter)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(X, y):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.class_counts[yi] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> f <span class="kw">in</span> xi:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.feature_counts[yi][f] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> {}</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="va">self</span>.class_counts:</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            log_prob <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> f <span class="kw">in</span> x:</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>                count <span class="op">=</span> <span class="va">self</span>.feature_counts[c][f]</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>                total <span class="op">=</span> <span class="bu">sum</span>(<span class="va">self</span>.feature_counts[c].values())</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>                log_prob <span class="op">+=</span> np.log((count <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (total <span class="op">+</span> <span class="bu">len</span>(<span class="va">self</span>.feature_counts[c])))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            prior <span class="op">=</span> np.log(<span class="va">self</span>.class_counts[c] <span class="op">/</span> <span class="va">self</span>.total)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            scores[c] <span class="op">=</span> prior <span class="op">+</span> log_prob</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">max</span>(scores, key<span class="op">=</span>scores.get)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-3" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-3">Why It Matters</h4>
<ul>
<li>Fast: simple counting, no iteration.</li>
<li>Scalable: works great on large text corpora.</li>
<li>Robust: handles high-dimensional sparse data.</li>
<li>Interpretable: probabilities explain predictions.</li>
<li>Versatile: variants handle continuous (Gaussian) or multinomial data.</li>
</ul>
<p>Common variants:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Type</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bernoulli</td>
<td>Binary features (word presence)</td>
</tr>
<tr class="even">
<td>Multinomial</td>
<td>Word counts (text)</td>
</tr>
<tr class="odd">
<td>Gaussian</td>
<td>Continuous data (e.g.&nbsp;sensor readings)</td>
</tr>
</tbody>
</table>
</section>
<section id="a-gentle-proof-why-it-works-3" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-3">A Gentle Proof (Why It Works)</h4>
<p>From Bayes’ theorem: <span class="math display">\[
P(y|x) \propto P(y) \prod_{j=1}^d P(x_j|y)
\]</span></p>
<p>The independence assumption simplifies joint probabilities into per-feature terms, reducing exponential complexity to linear. Although independence is rarely true, the resulting classifier still performs well when relative likelihoods are correct, <em>a happy accident of probability algebra.</em></p>
</section>
<section id="try-it-yourself-3" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-3">Try It Yourself</h4>
<ol type="1">
<li>Build a spam detector with “free”, “offer”, “hello”.</li>
<li>Compute probabilities manually for small dataset.</li>
<li>Try Gaussian version on numeric features.</li>
<li>Compare accuracy vs.&nbsp;Logistic Regression.</li>
<li>Observe how Laplace smoothing affects zero counts.</li>
</ol>
</section>
<section id="test-cases-3" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-3">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Variant</th>
<th>Expected</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Text spam detection</td>
<td>Multinomial</td>
<td>Good accuracy</td>
</tr>
<tr class="even">
<td>Binary word features</td>
<td>Bernoulli</td>
<td>Robust prediction</td>
</tr>
<tr class="odd">
<td>Sensor data</td>
<td>Gaussian</td>
<td>Smooth decision boundary</td>
</tr>
<tr class="even">
<td>Small dataset</td>
<td>Any</td>
<td>Needs smoothing</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-3" class="level4">
<h4 class="anchored" data-anchor-id="complexity-3">Complexity</h4>
<ul>
<li>Training: ( O(nd) ) (counting)</li>
<li>Prediction: ( O(kd) ) per example</li>
<li>Space: ( O(kd) )</li>
</ul>
<p>Naive Bayes is your probabilistic compass, simple counts and multiplications that turn uncertainty into decisions.</p>
</section>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">905. Logistic Regression</h3>
<p>Logistic Regression is the workhorse of classification, a simple, powerful model that predicts probabilities instead of raw scores. It draws a decision boundary in feature space using the logistic (sigmoid) function, mapping linear combinations into the range (0, 1).</p>
<section id="what-problem-are-we-solving-4" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-4">What Problem Are We Solving?</h4>
<p>We want to predict a binary label <span class="math inline">\(y \in \{0, 1\}\)</span> from a feature vector <span class="math inline">\(x \in \mathbb{R}^d\)</span>,<br>
where the output is a probability rather than a discrete class.</p>
<p>We model: <span class="math display">\[
P(y = 1 \mid x) = \sigma(w^\top x + b)
\]</span> where: <span class="math display">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span> is the sigmoid function.</p>
<p>Decision rule: <span class="math display">\[
\hat{y} =
\begin{cases}
1, &amp; \text{if } \sigma(w^\top x + b) &gt; 0.5,\\
0, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>We learn ( w, b ) by maximum likelihood estimation, equivalent to minimizing log loss: <span class="math display">\[
L(w, b) = -\frac{1}{n} \sum_{i=1}^{n} \big[ y_i \log \hat{p}_i + (1 - y_i) \log (1 - \hat{p}_i) \big]
\]</span></p>
</section>
<section id="how-does-it-work-plain-language-4" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-4">How Does It Work (Plain Language)?</h4>
<p>Imagine fitting a smooth S-shaped curve that separates two classes. Instead of hard cutoffs, Logistic Regression gives <em>confidence</em> in each prediction.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 23%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td>Start with random weights ( w, b )</td>
</tr>
<tr class="even">
<td>2</td>
<td>Predict</td>
<td>Compute <span class="math inline">\(\hat{p}_i = \sigma(w^\top x_i + b)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute Loss</td>
<td>Cross-entropy between predicted and true labels</td>
</tr>
<tr class="even">
<td>4</td>
<td>Update</td>
<td>Adjust weights using gradient descent</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until loss converges</td>
</tr>
</tbody>
</table>
<p>Gradient update: <span class="math display">\[
w := w - \eta \frac{\partial L}{\partial w} = w - \eta (X^\top (\hat{p} - y))
\]</span> <span class="math display">\[
b := b - \eta \sum_i (\hat{p}_i - y_i)
\]</span></p>
</section>
<section id="tiny-code-easy-versions-4" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-4">Tiny Code (Easy Versions)</h4>
<p>Python (from scratch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic_regression(X, y, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> X.shape</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(d)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> X <span class="op">@</span> w <span class="op">+</span> b</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> sigmoid(z)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        dw <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (p <span class="op">-</span> y)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        db <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> np.<span class="bu">sum</span>(p <span class="op">-</span> y)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        w <span class="op">-=</span> lr <span class="op">*</span> dw</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        b <span class="op">-=</span> lr <span class="op">*</span> db</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w, b</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, w, b):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (sigmoid(X <span class="op">@</span> w <span class="op">+</span> b) <span class="op">&gt;=</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each iteration:</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Compute z = w·x + b</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Compute p = 1 / (1 + exp(-z))</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Compute gradients dw, db</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Update parameters</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-4" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-4">Why It Matters</h4>
<ul>
<li>Interpretable: coefficients show feature influence.</li>
<li>Probabilistic output: unlike hard-margin models.</li>
<li>Foundation for neural networks (sigmoid neurons).</li>
<li>Efficient for large-scale classification.</li>
<li>Extensible: with regularization (L1, L2).</li>
</ul>
<p>Variants:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>L1 (Lasso)</td>
<td>Sparse weights</td>
</tr>
<tr class="even">
<td>L2 (Ridge)</td>
<td>Smooth regularization</td>
</tr>
<tr class="odd">
<td>Multinomial</td>
<td>Multi-class via softmax</td>
</tr>
</tbody>
</table>
</section>
<section id="a-gentle-proof-why-it-works-4" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-4">A Gentle Proof (Why It Works)</h4>
<p>Logistic regression arises from maximum likelihood estimation for Bernoulli-distributed labels: <span class="math display">\[
P(y_i \mid x_i) = \hat{p}_i^{y_i} (1 - \hat{p}_i)^{1 - y_i}
\]</span></p>
<p>Maximizing likelihood is equivalent to minimizing log loss. Gradient descent guarantees convergence to a convex global minimum, no local traps.</p>
</section>
<section id="try-it-yourself-4" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-4">Try It Yourself</h4>
<ol type="1">
<li>Create 2D dataset with two classes.</li>
<li>Plot sigmoid boundary and probabilities.</li>
<li>Add regularization, watch feature weights shrink.</li>
<li>Compare with Naive Bayes on same data.</li>
<li>Extend to multiclass with softmax.</li>
</ol>
</section>
<section id="test-cases-4" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-4">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Expected Boundary</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linearly separable</td>
<td>Straight line</td>
<td>Perfect separation</td>
</tr>
<tr class="even">
<td>Overlapping classes</td>
<td>Smooth transition</td>
<td>Probabilistic</td>
</tr>
<tr class="odd">
<td>High-dimensional text</td>
<td>Sparse weights</td>
<td>L1 regularization</td>
</tr>
<tr class="even">
<td>Imbalanced classes</td>
<td>Biased toward majority</td>
<td>Use class weights</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-4" class="level4">
<h4 class="anchored" data-anchor-id="complexity-4">Complexity</h4>
<ul>
<li>Training: ( O(nd) ) per epoch</li>
<li>Prediction: ( O(d) ) per example</li>
<li>Space: ( O(d) )</li>
</ul>
<p>Logistic Regression is your gateway from geometry to probability, drawing decision curves that <em>think in confidence, not absolutes</em>.</p>
</section>
</section>
<section id="perceptron" class="level3">
<h3 class="anchored" data-anchor-id="perceptron">906. Perceptron</h3>
<p>The Perceptron is one of the earliest and simplest models of a neuron, a linear classifier that learns by trial and error. It draws a hyperplane that separates two classes, adjusting its weights whenever it makes a mistake.</p>
<section id="what-problem-are-we-solving-5" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-5">What Problem Are We Solving?</h4>
<p>We want to find a linear boundary that separates two classes <span class="math inline">\(y \in \{-1, +1\}\)</span> given feature vectors <span class="math inline">\(x \in \mathbb{R}^d\)</span>.</p>
<p>The Perceptron seeks weights <span class="math inline">\(w\)</span> and bias <span class="math inline">\(b\)</span> such that:</p>
<p><span class="math display">\[
y_i (w^\top x_i + b) &gt; 0 \quad \forall i
\]</span></p>
<p>That means all positive examples lie on one side of the boundary, and all negative examples lie on the other.</p>
</section>
<section id="how-does-it-work-plain-language-5" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-5">How Does It Work (Plain Language)?</h4>
<p>Imagine a line (or plane) that classifies points.<br>
If a point is misclassified, the Perceptron nudges the line toward it, step by step, until all points are correctly classified (if possible).</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 14%">
<col style="width: 80%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td>Set <span class="math inline">\(w = 0\)</span>, <span class="math inline">\(b = 0\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Iterate</td>
<td>For each training example:</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Predict</td>
<td><span class="math inline">\(\hat{y} = \text{sign}(w^\top x + b)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update</td>
<td>If wrong, adjust: <span class="math inline">\(w \gets w + \eta y x\)</span>, <span class="math inline">\(b \gets b + \eta y\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until no mistakes or max epochs</td>
</tr>
</tbody>
</table>
<p>Each update shifts the boundary toward the misclassified point, improving alignment.</p>
</section>
<section id="tiny-code-easy-versions-5" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-5">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perceptron(X, y, lr<span class="op">=</span><span class="fl">1.0</span>, epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> X.shape</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(d)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        errors <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(X, y):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> yi <span class="op">*</span> (np.dot(w, xi) <span class="op">+</span> b) <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                w <span class="op">+=</span> lr <span class="op">*</span> yi <span class="op">*</span> xi</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                b <span class="op">+=</span> lr <span class="op">*</span> yi</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>                errors <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> errors <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w, b</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, w, b):</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sign(X <span class="op">@</span> w <span class="op">+</span> b)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each (x_i, y_i):</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">//   if y_i * (dot(w, x_i) + b) &lt;= 0:</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">//       w = w + lr * y_i * x_i</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">//       b = b + lr * y_i</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-5" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-5">Why It Matters</h4>
<ul>
<li>Historical cornerstone of neural networks.</li>
<li>Simple online learning: updates on mistakes only.</li>
<li>Converges if data is linearly separable.</li>
<li>Basis for modern models (SVM, SGDClassifier).</li>
</ul>
<p>You can think of it as a “one-neuron brain”, small, fast, and surprisingly effective when boundaries are linear.</p>
</section>
<section id="a-gentle-proof-why-it-works-5" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-5">A Gentle Proof (Why It Works)</h4>
<p>If the data is linearly separable, the Perceptron converges in a finite number of steps.</p>
<p>Let the margin be:</p>
<p><span class="math display">\[
\gamma = \min_i \frac{y_i (w^* \cdot x_i)}{\lVert w^* \rVert}
\]</span></p>
<p>Then the number of updates <span class="math inline">\(U\)</span> satisfies:</p>
<p><span class="math display">\[
U \le \left(\frac{R}{\gamma}\right)^2
\]</span></p>
<p>where <span class="math inline">\(R = \max_i \lVert x_i \rVert\)</span>.</p>
<p>Each mistake improves alignment with the true separator, so the algorithm cannot loop forever.</p>
</section>
<section id="try-it-yourself-5" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-5">Try It Yourself</h4>
<ol type="1">
<li>Train on 2D linearly separable data (for example, two distinct clusters).<br>
</li>
<li>Visualize the decision boundary after each epoch.<br>
</li>
<li>Flip a few labels to observe non-convergence.<br>
</li>
<li>Try different learning rates such as <span class="math inline">\(\eta = 0.1\)</span> and <span class="math inline">\(\eta = 1.0\)</span>.<br>
</li>
<li>Compare the behavior with Logistic Regression on the same data.</li>
</ol>
</section>
<section id="test-cases-5" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-5">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Expected Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linearly separable</td>
<td>Converges to correct hyperplane</td>
</tr>
<tr class="even">
<td>Overlapping classes</td>
<td>Oscillates or never converges</td>
</tr>
<tr class="odd">
<td>High dimension</td>
<td>Learns if separable</td>
</tr>
<tr class="even">
<td>Random noise</td>
<td>May not converge</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-5" class="level4">
<h4 class="anchored" data-anchor-id="complexity-5">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(nd \times \text{epochs})\)</span></li>
<li>Space: <span class="math inline">\(O(d)\)</span></li>
</ul>
<p>The Perceptron is your first glimpse of a learning machine, a simple rule that sees mistakes, learns, and moves on.</p>
</section>
</section>
<section id="decision-tree-cart" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-cart">907. Decision Tree (CART)</h3>
<p>Decision Trees split data step by step, forming a hierarchy of decisions that classify or predict outcomes. Each node asks a yes/no question about a feature, dividing the data until it becomes pure, mostly one class or tightly clustered values.</p>
<section id="what-problem-are-we-solving-6" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-6">What Problem Are We Solving?</h4>
<p>We want a model that is:</p>
<ul>
<li>Interpretable — the reasoning is visible in the tree.<br>
</li>
<li>Flexible — handles numeric and categorical data.<br>
</li>
<li>Recursive — builds structure by partitioning data into smaller, simpler subsets.</li>
</ul>
<p>Given training data <span class="math inline">\((x_i, y_i)\)</span>, the goal is to find a sequence of splits that minimize impurity or variance.<br>
Formally, at each node we choose a feature <span class="math inline">\(j\)</span> and threshold <span class="math inline">\(t\)</span> to minimize:</p>
<p><span class="math display">\[
\min_{j, t} \Bigg(
\frac{n_L}{n} \cdot \text{Impurity}(\text{left}) +
\frac{n_R}{n} \cdot \text{Impurity}(\text{right})
\Bigg)
\]</span></p>
<p>where <span class="math inline">\(n_L\)</span> and <span class="math inline">\(n_R\)</span> are the sizes of the left and right subsets.</p>
<p>Common impurity measures for classification:</p>
<ul>
<li><p>Gini impurity<br>
<span class="math display">\[
G = 1 - \sum_c p_c^2
\]</span></p></li>
<li><p>Entropy<br>
<span class="math display">\[
H = -\sum_c p_c \log_2 p_c
\]</span></p></li>
</ul>
<p>For regression tasks, impurity is often measured by variance.</p>
</section>
<section id="how-does-it-work-plain-language-6" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-6">How Does It Work (Plain Language)?</h4>
<p>Think of the tree as a flowchart. Each node asks a simple yes/no question,<br>
“Is feature <span class="math inline">\(x_j \le t\)</span>?”, and sends the sample left or right depending on the answer.<br>
The algorithm repeats recursively, choosing the best question at each node.</p>
<p>Step-by-step (CART algorithm):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 15%">
<col style="width: 79%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start</td>
<td>All data at the root node</td>
</tr>
<tr class="even">
<td>2</td>
<td>Search Splits</td>
<td>For each feature <span class="math inline">\(j\)</span> and threshold <span class="math inline">\(t\)</span>, compute impurity reduction</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Best Split</td>
<td>Choose <span class="math inline">\((j, t)\)</span> that maximizes impurity gain</td>
</tr>
<tr class="even">
<td>4</td>
<td>Divide</td>
<td>Split dataset into left/right subsets</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Recurse on each subset until stopping rule</td>
</tr>
<tr class="even">
<td>6</td>
<td>Label Leaves</td>
<td>Assign majority class or mean value</td>
</tr>
</tbody>
</table>
<p>Stopping rules include:</p>
<ul>
<li>Maximum depth<br>
</li>
<li>Minimum number of samples per leaf<br>
</li>
<li>No impurity improvement</li>
</ul>
</section>
<section id="tiny-code-easy-versions-6" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-6">Tiny Code (Easy Versions)</h4>
<p>Python (Simplified Binary Tree Split Finder)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gini(y):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    _, counts <span class="op">=</span> np.unique(y, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> counts <span class="op">/</span> <span class="bu">len</span>(y)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(p  <span class="dv">2</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> best_split(X, y):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    best_gain, best_j, best_t <span class="op">=</span> <span class="dv">0</span>, <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    base_impurity <span class="op">=</span> gini(y)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> X.shape</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(d):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        thresholds <span class="op">=</span> np.unique(X[:, j])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> thresholds:</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>            left <span class="op">=</span> y[X[:, j] <span class="op">&lt;=</span> t]</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            right <span class="op">=</span> y[X[:, j] <span class="op">&gt;</span> t]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(left) <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">len</span>(right) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>            impurity <span class="op">=</span> (<span class="bu">len</span>(left) <span class="op">*</span> gini(left) <span class="op">+</span> <span class="bu">len</span>(right) <span class="op">*</span> gini(right)) <span class="op">/</span> <span class="bu">len</span>(y)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>            gain <span class="op">=</span> base_impurity <span class="op">-</span> impurity</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> gain <span class="op">&gt;</span> best_gain:</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>                best_gain, best_j, best_t <span class="op">=</span> gain, j, t</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_j, best_t, best_gain</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each node:</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Compute impurity at the node</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. For each feature, test possible thresholds</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Pick the split with highest impurity reduction</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Recurse on left and right subsets</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-6" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-6">Why It Matters</h4>
<ul>
<li>Interpretable: each split is a human-readable rule.</li>
<li>Nonlinear boundaries: trees can represent piecewise decision regions.</li>
<li>Versatile: supports both classification and regression.</li>
<li>Scalable foundation: used in Random Forests and Gradient Boosting.</li>
<li>No preprocessing: no need to normalize or scale features.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-6" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-6">A Gentle Proof (Why It Works)</h4>
<p>Each split reduces total impurity:</p>
<p><span class="math display">\[
\Delta = I_{\text{parent}} - \frac{n_L}{n} I_{\text{left}} - \frac{n_R}{n} I_{\text{right}}
\]</span></p>
<p>Because impurity measures ( I() ) are non-negative and splitting always lowers impurity (or halts when no improvement), the algorithm eventually reaches a point where no further gain is possible. Thus, the tree converges to a structure with locally optimal splits.</p>
</section>
<section id="try-it-yourself-6" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-6">Try It Yourself</h4>
<ol type="1">
<li>Train a tree on a simple dataset (e.g.&nbsp;two clusters).</li>
<li>Print the rules: “if feature ≤ threshold, go left.”</li>
<li>Limit the maximum depth to avoid overfitting.</li>
<li>Compare Gini vs Entropy for the same data.</li>
<li>Visualize decision boundaries in 2D.</li>
</ol>
</section>
<section id="test-cases-6" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-6">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Expected Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linearly separable data</td>
<td>Single root split</td>
</tr>
<tr class="even">
<td>Categorical + numeric mix</td>
<td>Handles both</td>
</tr>
<tr class="odd">
<td>Overfitting example</td>
<td>Deep tree memorizes data</td>
</tr>
<tr class="even">
<td>Pruned tree</td>
<td>Better generalization</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-6" class="level4">
<h4 class="anchored" data-anchor-id="complexity-6">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(n d \log n)\)</span></li>
<li>Prediction: <span class="math inline">\(O(\text{depth})\)</span></li>
<li>Space: proportional to number of nodes</li>
</ul>
<p>A Decision Tree is a divide-and-conquer learner, each question splits uncertainty, carving out regions of clarity until the data is neatly classified.</p>
</section>
</section>
<section id="id3-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="id3-algorithm">908. ID3 Algorithm</h3>
<p>The ID3 (Iterative Dichotomiser 3) algorithm builds a decision tree using information gain, a measure of how much a feature helps reduce uncertainty (entropy). It’s one of the earliest and most influential tree-learning algorithms, forming the foundation for later methods like C4.5 and CART.</p>
<section id="what-problem-are-we-solving-7" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-7">What Problem Are We Solving?</h4>
<p>We want to learn a classification tree that explains the data using informative splits.<br>
Each split should maximize reduction in entropy, making the resulting subsets as pure as possible.</p>
<p>Given a dataset <span class="math inline">\(D\)</span> with classes <span class="math inline">\(C_1, C_2, \ldots, C_k\)</span>, the entropy of the set is:</p>
<p><span class="math display">\[
H(D) = - \sum_{c=1}^{k} p_c \log_2 p_c
\]</span></p>
<p>where <span class="math inline">\(p_c\)</span> is the proportion of samples in class <span class="math inline">\(C_c\)</span>.</p>
<p>When we split <span class="math inline">\(D\)</span> by a feature <span class="math inline">\(A\)</span> with possible values <span class="math inline">\(\{v_1, v_2, \dots, v_m\}\)</span>,<br>
the information gain is:</p>
<p><span class="math display">\[
\text{Gain}(D, A) = H(D) - \sum_{i=1}^{m} \frac{|D_{v_i}|}{|D|} \, H(D_{v_i})
\]</span></p>
<p>We choose the feature <span class="math inline">\(A\)</span> that maximizes <span class="math inline">\(\text{Gain}(D, A)\)</span>.</p>
</section>
<section id="how-does-it-work-plain-language-7" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-7">How Does It Work (Plain Language)?</h4>
<p>Think of ID3 as a “twenty questions” learner, each question (feature) splits the data to make it more predictable. It always picks the most informative question first, then recurses.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 29%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute Entropy</td>
<td>Measure impurity of the current dataset</td>
</tr>
<tr class="even">
<td>2</td>
<td>For Each Feature</td>
<td>Compute expected entropy after splitting</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Select Best Feature</td>
<td>Pick feature with maximum information gain</td>
</tr>
<tr class="even">
<td>4</td>
<td>Split</td>
<td>Partition data by feature values</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Recurse</td>
<td>Build subtree for each subset</td>
</tr>
<tr class="even">
<td>6</td>
<td>Stop</td>
<td>If all samples have same class, or no features left</td>
</tr>
</tbody>
</table>
</section>
<section id="example" class="level4">
<h4 class="anchored" data-anchor-id="example">Example</h4>
<p>Suppose we have weather data:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Outlook</th>
<th>Temperature</th>
<th>Humidity</th>
<th>Wind</th>
<th>Play</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sunny</td>
<td>Hot</td>
<td>High</td>
<td>Weak</td>
<td>No</td>
</tr>
<tr class="even">
<td>Overcast</td>
<td>Cool</td>
<td>Normal</td>
<td>Strong</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Rain</td>
<td>Mild</td>
<td>High</td>
<td>Weak</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>ID3 computes entropy of <code>Play</code>, evaluates gain for each feature, and splits on the one with highest information gain (e.g., <code>Outlook</code>).</p>
</section>
<section id="tiny-code-easy-versions-7" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-7">Tiny Code (Easy Versions)</h4>
<p>Python (Simplified ID3)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(y):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    _, counts <span class="op">=</span> np.unique(y, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> counts <span class="op">/</span> <span class="bu">len</span>(y)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(p <span class="op">*</span> np.log2(p <span class="op">+</span> <span class="fl">1e-9</span>))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> info_gain(X_col, y):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    values, counts <span class="op">=</span> np.unique(X_col, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    weighted_entropy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v, c <span class="kw">in</span> <span class="bu">zip</span>(values, counts):</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        subset <span class="op">=</span> y[X_col <span class="op">==</span> v]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        weighted_entropy <span class="op">+=</span> (c <span class="op">/</span> <span class="bu">len</span>(y)) <span class="op">*</span> entropy(subset)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> entropy(y) <span class="op">-</span> weighted_entropy</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> best_feature(X, y):</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    gains <span class="op">=</span> [info_gain(X[:, j], y) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>])]</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmax(gains)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Compute entropy of current dataset</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. For each feature, partition by value and compute weighted entropy</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Select feature with maximum gain</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Recurse on subsets</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-7" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-7">Why It Matters</h4>
<ul>
<li>Interpretable: produces readable decision rules.</li>
<li>Greedy yet effective: picks best local split.</li>
<li>Foundation: forms basis of C4.5 (handles continuous values, pruning).</li>
<li>No assumptions: works directly from data frequencies.</li>
</ul>
<p>ID3 was a major step in symbolic AI and machine learning, showing that <em>decision-making can be learned from data itself.</em></p>
</section>
<section id="a-gentle-proof-why-it-works-7" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-7">A Gentle Proof (Why It Works)</h4>
<p>Entropy measures average uncertainty:</p>
<p><span class="math display">\[
H(D) = - \sum_c p_c \log_2 p_c
\]</span></p>
<p>Splitting reduces entropy by creating subsets that are purer on average.<br>
Because <span class="math inline">\(\text{Gain}(D, A) \ge 0\)</span>, each split either improves or leaves entropy unchanged.<br>
The recursion terminates when subsets are perfectly pure (<span class="math inline">\(H = 0\)</span>) or no features remain.</p>
</section>
<section id="try-it-yourself-7" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-7">Try It Yourself</h4>
<ol type="1">
<li>Build a small dataset (like “Play Tennis”).</li>
<li>Compute entropy by hand.</li>
<li>Evaluate information gain for each feature.</li>
<li>Draw the resulting tree.</li>
<li>Compare results with CART (Gini-based).</li>
</ol>
</section>
<section id="test-cases-7" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-7">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Expected Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pure class labels</td>
<td>Stop immediately</td>
</tr>
<tr class="even">
<td>Mixed features</td>
<td>Choose most informative</td>
</tr>
<tr class="odd">
<td>Duplicate examples</td>
<td>Handle consistently</td>
</tr>
<tr class="even">
<td>Numeric features</td>
<td>Requires discretization</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-7" class="level4">
<h4 class="anchored" data-anchor-id="complexity-7">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(n d \log n)\)</span> (depends on splits)</li>
<li>Prediction: <span class="math inline">\(O(\text{depth})\)</span></li>
<li>Space: proportional to number of nodes</li>
</ul>
<p>The ID3 algorithm learns by asking the right questions first, each split is a move toward certainty, a step in building knowledge from entropy.</p>
</section>
</section>
<section id="k-nearest-neighbors-knn" class="level3">
<h3 class="anchored" data-anchor-id="k-nearest-neighbors-knn">909. k-Nearest Neighbors (kNN)</h3>
<p>The k-Nearest Neighbors (kNN) algorithm classifies a new point by looking at its closest examples in the training data. It’s a simple, memory-based method: similar points tend to share the same label. Instead of learning parameters, kNN stores all training data and uses proximity to infer predictions.</p>
<section id="what-problem-are-we-solving-8" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-8">What Problem Are We Solving?</h4>
<p>We want a non-parametric way to perform classification or regression purely based on similarity.</p>
<p>Given:</p>
<ul>
<li>A training set <span class="math inline">\((x_1, y_1), \dots, (x_n, y_n)\)</span></li>
<li>A distance function <span class="math inline">\(d(x, x')\)</span></li>
<li>A chosen number of neighbors <span class="math inline">\(k\)</span></li>
</ul>
<p>The goal is to predict <span class="math inline">\(\hat{y}\)</span> for a new query point <span class="math inline">\(x\)</span>.</p>
<p>For classification:</p>
<p><span class="math display">\[
\hat{y} = \arg\max_{c} \sum_{i \in \mathcal{N}_k(x)} \mathbf{1}(y_i = c)
\]</span></p>
<p>For regression:</p>
<p><span class="math display">\[
\hat{y} = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x)} y_i
\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}_k(x)\)</span> is the set of indices corresponding to the <span class="math inline">\(k\)</span> nearest neighbors of <span class="math inline">\(x\)</span>.</p>
</section>
<section id="how-does-it-work-plain-language-8" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-8">How Does It Work (Plain Language)?</h4>
<p>Imagine plotting your dataset on a plane. When a new point arrives, you measure how far it is from every known point, pick the <span class="math inline">\(k\)</span> closest, and use their labels to make a decision. The model doesn’t generalize, it <em>remembers</em>.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 26%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Choose <span class="math inline">\(k\)</span></td>
<td>Decide how many neighbors to consider</td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute Distances</td>
<td>Measure distance between query and all training points</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Find Neighbors</td>
<td>Select <span class="math inline">\(k\)</span> closest samples</td>
</tr>
<tr class="even">
<td>4</td>
<td>Aggregate</td>
<td>Majority vote (classification) or mean (regression)</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Predict</td>
<td>Return the final label or value</td>
</tr>
</tbody>
</table>
</section>
<section id="example-1" class="level4">
<h4 class="anchored" data-anchor-id="example-1">Example</h4>
<p>Suppose <span class="math inline">\(k = 3\)</span>:</p>
<ul>
<li>Nearest neighbors’ labels: <code>[A, A, B]</code></li>
<li>Majority class: <span class="math inline">\(A \Rightarrow \hat{y} = A\)</span></li>
</ul>
<p>For regression:</p>
<ul>
<li>Nearest neighbor values: <span class="math inline">\([4.0, 5.0, 3.0]\)</span></li>
<li>Prediction:</li>
</ul>
<p><span class="math display">\[
\hat{y} = \frac{4.0 + 5.0 + 3.0}{3} = 4.0
\]</span></p>
</section>
<section id="tiny-code-easy-versions-8" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-8">Tiny Code (Easy Versions)</h4>
<p>Python (Simple kNN Classifier)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knn_predict(X_train, y_train, x_query, k<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    distances <span class="op">=</span> np.linalg.norm(X_train <span class="op">-</span> x_query, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    k_idx <span class="op">=</span> np.argsort(distances)[:k]</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    k_labels <span class="op">=</span> y_train[k_idx]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    most_common <span class="op">=</span> Counter(k_labels).most_common(<span class="dv">1</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> most_common[<span class="dv">0</span>][<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each query point:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Compute distance to all training points</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Sort and pick top-k neighbors</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Take majority label (classification) or mean (regression)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-8" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-8">Why It Matters</h4>
<ul>
<li>Intuitive: classifies by similarity</li>
<li>No training: all computation at query time</li>
<li>Versatile: works for classification and regression</li>
<li>Strong baseline: often a first model to compare others against</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-8" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-8">A Gentle Proof (Why It Works)</h4>
<p>If <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(k \to \infty\)</span>, and <span class="math inline">\(\frac{k}{n} \to 0\)</span>, then the kNN classifier approaches the Bayes optimal classifier, the theoretical best.</p>
<p>Reasoning:</p>
<ul>
<li>Neighbors approximate the local distribution near <span class="math inline">\(x\)</span></li>
<li>Majority vote converges to the most probable class</li>
</ul>
<p>Thus, kNN is statistically consistent under these conditions.</p>
</section>
<section id="try-it-yourself-8" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-8">Try It Yourself</h4>
<ol type="1">
<li>Generate a 2D dataset with two colored clusters</li>
<li>Try <span class="math inline">\(k = 1, 3, 5\)</span> and visualize decision boundaries</li>
<li>Add noise and increase <span class="math inline">\(k\)</span>, observe smoothing</li>
<li>Experiment with distance metrics (Euclidean, Manhattan)</li>
<li>Compare results with a Decision Tree or Logistic Regression</li>
</ol>
</section>
<section id="test-cases-8" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-8">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th><span class="math inline">\(k\)</span></th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Two clusters</td>
<td>1</td>
<td>Sharp, irregular boundary</td>
</tr>
<tr class="even">
<td>Two clusters</td>
<td>5</td>
<td>Smooth boundary</td>
</tr>
<tr class="odd">
<td>Noisy labels</td>
<td>Large</td>
<td>More robust</td>
</tr>
<tr class="even">
<td>Overlapping classes</td>
<td>Small</td>
<td>Flexible but unstable</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-8" class="level4">
<h4 class="anchored" data-anchor-id="complexity-8">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(1)\)</span> (lazy learner)</li>
<li>Prediction: <span class="math inline">\(O(n \cdot d)\)</span> (compute all distances)</li>
<li>Space: <span class="math inline">\(O(n \cdot d)\)</span></li>
</ul>
<p>The k-Nearest Neighbors algorithm is a <em>memory of examples</em>: it learns nothing explicitly, but answers by looking around.</p>
</section>
</section>
<section id="linear-discriminant-analysis-lda" class="level3">
<h3 class="anchored" data-anchor-id="linear-discriminant-analysis-lda">910. Linear Discriminant Analysis (LDA)</h3>
<p>Linear Discriminant Analysis (LDA) is a probabilistic classifier that finds a linear boundary between classes by modeling each as a Gaussian distribution and assuming they share the same covariance. It combines geometry and probability, drawing decision lines where likelihoods balance.</p>
<section id="what-problem-are-we-solving-9" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-9">What Problem Are We Solving?</h4>
<p>We want to classify samples into <span class="math inline">\(K\)</span> classes by assuming:</p>
<ol type="1">
<li>Each class <span class="math inline">\(C_k\)</span> follows a Gaussian (normal) distribution</li>
<li>All classes share the same covariance matrix <span class="math inline">\(\Sigma\)</span></li>
</ol>
<p>Given a point <span class="math inline">\(x\)</span>, we choose the class with the highest posterior probability:</p>
<p><span class="math display">\[
\hat{y} = \arg\max_k ; P(C_k \mid x)
\]</span></p>
<p>By Bayes’ theorem:</p>
<p><span class="math display">\[
P(C_k \mid x) \propto P(x \mid C_k) , P(C_k)
\]</span></p>
<p>Since <span class="math inline">\(P(x)\)</span> is constant across classes, we compare discriminant scores:</p>
<p><span class="math display">\[
\delta_k(x) = x^\top \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^\top \Sigma^{-1} \mu_k + \log P(C_k)
\]</span></p>
<p>The predicted class is the one with the largest <span class="math inline">\(\delta_k(x)\)</span>.</p>
</section>
<section id="how-does-it-work-plain-language-9" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-9">How Does It Work (Plain Language)?</h4>
<p>LDA imagines each class as a cloud shaped like an ellipse (Gaussian). It computes how likely a new point is to come from each cloud, then picks the class with the highest likelihood. Because covariances are shared, the boundaries between clouds are linear.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 24%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Estimate Mean</td>
<td><span class="math inline">\(\mu_k = \frac{1}{n_k} \sum_{i \in C_k} x_i\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Estimate Shared Covariance</td>
<td><span class="math inline">\(\Sigma = \frac{1}{n - K} \sum_{k=1}^K \sum_{i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^\top\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Estimate Priors</td>
<td><span class="math inline">\(P(C_k) = \frac{n_k}{n}\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Compute Discriminant</td>
<td><span class="math inline">\(\delta_k(x)\)</span> for each class</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Predict</td>
<td>Choose class with largest <span class="math inline">\(\delta_k(x)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-2-class-case" class="level4">
<h4 class="anchored" data-anchor-id="example-2-class-case">Example (2-Class Case)</h4>
<p>When <span class="math inline">\(K=2\)</span>, the decision boundary is a line:</p>
<p><span class="math display">\[
(w^\top x) + w_0 = 0
\]</span></p>
<p>where</p>
<p><span class="math display">\[
w = \Sigma^{-1}(\mu_1 - \mu_2), \quad
w_0 = -\frac{1}{2} (\mu_1 + \mu_2)^\top \Sigma^{-1}(\mu_1 - \mu_2) + \log\frac{P(C_1)}{P(C_2)}
\]</span></p>
<p>The sign of <span class="math inline">\((w^\top x + w_0)\)</span> determines the predicted class.</p>
</section>
<section id="tiny-code-easy-versions-9" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-9">Tiny Code (Easy Versions)</h4>
<p>Python (2-Class LDA)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lda_fit(X, y):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> np.unique(y)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> X.shape</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    means <span class="op">=</span> {c: X[y <span class="op">==</span> c].mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> c <span class="kw">in</span> classes}</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    priors <span class="op">=</span> {c: <span class="bu">len</span>(X[y <span class="op">==</span> c]) <span class="op">/</span> n <span class="cf">for</span> c <span class="kw">in</span> classes}</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Shared covariance</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">=</span> np.zeros((d, d))</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> classes:</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        Xc <span class="op">=</span> X[y <span class="op">==</span> c] <span class="op">-</span> means[c]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        cov <span class="op">+=</span> Xc.T <span class="op">@</span> Xc</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">/=</span> (n <span class="op">-</span> <span class="bu">len</span>(classes))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    inv_cov <span class="op">=</span> np.linalg.inv(cov)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> means, priors, inv_cov</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lda_predict(X, means, priors, inv_cov):</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> means:</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        term <span class="op">=</span> X <span class="op">@</span> inv_cov <span class="op">@</span> means[c]</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        const <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> means[c].T <span class="op">@</span> inv_cov <span class="op">@</span> means[c] <span class="op">+</span> np.log(priors[c])</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        scores.append(term <span class="op">+</span> const)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(scores).argmax(axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-9" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-9">Why It Matters</h4>
<ul>
<li>Interpretable: gives explicit linear boundaries.</li>
<li>Probabilistic: outputs class posteriors.</li>
<li>Efficient: closed-form solution (no gradient descent).</li>
<li>Robust: handles small datasets gracefully.</li>
<li>Foundational: basis for Fisher’s discriminant and QDA.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-9" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-9">A Gentle Proof (Why It Works)</h4>
<p>From Bayes’ rule:</p>
<p><span class="math display">\[
P(C_k \mid x) \propto \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma^{-1}(x - \mu_k)\right) P(C_k)
\]</span></p>
<p>Taking logs and simplifying, quadratic terms in <span class="math inline">\(x\)</span> cancel because <span class="math inline">\(\Sigma\)</span> is shared. The result is a linear function of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\delta_k(x) = x^\top \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^\top \Sigma^{-1} \mu_k + \log P(C_k)
\]</span></p>
<p>Thus, class boundaries are linear hyperplanes.</p>
</section>
<section id="try-it-yourself-9" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-9">Try It Yourself</h4>
<ol type="1">
<li>Create a 2D dataset with two Gaussian clusters.</li>
<li>Fit LDA and plot boundary line.</li>
<li>Compare with Logistic Regression, note similarity.</li>
<li>Add imbalance, check how priors shift the line.</li>
<li>Try <span class="math inline">\(K &gt; 2\)</span> and visualize multi-class regions.</li>
</ol>
</section>
<section id="test-cases-9" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-9">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Classes</th>
<th>Expected Boundary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Two Gaussian blobs</td>
<td>2</td>
<td>Linear line between centroids</td>
</tr>
<tr class="even">
<td>Unequal covariance</td>
<td>2</td>
<td>LDA struggles (try QDA)</td>
</tr>
<tr class="odd">
<td>Multi-class</td>
<td>3</td>
<td>Piecewise linear regions</td>
</tr>
<tr class="even">
<td>Imbalanced classes</td>
<td>2</td>
<td>Boundary shifts toward majority</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-9" class="level4">
<h4 class="anchored" data-anchor-id="complexity-9">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(n d^2)\)</span> (covariance computation)</li>
<li>Prediction: <span class="math inline">\(O(K d^2)\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(K d + d^2)\)</span></li>
</ul>
<p>Linear Discriminant Analysis is where statistics meets geometry, boundaries emerge from likelihoods, separating classes with lines of equal belief.</p>
</section>
</section>
</section>
<section id="section-92.-ensemble-methods" class="level1">
<h1>Section 92. Ensemble Methods</h1>
<section id="bagging-bootstrap-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="bagging-bootstrap-aggregation">911. Bagging (Bootstrap Aggregation)</h3>
<p>Bagging, short for Bootstrap Aggregation, is an ensemble method that improves the stability and accuracy of machine learning models by combining multiple versions of the same algorithm trained on random subsets of the data. It’s particularly effective for high-variance models like decision trees.</p>
<section id="what-problem-are-we-solving-10" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-10">What Problem Are We Solving?</h4>
<p>Many models, especially decision trees, are unstable, small changes in the training data can produce very different results. Bagging reduces variance by training multiple models on different bootstrap samples and averaging their predictions.</p>
<p>We aim to construct an ensemble predictor:</p>
<p><span class="math display">\[
\hat{f}*{\text{bag}}(x) = \frac{1}{B} \sum*{b=1}^{B} \hat{f}^{(b)}(x)
\]</span></p>
<p>where each <span class="math inline">\(\hat{f}^{(b)}\)</span> is trained on a different bootstrap sample (random sample with replacement).</p>
</section>
<section id="how-does-it-work-plain-language-10" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-10">How Does It Work (Plain Language)?</h4>
<p>Bagging is like gathering many opinions. Each model sees a slightly different dataset (due to random sampling), learns its own perspective, and then the ensemble averages or votes over all predictions.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 28%">
<col style="width: 67%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Sample with Replacement</td>
<td>Create <span class="math inline">\(B\)</span> bootstrap datasets, each the same size as the original</td>
</tr>
<tr class="even">
<td>2</td>
<td>Train Models</td>
<td>Train base learner (e.g., decision tree) on each sample</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Aggregate Predictions</td>
<td>For regression: average; for classification: majority vote</td>
</tr>
<tr class="even">
<td>4</td>
<td>Final Output</td>
<td>Combined ensemble prediction</td>
</tr>
</tbody>
</table>
<p>Because each base model sees a slightly different version of the data, their errors partially cancel out when aggregated.</p>
</section>
<section id="example-2" class="level4">
<h4 class="anchored" data-anchor-id="example-2">Example</h4>
<p>If <span class="math inline">\(B = 3\)</span> and the models predict <span class="math inline">\([0.8, 0.6, 0.9]\)</span> (regression), the ensemble prediction is:</p>
<p><span class="math display">\[
\hat{y} = \frac{0.8 + 0.6 + 0.9}{3} = 0.7667
\]</span></p>
<p>For classification, if votes are <code>[A, A, B]</code>, majority vote gives <code>A</code>.</p>
</section>
<section id="tiny-code-easy-versions-10" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-10">Tiny Code (Easy Versions)</h4>
<p>Python (Bagging with Decision Trees)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bagging_predict(X_train, y_train, X_test, B<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(X_train)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> []</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.random.choice(n, n, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        Xb, yb <span class="op">=</span> X_train[idx], y_train[idx]</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        model.fit(Xb, yb)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        preds.append(model.predict(X_test))</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> np.array(preds)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Majority vote across models</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> [np.bincount(col).argmax() <span class="cf">for</span> col <span class="kw">in</span> preds.T]</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(y_pred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each bootstrap iteration:</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Sample training data with replacement</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Train base model (e.g. decision tree)</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Store predictions</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Combine via majority vote or average</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-10" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-10">Why It Matters</h4>
<ul>
<li>Reduces variance: stabilizes noisy learners.</li>
<li>Improves generalization: especially for decision trees.</li>
<li>Parallelizable: models are trained independently.</li>
<li>Foundation for Random Forests: which add random feature selection.</li>
</ul>
<p>Bagging is especially useful when:</p>
<ul>
<li>The base model has high variance (like decision trees)</li>
<li>There’s enough data to create diverse bootstrap samples</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-10" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-10">A Gentle Proof (Why It Works)</h4>
<p>Let <span class="math inline">\(\hat{f}(x)\)</span> be a base learner with variance <span class="math inline">\(\text{Var}[\hat{f}(x)]\)</span> and covariance <span class="math inline">\(\rho\)</span> between learners. The ensemble variance is:</p>
<p><span class="math display">\[
\text{Var}[\hat{f}_{\text{bag}}(x)] = \rho , \text{Var}[\hat{f}(x)] + \frac{1 - \rho}{B} , \text{Var}[\hat{f}(x)]
\]</span></p>
<p>As <span class="math inline">\(B\)</span> grows, the second term shrinks, reducing total variance. If base models are uncorrelated (<span class="math inline">\(\rho \approx 0\)</span>), bagging greatly improves stability.</p>
</section>
<section id="try-it-yourself-10" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-10">Try It Yourself</h4>
<ol type="1">
<li>Train a single decision tree, note its accuracy.</li>
<li>Train a bagged ensemble of 20 trees, compare stability.</li>
<li>Plot decision boundaries, bagging smooths jagged edges.</li>
<li>Try on noisy datasets, variance reduction is evident.</li>
<li>Compare with Random Forest (adds feature randomness).</li>
</ol>
</section>
<section id="test-cases-10" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-10">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Base Learner</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noisy data</td>
<td>Decision Tree</td>
<td>Variance reduced</td>
</tr>
<tr class="even">
<td>Smooth data</td>
<td>Linear Model</td>
<td>Little improvement</td>
</tr>
<tr class="odd">
<td>Large dataset</td>
<td>Any</td>
<td>Ensemble converges</td>
</tr>
<tr class="even">
<td>Small dataset</td>
<td>Tree</td>
<td>Bootstrapping adds diversity</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-10" class="level4">
<h4 class="anchored" data-anchor-id="complexity-10">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(B \times T)\)</span>, where <span class="math inline">\(T\)</span> is cost of base learner</li>
<li>Prediction: <span class="math inline">\(O(B)\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(B)\)</span> models</li>
</ul>
<p>Bagging is the wisdom of the crowd, many unstable learners combining their voices to produce one strong, stable prediction.</p>
</section>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">912. Random Forest</h3>
<p>A Random Forest is an ensemble of decision trees, each trained on a different random subset of data and features. By combining bagging with feature randomness, it builds a collection of diverse trees whose collective vote reduces overfitting and improves generalization.</p>
<section id="what-problem-are-we-solving-11" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-11">What Problem Are We Solving?</h4>
<p>Even with bagging, if each decision tree sees the same features, they might make similar splits, leading to correlated errors. Random Forests fix this by adding feature randomness, ensuring each tree explores different dimensions of the feature space.</p>
<p>Given:</p>
<ul>
<li><span class="math inline">\(B\)</span> trees</li>
<li><span class="math inline">\(m\)</span> randomly selected features at each split (typically <span class="math inline">\(m = \sqrt{d}\)</span> for classification)</li>
</ul>
<p>The final ensemble prediction is:</p>
<p>For classification: <span class="math display">\[
\hat{y} = \arg\max_c \sum_{b=1}^{B} \mathbf{1}!\big(\hat{y}^{(b)} = c\big)
\]</span></p>
<p>For regression: <span class="math display">\[
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} \hat{y}^{(b)}
\]</span></p>
</section>
<section id="how-does-it-work-plain-language-11" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-11">How Does It Work (Plain Language)?</h4>
<p>Think of a forest as a team of decision trees, each one grows on a slightly different dataset and focuses on different features. Individually, trees may overfit, but together, they generalize well.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 30%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Bootstrap Sampling</td>
<td>Draw random samples (with replacement) for each tree</td>
</tr>
<tr class="even">
<td>2</td>
<td>Feature Subsampling</td>
<td>At each node, randomly pick <span class="math inline">\(m\)</span> features</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Train Trees</td>
<td>Grow full decision trees without pruning</td>
</tr>
<tr class="even">
<td>4</td>
<td>Aggregate Predictions</td>
<td>Majority vote or average over trees</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Out-of-Bag Estimate</td>
<td>Use unused samples for validation (no extra test set)</td>
</tr>
</tbody>
</table>
</section>
<section id="example-3" class="level4">
<h4 class="anchored" data-anchor-id="example-3">Example</h4>
<p>Suppose you train <span class="math inline">\(B = 5\)</span> trees, each using a different bootstrap sample and random subset of features. Their predictions for a new input are <code>[A, B, A, A, B]</code>. The final majority vote is:</p>
<p><span class="math display">\[
\hat{y} = A
\]</span></p>
</section>
<section id="tiny-code-easy-versions-11" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-11">Tiny Code (Easy Versions)</h4>
<p>Python (Simplified Random Forest)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_forest_predict(X_train, y_train, X_test, B<span class="op">=</span><span class="dv">10</span>, m<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> X_train.shape</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="bu">int</span>(np.sqrt(d))</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> []</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Bootstrap sample</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.random.choice(n, n, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        Xb, yb <span class="op">=</span> X_train[idx], y_train[idx]</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train tree with feature subsampling</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> np.random.choice(d, m, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> DecisionTreeClassifier(max_features<span class="op">=</span>m)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        model.fit(Xb[:, features], yb)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        preds.append(model.predict(X_test[:, features]))</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> np.array(preds)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> [np.bincount(col).argmax() <span class="cf">for</span> col <span class="kw">in</span> preds.T]</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(y_pred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each tree:</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Sample data with replacement</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Randomly select subset of features at each split</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Train decision tree</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Aggregate predictions via vote or average</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-11" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-11">Why It Matters</h4>
<ul>
<li>Reduces variance: decorrelated trees = more stable ensemble</li>
<li>Handles high dimensions: feature subsampling improves scalability</li>
<li>Built-in validation: out-of-bag (OOB) score estimates test error</li>
<li>Robust: works with both categorical and numerical features</li>
<li>Default strong baseline: great performance with minimal tuning</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-11" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-11">A Gentle Proof (Why It Works)</h4>
<p>Variance of a bagged model: <span class="math display">\[
\text{Var}\big[\hat{f}_{\text{bag}}\big] = \rho , \sigma^2 + \frac{1 - \rho}{B} \sigma^2
\]</span></p>
<p>Adding feature randomness reduces <span class="math inline">\(\rho\)</span> (correlation) between trees, which lowers ensemble variance even further. Thus, Random Forests outperform plain bagging when features are correlated.</p>
</section>
<section id="try-it-yourself-11" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-11">Try It Yourself</h4>
<ol type="1">
<li>Train a Random Forest on a noisy dataset.</li>
<li>Compare its accuracy with a single Decision Tree.</li>
<li>Inspect <code>feature_importances_</code>, which features matter most?</li>
<li>Adjust number of trees <span class="math inline">\(B\)</span> and features <span class="math inline">\(m\)</span>.</li>
<li>Evaluate out-of-bag (OOB) error vs.&nbsp;test set error.</li>
</ol>
</section>
<section id="test-cases-11" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-11">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 19%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Base Learner</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noisy data</td>
<td>Decision Tree</td>
<td>Lower variance</td>
</tr>
<tr class="even">
<td>Correlated features</td>
<td>Decision Tree</td>
<td>Feature subsampling helps</td>
</tr>
<tr class="odd">
<td>High-dimensional</td>
<td>Decision Tree</td>
<td>Random feature selection essential</td>
</tr>
<tr class="even">
<td>Small dataset</td>
<td>Decision Tree</td>
<td>Bootstrap diversity adds robustness</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-11" class="level4">
<h4 class="anchored" data-anchor-id="complexity-11">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(B \times n \log n)\)</span> (per tree)</li>
<li>Prediction: <span class="math inline">\(O(B \times \text{depth})\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(B)\)</span> trees stored</li>
</ul>
<p>A Random Forest is a crowd of decision trees, each seeing the world differently, together, they make a balanced, well-grounded judgment.</p>
</section>
</section>
<section id="adaboost-adaptive-boosting" class="level3">
<h3 class="anchored" data-anchor-id="adaboost-adaptive-boosting">913. AdaBoost (Adaptive Boosting)</h3>
<p>AdaBoost, short for Adaptive Boosting, is a powerful ensemble method that builds a strong classifier from a collection of weak learners, often shallow decision stumps. Each learner focuses more on the mistakes of the previous ones, “adapting” as the ensemble grows.</p>
<section id="what-problem-are-we-solving-12" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-12">What Problem Are We Solving?</h4>
<p>Many simple models (like one-level decision trees) perform only slightly better than random guessing. AdaBoost amplifies their strength by combining them into a weighted ensemble that focuses on hard-to-classify examples.</p>
<p>We aim to build a final classifier:</p>
<p><span class="math display">\[
F(x) = \sum_{t=1}^{T} \alpha_t , h_t(x)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(h_t(x)\)</span> is the weak learner at iteration <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\alpha_t\)</span> is its weight (how much trust we give it)</li>
</ul>
<p>The final prediction is:</p>
<p><span class="math display">\[
\hat{y} = \text{sign}\big(F(x)\big)
\]</span></p>
</section>
<section id="how-does-it-work-plain-language-12" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-12">How Does It Work (Plain Language)?</h4>
<p>AdaBoost is like a teacher who keeps re-teaching the hardest questions. After each round, it increases the weight of misclassified points so that the next learner pays more attention to them.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 26%">
<col style="width: 69%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td>All samples get equal weight <span class="math inline">\(w_i = \frac{1}{n}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Train Weak Learner</td>
<td>Fit <span class="math inline">\(h_t(x)\)</span> to weighted data</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute Error</td>
<td><span class="math inline">\(\varepsilon_t = \sum_i w_i , \mathbf{1}(h_t(x_i) \ne y_i)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Compute Learner Weight</td>
<td><span class="math inline">\(\alpha_t = \frac{1}{2} \ln \frac{1 - \varepsilon_t}{\varepsilon_t}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update Weights</td>
<td><span class="math inline">\(w_i \leftarrow w_i , e^{-\alpha_t y_i h_t(x_i)}\)</span></td>
</tr>
<tr class="even">
<td>6</td>
<td>Normalize</td>
<td>Scale <span class="math inline">\(w_i\)</span> so <span class="math inline">\(\sum_i w_i = 1\)</span></td>
</tr>
<tr class="odd">
<td>7</td>
<td>Repeat</td>
<td>For <span class="math inline">\(t = 1, 2, \dots, T\)</span></td>
</tr>
</tbody>
</table>
<p>Hard examples get higher weights, so subsequent learners focus on them.</p>
</section>
<section id="example-4" class="level4">
<h4 class="anchored" data-anchor-id="example-4">Example</h4>
<p>Suppose a weak learner gets 80% accuracy (<span class="math inline">\(\varepsilon = 0.2\)</span>). Its weight is:</p>
<p><span class="math display">\[
\alpha = \frac{1}{2} \ln\frac{1 - 0.2}{0.2} = 0.693
\]</span></p>
<p>Misclassified points get their weights increased by <span class="math inline">\(e^{+\alpha}\)</span>, making them more important next round.</p>
</section>
<section id="tiny-code-easy-versions-12" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-12">Tiny Code (Easy Versions)</h4>
<p>Python (Binary AdaBoost with Decision Stumps)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adaboost(X, y, T<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.ones(n) <span class="op">/</span> n</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    models, alphas <span class="op">=</span> [], []</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train weak learner (simple threshold)</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        thresh <span class="op">=</span> np.random.choice(X[:, <span class="dv">0</span>])</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> np.where(X[:, <span class="dv">0</span>] <span class="op">&lt;</span> thresh, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        err <span class="op">=</span> np.<span class="bu">sum</span>(w <span class="op">*</span> (preds <span class="op">!=</span> y)) <span class="op">/</span> np.<span class="bu">sum</span>(w)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> err <span class="op">&gt;</span> <span class="fl">0.5</span>: </span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.log((<span class="dv">1</span> <span class="op">-</span> err) <span class="op">/</span> (err <span class="op">+</span> <span class="fl">1e-9</span>))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        w <span class="op">*=</span> np.exp(<span class="op">-</span>alpha <span class="op">*</span> y <span class="op">*</span> preds)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        w <span class="op">/=</span> np.<span class="bu">sum</span>(w)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        models.append(thresh)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>        alphas.append(alpha)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(X_test):</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_test))</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> thresh, alpha <span class="kw">in</span> <span class="bu">zip</span>(models, alphas):</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> np.where(X_test[:, <span class="dv">0</span>] <span class="op">&lt;</span> thresh, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> alpha <span class="op">*</span> preds</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.sign(total)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predict</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize sample weights equally</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co">// For each round:</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">//   1. Train weak learner on weighted data</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">//   2. Compute error and alpha</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">//   3. Update sample weights</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Combine all weak learners with weighted vote</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-12" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-12">Why It Matters</h4>
<ul>
<li>Boosts weak models into strong ones.</li>
<li>Focuses on hard cases, adaptive weighting.</li>
<li>Theoretical guarantee: minimizes exponential loss.</li>
<li>No overfitting for small T (if weak learners are simple).</li>
<li>Foundation for gradient boosting methods.</li>
</ul>
<p>Common base learners:</p>
<ul>
<li>Decision stumps (1-split trees)</li>
<li>Small depth decision trees</li>
<li>Simple linear classifiers</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-12" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-12">A Gentle Proof (Why It Works)</h4>
<p>AdaBoost minimizes the exponential loss:</p>
<p><span class="math display">\[
L = \sum_{i=1}^{n} e^{-y_i F(x_i)}
\]</span></p>
<p>Each iteration chooses <span class="math inline">\(h_t\)</span> and <span class="math inline">\(\alpha_t\)</span> to greedily reduce <span class="math inline">\(L\)</span>. As <span class="math inline">\(F(x)\)</span> grows, correctly classified points (<span class="math inline">\(y_i F(x_i) &gt; 0\)</span>) get tiny weights, while errors dominate the next round’s objective. Thus, the ensemble naturally focuses on mistakes and builds margin over time.</p>
</section>
<section id="try-it-yourself-12" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-12">Try It Yourself</h4>
<ol type="1">
<li>Train AdaBoost with <span class="math inline">\(T = 10\)</span> decision stumps.</li>
<li>Plot sample weights, see focus shift to errors.</li>
<li>Increase <span class="math inline">\(T\)</span>: bias decreases, variance stabilizes.</li>
<li>Compare with Bagging, AdaBoost is sequential, not parallel.</li>
<li>Check performance on noisy data, too large <span class="math inline">\(T\)</span> may overfit.</li>
</ol>
</section>
<section id="test-cases-12" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-12">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Base Learner</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple linear separable</td>
<td>Stumps</td>
<td>Boosts to perfect accuracy</td>
</tr>
<tr class="even">
<td>Overlapping classes</td>
<td>Stumps</td>
<td>Focuses on ambiguous points</td>
</tr>
<tr class="odd">
<td>High noise</td>
<td>Stumps</td>
<td>Overfits if <span class="math inline">\(T\)</span> too large</td>
</tr>
<tr class="even">
<td>Balanced dataset</td>
<td>Stumps</td>
<td>Fast convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-12" class="level4">
<h4 class="anchored" data-anchor-id="complexity-12">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(T \cdot n \cdot C)\)</span> (weak learner cost <span class="math inline">\(C\)</span>)</li>
<li>Prediction: <span class="math inline">\(O(T)\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(T)\)</span> weak models</li>
</ul>
<p>AdaBoost is an iterative amplifier, it listens to every mistake, learns from it, and builds a chorus of weak voices into one strong, confident decision.</p>
</section>
</section>
<section id="gradient-boosting" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting">914. Gradient Boosting</h3>
<p>Gradient Boosting is a powerful ensemble technique that builds models stage by stage, each new learner corrects the residual errors of the previous ensemble by following the gradient of a loss function. It generalizes AdaBoost to arbitrary differentiable losses and base learners.</p>
<section id="what-problem-are-we-solving-13" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-13">What Problem Are We Solving?</h4>
<p>We want a way to combine many weak learners (like shallow trees) into a strong one by minimizing a loss function:</p>
<p><span class="math display">\[
L = \sum_{i=1}^n \ell(y_i, F(x_i))
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(F(x)\)</span> is the ensemble model, built as a sum of weak learners</li>
<li><span class="math inline">\(\ell(y, \hat{y})\)</span> is a differentiable loss (e.g., squared error, log-loss)</li>
</ul>
<p>We iteratively add learners that move <span class="math inline">\(F(x)\)</span> in the direction of the negative gradient of the loss with respect to predictions.</p>
</section>
<section id="how-does-it-work-plain-language-13" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-13">How Does It Work (Plain Language)?</h4>
<p>Think of it like gradient descent in function space. Each weak learner <span class="math inline">\(h_t(x)\)</span> learns to predict residuals, the direction to reduce error, not the labels directly. We then update the model by adding a scaled version of that learner.</p>
<p>Step-by-step (for regression):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 29%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize Model</td>
<td><span class="math inline">\(F_0(x) = \arg\min_c \sum_i \ell(y_i, c)\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>For each iteration <span class="math inline">\(t = 1..T\)</span>:</td>
<td></td>
</tr>
<tr class="odd">
<td>2a</td>
<td>Compute pseudo-residuals</td>
<td><span class="math inline">\(r_i^{(t)} = -\frac{\partial \ell(y_i, F_{t-1}(x_i))}{\partial F_{t-1}(x_i)}\)</span></td>
</tr>
<tr class="even">
<td>2b</td>
<td>Fit weak learner</td>
<td><span class="math inline">\(h_t(x)\)</span> to <span class="math inline">\((x_i, r_i^{(t)})\)</span></td>
</tr>
<tr class="odd">
<td>2c</td>
<td>Compute step size</td>
<td><span class="math inline">\(\gamma_t = \arg\min_\gamma \sum_i \ell(y_i, F_{t-1}(x_i) + \gamma h_t(x_i))\)</span></td>
</tr>
<tr class="even">
<td>2d</td>
<td>Update model</td>
<td><span class="math inline">\(F_t(x) = F_{t-1}(x) + \nu \gamma_t h_t(x)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Output final model</td>
<td><span class="math inline">\(F_T(x)\)</span></td>
</tr>
</tbody>
</table>
<p>The learning rate <span class="math inline">\(\nu\)</span> (<span class="math inline">\(0 &lt; \nu \le 1\)</span>) controls how much each learner contributes.</p>
</section>
<section id="example-squared-error" class="level4">
<h4 class="anchored" data-anchor-id="example-squared-error">Example (Squared Error)</h4>
<p>For <span class="math inline">\(\ell(y, F(x)) = \frac{1}{2}(y - F(x))^2\)</span>,</p>
<p><span class="math display">\[
r_i^{(t)} = y_i - F_{t-1}(x_i)
\]</span></p>
<p>So each new learner fits residuals (errors) directly.</p>
</section>
<section id="tiny-code-easy-versions-13" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-13">Tiny Code (Easy Versions)</h4>
<p>Python (Simplified Gradient Boosting for Regression)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_boosting(X, y, T<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.1</span>, depth<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> np.mean(y) <span class="op">*</span> np.ones(n)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    trees <span class="op">=</span> []</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        residuals <span class="op">=</span> y <span class="op">-</span> F</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        tree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span>depth)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        tree.fit(X, residuals)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        F <span class="op">+=</span> lr <span class="op">*</span> tree.predict(X)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        trees.append(tree)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F, trees</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize model prediction to mean(y)</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co">// For each iteration:</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">//   1. Compute residuals = y - prediction</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">//   2. Fit weak learner to residuals</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">//   3. Update prediction += learning_rate * new_prediction</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-13" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-13">Why It Matters</h4>
<ul>
<li>General framework: supports many losses (regression, classification).</li>
<li>Flexible: any differentiable loss function.</li>
<li>Accurate: successive correction yields high precision.</li>
<li>Controllable: learning rate + tree depth balance bias/variance.</li>
<li>Foundation: for modern implementations like XGBoost, LightGBM, CatBoost.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-13" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-13">A Gentle Proof (Why It Works)</h4>
<p>We build <span class="math inline">\(F(x)\)</span> incrementally to minimize:</p>
<p><span class="math display">\[
L(F) = \sum_i \ell(y_i, F(x_i))
\]</span></p>
<p>Each step adds <span class="math inline">\(h_t(x)\)</span> that approximates the negative gradient:</p>
<p><span class="math display">\[
r_i^{(t)} = -\frac{\partial L}{\partial F(x_i)} = -\frac{\partial \ell(y_i, F(x_i))}{\partial F(x_i)}
\]</span></p>
<p>Thus, the update:</p>
<p><span class="math display">\[
F_t(x) = F_{t-1}(x) + \nu \gamma_t h_t(x)
\]</span></p>
<p>acts like gradient descent, reducing loss at each iteration.</p>
</section>
<section id="try-it-yourself-13" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-13">Try It Yourself</h4>
<ol type="1">
<li>Fit Gradient Boosting with <span class="math inline">\(T=10\)</span>, <span class="math inline">\(lr=0.1\)</span> on simple regression data.</li>
<li>Plot predictions as learners accumulate, watch fit improve.</li>
<li>Compare with AdaBoost, note difference in update logic.</li>
<li>Experiment with deeper trees (reduce bias).</li>
<li>Decrease learning rate (increase <span class="math inline">\(T\)</span>), smoother convergence.</li>
</ol>
</section>
<section id="test-cases-13" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-13">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Loss</th>
<th>Expected Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Squared error</td>
<td>Fits linearly</td>
</tr>
<tr class="even">
<td>Nonlinear pattern</td>
<td>Squared error</td>
<td>Approximates curve</td>
</tr>
<tr class="odd">
<td>Classification</td>
<td>Log-loss</td>
<td>Logistic boosting</td>
</tr>
<tr class="even">
<td>High noise</td>
<td>Squared error</td>
<td>Overfits if <span class="math inline">\(T\)</span> too high</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-13" class="level4">
<h4 class="anchored" data-anchor-id="complexity-13">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(T \cdot n \cdot C)\)</span> (weak learner cost <span class="math inline">\(C\)</span>)</li>
<li>Prediction: <span class="math inline">\(O(T)\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(T)\)</span> learners</li>
</ul>
<p>Gradient Boosting is boosting with direction, each step follows the gradient, improving not by reweighting mistakes, but by <em>learning the shape of the loss itself</em>.</p>
</section>
</section>
<section id="xgboost-extreme-gradient-boosting" class="level3">
<h3 class="anchored" data-anchor-id="xgboost-extreme-gradient-boosting">915. XGBoost (Extreme Gradient Boosting)</h3>
<p>XGBoost (Extreme Gradient Boosting) is a high-performance, regularized implementation of gradient boosting. It extends the basic framework with second-order optimization, shrinkage, column sampling, and built-in regularization, achieving both speed and accuracy on large-scale datasets.</p>
<section id="what-problem-are-we-solving-14" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-14">What Problem Are We Solving?</h4>
<p>While Gradient Boosting provides strong accuracy, it can be slow and prone to overfitting. XGBoost addresses these by:</p>
<ol type="1">
<li>Adding regularization (L1 and L2) to penalize complexity.</li>
<li>Using second-order gradients for more accurate updates.</li>
<li>Employing column subsampling and shrinkage to improve generalization.</li>
<li>Implementing optimized tree building for large datasets.</li>
</ol>
<p>We still build a model:</p>
<p><span class="math display">\[
F(x) = \sum_{t=1}^T f_t(x), \quad f_t \in \mathcal{F}
\]</span></p>
<p>where each <span class="math inline">\(f_t\)</span> is a regression tree, and <span class="math inline">\(\mathcal{F}\)</span> is the space of trees.</p>
</section>
<section id="objective-function" class="level4">
<h4 class="anchored" data-anchor-id="objective-function">Objective Function</h4>
<p>The training objective at step <span class="math inline">\(t\)</span> is:</p>
<p><span class="math display">\[
\mathcal{L}^{(t)} = \sum_{i=1}^n \ell(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)
\]</span></p>
<p>Using a second-order Taylor expansion, we approximate:</p>
<p><span class="math display">\[
\mathcal{L}^{(t)} \approx \sum_{i=1}^n \Big[ g_i f_t(x_i) + \tfrac{1}{2} h_i f_t(x_i)^2 \Big] + \Omega(f_t)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(g_i = \frac{\partial \ell(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}\)</span></li>
<li><span class="math inline">\(h_i = \frac{\partial^2 \ell(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)2}}\)</span></li>
</ul>
<p>and the regularization term is:</p>
<p><span class="math display">\[
\Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\gamma\)</span>: penalty for number of leaves</li>
<li><span class="math inline">\(\lambda\)</span>: L2 regularization on leaf weights <span class="math inline">\(w_j\)</span></li>
</ul>
</section>
<section id="how-does-it-work-plain-language-14" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-14">How Does It Work (Plain Language)?</h4>
<p>At each iteration, XGBoost fits a new tree <span class="math inline">\(f_t\)</span> to the gradient and curvature of the loss. Each leaf has a weight computed analytically to minimize the approximate loss. This second-order information allows more precise optimization than simple residual fitting.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 2%">
<col style="width: 13%">
<col style="width: 84%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize</td>
<td><span class="math inline">\(\hat{y}^{(0)} = \arg\min_c \sum_i \ell(y_i, c)\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute Gradients</td>
<td><span class="math inline">\(g_i, h_i\)</span> for each training point</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Fit Tree</td>
<td>Use <span class="math inline">\(g_i, h_i\)</span> to find best splits and leaf weights</td>
</tr>
<tr class="even">
<td>4</td>
<td>Compute Leaf Weights</td>
<td><span class="math inline">\(w_j^* = - \frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Compute Split Gain</td>
<td><span class="math inline">\(\text{Gain} = \frac{1}{2}\Big[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}\Big] - \gamma\)</span></td>
</tr>
<tr class="even">
<td>6</td>
<td>Add Tree</td>
<td><span class="math inline">\(\hat{y}^{(t)} = \hat{y}^{(t-1)} + \eta f_t(x)\)</span></td>
</tr>
<tr class="odd">
<td>7</td>
<td>Repeat</td>
<td>Until <span class="math inline">\(T\)</span> trees or early stopping</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-conceptual-skeleton" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-conceptual-skeleton">Tiny Code (Conceptual Skeleton)</h4>
<p>Python (Conceptual Gradient Step)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudo-code (not actual implementation)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> grad(loss, y_pred)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> hess(loss, y_pred)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> build_tree(X, g, h, lambda_, gamma)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">+=</span> eta <span class="op">*</span> tree.predict(X)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>XGBoost’s C++ backend builds trees efficiently using histograms and parallelization.</p>
</section>
<section id="why-it-matters-14" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-14">Why It Matters</h4>
<ul>
<li>Regularized: avoids overfitting via <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span>.</li>
<li>Second-order optimization: faster, more accurate steps.</li>
<li>Shrinkage (learning rate): gradual updates improve generalization.</li>
<li>Column subsampling: decorrelates trees, reduces variance.</li>
<li>Highly optimized: supports parallel and distributed training.</li>
</ul>
<p>XGBoost is widely used in Kaggle competitions, finance, and industrial ML systems due to its balance of speed, accuracy, and robustness.</p>
</section>
<section id="a-gentle-proof-why-it-works-14" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-14">A Gentle Proof (Why It Works)</h4>
<p>The objective is approximated as:</p>
<p><span class="math display">\[
\tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^T \Big[ G_j w_j + \tfrac{1}{2}(H_j + \lambda)w_j^2 \Big] + \gamma T
\]</span></p>
<p>Minimizing with respect to <span class="math inline">\(w_j\)</span> gives:</p>
<p><span class="math display">\[
w_j^* = -\frac{G_j}{H_j + \lambda}
\]</span></p>
<p>Plugging back in gives optimal split gain:</p>
<p><span class="math display">\[
\text{Gain} = \frac{1}{2}\left(\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda}\right) - \gamma
\]</span></p>
<p>Each split maximizes gain, reducing loss greedily with regularization.</p>
</section>
<section id="try-it-yourself-14" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-14">Try It Yourself</h4>
<ol type="1">
<li>Train XGBoost on a small dataset (binary classification).</li>
<li>Inspect feature importances, which features dominate?</li>
<li>Adjust <span class="math inline">\(\eta\)</span> (learning rate): small <span class="math inline">\(\eta\)</span> + large <span class="math inline">\(T\)</span> = smoother convergence.</li>
<li>Tune <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\gamma\)</span> to balance bias and variance.</li>
<li>Compare with plain Gradient Boosting, faster convergence, less overfitting.</li>
</ol>
</section>
<section id="test-cases-14" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-14">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 20%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Task</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binary classification</td>
<td>Log-loss</td>
<td>Fast convergence</td>
</tr>
<tr class="even">
<td>Noisy regression</td>
<td>MSE</td>
<td>Stable via regularization</td>
</tr>
<tr class="odd">
<td>Wide data</td>
<td>Many features</td>
<td>Column subsampling helps</td>
</tr>
<tr class="even">
<td>Overfitting-prone</td>
<td>Any</td>
<td>Controlled by <span class="math inline">\(\lambda, \gamma\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-14" class="level4">
<h4 class="anchored" data-anchor-id="complexity-14">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(T \cdot n \log n)\)</span> (optimized tree splits)</li>
<li>Prediction: <span class="math inline">\(O(T \cdot \text{depth})\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(T)\)</span> trees</li>
</ul>
<p>XGBoost is Gradient Boosting, evolved, combining second-order learning, regularization, and computational engineering into a fast, reliable powerhouse for structured data.</p>
</section>
</section>
<section id="lightgbm-light-gradient-boosting-machine" class="level3">
<h3 class="anchored" data-anchor-id="lightgbm-light-gradient-boosting-machine">916. LightGBM (Light Gradient Boosting Machine)</h3>
<p>LightGBM is a highly efficient implementation of gradient boosting that focuses on speed, scalability, and memory efficiency. It introduces innovative techniques like histogram-based learning, leaf-wise growth, and Gradient-based One-Side Sampling (GOSS) to handle massive datasets without sacrificing accuracy.</p>
<section id="what-problem-are-we-solving-15" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-15">What Problem Are We Solving?</h4>
<p>Traditional gradient boosting (and even XGBoost) can become slow and memory-intensive on large, high-dimensional data. LightGBM tackles this by:</p>
<ol type="1">
<li>Reducing computation with histogram-based splits.</li>
<li>Growing trees leaf-wise, not level-wise (deeper, more accurate trees).</li>
<li>Using sampling techniques to focus on impactful data points.</li>
<li>Supporting categorical features natively.</li>
</ol>
<p>Its goal: fast training, low memory, high accuracy, especially for large-scale structured data.</p>
</section>
<section id="core-ideas" class="level4">
<h4 class="anchored" data-anchor-id="core-ideas">Core Ideas</h4>
<p>LightGBM builds additive models:</p>
<p><span class="math display">\[
F(x) = \sum_{t=1}^T f_t(x)
\]</span></p>
<p>Each <span class="math inline">\(f_t(x)\)</span> is a decision tree trained to minimize a second-order loss approximation:</p>
<p><span class="math display">\[
\mathcal{L}^{(t)} \approx \sum_{i=1}^{n} \Big[g_i f_t(x_i) + \tfrac{1}{2} h_i f_t(x_i)^2\Big] + \Omega(f_t)
\]</span></p>
<p>but with histogram-based splits and optimized sampling.</p>
</section>
<section id="how-does-it-work-plain-language-15" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-15">How Does It Work (Plain Language)?</h4>
<p>LightGBM streamlines tree boosting through three innovations:</p>
<ol type="1">
<li><p>Histogram-based Splitting: Continuous features are bucketed into discrete bins. This drastically reduces split search cost from <span class="math inline">\(O(n \cdot d)\)</span> to <span class="math inline">\(O(B \cdot d)\)</span>, where <span class="math inline">\(B\)</span> is the number of bins (e.g.&nbsp;255).</p></li>
<li><p>Leaf-wise Tree Growth (Best-First): Instead of expanding all leaves evenly (level-wise), LightGBM picks the leaf with the largest loss reduction to grow next. This allows deeper, more focused trees.</p></li>
<li><p>GOSS (Gradient-based One-Side Sampling): Keeps samples with large gradients (important for loss reduction) and randomly drops some with small gradients, reducing data while preserving learning direction.</p></li>
</ol>
</section>
<section id="step-by-step-summary" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary">Step-by-step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 30%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute Gradients</td>
<td><span class="math inline">\(g_i, h_i\)</span> for all samples</td>
</tr>
<tr class="even">
<td>2</td>
<td>Bucket Features</td>
<td>Build histograms of gradients by bin</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Split Search</td>
<td>For each feature, evaluate gain per bin</td>
</tr>
<tr class="even">
<td>4</td>
<td>Select Split</td>
<td>Maximize loss reduction</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Leaf-wise Growth</td>
<td>Expand leaf with largest gain</td>
</tr>
<tr class="even">
<td>6</td>
<td>Apply GOSS</td>
<td>Keep high-gradient samples, sample low ones</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Repeat</td>
<td>Until max leaves or stopping criterion</td>
</tr>
</tbody>
</table>
</section>
<section id="example-split-gain-formula" class="level4">
<h4 class="anchored" data-anchor-id="example-split-gain-formula">Example: Split Gain Formula</h4>
<p>The gain from splitting leaf <span class="math inline">\(j\)</span> is computed as:</p>
<p><span class="math display">\[
\text{Gain} = \frac{1}{2}\left( \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right) - \gamma
\]</span></p>
<p>where <span class="math inline">\(G\)</span> and <span class="math inline">\(H\)</span> are the sums of gradients and Hessians in left/right child leaves.</p>
</section>
<section id="tiny-code-conceptual-sketch" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-conceptual-sketch">Tiny Code (Conceptual Sketch)</h4>
<p>Python (Conceptual)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conceptual illustration, not full implementation</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    g, h <span class="op">=</span> compute_gradients(y, y_pred)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    hist <span class="op">=</span> build_histograms(X, g, h, bins<span class="op">=</span><span class="dv">255</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    best_split <span class="op">=</span> find_best_leafwise_split(hist)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> grow_tree(best_split)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">+=</span> lr <span class="op">*</span> tree.predict(X)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>In Practice: use <code>lightgbm.LGBMClassifier</code> or <code>LGBMRegressor</code> from the library.</p>
</section>
<section id="why-it-matters-15" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-15">Why It Matters</h4>
<ul>
<li>Speed: Histogram binning + GOSS = faster than XGBoost.</li>
<li>Accuracy: Leaf-wise trees focus on high-gain splits.</li>
<li>Memory efficiency: Uses compressed bins, not raw floats.</li>
<li>Scalable: Handles millions of rows and features.</li>
<li>Native support: Categorical variables and GPU acceleration.</li>
</ul>
<p>LightGBM is especially effective on large tabular datasets common in finance, recommender systems, and competitions.</p>
</section>
<section id="a-gentle-proof-why-it-works-15" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-15">A Gentle Proof (Why It Works)</h4>
<p>The histogram trick approximates continuous features into bins <span class="math inline">\(B_j\)</span>:</p>
<p><span class="math display">\[
\sum_{i \in B_j} g_i, \quad \sum_{i \in B_j} h_i
\]</span></p>
<p>Splits are evaluated using these aggregated values, preserving gradient statistics while reducing cost.</p>
<p>For GOSS:</p>
<ul>
<li>Keep top <span class="math inline">\(a%\)</span> high-gradient samples.</li>
<li>Sample <span class="math inline">\(b%\)</span> of low-gradient samples.</li>
<li>Apply a correction factor <span class="math inline">\(\frac{1-a}{b}\)</span> to maintain unbiasedness.</li>
</ul>
<p>This maintains a valid estimate of total gradient direction, ensuring convergence to the same optimum as full data.</p>
</section>
<section id="try-it-yourself-15" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-15">Try It Yourself</h4>
<ol type="1">
<li>Train LightGBM on a dataset with 1M samples, note speed.</li>
<li>Compare with XGBoost, see 2–3× faster training.</li>
<li>Tune <code>num_leaves</code>: higher → lower bias, higher variance.</li>
<li>Adjust <code>max_bin</code> for accuracy/speed tradeoff.</li>
<li>Enable <code>categorical_feature</code>, no one-hot encoding needed.</li>
</ol>
</section>
<section id="test-cases-15" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-15">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Task</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Large tabular</td>
<td>Classification</td>
<td>Fast, high accuracy</td>
</tr>
<tr class="even">
<td>Small dataset</td>
<td>Regression</td>
<td>Similar to XGBoost</td>
</tr>
<tr class="odd">
<td>Categorical-heavy</td>
<td>Classification</td>
<td>Built-in handling</td>
</tr>
<tr class="even">
<td>Noisy data</td>
<td>Any</td>
<td>May overfit, tune <code>num_leaves</code></td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-15" class="level4">
<h4 class="anchored" data-anchor-id="complexity-15">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(T \cdot B \cdot d)\)</span> (with <span class="math inline">\(B\)</span> bins, <span class="math inline">\(d\)</span> features)</li>
<li>Prediction: <span class="math inline">\(O(T \cdot \text{depth})\)</span></li>
<li>Space: <span class="math inline">\(O(B \cdot d)\)</span> for histograms</li>
</ul>
<p>LightGBM is gradient boosting on turbo mode, fast, memory-efficient, and laser-focused, designed for modern data scale and speed.</p>
</section>
</section>
<section id="catboost-categorical-boosting" class="level3">
<h3 class="anchored" data-anchor-id="catboost-categorical-boosting">917. CatBoost (Categorical Boosting)</h3>
<p>CatBoost is a gradient boosting library designed specifically to handle categorical features efficiently and avoid prediction shift (target leakage). It uses ordered boosting and target encoding with permutation-based statistics, producing fast, accurate, and stable models, especially for tabular data.</p>
<section id="what-problem-are-we-solving-16" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-16">What Problem Are We Solving?</h4>
<p>Many gradient boosting libraries require manual encoding (like one-hot or label encoding) for categorical features, which can lead to:</p>
<ul>
<li>Inefficient memory use (especially for high-cardinality features)</li>
<li>Overfitting due to target leakage (when target information leaks into training)</li>
</ul>
<p>CatBoost solves this by:</p>
<ol type="1">
<li>Natively encoding categorical features using permutation-driven target statistics.</li>
<li>Using ordered boosting, which prevents data from “seeing the future” during training.</li>
</ol>
<p>The model still follows the standard boosting formula:</p>
<p><span class="math display">\[
F(x) = \sum_{t=1}^T f_t(x)
\]</span></p>
<p>but with special handling for categorical variables and ordered learning.</p>
</section>
<section id="how-does-it-work-plain-language-16" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-16">How Does It Work (Plain Language)?</h4>
<p>CatBoost builds trees like Gradient Boosting, but adds two key innovations:</p>
<ol type="1">
<li><p>Ordered Target Encoding (OTEs): Instead of using the global target mean for encoding a category (which leaks information), CatBoost computes progressive averages over random permutations of the dataset. Each sample’s encoding uses only past samples.</p>
<p>Example for category <span class="math inline">\(c\)</span>: <span class="math display">\[
\text{Enc}(x_i) = \frac{\sum_{j &lt; i, x_j = c} y_j + a \cdot P}{N_{&lt;i, c} + a}
\]</span> where:</p>
<ul>
<li><span class="math inline">\(N_{&lt;i, c}\)</span> = count of category <span class="math inline">\(c\)</span> before position <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(P\)</span> = prior (global mean)</li>
<li><span class="math inline">\(a\)</span> = smoothing parameter</li>
</ul></li>
<li><p>Ordered Boosting: Instead of fitting residuals on the same dataset, it simulates online learning: each iteration uses only data that would have been available at that point. This avoids target leakage and improves generalization.</p></li>
</ol>
</section>
<section id="step-by-step-summary-1" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-1">Step-by-step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 32%">
<col style="width: 62%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Random Permutation</td>
<td>Shuffle training data</td>
</tr>
<tr class="even">
<td>2</td>
<td>Encode Categories</td>
<td>Compute target statistics in order</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute Gradients</td>
<td>Based on current predictions</td>
</tr>
<tr class="even">
<td>4</td>
<td>Fit Weak Learner</td>
<td>Train tree on transformed data</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Apply Ordered Boosting</td>
<td>Use only past information</td>
</tr>
<tr class="even">
<td>6</td>
<td>Add Tree</td>
<td>Update ensemble <span class="math inline">\(F_t(x) = F_{t-1}(x) + \eta f_t(x)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-target-encoding" class="level4">
<h4 class="anchored" data-anchor-id="example-target-encoding">Example: Target Encoding</h4>
<p>Suppose we have a categorical feature “Color” with samples in order:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Sample</th>
<th>Color</th>
<th>Target</th>
<th>Encoding (step-by-step)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Red</td>
<td>1</td>
<td><span class="math inline">\((a \cdot P) / a\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Blue</td>
<td>0</td>
<td><span class="math inline">\((a \cdot P) / a\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Red</td>
<td>0</td>
<td><span class="math inline">\((a \cdot P + 1) / (a + 1)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Blue</td>
<td>1</td>
<td><span class="math inline">\((a \cdot P + 0) / (a + 1)\)</span></td>
</tr>
</tbody>
</table>
<p>Each sample only uses past target values for encoding, avoiding leakage.</p>
</section>
<section id="tiny-code-easy-versions-14" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-14">Tiny Code (Easy Versions)</h4>
<p>Python (with CatBoost library)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> catboost <span class="im">import</span> CatBoostClassifier</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CatBoostClassifier(</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    depth<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    cat_features<span class="op">=</span>[<span class="st">'color'</span>, <span class="st">'brand'</span>, <span class="st">'city'</span>],</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    loss_function<span class="op">=</span><span class="st">'Logloss'</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Pseudocode:</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Shuffle data randomly</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. For each categorical feature, compute ordered target encoding</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Train gradient boosting trees on encoded data using ordered boosting</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-16" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-16">Why It Matters</h4>
<ul>
<li>No preprocessing required: handles categorical features natively.</li>
<li>Avoids target leakage: uses ordered statistics and boosting.</li>
<li>Fast and accurate: efficient C++ core with CPU/GPU support.</li>
<li>Great for tabular data: especially with mixed feature types.</li>
<li>Robust: less tuning required than XGBoost or LightGBM.</li>
</ul>
<p>CatBoost shines when datasets include categorical variables, small to medium size, and non-linear interactions.</p>
</section>
<section id="a-gentle-proof-why-it-works-16" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-16">A Gentle Proof (Why It Works)</h4>
<p>CatBoost combats target leakage by making feature transformations causally consistent:</p>
<ul>
<li>For encoding, only earlier samples influence the current one.</li>
<li>For boosting, each model sees predictions computed without access to the same sample.</li>
</ul>
<p>This ensures unbiased gradient estimation, improving stability and generalization.</p>
</section>
<section id="try-it-yourself-16" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-16">Try It Yourself</h4>
<ol type="1">
<li>Train CatBoost on a dataset with categorical columns.</li>
<li>Compare with one-hot encoding + XGBoost.</li>
<li>Observe better accuracy and less overfitting.</li>
<li>Visualize feature importances, categories are meaningful.</li>
<li>Try <code>model.get_feature_importance(prettified=True)</code> for insights.</li>
</ol>
</section>
<section id="test-cases-16" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-16">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Feature Type</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Categorical-heavy</td>
<td>Many strings</td>
<td>Superior to one-hot methods</td>
</tr>
<tr class="even">
<td>Continuous-only</td>
<td>Numeric</td>
<td>Similar to XGBoost</td>
</tr>
<tr class="odd">
<td>High-cardinality</td>
<td>IDs or names</td>
<td>Efficient via OTE</td>
</tr>
<tr class="even">
<td>Small dataset</td>
<td>Mixed</td>
<td>Stable, low overfitting</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-16" class="level4">
<h4 class="anchored" data-anchor-id="complexity-16">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(T \cdot n \log n)\)</span> (tree building)</li>
<li>Prediction: <span class="math inline">\(O(T \cdot \text{depth})\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(T)\)</span> trees + encoded features</li>
</ul>
<p>CatBoost is boosting done right for categorical data, blending probabilistic encoding, ordered learning, and gradient optimization into one cohesive system.</p>
</section>
</section>
<section id="stacking-stacked-generalization" class="level3">
<h3 class="anchored" data-anchor-id="stacking-stacked-generalization">918. Stacking (Stacked Generalization)</h3>
<p>Stacking (or Stacked Generalization) is an ensemble learning technique that combines multiple base models by training a meta-model to learn how to best blend their predictions. Instead of simple averaging or voting, it learns the optimal combination directly from data.</p>
<section id="what-problem-are-we-solving-17" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-17">What Problem Are We Solving?</h4>
<p>Individual models capture different patterns or biases in data. Simple ensembling (like bagging or boosting) may not exploit these complementary strengths effectively.</p>
<p>Stacking learns a meta-level model to optimally weight or combine the outputs of base learners, often yielding higher accuracy and robust generalization.</p>
<p>We aim to build a model:</p>
<p><span class="math display">\[
\hat{y} = g(f_1(x), f_2(x), \ldots, f_m(x))
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(f_i(x)\)</span> = base learner predictions</li>
<li><span class="math inline">\(g(\cdot)\)</span> = meta-model (e.g.&nbsp;linear regression, logistic regression)</li>
</ul>
</section>
<section id="how-does-it-work-plain-language-17" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-17">How Does It Work (Plain Language)?</h4>
<p>Stacking is a two-level learning system:</p>
<ol type="1">
<li>Level-0 (Base Learners): Train multiple diverse models (trees, SVMs, neural nets, etc.).</li>
<li>Level-1 (Meta Learner): Train a model on the out-of-fold predictions of base learners to predict the final output.</li>
</ol>
<p>By using out-of-fold (OOF) predictions, we ensure that the meta-model is trained on unseen data, preventing information leakage.</p>
</section>
<section id="step-by-step-summary-2" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-2">Step-by-step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 30%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Split Data into Folds</td>
<td>Create <span class="math inline">\(K\)</span> folds for cross-validation</td>
</tr>
<tr class="even">
<td>2</td>
<td>Train Base Models</td>
<td>Each <span class="math inline">\(f_i\)</span> trained on <span class="math inline">\((K-1)\)</span> folds, predicts held-out fold</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Collect OOF Predictions</td>
<td>Build new dataset of base predictions</td>
</tr>
<tr class="even">
<td>4</td>
<td>Train Meta-Model</td>
<td>Fit <span class="math inline">\(g\)</span> on OOF predictions vs.&nbsp;true labels</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Final Model</td>
<td>Base learners on full data + meta-model on full OOF</td>
</tr>
</tbody>
</table>
<p>The final prediction for a new <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\hat{y} = g(f_1(x), f_2(x), \ldots, f_m(x))
\]</span></p>
</section>
<section id="example-5" class="level4">
<h4 class="anchored" data-anchor-id="example-5">Example</h4>
<p>Suppose you have 3 base models: Logistic Regression (<span class="math inline">\(f_1\)</span>), Random Forest (<span class="math inline">\(f_2\)</span>), and SVM (<span class="math inline">\(f_3\)</span>). You generate their predictions on validation folds and stack them as new features:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Sample</th>
<th><span class="math inline">\(f_1(x)\)</span></th>
<th><span class="math inline">\(f_2(x)\)</span></th>
<th><span class="math inline">\(f_3(x)\)</span></th>
<th><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.2</td>
<td>0.4</td>
<td>0.3</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.8</td>
<td>0.9</td>
<td>0.7</td>
<td>1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.6</td>
<td>0.5</td>
<td>0.4</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Then train a meta-model <span class="math inline">\(g\)</span> (like Logistic Regression) on this table.</p>
</section>
<section id="tiny-code-easy-versions-15" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-15">Tiny Code (Easy Versions)</h4>
<p>Python (Stacking Example)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, GradientBoostingClassifier</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stacking_train_predict(X, y, X_test, folds<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    base_models <span class="op">=</span> [</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>),</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        GradientBoostingClassifier(),</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        SVC(probability<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>folds, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    meta_features <span class="op">=</span> np.zeros((<span class="bu">len</span>(y), <span class="bu">len</span>(base_models)))</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Level-0: generate out-of-fold predictions</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, model <span class="kw">in</span> <span class="bu">enumerate</span>(base_models):</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        oof_pred <span class="op">=</span> np.zeros(<span class="bu">len</span>(y))</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_idx, val_idx <span class="kw">in</span> kf.split(X):</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>            model.fit(X[train_idx], y[train_idx])</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>            oof_pred[val_idx] <span class="op">=</span> model.predict_proba(X[val_idx])[:, <span class="dv">1</span>]</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        meta_features[:, j] <span class="op">=</span> oof_pred</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Level-1: train meta-model</span></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    meta_model <span class="op">=</span> LogisticRegression()</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    meta_model.fit(meta_features, y)</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    test_meta <span class="op">=</span> np.column_stack([m.fit(X, y).predict_proba(X_test)[:, <span class="dv">1</span>] <span class="cf">for</span> m <span class="kw">in</span> base_models])</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> meta_model.predict(test_meta)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Train each base model on K-1 folds, predict held-out fold</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Combine out-of-fold predictions into meta dataset</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Train meta-model on this new dataset</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Use trained base + meta models for final predictions</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-17" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-17">Why It Matters</h4>
<ul>
<li>Combines strengths of diverse models.</li>
<li>Learns combination weights instead of assuming equal importance.</li>
<li>Reduces generalization error by leveraging complementary patterns.</li>
<li>Flexible framework: any model can serve as base or meta learner.</li>
</ul>
<p>Stacking is often the final layer in competitive ML pipelines, blending tree-based, linear, and deep models.</p>
</section>
<section id="a-gentle-proof-why-it-works-17" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-17">A Gentle Proof (Why It Works)</h4>
<p>By training <span class="math inline">\(g\)</span> on OOF predictions, stacking approximates the conditional expectation of <span class="math inline">\(y\)</span> given model outputs:</p>
<p><span class="math display">\[
g^*(f_1(x), \dots, f_m(x)) = \mathbb{E}[y \mid f_1(x), \dots, f_m(x)]
\]</span></p>
<p>This ensures unbiased learning since <span class="math inline">\(g\)</span> only sees predictions generated without using the true fold during fitting, avoiding overfitting and improving generalization.</p>
</section>
<section id="try-it-yourself-17" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-17">Try It Yourself</h4>
<ol type="1">
<li>Choose 3–5 base learners (diverse architectures).</li>
<li>Use 5-fold OOF stacking to create meta-features.</li>
<li>Train a simple meta-model (Logistic or Ridge).</li>
<li>Compare with averaging or voting, stacking often wins.</li>
<li>Try 2-level stacking (stack of stacks) for advanced setups.</li>
</ol>
</section>
<section id="test-cases-17" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-17">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 24%">
<col style="width: 22%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Base Models</th>
<th>Meta-Model</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mixed linear/nonlinear</td>
<td>Logistic, RF, GBM</td>
<td>Logistic</td>
<td>Best of both</td>
</tr>
<tr class="even">
<td>Noisy</td>
<td>Trees, SVM</td>
<td>Ridge</td>
<td>Smooth combination</td>
</tr>
<tr class="odd">
<td>Small dataset</td>
<td>Simple models</td>
<td>Low depth</td>
<td>Avoid overfitting</td>
</tr>
<tr class="even">
<td>Large dataset</td>
<td>Many base learners</td>
<td>Strong meta-model</td>
<td>Gains accuracy</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-17" class="level4">
<h4 class="anchored" data-anchor-id="complexity-17">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(K \cdot M \cdot C)\)</span> (folds × base models × cost)</li>
<li>Prediction: <span class="math inline">\(O(M + 1)\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(M)\)</span> models + meta dataset</li>
</ul>
<p>Stacking is where models learn to collaborate, a meta-learner orchestrates their voices, turning a chorus of predictions into harmony.</p>
</section>
</section>
<section id="voting-classifier" class="level3">
<h3 class="anchored" data-anchor-id="voting-classifier">919. Voting Classifier</h3>
<p>A Voting Classifier is one of the simplest ensemble methods, it combines predictions from multiple models and decides the final output by majority vote (for classification) or average (for regression). It doesn’t learn combination weights; instead, it relies on the collective “wisdom” of its models.</p>
<section id="what-problem-are-we-solving-18" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-18">What Problem Are We Solving?</h4>
<p>Individual models can be unstable or biased. Averaging their opinions often reduces variance and improves robustness.</p>
<p>The voting classifier offers a quick, interpretable way to aggregate multiple models without extra training.</p>
<p>We aim to combine <span class="math inline">\(M\)</span> models:</p>
<p>For classification: <span class="math display">\[
\hat{y} = \arg\max_{c} \sum_{m=1}^{M} \mathbf{1}(\hat{y}^{(m)} = c)
\]</span></p>
<p>For regression: <span class="math display">\[
\hat{y} = \frac{1}{M} \sum_{m=1}^{M} \hat{y}^{(m)}
\]</span></p>
</section>
<section id="how-does-it-work-plain-language-18" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-18">How Does It Work (Plain Language)?</h4>
<p>Imagine consulting several experts. Each gives a prediction. You don’t judge who’s better, you simply take the majority opinion (hard voting) or average their confidence (soft voting).</p>
<p>Two main modes:</p>
<ol type="1">
<li>Hard Voting: Use predicted class labels; majority wins.</li>
<li>Soft Voting: Use predicted probabilities; take the class with highest average probability.</li>
</ol>
</section>
<section id="step-by-step-summary-3" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-3">Step-by-step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 19%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Train Models</td>
<td>Fit each base model independently on the same dataset</td>
</tr>
<tr class="even">
<td>2</td>
<td>Get Predictions</td>
<td>For each model, compute <span class="math inline">\(\hat{y}^{(m)}\)</span> (hard) or <span class="math inline">\(p^{(m)}(y \mid x)\)</span> (soft)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Combine</td>
<td>Aggregate via majority (hard) or mean (soft)</td>
</tr>
<tr class="even">
<td>4</td>
<td>Decide Output</td>
<td>Return the class with most votes or highest mean probability</td>
</tr>
</tbody>
</table>
</section>
<section id="example-6" class="level4">
<h4 class="anchored" data-anchor-id="example-6">Example</h4>
<p>Suppose 3 classifiers give predictions:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logistic Regression</td>
<td>A</td>
</tr>
<tr class="even">
<td>Decision Tree</td>
<td>A</td>
</tr>
<tr class="odd">
<td>SVM</td>
<td>B</td>
</tr>
</tbody>
</table>
<p>Hard Voting:</p>
<ul>
<li>Votes: A(2), B(1) → Final: A</li>
</ul>
<p>Soft Voting (probabilities):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>P(A)</th>
<th>P(B)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logistic</td>
<td>0.7</td>
<td>0.3</td>
</tr>
<tr class="even">
<td>Tree</td>
<td>0.6</td>
<td>0.4</td>
</tr>
<tr class="odd">
<td>SVM</td>
<td>0.4</td>
<td>0.6</td>
</tr>
</tbody>
</table>
<p>Averaged:</p>
<ul>
<li><span class="math inline">\(P(A) = 0.57\)</span>, <span class="math inline">\(P(B) = 0.43\)</span> → Final: A</li>
</ul>
</section>
<section id="tiny-code-easy-versions-16" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-16">Tiny Code (Easy Versions)</h4>
<p>Python (Hard &amp; Soft Voting Example)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> VotingClassifier</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>clf1 <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>clf2 <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>clf3 <span class="op">=</span> SVC(probability<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>voting_clf <span class="op">=</span> VotingClassifier(</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    estimators<span class="op">=</span>[(<span class="st">'lr'</span>, clf1), (<span class="st">'dt'</span>, clf2), (<span class="st">'svm'</span>, clf3)],</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    voting<span class="op">=</span><span class="st">'soft'</span>  <span class="co"># 'hard' for majority vote</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>voting_clf.fit(X_train, y_train)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> voting_clf.predict(X_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Train multiple base models</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Collect predictions</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Hard vote: pick class with majority votes</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Soft vote: average predicted probabilities, pick highest</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-18" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-18">Why It Matters</h4>
<ul>
<li>Simple &amp; effective: improves accuracy without extra complexity.</li>
<li>Low risk: doesn’t require tuning or meta-learning.</li>
<li>Stable: smooths out individual model errors.</li>
<li>Versatile: works with any classifiers or regressors.</li>
</ul>
<p>Ideal when:</p>
<ul>
<li>Models are independent or have diverse biases.</li>
<li>You want quick ensemble gains without cross-validation or meta-models.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-18" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-18">A Gentle Proof (Why It Works)</h4>
<p>If each model has error rate <span class="math inline">\(p &lt; 0.5\)</span> and errors are independent, the probability that the majority vote is wrong decreases exponentially with the number of models:</p>
<p><span class="math display">\[
P(\text{majority wrong}) = \sum_{k = \lceil M/2 \rceil}^{M} \binom{M}{k} p^k (1-p)^{M-k}
\]</span></p>
<p>This is the Condorcet Jury Theorem: the majority decision is more likely correct than any individual voter, assuming independence and competence.</p>
</section>
<section id="try-it-yourself-18" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-18">Try It Yourself</h4>
<ol type="1">
<li>Combine logistic regression, decision tree, and kNN.</li>
<li>Try both <code>voting='hard'</code> and <code>voting='soft'</code>.</li>
<li>Compare accuracy with individual models.</li>
<li>Add a poor-performing model, see how it dilutes performance.</li>
<li>Test on noisy data, ensembles resist overfitting.</li>
</ol>
</section>
<section id="test-cases-18" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-18">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Mode</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Balanced classes</td>
<td>Hard</td>
<td>Stable accuracy</td>
</tr>
<tr class="even">
<td>Probabilistic models</td>
<td>Soft</td>
<td>Better calibration</td>
</tr>
<tr class="odd">
<td>Correlated models</td>
<td>Either</td>
<td>Limited gain</td>
</tr>
<tr class="even">
<td>Diverse models</td>
<td>Soft</td>
<td>Strong improvement</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-18" class="level4">
<h4 class="anchored" data-anchor-id="complexity-18">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(\sum_m C_m)\)</span> (sum of individual costs)</li>
<li>Prediction: <span class="math inline">\(O(M)\)</span> per sample</li>
<li>Space: <span class="math inline">\(O(M)\)</span> models</li>
</ul>
<p>The Voting Classifier is the simplest ensemble, no blending, no boosting, just a democratic vote where every model has a say, and consensus drives accuracy.</p>
</section>
</section>
<section id="snapshot-ensemble" class="level3">
<h3 class="anchored" data-anchor-id="snapshot-ensemble">920. Snapshot Ensemble</h3>
<p>A Snapshot Ensemble is an elegant technique that turns a single training run into multiple models by capturing the network’s weights at different points during training, typically when it converges to different local minima using a cyclical learning rate schedule. These snapshots are later combined to form an ensemble, improving generalization without extra training cost.</p>
<section id="what-problem-are-we-solving-19" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-19">What Problem Are We Solving?</h4>
<p>Ensembles improve accuracy by averaging predictions from diverse models, but training many deep networks is expensive. Snapshot Ensembles solve this by reusing one training trajectory, capturing multiple diverse states of the same model as it oscillates through different local minima.</p>
<p>We want to approximate an ensemble:</p>
<p><span class="math display">\[
\hat{y} = \frac{1}{M} \sum_{m=1}^{M} f(x; \theta_m)
\]</span></p>
<p>where each <span class="math inline">\(\theta_m\)</span> is a snapshot of the model parameters at a different point in training.</p>
</section>
<section id="how-does-it-work-plain-language-19" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-19">How Does It Work (Plain Language)?</h4>
<p>Instead of training <span class="math inline">\(M\)</span> models independently, we vary the learning rate cyclically, letting the optimizer move into and out of multiple minima. At the end of each cycle (when the learning rate is small), we save the weights. Each saved model is then used in the final ensemble.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 35%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize Model</td>
<td>Start from random weights</td>
</tr>
<tr class="even">
<td>2</td>
<td>Use Cyclical Learning Rate</td>
<td>Schedule (like cosine annealing) repeats <span class="math inline">\(M\)</span> cycles</td>
</tr>
<tr class="odd">
<td>3</td>
<td>At Each Cycle End</td>
<td>Save weights (snapshot)</td>
</tr>
<tr class="even">
<td>4</td>
<td>Continue Training</td>
<td>Restart learning rate high again</td>
</tr>
<tr class="odd">
<td>5</td>
<td>After Training</td>
<td>Combine all snapshots (average predictions)</td>
</tr>
</tbody>
</table>
<p>Thus, <span class="math inline">\(M\)</span> snapshots <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(M\)</span> models, all from one training process.</p>
</section>
<section id="learning-rate-schedule-cosine-annealing" class="level4">
<h4 class="anchored" data-anchor-id="learning-rate-schedule-cosine-annealing">Learning Rate Schedule (Cosine Annealing)</h4>
<p>The learning rate <span class="math inline">\(\eta(t)\)</span> is decayed and restarted periodically:</p>
<p><span class="math display">\[
\eta(t) = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min}) \left( 1 + \cos\left( \frac{\pi \bmod(t-1, T/M)}{T/M} \right) \right)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(T\)</span> = total iterations</li>
<li><span class="math inline">\(M\)</span> = number of cycles</li>
<li><span class="math inline">\(\eta_{\max}, \eta_{\min}\)</span> = bounds of learning rate</li>
</ul>
<p>This cyclical schedule ensures exploration of multiple basins of attraction.</p>
</section>
<section id="example-7" class="level4">
<h4 class="anchored" data-anchor-id="example-7">Example</h4>
<p>Suppose you plan <span class="math inline">\(M = 3\)</span> snapshots in 90 epochs:</p>
<ul>
<li>Each cycle = 30 epochs</li>
<li>Learning rate restarts every 30 epochs</li>
<li>Save model at epoch 30, 60, 90</li>
</ul>
<p>Final prediction:</p>
<p><span class="math display">\[
\hat{y} = \frac{1}{3}(f_1(x) + f_2(x) + f_3(x))
\]</span></p>
</section>
<section id="tiny-code-easy-versions-17" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-17">Tiny Code (Easy Versions)</h4>
<p>Python (PyTorch-style Example)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> CosineAnnealingLR</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyNetwork()</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> CosineAnnealingLR(optimizer, T_max<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>snapshots <span class="op">=</span> []</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">90</span>):</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    train_one_epoch(model, optimizer, data_loader)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">30</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        snapshots.append(model.state_dict().copy())</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensemble predictions</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ensemble_predict(x):</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> []</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> snapshots:</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        model.load_state_dict(s)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>        preds.append(model(x))</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(preds) <span class="op">/</span> <span class="bu">len</span>(preds)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Train model with cosine-annealed learning rate</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Every cycle:</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">//   1. Save model weights (snapshot)</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">//   2. Restart learning rate high</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">// After training: average predictions across snapshots</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-19" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-19">Why It Matters</h4>
<ul>
<li>Free ensemble: No additional training runs.</li>
<li>Diversity from a single trajectory: Each snapshot represents a different local minimum.</li>
<li>Improved generalization: Averages out variance and noise.</li>
<li>Elegant and efficient: Same training time as one model.</li>
</ul>
<p>It’s particularly useful in deep learning where full ensembles are costly.</p>
</section>
<section id="a-gentle-proof-why-it-works-19" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-19">A Gentle Proof (Why It Works)</h4>
<p>In non-convex optimization, stochastic gradient descent may converge to multiple local minima depending on the learning rate. By restarting the learning rate, we push the model out of one basin and into another, capturing multiple diverse hypotheses. Averaging their outputs reduces generalization error:</p>
<p><span class="math display">\[
E[(f(x) - y)^2] = \text{bias}^2 + \text{variance}
\]</span></p>
<p>Snapshot averaging lowers the variance term.</p>
</section>
<section id="try-it-yourself-19" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-19">Try It Yourself</h4>
<ol type="1">
<li><p>Train a CNN with cosine annealing and <span class="math inline">\(M = 3\)</span> cycles.</p></li>
<li><p>Save weights at end of each cycle.</p></li>
<li><p>Compare accuracy:</p>
<ul>
<li>Single final model</li>
<li>Snapshot ensemble (average predictions)</li>
</ul></li>
<li><p>Observe improved validation accuracy and smoother learning.</p></li>
<li><p>Increase <span class="math inline">\(M\)</span> for more diversity (up to diminishing returns).</p></li>
</ol>
</section>
<section id="test-cases-19" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-19">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Model</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CIFAR-10</td>
<td>CNN</td>
<td>Higher test accuracy</td>
</tr>
<tr class="even">
<td>MNIST</td>
<td>MLP</td>
<td>More stable predictions</td>
</tr>
<tr class="odd">
<td>Large network</td>
<td>ResNet</td>
<td>Similar to multi-model ensemble</td>
</tr>
<tr class="even">
<td>Limited compute</td>
<td>Any</td>
<td>Great tradeoff: ensemble at single cost</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-19" class="level4">
<h4 class="anchored" data-anchor-id="complexity-19">Complexity</h4>
<ul>
<li>Training: <span class="math inline">\(O(T)\)</span> (same as single model)</li>
<li>Prediction: <span class="math inline">\(O(M)\)</span> per sample (ensemble averaging)</li>
<li>Space: <span class="math inline">\(O(M)\)</span> snapshots</li>
</ul>
<p>Snapshot Ensembles give you the best of both worlds, the power of ensembling and the efficiency of single-model training, harnessing learning rate cycles to explore and capture diverse solutions.</p>
</section>
</section>
</section>
<section id="section-93.-gradient-methods" class="level1">
<h1>Section 93. Gradient Methods</h1>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">921. Gradient Descent</h3>
<p>Gradient Descent is the foundational optimization algorithm for training machine learning models. It works by iteratively moving parameters in the direction that reduces the loss function, following the slope downhill until reaching a (local) minimum.</p>
<section id="what-problem-are-we-solving-20" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-20">What Problem Are We Solving?</h4>
<p>Most learning problems involve minimizing a loss function <span class="math display">\[
L(\theta)
\]</span> over parameters <span class="math inline">\(\theta\)</span>. Closed-form solutions are rare for nonlinear functions, so we need an iterative method to approach the minimum.</p>
<p>Gradient Descent updates parameters by moving opposite to the gradient of the loss:</p>
<p><span class="math display">\[
\theta \leftarrow \theta - \eta , \nabla_\theta L(\theta)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\nabla_\theta L(\theta)\)</span> is the gradient (direction of steepest ascent),</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate, controlling step size.</li>
</ul>
<p>The intuition: If the gradient points uphill, moving in the opposite direction reduces the loss.</p>
</section>
<section id="how-does-it-work-plain-language-20" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-20">How Does It Work (Plain Language)?</h4>
<p>Imagine standing on a curved surface representing your loss function. You want to find the lowest valley. At each step:</p>
<ol type="1">
<li>Measure the slope (gradient).</li>
<li>Step downhill a small amount.</li>
<li>Repeat until the slope flattens (near minimum).</li>
</ol>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 28%">
<col style="width: 67%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize parameters</td>
<td>Choose random <span class="math inline">\(\theta_0\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute gradient</td>
<td><span class="math inline">\(\nabla_\theta L(\theta_t)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update</td>
<td><span class="math inline">\(\theta_{t+1} = \theta_t - \eta , \nabla_\theta L(\theta_t)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Check convergence</td>
<td>Stop if gradient small or change minimal</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until loss stabilizes</td>
</tr>
</tbody>
</table>
</section>
<section id="example-8" class="level4">
<h4 class="anchored" data-anchor-id="example-8">Example</h4>
<p>Suppose <span class="math display">\[
L(\theta) = \theta^2
\]</span> Then <span class="math display">\[
\nabla_\theta L = 2\theta
\]</span> With <span class="math inline">\(\eta = 0.1\)</span> and <span class="math inline">\(\theta_0 = 1.0\)</span>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Iteration</th>
<th><span class="math inline">\(\theta\)</span></th>
<th><span class="math inline">\(L(\theta)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.800</td>
<td>0.640</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.640</td>
<td>0.410</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.512</td>
<td>0.262</td>
</tr>
</tbody>
</table>
<p>Each step moves <span class="math inline">\(\theta\)</span> closer to 0 (the minimum).</p>
</section>
<section id="tiny-code-easy-versions-18" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-18">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.1</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> L_grad(theta)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> gradient_descent(L_grad, theta0<span class="op">=</span><span class="fl">1.0</span>, lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">;</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> theta<span class="op">;</span>  <span class="co">// d/dθ (θ²)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-20" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-20">Why It Matters</h4>
<ul>
<li>Universal optimizer: works for any differentiable loss.</li>
<li>Foundation of deep learning: every neural network is trained via gradient descent (or variants).</li>
<li>Conceptual bridge: introduces learning rate, convergence, and curvature intuition.</li>
<li>Extensible: leads to SGD, Momentum, Adam, etc.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-20" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-20">A Gentle Proof (Why It Works)</h4>
<p>Near the minimum, Taylor expansion:</p>
<p><span class="math display">\[
L(\theta + \Delta \theta) \approx L(\theta) + \nabla_\theta L(\theta)^\top \Delta \theta
\]</span></p>
<p>To reduce <span class="math inline">\(L\)</span>, choose <span class="math inline">\(\Delta \theta\)</span> in direction <span class="math inline">\(-\nabla_\theta L\)</span>. If <span class="math inline">\(\eta\)</span> is small enough, each update decreases loss monotonically.</p>
<p>Convergence rate (for convex <span class="math inline">\(L\)</span>):</p>
<p><span class="math display">\[
L(\theta_t) - L(\theta^*) \le \frac{C}{t}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> depends on smoothness of <span class="math inline">\(L\)</span>.</p>
</section>
<section id="try-it-yourself-20" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-20">Try It Yourself</h4>
<ol type="1">
<li>Minimize <span class="math inline">\(L(\theta) = (\theta - 3)^2\)</span>.</li>
<li>Experiment with <span class="math inline">\(\eta = 0.01, 0.1, 1.0\)</span>.</li>
<li>Plot trajectory of <span class="math inline">\(\theta_t\)</span> over iterations.</li>
<li>Try multivariate <span class="math inline">\(L(\theta_1, \theta_2) = \theta_1^2 + 2\theta_2^2\)</span>.</li>
<li>Visualize gradient arrows on contour plot.</li>
</ol>
</section>
<section id="test-cases-20" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-20">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Loss</th>
<th>Initial <span class="math inline">\(\theta\)</span></th>
<th><span class="math inline">\(\eta\)</span></th>
<th>Convergence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\theta^2\)</span></td>
<td>1.0</td>
<td>0.1</td>
<td>Smooth descent</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\theta^2\)</span></td>
<td>1.0</td>
<td>1.0</td>
<td>Overshoot oscillations</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\theta^2\)</span></td>
<td>1.0</td>
<td>0.01</td>
<td>Slow convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-20" class="level4">
<h4 class="anchored" data-anchor-id="complexity-20">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span> (for <span class="math inline">\(d\)</span> parameters)</li>
<li>Memory: <span class="math inline">\(O(d)\)</span></li>
<li>Convergence: depends on curvature and step size</li>
</ul>
<p>Gradient Descent is the engine of learning, a simple, steady march downhill, powering everything from linear regression to deep neural networks.</p>
</section>
</section>
<section id="stochastic-gradient-descent-sgd" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd">922. Stochastic Gradient Descent (SGD)</h3>
<p>Stochastic Gradient Descent (SGD) is a faster, noisier cousin of classical gradient descent. Instead of using all training samples to compute the gradient at each step, SGD updates parameters using a single sample (or small batch), giving it speed, randomness, and the ability to escape shallow minima.</p>
<section id="what-problem-are-we-solving-21" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-21">What Problem Are We Solving?</h4>
<p>In large-scale learning, computing the full gradient</p>
<p><span class="math display">\[
\nabla_\theta L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \ell(x_i, y_i; \theta)
\]</span></p>
<p>is too slow, especially when <span class="math inline">\(N\)</span> is huge.</p>
<p>SGD approximates the full gradient using a single random sample (or mini-batch):</p>
<p><span class="math display">\[
\theta \leftarrow \theta - \eta , \nabla_\theta \ell(x_i, y_i; \theta)
\]</span></p>
<p>This gives a cheap, noisy estimate that works well on average.</p>
</section>
<section id="how-does-it-work-plain-language-21" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-21">How Does It Work (Plain Language)?</h4>
<p>Think of classical gradient descent as checking every data point before taking a step, very precise, but slow. SGD, instead, takes a quick glance at one example, makes a guess, adjusts a bit, and keeps going, learning through many small, noisy updates.</p>
<p>Over time, these noisy steps average out to move roughly in the right direction.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 30%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Shuffle Data</td>
<td>Randomize order of training samples</td>
</tr>
<tr class="even">
<td>2</td>
<td>Pick One Sample</td>
<td><span class="math inline">\((x_i, y_i)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute Gradient</td>
<td><span class="math inline">\(g_i = \nabla_\theta \ell(x_i, y_i; \theta)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update Parameters</td>
<td><span class="math inline">\(\theta \gets \theta - \eta g_i\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>For all samples (1 epoch), then reshuffle</td>
</tr>
</tbody>
</table>
</section>
<section id="example-9" class="level4">
<h4 class="anchored" data-anchor-id="example-9">Example</h4>
<p>Suppose loss is <span class="math display">\[
L(\theta) = \frac{1}{N}\sum_{i=1}^{N} (\theta - x_i)^2
\]</span></p>
<p>Full gradient: <span class="math display">\[
\nabla_\theta L = 2(\theta - \bar{x})
\]</span></p>
<p>SGD update with one sample <span class="math inline">\(x_i\)</span>: <span class="math display">\[
\theta \gets \theta - \eta \cdot 2(\theta - x_i)
\]</span></p>
<p>Each update pulls <span class="math inline">\(\theta\)</span> a little toward a single data point, gradually converging toward <span class="math inline">\(\bar{x}\)</span>.</p>
</section>
<section id="tiny-code-easy-versions-19" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-19">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd(X, y, lr<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> np.random.permutation(n):</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (theta <span class="op">-</span> X[i])</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> epoch <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> epoch <span class="op">&lt;</span> epochs<span class="op">;</span> epoch<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">(</span>data<span class="op">);</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> <span class="op">(</span>theta <span class="op">-</span> x<span class="op">[</span>i<span class="op">]);</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-21" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-21">Why It Matters</h4>
<ul>
<li>Fast and scalable: one update per sample.</li>
<li>Online learning: can adapt continuously as new data arrives.</li>
<li>Noise helps escape local minima.</li>
<li>Foundation for deep learning: modern optimizers (Adam, RMSProp) are built on SGD.</li>
</ul>
<p>It’s the optimizer that lets neural networks train on massive datasets without waiting forever.</p>
</section>
<section id="a-gentle-proof-why-it-works-21" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-21">A Gentle Proof (Why It Works)</h4>
<p>If <span class="math inline">\(\mathbb{E}[\nabla_\theta \ell(x_i, y_i; \theta)] = \nabla_\theta L(\theta)\)</span>, then on average, SGD follows the true gradient:</p>
<p><span class="math display">\[
\mathbb{E}[\theta_{t+1}] = \theta_t - \eta , \nabla_\theta L(\theta_t)
\]</span></p>
<p>So even though each step is noisy, the expected direction is correct. With a decaying learning rate <span class="math inline">\(\eta_t\)</span>, SGD converges to the global minimum for convex functions.</p>
</section>
<section id="try-it-yourself-21" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-21">Try It Yourself</h4>
<ol type="1">
<li>Implement SGD for linear regression.</li>
<li>Compare convergence vs full-batch gradient descent.</li>
<li>Plot the noisy trajectory over a contour map of the loss.</li>
<li>Try different learning rates: <span class="math inline">\(\eta = 0.01\)</span>, <span class="math inline">\(0.1\)</span>, <span class="math inline">\(1.0\)</span>.</li>
<li>Add a mini-batch size (e.g.&nbsp;16 or 32) and observe smoothing.</li>
</ol>
</section>
<section id="test-cases-21" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-21">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Batch Size</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Small</td>
<td>1</td>
<td>High variance, quick updates</td>
</tr>
<tr class="even">
<td>Large</td>
<td>32</td>
<td>Smooth path, stable convergence</td>
</tr>
<tr class="odd">
<td>Non-convex</td>
<td>1</td>
<td>Escapes local minima</td>
</tr>
<tr class="even">
<td>Streaming</td>
<td>1</td>
<td>Online adaptation</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-21" class="level4">
<h4 class="anchored" data-anchor-id="complexity-21">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(1)\)</span></li>
<li>Per epoch: <span class="math inline">\(O(N)\)</span></li>
<li>Memory: <span class="math inline">\(O(d)\)</span></li>
<li>Convergence: depends on learning rate schedule and noise level</li>
</ul>
<p>Stochastic Gradient Descent trades precision for speed, learning by trial, noise, and many small steps, making it the beating heart of modern optimization.</p>
</section>
</section>
<section id="mini-batch-stochastic-gradient-descent-mini-batch-sgd" class="level3">
<h3 class="anchored" data-anchor-id="mini-batch-stochastic-gradient-descent-mini-batch-sgd">923. Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD)</h3>
<p>Mini-Batch SGD strikes a balance between full-batch gradient descent and purely stochastic updates. Instead of using all samples or just one, it updates parameters using small batches of data, combining efficiency, stability, and speed, making it the default choice in deep learning.</p>
<section id="what-problem-are-we-solving-22" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-22">What Problem Are We Solving?</h4>
<p>Full-batch gradient descent:</p>
<p><span class="math display">\[
\nabla_\theta L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \ell(x_i, y_i; \theta)
\]</span></p>
<p>is accurate but slow.</p>
<p>Stochastic gradient descent:</p>
<p><span class="math display">\[
\nabla_\theta L(\theta) \approx \nabla_\theta \ell(x_i, y_i; \theta)
\]</span></p>
<p>is fast but noisy.</p>
<p>Mini-batch SGD finds the middle ground:</p>
<p><span class="math display">\[
\nabla_\theta L(\theta) \approx \frac{1}{B} \sum_{i=1}^{B} \nabla_\theta \ell(x_i, y_i; \theta)
\]</span></p>
<p>where <span class="math inline">\(B\)</span> is the batch size (e.g.&nbsp;16, 32, 64). This approach uses parallel computation and smoother gradients.</p>
</section>
<section id="how-does-it-work-plain-language-22" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-22">How Does It Work (Plain Language)?</h4>
<p>Instead of looking at one data point (SGD) or all (full-batch), we take a handful, a mini-batch, to approximate the direction downhill. It’s like taking several opinions before deciding, rather than polling everyone or just one.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 30%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Shuffle dataset</td>
<td>Randomize order</td>
</tr>
<tr class="even">
<td>2</td>
<td>Split into batches</td>
<td>Each of size <span class="math inline">\(B\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Loop over batches</td>
<td>Compute gradient per batch</td>
</tr>
<tr class="even">
<td>4</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta \gets \theta - \eta , g_{\text{batch}}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat per epoch</td>
<td>Continue until convergence</td>
</tr>
</tbody>
</table>
<p>Each batch gives a gradient estimate with lower variance than SGD, and much faster than full-batch.</p>
</section>
<section id="example-10" class="level4">
<h4 class="anchored" data-anchor-id="example-10">Example</h4>
<p>Suppose <span class="math inline">\(N = 1000\)</span> samples, batch size <span class="math inline">\(B = 50\)</span>: Each epoch = <span class="math inline">\(1000 / 50 = 20\)</span> updates. Each update:</p>
<p><span class="math display">\[
g = \frac{1}{50} \sum_{i=1}^{50} \nabla_\theta \ell(x_i, y_i)
\]</span></p>
<p>and <span class="math display">\[
\theta \gets \theta - \eta g
\]</span></p>
<p>The model sees all data per epoch, but in chunks, efficient for vectorized computation (e.g.&nbsp;GPUs).</p>
</section>
<section id="tiny-code-easy-versions-20" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-20">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> minibatch_sgd(X, y, lr<span class="op">=</span><span class="fl">0.1</span>, batch_size<span class="op">=</span><span class="dv">32</span>, epochs<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.permutation(n)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n, batch_size):</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>            end <span class="op">=</span> start <span class="op">+</span> batch_size</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> indices[start:end]</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> np.mean(<span class="dv">2</span> <span class="op">*</span> (theta <span class="op">-</span> X[batch]))</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> epoch <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> epoch <span class="op">&lt;</span> epochs<span class="op">;</span> epoch<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">(</span>data<span class="op">);</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i <span class="op">+=</span> batch_size<span class="op">)</span> <span class="op">{</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> end <span class="op">=</span> min<span class="op">(</span>i <span class="op">+</span> batch_size<span class="op">,</span> n<span class="op">);</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> i<span class="op">;</span> j <span class="op">&lt;</span> end<span class="op">;</span> j<span class="op">++)</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">+=</span> <span class="dv">2</span> <span class="op">*</span> <span class="op">(</span>theta <span class="op">-</span> x<span class="op">[</span>j<span class="op">]);</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">/=</span> <span class="op">(</span>end <span class="op">-</span> i<span class="op">);</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-22" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-22">Why It Matters</h4>
<ul>
<li>Computationally efficient: leverages vectorization and parallelism.</li>
<li>Less noisy: gradient estimate is more accurate than single-sample SGD.</li>
<li>Faster convergence: smoother path to minimum.</li>
<li>GPU-friendly: batches fit hardware constraints.</li>
</ul>
<p>It’s the standard optimizer for training neural networks.</p>
</section>
<section id="a-gentle-proof-why-it-works-22" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-22">A Gentle Proof (Why It Works)</h4>
<p>If <span class="math inline">\(\mathbb{E}[g_B] = \nabla_\theta L(\theta)\)</span>, then batch gradient <span class="math inline">\(g_B\)</span> is an unbiased estimator of the true gradient.</p>
<p>Variance decreases as batch size <span class="math inline">\(B\)</span> increases:</p>
<p><span class="math display">\[
\text{Var}(g_B) = \frac{\sigma^2}{B}
\]</span></p>
<p>Hence, larger batches give smoother but costlier updates. Mini-batches find a sweet spot between cost and stability.</p>
</section>
<section id="try-it-yourself-22" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-22">Try It Yourself</h4>
<ol type="1">
<li>Train linear regression with batch sizes 1, 32, 128.</li>
<li>Plot loss vs iteration, see smoother curves for larger batches.</li>
<li>Observe compute time per epoch.</li>
<li>Use batch size = 32 (common default).</li>
<li>Experiment with decaying learning rate <span class="math inline">\(\eta_t\)</span>.</li>
</ol>
</section>
<section id="test-cases-22" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-22">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Batch Size</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>High noise, fast updates</td>
</tr>
<tr class="even">
<td>32</td>
<td>Balanced performance</td>
</tr>
<tr class="odd">
<td>512</td>
<td>Smooth path, more compute per step</td>
</tr>
<tr class="even">
<td>N (full batch)</td>
<td>Exact gradient, slow</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-22" class="level4">
<h4 class="anchored" data-anchor-id="complexity-22">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(B \cdot d)\)</span></li>
<li>Per epoch: <span class="math inline">\(O(N \cdot d)\)</span></li>
<li>Memory: <span class="math inline">\(O(B \cdot d)\)</span></li>
<li>Convergence: faster in practice due to variance reduction</li>
</ul>
<p>Mini-Batch SGD is the workhorse of modern optimization, efficient, smooth, and perfectly tuned for parallel computation.</p>
</section>
</section>
<section id="momentum" class="level3">
<h3 class="anchored" data-anchor-id="momentum">924. Momentum</h3>
<p>Momentum is a powerful enhancement to gradient descent that helps it move faster in the right direction and smooth out oscillations. It does this by adding a velocity term that remembers past updates, so the optimizer can accelerate down long slopes and resist noisy zigzags.</p>
<section id="what-problem-are-we-solving-23" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-23">What Problem Are We Solving?</h4>
<p>Plain gradient descent can get stuck or slow down when the surface curves differently along different directions, like a narrow valley.</p>
<p>It may oscillate across steep walls, inching forward slowly:</p>
<p><span class="math display">\[
\theta \leftarrow \theta - \eta , \nabla_\theta L(\theta)
\]</span></p>
<p>Momentum solves this by accumulating past gradients in a velocity term, allowing consistent movement in the dominant direction.</p>
</section>
<section id="the-update-rule" class="level4">
<h4 class="anchored" data-anchor-id="the-update-rule">The Update Rule</h4>
<p>Let <span class="math inline">\(v_t\)</span> be the velocity at step <span class="math inline">\(t\)</span>. Then:</p>
<p><span class="math display">\[
v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta_t)
\]</span></p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta v_t
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\beta\)</span> (0.8–0.99) controls how much of the past momentum to keep,</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate.</li>
</ul>
<p>This is equivalent to applying an exponential moving average to gradients.</p>
</section>
<section id="how-does-it-work-plain-language-23" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-23">How Does It Work (Plain Language)?</h4>
<p>Imagine rolling a ball down a hill. Without momentum, it stops at every bump. With momentum, it keeps rolling, carrying energy from previous steps, smoothing small oscillations and speeding up descent.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 42%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(v_0 = 0\)</span></td>
<td>Start parameters and velocity</td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute gradient <span class="math inline">\(g_t\)</span></td>
<td><span class="math inline">\(g_t = \nabla_\theta L(\theta_t)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update velocity</td>
<td><span class="math inline">\(v_t = \beta v_{t-1} + (1 - \beta) g_t\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta_{t+1} = \theta_t - \eta v_t\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="example-11" class="level4">
<h4 class="anchored" data-anchor-id="example-11">Example</h4>
<p>Suppose we’re minimizing <span class="math display">\[
L(\theta_1, \theta_2) = 100\theta_1^2 + \theta_2^2
\]</span></p>
<p>The surface is steep along <span class="math inline">\(\theta_1\)</span>, flat along <span class="math inline">\(\theta_2\)</span>. Without momentum: steps oscillate along <span class="math inline">\(\theta_1\)</span>. With momentum: velocity accumulates along <span class="math inline">\(\theta_2\)</span>, reducing zigzags and converging faster.</p>
</section>
<section id="tiny-code-easy-versions-21" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-21">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_momentum(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.1</span>, beta<span class="op">=</span><span class="fl">0.9</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> L_grad(theta)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> beta <span class="op">*</span> v <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta) <span class="op">*</span> grad</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> v</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> gradient_descent_momentum(L_grad, theta0<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> v <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">,</span> beta <span class="op">=</span> <span class="fl">0.9</span><span class="op">;</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> theta<span class="op">;</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> beta <span class="op">*</span> v <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta<span class="op">)</span> <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> v<span class="op">;</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-23" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-23">Why It Matters</h4>
<ul>
<li>Accelerates convergence on flat regions.</li>
<li>Reduces oscillation in steep valleys.</li>
<li>Simple and robust, works with any gradient-based optimizer.</li>
<li>Foundation for advanced methods (Nesterov, Adam, RMSProp).</li>
</ul>
<p>Momentum turns gradient descent from a cautious walker into a rolling ball, smooth, decisive, and fast.</p>
</section>
<section id="a-gentle-proof-why-it-works-23" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-23">A Gentle Proof (Why It Works)</h4>
<p>Momentum approximates a first-order low-pass filter over gradients:</p>
<p><span class="math display">\[
v_t \approx \sum_{k=0}^{t} (1 - \beta)\beta^{t-k} \nabla_\theta L(\theta_k)
\]</span></p>
<p>So the update direction becomes a weighted average of past gradients. This averaging suppresses high-frequency noise, stabilizing convergence.</p>
</section>
<section id="try-it-yourself-23" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-23">Try It Yourself</h4>
<ol type="1">
<li>Train gradient descent on <span class="math inline">\(L(\theta_1, \theta_2) = 100\theta_1^2 + \theta_2^2\)</span>.</li>
<li>Compare with and without momentum.</li>
<li>Try <span class="math inline">\(\beta = 0.8, 0.9, 0.99\)</span>, higher <span class="math inline">\(\beta\)</span> = smoother motion.</li>
<li>Plot trajectories on contour maps.</li>
<li>Experiment with large learning rate <span class="math inline">\(\eta\)</span>, momentum allows bigger steps.</li>
</ol>
</section>
<section id="test-cases-23" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-23">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Surface</th>
<th><span class="math inline">\(\beta\)</span></th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Flat</td>
<td>0.9</td>
<td>Faster acceleration</td>
</tr>
<tr class="even">
<td>Steep valley</td>
<td>0.9</td>
<td>Less oscillation</td>
</tr>
<tr class="odd">
<td>Noisy gradient</td>
<td>0.99</td>
<td>Smoother updates</td>
</tr>
<tr class="even">
<td>Convex</td>
<td>0.8</td>
<td>Converges steadily</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-23" class="level4">
<h4 class="anchored" data-anchor-id="complexity-23">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span></li>
<li>Memory: <span class="math inline">\(O(d)\)</span> (for velocity)</li>
<li>Convergence: faster than vanilla GD on ill-conditioned surfaces</li>
</ul>
<p>Momentum remembers where it’s been, letting the optimizer glide past bumps and speed through valleys toward the minimum.</p>
</section>
</section>
<section id="nesterov-accelerated-gradient-nag" class="level3">
<h3 class="anchored" data-anchor-id="nesterov-accelerated-gradient-nag">925. Nesterov Accelerated Gradient (NAG)</h3>
<p>Nesterov Accelerated Gradient (NAG) refines standard momentum by adding a “lookahead” step, it anticipates where the parameters are moving and computes the gradient after taking that step. This small change provides stronger convergence guarantees and better control near minima.</p>
<section id="what-problem-are-we-solving-24" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-24">What Problem Are We Solving?</h4>
<p>Regular momentum helps gradient descent gain speed, but it can overshoot when nearing the optimum. Nesterov fixes this by peeking ahead before applying the gradient, ensuring updates are guided by where the parameters are <em>going</em>, not where they <em>were</em>.</p>
<p>Momentum update: <span class="math display">\[
v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta L(\theta_t)
\]</span></p>
<p>Nesterov update: <span class="math display">\[
v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta L(\theta_t - \eta \beta v_{t-1})
\]</span></p>
<p>Then: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta v_t
\]</span></p>
<p>The gradient is evaluated at the lookahead point, giving earlier correction.</p>
</section>
<section id="how-does-it-work-plain-language-24" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-24">How Does It Work (Plain Language)?</h4>
<p>Think of running downhill:</p>
<ul>
<li>Momentum: you move based on your current velocity, then adjust after seeing where you end up.</li>
<li>Nesterov: you look ahead before stepping, adjusting your direction proactively.</li>
</ul>
<p>This anticipation helps prevent overshooting and makes convergence smoother.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 31%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute lookahead point</td>
<td><span class="math inline">\(\theta_{\text{look}} = \theta_t - \eta \beta v_{t-1}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute gradient</td>
<td><span class="math inline">\(g_t = \nabla_\theta L(\theta_{\text{look}})\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update velocity</td>
<td><span class="math inline">\(v_t = \beta v_{t-1} + (1 - \beta) g_t\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta_{t+1} = \theta_t - \eta v_t\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-12" class="level4">
<h4 class="anchored" data-anchor-id="example-12">Example</h4>
<p>Suppose <span class="math inline">\(L(\theta) = \theta^2\)</span>. We start with <span class="math inline">\(\theta_0 = 1.0\)</span>, <span class="math inline">\(\beta = 0.9\)</span>, <span class="math inline">\(\eta = 0.1\)</span>.</p>
<p>Each step:</p>
<ol type="1">
<li>Look ahead: <span class="math inline">\(\theta_{\text{look}} = \theta - 0.1 \times 0.9 v\)</span></li>
<li>Compute gradient at <span class="math inline">\(\theta_{\text{look}}\)</span></li>
<li>Update <span class="math inline">\(v\)</span>, then <span class="math inline">\(\theta\)</span></li>
</ol>
<p>This “peek” allows faster, more stable convergence than standard momentum.</p>
</section>
<section id="tiny-code-easy-versions-22" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-22">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nesterov(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.1</span>, beta<span class="op">=</span><span class="fl">0.9</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        lookahead <span class="op">=</span> theta <span class="op">-</span> lr <span class="op">*</span> beta <span class="op">*</span> v</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> L_grad(lookahead)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> beta <span class="op">*</span> v <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta) <span class="op">*</span> grad</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> v</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> nesterov(L_grad, theta0<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> v <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">,</span> beta <span class="op">=</span> <span class="fl">0.9</span><span class="op">;</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> lookahead <span class="op">=</span> theta <span class="op">-</span> lr <span class="op">*</span> beta <span class="op">*</span> v<span class="op">;</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> lookahead<span class="op">;</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> beta <span class="op">*</span> v <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta<span class="op">)</span> <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> v<span class="op">;</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-24" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-24">Why It Matters</h4>
<ul>
<li>Faster convergence on convex problems.</li>
<li>More stable near minima, corrects direction before stepping.</li>
<li>Widely used in deep learning (e.g., SGD with Nesterov).</li>
<li>Theoretically strong: provably optimal for convex optimization.</li>
</ul>
<p>In practice, NAG improves both speed and stability, often outperforming plain momentum.</p>
</section>
<section id="a-gentle-proof-why-it-works-24" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-24">A Gentle Proof (Why It Works)</h4>
<p>Nesterov’s method comes from convex optimization theory. For smooth convex functions:</p>
<p><span class="math display">\[
L(\theta_t) - L(\theta^*) \le O\left(\frac{1}{t^2}\right)
\]</span></p>
<p>compared to <span class="math inline">\(O\left(\frac{1}{t}\right)\)</span> for standard gradient descent. The key is anticipation: evaluating gradients at <span class="math inline">\(\theta_t - \eta \beta v_{t-1}\)</span> leads to earlier correction and tighter bounds.</p>
</section>
<section id="try-it-yourself-24" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-24">Try It Yourself</h4>
<ol type="1">
<li>Train on <span class="math inline">\(L(\theta) = \theta^2\)</span> with momentum vs NAG.</li>
<li>Plot convergence curves, NAG is faster and smoother.</li>
<li>Test <span class="math inline">\(\beta = 0.8, 0.9, 0.99\)</span>.</li>
<li>Try non-convex loss (e.g.&nbsp;multi-basin), note improved control.</li>
<li>Compare training speed on a small neural net.</li>
</ol>
</section>
<section id="test-cases-24" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-24">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Surface</th>
<th>Optimizer</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Convex quadratic</td>
<td>NAG</td>
<td>Faster than momentum</td>
</tr>
<tr class="even">
<td>Non-convex</td>
<td>NAG</td>
<td>More stable</td>
</tr>
<tr class="odd">
<td>Valley-shaped</td>
<td>NAG</td>
<td>Less oscillation</td>
</tr>
<tr class="even">
<td>High curvature</td>
<td>NAG</td>
<td>Controlled updates</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-24" class="level4">
<h4 class="anchored" data-anchor-id="complexity-24">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span></li>
<li>Memory: <span class="math inline">\(O(d)\)</span></li>
<li>Convergence: <span class="math inline">\(O(1/t^2)\)</span> (convex), faster in practice</li>
</ul>
<p>Nesterov Accelerated Gradient is momentum with foresight, a runner who glances ahead before stepping, reaching the finish line faster and more gracefully.</p>
</section>
</section>
<section id="adagrad-adaptive-gradient" class="level3">
<h3 class="anchored" data-anchor-id="adagrad-adaptive-gradient">926. AdaGrad (Adaptive Gradient)</h3>
<p>AdaGrad, short for <em>Adaptive Gradient</em>, automatically adjusts the learning rate for each parameter based on how frequently it’s been updated. Parameters with frequent large gradients get smaller steps, while rarely updated ones get larger steps, perfect for sparse features or imbalanced data.</p>
<section id="what-problem-are-we-solving-25" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-25">What Problem Are We Solving?</h4>
<p>Standard gradient descent uses a single global learning rate:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)
\]</span></p>
<p>But in real data:</p>
<ul>
<li>Some parameters update often (high gradient frequency),</li>
<li>Others rarely change (low frequency).</li>
</ul>
<p>A single learning rate can either overshoot some or underfit others. AdaGrad adapts <span class="math inline">\(\eta\)</span> individually per dimension.</p>
</section>
<section id="the-update-rule-1" class="level4">
<h4 class="anchored" data-anchor-id="the-update-rule-1">The Update Rule</h4>
<p>Each parameter <span class="math inline">\(\theta_i\)</span> maintains a cumulative sum of squared gradients:</p>
<p><span class="math display">\[
G_{t,i} = G_{t-1,i} + g_{t,i}^2
\]</span></p>
<p>Then update:</p>
<p><span class="math display">\[
\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_{t,i}} + \epsilon} , g_{t,i}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(g_{t,i}\)</span> is the gradient of parameter <span class="math inline">\(i\)</span> at step <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\epsilon\)</span> avoids division by zero (e.g.&nbsp;<span class="math inline">\(10^{-8}\)</span>),</li>
<li><span class="math inline">\(\eta\)</span> is the initial learning rate.</li>
</ul>
<p>So the effective learning rate shrinks over time:</p>
<p><span class="math display">\[
\eta_{t,i} = \frac{\eta}{\sqrt{G_{t,i}} + \epsilon}
\]</span></p>
</section>
<section id="how-does-it-work-plain-language-25" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-25">How Does It Work (Plain Language)?</h4>
<p>Think of AdaGrad as giving each parameter its own step size, inversely proportional to how often it’s been updated.</p>
<ul>
<li>Frequently changing parameters take smaller steps.</li>
<li>Rare ones take bigger steps to catch up.</li>
</ul>
<p>This is especially useful in NLP, recommenders, and sparse vectors (e.g.&nbsp;word embeddings).</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 38%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(\theta\)</span>, <span class="math inline">\(G=0\)</span></td>
<td>Start parameters and accumulators</td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute gradient <span class="math inline">\(g\)</span></td>
<td><span class="math inline">\(\nabla_\theta L(\theta_t)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update accumulator</td>
<td><span class="math inline">\(G \gets G + g \odot g\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Scale learning rate</td>
<td><span class="math inline">\(\eta_t = \eta / \sqrt{G + \epsilon}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta \gets \theta - \eta_t \odot g\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-13" class="level4">
<h4 class="anchored" data-anchor-id="example-13">Example</h4>
<p>Suppose we optimize <span class="math inline">\(L(\theta_1, \theta_2)\)</span>. Gradients over time:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th><span class="math inline">\(g_1\)</span></th>
<th><span class="math inline">\(g_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1.0</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.0</td>
<td>0.1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.0</td>
<td>0.1</td>
</tr>
</tbody>
</table>
<p>Then <span class="math inline">\(G_1\)</span> grows faster than <span class="math inline">\(G_2\)</span>:</p>
<ul>
<li><span class="math inline">\(\eta_1\)</span> shrinks more, slowing updates,</li>
<li><span class="math inline">\(\eta_2\)</span> stays larger, adapting slower-moving parameter.</li>
</ul>
</section>
<section id="tiny-code-easy-versions-23" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-23">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adagrad(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.1</span>, eps<span class="op">=</span><span class="fl">1e-8</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> L_grad(theta)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        G <span class="op">+=</span> grad  <span class="dv">2</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> grad <span class="op">/</span> (np.sqrt(G) <span class="op">+</span> eps)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> adagrad(L_grad, np.array([<span class="fl">1.0</span>]))</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> G <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">,</span> eps <span class="op">=</span> <span class="fl">1e-8</span><span class="op">;</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> theta<span class="op">;</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    G <span class="op">+=</span> grad <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> grad <span class="op">/</span> <span class="op">(</span>sqrt<span class="op">(</span>G<span class="op">)</span> <span class="op">+</span> eps<span class="op">);</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-25" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-25">Why It Matters</h4>
<ul>
<li>Adaptive step sizes: no manual tuning per parameter.</li>
<li>Works well with sparse features.</li>
<li>Monotonic learning rate decay: stabilizes convergence.</li>
<li>Foundation for RMSProp and Adam.</li>
</ul>
<p>AdaGrad was one of the first adaptive optimizers, paving the way for modern deep learning methods.</p>
</section>
<section id="a-gentle-proof-why-it-works-25" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-25">A Gentle Proof (Why It Works)</h4>
<p>AdaGrad performs a diagonal preconditioning of the gradient:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta G_t^{-1/2} g_t
\]</span></p>
<p>Here <span class="math inline">\(G_t^{-1/2}\)</span> rescales each dimension based on historical magnitude. This gives an effective second-order correction, similar in spirit to Newton’s method, but simpler.</p>
</section>
<section id="try-it-yourself-25" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-25">Try It Yourself</h4>
<ol type="1">
<li>Optimize <span class="math inline">\(L(\theta_1, \theta_2) = \theta_1^2 + 100\theta_2^2\)</span>.</li>
<li>Compare vanilla GD vs AdaGrad.</li>
<li>Observe smoother path and adaptive speed.</li>
<li>Try on sparse feature vector (many zeros).</li>
<li>Track <span class="math inline">\(\eta_t\)</span>, see how it decays.</li>
</ol>
</section>
<section id="test-cases-25" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-25">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Surface</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sparse</td>
<td>Fast convergence</td>
</tr>
<tr class="even">
<td>Dense</td>
<td>Step size decays too quickly</td>
</tr>
<tr class="odd">
<td>Long training</td>
<td>Stops early</td>
</tr>
<tr class="even">
<td>NLP embeddings</td>
<td>Very effective</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-25" class="level4">
<h4 class="anchored" data-anchor-id="complexity-25">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span></li>
<li>Memory: <span class="math inline">\(O(d)\)</span> (accumulator <span class="math inline">\(G\)</span>)</li>
<li>Convergence: smooth but may stagnate (too small <span class="math inline">\(\eta_t\)</span>)</li>
</ul>
<p>AdaGrad learns how to learn, scaling each dimension individually, letting rarely updated features speak louder and frequent ones quiet down.</p>
</section>
</section>
<section id="rmsprop-root-mean-square-propagation" class="level3">
<h3 class="anchored" data-anchor-id="rmsprop-root-mean-square-propagation">927. RMSProp (Root Mean Square Propagation)</h3>
<p>RMSProp improves upon AdaGrad by preventing its learning rate from decaying too quickly. It keeps an exponentially weighted moving average of squared gradients instead of summing them forever, maintaining adaptability while avoiding stagnation.</p>
<section id="what-problem-are-we-solving-26" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-26">What Problem Are We Solving?</h4>
<p>In AdaGrad, the denominator</p>
<p><span class="math display">\[
\sqrt{G_t} = \sqrt{\sum_{i=1}^t g_i^2}
\]</span></p>
<p>keeps growing, so the effective learning rate</p>
<p><span class="math display">\[
\eta_t = \frac{\eta}{\sqrt{G_t} + \epsilon}
\]</span></p>
<p>shrinks too much over time. Training may stop early, especially in non-convex settings like deep neural networks.</p>
<p>RMSProp fixes this by using a decaying average of squared gradients, balancing adaptivity and persistence.</p>
</section>
<section id="the-update-rule-2" class="level4">
<h4 class="anchored" data-anchor-id="the-update-rule-2">The Update Rule</h4>
<p>Maintain an exponential moving average of squared gradients:</p>
<p><span class="math display">\[
E[g^2]*t = \beta E[g^2]*{t-1} + (1 - \beta) g_t^2
\]</span></p>
<p>Then update parameters:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t} + \epsilon} , g_t
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\beta\)</span> is the decay rate (typically 0.9),</li>
<li><span class="math inline">\(\epsilon\)</span> (e.g.&nbsp;<span class="math inline">\(10^{-8}\)</span>) prevents division by zero,</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate (often <span class="math inline">\(10^{-3}\)</span> or <span class="math inline">\(10^{-4}\)</span>).</li>
</ul>
</section>
<section id="how-does-it-work-plain-language-26" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-26">How Does It Work (Plain Language)?</h4>
<p>RMSProp tracks the recent average magnitude of gradients. If gradients are large in one direction, it shrinks the learning rate there; if small, it increases it. Unlike AdaGrad, it forgets old gradients, keeping training adaptive throughout.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 36%">
<col style="width: 59%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(\theta\)</span>, <span class="math inline">\(E[g^2]=0\)</span></td>
<td>Parameters + running average</td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute gradient <span class="math inline">\(g_t\)</span></td>
<td><span class="math inline">\(\nabla_\theta L(\theta_t)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update average</td>
<td><span class="math inline">\(E[g^2]*t = \beta E[g^2]*{t-1} + (1 - \beta) g_t^2\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Scale learning rate</td>
<td><span class="math inline">\(\eta_t = \eta / \sqrt{E[g^2]_t + \epsilon}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta \gets \theta - \eta_t g_t\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-14" class="level4">
<h4 class="anchored" data-anchor-id="example-14">Example</h4>
<p>Suppose gradients oscillate:</p>
<ul>
<li>Step 1: <span class="math inline">\(g = 1.0\)</span></li>
<li>Step 2: <span class="math inline">\(g = 0.2\)</span></li>
<li>Step 3: <span class="math inline">\(g = 0.5\)</span></li>
</ul>
<p>Instead of summing (<span class="math inline">\(1.0^2 + 0.2^2 + 0.5^2\)</span>), RMSProp uses:</p>
<p><span class="math display">\[
E[g^2]*t = 0.9 E[g^2]*{t-1} + 0.1 g_t^2
\]</span></p>
<p>which emphasizes recent gradients, letting learning rates adapt continuously.</p>
</section>
<section id="tiny-code-easy-versions-24" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-24">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rmsprop(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.01</span>, beta<span class="op">=</span><span class="fl">0.9</span>, eps<span class="op">=</span><span class="fl">1e-8</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    Eg2 <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> L_grad(theta)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        Eg2 <span class="op">=</span> beta <span class="op">*</span> Eg2 <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta) <span class="op">*</span> grad  <span class="dv">2</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> grad <span class="op">/</span> (np.sqrt(Eg2) <span class="op">+</span> eps)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> rmsprop(L_grad, np.array([<span class="fl">1.0</span>]))</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> Eg2 <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> lr <span class="op">=</span> <span class="fl">0.01</span><span class="op">,</span> beta <span class="op">=</span> <span class="fl">0.9</span><span class="op">,</span> eps <span class="op">=</span> <span class="fl">1e-8</span><span class="op">;</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> grad <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> theta<span class="op">;</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    Eg2 <span class="op">=</span> beta <span class="op">*</span> Eg2 <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta<span class="op">)</span> <span class="op">*</span> grad <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> grad <span class="op">/</span> <span class="op">(</span>sqrt<span class="op">(</span>Eg2<span class="op">)</span> <span class="op">+</span> eps<span class="op">);</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-26" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-26">Why It Matters</h4>
<ul>
<li>Adaptive learning rates across dimensions.</li>
<li>No premature decay, unlike AdaGrad.</li>
<li>Stable training for deep networks (especially RNNs).</li>
<li>Default optimizer in many early deep learning frameworks (e.g.&nbsp;TensorFlow’s default before Adam).</li>
</ul>
<p>It’s like a thermostat for learning rates, adjusting constantly based on recent temperature (gradient energy).</p>
</section>
<section id="a-gentle-proof-why-it-works-26" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-26">A Gentle Proof (Why It Works)</h4>
<p>RMSProp approximates second-order curvature using moving averages:</p>
<p><span class="math display">\[
\mathbb{E}[g^2]_t \approx \text{diag}(H)
\]</span></p>
<p>so step size per dimension becomes:</p>
<p><span class="math display">\[
\Delta \theta_i \propto \frac{1}{\sqrt{\mathbb{E}[g_i^2]_t}}
\]</span></p>
<p>acting as a preconditioner, stabilizing updates along steep or flat directions.</p>
</section>
<section id="try-it-yourself-26" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-26">Try It Yourself</h4>
<ol type="1">
<li>Train <span class="math inline">\(L(\theta) = 100\theta_1^2 + \theta_2^2\)</span>.</li>
<li>Compare AdaGrad vs RMSProp.</li>
<li>Plot learning rates over time, RMSProp stays active longer.</li>
<li>Try <span class="math inline">\(\beta = 0.9, 0.99\)</span>.</li>
<li>Observe smoother convergence on non-stationary gradients.</li>
</ol>
</section>
<section id="test-cases-26" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-26">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Surface</th>
<th>Optimizer</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Steep valley</td>
<td>RMSProp</td>
<td>Smooth convergence</td>
</tr>
<tr class="even">
<td>Sparse data</td>
<td>RMSProp</td>
<td>Adapts per dimension</td>
</tr>
<tr class="odd">
<td>Long training</td>
<td>RMSProp</td>
<td>Keeps learning</td>
</tr>
<tr class="even">
<td>Non-stationary</td>
<td>RMSProp</td>
<td>Stable updates</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-26" class="level4">
<h4 class="anchored" data-anchor-id="complexity-26">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span></li>
<li>Memory: <span class="math inline">\(O(d)\)</span> (running average)</li>
<li>Convergence: faster, more stable than AdaGrad in deep nets</li>
</ul>
<p>RMSProp is AdaGrad with a memory, adaptive, forgetful, and tuned for the dynamic landscapes of deep learning.</p>
</section>
</section>
<section id="adam-adaptive-moment-estimation" class="level3">
<h3 class="anchored" data-anchor-id="adam-adaptive-moment-estimation">928. Adam (Adaptive Moment Estimation)</h3>
<p>Adam, short for <em>Adaptive Moment Estimation</em>, combines the strengths of Momentum and RMSProp. It keeps track of both the mean (first moment) and variance (second moment) of gradients to provide adaptive learning rates and smooth, stable convergence, making it the most widely used optimizer in deep learning.</p>
<section id="what-problem-are-we-solving-27" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-27">What Problem Are We Solving?</h4>
<p>SGD may oscillate or require careful tuning of the learning rate. Momentum accelerates convergence but lacks adaptivity. RMSProp adapts learning rates but ignores direction history.</p>
<p>Adam merges both ideas, maintaining:</p>
<ul>
<li>Velocity (first moment): exponential moving average of gradients.</li>
<li>Scale (second moment): exponential moving average of squared gradients.</li>
</ul>
<p>Together, these stabilize updates and adapt per-parameter step sizes.</p>
</section>
<section id="the-update-rule-3" class="level4">
<h4 class="anchored" data-anchor-id="the-update-rule-3">The Update Rule</h4>
<p>At each step <span class="math inline">\(t\)</span>:</p>
<ol type="1">
<li><p>Compute gradient: <span class="math display">\[
g_t = \nabla_\theta L(\theta_t)
\]</span></p></li>
<li><p>Update biased first moment (mean): <span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]</span></p></li>
<li><p>Update biased second moment (variance): <span class="math display">\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]</span></p></li>
<li><p>Bias correction: <span class="math display">\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\]</span></p></li>
<li><p>Parameter update: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta , \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\]</span></p></li>
</ol>
<p>Typical hyperparameters:</p>
<ul>
<li><span class="math inline">\(\eta = 10^{-3}\)</span></li>
<li><span class="math inline">\(\beta_1 = 0.9\)</span></li>
<li><span class="math inline">\(\beta_2 = 0.999\)</span></li>
<li><span class="math inline">\(\epsilon = 10^{-8}\)</span></li>
</ul>
</section>
<section id="how-does-it-work-plain-language-27" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-27">How Does It Work (Plain Language)?</h4>
<p>Adam blends momentum’s direction memory with RMSProp’s adaptive scaling.</p>
<p>Think of it as a smart autopilot:</p>
<ul>
<li>The first moment (like velocity) keeps moving consistently downhill.</li>
<li>The second moment (like shock absorbers) adjusts step size to terrain roughness.</li>
</ul>
<p>Together, they make learning fast, smooth, and robust to noise.</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 20%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute <span class="math inline">\(g_t\)</span></td>
<td>Gradient at current step</td>
</tr>
<tr class="even">
<td>2</td>
<td>Update <span class="math inline">\(m_t\)</span>, <span class="math inline">\(v_t\)</span></td>
<td>Moving averages of gradient and square</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Correct bias</td>
<td>Adjust for initialization bias</td>
</tr>
<tr class="even">
<td>4</td>
<td>Compute update</td>
<td>Scale gradient by <span class="math inline">\(\sqrt{v_t}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Apply step</td>
<td><span class="math inline">\(\theta \gets \theta - \eta \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-15" class="level4">
<h4 class="anchored" data-anchor-id="example-15">Example</h4>
<p>Let <span class="math inline">\(L(\theta) = \theta^2\)</span>. Starting at <span class="math inline">\(\theta_0 = 1.0\)</span>:</p>
<ul>
<li><span class="math inline">\(m_t\)</span> smooths gradient direction,</li>
<li><span class="math inline">\(v_t\)</span> scales by recent gradient energy,</li>
<li>Learning rate adapts per step, larger early, smaller later.</li>
</ul>
</section>
<section id="tiny-code-easy-versions-25" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-25">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.001</span>, beta1<span class="op">=</span><span class="fl">0.9</span>, beta2<span class="op">=</span><span class="fl">0.999</span>, eps<span class="op">=</span><span class="fl">1e-8</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, steps <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> L_grad(theta)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> beta1 <span class="op">*</span> m <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> g</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> beta2 <span class="op">*</span> v <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> (g  <span class="dv">2</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>        m_hat <span class="op">=</span> m <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta1  t)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>        v_hat <span class="op">=</span> v <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta2  t)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> m_hat <span class="op">/</span> (np.sqrt(v_hat) <span class="op">+</span> eps)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> adam(L_grad, np.array([<span class="fl">1.0</span>]))</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> m <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> v <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> lr <span class="op">=</span> <span class="fl">0.001</span><span class="op">,</span> beta1 <span class="op">=</span> <span class="fl">0.9</span><span class="op">,</span> beta2 <span class="op">=</span> <span class="fl">0.999</span><span class="op">,</span> eps <span class="op">=</span> <span class="fl">1e-8</span><span class="op">;</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> t <span class="op">&lt;=</span> <span class="dv">100</span><span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> g <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> theta<span class="op">;</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> beta1 <span class="op">*</span> m <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta1<span class="op">)</span> <span class="op">*</span> g<span class="op">;</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> beta2 <span class="op">*</span> v <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta2<span class="op">)</span> <span class="op">*</span> g <span class="op">*</span> g<span class="op">;</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> m_hat <span class="op">=</span> m <span class="op">/</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> pow<span class="op">(</span>beta1<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> v_hat <span class="op">=</span> v <span class="op">/</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> pow<span class="op">(</span>beta2<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> m_hat <span class="op">/</span> <span class="op">(</span>sqrt<span class="op">(</span>v_hat<span class="op">)</span> <span class="op">+</span> eps<span class="op">);</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-27" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-27">Why It Matters</h4>
<ul>
<li>Adaptive learning rates per parameter.</li>
<li>Momentum for speed, RMS scaling for stability.</li>
<li>Little hyperparameter tuning needed.</li>
<li>Works well in deep networks, especially with noisy gradients.</li>
</ul>
<p>Adam is the default optimizer for most modern deep learning models, from CNNs to Transformers.</p>
</section>
<section id="a-gentle-proof-why-it-works-27" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-27">A Gentle Proof (Why It Works)</h4>
<p>Bias correction ensures unbiased estimates of first and second moments:</p>
<p><span class="math display">\[
\mathbb{E}[m_t] = \nabla_\theta L(\theta_t), \quad \mathbb{E}[v_t] = \mathbb{E}[g_t^2]
\]</span></p>
<p>Using these, Adam approximates a second-order preconditioner:</p>
<p><span class="math display">\[
\Delta \theta \propto \frac{m_t}{\sqrt{v_t}}
\]</span></p>
<p>which stabilizes steps in ill-conditioned landscapes, giving faster and safer convergence.</p>
</section>
<section id="try-it-yourself-27" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-27">Try It Yourself</h4>
<ol type="1">
<li>Train a neural net with Adam vs SGD.</li>
<li>Observe faster convergence.</li>
<li>Try <span class="math inline">\(\beta_1 = 0.9\)</span>, <span class="math inline">\(\beta_2 = 0.999\)</span>, <span class="math inline">\(\eta = 10^{-3}\)</span>.</li>
<li>Experiment with decoupled weight decay (AdamW).</li>
<li>Plot learning curve vs iterations.</li>
</ol>
</section>
<section id="test-cases-27" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-27">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Optimizer</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deep net</td>
<td>Adam</td>
<td>Fast, smooth convergence</td>
</tr>
<tr class="even">
<td>Sparse features</td>
<td>Adam</td>
<td>Stable per-parameter steps</td>
</tr>
<tr class="odd">
<td>Noisy gradients</td>
<td>Adam</td>
<td>Robust updates</td>
</tr>
<tr class="even">
<td>Flat minima</td>
<td>Adam</td>
<td>Settles gracefully</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-27" class="level4">
<h4 class="anchored" data-anchor-id="complexity-27">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span></li>
<li>Memory: <span class="math inline">\(O(2d)\)</span> (for <span class="math inline">\(m\)</span>, <span class="math inline">\(v\)</span>)</li>
<li>Convergence: fast, stable, widely applicable</li>
</ul>
<p>Adam is the “automatic transmission” of optimization, combining speed, control, and adaptability for modern machine learning.</p>
</section>
</section>
<section id="adamw-adam-with-decoupled-weight-decay" class="level3">
<h3 class="anchored" data-anchor-id="adamw-adam-with-decoupled-weight-decay">929. AdamW (Adam with Decoupled Weight Decay)</h3>
<p>AdamW is a refined version of Adam that fixes a subtle but important issue: the way weight decay (L2 regularization) is applied. In standard Adam, L2 penalty interacts incorrectly with the adaptive learning rate. AdamW decouples weight decay from the gradient update, yielding more accurate regularization and better generalization.</p>
<section id="what-problem-are-we-solving-28" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-28">What Problem Are We Solving?</h4>
<p>In classical gradient descent, L2 regularization (weight decay) is applied as:</p>
<p><span class="math display">\[
\theta \leftarrow \theta - \eta , (\nabla_\theta L + \lambda \theta)
\]</span></p>
<p>But in Adam, gradients are rescaled by <span class="math inline">\(\sqrt{v_t}\)</span>, so adding <span class="math inline">\(\lambda \theta\)</span> inside the gradient term causes the penalty to scale unevenly per parameter, not true weight decay.</p>
<p>AdamW solves this by decoupling weight decay from gradient scaling.</p>
</section>
<section id="the-update-rule-4" class="level4">
<h4 class="anchored" data-anchor-id="the-update-rule-4">The Update Rule</h4>
<p>Same as Adam, but with separate weight decay:</p>
<ol type="1">
<li><p>Gradient moments: <span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]</span> <span class="math display">\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]</span></p></li>
<li><p>Bias correction: <span class="math display">\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\]</span></p></li>
<li><p>Parameter update with decoupled decay: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right)
\]</span></p></li>
</ol>
<p>where <span class="math inline">\(\lambda\)</span> is the weight decay coefficient.</p>
</section>
<section id="how-does-it-work-plain-language-28" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-28">How Does It Work (Plain Language)?</h4>
<p>In Adam, L2 regularization is mistakenly scaled by adaptive learning rates. In AdamW, weight decay is applied directly to parameters, just like in SGD.</p>
<p>This ensures a consistent shrinkage per step, independent of the gradient magnitude.</p>
<p>Think of it as:</p>
<ul>
<li>Adam: “rescaled” penalty → irregular regularization.</li>
<li>AdamW: “pure” decay → consistent regularization strength.</li>
</ul>
</section>
<section id="step-by-step-summary-4" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-4">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 22%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute gradient <span class="math inline">\(g_t\)</span></td>
<td><span class="math inline">\(\nabla_\theta L(\theta_t)\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Update moments</td>
<td><span class="math inline">\(m_t\)</span>, <span class="math inline">\(v_t\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Apply bias correction</td>
<td><span class="math inline">\(\hat{m}_t\)</span>, <span class="math inline">\(\hat{v}_t\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update weights</td>
<td><span class="math inline">\(\theta \gets \theta - \eta , \hat{m}_t / \sqrt{\hat{v}_t + \epsilon}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Apply decay</td>
<td><span class="math inline">\(\theta \gets \theta - \eta \lambda \theta\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-16" class="level4">
<h4 class="anchored" data-anchor-id="example-16">Example</h4>
<p>For a parameter <span class="math inline">\(\theta = 1.0\)</span>, let <span class="math inline">\(\eta = 0.01\)</span>, <span class="math inline">\(\lambda = 0.1\)</span>.</p>
<p>The decay step is simple:</p>
<p><span class="math display">\[
\theta \gets \theta - \eta \lambda \theta = \theta(1 - 0.001)
\]</span></p>
<p>This decay happens independently of gradient magnitude, giving clean regularization.</p>
</section>
<section id="tiny-code-easy-versions-26" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-26">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adamw(L_grad, theta0, lr<span class="op">=</span><span class="fl">0.001</span>, beta1<span class="op">=</span><span class="fl">0.9</span>, beta2<span class="op">=</span><span class="fl">0.999</span>, wd<span class="op">=</span><span class="fl">0.01</span>, eps<span class="op">=</span><span class="fl">1e-8</span>, steps<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta0</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, steps <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> L_grad(theta)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> beta1 <span class="op">*</span> m <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> g</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> beta2 <span class="op">*</span> v <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> (g  <span class="dv">2</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>        m_hat <span class="op">=</span> m <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta1  t)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>        v_hat <span class="op">=</span> v <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta2  t)</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> lr <span class="op">*</span> (m_hat <span class="op">/</span> (np.sqrt(v_hat) <span class="op">+</span> eps) <span class="op">+</span> wd <span class="op">*</span> theta)</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: minimize L(theta) = theta^2</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>L_grad <span class="op">=</span> <span class="kw">lambda</span> t: <span class="dv">2</span> <span class="op">*</span> t</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>theta_opt <span class="op">=</span> adamw(L_grad, np.array([<span class="fl">1.0</span>]))</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> theta <span class="op">=</span> <span class="fl">1.0</span><span class="op">,</span> m <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> v <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> lr <span class="op">=</span> <span class="fl">0.001</span><span class="op">,</span> beta1 <span class="op">=</span> <span class="fl">0.9</span><span class="op">,</span> beta2 <span class="op">=</span> <span class="fl">0.999</span><span class="op">,</span> wd <span class="op">=</span> <span class="fl">0.01</span><span class="op">,</span> eps <span class="op">=</span> <span class="fl">1e-8</span><span class="op">;</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> t <span class="op">&lt;=</span> <span class="dv">100</span><span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> g <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> theta<span class="op">;</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> beta1 <span class="op">*</span> m <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta1<span class="op">)</span> <span class="op">*</span> g<span class="op">;</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> beta2 <span class="op">*</span> v <span class="op">+</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> beta2<span class="op">)</span> <span class="op">*</span> g <span class="op">*</span> g<span class="op">;</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> m_hat <span class="op">=</span> m <span class="op">/</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> pow<span class="op">(</span>beta1<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> v_hat <span class="op">=</span> v <span class="op">/</span> <span class="op">(</span><span class="dv">1</span> <span class="op">-</span> pow<span class="op">(</span>beta2<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> lr <span class="op">*</span> <span class="op">(</span>m_hat <span class="op">/</span> <span class="op">(</span>sqrt<span class="op">(</span>v_hat<span class="op">)</span> <span class="op">+</span> eps<span class="op">)</span> <span class="op">+</span> wd <span class="op">*</span> theta<span class="op">);</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Optimal θ: </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> theta<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-28" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-28">Why It Matters</h4>
<ul>
<li>True weight decay: consistent L2 penalty.</li>
<li>Better generalization: especially in deep nets.</li>
<li>Fixes Adam’s bias toward larger weights.</li>
<li>Default in PyTorch (<code>torch.optim.AdamW</code>) and Transformers.</li>
</ul>
<p>AdamW is a must-have for training large neural networks, like BERT and GPT.</p>
</section>
<section id="a-gentle-proof-why-it-works-28" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-28">A Gentle Proof (Why It Works)</h4>
<p>In Adam, L2 penalty gets scaled by <span class="math inline">\(\frac{1}{\sqrt{v_t}}\)</span>, which breaks theoretical equivalence to weight decay.</p>
<p>By decoupling:</p>
<p><span class="math display">\[
\text{Update: } \theta \gets \theta - \eta \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}
\]</span></p>
<p><span class="math display">\[
\text{Decay: } \theta \gets (1 - \eta \lambda)\theta
\]</span></p>
<p>the regularization term acts linearly and consistently, independent of gradient statistics.</p>
</section>
<section id="try-it-yourself-28" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-28">Try It Yourself</h4>
<ol type="1">
<li>Train a small CNN with Adam vs AdamW.</li>
<li>Compare validation loss and weight norms.</li>
<li>Try <span class="math inline">\(\lambda = 0.01, 0.1, 0.001\)</span>.</li>
<li>Observe smoother convergence and better test performance.</li>
<li>Track norm of <span class="math inline">\(\theta\)</span>, AdamW keeps it controlled.</li>
</ol>
</section>
<section id="test-cases-28" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-28">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Optimizer</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MLP</td>
<td>Adam</td>
<td>Overfits (weights grow)</td>
</tr>
<tr class="even">
<td>MLP</td>
<td>AdamW</td>
<td>Better generalization</td>
</tr>
<tr class="odd">
<td>Transformer</td>
<td>AdamW</td>
<td>Stable training</td>
</tr>
<tr class="even">
<td>CNN</td>
<td>AdamW</td>
<td>Controlled weight growth</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-28" class="level4">
<h4 class="anchored" data-anchor-id="complexity-28">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(d)\)</span></li>
<li>Memory: <span class="math inline">\(O(2d)\)</span></li>
<li>Convergence: same as Adam, but better generalization</li>
</ul>
<p>AdamW keeps Adam’s adaptive brilliance but restores proper regularization, a simple fix with profound impact on modern deep learning.</p>
</section>
</section>
<section id="l-bfgs-limited-memory-broydenfletchergoldfarbshanno" class="level3">
<h3 class="anchored" data-anchor-id="l-bfgs-limited-memory-broydenfletchergoldfarbshanno">930. L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)</h3>
<p>L-BFGS is a powerful quasi-Newton optimization method that approximates second-order curvature (the Hessian) using only a small amount of memory. It achieves faster convergence than first-order methods like SGD by modeling the local shape of the loss, without explicitly computing the Hessian.</p>
<section id="what-problem-are-we-solving-29" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-29">What Problem Are We Solving?</h4>
<p>In optimization, Newton’s Method updates parameters using the inverse Hessian:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - H_t^{-1} \nabla_\theta L(\theta_t)
\]</span></p>
<p>But for high-dimensional problems, storing and inverting <span class="math inline">\(H_t\)</span> (size <span class="math inline">\(d \times d\)</span>) is too costly, <span class="math inline">\(O(d^2)\)</span> memory and <span class="math inline">\(O(d^3)\)</span> compute.</p>
<p>L-BFGS (Limited-memory BFGS) solves this by:</p>
<ul>
<li>Never storing the full Hessian.</li>
<li>Maintaining a rolling history of gradient and parameter changes to approximate <span class="math inline">\(H_t^{-1}\)</span> efficiently.</li>
</ul>
</section>
<section id="the-update-rule-conceptual-form" class="level4">
<h4 class="anchored" data-anchor-id="the-update-rule-conceptual-form">The Update Rule (Conceptual Form)</h4>
<p>Given gradients <span class="math inline">\(g_t\)</span> and parameter steps <span class="math inline">\(s_t = \theta_{t+1} - \theta_t\)</span>:</p>
<ol type="1">
<li><p>Compute gradient change: <span class="math display">\[
y_t = g_{t+1} - g_t
\]</span></p></li>
<li><p>Update Hessian inverse estimate <span class="math inline">\(H_{t+1}\)</span> using rank-2 correction (BFGS rule):</p>
<p><span class="math display">\[
H_{t+1} = (I - \rho_t s_t y_t^\top) H_t (I - \rho_t y_t s_t^\top) + \rho_t s_t s_t^\top
\]</span></p>
<p>where <span class="math inline">\(\rho_t = \frac{1}{y_t^\top s_t}\)</span>.</p></li>
<li><p>Use <span class="math inline">\(H_{t+1}\)</span> to compute the next update direction: <span class="math display">\[
p_t = -H_{t+1} g_{t+1}
\]</span></p></li>
</ol>
<p>L-BFGS keeps only a few <span class="math inline">\((s_t, y_t)\)</span> pairs (e.g.&nbsp;last 10) to reduce memory from <span class="math inline">\(O(d^2)\)</span> to <span class="math inline">\(O(md)\)</span>.</p>
</section>
<section id="how-does-it-work-plain-language-29" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-29">How Does It Work (Plain Language)?</h4>
<p>Think of L-BFGS as a smart gradient descent:</p>
<ul>
<li>It “remembers” how the gradient changes as you move.</li>
<li>From those changes, it infers the curvature (how steep or flat), like building a local map.</li>
<li>It then adjusts step sizes per direction for faster progress.</li>
</ul>
<p>You don’t need the full Hessian, just the history of steps.</p>
</section>
<section id="step-by-step-summary-5" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-5">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 27%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute gradient <span class="math inline">\(g_t\)</span></td>
<td>Evaluate at current <span class="math inline">\(\theta_t\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Determine search direction</td>
<td><span class="math inline">\(p_t = -H_t g_t\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Line search</td>
<td>Find step size <span class="math inline">\(\alpha_t\)</span> minimizing <span class="math inline">\(L(\theta_t + \alpha_t p_t)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta_{t+1} = \theta_t + \alpha_t p_t\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Store history</td>
<td><span class="math inline">\((s_t, y_t)\)</span> pairs for next iteration</td>
</tr>
<tr class="even">
<td>6</td>
<td>Update inverse Hessian</td>
<td>Using limited-memory formula</td>
</tr>
</tbody>
</table>
</section>
<section id="example-17" class="level4">
<h4 class="anchored" data-anchor-id="example-17">Example</h4>
<p>Suppose you minimize <span class="math inline">\(L(\theta) = \theta_1^2 + 10 \theta_2^2\)</span> (an elongated bowl). Vanilla gradient descent zigzags along the steep axis, converging slowly. L-BFGS estimates curvature and preconditions the gradient, taking direct diagonal paths toward the minimum.</p>
</section>
<section id="tiny-code-easy-versions-27" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-27">Tiny Code (Easy Versions)</h4>
<p>Python (using SciPy)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> fmin_l_bfgs_b</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(theta):</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(theta  <span class="dv">2</span>), <span class="dv">2</span> <span class="op">*</span> theta  <span class="co"># return (loss, gradient)</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>theta0 <span class="op">=</span> np.array([<span class="fl">5.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>theta_opt, f_min, info <span class="op">=</span> fmin_l_bfgs_b(loss, theta0)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Optimal θ:"</span>, theta_opt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Pseudocode only: implementing L-BFGS manually requires line search and history buffers</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Initialize theta, grad</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Maintain arrays for s[i] = delta_theta, y[i] = delta_grad (limited m history)</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Two-loop recursion to compute search direction</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Perform line search to find optimal step size</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">// 5. Update theta, store new s, y</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-29" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-29">Why It Matters</h4>
<ul>
<li>Faster convergence than first-order methods.</li>
<li>No Hessian needed, uses gradient differences.</li>
<li>Ideal for convex, smooth functions.</li>
<li>Common in logistic regression, SVMs, and classical ML (before deep nets).</li>
</ul>
<p>In deep learning, L-BFGS is used for fine-tuning or small networks (where full batches are feasible).</p>
</section>
<section id="a-gentle-proof-why-it-works-29" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-29">A Gentle Proof (Why It Works)</h4>
<p>BFGS ensures positive definiteness of <span class="math inline">\(H_t\)</span> if <span class="math inline">\(y_t^\top s_t &gt; 0\)</span>, guaranteeing descent direction:</p>
<p><span class="math display">\[
g_t^\top p_t = -g_t^\top H_t g_t &lt; 0
\]</span></p>
<p>Thus, each step moves downhill.</p>
<p>L-BFGS approximates the Newton step:</p>
<p><span class="math display">\[
\Delta \theta = -H_t^{-1} g_t
\]</span></p>
<p>using a compact two-loop recursion that reconstructs <span class="math inline">\(H_t g_t\)</span> from stored <span class="math inline">\((s, y)\)</span> pairs.</p>
</section>
<section id="try-it-yourself-29" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-29">Try It Yourself</h4>
<ol type="1">
<li>Compare L-BFGS vs SGD on quadratic <span class="math inline">\(L(\theta) = \theta_1^2 + 100\theta_2^2\)</span>.</li>
<li>Visualize trajectories, L-BFGS converges in fewer steps.</li>
<li>Vary history size <span class="math inline">\(m = 3, 5, 10\)</span>.</li>
<li>Try with and without line search.</li>
<li>Use <code>fmin_l_bfgs_b</code> in SciPy for logistic regression.</li>
</ol>
</section>
<section id="test-cases-29" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-29">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Function</th>
<th>Optimizer</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Quadratic bowl</td>
<td>L-BFGS</td>
<td>Fast convergence</td>
</tr>
<tr class="even">
<td>Ill-conditioned</td>
<td>L-BFGS</td>
<td>Adjusts curvature</td>
</tr>
<tr class="odd">
<td>Non-convex</td>
<td>L-BFGS</td>
<td>May get stuck in local minima</td>
</tr>
<tr class="even">
<td>Deep net</td>
<td>L-BFGS</td>
<td>Works only with full-batch training</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-29" class="level4">
<h4 class="anchored" data-anchor-id="complexity-29">Complexity</h4>
<ul>
<li>Per iteration: <span class="math inline">\(O(md)\)</span> (with memory <span class="math inline">\(m\)</span>)</li>
<li>Memory: <span class="math inline">\(O(md)\)</span></li>
<li>Convergence: superlinear for convex smooth functions</li>
</ul>
<p>L-BFGS is a “memory-efficient Newton’s method”, harnessing curvature from history, not Hessians, to speed up convergence on smooth terrains.</p>
</section>
</section>
</section>
<section id="section-94.-deep-learning" class="level1">
<h1>Section 94. Deep Learning</h1>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation">931. Backpropagation</h3>
<p>Backpropagation is the cornerstone algorithm for training neural networks. It efficiently computes gradients of the loss with respect to every weight by applying the chain rule of calculus through the network, allowing gradient-based optimizers like SGD or Adam to update parameters.</p>
<section id="what-problem-are-we-solving-30" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-30">What Problem Are We Solving?</h4>
<p>In neural networks, we want to minimize a loss function <span class="math display">\[
L(\theta)
\]</span> over parameters <span class="math inline">\(\theta\)</span> (weights and biases). Directly computing <span class="math inline">\(\frac{\partial L}{\partial \theta}\)</span> for every parameter by hand is infeasible, too many dependencies, too many paths.</p>
<p>Backpropagation provides a systematic, efficient way to compute all gradients in one backward pass using the chain rule.</p>
</section>
<section id="the-core-idea" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea">The Core Idea</h4>
<p>Every layer in the network transforms an input: <span class="math display">\[
a^{(l)} = f^{(l)}(W^{(l)} a^{(l-1)} + b^{(l)})
\]</span></p>
<p>Loss <span class="math inline">\(L\)</span> depends on the final output <span class="math inline">\(a^{(L)}\)</span>. By applying the chain rule, we propagate derivatives backward from the output to each layer.</p>
<p>For each layer <span class="math inline">\(l\)</span>, we compute:</p>
<ol type="1">
<li><p>Output error: <span class="math display">\[
\delta^{(L)} = \nabla_{a^{(L)}} L \odot f'^{(L)}(z^{(L)})
\]</span></p></li>
<li><p>Backward recursion: <span class="math display">\[
\delta^{(l)} = (W^{(l+1)})^\top \delta^{(l+1)} \odot f'^{(l)}(z^{(l)})
\]</span></p></li>
<li><p>Gradients: <span class="math display">\[
\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^\top, \quad
\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}
\]</span></p></li>
</ol>
<p>Then update parameters with any gradient-based optimizer.</p>
</section>
<section id="how-does-it-work-plain-language-30" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-30">How Does It Work (Plain Language)?</h4>
<p>Think of backpropagation like blaming errors:</p>
<ul>
<li>The output layer compares its prediction to the target and computes an error.</li>
<li>Each hidden layer asks, “How much did I contribute to that error?” It passes credit (or blame) backward through connections.</li>
</ul>
<p>By repeating this, each neuron learns how to adjust its weights to reduce future mistakes.</p>
</section>
<section id="step-by-step-summary-6" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-6">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 39%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Forward Pass</td>
<td>Compute activations layer by layer</td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute Loss</td>
<td>Compare prediction to target</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Initialize Output Error</td>
<td>Derivative of loss w.r.t. final layer</td>
</tr>
<tr class="even">
<td>4</td>
<td>Backward Pass</td>
<td>Apply chain rule layer by layer</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Compute Gradients</td>
<td>For all weights and biases</td>
</tr>
<tr class="even">
<td>6</td>
<td>Update Parameters</td>
<td>Using optimizer (e.g.&nbsp;SGD, Adam)</td>
</tr>
</tbody>
</table>
</section>
<section id="example-18" class="level4">
<h4 class="anchored" data-anchor-id="example-18">Example</h4>
<p>For a 2-layer network: <span class="math display">\[
a^{(1)} = \sigma(W^{(1)} x + b^{(1)})
\]</span> <span class="math display">\[
a^{(2)} = \text{softmax}(W^{(2)} a^{(1)} + b^{(2)})
\]</span> <span class="math display">\[
L = \text{cross\_entropy}(a^{(2)}, y)
\]</span></p>
<p>Then:</p>
<ul>
<li>Output layer error: <span class="math display">\[
\delta^{(2)} = a^{(2)} - y
\]</span></li>
<li>Hidden layer error: <span class="math display">\[
\delta^{(1)} = (W^{(2)})^\top \delta^{(2)} \odot \sigma'(z^{(1)})
\]</span></li>
</ul>
</section>
<section id="tiny-code-easy-versions-28" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-28">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple 1-hidden-layer NN</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x): <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_deriv(x): <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]])</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward</span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>    z1 <span class="op">=</span> X <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>    a1 <span class="op">=</span> sigmoid(z1)</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> a1 <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>    a2 <span class="op">=</span> sigmoid(z2)</span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean((y <span class="op">-</span> a2)  <span class="dv">2</span>)</span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward</span></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>    dL_da2 <span class="op">=</span> <span class="op">-</span>(y <span class="op">-</span> a2)</span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>    da2_dz2 <span class="op">=</span> sigmoid_deriv(a2)</span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>    dz2_dW2 <span class="op">=</span> a1</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>    delta2 <span class="op">=</span> dL_da2 <span class="op">*</span> da2_dz2</span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>    dW2 <span class="op">=</span> dz2_dW2.T <span class="op">@</span> delta2</span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>    db2 <span class="op">=</span> np.<span class="bu">sum</span>(delta2, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>    delta1 <span class="op">=</span> (delta2 <span class="op">@</span> W2.T) <span class="op">*</span> sigmoid_deriv(a1)</span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>    dW1 <span class="op">=</span> X.T <span class="op">@</span> delta1</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a>    db1 <span class="op">=</span> np.<span class="bu">sum</span>(delta1, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update</span></span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>    W1 <span class="op">-=</span> lr <span class="op">*</span> dW1</span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">-=</span> lr <span class="op">*</span> db1</span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>    W2 <span class="op">-=</span> lr <span class="op">*</span> dW2</span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">-=</span> lr <span class="op">*</span> db2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Forward pass: compute activations</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Compute loss and output error</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Backward pass: propagate errors using chain rule</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Compute weight gradients</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 5. Update weights: W -= lr * grad</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-30" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-30">Why It Matters</h4>
<ul>
<li>Efficient: computes all gradients in <span class="math inline">\(O(\text{\#parameters})\)</span>.</li>
<li>Scalable: works for any differentiable network.</li>
<li>Foundation of deep learning: every modern network, CNNs, RNNs, Transformers, uses it.</li>
<li>Enables automatic differentiation (autograd).</li>
</ul>
<p>Backpropagation turned neural networks from theory to practice, enabling the modern AI revolution.</p>
</section>
<section id="a-gentle-proof-why-it-works-30" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-30">A Gentle Proof (Why It Works)</h4>
<p>For composed functions <span class="math display">\[
f(x) = f_3(f_2(f_1(x)))
\]</span> By the chain rule: <span class="math display">\[
\frac{df}{dx} = f_3'(f_2(f_1(x))) \cdot f_2'(f_1(x)) \cdot f_1'(x)
\]</span></p>
<p>Backprop efficiently applies this recursively, caching intermediate results (activations) from the forward pass, avoiding recomputation.</p>
</section>
<section id="try-it-yourself-30" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-30">Try It Yourself</h4>
<ol type="1">
<li>Implement a 2-layer perceptron.</li>
<li>Compute gradients manually (small net) and compare with backprop.</li>
<li>Visualize <span class="math inline">\(\delta\)</span> values per layer.</li>
<li>Add ReLU activation and note simplicity of derivative.</li>
<li>Compare training speed with/without caching activations.</li>
</ol>
</section>
<section id="test-cases-30" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-30">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Network</th>
<th>Layers</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear</td>
<td>1</td>
<td>Gradients match analytical</td>
</tr>
<tr class="even">
<td>MLP</td>
<td>2</td>
<td>Smooth convergence</td>
</tr>
<tr class="odd">
<td>Deep net</td>
<td>10</td>
<td>Works with caching</td>
</tr>
<tr class="even">
<td>Nonlinear</td>
<td>ReLU</td>
<td>Sparse gradients, stable</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-30" class="level4">
<h4 class="anchored" data-anchor-id="complexity-30">Complexity</h4>
<ul>
<li>Forward pass: <span class="math inline">\(O(P)\)</span></li>
<li>Backward pass: <span class="math inline">\(O(P)\)</span></li>
<li>Memory: <span class="math inline">\(O(P)\)</span> for activations (<span class="math inline">\(P\)</span> = number of parameters)</li>
</ul>
<p>Backpropagation is the heart of learning, applying calculus in reverse to teach machines how to adjust every connection toward better predictions.</p>
</section>
</section>
<section id="xavier-he-initialization" class="level3">
<h3 class="anchored" data-anchor-id="xavier-he-initialization">932. Xavier / He Initialization</h3>
<p>Neural networks learn by propagating signals forward and gradients backward. But if the weights are too large or too small, activations explode or vanish across layers. Xavier and He initialization solve this by scaling the initial weights according to the number of input (and sometimes output) connections, keeping the signal’s variance stable throughout the network.</p>
<section id="what-problem-are-we-solving-31" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-31">What Problem Are We Solving?</h4>
<p>When initializing neural network weights, two problems often occur:</p>
<ol type="1">
<li><p>Exploding activations: If weights are too large, activations grow exponentially with each layer.</p></li>
<li><p>Vanishing activations: If weights are too small, signals shrink to zero, gradients vanish.</p></li>
</ol>
<p>We want the variance of activations to remain approximately constant across layers: <span class="math display">\[
\mathrm{Var}[a^{(l)}] \approx \mathrm{Var}[a^{(l-1)}]
\]</span></p>
<p>Xavier and He initializations use statistical reasoning to find the right scale.</p>
</section>
<section id="the-core-idea-1" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-1">The Core Idea</h4>
<p>For a neuron: <span class="math display">\[
a^{(l)} = f(W^{(l)} a^{(l-1)} + b^{(l)})
\]</span></p>
<p>Let each input have variance <span class="math inline">\(\mathrm{Var}[a^{(l-1)}] = v\)</span>, and each weight have variance <span class="math inline">\(\mathrm{Var}[W^{(l)}] = \sigma^2\)</span>.</p>
<p>If we want to preserve variance, we need: <span class="math display">\[
n_{\text{in}} \sigma^2 = 1
\]</span></p>
<p>So we set: <span class="math display">\[
\sigma^2 = \frac{1}{n_{\text{in}}}
\]</span></p>
<p>This gives Xavier (Glorot) Initialization.</p>
</section>
<section id="formulas" class="level4">
<h4 class="anchored" data-anchor-id="formulas">Formulas</h4>
<section id="xavier-glorot-initialization" class="level5">
<h5 class="anchored" data-anchor-id="xavier-glorot-initialization">Xavier (Glorot) Initialization</h5>
<p>For tanh or sigmoid activations (zero-centered):</p>
<ul>
<li>Normal distribution: <span class="math display">\[
W \sim \mathcal{N}\left(0, \frac{1}{n_{\text{in}}}\right)
\]</span></li>
<li>Uniform distribution: <span class="math display">\[
W \sim U\left[-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right]
\]</span></li>
</ul>
</section>
<section id="he-initialization" class="level5">
<h5 class="anchored" data-anchor-id="he-initialization">He Initialization</h5>
<p>For ReLU activations (half the inputs zeroed):</p>
<ul>
<li>Normal: <span class="math display">\[
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
\]</span></li>
<li>Uniform: <span class="math display">\[
W \sim U\left[-\sqrt{\frac{6}{n_{\text{in}}}}, \sqrt{\frac{6}{n_{\text{in}}}}\right]
\]</span></li>
</ul>
<p>He initialization doubles the variance to account for ReLU’s sparsity.</p>
</section>
</section>
<section id="how-does-it-work-plain-language-31" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-31">How Does It Work (Plain Language)?</h4>
<p>Imagine passing water through a series of pipes (layers). If some pipes widen and others shrink, the flow changes unpredictably. Xavier and He make each layer’s pipe diameter proportional to its number of inputs, keeping the “flow” (variance) consistent.</p>
<ul>
<li>Xavier: balances forward and backward flow for smooth gradient propagation.</li>
<li>He: boosts variance for ReLU, since half the activations get zeroed.</li>
</ul>
</section>
<section id="step-by-step-summary-7" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-7">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Choose activation</td>
<td><code>tanh</code> → Xavier, <code>ReLU</code> → He</td>
</tr>
<tr class="even">
<td>2</td>
<td>Count fan-in</td>
<td><span class="math inline">\(n_{\text{in}} = \text{\# inputs}\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Sample weights</td>
<td>From scaled normal/uniform distribution</td>
</tr>
<tr class="even">
<td>4</td>
<td>Initialize biases</td>
<td>Typically zeros</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Start training</td>
<td>Activations maintain healthy variance</td>
</tr>
</tbody>
</table>
</section>
<section id="example-19" class="level4">
<h4 class="anchored" data-anchor-id="example-19">Example</h4>
<p>Suppose a layer has 256 inputs and 128 outputs.</p>
<ul>
<li><p>Xavier (tanh): <span class="math display">\[
\text{std} = \sqrt{\frac{2}{256 + 128}} = 0.05
\]</span> <span class="math display">\[
W \sim \mathcal{N}(0, 0.05^2)
\]</span></p></li>
<li><p>He (ReLU): <span class="math display">\[
\text{std} = \sqrt{\frac{2}{256}} = 0.088
\]</span> <span class="math display">\[
W \sim \mathcal{N}(0, 0.088^2)
\]</span></p></li>
</ul>
</section>
<section id="tiny-code-easy-versions-29" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-29">Tiny Code (Easy Versions)</h4>
<p>Python (NumPy)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> xavier_init(n_in, n_out):</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    limit <span class="op">=</span> np.sqrt(<span class="dv">6</span> <span class="op">/</span> (n_in <span class="op">+</span> n_out))</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.uniform(<span class="op">-</span>limit, limit, (n_in, n_out))</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> he_init(n_in, n_out):</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> np.sqrt(<span class="dv">2</span> <span class="op">/</span> n_in)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.randn(n_in, n_out) <span class="op">*</span> std</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Example</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> xavier_init(<span class="dv">256</span>, <span class="dv">128</span>)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> he_init(<span class="dv">256</span>, <span class="dv">128</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;math.h&gt;</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdlib.h&gt;</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> rand_uniform<span class="op">(</span><span class="dt">double</span> a<span class="op">,</span> <span class="dt">double</span> b<span class="op">)</span> <span class="op">{</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">+</span> <span class="op">(</span>b <span class="op">-</span> a<span class="op">)</span> <span class="op">*</span> <span class="op">((</span><span class="dt">double</span><span class="op">)</span> rand<span class="op">()</span> <span class="op">/</span> RAND_MAX<span class="op">);</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> xavier_init<span class="op">(</span><span class="dt">double</span><span class="op">*</span> W<span class="op">,</span> <span class="dt">int</span> n_in<span class="op">,</span> <span class="dt">int</span> n_out<span class="op">)</span> <span class="op">{</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> limit <span class="op">=</span> sqrt<span class="op">(</span><span class="fl">6.0</span> <span class="op">/</span> <span class="op">(</span>n_in <span class="op">+</span> n_out<span class="op">));</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n_in <span class="op">*</span> n_out<span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>        W<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> rand_uniform<span class="op">(-</span>limit<span class="op">,</span> limit<span class="op">);</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-31" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-31">Why It Matters</h4>
<ul>
<li>Prevents vanishing/exploding activations.</li>
<li>Keeps gradient variance stable across layers.</li>
<li>Essential for deep networks (10+ layers).</li>
<li>Simple change → massive improvement in training stability.</li>
</ul>
<p>Without proper initialization, even the best optimizers fail to converge.</p>
</section>
<section id="a-gentle-proof-why-it-works-31" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-31">A Gentle Proof (Why It Works)</h4>
<p>Variance of a neuron’s output: <span class="math display">\[
\mathrm{Var}[a^{(l)}] = n_{\text{in}} \mathrm{Var}[W^{(l)}] \mathrm{Var}[a^{(l-1)}]
\]</span></p>
<p>To maintain <span class="math inline">\(\mathrm{Var}[a^{(l)}] = \mathrm{Var}[a^{(l-1)}]\)</span>, set: <span class="math display">\[
\mathrm{Var}[W^{(l)}] = \frac{1}{n_{\text{in}}}
\]</span></p>
<p>ReLU halves the effective variance, so multiply by 2 → He initialization.</p>
</section>
<section id="try-it-yourself-31" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-31">Try It Yourself</h4>
<ol type="1">
<li>Train a deep MLP with random small weights → observe vanishing gradients.</li>
<li>Repeat with Xavier → stable learning.</li>
<li>Try ReLU with He → faster convergence.</li>
<li>Compare variance of activations layer-by-layer.</li>
<li>Plot histograms of activations during forward pass.</li>
</ol>
</section>
<section id="test-cases-31" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-31">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Activation</th>
<th>Method</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tanh</td>
<td>Xavier</td>
<td>Balanced activations</td>
</tr>
<tr class="even">
<td>sigmoid</td>
<td>Xavier</td>
<td>Stable early layers</td>
</tr>
<tr class="odd">
<td>ReLU</td>
<td>He</td>
<td>Non-zero stable variance</td>
</tr>
<tr class="even">
<td>LeakyReLU</td>
<td>He</td>
<td>Works well</td>
</tr>
<tr class="odd">
<td>None</td>
<td>Random small</td>
<td>Vanishing gradients</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-31" class="level4">
<h4 class="anchored" data-anchor-id="complexity-31">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(P)\)</span> (one-time setup)</li>
<li>Space: <span class="math inline">\(O(P)\)</span> (weights)</li>
<li>Benefit: stabilizes signal propagation from the start</li>
</ul>
<p>Proper initialization is step 0 of learning, Xavier and He set the stage so your optimizer doesn’t have to fight bad scaling before it can learn.</p>
</section>
</section>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">933. Dropout</h3>
<p>Dropout is a regularization technique for neural networks that prevents overfitting by randomly turning off neurons during training. Each training pass samples a smaller subnetwork, forcing the model to rely on distributed representations rather than memorizing patterns.</p>
<section id="what-problem-are-we-solving-32" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-32">What Problem Are We Solving?</h4>
<p>Deep networks with millions of parameters can easily overfit, they memorize training data instead of learning general patterns. Even with early stopping or weight decay, neurons can still co-adapt (depend too strongly on one another).</p>
<p>Dropout breaks these co-adaptations by making each neuron unreliable during training, like training an ensemble of smaller networks that must all perform well.</p>
</section>
<section id="the-core-idea-2" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-2">The Core Idea</h4>
<p>During training, each neuron’s output is randomly “dropped” with probability <span class="math inline">\(p\)</span>. Formally, for activation <span class="math inline">\(a_i^{(l)}\)</span> in layer <span class="math inline">\(l\)</span>:</p>
<p><span class="math display">\[
\tilde{a}_i^{(l)} = r_i^{(l)} \cdot a_i^{(l)}, \quad r_i^{(l)} \sim \text{Bernoulli}(1 - p)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(p\)</span> = dropout rate (e.g.&nbsp;0.5)</li>
<li><span class="math inline">\(r_i^{(l)}\)</span> = random mask (0 or 1)</li>
</ul>
<p>At inference time, no neurons are dropped, instead, activations are scaled by <span class="math inline">\((1 - p)\)</span> to match expected output.</p>
</section>
<section id="how-does-it-work-plain-language-32" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-32">How Does It Work (Plain Language)?</h4>
<p>Imagine a classroom where half the students are randomly asked to leave during each discussion. The remaining students must still solve the problem, so everyone learns independently and can handle missing teammates later.</p>
<p>Dropout does the same for neurons:</p>
<ul>
<li>During training → random neurons go silent.</li>
<li>During testing → everyone returns, but each neuron’s output is scaled down.</li>
</ul>
<p>This creates robustness and prevents reliance on any single feature.</p>
</section>
<section id="step-by-step-summary-8" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-8">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 47%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Choose dropout rate <span class="math inline">\(p\)</span></td>
<td>Common: 0.2–0.5</td>
</tr>
<tr class="even">
<td>2</td>
<td>Sample mask <span class="math inline">\(r_i \sim \text{Bernoulli}(1 - p)\)</span></td>
<td>One per neuron</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Apply mask</td>
<td><span class="math inline">\(\tilde{a}_i = r_i \cdot a_i\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Forward pass</td>
<td>Use masked activations</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Backprop</td>
<td>Gradients flow only through active neurons</td>
</tr>
<tr class="even">
<td>6</td>
<td>Inference</td>
<td>Use all neurons, scale activations by <span class="math inline">\((1 - p)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-20" class="level4">
<h4 class="anchored" data-anchor-id="example-20">Example</h4>
<p>For a hidden layer: <span class="math display">\[
a^{(l)} = f(W^{(l)} a^{(l-1)} + b^{(l)})
\]</span> Apply dropout: <span class="math display">\[
r^{(l)} \sim \text{Bernoulli}(1 - p)
\]</span> <span class="math display">\[
\tilde{a}^{(l)} = r^{(l)} \odot a^{(l)}
\]</span></p>
<p>At inference: <span class="math display">\[
a^{(l)}*{\text{test}} = (1 - p) a^{(l)}*{\text{train}}
\]</span></p>
</section>
<section id="tiny-code-easy-versions-30" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-30">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dropout_layer(a, p<span class="op">=</span><span class="fl">0.5</span>, train<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train:</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> (np.random.rand(<span class="op">*</span>a.shape) <span class="op">&gt;</span> p).astype(<span class="bu">float</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">*</span> mask <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> p)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a  <span class="co"># scaled during training</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb60"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdlib.h&gt;</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> dropout<span class="op">(</span><span class="dt">double</span><span class="op">*</span> a<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">double</span> p<span class="op">)</span> <span class="op">{</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">double</span> r <span class="op">=</span> <span class="op">(</span><span class="dt">double</span><span class="op">)</span> rand<span class="op">()</span> <span class="op">/</span> RAND_MAX<span class="op">;</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        a<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="op">(</span>r <span class="op">&gt;</span> p<span class="op">)</span> <span class="op">?</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1.0</span> <span class="op">-</span> p<span class="op">)</span> <span class="op">:</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-32" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-32">Why It Matters</h4>
<ul>
<li>Reduces overfitting: prevents reliance on single neurons.</li>
<li>Acts as ensemble averaging: each training iteration samples a subnetwork.</li>
<li>Improves generalization: especially for fully connected layers.</li>
<li>Simple and effective: no extra parameters or complexity.</li>
</ul>
<p>Dropout was one of the major breakthroughs enabling deep neural networks to generalize well.</p>
</section>
<section id="a-gentle-proof-why-it-works-32" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-32">A Gentle Proof (Why It Works)</h4>
<p>The expected activation stays constant:</p>
<p><span class="math display">\[
\mathbb{E}[\tilde{a}_i] = (1 - p)a_i
\]</span></p>
<p>To preserve magnitude, we scale during training by <span class="math inline">\(\frac{1}{1 - p}\)</span>: <span class="math display">\[
\tilde{a}_i = \frac{r_i}{1 - p} a_i
\]</span></p>
<p>Thus: <span class="math display">\[
\mathbb{E}[\tilde{a}_i] = a_i
\]</span></p>
<p>This keeps mean activations unchanged across training and inference.</p>
</section>
<section id="try-it-yourself-32" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-32">Try It Yourself</h4>
<ol type="1">
<li>Train a small MLP on MNIST with and without dropout.</li>
<li>Compare train and test accuracy.</li>
<li>Experiment with <span class="math inline">\(p = 0.2, 0.5, 0.8\)</span>.</li>
<li>Apply dropout only on hidden layers.</li>
<li>Observe slower training but better generalization.</li>
</ol>
</section>
<section id="test-cases-32" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-32">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Network</th>
<th>Dropout Rate</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0 (None)</td>
<td>0.0</td>
<td>Overfitting</td>
</tr>
<tr class="even">
<td>Moderate</td>
<td>0.5</td>
<td>Best generalization</td>
</tr>
<tr class="odd">
<td>High</td>
<td>0.8</td>
<td>Underfitting</td>
</tr>
<tr class="even">
<td>ConvNet</td>
<td>0.3</td>
<td>Stable results</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-32" class="level4">
<h4 class="anchored" data-anchor-id="complexity-32">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N)\)</span> per layer (mask sampling)</li>
<li>Memory: <span class="math inline">\(O(N)\)</span> (mask storage)</li>
<li>Inference cost: none</li>
</ul>
<p>Dropout makes your network forget, just enough to remember the right things.</p>
</section>
</section>
<section id="batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="batch-normalization">934. Batch Normalization</h3>
<p>Batch Normalization (BatchNorm) stabilizes and accelerates training by normalizing activations across each mini-batch. It keeps layer outputs well-behaved, neither exploding nor vanishing, so networks can train deeper, faster, and with higher learning rates.</p>
<section id="what-problem-are-we-solving-33" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-33">What Problem Are We Solving?</h4>
<p>During training, as earlier layers change, the input distribution to later layers also shifts, a problem called internal covariate shift. This makes optimization harder because each layer must constantly adapt to the changing distribution of its inputs.</p>
<p>BatchNorm reduces this drift by normalizing layer activations to have zero mean and unit variance, then allowing the network to re-learn an appropriate scale and offset.</p>
</section>
<section id="the-core-idea-3" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-3">The Core Idea</h4>
<p>For each mini-batch and feature channel:</p>
<p><span class="math display">\[
\mu_B = \frac{1}{m} \sum_{i=1}^m x_i
\]</span> <span class="math display">\[
\sigma_B^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2
\]</span></p>
<p>Normalize and re-scale:</p>
<p><span class="math display">\[
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
\]</span> <span class="math display">\[
y_i = \gamma \hat{x}_i + \beta
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learned parameters for scale and shift.</li>
<li><span class="math inline">\(\epsilon\)</span> prevents division by zero.</li>
</ul>
<p>At inference, moving averages of <span class="math inline">\(\mu_B\)</span> and <span class="math inline">\(\sigma_B^2\)</span> are used instead of batch statistics.</p>
</section>
<section id="how-does-it-work-plain-language-33" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-33">How Does It Work (Plain Language)?</h4>
<p>Imagine each neuron’s output as students’ test scores, some high, some low, with wildly different averages in every class. BatchNorm centers (subtract mean) and scales (divide by std) those scores, so the next layer always receives values in a similar range. Then it lets the model reintroduce personality with learned <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>This makes the optimization landscape smoother and easier to traverse.</p>
</section>
<section id="step-by-step-summary-9" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-9">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 35%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute mean and variance</td>
<td>From mini-batch</td>
</tr>
<tr class="even">
<td>2</td>
<td>Normalize</td>
<td>Subtract mean, divide by std</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Scale and shift</td>
<td>Apply learned <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\beta\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Forward pass</td>
<td>Use normalized activations</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Backpropagate</td>
<td>Compute gradients for <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\beta\)</span></td>
</tr>
<tr class="even">
<td>6</td>
<td>Inference</td>
<td>Use running averages of <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-21" class="level4">
<h4 class="anchored" data-anchor-id="example-21">Example</h4>
<p>For a layer with input <span class="math inline">\(x = [x_1, x_2, \dots, x_m]\)</span>:</p>
<p><span class="math display">\[
\mu_B = \frac{1}{m} \sum_i x_i, \quad
\sigma_B^2 = \frac{1}{m} \sum_i (x_i - \mu_B)^2
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad
y_i = \gamma \hat{x}_i + \beta
\]</span></p>
<p>At test time: <span class="math display">\[
y_i = \frac{x_i - \mu_{\text{running}}}{\sqrt{\sigma_{\text{running}}^2 + \epsilon}} \gamma + \beta
\]</span></p>
</section>
<section id="tiny-code-easy-versions-31" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-31">Tiny Code (Easy Versions)</h4>
<p>Python (NumPy)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batchnorm_forward(x, gamma, beta, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> np.var(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    x_hat <span class="op">=</span> (x <span class="op">-</span> mu) <span class="op">/</span> np.sqrt(var <span class="op">+</span> eps)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> gamma <span class="op">*</span> x_hat <span class="op">+</span> beta</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;math.h&gt;</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> batchnorm<span class="op">(</span><span class="dt">double</span><span class="op">*</span> x<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> y<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">double</span> gamma<span class="op">,</span> <span class="dt">double</span> beta<span class="op">,</span> <span class="dt">double</span> eps<span class="op">)</span> <span class="op">{</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> mean <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> var <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> mean <span class="op">+=</span> x<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">/=</span> n<span class="op">;</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> var <span class="op">+=</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">*</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">);</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    var <span class="op">/=</span> n<span class="op">;</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> gamma <span class="op">*</span> <span class="op">((</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">/</span> sqrt<span class="op">(</span>var <span class="op">+</span> eps<span class="op">))</span> <span class="op">+</span> beta<span class="op">;</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-33" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-33">Why It Matters</h4>
<ul>
<li>Faster convergence: allows higher learning rates.</li>
<li>Improved stability: keeps gradients in a healthy range.</li>
<li>Regularization effect: slight noise from batch stats reduces overfitting.</li>
<li>Easier initialization: less sensitive to starting weights.</li>
</ul>
<p>BatchNorm became the default normalization in CNNs, RNNs, and Transformers before layer normalization took over for non-convolutional models.</p>
</section>
<section id="a-gentle-proof-why-it-works-33" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-33">A Gentle Proof (Why It Works)</h4>
<p>Gradient explosion happens when activation variance grows with depth. BatchNorm constrains variance to <span class="math inline">\(\approx 1\)</span>, preventing blow-up.</p>
<p>For any neuron: <span class="math display">\[
\mathrm{Var}[\hat{x}] = \frac{\mathrm{Var}[x]}{\sigma_B^2 + \epsilon} \approx 1
\]</span></p>
<p>Thus, the gradient with respect to <span class="math inline">\(x\)</span> is also stabilized, leading to smoother optimization and more reliable updates.</p>
</section>
<section id="try-it-yourself-33" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-33">Try It Yourself</h4>
<ol type="1">
<li>Train a 5-layer MLP with and without BatchNorm.</li>
<li>Observe faster convergence with normalization.</li>
<li>Vary batch size, small batches make BatchNorm noisier.</li>
<li>Compare with LayerNorm (used in Transformers).</li>
<li>Inspect <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\beta\)</span> after training, they adaptively re-scale outputs.</li>
</ol>
</section>
<section id="test-cases-33" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-33">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Architecture</th>
<th>Normalization</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MLP</td>
<td>None</td>
<td>Slow convergence</td>
</tr>
<tr class="even">
<td>MLP</td>
<td>BatchNorm</td>
<td>Faster and stable</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td>BatchNorm</td>
<td>Smoother gradients</td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>LayerNorm</td>
<td>Works better for sequences</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-33" class="level4">
<h4 class="anchored" data-anchor-id="complexity-33">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N)\)</span> per batch</li>
<li>Memory: <span class="math inline">\(O(N)\)</span> for storing mean, variance</li>
<li>Inference: cached running averages</li>
</ul>
<p>BatchNorm is one of the simplest yet most transformative tricks in deep learning, a normalization layer that turned unstable deep networks into trainable ones.</p>
</section>
</section>
<section id="layer-normalization" class="level3">
<h3 class="anchored" data-anchor-id="layer-normalization">935. Layer Normalization</h3>
<p>Layer Normalization (LayerNorm) normalizes activations within each sample, not across a batch. It provides stable training for models where batch statistics are unreliable, especially RNNs, Transformers, and other sequence models.</p>
<section id="what-problem-are-we-solving-34" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-34">What Problem Are We Solving?</h4>
<p>Batch Normalization relies on mini-batch statistics. But in tasks like sequence modeling, reinforcement learning, or variable-length inputs:</p>
<ul>
<li>Batch sizes can be very small.</li>
<li>The same layer can process very different sequences.</li>
<li>Batch statistics fluctuate too much.</li>
</ul>
<p>LayerNorm solves this by normalizing activations across the features of each individual sample, making it independent of batch size.</p>
</section>
<section id="the-core-idea-4" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-4">The Core Idea</h4>
<p>Given an input vector for one sample: <span class="math display">\[
x = [x_1, x_2, \dots, x_H]
\]</span> where <span class="math inline">\(H\)</span> is the number of hidden units (features).</p>
<p>Compute per-sample statistics: <span class="math display">\[
\mu = \frac{1}{H} \sum_{i=1}^H x_i
\]</span> <span class="math display">\[
\sigma^2 = \frac{1}{H} \sum_{i=1}^H (x_i - \mu)^2
\]</span></p>
<p>Normalize and re-scale: <span class="math display">\[
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad
y_i = \gamma_i \hat{x}_i + \beta_i
\]</span></p>
<p>Here <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\beta_i\)</span> are learned per-feature scaling and shifting parameters.</p>
</section>
<section id="how-does-it-work-plain-language-34" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-34">How Does It Work (Plain Language)?</h4>
<p>Think of each neuron’s activations in a single input as a mini-ecosystem. LayerNorm makes sure that, for every input, the activations have a consistent “climate” (mean 0, variance 1), regardless of the batch or sequence.</p>
<p>Instead of comparing across examples, it standardizes within each example.</p>
<p>This makes it robust for small batches, sequential data, and attention-based architectures.</p>
</section>
<section id="step-by-step-summary-10" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-10">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 50%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Take one sample (vector of activations)</td>
<td><span class="math inline">\(x = [x_1, ..., x_H]\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute mean and variance</td>
<td>Over features, not batch</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Normalize</td>
<td>Subtract mean, divide by std</td>
</tr>
<tr class="even">
<td>4</td>
<td>Re-scale</td>
<td>Multiply by <span class="math inline">\(\gamma\)</span>, add <span class="math inline">\(\beta\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Use output</td>
<td>Feed normalized activations forward</td>
</tr>
</tbody>
</table>
</section>
<section id="example-22" class="level4">
<h4 class="anchored" data-anchor-id="example-22">Example</h4>
<p>For a Transformer hidden state <span class="math inline">\(x \in \mathbb{R}^{d_{\text{model}}}\)</span>:</p>
<p><span class="math display">\[
\mu = \frac{1}{d_{\text{model}}} \sum_i x_i, \quad
\sigma^2 = \frac{1}{d_{\text{model}}} \sum_i (x_i - \mu)^2
\]</span> <span class="math display">\[
\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\]</span></p>
</section>
<section id="tiny-code-easy-versions-32" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-32">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> layernorm(x, gamma, beta, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.mean(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> np.var(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    x_hat <span class="op">=</span> (x <span class="op">-</span> mu) <span class="op">/</span> np.sqrt(var <span class="op">+</span> eps)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gamma <span class="op">*</span> x_hat <span class="op">+</span> beta</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;math.h&gt;</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> layernorm<span class="op">(</span><span class="dt">double</span><span class="op">*</span> x<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> y<span class="op">,</span> <span class="dt">int</span> H<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> gamma<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> beta<span class="op">,</span> <span class="dt">double</span> eps<span class="op">)</span> <span class="op">{</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> mean <span class="op">=</span> <span class="fl">0.0</span><span class="op">,</span> var <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> H<span class="op">;</span> i<span class="op">++)</span> mean <span class="op">+=</span> x<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">/=</span> H<span class="op">;</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> H<span class="op">;</span> i<span class="op">++)</span> var <span class="op">+=</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">*</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">);</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    var <span class="op">/=</span> H<span class="op">;</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> H<span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> gamma<span class="op">[</span>i<span class="op">]</span> <span class="op">*</span> <span class="op">((</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">/</span> sqrt<span class="op">(</span>var <span class="op">+</span> eps<span class="op">))</span> <span class="op">+</span> beta<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-34" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-34">Why It Matters</h4>
<ul>
<li>Works with any batch size, even 1.</li>
<li>Stabilizes RNNs and Transformers where BatchNorm fails.</li>
<li>Smooths optimization landscape and gradient flow.</li>
<li>Adds minimal computational overhead.</li>
</ul>
<p>LayerNorm is one of the key innovations that made Transformers (like GPT and BERT) train stably across long sequences and variable contexts.</p>
</section>
<section id="a-gentle-proof-why-it-works-34" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-34">A Gentle Proof (Why It Works)</h4>
<p>For each sample: <span class="math display">\[
\mathbb{E}[\hat{x}] = 0, \quad \mathrm{Var}[\hat{x}] = 1
\]</span></p>
<p>Thus the normalized activations maintain consistent magnitude, ensuring gradients stay stable: <span class="math display">\[
\frac{\partial L}{\partial x_i} = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}}
\left( \frac{\partial L}{\partial \hat{x}_i} -
\frac{1}{H} \sum_j \frac{\partial L}{\partial \hat{x}_j} -
\frac{\hat{x}_i}{H} \sum_j \frac{\partial L}{\partial \hat{x}_j} \hat{x}_j \right)
\]</span></p>
<p>This preserves zero-mean gradients, avoiding drift across layers.</p>
</section>
<section id="try-it-yourself-34" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-34">Try It Yourself</h4>
<ol type="1">
<li>Train a Transformer block with and without LayerNorm.</li>
<li>Compare loss stability, LayerNorm smooths training.</li>
<li>Try tiny batch sizes (<span class="math inline">\(B=1\)</span> or <span class="math inline">\(B=2\)</span>).</li>
<li>Apply before or after residual connections.</li>
<li>Inspect learned <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\beta\)</span>, they adapt output range.</li>
</ol>
</section>
<section id="test-cases-34" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-34">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Normalization</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MLP</td>
<td>BatchNorm</td>
<td>OK</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>BatchNorm</td>
<td>Unstable</td>
</tr>
<tr class="odd">
<td>RNN</td>
<td>LayerNorm</td>
<td>Stable</td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>LayerNorm</td>
<td>Standard choice</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-34" class="level4">
<h4 class="anchored" data-anchor-id="complexity-34">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(H)\)</span> per sample</li>
<li>Memory: <span class="math inline">\(O(H)\)</span> for mean, variance</li>
<li>Inference: identical cost</li>
</ul>
<p>Layer Normalization is the steadying hand of modern deep networks, ensuring consistent activations, even when the batch is small or the sequence is long.</p>
</section>
</section>
<section id="gradient-clipping" class="level3">
<h3 class="anchored" data-anchor-id="gradient-clipping">936. Gradient Clipping</h3>
<p>Gradient Clipping is a stabilization technique that prevents gradients from becoming too large during backpropagation. It’s especially important in deep networks and RNNs, where exploding gradients can cause wild parameter updates and destroy training progress.</p>
<section id="what-problem-are-we-solving-35" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-35">What Problem Are We Solving?</h4>
<p>During training, gradients can sometimes explode, grow exponentially through many layers or time steps. This happens when multiplying large weights repeatedly in the chain rule.</p>
<p>If gradients become huge:</p>
<ul>
<li>The optimizer takes giant steps, overshooting the minimum.</li>
<li>The loss becomes NaN due to numerical overflow.</li>
<li>Learning diverges completely.</li>
</ul>
<p>Gradient clipping solves this by limiting the gradient’s magnitude before updating weights.</p>
</section>
<section id="the-core-idea-5" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-5">The Core Idea</h4>
<p>Let <span class="math inline">\(g\)</span> be the full gradient vector. Compute its L2 norm: <span class="math display">\[
|g|_2 = \sqrt{\sum_i g_i^2}
\]</span></p>
<p>If <span class="math inline">\(|g|_2\)</span> exceeds a threshold <span class="math inline">\(c\)</span>, scale it down: <span class="math display">\[
g' = g \cdot \frac{c}{|g|_2}
\]</span></p>
<p>Then use <span class="math inline">\(g'\)</span> for the update.</p>
<p>This ensures: <span class="math display">\[
|g'|_2 \le c
\]</span></p>
</section>
<section id="how-does-it-work-plain-language-35" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-35">How Does It Work (Plain Language)?</h4>
<p>Imagine trying to steer a car downhill, if you push the gas too hard, you’ll spin out of control. Gradient clipping is like setting a speed limiter: it allows motion, but caps it at a safe velocity.</p>
<p>If gradients are small → do nothing. If they explode → scale them back smoothly to the target range.</p>
</section>
<section id="step-by-step-summary-11" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-11">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 41%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute gradients</td>
<td><span class="math inline">\(g = \nabla_\theta L\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute norm</td>
<td><span class="math inline">\(|g|_2 = \sqrt{\sum_i g_i^2}\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compare with threshold <span class="math inline">\(c\)</span></td>
<td>Typical <span class="math inline">\(c = 1.0\)</span> or <span class="math inline">\(5.0\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>If too large, rescale</td>
<td><span class="math inline">\(g \gets g \cdot \frac{c}{|g|_2}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update parameters</td>
<td><span class="math inline">\(\theta \gets \theta - \eta g\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-23" class="level4">
<h4 class="anchored" data-anchor-id="example-23">Example</h4>
<p>Suppose: <span class="math display">\[
g = [3, 4], \quad |g|_2 = 5
\]</span> and threshold <span class="math inline">\(c = 2\)</span>.</p>
<p>Then: <span class="math display">\[
g' = g \cdot \frac{2}{5} = [1.2, 1.6]
\]</span> Now <span class="math inline">\(|g'|_2 = 2\)</span>, safely within the limit.</p>
</section>
<section id="variants" class="level4">
<h4 class="anchored" data-anchor-id="variants">Variants</h4>
<ol type="1">
<li><p>Global norm clipping Applies the same scaling factor to all parameters.</p>
<p><span class="math display">\[
g' = g \cdot \frac{c}{\max(|g|_2, c)}
\]</span></p></li>
<li><p>Per-layer or per-parameter clipping Each tensor’s gradient is clipped individually.</p></li>
<li><p>Value clipping Clamp each gradient component: <span class="math display">\[
g_i' = \text{clip}(g_i, -c, c)
\]</span></p></li>
</ol>
</section>
<section id="tiny-code-easy-versions-33" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-33">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clip_gradients(grads, clip_value<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> np.sqrt(<span class="bu">sum</span>(np.<span class="bu">sum</span>(g  <span class="dv">2</span>) <span class="cf">for</span> g <span class="kw">in</span> grads))</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm <span class="op">&gt;</span> clip_value:</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> clip_value <span class="op">/</span> norm</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        grads <span class="op">=</span> [g <span class="op">*</span> scale <span class="cf">for</span> g <span class="kw">in</span> grads]</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grads</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb66"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;math.h&gt;</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> clip_gradient<span class="op">(</span><span class="dt">double</span><span class="op">*</span> g<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">double</span> c<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> norm <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> norm <span class="op">+=</span> g<span class="op">[</span>i<span class="op">]</span> <span class="op">*</span> g<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> sqrt<span class="op">(</span>norm<span class="op">);</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>norm <span class="op">&gt;</span> c<span class="op">)</span> <span class="op">{</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>        <span class="dt">double</span> scale <span class="op">=</span> c <span class="op">/</span> norm<span class="op">;</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> g<span class="op">[</span>i<span class="op">]</span> <span class="op">*=</span> scale<span class="op">;</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-35" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-35">Why It Matters</h4>
<ul>
<li>Prevents exploding gradients, especially in RNNs.</li>
<li>Makes training stable for deep networks.</li>
<li>Allows larger learning rates safely.</li>
<li>Used with optimizers like SGD, Adam, and RMSProp.</li>
</ul>
<p>Without clipping, long-sequence models like LSTMs and Transformers would be nearly impossible to train reliably.</p>
</section>
<section id="a-gentle-proof-why-it-works-35" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-35">A Gentle Proof (Why It Works)</h4>
<p>Clipping constrains the update step size:</p>
<p><span class="math display">\[
|\Delta \theta|_2 = \eta |g'|_2 \le \eta c
\]</span></p>
<p>Thus, even if raw gradients explode, the parameter change per step remains bounded, preventing numerical instability.</p>
<p>It doesn’t bias directions significantly (since scaling is uniform), only magnitudes.</p>
</section>
<section id="try-it-yourself-35" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-35">Try It Yourself</h4>
<ol type="1">
<li>Train an RNN on long sequences with and without clipping.</li>
<li>Observe loss divergence when not clipped.</li>
<li>Try <span class="math inline">\(c = 0.1, 1.0, 5.0\)</span>.</li>
<li>Plot <span class="math inline">\(|g|_2\)</span> per step, clipping caps spikes.</li>
<li>Combine with Adam, clipping still effective.</li>
</ol>
</section>
<section id="test-cases-35" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-35">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Threshold <span class="math inline">\(c\)</span></th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RNN</td>
<td>None</td>
<td>Exploding gradients</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>1.0</td>
<td>Stable</td>
</tr>
<tr class="odd">
<td>LSTM</td>
<td>5.0</td>
<td>Smooth training</td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>1.0</td>
<td>Standard setup</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-35" class="level4">
<h4 class="anchored" data-anchor-id="complexity-35">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(P)\)</span> (sum and rescale)</li>
<li>Memory: negligible</li>
<li>Effect: stabilizes updates without harming convergence</li>
</ul>
<p>Gradient clipping is a small, almost invisible trick, but it’s the difference between chaos and control in deep learning training.</p>
</section>
</section>
<section id="early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping">937. Early Stopping</h3>
<p>Early Stopping is a simple yet powerful regularization technique that halts training when performance on a validation set stops improving. It prevents overfitting by saving the model at its best generalization point, before it starts memorizing the training data.</p>
<section id="what-problem-are-we-solving-36" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-36">What Problem Are We Solving?</h4>
<p>When training deep networks, loss on the training set usually keeps decreasing, but loss on the validation set may start to rise after a certain number of epochs. This is the point where the model begins to overfit, learning noise instead of structure.</p>
<p>Instead of guessing the right number of epochs, Early Stopping automatically detects this turning point and freezes the model there.</p>
</section>
<section id="the-core-idea-6" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-6">The Core Idea</h4>
<p>Monitor validation loss <span class="math inline">\(L_{\text{val}}\)</span> over epochs:</p>
<ol type="1">
<li><p>Keep track of the best validation loss so far: <span class="math display">\[
L^* = \min_t L_{\text{val}}(t)
\]</span></p></li>
<li><p>If loss hasn’t improved for <span class="math inline">\(p\)</span> epochs (the patience parameter), stop training.</p></li>
</ol>
<p>Formally:</p>
<p><span class="math display">\[
\text{if } L_{\text{val}}(t) &gt; L^* - \delta \text{ for } p \text{ epochs, stop.}
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(\delta\)</span> = minimum improvement required</li>
<li><span class="math inline">\(p\)</span> = patience (tolerance before stopping)</li>
</ul>
</section>
<section id="how-does-it-work-plain-language-36" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-36">How Does It Work (Plain Language)?</h4>
<p>Think of training like baking bread. If you bake too little, it’s undercooked (underfitting). If you bake too long, it burns (overfitting). Early stopping watches the oven and stops at just the right moment, when validation loss smells perfect.</p>
<p>It’s a form of implicit regularization, no penalty term, just an intelligent pause.</p>
</section>
<section id="step-by-step-summary-12" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-12">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 30%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Split data</td>
<td>Training + validation sets</td>
</tr>
<tr class="even">
<td>2</td>
<td>Train model</td>
<td>Update weights per epoch</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Evaluate on validation</td>
<td>Compute <span class="math inline">\(L_{\text{val}}\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Track best score</td>
<td>Save model when validation improves</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Apply patience</td>
<td>Wait a few epochs before stopping</td>
</tr>
<tr class="even">
<td>6</td>
<td>Restore best weights</td>
<td>Revert to snapshot with lowest validation loss</td>
</tr>
</tbody>
</table>
</section>
<section id="example-24" class="level4">
<h4 class="anchored" data-anchor-id="example-24">Example</h4>
<p>Suppose we train for 100 epochs:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Epoch</th>
<th>Train Loss</th>
<th>Val Loss</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1–10</td>
<td>↓</td>
<td>↓</td>
<td>Improving</td>
</tr>
<tr class="even">
<td>11–30</td>
<td>↓</td>
<td>Plateau</td>
<td>Watch</td>
</tr>
<tr class="odd">
<td>31–40</td>
<td>↓</td>
<td>↑</td>
<td>Stop!</td>
</tr>
</tbody>
</table>
<p>If patience = 10, we stop after epoch 40 and restore weights from epoch 30, the best generalization point.</p>
</section>
<section id="tiny-code-easy-versions-34" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-34">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>best_val <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>patience, wait <span class="op">=</span> <span class="dv">10</span>, <span class="dv">0</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>best_weights <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>    train_one_epoch(model)</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> evaluate(model)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_loss <span class="op">&lt;</span> best_val <span class="op">-</span> <span class="fl">1e-4</span>:</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>        best_val <span class="op">=</span> val_loss</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>        best_weights <span class="op">=</span> model.copy_weights()</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>        wait <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>        wait <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> wait <span class="op">&gt;=</span> patience:</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Early stopping at epoch"</span>, epoch)</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>        model.load_weights(best_weights)</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb68"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> best_val <span class="op">=</span> <span class="fl">1e9</span><span class="op">;</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> patience <span class="op">=</span> <span class="dv">10</span><span class="op">,</span> wait <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> epoch <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> epoch <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> epoch<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> val_loss <span class="op">=</span> evaluate_model<span class="op">();</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>val_loss <span class="op">&lt;</span> best_val <span class="op">-</span> <span class="fl">1e-4</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        best_val <span class="op">=</span> val_loss<span class="op">;</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        save_model<span class="op">();</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>        wait <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>        wait<span class="op">++;</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>wait <span class="op">&gt;=</span> patience<span class="op">)</span> <span class="op">{</span></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">"Early stopping at epoch </span><span class="sc">%d\n</span><span class="st">"</span><span class="op">,</span> epoch<span class="op">);</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>        load_best_model<span class="op">();</span></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span><span class="op">;</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-36" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-36">Why It Matters</h4>
<ul>
<li>Prevents overfitting without tuning extra hyperparameters.</li>
<li>Automatically picks the best epoch for generalization.</li>
<li>Reduces training time, stops before diminishing returns.</li>
<li>Common in deep learning frameworks (e.g.&nbsp;<code>Keras.callbacks.EarlyStopping</code>).</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-36" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-36">A Gentle Proof (Why It Works)</h4>
<p>Generalization error <span class="math inline">\(E_g\)</span> typically follows a U-shaped curve:</p>
<p><span class="math display">\[
E_g(t) = E_{\text{train}}(t) + \text{overfit}(t)
\]</span></p>
<p>Early stopping halts before the overfit term dominates, capturing the minimum of <span class="math inline">\(E_g(t)\)</span>: <span class="math display">\[
t^* = \arg\min_t E_{\text{val}}(t)
\]</span></p>
<p>This acts like L2 regularization in effect, it limits the total weight growth by limiting optimization time.</p>
</section>
<section id="try-it-yourself-36" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-36">Try It Yourself</h4>
<ol type="1">
<li>Train a small MLP on MNIST with patience = 5, 10, 20.</li>
<li>Plot training and validation loss.</li>
<li>Watch how longer patience slightly increases overfitting.</li>
<li>Combine with dropout for extra regularization.</li>
<li>Compare with L2 weight decay, similar effects, different mechanisms.</li>
</ol>
</section>
<section id="test-cases-36" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-36">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Early Stopping</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MLP</td>
<td>None</td>
<td>Overfits after ~40 epochs</td>
</tr>
<tr class="even">
<td>MLP</td>
<td>Patience=10</td>
<td>Best validation at epoch 30</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td>Patience=5</td>
<td>Stops early, best accuracy</td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>Enabled</td>
<td>Stable convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-36" class="level4">
<h4 class="anchored" data-anchor-id="complexity-36">Complexity</h4>
<ul>
<li>Time: saves time by halting early</li>
<li>Memory: <span class="math inline">\(O(P)\)</span> (snapshot of best weights)</li>
<li>Hyperparameters: patience, min delta</li>
</ul>
<p>Early Stopping is the simplest kind of wisdom in machine learning, it doesn’t fight overfitting, it just knows when to walk away.</p>
</section>
</section>
<section id="weight-decay" class="level3">
<h3 class="anchored" data-anchor-id="weight-decay">938. Weight Decay</h3>
<p>Weight Decay (also known as L2 Regularization) penalizes large parameter values by adding a small term to the loss function that discourages overly complex models. It gently “shrinks” weights toward zero, preventing overfitting and improving generalization.</p>
<section id="what-problem-are-we-solving-37" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-37">What Problem Are We Solving?</h4>
<p>In neural networks, large weights can cause the model to memorize training data or become unstable during optimization. We want the model to remain smooth, where small changes in input don’t cause large swings in output.</p>
<p>Weight decay adds a penalty proportional to the squared magnitude of the weights, steering optimization toward simpler solutions.</p>
</section>
<section id="the-core-idea-7" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-7">The Core Idea</h4>
<p>We modify the objective function from just the data loss <span class="math inline">\(L(\theta)\)</span> to include a regularization term:</p>
<p><span class="math display">\[
L_{\text{total}}(\theta) = L(\theta) + \frac{\lambda}{2} |\theta|_2^2
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\theta\)</span> = vector of all model parameters</li>
<li><span class="math inline">\(\lambda\)</span> = weight decay coefficient (regularization strength)</li>
</ul>
<p>The gradient update becomes:</p>
<p><span class="math display">\[
\theta \gets \theta - \eta (\nabla_\theta L(\theta) + \lambda \theta)
\]</span></p>
<p>The extra term <span class="math inline">\(\lambda \theta\)</span> acts like a spring, pulling weights gently toward zero at each step.</p>
</section>
<section id="how-does-it-work-plain-language-37" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-37">How Does It Work (Plain Language)?</h4>
<p>Imagine your weights as rubber bands stretched in different directions by training. Weight decay constantly tugs them back toward the origin, not hard enough to stop learning, just enough to prevent them from stretching too far.</p>
<p>The result:</p>
<ul>
<li>Simpler functions</li>
<li>Smoother decision boundaries</li>
<li>Better generalization</li>
</ul>
</section>
<section id="step-by-step-summary-13" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-13">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 18%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Define loss</td>
<td><span class="math inline">\(L_{\text{total}} = L + \frac{\lambda}{2}|\theta|^2\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Compute gradient</td>
<td><span class="math inline">\(\nabla_\theta L_{\text{total}} = \nabla_\theta L + \lambda \theta\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update weights</td>
<td><span class="math inline">\(\theta \gets \theta - \eta \nabla_\theta L_{\text{total}}\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Repeat</td>
<td>Continue training with regularized updates</td>
</tr>
</tbody>
</table>
</section>
<section id="example-25" class="level4">
<h4 class="anchored" data-anchor-id="example-25">Example</h4>
<p>Suppose: <span class="math display">\[
L(\theta) = (y - \theta x)^2, \quad \lambda = 0.1
\]</span></p>
<p>Then the total loss is: <span class="math display">\[
L_{\text{total}}(\theta) = (y - \theta x)^2 + 0.05 \theta^2
\]</span></p>
<p>Gradient: <span class="math display">\[
\frac{dL_{\text{total}}}{d\theta} = -2x(y - \theta x) + 0.1 \theta
\]</span></p>
<p>So even if the data term is small, <span class="math inline">\(\theta\)</span> will still slowly shrink toward zero.</p>
</section>
<section id="tiny-code-easy-versions-35" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-35">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> weight_decay_update(theta, grad, lr<span class="op">=</span><span class="fl">0.01</span>, wd<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta <span class="op">-</span> lr <span class="op">*</span> (grad <span class="op">+</span> wd <span class="op">*</span> theta)</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>])</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>])</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>new_theta <span class="op">=</span> weight_decay_update(theta, grad)</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(new_theta)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb70"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> weight_decay_update<span class="op">(</span><span class="dt">double</span><span class="op">*</span> theta<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> grad<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">double</span> lr<span class="op">,</span> <span class="dt">double</span> wd<span class="op">)</span> <span class="op">{</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>        theta<span class="op">[</span>i<span class="op">]</span> <span class="op">-=</span> lr <span class="op">*</span> <span class="op">(</span>grad<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> wd <span class="op">*</span> theta<span class="op">[</span>i<span class="op">]);</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-37" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-37">Why It Matters</h4>
<ul>
<li>Prevents overfitting by penalizing large weights.</li>
<li>Improves generalization and smooths decision boundaries.</li>
<li>Works well with SGD and most optimizers.</li>
<li>Helps optimization by avoiding sharp minima.</li>
</ul>
<p>Note: In AdamW, weight decay is applied <em>separately</em> (decoupled) from the gradient update for better consistency.</p>
</section>
<section id="a-gentle-proof-why-it-works-37" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-37">A Gentle Proof (Why It Works)</h4>
<p>Regularization modifies the optimization trajectory.</p>
<p>For quadratic loss: <span class="math display">\[
\min_\theta |X\theta - y|^2 + \lambda |\theta|^2
\]</span> has the closed-form solution: <span class="math display">\[
\theta^* = (X^\top X + \lambda I)^{-1} X^\top y
\]</span></p>
<p>Here, <span class="math inline">\(\lambda I\)</span> dampens directions with small eigenvalues, reducing sensitivity to noise, equivalent to ridge regression.</p>
</section>
<section id="try-it-yourself-37" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-37">Try It Yourself</h4>
<ol type="1">
<li><p>Train a linear regression with and without weight decay.</p></li>
<li><p>Plot weights, with decay, they remain smaller and smoother.</p></li>
<li><p>Tune <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li>Too small → overfit</li>
<li>Too large → underfit</li>
</ul></li>
<li><p>Combine with dropout or early stopping.</p></li>
<li><p>Observe training vs validation accuracy difference.</p></li>
</ol>
</section>
<section id="test-cases-37" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-37">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Regularization</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>None</td>
<td>Overfits noise</td>
</tr>
<tr class="even">
<td>Linear Regression</td>
<td>L2 (λ=0.1)</td>
<td>Smooths weights</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td>λ=1e-4</td>
<td>Better generalization</td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>λ=0.01</td>
<td>Common default</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-37" class="level4">
<h4 class="anchored" data-anchor-id="complexity-37">Complexity</h4>
<ul>
<li>Time: negligible (<span class="math inline">\(O(P)\)</span>)</li>
<li>Memory: negligible (same parameter size)</li>
<li>Effect: gradual shrinkage toward smoother models</li>
</ul>
<p>Weight decay is the quiet discipline of neural networks, a small tug that keeps learning balanced between flexibility and restraint.</p>
</section>
</section>
<section id="learning-rate-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate-scheduling">939. Learning Rate Scheduling</h3>
<p>Learning Rate Scheduling dynamically adjusts the optimizer’s learning rate during training to balance exploration (large steps) and convergence (small steps). It is a crucial technique for achieving faster convergence, smoother optimization, and better generalization.</p>
<section id="what-problem-are-we-solving-38" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-38">What Problem Are We Solving?</h4>
<p>A fixed learning rate <span class="math inline">\(\eta\)</span> can cause problems:</p>
<ul>
<li>If <span class="math inline">\(\eta\)</span> is too high, training oscillates or diverges.</li>
<li>If <span class="math inline">\(\eta\)</span> is too low, convergence is painfully slow or stuck in local minima.</li>
</ul>
<p>The solution: start with a large learning rate for exploration and gradually decrease it as training stabilizes. This allows the optimizer to take big steps early, then refine carefully near minima.</p>
</section>
<section id="the-core-idea-8" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-8">The Core Idea</h4>
<p>We modify the learning rate <span class="math inline">\(\eta_t\)</span> as a function of time (epoch or step):</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \nabla_\theta L(\theta_t)
\]</span></p>
<p>where <span class="math inline">\(\eta_t\)</span> is updated by a schedule function.</p>
<p>Common schedules:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 62%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Schedule</th>
<th>Formula</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Step Decay</td>
<td><span class="math inline">\(\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}\)</span></td>
<td>Drop every <span class="math inline">\(s\)</span> epochs</td>
</tr>
<tr class="even">
<td>Exponential Decay</td>
<td><span class="math inline">\(\eta_t = \eta_0 e^{-kt}\)</span></td>
<td>Smooth exponential decrease</td>
</tr>
<tr class="odd">
<td>Cosine Annealing</td>
<td><span class="math inline">\(\eta_t = \eta_{\min} + \frac{1}{2}(\eta_0 - \eta_{\min})(1 + \cos(\pi t / T))\)</span></td>
<td>Gradual warm-to-cool</td>
</tr>
<tr class="even">
<td>Linear Decay</td>
<td><span class="math inline">\(\eta_t = \eta_0 (1 - t/T)\)</span></td>
<td>Simple linear decline</td>
</tr>
<tr class="odd">
<td>Cyclical (CLR)</td>
<td>Oscillates between <span class="math inline">\(\eta_{\min}\)</span> and <span class="math inline">\(\eta_{\max}\)</span></td>
<td>Periodic exploration</td>
</tr>
<tr class="even">
<td>Warmup</td>
<td>Gradually increase <span class="math inline">\(\eta_t\)</span> in early epochs</td>
<td>Prevents instability</td>
</tr>
</tbody>
</table>
</section>
<section id="how-does-it-work-plain-language-38" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-38">How Does It Work (Plain Language)?</h4>
<p>Think of training like hiking down a mountain. At first, you take big confident steps to move quickly. As you approach the valley, you slow down, adjusting carefully to avoid overshooting the minimum.</p>
<p>Learning rate scheduling does the same, it controls how fast you “walk” through the loss landscape.</p>
</section>
<section id="step-by-step-summary-14" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-14">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 49%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Choose base learning rate <span class="math inline">\(\eta_0\)</span></td>
<td>Often 0.1 or 0.001</td>
</tr>
<tr class="even">
<td>2</td>
<td>Select schedule type</td>
<td>Step, exponential, cosine, etc.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update <span class="math inline">\(\eta_t\)</span> at each step</td>
<td>According to chosen rule</td>
</tr>
<tr class="even">
<td>4</td>
<td>Pass <span class="math inline">\(\eta_t\)</span> to optimizer</td>
<td>Adjust step size dynamically</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Monitor training</td>
<td>Verify stable convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="example-step-decay" class="level4">
<h4 class="anchored" data-anchor-id="example-step-decay">Example: Step Decay</h4>
<p><span class="math display">\[
\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}
\]</span></p>
<p>If <span class="math inline">\(\eta_0 = 0.1\)</span>, <span class="math inline">\(\gamma = 0.5\)</span>, <span class="math inline">\(s = 10\)</span>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Epoch</th>
<th>Learning Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0–9</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>10–19</td>
<td>0.05</td>
</tr>
<tr class="odd">
<td>20–29</td>
<td>0.025</td>
</tr>
</tbody>
</table>
</section>
<section id="example-cosine-annealing" class="level4">
<h4 class="anchored" data-anchor-id="example-cosine-annealing">Example: Cosine Annealing</h4>
<p>Smoothly decreases and restarts periodically:</p>
<p><span class="math display">\[
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_0 - \eta_{\min})(1 + \cos(\pi t / T))
\]</span></p>
<p>Gives a gentle oscillation, large early steps, small near minima, then restarts for exploration.</p>
</section>
<section id="tiny-code-easy-versions-36" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-36">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cosine_annealing_lr(t, T, eta_min<span class="op">=</span><span class="fl">1e-5</span>, eta_max<span class="op">=</span><span class="fl">1e-2</span>):</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> eta_min <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (eta_max <span class="op">-</span> eta_min) <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> np.cos(np.pi <span class="op">*</span> t <span class="op">/</span> T))</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">10</span>):</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(epoch, cosine_annealing_lr(epoch, <span class="dv">100</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;math.h&gt;</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> cosine_annealing_lr<span class="op">(</span><span class="dt">int</span> t<span class="op">,</span> <span class="dt">int</span> T<span class="op">,</span> <span class="dt">double</span> eta_min<span class="op">,</span> <span class="dt">double</span> eta_max<span class="op">)</span> <span class="op">{</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> eta_min <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">(</span>eta_max <span class="op">-</span> eta_min<span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="dv">1</span> <span class="op">+</span> cos<span class="op">(</span>M_PI <span class="op">*</span> t <span class="op">/</span> T<span class="op">));</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-38" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-38">Why It Matters</h4>
<ul>
<li>Improves convergence, faster and smoother than constant <span class="math inline">\(\eta\)</span>.</li>
<li>Reduces overfitting, smaller steps near minima generalize better.</li>
<li>Enables large initial learning rates safely.</li>
<li>Used in most modern training pipelines (e.g.&nbsp;ResNet, Transformers).</li>
</ul>
<p>Schedulers are not just about speed, they shape how the optimizer explores and settles in the loss surface.</p>
</section>
<section id="a-gentle-proof-why-it-works-38" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-38">A Gentle Proof (Why It Works)</h4>
<p>In convex optimization, convergence speed depends on <span class="math inline">\(\eta_t\)</span>:</p>
<p><span class="math display">\[
L(\theta_t) - L^* \le \frac{C}{\sum_{i=1}^t \eta_i}
\]</span></p>
<p>Thus, decreasing <span class="math inline">\(\eta_t\)</span> ensures the cumulative step sizes converge, stabilizing updates as the model nears the optimum.</p>
<p>For cosine schedules, periodic restarts avoid getting stuck in flat minima, providing implicit regularization.</p>
</section>
<section id="try-it-yourself-38" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-38">Try It Yourself</h4>
<ol type="1">
<li><p>Train a CNN with constant vs scheduled learning rates.</p></li>
<li><p>Compare convergence speed and validation accuracy.</p></li>
<li><p>Try:</p>
<ul>
<li>Step decay (<span class="math inline">\(\gamma=0.5\)</span> every 10 epochs)</li>
<li>Cosine annealing</li>
<li>Cyclical (CLR)</li>
</ul></li>
<li><p>Plot <span class="math inline">\(\eta_t\)</span> across epochs, visualize the schedule’s shape.</p></li>
<li><p>Combine with warmup for Transformers (e.g.&nbsp;increase <span class="math inline">\(\eta\)</span> linearly for first 5% of steps).</p></li>
</ol>
</section>
<section id="test-cases-38" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-38">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Schedule</th>
<th>Model</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Constant</td>
<td>CNN</td>
<td>Slow, plateau early</td>
</tr>
<tr class="even">
<td>Step Decay</td>
<td>CNN</td>
<td>Sharp convergence</td>
</tr>
<tr class="odd">
<td>Cosine Annealing</td>
<td>ResNet</td>
<td>Smooth decay, best accuracy</td>
</tr>
<tr class="even">
<td>Cyclical</td>
<td>Transformer</td>
<td>Recurrent exploration</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-38" class="level4">
<h4 class="anchored" data-anchor-id="complexity-38">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(1)\)</span> per step (lightweight update)</li>
<li>Memory: negligible</li>
<li>Benefit: faster, more stable convergence</li>
</ul>
<p>Learning rate scheduling is like giving your optimizer intuition, knowing when to rush, when to slow, and when to take a deep breath before climbing again.</p>
</section>
</section>
<section id="residual-connections" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections">940. Residual Connections</h3>
<p>Residual Connections (also called skip connections) allow information to bypass one or more layers in a neural network. They were introduced in ResNet (2015) to solve the problem of vanishing gradients and enable the successful training of very deep networks, sometimes hundreds or even thousands of layers deep.</p>
<section id="what-problem-are-we-solving-39" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-39">What Problem Are We Solving?</h4>
<p>As networks get deeper:</p>
<ul>
<li>Gradients can vanish or explode during backpropagation.</li>
<li>Training error may increase even when the model has more capacity.</li>
<li>Deeper networks may learn slower or get stuck in poor local minima.</li>
</ul>
<p>Residual connections fix this by providing shortcut paths that let gradients flow more directly through the network.</p>
</section>
<section id="the-core-idea-9" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-9">The Core Idea</h4>
<p>A standard layer learns a mapping:</p>
<p><span class="math display">\[
y = \mathcal{F}(x)
\]</span></p>
<p>A residual layer instead learns:</p>
<p><span class="math display">\[
y = \mathcal{F}(x) + x
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathcal{F}(x)\)</span> = residual mapping (e.g.&nbsp;convolution, batch norm, activation)</li>
<li><span class="math inline">\(x\)</span> = identity shortcut (input)</li>
</ul>
<p>During backpropagation, gradients can flow both through <span class="math inline">\(\mathcal{F}(x)\)</span> and directly through <span class="math inline">\(x\)</span>, avoiding vanishing.</p>
</section>
<section id="how-does-it-work-plain-language-39" class="level4">
<h4 class="anchored" data-anchor-id="how-does-it-work-plain-language-39">How Does It Work (Plain Language)?</h4>
<p>Imagine building a tower of blocks, each layer adds a modification to what came before. Residual connections let some blocks pass their output straight up, untouched. This keeps information fresh and prevents earlier knowledge from fading as the tower grows taller.</p>
<p>In other words, layers don’t have to learn the <em>entire</em> transformation, they only need to learn the difference (the residual).</p>
</section>
<section id="step-by-step-summary-15" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-15">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Input</td>
<td><span class="math inline">\(x\)</span> enters the block</td>
</tr>
<tr class="even">
<td>2</td>
<td>Transform</td>
<td>Apply operations to compute <span class="math inline">\(\mathcal{F}(x)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Skip</td>
<td>Pass <span class="math inline">\(x\)</span> directly to the output</td>
</tr>
<tr class="even">
<td>4</td>
<td>Combine</td>
<td>Add: <span class="math inline">\(y = \mathcal{F}(x) + x\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Next block</td>
<td>Feed <span class="math inline">\(y\)</span> forward to the next layer</td>
</tr>
</tbody>
</table>
</section>
<section id="example-26" class="level4">
<h4 class="anchored" data-anchor-id="example-26">Example</h4>
<p>In a simple feedforward block: <span class="math display">\[
\mathcal{F}(x) = W_2 \sigma(W_1 x)
\]</span> then: <span class="math display">\[
y = W_2 \sigma(W_1 x) + x
\]</span></p>
<p>If <span class="math inline">\(\mathcal{F}(x)\)</span> becomes very small (close to zero), the block simply passes input forward: <span class="math inline">\(y \approx x\)</span>, making it easy to train identity mappings when needed.</p>
</section>
<section id="tiny-code-easy-versions-37" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-37">Tiny Code (Easy Versions)</h4>
<p>Python (NumPy)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x): <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> residual_block(x, W1, W2):</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> relu(x <span class="op">@</span> W1)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> F <span class="op">@</span> W2</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F <span class="op">+</span> x  <span class="co"># skip connection</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb74"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> residual_block<span class="op">(</span><span class="dt">double</span><span class="op">*</span> x<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> W1<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> W2<span class="op">,</span> <span class="dt">double</span><span class="op">*</span> y<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">int</span> m<span class="op">)</span> <span class="op">{</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Compute F(x)</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span><span class="op">*</span> F <span class="op">=</span> malloc<span class="op">(</span><span class="kw">sizeof</span><span class="op">(</span><span class="dt">double</span><span class="op">)</span> <span class="op">*</span> n<span class="op">);</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        F<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> m<span class="op">;</span> j<span class="op">++)</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>            F<span class="op">[</span>i<span class="op">]</span> <span class="op">+=</span> x<span class="op">[</span>j<span class="op">]</span> <span class="op">*</span> W1<span class="op">[</span>j <span class="op">*</span> n <span class="op">+</span> i<span class="op">];</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>F<span class="op">[</span>i<span class="op">]</span> <span class="op">&lt;</span> <span class="dv">0</span><span class="op">)</span> F<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> <span class="co">// ReLU</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Second linear + skip connection</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> F<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        y<span class="op">[</span>i<span class="op">]</span> <span class="op">+=</span> x<span class="op">[</span>i<span class="op">];</span> <span class="co">// add skip</span></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    free<span class="op">(</span>F<span class="op">);</span></span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-39" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-39">Why It Matters</h4>
<ul>
<li>Fixes vanishing gradients: gradients flow through identity path.</li>
<li>Enables ultra-deep networks: ResNets up to 1000+ layers train reliably.</li>
<li>Improves generalization: encourages small, incremental refinements.</li>
<li>Simplifies learning: layers learn <em>residual corrections</em>, not full mappings.</li>
<li>Ubiquitous in modern architectures: ResNet, DenseNet, Transformers.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-39" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-39">A Gentle Proof (Why It Works)</h4>
<p>During backpropagation, the gradient flows through both paths:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \left( \frac{\partial \mathcal{F}(x)}{\partial x} + I \right)
\]</span></p>
<p>The identity term (<span class="math inline">\(I\)</span>) ensures that the gradient never becomes zero, even if <span class="math inline">\(\frac{\partial \mathcal{F}(x)}{\partial x}\)</span> vanishes, maintaining stable gradient flow across deep stacks.</p>
</section>
<section id="variants-1" class="level4">
<h4 class="anchored" data-anchor-id="variants-1">Variants</h4>
<ol type="1">
<li><p>Pre-activation ResNet Apply BatchNorm and ReLU <em>before</em> <span class="math inline">\(\mathcal{F}(x)\)</span> for smoother training. <span class="math display">\[
y = x + \mathcal{F}(\text{BN}(\text{ReLU}(x)))
\]</span></p></li>
<li><p>Projection shortcut (for dimension mismatch) If <span class="math inline">\(\mathcal{F}(x)\)</span> changes dimensions: <span class="math display">\[
y = \mathcal{F}(x) + W_s x
\]</span> where <span class="math inline">\(W_s\)</span> is a <span class="math inline">\(1 \times 1\)</span> convolution or linear projection.</p></li>
</ol>
</section>
<section id="try-it-yourself-39" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-39">Try It Yourself</h4>
<ol type="1">
<li>Train a 10-layer MLP with and without skip connections, note which converges faster.</li>
<li>Visualize gradient norms per layer.</li>
<li>Try stacking 100+ residual blocks.</li>
<li>Replace addition with concatenation (DenseNet-style).</li>
<li>Combine with normalization and activation ordering (pre-activation).</li>
</ol>
</section>
<section id="test-cases-39" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-39">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Architecture</th>
<th>Residuals</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10-layer MLP</td>
<td>None</td>
<td>Vanishing gradients</td>
</tr>
<tr class="even">
<td>10-layer MLP</td>
<td>Skip connections</td>
<td>Stable</td>
</tr>
<tr class="odd">
<td>100-layer CNN</td>
<td>None</td>
<td>Diverges</td>
</tr>
<tr class="even">
<td>100-layer CNN</td>
<td>Residual</td>
<td>Trains successfully</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-39" class="level4">
<h4 class="anchored" data-anchor-id="complexity-39">Complexity</h4>
<ul>
<li>Time: negligible overhead (just addition)</li>
<li>Memory: adds one buffer for skip tensor</li>
<li>Effect: stabilizes training of deep models</li>
</ul>
<p>Residual connections turned depth from a problem into a superpower, they let networks grow taller, learn faster, and see farther without forgetting where they came from.</p>
</section>
</section>
</section>
<section id="section-95.-sequence-models" class="level1">
<h1>Section 95. Sequence Models</h1>
<section id="hidden-markov-model-forwardbackward-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="hidden-markov-model-forwardbackward-algorithm">941. Hidden Markov Model (Forward–Backward Algorithm)</h3>
<p>The Hidden Markov Model (HMM) is a foundational probabilistic model for sequential data, where we observe outcomes that depend on hidden (unseen) states. The Forward–Backward algorithm allows us to efficiently compute probabilities of sequences and estimate hidden states over time.</p>
<section id="what-problem-are-we-solving-40" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-40">What Problem Are We Solving?</h4>
<p>Suppose we observe a sequence of outputs (like words, sounds, or sensor readings), but we suspect these were generated by hidden internal states, for example:</p>
<ul>
<li>In speech recognition: hidden phonemes → observed sounds</li>
<li>In finance: hidden market regimes → observed prices</li>
<li>In biology: hidden gene states → observed nucleotide sequences</li>
</ul>
<p>We want to compute:</p>
<ol type="1">
<li>The likelihood of the observed sequence given the model parameters.</li>
<li>The posterior probabilities of hidden states over time.</li>
</ol>
<p>That’s what the Forward–Backward algorithm does.</p>
</section>
<section id="the-core-idea-10" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-10">The Core Idea</h4>
<p>We define:</p>
<ul>
<li>Hidden states: <span class="math inline">\(S = {s_1, s_2, \dots, s_N}\)</span></li>
<li>Observations: <span class="math inline">\(O = (o_1, o_2, \dots, o_T)\)</span></li>
<li>Transition probabilities: <span class="math inline">\(A_{ij} = P(s_j \mid s_i)\)</span></li>
<li>Emission probabilities: <span class="math inline">\(B_j(o_t) = P(o_t \mid s_j)\)</span></li>
<li>Initial probabilities: <span class="math inline">\(\pi_i = P(s_i \text{ at } t=1)\)</span></li>
</ul>
<p>We want <span class="math inline">\(P(O \mid \text{model})\)</span>.</p>
</section>
<section id="step-1-forward-pass-α-values" class="level4">
<h4 class="anchored" data-anchor-id="step-1-forward-pass-α-values">Step 1: Forward Pass (α values)</h4>
<p>Compute probability of partial observation up to time <span class="math inline">\(t\)</span> and ending in state <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
\alpha_t(i) = P(o_1, o_2, \dots, o_t, s_i \mid \text{model})
\]</span></p>
<p>Recurrence:</p>
<p><span class="math display">\[
\alpha_t(i) = \left[\sum_{j=1}^N \alpha_{t-1}(j) A_{ji}\right] B_i(o_t)
\]</span></p>
<p>Initialize:</p>
<p><span class="math display">\[
\alpha_1(i) = \pi_i B_i(o_1)
\]</span></p>
</section>
<section id="step-2-backward-pass-β-values" class="level4">
<h4 class="anchored" data-anchor-id="step-2-backward-pass-β-values">Step 2: Backward Pass (β values)</h4>
<p>Compute probability of the remaining observations from <span class="math inline">\(t+1\)</span> to <span class="math inline">\(T\)</span>, given state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[
\beta_t(i) = P(o_{t+1}, \dots, o_T \mid s_i, \text{model})
\]</span></p>
<p>Recurrence:</p>
<p><span class="math display">\[
\beta_t(i) = \sum_{j=1}^N A_{ij} B_j(o_{t+1}) \beta_{t+1}(j)
\]</span></p>
<p>Initialize:</p>
<p><span class="math display">\[
\beta_T(i) = 1
\]</span></p>
</section>
<section id="step-3-combine-posterior-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="step-3-combine-posterior-probabilities">Step 3: Combine (Posterior Probabilities)</h4>
<p>The probability of being in state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[
\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{\sum_{k=1}^N \alpha_t(k) \beta_t(k)}
\]</span></p>
<p>The total sequence likelihood:</p>
<p><span class="math display">\[
P(O \mid \text{model}) = \sum_{i=1}^N \alpha_T(i)
\]</span></p>
</section>
<section id="how-it-works-plain-language" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language">How It Works (Plain Language)</h4>
<p>The forward pass collects evidence from the past, “How likely are we to reach this state given everything so far?” The backward pass collects evidence from the future, “How likely are we to see the rest of the data given we are here now?”</p>
<p>By multiplying them, we see the full picture: the probability that each hidden state was active at each time step.</p>
</section>
<section id="tiny-code-easy-versions-38" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-38">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_backward(A, B, pi, O):</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> <span class="bu">len</span>(O)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> np.zeros((T, N))</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    alpha[<span class="dv">0</span>] <span class="op">=</span> pi <span class="op">*</span> B[:, O[<span class="dv">0</span>]]</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>        alpha[t] <span class="op">=</span> (alpha[t<span class="op">-</span><span class="dv">1</span>] <span class="op">@</span> A) <span class="op">*</span> B[:, O[t]]</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward</span></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.zeros((T, N))</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    beta[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T<span class="op">-</span><span class="dv">1</span>)):</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>        beta[t] <span class="op">=</span> (A <span class="op">@</span> (B[:, O[t<span class="op">+</span><span class="dv">1</span>]] <span class="op">*</span> beta[t<span class="op">+</span><span class="dv">1</span>]))</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior</span></span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">=</span> alpha <span class="op">*</span> beta</span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">/=</span> gamma.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb75-22"><a href="#cb75-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha, beta, gamma</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb76"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">[</span><span class="dv">0</span><span class="op">][</span>i<span class="op">]</span> <span class="op">=</span> pi<span class="op">[</span>i<span class="op">]</span> <span class="op">*</span> B<span class="op">[</span>i<span class="op">][</span>O<span class="op">[</span><span class="dv">0</span><span class="op">]];</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">[</span>t<span class="op">][</span>i<span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> N<span class="op">;</span> j<span class="op">++)</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">[</span>t<span class="op">][</span>i<span class="op">]</span> <span class="op">+=</span> alpha<span class="op">[</span>t<span class="op">-</span><span class="dv">1</span><span class="op">][</span>j<span class="op">]</span> <span class="op">*</span> A<span class="op">[</span>j<span class="op">][</span>i<span class="op">];</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">[</span>t<span class="op">][</span>i<span class="op">]</span> <span class="op">*=</span> B<span class="op">[</span>i<span class="op">][</span>O<span class="op">[</span>t<span class="op">]];</span></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-40" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-40">Why It Matters</h4>
<ul>
<li>Foundation of probabilistic sequence models, used in speech, NLP, and bioinformatics.</li>
<li>Efficient dynamic programming, avoids exponential complexity.</li>
<li>Forms the basis of EM training (Baum–Welch algorithm).</li>
<li>Interpretable, gives posterior state probabilities over time.</li>
</ul>
<p>Without it, computing probabilities across all possible state paths would take <span class="math inline">\(O(N^T)\)</span> time; this reduces it to <span class="math inline">\(O(N^2T)\)</span>.</p>
</section>
<section id="a-gentle-proof-why-it-works-40" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-40">A Gentle Proof (Why It Works)</h4>
<p>Because the model is Markovian, the probability of reaching a state depends only on the previous state, not the entire history. This allows recursive computation of partial probabilities (forward) and remaining probabilities (backward).</p>
<p>The product <span class="math inline">\(\alpha_t(i)\beta_t(i)\)</span> covers both halves of the sequence, before and after time <span class="math inline">\(t\)</span>, giving the full joint probability.</p>
</section>
<section id="try-it-yourself-40" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-40">Try It Yourself</h4>
<ol type="1">
<li>Build a simple weather model (Sunny/Rainy).</li>
<li>Define <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(\pi\)</span>, and observation sequence.</li>
<li>Run the forward–backward algorithm.</li>
<li>Plot <span class="math inline">\(\gamma_t(i)\)</span> for each state, see how hidden states fluctuate.</li>
<li>Compare with known labels to verify interpretation.</li>
</ol>
</section>
<section id="test-cases-40" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-40">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 27%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>States</th>
<th>Observations</th>
<th>Goal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weather</td>
<td>Rainy, Sunny</td>
<td>Walk, Shop, Clean</td>
<td>Infer most likely weather</td>
</tr>
<tr class="even">
<td>Speech</td>
<td>Phonemes</td>
<td>Audio features</td>
<td>Compute state probabilities</td>
</tr>
<tr class="odd">
<td>DNA</td>
<td>Hidden motifs</td>
<td>Nucleotide symbols</td>
<td>Find likely motif positions</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-40" class="level4">
<h4 class="anchored" data-anchor-id="complexity-40">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N^2T)\)</span></li>
<li>Space: <span class="math inline">\(O(NT)\)</span></li>
<li>Benefit: Efficient inference in hidden state models</li>
</ul>
<p>The Forward–Backward algorithm is how we “listen” to hidden processes, blending past and future evidence to decode what we can’t directly see.</p>
</section>
</section>
<section id="viterbi-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="viterbi-algorithm">942. Viterbi Algorithm</h3>
<p>The Viterbi algorithm is a dynamic programming method used to find the most probable sequence of hidden states in a Hidden Markov Model (HMM), given a sequence of observations. It is the backbone of modern speech recognition, part-of-speech tagging, gene sequence decoding, and many other sequence labeling tasks.</p>
<section id="what-problem-are-we-solving-41" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-41">What Problem Are We Solving?</h4>
<p>In an HMM, many different state sequences could produce the same observations. We want to find the single most likely path of hidden states:</p>
<p><span class="math display">\[
S^* = \arg\max_S P(S \mid O, \text{model})
\]</span></p>
<p>Naively, this requires checking all possible paths, an exponential number in <span class="math inline">\(T\)</span>. The Viterbi algorithm makes this efficient using dynamic programming.</p>
</section>
<section id="the-core-idea-11" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-11">The Core Idea</h4>
<p>We recursively compute the maximum probability of any path ending in a given state at each time step.</p>
<p>Define:</p>
<ul>
<li><span class="math inline">\(A_{ij} = P(s_j \mid s_i)\)</span> (state transition)</li>
<li><span class="math inline">\(B_j(o_t) = P(o_t \mid s_j)\)</span> (emission)</li>
<li><span class="math inline">\(\pi_i = P(s_i)\)</span> (initial state)</li>
<li>Observations <span class="math inline">\(O = (o_1, o_2, \dots, o_T)\)</span></li>
</ul>
<p>We define:</p>
<p><span class="math display">\[
\delta_t(i) = \max_{s_1, \dots, s_{t-1}} P(s_1, \dots, s_t = i, o_1, \dots, o_t \mid \text{model})
\]</span></p>
<p>and a backpointer <span class="math inline">\(\psi_t(i)\)</span> to remember which state led to the best path.</p>
</section>
<section id="step-by-step-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-algorithm">Step-by-Step Algorithm</h4>
<ol type="1">
<li>Initialization</li>
</ol>
<p><span class="math display">\[
\delta_1(i) = \pi_i B_i(o_1), \quad \psi_1(i) = 0
\]</span></p>
<ol start="2" type="1">
<li>Recursion</li>
</ol>
<p>For each time <span class="math inline">\(t = 2, \dots, T\)</span>:</p>
<p><span class="math display">\[
\delta_t(i) = \max_j [\delta_{t-1}(j) A_{ji}] , B_i(o_t)
\]</span></p>
<p>and record the argmax:</p>
<p><span class="math display">\[
\psi_t(i) = \arg\max_j [\delta_{t-1}(j) A_{ji}]
\]</span></p>
<ol start="3" type="1">
<li>Termination</li>
</ol>
<p><span class="math display">\[
P^* = \max_i \delta_T(i), \quad s_T^* = \arg\max_i \delta_T(i)
\]</span></p>
<ol start="4" type="1">
<li>Path Backtracking</li>
</ol>
<p>For <span class="math inline">\(t = T-1, T-2, \dots, 1\)</span>:</p>
<p><span class="math display">\[
s_t^* = \psi_{t+1}(s_{t+1}^*)
\]</span></p>
</section>
<section id="how-it-works-plain-language-1" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-1">How It Works (Plain Language)</h4>
<p>Think of it like navigating a maze where each junction represents a hidden state and each step an observation. At each step, you only keep the best path that led to each possible state, discarding all others. By the end, you just walk backward through the saved pointers to recover the most likely route.</p>
</section>
<section id="example-27" class="level4">
<h4 class="anchored" data-anchor-id="example-27">Example</h4>
<p>Let’s say we have two hidden states: Rainy (R) and Sunny (S), and observed actions: Walk, Shop, Clean.</p>
<p>We can compute the most likely weather sequence that explains these actions using the Viterbi algorithm, just like teaching a model to “guess the sky” from someone’s habits.</p>
</section>
<section id="tiny-code-easy-versions-39" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-39">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> viterbi(A, B, pi, O):</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> <span class="bu">len</span>(O)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> np.zeros((T, N))</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    psi <span class="op">=</span> np.zeros((T, N), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialization</span></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>    delta[<span class="dv">0</span>] <span class="op">=</span> pi <span class="op">*</span> B[:, O[<span class="dv">0</span>]]</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recursion</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>            seq_probs <span class="op">=</span> delta[t<span class="op">-</span><span class="dv">1</span>] <span class="op">*</span> A[:, i]</span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>            psi[t, i] <span class="op">=</span> np.argmax(seq_probs)</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>            delta[t, i] <span class="op">=</span> np.<span class="bu">max</span>(seq_probs) <span class="op">*</span> B[i, O[t]]</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Termination</span></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> np.zeros(T, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>    states[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> np.argmax(delta[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T<span class="op">-</span><span class="dv">1</span>)):</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>        states[t] <span class="op">=</span> psi[t<span class="op">+</span><span class="dv">1</span>, states[t<span class="op">+</span><span class="dv">1</span>]]</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> states</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb78"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">[</span><span class="dv">0</span><span class="op">][</span>i<span class="op">]</span> <span class="op">=</span> pi<span class="op">[</span>i<span class="op">]</span> <span class="op">*</span> B<span class="op">[</span>i<span class="op">][</span>O<span class="op">[</span><span class="dv">0</span><span class="op">]];</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>        <span class="dt">double</span> max_val <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> argmax <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> N<span class="op">;</span> j<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>            <span class="dt">double</span> val <span class="op">=</span> delta<span class="op">[</span>t<span class="op">-</span><span class="dv">1</span><span class="op">][</span>j<span class="op">]</span> <span class="op">*</span> A<span class="op">[</span>j<span class="op">][</span>i<span class="op">];</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>val <span class="op">&gt;</span> max_val<span class="op">)</span> <span class="op">{</span> max_val <span class="op">=</span> val<span class="op">;</span> argmax <span class="op">=</span> j<span class="op">;</span> <span class="op">}</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        delta<span class="op">[</span>t<span class="op">][</span>i<span class="op">]</span> <span class="op">=</span> max_val <span class="op">*</span> B<span class="op">[</span>i<span class="op">][</span>O<span class="op">[</span>t<span class="op">]];</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>        psi<span class="op">[</span>t<span class="op">][</span>i<span class="op">]</span> <span class="op">=</span> argmax<span class="op">;</span></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-41" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-41">Why It Matters</h4>
<ul>
<li>Efficient decoding, reduces exponential search to <span class="math inline">\(O(N^2T)\)</span>.</li>
<li>Most probable state sequence, not just per-state probabilities.</li>
<li>Used everywhere, speech, POS tagging, gesture recognition, DNA sequencing.</li>
<li>Foundation for structured prediction in sequence models.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-41" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-41">A Gentle Proof (Why It Works)</h4>
<p>We use dynamic programming: At each time <span class="math inline">\(t\)</span>, the optimal path to state <span class="math inline">\(i\)</span> must come from the optimal path to some previous state <span class="math inline">\(j\)</span> at time <span class="math inline">\(t-1\)</span>.</p>
<p>Formally, the Bellman optimality principle ensures:</p>
<p><span class="math display">\[
\max_{s_1,\dots,s_t} P(s_1,\dots,s_t,o_1,\dots,o_t) =
\max_j [\max_{s_1,\dots,s_{t-1}} P(s_1,\dots,s_{t-1},o_1,\dots,o_{t-1}) P(s_t=i \mid s_{t-1}=j) P(o_t \mid s_t)]
\]</span></p>
<p>This recursion allows efficient accumulation of probabilities.</p>
</section>
<section id="try-it-yourself-41" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-41">Try It Yourself</h4>
<ol type="1">
<li>Define a 2-state (Rainy, Sunny) HMM with 3 observations (Walk, Shop, Clean).</li>
<li>Run the Viterbi algorithm manually or in Python.</li>
<li>Track <span class="math inline">\(\delta_t(i)\)</span> at each step in a table.</li>
<li>Recover the best state path and compare with your intuition.</li>
<li>Visualize the path probabilities, see where the decision flips.</li>
</ol>
</section>
<section id="test-cases-41" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-41">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 22%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>States</th>
<th>Observations</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weather model</td>
<td>Rainy, Sunny</td>
<td>Walk, Shop, Clean</td>
<td>[Rainy, Rainy, Sunny]</td>
</tr>
<tr class="even">
<td>Speech phonemes</td>
<td>Hidden phonemes</td>
<td>Acoustic signals</td>
<td>Most likely phoneme sequence</td>
</tr>
<tr class="odd">
<td>DNA decoding</td>
<td>Hidden regions</td>
<td>Base pairs</td>
<td>Gene region sequence</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-41" class="level4">
<h4 class="anchored" data-anchor-id="complexity-41">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N^2T)\)</span></li>
<li>Space: <span class="math inline">\(O(NT)\)</span></li>
<li>Output: single most probable hidden state sequence</li>
</ul>
<p>The Viterbi algorithm turns uncertainty into structure, instead of guessing, it reconstructs the hidden path that best explains the story behind every observed sequence.</p>
</section>
</section>
<section id="baumwelch-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="baumwelch-algorithm">943. Baum–Welch Algorithm</h3>
<p>The Baum–Welch algorithm is the classic way to train a Hidden Markov Model (HMM) when the hidden states are unknown. It is an instance of the Expectation–Maximization (EM) algorithm, alternating between estimating expected counts of transitions and emissions, and updating model parameters accordingly.</p>
<p>It allows an HMM to <em>learn from data</em>, discovering transition probabilities, emission probabilities, and initial distributions that best explain the observed sequences.</p>
<section id="what-problem-are-we-solving-42" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-42">What Problem Are We Solving?</h4>
<p>Suppose we have a dataset of observed sequences <span class="math inline">\(O = (o_1, o_2, \dots, o_T)\)</span> but no knowledge of which hidden states produced them. We want to learn the HMM parameters <span class="math inline">\(\lambda = (A, B, \pi)\)</span> that maximize the likelihood:</p>
<p><span class="math display">\[
P(O \mid \lambda)
\]</span></p>
<p>Directly optimizing this is infeasible, there are exponentially many hidden state sequences. Baum–Welch solves this by using expected counts of transitions and emissions, computed via the Forward–Backward algorithm.</p>
</section>
<section id="the-core-idea-12" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-12">The Core Idea</h4>
<p>Each iteration has two steps:</p>
<ol type="1">
<li>Expectation (E-step) Compute the expected number of times each transition or emission occurred, given the current model.</li>
<li>Maximization (M-step) Re-estimate the model parameters <span class="math inline">\(A, B, \pi\)</span> using those expected counts.</li>
</ol>
<p>Repeat until convergence.</p>
</section>
<section id="step-1-compute-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="step-1-compute-probabilities">Step 1: Compute Probabilities</h4>
<p>Using Forward–Backward from before, compute:</p>
<ul>
<li><p>Forward probability:</p>
<p><span class="math display">\[
\alpha_t(i) = P(o_1, \dots, o_t, s_t = i \mid \lambda)
\]</span></p></li>
<li><p>Backward probability:</p>
<p><span class="math display">\[
\beta_t(i) = P(o_{t+1}, \dots, o_T \mid s_t = i, \lambda)
\]</span></p></li>
</ul>
<p>Then define two expected quantities:</p>
<ol type="1">
<li><p>State occupancy probability:</p>
<p><span class="math display">\[
\gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O \mid \lambda)}
\]</span></p></li>
<li><p>Transition probability:</p>
<p><span class="math display">\[
\xi_t(i,j) = \frac{\alpha_t(i) A_{ij} B_j(o_{t+1}) \beta_{t+1}(j)}{P(O \mid \lambda)}
\]</span></p></li>
</ol>
</section>
<section id="step-2-re-estimation-formulas" class="level4">
<h4 class="anchored" data-anchor-id="step-2-re-estimation-formulas">Step 2: Re-estimation Formulas</h4>
<p>With <span class="math inline">\(\gamma_t(i)\)</span> and <span class="math inline">\(\xi_t(i,j)\)</span> computed, update:</p>
<ul>
<li><p>Initial state distribution:</p>
<p><span class="math display">\[
\pi_i' = \gamma_1(i)
\]</span></p></li>
<li><p>Transition matrix:</p>
<p><span class="math display">\[
A_{ij}' = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
\]</span></p></li>
<li><p>Emission probabilities:</p>
<p><span class="math display">\[
B_j'(k) = \frac{\sum_{t: o_t = k} \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}
\]</span></p></li>
</ul>
<p>Repeat until <span class="math inline">\(P(O \mid \lambda)\)</span> converges (or increases by less than a small <span class="math inline">\(\epsilon\)</span>).</p>
</section>
<section id="how-it-works-plain-language-2" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-2">How It Works (Plain Language)</h4>
<p>Think of Baum–Welch as guessing, explaining, and adjusting:</p>
<ol type="1">
<li>Guess the model parameters.</li>
<li>Explain the data, use Forward–Backward to estimate which hidden states likely generated each observation.</li>
<li>Adjust, reassign probability mass to transitions and emissions that better match those hidden-state expectations.</li>
</ol>
<p>Each cycle refines the model, making the explanation more plausible.</p>
</section>
<section id="example-28" class="level4">
<h4 class="anchored" data-anchor-id="example-28">Example</h4>
<p>For a weather model:</p>
<ul>
<li>Hidden states: Rainy, Sunny</li>
<li>Observations: Walk, Shop, Clean</li>
</ul>
<p>If the algorithm sees many “Walk” observations, it gradually increases <span class="math inline">\(P(\text{Walk} \mid \text{Sunny})\)</span>. If “Clean” often follows “Walk,” it adjusts transition <span class="math inline">\(P(\text{Sunny} \to \text{Rainy})\)</span>. This iterative learning continues until the model stabilizes.</p>
</section>
<section id="tiny-code-simplified" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified">Tiny Code (Simplified)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> baum_welch(O, N, M, max_iters<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> <span class="bu">len</span>(O)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.full((N, N), <span class="fl">1.0</span> <span class="op">/</span> N)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> np.full((N, M), <span class="fl">1.0</span> <span class="op">/</span> M)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> np.full(N, <span class="fl">1.0</span> <span class="op">/</span> N)</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> np.zeros((T, N))</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>        beta <span class="op">=</span> np.zeros((T, N))</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward</span></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>        alpha[<span class="dv">0</span>] <span class="op">=</span> pi <span class="op">*</span> B[:, O[<span class="dv">0</span>]]</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>            alpha[t] <span class="op">=</span> (alpha[t<span class="op">-</span><span class="dv">1</span>] <span class="op">@</span> A) <span class="op">*</span> B[:, O[t]]</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward</span></span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>        beta[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T<span class="op">-</span><span class="dv">1</span>)):</span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>            beta[t] <span class="op">=</span> (A <span class="op">@</span> (B[:, O[t<span class="op">+</span><span class="dv">1</span>]] <span class="op">*</span> beta[t<span class="op">+</span><span class="dv">1</span>]))</span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gamma &amp; Xi</span></span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>        denom <span class="op">=</span> (alpha[<span class="op">-</span><span class="dv">1</span>].<span class="bu">sum</span>())</span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>        gamma <span class="op">=</span> (alpha <span class="op">*</span> beta) <span class="op">/</span> denom</span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a>        xi <span class="op">=</span> np.zeros((T<span class="op">-</span><span class="dv">1</span>, N, N))</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>            xi[t] <span class="op">=</span> (alpha[t][:, <span class="va">None</span>] <span class="op">*</span> A <span class="op">*</span> B[:, O[t<span class="op">+</span><span class="dv">1</span>]] <span class="op">*</span> beta[t<span class="op">+</span><span class="dv">1</span>]) <span class="op">/</span> denom</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Re-estimate</span></span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>        pi <span class="op">=</span> gamma[<span class="dv">0</span>]</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> xi.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> gamma[:<span class="op">-</span><span class="dv">1</span>].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)[:, <span class="va">None</span>]</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>                mask <span class="op">=</span> np.array(O) <span class="op">==</span> k</span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a>                B[j, k] <span class="op">=</span> gamma[mask, j].<span class="bu">sum</span>() <span class="op">/</span> gamma[:, j].<span class="bu">sum</span>()</span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, B, pi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-42" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-42">Why It Matters</h4>
<ul>
<li>Teaches HMMs to learn from data, without labeled states.</li>
<li>Foundation for speech recognition, NLP tagging, and biological sequence modeling.</li>
<li>Unsupervised EM framework, interpretable and statistically sound.</li>
<li>Used before neural sequence models dominated modern AI.</li>
</ul>
<p>Even today, it remains a conceptual foundation for EM and latent-variable models.</p>
</section>
<section id="a-gentle-proof-why-it-works-42" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-42">A Gentle Proof (Why It Works)</h4>
<p>Baum–Welch maximizes a lower bound on <span class="math inline">\(\log P(O \mid \lambda)\)</span> using the EM principle.</p>
<ul>
<li>E-step: compute expected sufficient statistics of hidden variables given <span class="math inline">\(\lambda^{(old)}\)</span>.</li>
<li>M-step: maximize expected log-likelihood over <span class="math inline">\(\lambda\)</span>.</li>
</ul>
<p>Each iteration guarantees:</p>
<p><span class="math display">\[
P(O \mid \lambda^{(new)}) \ge P(O \mid \lambda^{(old)})
\]</span></p>
<p>This monotonic improvement continues until convergence.</p>
</section>
<section id="try-it-yourself-42" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-42">Try It Yourself</h4>
<ol type="1">
<li>Generate a synthetic sequence from a known HMM.</li>
<li>Erase the hidden states.</li>
<li>Initialize random <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(\pi\)</span>.</li>
<li>Run Baum–Welch for several iterations.</li>
<li>Compare learned parameters with the true ones.</li>
</ol>
<p>You’ll see the model “rediscover” the hidden dynamics from the observations alone.</p>
</section>
<section id="test-cases-42" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-42">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Example</th>
<th>Hidden States</th>
<th>Observations</th>
<th>Goal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weather</td>
<td>Rainy, Sunny</td>
<td>Walk, Shop, Clean</td>
<td>Learn <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(\pi\)</span></td>
</tr>
<tr class="even">
<td>DNA</td>
<td>Gene/Non-Gene</td>
<td>Nucleotides</td>
<td>Estimate transition + emission structure</td>
</tr>
<tr class="odd">
<td>POS Tagging</td>
<td>Noun, Verb, Adj</td>
<td>Words</td>
<td>Learn emission patterns</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-42" class="level4">
<h4 class="anchored" data-anchor-id="complexity-42">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N^2T)\)</span> per iteration</li>
<li>Space: <span class="math inline">\(O(NT)\)</span></li>
<li>Convergence: monotonic but not guaranteed to global optimum</li>
</ul>
<p>Baum–Welch is the “teacher” of Hidden Markov Models, it listens to sequences, guesses the unseen causes, and refines its understanding until the hidden structure of the world becomes clear.</p>
</section>
</section>
<section id="beam-search" class="level3">
<h3 class="anchored" data-anchor-id="beam-search">944. Beam Search</h3>
<p>Beam Search is a heuristic search algorithm widely used in sequence decoding, especially in speech recognition, neural machine translation, and text generation. It is a clever compromise between greedy search (fast but myopic) and exhaustive search (accurate but expensive), balancing efficiency and quality by exploring only the most promising candidate paths at each step.</p>
<section id="what-problem-are-we-solving-43" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-43">What Problem Are We Solving?</h4>
<p>Many sequence models (like RNNs, Transformers, or HMMs) generate outputs step by step, predicting the next symbol based on prior ones. The number of possible sequences grows exponentially with length, making exact decoding infeasible.</p>
<p>We want to find a high-probability output sequence without exploring every possible path. Beam Search achieves this by keeping only the top <span class="math inline">\(k\)</span> candidates (the “beam”) at each step.</p>
</section>
<section id="the-core-idea-13" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-13">The Core Idea</h4>
<p>Let the model define conditional probabilities:</p>
<p><span class="math display">\[
P(y_1, y_2, \dots, y_T) = \prod_{t=1}^{T} P(y_t \mid y_{&lt;t})
\]</span></p>
<p>At each decoding step <span class="math inline">\(t\)</span>, instead of taking only the single most likely token (greedy), or all tokens (exhaustive), Beam Search keeps the <span class="math inline">\(k\)</span> most probable partial sequences according to their cumulative log probability.</p>
<p>Formally, define:</p>
<ul>
<li>Beam width <span class="math inline">\(k\)</span> (e.g., 3–10)</li>
<li>Cumulative log probability:</li>
</ul>
<p><span class="math display">\[
\text{score}(y_{1:t}) = \sum_{i=1}^t \log P(y_i \mid y_{&lt;i})
\]</span></p>
<p>At each step, expand all candidates by one token, compute new scores, and keep only the best <span class="math inline">\(k\)</span>.</p>
</section>
<section id="step-by-step-summary-16" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-16">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start with initial token (e.g., <code>&lt;start&gt;</code>) and empty beam</td>
</tr>
<tr class="even">
<td>2</td>
<td>Expand each beam by all possible next tokens</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute log probabilities for all expanded sequences</td>
</tr>
<tr class="even">
<td>4</td>
<td>Select top <span class="math inline">\(k\)</span> sequences with highest scores</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat until end-of-sequence <code>&lt;eos&gt;</code> is reached or length limit</td>
</tr>
<tr class="even">
<td>6</td>
<td>Return the sequence with highest score (or normalized score)</td>
</tr>
</tbody>
</table>
</section>
<section id="how-it-works-plain-language-3" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-3">How It Works (Plain Language)</h4>
<p>Imagine you’re guessing a sentence word by word. At each step, you write down the few most likely sentences so far, say the top 3. Then you extend <em>each</em> of those by one new word, compute their new probabilities, and again keep only the best 3 overall. You keep going until the sentence ends.</p>
<p>Beam Search doesn’t guarantee the absolute best sentence, but it almost always finds a very good one, much faster than trying them all.</p>
</section>
<section id="example-29" class="level4">
<h4 class="anchored" data-anchor-id="example-29">Example</h4>
<p>Suppose your model can generate only <code>A</code>, <code>B</code>, or <code>C</code> at each step, and beam width <span class="math inline">\(k=2\)</span>.</p>
<p>At step 1:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Candidate</th>
<th>Prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0.6</td>
</tr>
<tr class="even">
<td>B</td>
<td>0.3</td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.1</td>
</tr>
</tbody>
</table>
<p>Keep top 2 → {A, B}</p>
<p>At step 2 (expanding A and B):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Sequence</th>
<th>Cumulative Prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AA</td>
<td>0.6×0.5=0.30</td>
</tr>
<tr class="even">
<td>AB</td>
<td>0.6×0.3=0.18</td>
</tr>
<tr class="odd">
<td>BA</td>
<td>0.3×0.4=0.12</td>
</tr>
<tr class="even">
<td>BB</td>
<td>0.3×0.6=0.18</td>
</tr>
</tbody>
</table>
<p>Keep top 2 → {AA, AB} Continue until end.</p>
</section>
<section id="tiny-code-easy-versions-40" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-40">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beam_search_step(probs, beam, k):</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    new_beam <span class="op">=</span> []</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> seq, score <span class="kw">in</span> beam:</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> token, p <span class="kw">in</span> <span class="bu">enumerate</span>(probs):</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>            new_beam.append((seq <span class="op">+</span> [token], score <span class="op">+</span> np.log(p)))</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep top k</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    new_beam <span class="op">=</span> <span class="bu">sorted</span>(new_beam, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[:k]</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_beam</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>probs_t1 <span class="op">=</span> [<span class="fl">0.6</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>]</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>beam <span class="op">=</span> [([], <span class="dv">0</span>)]</span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>beam <span class="op">=</span> beam_search_step(probs_t1, beam, <span class="dv">2</span>)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(beam)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb81"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">typedef</span> <span class="kw">struct</span> <span class="op">{</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> seq<span class="op">[</span><span class="dv">100</span><span class="op">];</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> score<span class="op">;</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span> Beam<span class="op">;</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> beam_search_step<span class="op">(</span><span class="dt">double</span> <span class="op">*</span>probs<span class="op">,</span> Beam <span class="op">*</span>beam<span class="op">,</span> Beam <span class="op">*</span>next<span class="op">,</span> <span class="dt">int</span> k<span class="op">,</span> <span class="dt">int</span> vocab_size<span class="op">)</span> <span class="op">{</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> count <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> k<span class="op">;</span> i<span class="op">++)</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> vocab_size<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>            next<span class="op">[</span>count<span class="op">]</span> <span class="op">=</span> beam<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>            next<span class="op">[</span>count<span class="op">].</span>seq<span class="op">[</span>i<span class="op">+</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> t<span class="op">;</span></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>            next<span class="op">[</span>count<span class="op">].</span>score <span class="op">+=</span> log<span class="op">(</span>probs<span class="op">[</span>t<span class="op">]);</span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>            count<span class="op">++;</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">// sort and prune to top k (omitted for brevity)</span></span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-43" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-43">Why It Matters</h4>
<ul>
<li>Balances accuracy and efficiency, between greedy and exhaustive search.</li>
<li>Standard in decoding for neural sequence models.</li>
<li>Allows exploration of multiple hypotheses.</li>
<li>Often used with normalization tricks (e.g., length penalty).</li>
<li>Simple, general, and tunable.</li>
</ul>
<p>Beam width controls the trade-off:</p>
<ul>
<li>Small <span class="math inline">\(k\)</span> → fast but sometimes suboptimal.</li>
<li>Large <span class="math inline">\(k\)</span> → better results but slower decoding.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-43" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-43">A Gentle Proof (Why It Works)</h4>
<p>Beam Search is not optimal (it prunes), but it’s a heuristic approximation to best-first search. By keeping top-<span class="math inline">\(k\)</span> partial hypotheses under cumulative probability, it approximates:</p>
<p><span class="math display">\[
S^* = \arg\max_S \prod_t P(y_t \mid y_{&lt;t})
\]</span></p>
<p>Because log is monotonic, maximizing the log sum yields the same order as maximizing product probabilities.</p>
</section>
<section id="try-it-yourself-43" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-43">Try It Yourself</h4>
<ol type="1">
<li><p>Train a simple RNN for text generation.</p></li>
<li><p>Compare three decoding modes:</p>
<ul>
<li>Greedy (<span class="math inline">\(k=1\)</span>)</li>
<li>Beam (<span class="math inline">\(k=3\)</span>)</li>
<li>Sampling (stochastic decoding)</li>
</ul></li>
<li><p>Observe the trade-off between coherence and diversity.</p></li>
<li><p>Add length normalization: <span class="math display">\[
\text{score}' = \frac{\text{score}}{(5 + |y|)^\alpha / (5 + 1)^\alpha}
\]</span> to prevent short sentences from dominating.</p></li>
</ol>
</section>
<section id="test-cases-43" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-43">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 26%">
<col style="width: 15%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Task</th>
<th>Beam Width</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RNN</td>
<td>Text generation</td>
<td>1</td>
<td>Coherent but dull</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>Text generation</td>
<td>5</td>
<td>More fluent, longer context</td>
</tr>
<tr class="odd">
<td>Transformer</td>
<td>Translation</td>
<td>4–8</td>
<td>State-of-the-art results</td>
</tr>
<tr class="even">
<td>HMM</td>
<td>Sequence labeling</td>
<td>3</td>
<td>Near-optimal decoding</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-43" class="level4">
<h4 class="anchored" data-anchor-id="complexity-43">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(k \cdot T \cdot V)\)</span> (beam width × sequence length × vocabulary size)</li>
<li>Space: <span class="math inline">\(O(k \cdot T)\)</span></li>
<li>Trade-off: quality vs computation</li>
</ul>
<p>Beam Search is the decoder’s workhorse, pruning wisely, it finds the path that feels right, even if it’s not the only one that could have been.</p>
</section>
</section>
<section id="greedy-decoding" class="level3">
<h3 class="anchored" data-anchor-id="greedy-decoding">945. Greedy Decoding</h3>
<p>Greedy Decoding is the simplest decoding method for sequence models. At every time step, it chooses the most probable next token according to the model’s current prediction, without reconsidering earlier choices.</p>
<p>It’s fast, deterministic, and often surprisingly effective, though it can miss globally optimal or more diverse solutions.</p>
<section id="what-problem-are-we-solving-44" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-44">What Problem Are We Solving?</h4>
<p>Given a model that defines a probability over sequences:</p>
<p><span class="math display">\[
P(y_1, y_2, \dots, y_T) = \prod_{t=1}^{T} P(y_t \mid y_{&lt;t})
\]</span></p>
<p>we want to find the output sequence with the highest probability.</p>
<p>Full search is exponential in <span class="math inline">\(T\)</span>, so we approximate it. Greedy decoding picks the best local choice at each step:</p>
<p><span class="math display">\[
y_t^* = \arg\max_y P(y \mid y_{&lt;t})
\]</span></p>
<p>This yields a single sequence, quickly.</p>
</section>
<section id="the-core-idea-14" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-14">The Core Idea</h4>
<p>Instead of exploring multiple candidates (like Beam Search), we take a single path through the sequence, always choosing the next token with the highest conditional probability.</p>
<p>Formally:</p>
<p><span class="math display">\[
\hat{y} = [,y_1^*, y_2^*, \dots, y_T^*,], \quad
y_t^* = \arg\max_y P(y \mid y_{&lt;t})
\]</span></p>
<p>No backtracking, no search, just straightforward next-step prediction.</p>
</section>
<section id="how-it-works-plain-language-4" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-4">How It Works (Plain Language)</h4>
<p>Imagine you’re building a sentence word by word, always picking the word that “feels most right” at that moment. You never look back to fix an earlier mistake, and you never explore alternative phrasings.</p>
<p>Greedy decoding is that, a single straight-line walk through the model’s probability landscape.</p>
</section>
<section id="step-by-step-summary-17" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-17">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 10%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start</td>
<td>Begin with a special <code>&lt;start&gt;</code> token</td>
</tr>
<tr class="even">
<td>2</td>
<td>Predict</td>
<td>Compute next-token probabilities <span class="math inline">\(P(y_t \mid y_{&lt;t})\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Select</td>
<td>Choose the token with the highest probability</td>
</tr>
<tr class="even">
<td>4</td>
<td>Append</td>
<td>Add it to the output sequence</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Continue until <code>&lt;eos&gt;</code> or length limit</td>
</tr>
</tbody>
</table>
</section>
<section id="example-30" class="level4">
<h4 class="anchored" data-anchor-id="example-30">Example</h4>
<p>If your model outputs token probabilities like this:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Token Probabilities</th>
<th>Chosen</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>{“A”: 0.6, “B”: 0.4}</td>
<td>“A”</td>
</tr>
<tr class="even">
<td>2</td>
<td>{“A”: 0.3, “C”: 0.7}</td>
<td>“C”</td>
</tr>
<tr class="odd">
<td>3</td>
<td>{“B”: 0.8, “D”: 0.2}</td>
<td>“B”</td>
</tr>
</tbody>
</table>
<p>Then the greedy-decoded output is: A C B.</p>
<p>It’s fast, but if “A C B” leads to a dead end while “A B B” had higher total probability, greedy decoding won’t find that.</p>
</section>
<section id="tiny-code-easy-versions-41" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-41">Tiny Code (Easy Versions)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, start_token, max_len):</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>    seq <span class="op">=</span> [start_token]</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_len):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> model.predict_next(seq)</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>        next_token <span class="op">=</span> np.argmax(probs)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        seq.append(next_token)</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> next_token <span class="op">==</span> model.eos_token:</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seq</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb83"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> greedy_decode<span class="op">(</span><span class="dt">double</span> <span class="op">(*</span>predict_next<span class="op">)(</span><span class="dt">int</span><span class="op">*,</span> <span class="dt">int</span><span class="op">),</span> <span class="dt">int</span> start<span class="op">,</span> <span class="dt">int</span> eos<span class="op">,</span> <span class="dt">int</span> max_len<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>output<span class="op">)</span> <span class="op">{</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    output<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> start<span class="op">;</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> t <span class="op">&lt;</span> max_len<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> next <span class="op">=</span> argmax<span class="op">(</span>predict_next<span class="op">(</span>output<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>        output<span class="op">[</span>t<span class="op">]</span> <span class="op">=</span> next<span class="op">;</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>next <span class="op">==</span> eos<span class="op">)</span> <span class="cf">break</span><span class="op">;</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-44" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-44">Why It Matters</h4>
<ul>
<li>Fast and simple: no beam management or probability bookkeeping.</li>
<li>Deterministic: same input → same output.</li>
<li>Useful for debugging or baseline generation.</li>
<li>Strong for confident models: works well when probabilities are sharply peaked.</li>
<li>Weak for ambiguous or uncertain models: may miss better long-term paths.</li>
</ul>
<p>Greedy decoding is widely used for inference in models that are well-calibrated, like some classification or captioning systems.</p>
</section>
<section id="a-gentle-proof-why-it-works-and-when-it-doesnt" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-and-when-it-doesnt">A Gentle Proof (Why It Works and When It Doesn’t)</h4>
<p>Let’s define the global optimum as:</p>
<p><span class="math display">\[
y^* = \arg\max_y P(y)
\]</span></p>
<p>and the greedy sequence as:</p>
<p><span class="math display">\[
\hat{y}*t = \arg\max_y P(y_t \mid \hat{y}*{&lt;t})
\]</span></p>
<p>Greedy decoding maximizes <em>locally</em> at each step, not <em>globally</em>. By the inequality:</p>
<p><span class="math display">\[
\max_y \prod_t P(y_t \mid y_{&lt;t}) \neq \prod_t \max_y P(y_t \mid y_{&lt;t})
\]</span></p>
<p>it can fail to reach the global maximum. However, if <span class="math inline">\(P(y_t \mid y_{&lt;t})\)</span> is sharply peaked (low entropy), greedy often approximates the true best sequence.</p>
</section>
<section id="try-it-yourself-44" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-44">Try It Yourself</h4>
<ol type="1">
<li><p>Train a small character-level language model.</p></li>
<li><p>Decode text using:</p>
<ul>
<li>Greedy decoding</li>
<li>Beam Search (<span class="math inline">\(k=3\)</span>)</li>
<li>Sampling (randomized)</li>
</ul></li>
<li><p>Compare output quality and diversity.</p></li>
<li><p>Observe: Greedy outputs are fluent but repetitive; beam outputs are more coherent; sampling is more creative.</p></li>
</ol>
</section>
<section id="test-cases-44" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-44">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Task</th>
<th>Decoding</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Seq2Seq</td>
<td>Translation</td>
<td>Greedy</td>
<td>Fast, sometimes short sentences</td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>Text generation</td>
<td>Greedy</td>
<td>Fluent but deterministic</td>
</tr>
<tr class="odd">
<td>Speech model</td>
<td>Transcription</td>
<td>Greedy</td>
<td>Works well when signal clear</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>Poetry generation</td>
<td>Greedy</td>
<td>Repetitive phrases</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-44" class="level4">
<h4 class="anchored" data-anchor-id="complexity-44">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T \cdot V)\)</span> (same as model inference)</li>
<li>Space: <span class="math inline">\(O(T)\)</span></li>
<li>Trade-off: speed vs exploration</li>
</ul>
<p>Greedy decoding is the quick and confident storyteller, it never doubts its next word, even if a little hesitation might have led to something greater.</p>
</section>
</section>
<section id="connectionist-temporal-classification-ctc" class="level3">
<h3 class="anchored" data-anchor-id="connectionist-temporal-classification-ctc">946. Connectionist Temporal Classification (CTC)</h3>
<p>Connectionist Temporal Classification (CTC) is a training and decoding method designed for sequence-to-sequence tasks where the alignment between inputs and outputs is unknown, like speech recognition, handwriting recognition, or gesture decoding.</p>
<p>It allows neural networks (especially RNNs or Transformers) to map variable-length input sequences to shorter output sequences without explicit frame-level alignment.</p>
<section id="what-problem-are-we-solving-45" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-45">What Problem Are We Solving?</h4>
<p>Suppose we have:</p>
<ul>
<li>Input sequence <span class="math inline">\(X = (x_1, x_2, \dots, x_T)\)</span> (like audio frames)</li>
<li>Target output sequence <span class="math inline">\(Y = (y_1, y_2, \dots, y_L)\)</span> (like letters or words)</li>
</ul>
<p>We don’t know <em>which input frame corresponds to which output token</em>. Traditional supervised learning requires alignment, but labeling every frame is expensive. CTC solves this by marginalizing over all possible alignments between inputs and outputs.</p>
</section>
<section id="the-core-idea-15" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-15">The Core Idea</h4>
<p>CTC introduces an extra “blank” symbol (∅) and allows repetitions of labels. A valid alignment <span class="math inline">\(\pi = (\pi_1, \pi_2, \dots, \pi_T)\)</span> maps to a final output <span class="math inline">\(Y\)</span> by:</p>
<ol type="1">
<li>Removing all blanks (∅).</li>
<li>Collapsing repeated symbols.</li>
</ol>
<p>Example: <span class="math inline">\(\pi = [∅, h, h, ∅, e, e, ∅, l, l, o, ∅]\)</span> → <span class="math inline">\(Y = [h, e, l, o]\)</span></p>
<p>The model predicts a probability distribution over all possible alignments.</p>
<p>The total probability of output <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math display">\[
P(Y \mid X) = \sum_{\pi \in \mathcal{B}^{-1}(Y)} P(\pi \mid X)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}\)</span> is the collapsing function (remove blanks + merge repeats).</p>
</section>
<section id="step-by-step-summary-18" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-18">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Define vocabulary with blank (∅) symbol</td>
</tr>
<tr class="even">
<td>2</td>
<td>Network outputs probabilities <span class="math inline">\(P(\pi_t \mid x_t)\)</span> per frame</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Enumerate all alignments <span class="math inline">\(\pi\)</span> that map to <span class="math inline">\(Y\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Sum probabilities of all valid paths (Forward–Backward algorithm)</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Maximize <span class="math inline">\(\log P(Y \mid X)\)</span> during training</td>
</tr>
</tbody>
</table>
</section>
<section id="the-ctc-objective" class="level4">
<h4 class="anchored" data-anchor-id="the-ctc-objective">The CTC Objective</h4>
<p>For a target sequence <span class="math inline">\(Y = (y_1, \dots, y_L)\)</span>, define an extended sequence by inserting blanks between symbols and at the ends:</p>
<p><span class="math display">\[
\tilde{Y} = (∅, y_1, ∅, y_2, ∅, \dots, y_L, ∅)
\]</span></p>
<p>Then recursively compute forward probabilities <span class="math inline">\(\alpha_t(s)\)</span> over positions <span class="math inline">\(s\)</span> in <span class="math inline">\(\tilde{Y}\)</span>:</p>
<p>Initialization: <span class="math display">\[
\alpha_1(1) = P(∅ \mid x_1), \quad \alpha_1(2) = P(y_1 \mid x_1)
\]</span></p>
<p>Recurrence: <span class="math display">\[
\alpha_t(s) = P(\tilde{Y}*s \mid x_t) \times \left[
\alpha*{t-1}(s) + \alpha_{t-1}(s-1) + \mathbb{1}*{\tilde{Y}*s \ne \tilde{Y}*{s-2}} \alpha*{t-1}(s-2)
\right]
\]</span></p>
<p>Final probability: <span class="math display">\[
P(Y \mid X) = \alpha_T(S-1) + \alpha_T(S)
\]</span> where <span class="math inline">\(S = |\tilde{Y}|\)</span>.</p>
</section>
<section id="how-it-works-plain-language-5" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-5">How It Works (Plain Language)</h4>
<p>Imagine listening to a sentence: some sounds are stretched, some skipped, some blurred. CTC tells the network:</p>
<blockquote class="blockquote">
<p>“Don’t worry about timing. Just make sure that, somewhere in your predictions, the right sequence of symbols appears, in order.”</p>
</blockquote>
<p>The blank symbol allows pauses and variable timing; summing over alignments lets the model learn flexible mappings automatically.</p>
</section>
<section id="example-31" class="level4">
<h4 class="anchored" data-anchor-id="example-31">Example</h4>
<p>If the input has 5 frames and the target is “HI”, the possible alignments include:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Frame Sequence</th>
<th>Collapsed Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[H, H, ∅, I, I]</td>
<td>HI</td>
</tr>
<tr class="even">
<td>[∅, H, ∅, I, ∅]</td>
<td>HI</td>
</tr>
<tr class="odd">
<td>[H, ∅, H, I, ∅]</td>
<td>HI</td>
</tr>
</tbody>
</table>
<p>CTC sums over all of them, not just one.</p>
</section>
<section id="tiny-code-simplified-1" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified-1">Tiny Code (Simplified)</h4>
<p>Python (Pseudo)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ctc_forward(log_probs, target, blank<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    T, V <span class="op">=</span> log_probs.shape</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    target_ext <span class="op">=</span> [blank] <span class="op">+</span> [t <span class="cf">for</span> t <span class="kw">in</span> target <span class="cf">for</span> _ <span class="kw">in</span> (<span class="dv">0</span>, <span class="dv">1</span>)] <span class="op">+</span> [blank]</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> <span class="bu">len</span>(target_ext)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> np.full((T, S), <span class="op">-</span>np.inf)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    alpha[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">=</span> log_probs[<span class="dv">0</span>, blank]</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    alpha[<span class="dv">0</span>, <span class="dv">1</span>] <span class="op">=</span> log_probs[<span class="dv">0</span>, target_ext[<span class="dv">1</span>]]</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(S):</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>            prev <span class="op">=</span> [alpha[t<span class="op">-</span><span class="dv">1</span>, s]]</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> s <span class="op">&gt;</span> <span class="dv">0</span>: prev.append(alpha[t<span class="op">-</span><span class="dv">1</span>, s<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> s <span class="op">&gt;</span> <span class="dv">1</span> <span class="kw">and</span> target_ext[s] <span class="op">!=</span> target_ext[s<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>                prev.append(alpha[t<span class="op">-</span><span class="dv">1</span>, s<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>            alpha[t, s] <span class="op">=</span> np.logaddexp.<span class="bu">reduce</span>(prev) <span class="op">+</span> log_probs[t, target_ext[s]]</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.logaddexp(alpha[T<span class="op">-</span><span class="dv">1</span>, S<span class="op">-</span><span class="dv">1</span>], alpha[T<span class="op">-</span><span class="dv">1</span>, S<span class="op">-</span><span class="dv">2</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb85"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co">// compute alpha[t][s] forward matrix (log-sum-exp form)</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="co">// requires careful numerical stability handling</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-45" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-45">Why It Matters</h4>
<ul>
<li>No need for pre-aligned data, learns timing automatically.</li>
<li>Robust to variable-length inputs and outputs.</li>
<li>Foundation of speech recognition models like DeepSpeech.</li>
<li>Still used in modern architectures (e.g., wav2vec 2.0 pretraining).</li>
<li>Differentiable and trainable end-to-end.</li>
</ul>
<p>Without CTC, many sequence problems (like speech or handwriting) would require labor-intensive frame-level labeling.</p>
</section>
<section id="a-gentle-proof-why-it-works-44" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-44">A Gentle Proof (Why It Works)</h4>
<p>Because each output symbol can appear in multiple time frames, we marginalize over alignments. Using the Forward–Backward algorithm, we efficiently sum the exponentially many paths:</p>
<p><span class="math display">\[
P(Y \mid X) = \sum_{\pi \in \mathcal{B}^{-1}(Y)} \prod_{t=1}^T P(\pi_t \mid x_t)
\]</span></p>
<p>This can be computed recursively in <span class="math inline">\(O(TL)\)</span> time, not <span class="math inline">\(O(V^T)\)</span>.</p>
</section>
<section id="try-it-yourself-45" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-45">Try It Yourself</h4>
<ol type="1">
<li>Generate a toy input of 5 timesteps and target word “CAT”.</li>
<li>Enumerate all possible alignments.</li>
<li>Collapse them using the blank rule.</li>
<li>Compute probabilities manually and verify with CTC forward recursion.</li>
<li>Try training a small RNN to map sine-wave patterns to letter sequences.</li>
</ol>
</section>
<section id="test-cases-45" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-45">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Application</th>
<th>Input</th>
<th>Output</th>
<th>Alignment Known?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Speech recognition</td>
<td>Audio frames</td>
<td>Text</td>
<td>No</td>
</tr>
<tr class="even">
<td>Handwriting recognition</td>
<td>Pen strokes</td>
<td>Text</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Sign language</td>
<td>Video frames</td>
<td>Glosses</td>
<td>No</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-45" class="level4">
<h4 class="anchored" data-anchor-id="complexity-45">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(TL)\)</span></li>
<li>Space: <span class="math inline">\(O(TL)\)</span></li>
<li>Key advantage: Differentiable marginalization over alignments</li>
</ul>
<p>CTC lets neural networks learn <em>what</em> to say without being told <em>when</em> to say it, transforming chaotic sequences into structured meaning, one blank at a time.</p>
</section>
</section>
<section id="attention-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanism">947. Attention Mechanism</h3>
<p>The Attention Mechanism is one of the most transformative ideas in modern AI. It allows a model to focus selectively on the most relevant parts of its input when making a decision, just as humans do when reading, listening, or reasoning.</p>
<p>Originally introduced for neural machine translation, attention is now a core component of Transformers, powering GPT, BERT, and nearly every state-of-the-art model in language, vision, and beyond.</p>
<section id="what-problem-are-we-solving-46" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-46">What Problem Are We Solving?</h4>
<p>Sequence models like RNNs compress all past information into a single hidden vector. This makes it hard for them to remember long contexts, they forget earlier words or events.</p>
<p>We want a model that can:</p>
<ul>
<li>Look back over all previous positions,</li>
<li>Weigh them by relevance,</li>
<li>Combine them dynamically at each step.</li>
</ul>
<p>That’s exactly what attention does.</p>
</section>
<section id="the-core-idea-16" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-16">The Core Idea</h4>
<p>At each decoding step <span class="math inline">\(t\)</span>, the model computes how strongly it should attend to each encoder state <span class="math inline">\(h_i\)</span> (where <span class="math inline">\(i = 1, 2, \dots, T\)</span>).</p>
<p>We define:</p>
<ul>
<li>Query vector <span class="math inline">\(q_t\)</span> (from the decoder state)</li>
<li>Key vectors <span class="math inline">\(k_i\)</span> (from encoder states)</li>
<li>Value vectors <span class="math inline">\(v_i\)</span> (information to extract)</li>
</ul>
<p>The attention weights <span class="math inline">\(\alpha_{t,i}\)</span> measure the relevance of <span class="math inline">\(h_i\)</span> to <span class="math inline">\(q_t\)</span>:</p>
<p><span class="math display">\[
e_{t,i} = \text{score}(q_t, k_i)
\]</span></p>
<p>Then normalize via softmax:</p>
<p><span class="math display">\[
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
\]</span></p>
<p>Finally, compute the context vector as the weighted average:</p>
<p><span class="math display">\[
c_t = \sum_i \alpha_{t,i} v_i
\]</span></p>
<p>The decoder uses <span class="math inline">\(c_t\)</span> as additional information to predict the next token.</p>
</section>
<section id="common-scoring-functions" class="level4">
<h4 class="anchored" data-anchor-id="common-scoring-functions">Common Scoring Functions</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 52%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Formula</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dot</td>
<td><span class="math inline">\(e_{t,i} = q_t^\top k_i\)</span></td>
<td>Simple, fast</td>
</tr>
<tr class="even">
<td>Scaled Dot</td>
<td><span class="math inline">\(e_{t,i} = \frac{q_t^\top k_i}{\sqrt{d_k}}\)</span></td>
<td>Used in Transformers</td>
</tr>
<tr class="odd">
<td>Additive (Bahdanau)</td>
<td><span class="math inline">\(e_{t,i} = v_a^\top \tanh(W_q q_t + W_k k_i)\)</span></td>
<td>Learnable combination</td>
</tr>
</tbody>
</table>
</section>
<section id="how-it-works-plain-language-6" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-6">How It Works (Plain Language)</h4>
<p>Imagine translating a sentence word by word. When translating “bank,” the model should look closely at the nearby words, “river” or “loan”, to decide its meaning. Instead of relying on a single memory, attention lets the model look back at <em>all</em> words, weigh them, and use that context for its current decision.</p>
<p>Attention turns sequence modeling from a linear memory into a differentiable lookup table, the model “consults” all previous information on demand.</p>
</section>
<section id="step-by-step-summary-19" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-19">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Compute similarity <span class="math inline">\(e_{t,i}\)</span> between query <span class="math inline">\(q_t\)</span> and each key <span class="math inline">\(k_i\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Apply softmax to get attention weights <span class="math inline">\(\alpha_{t,i}\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute weighted sum <span class="math inline">\(c_t = \sum_i \alpha_{t,i} v_i\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Combine <span class="math inline">\(c_t\)</span> with decoder state to generate next output</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat for each decoding step</td>
</tr>
</tbody>
</table>
</section>
<section id="example-32" class="level4">
<h4 class="anchored" data-anchor-id="example-32">Example</h4>
<p>For a simple translation model: Input sentence: “Le chat dort.” Output so far: “The cat …” When predicting “sleeps,” the attention weights peak at “dort,” allowing the model to pull the right context.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Source</th>
<th>Attention Weight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Le</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>chat</td>
<td>0.20</td>
</tr>
<tr class="odd">
<td>dort</td>
<td>0.75</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-easy-versions-42" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-easy-versions-42">Tiny Code (Easy Versions)</h4>
<p>Python (Scaled Dot-Product Attention)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V):</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> K.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Q <span class="op">@</span> K.T <span class="op">/</span> np.sqrt(d_k)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.exp(scores).<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights <span class="op">@</span> V, weights</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb87"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> attention<span class="op">(</span><span class="dt">double</span> <span class="op">*</span>Q<span class="op">,</span> <span class="dt">double</span> <span class="op">*</span>K<span class="op">,</span> <span class="dt">double</span> <span class="op">*</span>V<span class="op">,</span> <span class="dt">double</span> <span class="op">*</span>out<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">int</span> d<span class="op">)</span> <span class="op">{</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">double</span> scale <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> sqrt<span class="op">((</span><span class="dt">double</span><span class="op">)</span>d<span class="op">);</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>        <span class="dt">double</span> sum <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> n<span class="op">;</span> j<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>            <span class="dt">double</span> score <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> d<span class="op">;</span> k<span class="op">++)</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>                score <span class="op">+=</span> Q<span class="op">[</span>i<span class="op">*</span>d <span class="op">+</span> k<span class="op">]</span> <span class="op">*</span> K<span class="op">[</span>j<span class="op">*</span>d <span class="op">+</span> k<span class="op">];</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>            score <span class="op">*=</span> scale<span class="op">;</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>            <span class="dt">double</span> expv <span class="op">=</span> exp<span class="op">(</span>score<span class="op">);</span></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>            sum <span class="op">+=</span> expv<span class="op">;</span></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>            out<span class="op">[</span>i<span class="op">*</span>d <span class="op">+</span> k<span class="op">]</span> <span class="op">+=</span> <span class="op">(</span>expv <span class="op">/</span> sum<span class="op">)</span> <span class="op">*</span> V<span class="op">[</span>j<span class="op">*</span>d <span class="op">+</span> k<span class="op">];</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-46" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-46">Why It Matters</h4>
<ul>
<li>Solves long-term dependency problems in RNNs.</li>
<li>Fully differentiable, trained end-to-end.</li>
<li>Interpretable, attention weights visualize what the model “looks at.”</li>
<li>Forms the heart of the Transformer architecture (via multi-head self-attention).</li>
<li>General mechanism, works for language, vision, time series, reinforcement learning, and more.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-45" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-45">A Gentle Proof (Why It Works)</h4>
<p>The context vector <span class="math inline">\(c_t\)</span> is the expected value of encoder representations under the attention distribution <span class="math inline">\(\alpha_t\)</span>:</p>
<p><span class="math display">\[
c_t = \mathbb{E}_{i \sim \alpha_t}[v_i]
\]</span></p>
<p>Since <span class="math inline">\(\alpha_t\)</span> sums to 1 and depends smoothly on <span class="math inline">\(q_t\)</span>, the entire mechanism is differentiable and trainable via backpropagation. It learns to assign high weights to inputs that reduce loss, aligning relevant features dynamically.</p>
</section>
<section id="variants-2" class="level4">
<h4 class="anchored" data-anchor-id="variants-2">Variants</h4>
<ol type="1">
<li>Additive Attention (Bahdanau), nonlinear score function.</li>
<li>Multiplicative / Scaled Dot (Luong, Vaswani), efficient matrix multiplication form.</li>
<li>Self-Attention, queries, keys, and values come from the same sequence.</li>
<li>Multi-Head Attention, multiple subspaces capture different relations.</li>
</ol>
</section>
<section id="try-it-yourself-46" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-46">Try It Yourself</h4>
<ol type="1">
<li>Implement dot-product attention on small random vectors.</li>
<li>Visualize attention weights for different queries.</li>
<li>Extend to self-attention by using the same matrix for Q, K, V.</li>
<li>Observe how multi-head attention mixes features.</li>
<li>Apply attention weights to words in a sentence, see which parts light up.</li>
</ol>
</section>
<section id="test-cases-46" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-46">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Model</th>
<th>Attention Type</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Machine translation</td>
<td>Seq2Seq</td>
<td>Bahdanau</td>
<td>Improves alignment</td>
</tr>
<tr class="even">
<td>Summarization</td>
<td>Transformer</td>
<td>Self-attention</td>
<td>Captures context</td>
</tr>
<tr class="odd">
<td>Vision</td>
<td>ViT</td>
<td>Multi-head</td>
<td>Global pixel context</td>
</tr>
<tr class="even">
<td>Speech recognition</td>
<td>Transformer-ASR</td>
<td>Cross-attention</td>
<td>Stable decoding</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-46" class="level4">
<h4 class="anchored" data-anchor-id="complexity-46">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T^2 d)\)</span></li>
<li>Space: <span class="math inline">\(O(T^2)\)</span></li>
<li>Trade-off: better context vs quadratic cost</li>
</ul>
<p>Attention is the bridge between memory and reasoning, letting neural networks not just remember, but <em>choose</em> what to remember.</p>
</section>
</section>
<section id="transformer-decoder" class="level3">
<h3 class="anchored" data-anchor-id="transformer-decoder">948. Transformer Decoder</h3>
<p>The Transformer Decoder is the engine of modern generative AI, a sequence model that uses self-attention instead of recurrence to process context. It’s the key component behind GPT-style models, capable of modeling long-range dependencies efficiently and producing coherent text, code, and more.</p>
<section id="what-problem-are-we-solving-47" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-47">What Problem Are We Solving?</h4>
<p>Traditional sequence models (RNNs, LSTMs) process inputs step by step. This makes them slow, sequential, and limited in remembering far-away context. Transformers replace recurrence with parallel self-attention, enabling global context access and massive scalability.</p>
<p>The decoder part specifically handles auto-regressive generation, predicting the next token given all previous ones.</p>
</section>
<section id="the-core-idea-17" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-17">The Core Idea</h4>
<p>At each decoding step <span class="math inline">\(t\)</span>, the model attends to:</p>
<ol type="1">
<li>All previous tokens in the output sequence (via <em>masked self-attention</em>).</li>
<li>All encoder outputs (via <em>encoder–decoder attention</em>).</li>
<li>Its own learned hidden representation, layer by layer.</li>
</ol>
<p>Each decoder layer has three sub-layers:</p>
<ol type="1">
<li><p>Masked Self-Attention</p>
<ul>
<li><p>Computes attention over past positions only (no future leakage).<br>
</p></li>
<li><p>Uses a triangular mask <span class="math inline">\(M\)</span> where:</p>
<p><span class="math display">\[
M_{ij} =
\begin{cases}
0, &amp; j \le i,\\
-\infty, &amp; j &gt; i
\end{cases}
\]</span></p>
<p>so that the softmax ignores future tokens.</p></li>
</ul></li>
<li><p>Encoder–Decoder Attention</p>
<ul>
<li>Attends to encoder outputs (useful in translation or other paired tasks).</li>
<li>Queries come from the decoder, keys/values from the encoder.</li>
</ul></li>
<li><p>Feedforward Network</p>
<ul>
<li>Two linear transformations with ReLU or GELU: <span class="math display">\[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\]</span></li>
</ul></li>
</ol>
<p>Each sub-layer includes:</p>
<ul>
<li>Residual connection</li>
<li>Layer normalization</li>
</ul>
</section>
<section id="the-transformer-decoder-block" class="level4">
<h4 class="anchored" data-anchor-id="the-transformer-decoder-block">The Transformer Decoder Block</h4>
<p>For each token representation <span class="math inline">\(x_t\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
z_1 &amp;= \text{LayerNorm}(x_t + \text{SelfAttention}(x_t)) \
z_2 &amp;= \text{LayerNorm}(z_1 + \text{CrossAttention}(z_1, E)) \
y_t &amp;= \text{LayerNorm}(z_2 + \text{FFN}(z_2))
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(E\)</span> are encoder outputs (for tasks like translation). In pure language models (like GPT), the encoder is omitted, only self-attention is used.</p>
</section>
<section id="how-it-works-plain-language-7" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-7">How It Works (Plain Language)</h4>
<p>Imagine writing a story. At each word, you look back at everything you’ve written so far, but not ahead, to decide what comes next. You also consult a memory of “facts” (from an encoder or context). The decoder does the same, combining self-attention and feedforward reasoning to build meaning word by word.</p>
</section>
<section id="step-by-step-summary-20" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-20">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 37%">
<col style="width: 56%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Sub-Layer</th>
<th>Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Masked Self-Attention</td>
<td>Look at previous tokens</td>
</tr>
<tr class="even">
<td>2</td>
<td>Encoder–Decoder Attention</td>
<td>Look at encoded input (optional)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Feedforward</td>
<td>Nonlinear transformation</td>
</tr>
<tr class="even">
<td>4</td>
<td>Residual + Normalization</td>
<td>Stability and gradient flow</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Output Projection</td>
<td>Map hidden state to vocabulary logits</td>
</tr>
</tbody>
</table>
</section>
<section id="example-auto-regressive-generation" class="level4">
<h4 class="anchored" data-anchor-id="example-auto-regressive-generation">Example: Auto-Regressive Generation</h4>
<p>At inference time:</p>
<ol type="1">
<li>Start with <code>&lt;BOS&gt;</code> (beginning of sequence).</li>
<li>Predict next token using softmax over output logits: <span class="math display">\[
P(y_t \mid y_{&lt;t}) = \text{softmax}(W_o h_t)
\]</span></li>
<li>Append <span class="math inline">\(y_t\)</span> and feed back into model.</li>
<li>Repeat until <code>&lt;EOS&gt;</code> or length limit.</li>
</ol>
</section>
<section id="tiny-code-simplified-2" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified-2">Tiny Code (Simplified)</h4>
<p>Python (NumPy prototype)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x))</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> e <span class="op">/</span> e.<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(Q, K, V, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> Q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Q <span class="op">@</span> K.T <span class="op">/</span> np.sqrt(d_k)</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">+=</span> mask  <span class="co"># apply -inf for masked positions</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> softmax(scores)</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights <span class="op">@</span> V</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transformer_decoder_step(Q, K, V, mask):</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> attention(Q, K, V, mask)</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context <span class="op">+</span> Q  <span class="co"># simple residual form</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb89"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each token t: compute Q,K,V; mask future positions; apply dot-product attention.</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Add residual and normalization steps (omitted for brevity).</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-47" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-47">Why It Matters</h4>
<ul>
<li>Replaces recurrence with attention → parallelizable and faster.</li>
<li>Captures global context across long sequences.</li>
<li>Scales beautifully → supports huge models (billions of parameters).</li>
<li>Foundation of GPT, BERT, T5, LLaMA, etc.</li>
<li>Language, vision, audio, and multimodal unification.</li>
</ul>
<p>The decoder architecture is the reason large language models can write essays, code, and poetry by simply attending over massive context windows.</p>
</section>
<section id="a-gentle-proof-why-it-works-46" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-46">A Gentle Proof (Why It Works)</h4>
<p>Self-attention builds pairwise dependencies between all positions in a sequence. Unlike RNNs that rely on chain recurrence:</p>
<p><span class="math display">\[
h_t = f(h_{t-1}, x_t)
\]</span></p>
<p>the Transformer computes all <span class="math inline">\(h_t\)</span> simultaneously via attention weights. The model thus learns contextual relationships in a single step, enabling efficient long-range reasoning.</p>
<p>The mask ensures causality, no “peeking” at future tokens.</p>
</section>
<section id="try-it-yourself-47" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-47">Try It Yourself</h4>
<ol type="1">
<li>Implement a 2-layer decoder-only Transformer on a toy dataset.</li>
<li>Compare with an RNN: note faster convergence and better long-term recall.</li>
<li>Visualize attention weights during decoding, see how tokens attend backward.</li>
<li>Experiment with different mask sizes (context windows).</li>
<li>Try greedy vs beam search decoding.</li>
</ol>
</section>
<section id="test-cases-47" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-47">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Task</th>
<th>Type</th>
<th>Decoder Only?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT</td>
<td>Text generation</td>
<td>Language</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>T5</td>
<td>Translation</td>
<td>Encoder–Decoder</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Whisper</td>
<td>Speech recognition</td>
<td>Encoder–Decoder</td>
<td>No</td>
</tr>
<tr class="even">
<td>LLaMA</td>
<td>Chat model</td>
<td>Language</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-47" class="level4">
<h4 class="anchored" data-anchor-id="complexity-47">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T^2 d)\)</span> per layer</li>
<li>Space: <span class="math inline">\(O(T^2)\)</span></li>
<li>Parallelism: fully parallel per token</li>
</ul>
<p>The Transformer Decoder is the modern storyteller, reading everything it’s written, remembering context precisely, and weaving coherent sequences without ever looping back in time.</p>
</section>
</section>
<section id="seq2seq-with-attention" class="level3">
<h3 class="anchored" data-anchor-id="seq2seq-with-attention">949. Seq2Seq with Attention</h3>
<p>Seq2Seq with Attention combines two powerful ideas, sequence-to-sequence modeling and the attention mechanism, to enable flexible mapping from one sequence to another, especially when input and output lengths differ. It marked a major breakthrough in neural machine translation, later evolving into the foundation of modern Transformer architectures.</p>
<section id="what-problem-are-we-solving-48" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-48">What Problem Are We Solving?</h4>
<p>In classic sequence-to-sequence (Seq2Seq) models using RNNs:</p>
<ul>
<li>The encoder reads the entire input sequence into a fixed-length vector.</li>
<li>The decoder generates outputs step by step, based only on that vector.</li>
</ul>
<p>Problem: long sequences cause information bottlenecks, early tokens are forgotten, context blurs.</p>
<p>The solution: add attention, so the decoder can look back at <em>all</em> encoder states, not just the final one.</p>
</section>
<section id="the-core-idea-18" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-18">The Core Idea</h4>
<p>Seq2Seq with Attention has two main components:</p>
<ol type="1">
<li><p>Encoder Reads the input sequence <span class="math inline">\((x_1, \dots, x_T)\)</span> and produces a sequence of hidden states: <span class="math display">\[
h_i = \text{Encoder}(x_i, h_{i-1})
\]</span></p></li>
<li><p>Decoder Generates each output token <span class="math inline">\(y_t\)</span> while attending to all encoder states via attention weights <span class="math inline">\(\alpha_{t,i}\)</span>.</p></li>
</ol>
<p>At each decoding step <span class="math inline">\(t\)</span>:</p>
<ul>
<li>Compute alignment scores: <span class="math display">\[
e_{t,i} = \text{score}(s_{t-1}, h_i)
\]</span></li>
<li>Normalize with softmax: <span class="math display">\[
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
\]</span></li>
<li>Compute context vector: <span class="math display">\[
c_t = \sum_i \alpha_{t,i} h_i
\]</span></li>
<li>Update the decoder state: <span class="math display">\[
s_t = f(s_{t-1}, y_{t-1}, c_t)
\]</span></li>
<li>Predict next token: <span class="math display">\[
P(y_t \mid y_{&lt;t}, X) = \text{softmax}(W[s_t; c_t])
\]</span></li>
</ul>
</section>
<section id="how-it-works-plain-language-8" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-8">How It Works (Plain Language)</h4>
<p>Think of translation: The encoder reads “Le chat dort.” → it builds memory for each word. The decoder starts generating “The cat sleeps.” and, at each step, looks back at the encoder’s memory, focusing attention where it’s most relevant.</p>
<p>When producing “cat,” attention peaks on “chat.” When producing “sleeps,” attention peaks on “dort.”</p>
<p>This selective focusing eliminates the bottleneck and gives the model interpretability.</p>
</section>
<section id="step-by-step-summary-21" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-21">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 11%">
<col style="width: 83%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Encoder</td>
<td>Reads input sequence, produces hidden states</td>
</tr>
<tr class="even">
<td>2</td>
<td>Attention</td>
<td>Computes relevance of each encoder state to decoder state</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Context</td>
<td>Weighted sum of encoder hidden states</td>
</tr>
<tr class="even">
<td>4</td>
<td>Decoder</td>
<td>Uses previous token, context, and hidden state to predict next token</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Output</td>
<td>Softmax over vocabulary for next word</td>
</tr>
</tbody>
</table>
</section>
<section id="example-simple-translation" class="level4">
<h4 class="anchored" data-anchor-id="example-simple-translation">Example: Simple Translation</h4>
<p>Input: “je t’aime” Encoder outputs hidden states <span class="math inline">\(h_1, h_2, h_3\)</span>. Decoder generates:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Output</th>
<th>Attention Peaks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>I</td>
<td><span class="math inline">\(h_1\)</span> (je)</td>
</tr>
<tr class="even">
<td>2</td>
<td>love</td>
<td><span class="math inline">\(h_3\)</span> (aime)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>you</td>
<td><span class="math inline">\(h_2\)</span> (t’)</td>
</tr>
</tbody>
</table>
<p>This dynamic alignment enables fluent translations without explicit word alignment.</p>
</section>
<section id="tiny-code-simplified-3" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified-3">Tiny Code (Simplified)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x): <span class="cf">return</span> np.exp(x) <span class="op">/</span> np.exp(x).<span class="bu">sum</span>()</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention(decoder_state, encoder_states):</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> encoder_states <span class="op">@</span> decoder_state</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> softmax(scores)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> (encoder_states.T <span class="op">@</span> weights).T</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context, weights</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> seq2seq_step(prev_state, prev_output, encoder_states):</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>    context, _ <span class="op">=</span> attention(prev_state, encoder_states)</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    new_state <span class="op">=</span> np.tanh(prev_state <span class="op">+</span> context <span class="op">+</span> prev_output)</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_state</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb91"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each decoder step:</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="co">// 1. Compute scores = dot(decoder_state, encoder_states)</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co">// 2. Apply softmax to get attention weights</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="co">// 3. Compute weighted context vector</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="co">// 4. Combine with decoder RNN for next output</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-48" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-48">Why It Matters</h4>
<ul>
<li>Solves information bottleneck in basic Seq2Seq.</li>
<li>Improves translation, summarization, and speech tasks.</li>
<li>Interpretable via attention heatmaps.</li>
<li>Precursor to Transformers: replaces recurrence with self-attention.</li>
<li>Flexible for variable-length input/output.</li>
</ul>
<p>The attention mechanism transformed sequence learning from “compress and decode” to “consult and compose.”</p>
</section>
<section id="a-gentle-proof-why-it-works-47" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-47">A Gentle Proof (Why It Works)</h4>
<p>Without attention, the conditional probability is:</p>
<p><span class="math display">\[
P(y_t \mid y_{&lt;t}, X) = f(s_{t-1}, h_T)
\]</span></p>
<p>With attention, we instead marginalize over all encoder states:</p>
<p><span class="math display">\[
P(y_t \mid y_{&lt;t}, X) = f\left(s_{t-1}, \sum_i \alpha_{t,i} h_i\right)
\]</span></p>
<p>This turns a single-context model into a context distribution model, more expressive and differentiable, improving gradient flow and alignment learning.</p>
</section>
<section id="try-it-yourself-48" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-48">Try It Yourself</h4>
<ol type="1">
<li>Train a small Seq2Seq model on short English–French pairs.</li>
<li>Visualize attention weights per decoding step.</li>
<li>Increase sentence length, see how attention preserves context.</li>
<li>Compare with no-attention baseline.</li>
<li>Experiment with Bahdanau (additive) vs Luong (dot-product) scoring.</li>
</ol>
</section>
<section id="test-cases-48" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-48">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 18%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Input</th>
<th>Output</th>
<th>Attention Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Translation</td>
<td>“guten morgen”</td>
<td>“good morning”</td>
<td>Aligns words</td>
</tr>
<tr class="even">
<td>Summarization</td>
<td>Article</td>
<td>Summary</td>
<td>Focuses on main phrases</td>
</tr>
<tr class="odd">
<td>Speech recognition</td>
<td>Audio frames</td>
<td>Text</td>
<td>Time alignment</td>
</tr>
<tr class="even">
<td>Question answering</td>
<td>Context + question</td>
<td>Answer</td>
<td>Focuses on relevant span</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-48" class="level4">
<h4 class="anchored" data-anchor-id="complexity-48">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T_x T_y d)\)</span></li>
<li>Space: <span class="math inline">\(O(T_x d)\)</span></li>
<li>Improvement: parallelizable and interpretable</li>
</ul>
<p>Seq2Seq with Attention was the first neural model to “look while it speaks”, an elegant bridge between memory and focus that opened the path to the Transformer revolution.</p>
</section>
</section>
<section id="pointer-network" class="level3">
<h3 class="anchored" data-anchor-id="pointer-network">950. Pointer Network</h3>
<p>The Pointer Network is a fascinating twist on the standard sequence-to-sequence architecture, instead of predicting tokens from a fixed vocabulary, it points to elements in the input sequence as outputs.</p>
<p>It’s especially useful when the output is a reordering or subset of inputs, such as in sorting, routing, or combinatorial optimization problems.</p>
<section id="what-problem-are-we-solving-49" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-49">What Problem Are We Solving?</h4>
<p>Ordinary Seq2Seq models generate outputs from a fixed-size vocabulary. But what if the output must refer to specific positions in the input? For example:</p>
<ul>
<li>Sorting numbers (output is a permutation of input indices)</li>
<li>Solving Traveling Salesman Problem (output is sequence of city indices)</li>
<li>Extractive summarization (output is a subset of tokens)</li>
</ul>
<p>A normal softmax over a fixed vocabulary cannot express this, we need a model that can dynamically select input elements as outputs.</p>
</section>
<section id="the-core-idea-19" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-19">The Core Idea</h4>
<p>Pointer Networks reuse the attention mechanism as a pointer instead of a weighting function. At each decoding step, the model computes attention over the encoder inputs and selects one position as the next output.</p>
<p>For input sequence <span class="math inline">\(X = (x_1, x_2, \dots, x_n)\)</span> and decoder state <span class="math inline">\(s_t\)</span>:</p>
<ol type="1">
<li><p>Score each input: <span class="math display">\[
e_{t,i} = v_a^\top \tanh(W_1 h_i + W_2 s_t)
\]</span></p></li>
<li><p>Normalize with softmax: <span class="math display">\[
p_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
\]</span></p></li>
<li><p>The output at step <span class="math inline">\(t\)</span> is the index with highest probability: <span class="math display">\[
y_t = \arg\max_i p_{t,i}
\]</span></p></li>
</ol>
<p>So instead of generating tokens, it “points” to the relevant input position.</p>
</section>
<section id="how-it-works-plain-language-9" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-9">How It Works (Plain Language)</h4>
<p>Imagine you have a list of cities and you want the model to plan the shortest route. Rather than generating city names, it points to the next city index in the input list, like a finger tracing an optimal tour.</p>
<p>Each step’s attention distribution acts like a probability map over input positions.</p>
</section>
<section id="step-by-step-summary-22" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-22">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Encoder</td>
<td>Reads all input elements into hidden states <span class="math inline">\(h_i\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Decoder</td>
<td>Maintains a recurrent state <span class="math inline">\(s_t\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Attention</td>
<td>Computes scores <span class="math inline">\(e_{t,i}\)</span> for each input position</td>
</tr>
<tr class="even">
<td>4</td>
<td>Softmax</td>
<td>Converts to probability distribution <span class="math inline">\(p_{t,i}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Output</td>
<td>Picks the most probable input index</td>
</tr>
</tbody>
</table>
</section>
<section id="example-sorting-three-numbers" class="level4">
<h4 class="anchored" data-anchor-id="example-sorting-three-numbers">Example: Sorting Three Numbers</h4>
<p>Input: <code>[3, 1, 2]</code> Encoder hidden states represent each number. Decoder outputs indices <code>[2, 3, 1]</code>, corresponding to sorted order <code>[1, 2, 3]</code>.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Decoder Output</th>
<th>Selected Input</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2</td>
<td>1</td>
<td>smallest number</td>
</tr>
<tr class="even">
<td>2</td>
<td>3</td>
<td>2</td>
<td>next smallest</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
<td>3</td>
<td>largest</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-simplified-4" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified-4">Tiny Code (Simplified)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pointer_network_step(encoder_h, decoder_s):</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.tanh(encoder_h <span class="op">@</span> W1 <span class="op">+</span> decoder_s <span class="op">@</span> W2)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">@</span> v_a</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.exp(scores).<span class="bu">sum</span>()</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    pointer <span class="op">=</span> np.argmax(probs)</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pointer, probs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Outline)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb93"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Compute attention scores between decoder state and encoder states</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Apply softmax over input positions to get probabilities</span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Output the index with highest probability</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-49" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-49">Why It Matters</h4>
<ul>
<li>Dynamic output size: no need for fixed vocabularies.</li>
<li>Perfect for structured tasks: sorting, matching, routing.</li>
<li>Interpretable: attention weights show selection probabilities.</li>
<li>Bridges deep learning and combinatorial optimization.</li>
</ul>
<p>The model learns to select rather than generate, a subtle shift that opens deep learning to problems in graph theory and operations research.</p>
</section>
<section id="a-gentle-proof-why-it-works-48" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-48">A Gentle Proof (Why It Works)</h4>
<p>In normal attention-based Seq2Seq models, the attention weights <span class="math inline">\(\alpha_{t,i}\)</span> act as context weights:</p>
<p><span class="math display">\[
c_t = \sum_i \alpha_{t,i} h_i
\]</span></p>
<p>In a Pointer Network, these same weights form the output distribution:</p>
<p><span class="math display">\[
P(y_t = i \mid X, y_{&lt;t}) = \alpha_{t,i}
\]</span></p>
<p>Since <span class="math inline">\(\alpha_{t,i}\)</span> depends on both input representations and decoder state, the model learns differentiable selection behavior through standard backpropagation.</p>
</section>
<section id="try-it-yourself-49" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-49">Try It Yourself</h4>
<ol type="1">
<li>Train a Pointer Network to sort small lists of integers.</li>
<li>Visualize the attention weights as “pointers” over time.</li>
<li>Extend to Traveling Salesman Problem (TSP), the model outputs city order.</li>
<li>Try extractive summarization, selecting key sentences from text.</li>
<li>Compare with a fixed-vocabulary Seq2Seq baseline.</li>
</ol>
</section>
<section id="test-cases-49" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-49">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 23%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Input</th>
<th>Output</th>
<th>Model Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sorting</td>
<td>[5, 1, 4]</td>
<td>[2, 3, 1]</td>
<td>Picks smallest-first order</td>
</tr>
<tr class="even">
<td>TSP</td>
<td>5 city coordinates</td>
<td>Sequence of indices</td>
<td>Routes cities efficiently</td>
</tr>
<tr class="odd">
<td>Text summarization</td>
<td>Sentences</td>
<td>Selected subset</td>
<td>Extracts key lines</td>
</tr>
<tr class="even">
<td>Question answering</td>
<td>Context words</td>
<td>Word index</td>
<td>Points to answer span</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-49" class="level4">
<h4 class="anchored" data-anchor-id="complexity-49">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(n^2)\)</span> (attention over all inputs)</li>
<li>Space: <span class="math inline">\(O(n^2)\)</span></li>
<li>Output flexibility: up to length of input</li>
</ul>
<p>Pointer Networks show that deep learning models don’t always need to invent new words, sometimes, they just need to <em>point</em> to what’s already there.</p>
</section>
</section>
</section>
<section id="section-96.-metaheuristics" class="level1">
<h1>Section 96. Metaheuristics</h1>
<section id="genetic-algorithm-ga" class="level3">
<h3 class="anchored" data-anchor-id="genetic-algorithm-ga">951. Genetic Algorithm (GA)</h3>
<p>The Genetic Algorithm (GA) is an evolutionary search technique inspired by natural selection. It explores a large solution space by evolving a population of candidates through selection, crossover, and mutation, just like biological evolution improves species over generations.</p>
<section id="what-problem-are-we-solving-50" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-50">What Problem Are We Solving?</h4>
<p>Many real-world problems, scheduling, design, routing, optimization, have search spaces that are too large or complex for exact algorithms. Genetic Algorithms offer a stochastic, adaptive method to find <em>good enough</em> solutions without brute force.</p>
<p>They’re particularly useful when:</p>
<ul>
<li>The problem has no gradient or nonlinear objective</li>
<li>The solution space is discrete or combinatorial</li>
<li>We only need a near-optimal answer quickly</li>
</ul>
</section>
<section id="the-core-idea-20" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-20">The Core Idea</h4>
<p>A GA maintains a population of candidate solutions (chromosomes). Each solution is represented as a string (e.g., bits, numbers, symbols). Over generations, the population “evolves” toward better fitness.</p>
<p>Main steps:</p>
<ol type="1">
<li>Initialization, create random population <span class="math inline">\(P_0\)</span></li>
<li>Selection, choose better individuals for reproduction</li>
<li>Crossover, combine pairs to create offspring</li>
<li>Mutation, randomly tweak offspring to preserve diversity</li>
<li>Replacement, form the next generation <span class="math inline">\(P_{t+1}\)</span></li>
<li>Repeat until convergence or max generations</li>
</ol>
</section>
<section id="the-genetic-cycle-plain-language" class="level4">
<h4 class="anchored" data-anchor-id="the-genetic-cycle-plain-language">The Genetic Cycle (Plain Language)</h4>
<ol type="1">
<li>Start randomly, guess a bunch of solutions.</li>
<li>Score them, use a fitness function <span class="math inline">\(f(x)\)</span> to measure quality.</li>
<li>Breed survivors, the fittest “parents” mate to create children.</li>
<li>Mix and mutate, small random changes introduce variation.</li>
<li>Keep the best, select top candidates for the next generation.</li>
<li>Repeat, evolution continues until performance stabilizes.</li>
</ol>
</section>
<section id="mathematical-view" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-view">Mathematical View</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(x_i^{(t)}\)</span> be the <span class="math inline">\(i\)</span>-th individual at generation <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(f(x_i^{(t)})\)</span> be its fitness score</li>
</ul>
<p>The population evolves according to:</p>
<p><span class="math display">\[
P^{(t+1)} = \text{Select}\big(\text{Mutate}(\text{Crossover}(P^{(t)}))\big)
\]</span></p>
<p>The process balances exploration (via mutation) and exploitation (via selection).</p>
</section>
<section id="step-by-step-example-binary-optimization" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-binary-optimization">Step-by-Step Example (Binary Optimization)</h4>
<p>Goal: maximize <span class="math inline">\(f(x) = x^2\)</span>, where <span class="math inline">\(x\)</span> is a 5-bit binary number.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 28%">
<col style="width: 19%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Generation</th>
<th>Population</th>
<th>Fitness</th>
<th>Selected</th>
<th>Offspring</th>
<th>Mutation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>[01011, 10000, 00101, 11100]</td>
<td>[121, 256, 25, 784]</td>
<td>[11100, 10000]</td>
<td>[11100, 10000]</td>
<td>[11110, 10100]</td>
</tr>
<tr class="even">
<td>1</td>
<td>[11110, 10100, …]</td>
<td>[900, 400, …]</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>After several generations, the population converges toward <span class="math inline">\(x=11111\)</span> (31), the global optimum.</p>
</section>
<section id="tiny-code" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fitness(x):</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">*</span> x</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mutate(x):</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> <span class="dv">1</span> <span class="op">&lt;&lt;</span> random.randint(<span class="dv">0</span>, <span class="dv">4</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">^</span> mask</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crossover(a, b):</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>    point <span class="op">=</span> random.randint(<span class="dv">1</span>, <span class="dv">4</span>)</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> (<span class="dv">1</span> <span class="op">&lt;&lt;</span> point) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (a <span class="op">&amp;</span> mask) <span class="op">|</span> (b <span class="op">&amp;</span> <span class="op">~</span>mask)</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialization</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>pop <span class="op">=</span> [random.randint(<span class="dv">0</span>, <span class="dv">31</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> gen <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>    pop <span class="op">=</span> <span class="bu">sorted</span>(pop, key<span class="op">=</span>fitness, reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> pop[:<span class="dv">2</span>]</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    child <span class="op">=</span> mutate(crossover(a, b))</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>    pop[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> child</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Gen </span><span class="sc">{</span>gen<span class="sc">}</span><span class="ss">: best </span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">, fitness=</span><span class="sc">{</span>fitness(a)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Represent each candidate as an unsigned int.</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Apply crossover by bitmask, mutation by flipping random bits.</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Evaluate fitness and keep top individuals each generation.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-50" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-50">Why It Matters</h4>
<ul>
<li>Works for non-differentiable, discrete, or black-box problems.</li>
<li>Naturally parallel and stochastic.</li>
<li>Simple to implement and adapt.</li>
<li>Used in design optimization, route planning, neural architecture search, and more.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-49" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-49">A Gentle Proof (Why It Works)</h4>
<p>Genetic Algorithms approximate stochastic hill climbing with population diversity. Under the Schema Theorem, patterns (schemata) representing good partial solutions are exponentially propagated if they contribute above-average fitness:</p>
<p><span class="math display">\[
E[m(H, t+1)] \ge m(H, t) \frac{f(H)}{\bar{f}} (1 - p_c - p_m)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(m(H, t)\)</span> is number of schema instances</li>
<li><span class="math inline">\(f(H)\)</span> is average schema fitness</li>
<li><span class="math inline">\(\bar{f}\)</span> is population mean fitness</li>
<li><span class="math inline">\(p_c, p_m\)</span> are crossover and mutation probabilities</li>
</ul>
<p>Over time, beneficial schemata dominate the population.</p>
</section>
<section id="try-it-yourself-50" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-50">Try It Yourself</h4>
<ol type="1">
<li>Implement GA for function maximization <span class="math inline">\(f(x) = \sin(x)\)</span> on <span class="math inline">\([0, 2\pi]\)</span>.</li>
<li>Vary mutation rate and observe diversity.</li>
<li>Compare roulette-wheel vs tournament selection.</li>
<li>Visualize convergence of best fitness over generations.</li>
<li>Use GA to evolve weights for a tiny neural network.</li>
</ol>
</section>
<section id="test-cases-50" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-50">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 22%">
<col style="width: 25%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Representation</th>
<th>Fitness Function</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bit-string optimization</td>
<td>Binary</td>
<td>Problem-defined</td>
<td>Classic</td>
</tr>
<tr class="even">
<td>TSP</td>
<td>City sequence</td>
<td>Inverse path length</td>
<td>Popular benchmark</td>
</tr>
<tr class="odd">
<td>Symbolic regression</td>
<td>Expression tree</td>
<td>Error vs data</td>
<td>GP variant</td>
</tr>
<tr class="even">
<td>Scheduling</td>
<td>Job order</td>
<td>Minimize delay</td>
<td>Discrete</td>
</tr>
<tr class="odd">
<td>Neural architecture</td>
<td>Network structure</td>
<td>Validation accuracy</td>
<td>Meta-learning</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-50" class="level4">
<h4 class="anchored" data-anchor-id="complexity-50">Complexity</h4>
<ul>
<li><p>Time: <span class="math inline">\(O(P \cdot G \cdot C)\)</span></p>
<ul>
<li><span class="math inline">\(P\)</span>: population size</li>
<li><span class="math inline">\(G\)</span>: generations</li>
<li><span class="math inline">\(C\)</span>: fitness cost</li>
</ul></li>
<li><p>Space: <span class="math inline">\(O(P)\)</span></p></li>
</ul>
<p>Genetic Algorithms remind us that progress doesn’t need precision, it can emerge from variation, competition, and persistence.</p>
</section>
</section>
<section id="simulated-annealing-sa" class="level3">
<h3 class="anchored" data-anchor-id="simulated-annealing-sa">952. Simulated Annealing (SA)</h3>
<p>Simulated Annealing is a probabilistic optimization algorithm inspired by the physical process of heating and slowly cooling metal to remove defects. It’s a single-solution search method that balances exploration (trying new solutions) and exploitation (keeping good ones) using controlled randomness.</p>
<section id="what-problem-are-we-solving-51" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-51">What Problem Are We Solving?</h4>
<p>Many optimization problems have many local minima, places where the solution looks good but isn’t globally optimal. A simple hill-climbing algorithm can get stuck there.</p>
<p>Simulated Annealing solves this by occasionally accepting worse solutions early on, to escape local traps, and then gradually reducing randomness to settle into a good region.</p>
<p>It’s especially useful when:</p>
<ul>
<li>The search space is large or nonconvex</li>
<li>Gradients are unavailable or noisy</li>
<li>Deterministic methods fail due to local minima</li>
</ul>
</section>
<section id="the-core-idea-21" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-21">The Core Idea</h4>
<p>The algorithm mimics how materials anneal, heat, then cool slowly so particles settle into a low-energy (stable) state.</p>
<p>At each iteration:</p>
<ol type="1">
<li><p>Propose a new solution <span class="math inline">\(x'\)</span> from the current one <span class="math inline">\(x\)</span>.</p></li>
<li><p>Compute the energy (objective) difference <span class="math inline">\(\Delta E = f(x') - f(x)\)</span>.</p></li>
<li><p>Accept the new solution if:</p>
<ul>
<li><span class="math inline">\(\Delta E &lt; 0\)</span> (better), or</li>
<li>With probability <span class="math inline">\(p = \exp(-\Delta E / T)\)</span> (worse but allowed)</li>
</ul></li>
<li><p>Gradually lower the temperature <span class="math inline">\(T\)</span> → reduces randomness.</p></li>
<li><p>Stop when <span class="math inline">\(T\)</span> is very small or the solution stabilizes.</p></li>
</ol>
</section>
<section id="how-it-works-plain-language-10" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-10">How It Works (Plain Language)</h4>
<p>Imagine a ball rolling on a hilly landscape. At high “temperature,” it bounces around freely, sometimes climbing hills. As it cools, its movement becomes more restricted, until it finally rests in a deep valley, ideally the global minimum.</p>
<p>Early randomness helps exploration, later cooling helps convergence.</p>
</section>
<section id="mathematical-formulation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h4>
<p>Let <span class="math inline">\(f(x)\)</span> be the energy (objective function) to minimize. At iteration <span class="math inline">\(t\)</span>:</p>
<ol type="1">
<li>Propose new candidate <span class="math inline">\(x'\)</span> by perturbing <span class="math inline">\(x\)</span>.</li>
<li>Compute <span class="math inline">\(\Delta E = f(x') - f(x)\)</span>.</li>
<li>Accept <span class="math inline">\(x'\)</span> with probability:</li>
</ol>
<p><span class="math display">\[
P(\text{accept}) =
\begin{cases}
1, &amp; \text{if } \Delta E &lt; 0,\\
\exp(-\Delta E / T_t), &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<ol start="4" type="1">
<li>Update temperature <span class="math inline">\(T_t = \alpha T_{t-1}\)</span>, where <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span> is cooling rate.</li>
</ol>
</section>
<section id="step-by-step-example" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example">Step-by-Step Example</h4>
<p>Goal: minimize <span class="math inline">\(f(x) = x^2 + 10 \sin(x)\)</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 17%">
<col style="width: 12%">
<col style="width: 5%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Current <span class="math inline">\(x\)</span></th>
<th>Candidate <span class="math inline">\(x'\)</span></th>
<th><span class="math inline">\(\Delta E\)</span></th>
<th><span class="math inline">\(T\)</span></th>
<th>Accept?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.0</td>
<td>3.1</td>
<td>+4.7</td>
<td>1.0</td>
<td>Yes (random accept)</td>
</tr>
<tr class="even">
<td>2</td>
<td>3.1</td>
<td>1.5</td>
<td>-6.0</td>
<td>0.9</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.5</td>
<td>1.9</td>
<td>+0.5</td>
<td>0.81</td>
<td>Maybe (depends on <span class="math inline">\(e^{-0.5/T}\)</span>)</td>
</tr>
</tbody>
</table>
<p>Over time, acceptance of worse moves becomes rarer.</p>
</section>
<section id="tiny-code-1" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-1">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math, random</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> x2 <span class="op">+</span> <span class="dv">10</span> <span class="op">*</span> math.sin(x)</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    x_new <span class="op">=</span> x <span class="op">+</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    dE <span class="op">=</span> f(x_new) <span class="op">-</span> f(x)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dE <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">or</span> random.random() <span class="op">&lt;</span> math.exp(<span class="op">-</span>dE <span class="op">/</span> T):</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x_new</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>    T <span class="op">*=</span> alpha</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best x:"</span>, x, <span class="st">"f(x):"</span>, f(x))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb97"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize x randomly, set T = 1</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Loop: propose x', compute dE = f(x') - f(x)</span></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="co">// if dE &lt; 0 or exp(-dE/T) &gt; rand(), accept x'</span></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="co">// gradually reduce T</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-51" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-51">Why It Matters</h4>
<ul>
<li>Escapes local minima, a key advantage over greedy methods.</li>
<li>Works even on non-smooth, discontinuous, or discrete problems.</li>
<li>Simple to implement with minimal tuning.</li>
<li>Useful in scheduling, circuit design, path planning, and layout optimization.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-50" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-50">A Gentle Proof (Why It Works)</h4>
<p>Simulated Annealing is rooted in Markov Chain Monte Carlo (MCMC) theory. If the temperature decreases slowly enough, the algorithm converges (in probability) to the global minimum of <span class="math inline">\(f(x)\)</span>.</p>
<p>At equilibrium, the system samples states according to the Boltzmann distribution:</p>
<p><span class="math display">\[
P(x) \propto \exp(-f(x) / T)
\]</span></p>
<p>As <span class="math inline">\(T \to 0\)</span>, probability mass concentrates at the global optimum.</p>
</section>
<section id="try-it-yourself-51" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-51">Try It Yourself</h4>
<ol type="1">
<li><p>Minimize <span class="math inline">\(f(x) = x^2 + 10\sin(x)\)</span> and visualize the trajectory.</p></li>
<li><p>Experiment with different cooling schedules:</p>
<ul>
<li>Exponential: <span class="math inline">\(T_t = \alpha^t T_0\)</span></li>
<li>Linear: <span class="math inline">\(T_t = T_0 / (1 + \beta t)\)</span></li>
<li>Logarithmic: <span class="math inline">\(T_t = T_0 / \log(1 + t)\)</span></li>
</ul></li>
<li><p>Plot acceptance rate vs temperature.</p></li>
<li><p>Apply SA to the Traveling Salesman Problem.</p></li>
</ol>
</section>
<section id="test-cases-51" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-51">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Search Space</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Function minimization</td>
<td>Continuous</td>
<td>Simple benchmark</td>
</tr>
<tr class="even">
<td>TSP</td>
<td>Permutations</td>
<td>Classic combinatorial test</td>
</tr>
<tr class="odd">
<td>Job scheduling</td>
<td>Discrete</td>
<td>Large local minima count</td>
</tr>
<tr class="even">
<td>Neural net weights</td>
<td>Continuous</td>
<td>Experimental metaheuristic</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-51" class="level4">
<h4 class="anchored" data-anchor-id="complexity-51">Complexity</h4>
<ul>
<li><p>Time: <span class="math inline">\(O(N \times k)\)</span></p>
<ul>
<li><span class="math inline">\(N\)</span>: iterations</li>
<li><span class="math inline">\(k\)</span>: cost of evaluating <span class="math inline">\(f(x)\)</span></li>
</ul></li>
<li><p>Space: <span class="math inline">\(O(1)\)</span></p></li>
<li><p>Convergence depends on cooling schedule, slower = better.</p></li>
</ul>
<p>Simulated Annealing is like controlled chaos, it wanders aimlessly at first, then slowly finds order, cooling into a near-perfect solution one probability drop at a time.</p>
</section>
</section>
<section id="tabu-search" class="level3">
<h3 class="anchored" data-anchor-id="tabu-search">953. Tabu Search</h3>
<p>Tabu Search is a metaheuristic that guides a local search algorithm to escape local minima by remembering, and avoiding, recently visited solutions. It adds short-term memory to the optimization process, helping the algorithm explore new regions of the search space without cycling back.</p>
<section id="what-problem-are-we-solving-52" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-52">What Problem Are We Solving?</h4>
<p>Local search algorithms (like hill climbing) can get trapped in local optima, endlessly revisiting the same or similar solutions. Tabu Search introduces a memory-based strategy to overcome this, it forbids (makes <em>tabu</em>) moves that undo recent progress.</p>
<p>This is useful in hard combinatorial problems such as:</p>
<ul>
<li>Traveling Salesman Problem (TSP)</li>
<li>Scheduling and timetabling</li>
<li>Graph coloring</li>
<li>Resource allocation</li>
</ul>
</section>
<section id="the-core-idea-22" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-22">The Core Idea</h4>
<p>Tabu Search enhances simple hill climbing with three key mechanisms:</p>
<ol type="1">
<li><p>Tabu List (Memory) A short-term memory storing recent moves or attributes to prevent cycling.</p></li>
<li><p>Aspiration Criteria Allows overriding a tabu restriction if the move yields a significantly better solution.</p></li>
<li><p>Neighborhood Exploration Systematically evaluates nearby solutions (the neighborhood) and picks the best non-tabu one.</p></li>
</ol>
<p>The algorithm continues until a stopping condition is met, like reaching a time limit or no improvement after several iterations.</p>
</section>
<section id="how-it-works-plain-language-11" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-11">How It Works (Plain Language)</h4>
<p>Imagine you’re hiking through hills in the dark. You can only feel the ground beneath your feet, so you take the steepest path uphill. But if you always do that, you’ll end up stuck on a small hill.</p>
<p>Tabu Search helps by keeping track of where you’ve already been, forbidding you from stepping back too soon, and occasionally allowing a risky detour if it looks promising.</p>
</section>
<section id="mathematical-formulation-1" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-1">Mathematical Formulation</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(S\)</span> be the search space</li>
<li><span class="math inline">\(f(s)\)</span> the cost (to minimize)</li>
<li><span class="math inline">\(N(s)\)</span> the neighborhood of <span class="math inline">\(s\)</span></li>
</ul>
<p>At iteration <span class="math inline">\(t\)</span>:</p>
<ol type="1">
<li>From current solution <span class="math inline">\(s_t\)</span>, generate neighborhood <span class="math inline">\(N(s_t)\)</span>.</li>
<li>Exclude tabu moves, those stored in the tabu list <span class="math inline">\(T_t\)</span>.</li>
<li>Select the best candidate: <span class="math display">\[
s_{t+1} = \arg\min_{s' \in N(s_t) \setminus T_t} f(s')
\]</span></li>
<li>Update tabu list: <span class="math display">\[
T_{t+1} = (T_t \cup {\text{move}(s_t, s_{t+1})}) - \text{expired entries}
\]</span></li>
<li>If <span class="math inline">\(f(s_{t+1})\)</span> improves global best, update it.</li>
</ol>
</section>
<section id="step-by-step-example-tsp" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-tsp">Step-by-Step Example (TSP)</h4>
<p>Goal: find shortest route through cities A–E.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 19%">
<col style="width: 5%">
<col style="width: 14%">
<col style="width: 13%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Current Route</th>
<th>Cost</th>
<th>Tabu Moves</th>
<th>Best Move</th>
<th>Next Route</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>A–B–C–D–E</td>
<td>34</td>
<td>(swap A–B)</td>
<td>swap C–D</td>
<td>A–B–D–C–E</td>
</tr>
<tr class="even">
<td>2</td>
<td>A–B–D–C–E</td>
<td>31</td>
<td>(swap C–D)</td>
<td>swap B–D</td>
<td>A–D–B–C–E</td>
</tr>
<tr class="odd">
<td>3</td>
<td>A–D–B–C–E</td>
<td>29</td>
<td>(swap B–D)</td>
<td>swap A–D</td>
<td>A–B–C–D–E (tabu, skip)</td>
</tr>
<tr class="even">
<td>4</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>The algorithm avoids returning to previous routes too early, exploring new configurations.</p>
</section>
<section id="tiny-code-2" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-2">Tiny Code</h4>
<p>Python (Simplified)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tabu_search(init, neighbor_fn, cost_fn, max_iter<span class="op">=</span><span class="dv">100</span>, tabu_len<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> init</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    best <span class="op">=</span> init</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>    tabu_list <span class="op">=</span> []</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>        neighbors <span class="op">=</span> neighbor_fn(current)</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>        candidates <span class="op">=</span> [n <span class="cf">for</span> n <span class="kw">in</span> neighbors <span class="cf">if</span> n <span class="kw">not</span> <span class="kw">in</span> tabu_list]</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> candidates:</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>            candidates <span class="op">=</span> neighbors  <span class="co"># if all tabu, ignore restriction temporarily</span></span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>        next_candidate <span class="op">=</span> <span class="bu">min</span>(candidates, key<span class="op">=</span>cost_fn)</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cost_fn(next_candidate) <span class="op">&lt;</span> cost_fn(best):</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>            best <span class="op">=</span> next_candidate</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a>        tabu_list.append(next_candidate)</span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(tabu_list) <span class="op">&gt;</span> tabu_len:</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a>            tabu_list.pop(<span class="dv">0</span>)</span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-23"><a href="#cb98-23" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> next_candidate</span>
<span id="cb98-24"><a href="#cb98-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb99"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Store recent moves in a fixed-size array (tabu list).</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Each iteration, generate all neighbors, skip tabu ones.</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Choose best allowed neighbor; update tabu list.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-52" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-52">Why It Matters</h4>
<ul>
<li>Avoids cycles and premature convergence.</li>
<li>Can escape local minima without random jumps.</li>
<li>Adapts to many combinatorial optimization problems.</li>
<li>Often finds near-optimal solutions efficiently.</li>
</ul>
<p>It’s like giving hill climbing a memory, so it doesn’t make the same mistakes twice.</p>
</section>
<section id="a-gentle-proof-why-it-works-51" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-51">A Gentle Proof (Why It Works)</h4>
<p>Tabu Search maintains a diversity of explored regions by enforcing a short-term prohibition of recent states. This forces the search trajectory to move outward in the solution graph. The aspiration criterion ensures that even if a tabu move leads to a global improvement, it can be accepted, ensuring convergence toward strong candidates.</p>
<p>Over time, the algorithm approximates a balance between intensification (local search near good regions) and diversification (exploring new ones).</p>
</section>
<section id="try-it-yourself-52" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-52">Try It Yourself</h4>
<ol type="1">
<li>Implement a Tabu Search for the 8-queens problem.</li>
<li>Vary tabu list length, short lists cause cycling, long lists slow progress.</li>
<li>Add an aspiration rule that allows tabu moves if cost improves best-so-far.</li>
<li>Visualize path of cost per iteration.</li>
<li>Compare to hill climbing and simulated annealing.</li>
</ol>
</section>
<section id="test-cases-52" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-52">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Representation</th>
<th>Tabu Memory</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TSP</td>
<td>Route order</td>
<td>Recent swaps</td>
<td>Classic benchmark</td>
</tr>
<tr class="even">
<td>Job scheduling</td>
<td>Job order</td>
<td>Recent exchanges</td>
<td>Industrial optimization</td>
</tr>
<tr class="odd">
<td>Graph coloring</td>
<td>Node colors</td>
<td>Recent color changes</td>
<td>NP-hard</td>
</tr>
<tr class="even">
<td>Sudoku</td>
<td>Grid state</td>
<td>Cell assignments</td>
<td>Constraint satisfaction</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-52" class="level4">
<h4 class="anchored" data-anchor-id="complexity-52">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(n \cdot |N(s)|)\)</span> per iteration</li>
<li>Space: <span class="math inline">\(O(|T|)\)</span> (tabu list length)</li>
<li>Convergence: depends on neighborhood size and tabu length</li>
</ul>
<p>Tabu Search is a clever balance between memory and curiosity, it remembers enough to avoid loops but forgets just enough to keep exploring.</p>
</section>
</section>
<section id="particle-swarm-optimization-pso" class="level3">
<h3 class="anchored" data-anchor-id="particle-swarm-optimization-pso">954. Particle Swarm Optimization (PSO)</h3>
<p>Particle Swarm Optimization (PSO) is a population-based metaheuristic inspired by how flocks of birds or schools of fish move together toward food sources. Each “particle” represents a potential solution that moves through the search space, guided by its own experience and by the best performers in the swarm.</p>
<section id="what-problem-are-we-solving-53" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-53">What Problem Are We Solving?</h4>
<p>We often face optimization problems where:</p>
<ul>
<li>The objective function is nonlinear or non-differentiable</li>
<li>The landscape has many local optima</li>
<li>Gradient information is unavailable</li>
</ul>
<p>PSO provides a derivative-free, parallel, and adaptive way to explore such spaces efficiently. It’s widely used in control, parameter tuning, neural network training, and design optimization.</p>
</section>
<section id="the-core-idea-23" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-23">The Core Idea</h4>
<p>Each particle <span class="math inline">\(i\)</span> maintains:</p>
<ul>
<li>Position <span class="math inline">\(x_i\)</span>, current solution</li>
<li>Velocity <span class="math inline">\(v_i\)</span>, direction and step size</li>
<li>Personal best <span class="math inline">\(p_i\)</span>, best position found so far</li>
<li>Global best <span class="math inline">\(g\)</span>, best among all particles</li>
</ul>
<p>At every iteration:</p>
<ol type="1">
<li>Update velocity (momentum + attraction to bests)</li>
<li>Update position</li>
<li>Evaluate new position and update <span class="math inline">\(p_i\)</span> and <span class="math inline">\(g\)</span> if improved</li>
</ol>
</section>
<section id="mathematical-formulation-2" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-2">Mathematical Formulation</h4>
<p>For particle <span class="math inline">\(i\)</span> at iteration <span class="math inline">\(t\)</span>:</p>
<ol type="1">
<li><p>Velocity update: <span class="math display">\[
v_i(t+1) = \omega v_i(t)
+ c_1 r_1 (p_i - x_i(t))
+ c_2 r_2 (g - x_i(t))
\]</span></p></li>
<li><p>Position update: <span class="math display">\[
x_i(t+1) = x_i(t) + v_i(t+1)
\]</span></p></li>
</ol>
<p>where</p>
<ul>
<li><span class="math inline">\(\omega\)</span> is the inertia weight (controls exploration)</li>
<li><span class="math inline">\(c_1, c_2\)</span> are acceleration coefficients (self and social influence)</li>
<li><span class="math inline">\(r_1, r_2\)</span> are random numbers in <span class="math inline">\([0,1]\)</span></li>
</ul>
<p>The balance between inertia and attraction determines how the swarm explores and converges.</p>
</section>
<section id="how-it-works-plain-language-12" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-12">How It Works (Plain Language)</h4>
<p>Imagine a flock of birds looking for food on a landscape. Each bird remembers the best spot it has found and also watches the best spot found by any bird. They all adjust their flight, slightly toward where they found food before, and slightly toward where the best bird is going. Over time, the swarm naturally gathers around the optimal area.</p>
</section>
<section id="step-by-step-example-1" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-1">Step-by-Step Example</h4>
<p>Goal: minimize <span class="math inline">\(f(x) = x^2 + 3\sin(x)\)</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Particle</th>
<th>Initial <span class="math inline">\(x\)</span></th>
<th>Best <span class="math inline">\(p_i\)</span></th>
<th>Global <span class="math inline">\(g\)</span></th>
<th>Update Rule</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.5</td>
<td>2.5</td>
<td>1.7</td>
<td>Moves left</td>
</tr>
<tr class="even">
<td>2</td>
<td>-1.0</td>
<td>-1.0</td>
<td>1.7</td>
<td>Moves right</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3.1</td>
<td>3.1</td>
<td>1.7</td>
<td>Moves left</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>Eventually, all particles converge near <span class="math inline">\(x = 1.7\)</span> (the global minimum).</p>
</section>
<section id="tiny-code-3" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-3">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> x2 <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> math.sin(x)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>num_particles <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_particles)]</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> [<span class="dv">0</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_particles)]</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> x[:]</span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> <span class="bu">min</span>(p, key<span class="op">=</span>f)</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>w, c1, c2 <span class="op">=</span> <span class="fl">0.7</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_particles):</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a>        r1, r2 <span class="op">=</span> random.random(), random.random()</span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a>        v[i] <span class="op">=</span> w<span class="op">*</span>v[i] <span class="op">+</span> c1<span class="op">*</span>r1<span class="op">*</span>(p[i]<span class="op">-</span>x[i]) <span class="op">+</span> c2<span class="op">*</span>r2<span class="op">*</span>(g<span class="op">-</span>x[i])</span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>        x[i] <span class="op">+=</span> v[i]</span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(x[i]) <span class="op">&lt;</span> f(p[i]):</span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a>            p[i] <span class="op">=</span> x[i]</span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(x[i]) <span class="op">&lt;</span> f(g):</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> x[i]</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best solution:"</span>, g, <span class="st">"f(x):"</span>, f(g))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb101"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize arrays x[], v[], p[] for N particles.</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Loop: update v[i], x[i], evaluate f(x[i]),</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="co">// track global best g and local best p[i].</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-53" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-53">Why It Matters</h4>
<ul>
<li>Requires no gradient, ideal for black-box problems.</li>
<li>Fast, parallelizable, and robust.</li>
<li>Naturally balances exploration (through inertia) and exploitation (through social learning).</li>
<li>Effective across continuous and combinatorial spaces.</li>
</ul>
<p>PSO has become a go-to algorithm for parameter optimization in engineering, AI, and scientific modeling.</p>
</section>
<section id="a-gentle-proof-why-it-works-52" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-52">A Gentle Proof (Why It Works)</h4>
<p>The swarm’s behavior emerges from feedback loops between velocity, position, and best memories. When inertia is high (<span class="math inline">\(\omega\)</span> large), particles explore broadly. When <span class="math inline">\(\omega\)</span> is small, attraction to <span class="math inline">\(p_i\)</span> and <span class="math inline">\(g\)</span> dominates, leading to convergence.</p>
<p>Stability analysis shows that under suitable <span class="math inline">\(\omega, c_1, c_2\)</span> values (commonly <span class="math inline">\(\omega \in [0.4,0.9]\)</span>, <span class="math inline">\(c_1=c_2=2\)</span>), the system converges probabilistically toward an equilibrium near the optimum.</p>
</section>
<section id="try-it-yourself-53" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-53">Try It Yourself</h4>
<ol type="1">
<li><p>Plot particle trajectories for <span class="math inline">\(f(x)=x^2\)</span>.</p></li>
<li><p>Tune <span class="math inline">\(\omega\)</span>:</p>
<ul>
<li>Too high → oscillations</li>
<li>Too low → premature convergence</li>
</ul></li>
<li><p>Compare PSO with Simulated Annealing on the same function.</p></li>
<li><p>Extend to 2D functions like Rastrigin or Rosenbrock.</p></li>
<li><p>Observe swarm dynamics visually.</p></li>
</ol>
</section>
<section id="test-cases-53" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-53">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 14%">
<col style="width: 25%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Dimension</th>
<th>Objective</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sphere function</td>
<td>1–10D</td>
<td><span class="math inline">\(x^2\)</span></td>
<td>Fast convergence</td>
</tr>
<tr class="even">
<td>Rastrigin</td>
<td>2D</td>
<td>Multi-modal</td>
<td>Good global search</td>
</tr>
<tr class="odd">
<td>Neural network tuning</td>
<td>High-D</td>
<td>Validation error</td>
<td>Smooth exploration</td>
</tr>
<tr class="even">
<td>PID controller tuning</td>
<td>3D</td>
<td>Control error</td>
<td>Stable convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-53" class="level4">
<h4 class="anchored" data-anchor-id="complexity-53">Complexity</h4>
<ul>
<li><p>Time: <span class="math inline">\(O(N_p \cdot T)\)</span></p>
<ul>
<li><span class="math inline">\(N_p\)</span>: number of particles</li>
<li><span class="math inline">\(T\)</span>: iterations</li>
</ul></li>
<li><p>Space: <span class="math inline">\(O(N_p)\)</span></p></li>
<li><p>Convergence: depends on parameter balance and noise level</p></li>
</ul>
<p>Particle Swarm Optimization is evolution without genes, a choreography of searchers learning from one another until they flock around the truth.</p>
</section>
</section>
<section id="ant-colony-optimization-aco" class="level3">
<h3 class="anchored" data-anchor-id="ant-colony-optimization-aco">955. Ant Colony Optimization (ACO)</h3>
<p>Ant Colony Optimization (ACO) is a metaheuristic inspired by how real ants find the shortest path to food sources using pheromone trails. It turns collective behavior into a computational method for solving difficult combinatorial problems such as routing, scheduling, and network optimization.</p>
<section id="what-problem-are-we-solving-54" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-54">What Problem Are We Solving?</h4>
<p>Many combinatorial optimization problems, like the Traveling Salesman Problem (TSP) or network routing, are too large for exhaustive search. We need algorithms that can efficiently explore and share information about promising paths.</p>
<p>Ant Colony Optimization provides a distributed probabilistic search that gradually strengthens good solutions through reinforcement.</p>
</section>
<section id="the-core-idea-24" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-24">The Core Idea</h4>
<p>Ants explore possible solutions (paths) and communicate indirectly by depositing pheromones, which represent how desirable each choice is. Over time:</p>
<ul>
<li>Shorter or better paths accumulate more pheromone</li>
<li>Pheromones evaporate, reducing attraction to bad paths</li>
<li>The colony converges on high-quality solutions</li>
</ul>
</section>
<section id="mathematical-formulation-3" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-3">Mathematical Formulation</h4>
<p>Each ant builds a path step by step based on probabilities determined by:</p>
<ol type="1">
<li>Pheromone trail strength <span class="math inline">\(\tau_{ij}\)</span> on edge <span class="math inline">\((i, j)\)</span></li>
<li>Heuristic desirability <span class="math inline">\(\eta_{ij}\)</span> (e.g., inverse of distance)</li>
</ol>
<p>The probability that an ant moves from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> is:</p>
<p><span class="math display">\[
P_{ij} =
\frac{\tau_{ij}^\alpha \eta_{ij}^\beta}
{\sum_{k \in \text{allowed}} \tau_{ik}^\alpha \eta_{ik}^\beta}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> controls the influence of pheromones</li>
<li><span class="math inline">\(\beta\)</span> controls the influence of heuristic information</li>
</ul>
<p>After all ants construct solutions, pheromones are updated:</p>
<p><span class="math display">\[
\tau_{ij} \leftarrow (1 - \rho) \tau_{ij} + \sum_k \Delta \tau_{ij}^k
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is evaporation rate, and <span class="math display">\[
\Delta \tau_{ij}^k =
\begin{cases}
\frac{Q}{L_k}, &amp; \text{if ant } k \text{ used edge } (i,j),\\
0, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>with <span class="math inline">\(L_k\)</span> being the total length (cost) of ant <span class="math inline">\(k\)</span>’s path.</p>
</section>
<section id="how-it-works-plain-language-13" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-13">How It Works (Plain Language)</h4>
<p>Picture a group of ants searching for food. Initially, they wander randomly, leaving pheromone trails. Ants that happen to find a short path back to the nest reinforce it with more pheromone. Later ants prefer stronger trails, making short paths more likely to be reused. Evaporation ensures exploration doesn’t stop too soon.</p>
<p>Over time, the colony collectively “discovers” the best routes.</p>
</section>
<section id="step-by-step-example-traveling-salesman-problem" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-traveling-salesman-problem">Step-by-Step Example (Traveling Salesman Problem)</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 20%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Phase</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialization</td>
<td>Set pheromones <span class="math inline">\(\tau_{ij} = \tau_0\)</span> on all edges</td>
</tr>
<tr class="even">
<td>2</td>
<td>Construction</td>
<td>Each ant builds a complete tour probabilistically</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Evaluation</td>
<td>Compute length <span class="math inline">\(L_k\)</span> of each tour</td>
</tr>
<tr class="even">
<td>4</td>
<td>Update</td>
<td>Deposit pheromone <span class="math inline">\(\propto 1/L_k\)</span>; evaporate <span class="math inline">\(\rho\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until convergence or max iterations</td>
</tr>
</tbody>
</table>
<p>Ants indirectly collaborate through pheromone feedback, no direct communication needed.</p>
</section>
<section id="tiny-code-4" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-4">Tiny Code</h4>
<p>Python (Simplified TSP)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math, random</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distance(a, b): <span class="cf">return</span> math.hypot(a[<span class="dv">0</span>]<span class="op">-</span>b[<span class="dv">0</span>], a[<span class="dv">1</span>]<span class="op">-</span>b[<span class="dv">1</span>])</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>cities <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">1</span>,<span class="dv">5</span>), (<span class="dv">5</span>,<span class="dv">2</span>), (<span class="dv">6</span>,<span class="dv">6</span>)]</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(cities)</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>pheromone <span class="op">=</span> [[<span class="dv">1</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n)] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>alpha, beta, rho, Q <span class="op">=</span> <span class="fl">1.0</span>, <span class="fl">3.0</span>, <span class="fl">0.5</span>, <span class="dv">100</span></span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tour_length(tour):</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(distance(cities[tour[i]], cities[tour[(i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>n]]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n))</span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> choose_next(current, visited):</span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> []</span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb102-16"><a href="#cb102-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> j <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb102-17"><a href="#cb102-17" aria-hidden="true" tabindex="-1"></a>            tau <span class="op">=</span> pheromone[current][j]alpha</span>
<span id="cb102-18"><a href="#cb102-18" aria-hidden="true" tabindex="-1"></a>            eta <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> (distance(cities[current], cities[j]) <span class="op">+</span> <span class="fl">1e-9</span>))beta</span>
<span id="cb102-19"><a href="#cb102-19" aria-hidden="true" tabindex="-1"></a>            probs.append((j, tau <span class="op">*</span> eta))</span>
<span id="cb102-20"><a href="#cb102-20" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">sum</span>(p <span class="cf">for</span> _, p <span class="kw">in</span> probs)</span>
<span id="cb102-21"><a href="#cb102-21" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> random.random() <span class="op">*</span> total</span>
<span id="cb102-22"><a href="#cb102-22" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb102-23"><a href="#cb102-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, p <span class="kw">in</span> probs:</span>
<span id="cb102-24"><a href="#cb102-24" aria-hidden="true" tabindex="-1"></a>        s <span class="op">+=</span> p</span>
<span id="cb102-25"><a href="#cb102-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> s <span class="op">&gt;=</span> r:</span>
<span id="cb102-26"><a href="#cb102-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> j</span>
<span id="cb102-27"><a href="#cb102-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-28"><a href="#cb102-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb102-29"><a href="#cb102-29" aria-hidden="true" tabindex="-1"></a>    tours <span class="op">=</span> []</span>
<span id="cb102-30"><a href="#cb102-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):  <span class="co"># ants</span></span>
<span id="cb102-31"><a href="#cb102-31" aria-hidden="true" tabindex="-1"></a>        tour <span class="op">=</span> [random.randint(<span class="dv">0</span>, n<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb102-32"><a href="#cb102-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="bu">len</span>(tour) <span class="op">&lt;</span> n:</span>
<span id="cb102-33"><a href="#cb102-33" aria-hidden="true" tabindex="-1"></a>            next_city <span class="op">=</span> choose_next(tour[<span class="op">-</span><span class="dv">1</span>], tour)</span>
<span id="cb102-34"><a href="#cb102-34" aria-hidden="true" tabindex="-1"></a>            tour.append(next_city)</span>
<span id="cb102-35"><a href="#cb102-35" aria-hidden="true" tabindex="-1"></a>        tours.append(tour)</span>
<span id="cb102-36"><a href="#cb102-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-37"><a href="#cb102-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaporate</span></span>
<span id="cb102-38"><a href="#cb102-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb102-39"><a href="#cb102-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb102-40"><a href="#cb102-40" aria-hidden="true" tabindex="-1"></a>            pheromone[i][j] <span class="op">*=</span> (<span class="dv">1</span> <span class="op">-</span> rho)</span>
<span id="cb102-41"><a href="#cb102-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-42"><a href="#cb102-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># deposit</span></span>
<span id="cb102-43"><a href="#cb102-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tour <span class="kw">in</span> tours:</span>
<span id="cb102-44"><a href="#cb102-44" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> tour_length(tour)</span>
<span id="cb102-45"><a href="#cb102-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb102-46"><a href="#cb102-46" aria-hidden="true" tabindex="-1"></a>            a, b <span class="op">=</span> tour[i], tour[(i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>n]</span>
<span id="cb102-47"><a href="#cb102-47" aria-hidden="true" tabindex="-1"></a>            pheromone[a][b] <span class="op">+=</span> Q <span class="op">/</span> L</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-54" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-54">Why It Matters</h4>
<ul>
<li>Powerful for NP-hard problems (TSP, VRP, scheduling).</li>
<li>Adaptable to dynamic environments (e.g., changing networks).</li>
<li>Distributed and parallel by nature.</li>
<li>Combines exploitation (pheromones) and exploration (random choice).</li>
</ul>
<p>ACO is one of the most successful swarm intelligence algorithms in optimization and logistics.</p>
</section>
<section id="a-gentle-proof-why-it-works-53" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-53">A Gentle Proof (Why It Works)</h4>
<p>Ants collectively approximate a reinforcement learning process. Each pheromone update acts like a weighted reward signal:</p>
<p><span class="math display">\[
\tau_{ij}(t+1) = (1-\rho)\tau_{ij}(t) + \mathbb{E}\left[\frac{Q}{L}\right]
\]</span></p>
<p>Evaporation ensures forgetting of poor paths; reinforcement strengthens good ones. The colony asymptotically concentrates pheromones on optimal (or near-optimal) paths.</p>
</section>
<section id="try-it-yourself-54" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-54">Try It Yourself</h4>
<ol type="1">
<li><p>Implement ACO for a 10-city TSP.</p></li>
<li><p>Plot pheromone intensities after each iteration.</p></li>
<li><p>Tune <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\rho\)</span>:</p>
<ul>
<li>High <span class="math inline">\(\alpha\)</span> → exploitation dominates.</li>
<li>High <span class="math inline">\(\beta\)</span> → greedy heuristic bias.</li>
<li>Low <span class="math inline">\(\rho\)</span> → pheromone saturation.</li>
</ul></li>
<li><p>Compare convergence speed for different parameters.</p></li>
<li><p>Apply to network routing or job sequencing.</p></li>
</ol>
</section>
<section id="test-cases-54" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-54">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 23%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Problem</th>
<th>Search Space</th>
<th>Objective</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>TSP</td>
<td>Cities and edges</td>
<td>Shortest tour</td>
<td>Canonical benchmark</td>
</tr>
<tr class="even">
<td>Job scheduling</td>
<td>Tasks</td>
<td>Minimize makespan</td>
<td>Industrial planning</td>
</tr>
<tr class="odd">
<td>Graph coloring</td>
<td>Nodes</td>
<td>Fewer colors</td>
<td>Constraint satisfaction</td>
</tr>
<tr class="even">
<td>Network routing</td>
<td>Paths</td>
<td>Minimize latency</td>
<td>Dynamic version</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-54" class="level4">
<h4 class="anchored" data-anchor-id="complexity-54">Complexity</h4>
<ul>
<li><p>Time: <span class="math inline">\(O(m \cdot n^2)\)</span> per iteration</p>
<ul>
<li><span class="math inline">\(m\)</span>: number of ants</li>
<li><span class="math inline">\(n\)</span>: number of nodes</li>
</ul></li>
<li><p>Space: <span class="math inline">\(O(n^2)\)</span> (pheromone matrix)</p></li>
<li><p>Convergence: strongly influenced by evaporation <span class="math inline">\(\rho\)</span></p></li>
</ul>
<p>Ant Colony Optimization shows how <em>simple agents with local rules</em> can create intelligent, emergent global behavior, nature’s optimization at its finest, reimagined in code.</p>
</section>
</section>
<section id="differential-evolution-de" class="level3">
<h3 class="anchored" data-anchor-id="differential-evolution-de">956. Differential Evolution (DE)</h3>
<p>Differential Evolution (DE) is a population-based optimization algorithm designed for continuous-valued functions. It’s simple, robust, and remarkably effective, a minimalist cousin of Genetic Algorithms that replaces complex crossover rules with a single elegant operation: <em>vector difference mutation.</em></p>
<section id="what-problem-are-we-solving-55" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-55">What Problem Are We Solving?</h4>
<p>DE is used to solve continuous optimization problems where:</p>
<ul>
<li>The function is nonlinear or multimodal</li>
<li>Derivatives are unavailable or unreliable</li>
<li>We need a balance between exploration and precision</li>
</ul>
<p>It’s particularly effective for engineering design, neural tuning, and simulation calibration tasks where gradients are hard to compute.</p>
</section>
<section id="the-core-idea-25" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-25">The Core Idea</h4>
<p>A DE population consists of candidate solutions (vectors) that evolve over generations by combining other individuals to create new trial vectors.</p>
<p>Each generation involves three key steps:</p>
<ol type="1">
<li>Mutation, create a trial vector by adding a scaled difference between two population vectors to a third.</li>
<li>Crossover, mix components of the trial with the current vector.</li>
<li>Selection, keep the better vector based on fitness.</li>
</ol>
<p>This simple arithmetic process efficiently explores the space without needing derivatives.</p>
</section>
<section id="mathematical-formulation-4" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-4">Mathematical Formulation</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(x_i^{(t)}\)</span> be the <span class="math inline">\(i\)</span>-th individual at generation <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(f(x)\)</span> be the objective function to minimize</li>
</ul>
<p>Then for each <span class="math inline">\(x_i\)</span>:</p>
<ol type="1">
<li>Mutation:</li>
</ol>
<p><span class="math display">\[
v_i = x_{r_1} + F \cdot (x_{r_2} - x_{r_3})
\]</span></p>
<p>where <span class="math inline">\(r_1, r_2, r_3\)</span> are distinct random indices and <span class="math inline">\(F \in [0, 2]\)</span> is a scaling factor controlling mutation strength.</p>
<ol start="2" type="1">
<li>Crossover:</li>
</ol>
<p>For each dimension <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[
u_{i,j} =
\begin{cases}
v_{i,j}, &amp; \text{if } r_j &lt; C_r \text{ or } j = j_{\text{rand}},\\
x_{i,j}, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(C_r\)</span> is the crossover rate.</p>
<ol start="3" type="1">
<li>Selection:</li>
</ol>
<p><span class="math display">\[
x_i^{(t+1)} =
\begin{cases}
u_i, &amp; \text{if } f(u_i) &lt; f(x_i^{(t)}),\\
x_i^{(t)}, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
</section>
<section id="how-it-works-plain-language-14" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-14">How It Works (Plain Language)</h4>
<p>Imagine a group of explorers scattered across a mountain range (the search space). Each explorer tries new directions by combining the positions of three others, effectively following the vector between them. If a new position is better (lower altitude), it replaces the old one. Over time, the group collectively drifts downhill toward the global minimum.</p>
</section>
<section id="step-by-step-example-2" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-2">Step-by-Step Example</h4>
<p>Goal: minimize <span class="math inline">\(f(x, y) = x^2 + y^2\)</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 33%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Operation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize population</td>
<td>Random 2D points</td>
</tr>
<tr class="even">
<td>2</td>
<td>Mutation</td>
<td><span class="math inline">\(v_i = x_{r_1} + F(x_{r_2} - x_{r_3})\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Crossover</td>
<td>Mix <span class="math inline">\(v_i\)</span> with <span class="math inline">\(x_i\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Selection</td>
<td>Keep the lower <span class="math inline">\(f(x)\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat</td>
<td>Until convergence</td>
</tr>
</tbody>
</table>
<p>After a few generations, all individuals converge near <span class="math inline">\((0, 0)\)</span>, the optimal point.</p>
</section>
<section id="tiny-code-5" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-5">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y): <span class="cf">return</span> x2 <span class="op">+</span> y2</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>F, CR, NP, D <span class="op">=</span> <span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="dv">10</span>, <span class="dv">2</span></span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>pop <span class="op">=</span> [[random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(D)] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(NP)]</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> gen <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(NP):</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>        r1, r2, r3 <span class="op">=</span> random.sample(<span class="bu">range</span>(NP), <span class="dv">3</span>)</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> [pop[r1][j] <span class="op">+</span> F <span class="op">*</span> (pop[r2][j] <span class="op">-</span> pop[r3][j]) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(D)]</span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> [v[j] <span class="cf">if</span> random.random() <span class="op">&lt;</span> CR <span class="cf">else</span> pop[i][j] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(D)]</span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(<span class="op">*</span>u) <span class="op">&lt;</span> f(<span class="op">*</span>pop[i]):</span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a>            pop[i] <span class="op">=</span> u</span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> <span class="bu">min</span>(pop, key<span class="op">=</span><span class="kw">lambda</span> p: f(<span class="op">*</span>p))</span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best:"</span>, best, <span class="st">"f(x):"</span>, f(<span class="op">*</span>best))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb104"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize population of D-dimensional vectors.</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="co">// For each individual:</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="co">//   Select r1, r2, r3 distinct.</span></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="co">//   Compute mutant vector v = x[r1] + F*(x[r2]-x[r3]).</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a><span class="co">//   Perform crossover, evaluate trial u.</span></span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a><span class="co">//   Replace x[i] if f(u) &lt; f(x[i]).</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-55" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-55">Why It Matters</h4>
<ul>
<li>Few parameters, only <span class="math inline">\(F\)</span> and <span class="math inline">\(C_r\)</span>.</li>
<li>No derivatives required, works with black-box objectives.</li>
<li>Handles noisy and discontinuous functions.</li>
<li>Excellent global convergence on many continuous benchmarks.</li>
<li>Simple yet powerful, often outperforms more complex methods.</li>
</ul>
<p>Differential Evolution embodies the principle of “less is more”, minimal rules, maximal performance.</p>
</section>
<section id="a-gentle-proof-why-it-works-54" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-54">A Gentle Proof (Why It Works)</h4>
<p>The mutation rule ensures <em>directed diversity</em>: differences between random individuals guide exploration toward new, potentially better regions. This maintains population diversity and prevents premature convergence.</p>
<p>Under standard assumptions, DE converges stochastically to a global minimum as long as the mutation and selection preserve enough variance in the population.</p>
</section>
<section id="try-it-yourself-55" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-55">Try It Yourself</h4>
<ol type="1">
<li>Optimize <span class="math inline">\(f(x, y) = (x - 3)^2 + (y + 2)^2\)</span>.</li>
<li>Tune <span class="math inline">\(F\)</span> and <span class="math inline">\(C_r\)</span>, try <span class="math inline">\(F=0.5, 0.9, 1.2\)</span>.</li>
<li>Track convergence speed with different population sizes.</li>
<li>Compare to Particle Swarm Optimization on the same function.</li>
<li>Apply DE to train a neural network’s weights directly.</li>
</ol>
</section>
<section id="test-cases-55" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-55">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Domain</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sphere</td>
<td>Continuous</td>
<td>Smooth convex</td>
</tr>
<tr class="even">
<td>Rastrigin</td>
<td>Multimodal</td>
<td>Tests global search</td>
</tr>
<tr class="odd">
<td>Ackley</td>
<td>Nonlinear</td>
<td>Harsh gradients</td>
</tr>
<tr class="even">
<td>Engineering design</td>
<td>Real-valued</td>
<td>Practical use case</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-55" class="level4">
<h4 class="anchored" data-anchor-id="complexity-55">Complexity</h4>
<ul>
<li><p>Time: <span class="math inline">\(O(NP \cdot D \cdot G)\)</span></p>
<ul>
<li><span class="math inline">\(NP\)</span>: population size</li>
<li><span class="math inline">\(D\)</span>: dimension</li>
<li><span class="math inline">\(G\)</span>: generations</li>
</ul></li>
<li><p>Space: <span class="math inline">\(O(NP \cdot D)\)</span></p></li>
<li><p>Convergence: typically fast and stable for small parameter sets</p></li>
</ul>
<p>Differential Evolution is optimization stripped to its essentials, a quiet but relentless drift through space, driven only by difference and selection.</p>
</section>
</section>
<section id="harmony-search" class="level3">
<h3 class="anchored" data-anchor-id="harmony-search">957. Harmony Search</h3>
<p>Harmony Search (HS) is a metaheuristic optimization algorithm inspired by the improvisation process of musicians seeking harmony. Just as musicians adjust pitches to achieve a pleasing sound, the algorithm adjusts solution variables to minimize (or maximize) an objective function.</p>
<p>It’s simple, flexible, and effective for both continuous and discrete optimization, especially when the search space is irregular or noisy.</p>
<section id="what-problem-are-we-solving-56" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-56">What Problem Are We Solving?</h4>
<p>Many optimization problems are like musical compositions: they involve balancing multiple variables (or notes) to achieve an optimal outcome. Traditional algorithms can get stuck or require gradient information, but Harmony Search explores the space with creativity and variation, without derivatives.</p>
<p>It is widely used in:</p>
<ul>
<li>Engineering design optimization</li>
<li>Scheduling and allocation problems</li>
<li>Neural network parameter tuning</li>
<li>Structural design</li>
</ul>
</section>
<section id="the-core-idea-26" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-26">The Core Idea</h4>
<p>Harmony Search is based on a harmony memory (HM), a collection of solution vectors (harmonies). New solutions are generated by improvising from the memory, using three rules:</p>
<ol type="1">
<li>Memory consideration, pick a value from existing harmonies.</li>
<li>Pitch adjustment, fine-tune that value slightly.</li>
<li>Random selection, introduce entirely new values for diversity.</li>
</ol>
<p>The best harmonies are kept, replacing the worst, iteratively improving the composition.</p>
</section>
<section id="mathematical-formulation-5" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-5">Mathematical Formulation</h4>
<p>Let the optimization problem be:</p>
<p><span class="math display">\[
\min f(x_1, x_2, \dots, x_N)
\]</span></p>
<p>where each variable <span class="math inline">\(x_i\)</span> has range <span class="math inline">\([L_i, U_i]\)</span>.</p>
<p>At each iteration:</p>
<ol type="1">
<li><p>Initialize harmony memory (HM): <span class="math display">\[
HM = {x^{(1)}, x^{(2)}, \dots, x^{(HMS)}}
\]</span> where HMS = harmony memory size.</p></li>
<li><p>Improvise a new harmony <span class="math inline">\(x' = (x'_1, x'_2, \dots, x'_N)\)</span>: For each variable <span class="math inline">\(x_i\)</span>:</p>
<ul>
<li>With probability HMCR (Harmony Memory Considering Rate), choose <span class="math inline">\(x_i\)</span> from <span class="math inline">\(HM\)</span>.</li>
<li>Else, choose randomly from <span class="math inline">\([L_i, U_i]\)</span>.</li>
<li>With probability PAR (Pitch Adjusting Rate), modify it slightly: <span class="math display">\[
x'_i \leftarrow x'_i + \delta, \quad \delta \in [-bw, bw]
\]</span> where <span class="math inline">\(bw\)</span> is bandwidth (tuning parameter).</li>
</ul></li>
<li><p>Evaluate <span class="math inline">\(f(x')\)</span>.</p></li>
<li><p>Update HM, if <span class="math inline">\(x'\)</span> is better than the worst harmony, replace it.</p></li>
</ol>
<p>Repeat until stopping criterion (iterations or convergence) is met.</p>
</section>
<section id="how-it-works-plain-language-15" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-15">How It Works (Plain Language)</h4>
<p>Imagine a jazz band improvising. Each musician (variable) listens to others and chooses a note (value) from past harmonies or invents a new one. Occasionally, they tweak their pitch (fine-tune a parameter). As they play, the overall sound (objective function) improves, converging toward harmony, the optimal solution.</p>
</section>
<section id="step-by-step-example-3" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-3">Step-by-Step Example</h4>
<p>Goal: minimize <span class="math inline">\(f(x, y) = x^2 + y^2\)</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Action</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize HM</td>
<td>3 random pairs <span class="math inline">\((x, y)\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Improvise</td>
<td>Pick <span class="math inline">\(x, y\)</span> from HM or random</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Adjust pitch</td>
<td>Slightly modify <span class="math inline">\(x, y\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Evaluate</td>
<td>Compute <span class="math inline">\(f(x, y)\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update HM</td>
<td>Keep top 3 harmonies</td>
</tr>
</tbody>
</table>
<p>After several iterations, HM converges near <span class="math inline">\((0, 0)\)</span>, the global optimum.</p>
</section>
<section id="tiny-code-6" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-6">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y): <span class="cf">return</span> x2 <span class="op">+</span> y2</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>HMS, HMCR, PAR, bw <span class="op">=</span> <span class="dv">3</span>, <span class="fl">0.9</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>HM <span class="op">=</span> [[random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(HMS)]</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>    new <span class="op">=</span> []</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> HMCR:</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>            new.append(random.choice(HM)[i])</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> random.random() <span class="op">&lt;</span> PAR:</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>                new[i] <span class="op">+=</span> random.uniform(<span class="op">-</span>bw, bw)</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>            new.append(random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> f(<span class="op">*</span>new) <span class="op">&lt;</span> <span class="bu">max</span>(HM, key<span class="op">=</span><span class="kw">lambda</span> h: f(<span class="op">*</span>h), default<span class="op">=</span>new):</span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a>        HM[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> new</span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a>    HM.sort(key<span class="op">=</span><span class="kw">lambda</span> h: f(<span class="op">*</span>h))</span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-21"><a href="#cb105-21" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> HM[<span class="dv">0</span>]</span>
<span id="cb105-22"><a href="#cb105-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best:"</span>, best, <span class="st">"f(x):"</span>, f(<span class="op">*</span>best))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb106"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Store harmony memory as 2D array.</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Generate new harmony using HMCR, PAR, and bw parameters.</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="co">// Replace worst harmony if new one is better.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-56" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-56">Why It Matters</h4>
<ul>
<li>Derivative-free optimization.</li>
<li>Balance between memory and creativity.</li>
<li>Few parameters: HMCR, PAR, and bw.</li>
<li>Effective for complex, nonlinear, multi-modal problems.</li>
<li>Naturally supports multi-objective extensions.</li>
</ul>
<p>Harmony Search captures the essence of <em>explore, refine, and remember</em>, just like a real musician perfecting a melody.</p>
</section>
<section id="a-gentle-proof-why-it-works-55" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-55">A Gentle Proof (Why It Works)</h4>
<p>Harmony Search works through a stochastic combination of intensification (using memory) and diversification (random variation). The convergence of HM toward optimal solutions follows from probabilistic sampling, over time, high-fitness harmonies dominate the population.</p>
<p>With a high HMCR and moderate PAR, the algorithm maintains balance between reuse and innovation, preventing stagnation while refining solutions.</p>
</section>
<section id="try-it-yourself-56" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-56">Try It Yourself</h4>
<ol type="1">
<li>Optimize <span class="math inline">\(f(x, y) = x^2 + y^2 + 3\sin(2x)\)</span> using HS.</li>
<li>Vary HMCR (0.5–0.95) and PAR (0.1–0.5), observe exploration vs exploitation.</li>
<li>Use larger HM for higher-dimensional problems.</li>
<li>Compare with Genetic Algorithm and Differential Evolution.</li>
<li>Apply to parameter tuning (e.g., ML hyperparameters).</li>
</ol>
</section>
<section id="test-cases-56" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-56">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Domain</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sphere</td>
<td>Continuous</td>
<td>Simple benchmark</td>
</tr>
<tr class="even">
<td>Rastrigin</td>
<td>Nonlinear</td>
<td>Multi-modal surface</td>
</tr>
<tr class="odd">
<td>Scheduling</td>
<td>Discrete</td>
<td>Combinatorial use</td>
</tr>
<tr class="even">
<td>Neural tuning</td>
<td>Continuous</td>
<td>Efficient with few evaluations</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-56" class="level4">
<h4 class="anchored" data-anchor-id="complexity-56">Complexity</h4>
<ul>
<li><p>Time: <span class="math inline">\(O(HMS \times N \times T)\)</span></p>
<ul>
<li><span class="math inline">\(HMS\)</span>: harmony memory size</li>
<li><span class="math inline">\(N\)</span>: number of variables</li>
<li><span class="math inline">\(T\)</span>: iterations</li>
</ul></li>
<li><p>Space: <span class="math inline">\(O(HMS \times N)\)</span></p></li>
</ul>
<p>Harmony Search turns optimization into an art form, solutions “play together” and gradually find the most resonant chord in the landscape of possibilities.</p>
</section>
</section>
<section id="firefly-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="firefly-algorithm">958. Firefly Algorithm</h3>
<p>The Firefly Algorithm (FA) is a swarm intelligence method inspired by the flashing patterns of fireflies. Each firefly represents a candidate solution that moves toward brighter (better) ones, with movement intensity governed by light absorption and random perturbation.</p>
<p>It’s simple, parallel, and powerful for continuous, nonlinear, and multi-modal optimization problems.</p>
<section id="what-problem-are-we-solving-57" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-57">What Problem Are We Solving?</h4>
<p>Many optimization landscapes have multiple local minima. Classic gradient-based algorithms easily get stuck in one. Firefly Algorithm introduces <em>collective attraction</em> and <em>controlled randomness</em> to search globally while retaining local refinement.</p>
<p>It’s especially effective for:</p>
<ul>
<li>Engineering optimization</li>
<li>Image processing and feature selection</li>
<li>Machine learning parameter tuning</li>
<li>Benchmark function optimization</li>
</ul>
</section>
<section id="the-core-idea-27" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-27">The Core Idea</h4>
<p>Each firefly has a brightness proportional to its fitness (solution quality). A less bright firefly moves toward a brighter one, and brightness decreases with distance. Random motion ensures exploration when no brighter neighbor exists.</p>
</section>
<section id="mathematical-formulation-6" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-6">Mathematical Formulation</h4>
<p>For fireflies <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> at positions <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>, their movement is governed by:</p>
<ol type="1">
<li><p>Attractiveness function: <span class="math display">\[
\beta(r) = \beta_0 e^{-\gamma r^2}
\]</span> where</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the maximum attractiveness,</li>
<li><span class="math inline">\(\gamma\)</span> is the light absorption coefficient,</li>
<li><span class="math inline">\(r = |x_i - x_j|\)</span> is the distance.</li>
</ul></li>
<li><p>Movement rule: <span class="math display">\[
x_i \leftarrow x_i + \beta_0 e^{-\gamma r^2} (x_j - x_i) + \alpha (\text{rand} - 0.5)
\]</span></p></li>
</ol>
<p>where:</p>
<ul>
<li>The second term moves <span class="math inline">\(i\)</span> toward brighter firefly <span class="math inline">\(j\)</span>,</li>
<li>The third term adds random noise controlled by <span class="math inline">\(\alpha\)</span>.</li>
</ul>
</section>
<section id="how-it-works-plain-language-16" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-16">How It Works (Plain Language)</h4>
<p>Imagine fireflies on a dark field. Each emits light according to how “good” its location is. Dimmer fireflies fly toward brighter ones. The closer they are, the stronger the attraction. Over time, clusters of fireflies gather around the brightest spots, the best solutions.</p>
<p>Random motion keeps them from settling too early into suboptimal regions.</p>
</section>
<section id="step-by-step-summary-23" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-23">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize random population of fireflies</td>
</tr>
<tr class="even">
<td>2</td>
<td>Evaluate brightness using objective function</td>
</tr>
<tr class="odd">
<td>3</td>
<td>For each pair <span class="math inline">\((i,j)\)</span>, move <span class="math inline">\(i\)</span> toward <span class="math inline">\(j\)</span> if <span class="math inline">\(j\)</span> is brighter</td>
</tr>
<tr class="even">
<td>4</td>
<td>Add small random motion</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update brightness and repeat until convergence</td>
</tr>
</tbody>
</table>
</section>
<section id="example-33" class="level4">
<h4 class="anchored" data-anchor-id="example-33">Example</h4>
<p>Goal: minimize <span class="math inline">\(f(x) = x^2 + 3\sin(x)\)</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 27%">
<col style="width: 25%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Firefly Positions</th>
<th>Best Position</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Random scattered</td>
<td>1.7</td>
<td>Brightest (lowest <span class="math inline">\(f\)</span>)</td>
</tr>
<tr class="even">
<td>5</td>
<td>Cluster around 1.8</td>
<td>Stable region</td>
<td>Fewer random jumps</td>
</tr>
<tr class="odd">
<td>10</td>
<td>All near 1.7</td>
<td>Converged optimum</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-7" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-7">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random, math</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> x2 <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> math.sin(x)</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span>  <span class="co"># number of fireflies</span></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>beta0, gamma, alpha <span class="op">=</span> <span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.2</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>fireflies <span class="op">=</span> [random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> f(fireflies[j]) <span class="op">&lt;</span> f(fireflies[i]):</span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>                r <span class="op">=</span> <span class="bu">abs</span>(fireflies[i] <span class="op">-</span> fireflies[j])</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>                beta <span class="op">=</span> beta0 <span class="op">*</span> math.exp(<span class="op">-</span>gamma <span class="op">*</span> r2)</span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>                fireflies[i] <span class="op">+=</span> beta <span class="op">*</span> (fireflies[j] <span class="op">-</span> fireflies[i]) <span class="op">+</span> alpha <span class="op">*</span> (random.random() <span class="op">-</span> <span class="fl">0.5</span>)</span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> <span class="bu">min</span>(fireflies, key<span class="op">=</span>f)</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best:"</span>, best, <span class="st">"f(x):"</span>, f(best))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb108"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Initialize positions x[i]</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="co">// For each iteration:</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="co">//   For each pair (i,j):</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="co">//     if f(x[j]) &lt; f(x[i]): move x[i] toward x[j]</span></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a><span class="co">//     apply random perturbation</span></span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a><span class="co">// Track best solution.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-57" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-57">Why It Matters</h4>
<ul>
<li>Global and local search balance, controlled by <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\alpha\)</span>.</li>
<li>Simple yet robust for continuous problems.</li>
<li>Handles nonconvex, discontinuous, or noisy objectives.</li>
<li>Adaptable to multiobjective and discrete variants.</li>
<li>Naturally parallelizable, each firefly acts independently.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-56" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-56">A Gentle Proof (Why It Works)</h4>
<p>Attraction decays exponentially with distance, ensuring local convergence around good solutions while maintaining global diversity via random perturbations.</p>
<p>Formally, the expected displacement magnitude per iteration:</p>
<p><span class="math display">\[
E[|x_i^{t+1} - x_i^t|] \le \beta_0 e^{-\gamma r^2} |x_j - x_i| + O(\alpha)
\]</span></p>
<p>decreases as <span class="math inline">\(\gamma\)</span> increases or <span class="math inline">\(\alpha\)</span> decreases, guaranteeing stability once near minima.</p>
</section>
<section id="try-it-yourself-57" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-57">Try It Yourself</h4>
<ol type="1">
<li><p>Optimize <span class="math inline">\(f(x) = x^2 + 10\sin(5x)\)</span> and visualize movement.</p></li>
<li><p>Tune parameters:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> (randomness)</li>
<li><span class="math inline">\(\beta_0\)</span> (attraction)</li>
<li><span class="math inline">\(\gamma\)</span> (light absorption)</li>
</ul></li>
<li><p>Compare with Particle Swarm Optimization.</p></li>
<li><p>Run in 2D and plot trajectories.</p></li>
<li><p>Try multi-modal functions like Rastrigin or Ackley.</p></li>
</ol>
</section>
<section id="test-cases-57" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-57">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Domain</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sphere</td>
<td>Continuous</td>
<td>Fast convergence</td>
</tr>
<tr class="even">
<td>Rosenbrock</td>
<td>Nonlinear</td>
<td>Needs moderate randomness</td>
</tr>
<tr class="odd">
<td>Rastrigin</td>
<td>Multi-modal</td>
<td>Global search test</td>
</tr>
<tr class="even">
<td>Engineering design</td>
<td>Continuous</td>
<td>Real-world applicability</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-57" class="level4">
<h4 class="anchored" data-anchor-id="complexity-57">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(n^2)\)</span> per iteration (pairwise attraction)</li>
<li>Space: <span class="math inline">\(O(n)\)</span></li>
<li>Convergence: controlled by <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\gamma\)</span>, and iteration limit</li>
</ul>
<p>The Firefly Algorithm shows how collective attraction, simple, local, and luminous, can light the way to global optimization.</p>
</section>
</section>
<section id="bee-colony-optimization" class="level3">
<h3 class="anchored" data-anchor-id="bee-colony-optimization">959. Bee Colony Optimization</h3>
<p>Bee Colony Optimization (BCO) mimics how real bees forage for nectar, balancing exploration and exploitation through communication, recruitment, and local search. Each bee represents a potential solution that explores or refines different regions of the search space. Over time, the colony collectively converges on the most promising “nectar sources”, optimal or near-optimal solutions.</p>
<section id="what-problem-are-we-solving-58" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-58">What Problem Are We Solving?</h4>
<p>BCO is a population-based metaheuristic used for continuous and combinatorial optimization, especially where:</p>
<ul>
<li>The objective function is nonconvex or noisy</li>
<li>Gradient information is unavailable</li>
<li>Solutions must balance local refinement and global discovery</li>
</ul>
<p>Applications include:</p>
<ul>
<li>Routing and scheduling</li>
<li>Feature selection</li>
<li>Engineering design</li>
<li>Resource allocation</li>
</ul>
</section>
<section id="the-core-idea-28" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-28">The Core Idea</h4>
<p>A bee colony consists of:</p>
<ol type="1">
<li>Employed bees exploring known food sources (solutions)</li>
<li>Onlooker bees selecting food sources based on shared information</li>
<li>Scout bees randomly exploring new areas</li>
</ol>
<p>Through cycles of communication and movement, the colony gradually refines toward optimal solutions.</p>
</section>
<section id="mathematical-formulation-7" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-7">Mathematical Formulation</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(x_i\)</span> be the position (solution) of bee <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(f(x_i)\)</span> be the fitness (nectar quality)</li>
<li><span class="math inline">\(N\)</span> be the number of food sources</li>
</ul>
<p>Each iteration involves:</p>
<ol type="1">
<li><p>Employed Bee Phase: Generate a neighbor solution for each <span class="math inline">\(x_i\)</span>: <span class="math display">\[
v_{ij} = x_{ij} + \phi_{ij}(x_{ij} - x_{kj})
\]</span> where <span class="math inline">\(k \ne i\)</span> and <span class="math inline">\(\phi_{ij} \in [-1, 1]\)</span> is a random coefficient.</p></li>
<li><p>Fitness Evaluation: <span class="math display">\[
\text{fit}(x_i) = \frac{1}{1 + f(x_i)}
\]</span></p></li>
<li><p>Onlooker Bee Phase: Choose food sources with probability proportional to fitness: <span class="math display">\[
p_i = \frac{\text{fit}(x_i)}{\sum_{j=1}^{N} \text{fit}(x_j)}
\]</span></p></li>
<li><p>Scout Bee Phase: Replace stagnating solutions with random ones if they haven’t improved for a preset limit: <span class="math display">\[
x_i = \text{random}(L, U)
\]</span></p></li>
<li><p>Memorize the best solution.</p></li>
</ol>
</section>
<section id="how-it-works-plain-language-17" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-17">How It Works (Plain Language)</h4>
<p>Imagine a colony of bees searching for flowers. Each bee first explores an area (solution) and shares its nectar amount (fitness) in the hive. Onlooker bees watch and choose to follow successful bees to rich fields. If a bee’s source runs dry, it becomes a scout and searches anew. Over time, bees cluster around the richest flowers, the global optimum.</p>
</section>
<section id="step-by-step-summary-24" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-24">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize random food sources</td>
</tr>
<tr class="even">
<td>2</td>
<td>Employed bees explore neighbors</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Onlookers choose best sources</td>
</tr>
<tr class="even">
<td>4</td>
<td>Scouts search new regions</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Memorize best solution and repeat</td>
</tr>
</tbody>
</table>
</section>
<section id="example-34" class="level4">
<h4 class="anchored" data-anchor-id="example-34">Example</h4>
<p>Goal: minimize <span class="math inline">\(f(x) = x^2 + 4\sin(x)\)</span></p>
<p>At each iteration:</p>
<ul>
<li>Bees explore around current best <span class="math inline">\(x\)</span></li>
<li>Onlookers reinforce the promising region</li>
<li>Scouts prevent stagnation by adding randomness</li>
</ul>
<p>Eventually, bees gather near the global minimum around <span class="math inline">\(x \approx -1.4\)</span>.</p>
</section>
<section id="tiny-code-8" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-8">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random, math</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> x2 <span class="op">+</span> <span class="dv">4</span> <span class="op">*</span> math.sin(x)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>N, limit, max_iter <span class="op">=</span> <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span></span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>L, U <span class="op">=</span> <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span></span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>bees <span class="op">=</span> [random.uniform(L, U) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>trial <span class="op">=</span> [<span class="dv">0</span>]<span class="op">*</span>N</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> random.choice([j <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N) <span class="cf">if</span> j <span class="op">!=</span> i])</span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a>        phi <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb109-14"><a href="#cb109-14" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> bees[i] <span class="op">+</span> phi <span class="op">*</span> (bees[i] <span class="op">-</span> bees[k])</span>
<span id="cb109-15"><a href="#cb109-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(v) <span class="op">&lt;</span> f(bees[i]):</span>
<span id="cb109-16"><a href="#cb109-16" aria-hidden="true" tabindex="-1"></a>            bees[i], trial[i] <span class="op">=</span> v, <span class="dv">0</span></span>
<span id="cb109-17"><a href="#cb109-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb109-18"><a href="#cb109-18" aria-hidden="true" tabindex="-1"></a>            trial[i] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb109-19"><a href="#cb109-19" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> [<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>f(b)) <span class="cf">for</span> b <span class="kw">in</span> bees]</span>
<span id="cb109-20"><a href="#cb109-20" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="bu">sum</span>(prob)</span>
<span id="cb109-21"><a href="#cb109-21" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> [p<span class="op">/</span>s <span class="cf">for</span> p <span class="kw">in</span> prob]</span>
<span id="cb109-22"><a href="#cb109-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb109-23"><a href="#cb109-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> prob[i]:</span>
<span id="cb109-24"><a href="#cb109-24" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> random.choice([j <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N) <span class="cf">if</span> j <span class="op">!=</span> i])</span>
<span id="cb109-25"><a href="#cb109-25" aria-hidden="true" tabindex="-1"></a>            phi <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb109-26"><a href="#cb109-26" aria-hidden="true" tabindex="-1"></a>            v <span class="op">=</span> bees[i] <span class="op">+</span> phi <span class="op">*</span> (bees[i] <span class="op">-</span> bees[k])</span>
<span id="cb109-27"><a href="#cb109-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> f(v) <span class="op">&lt;</span> f(bees[i]):</span>
<span id="cb109-28"><a href="#cb109-28" aria-hidden="true" tabindex="-1"></a>                bees[i], trial[i] <span class="op">=</span> v, <span class="dv">0</span></span>
<span id="cb109-29"><a href="#cb109-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb109-30"><a href="#cb109-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> trial[i] <span class="op">&gt;</span> limit:</span>
<span id="cb109-31"><a href="#cb109-31" aria-hidden="true" tabindex="-1"></a>            bees[i] <span class="op">=</span> random.uniform(L, U)</span>
<span id="cb109-32"><a href="#cb109-32" aria-hidden="true" tabindex="-1"></a>            trial[i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb109-33"><a href="#cb109-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-34"><a href="#cb109-34" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> <span class="bu">min</span>(bees, key<span class="op">=</span>f)</span>
<span id="cb109-35"><a href="#cb109-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best:"</span>, best, <span class="st">"f(x):"</span>, f(best))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-58" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-58">Why It Matters</h4>
<ul>
<li>Combines exploration (scouts) and exploitation (employed bees).</li>
<li>Naturally parallelizable.</li>
<li>Requires few parameters, only colony size and scout limit.</li>
<li>Performs well in noisy, high-dimensional, and multi-modal spaces.</li>
<li>Adapts dynamically through feedback between bees.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-57" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-57">A Gentle Proof (Why It Works)</h4>
<p>At each cycle, the probability of selecting higher-fitness solutions increases due to proportional recruitment. The replacement of stagnant sources by scouts ensures continuous diversity. Under repeated sampling, convergence toward global optima is probabilistically guaranteed as exploration persists.</p>
</section>
<section id="try-it-yourself-58" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-58">Try It Yourself</h4>
<ol type="1">
<li>Minimize <span class="math inline">\(f(x, y) = x^2 + y^2\)</span> with 20 bees.</li>
<li>Adjust colony size <span class="math inline">\(N\)</span> and scout limit.</li>
<li>Compare convergence with Particle Swarm Optimization and Firefly Algorithm.</li>
<li>Visualize swarm behavior across iterations.</li>
<li>Try discrete adaptation for Traveling Salesman Problem.</li>
</ol>
</section>
<section id="test-cases-58" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-58">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Domain</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sphere</td>
<td>Continuous</td>
<td>Simple benchmark</td>
</tr>
<tr class="even">
<td>Ackley</td>
<td>Nonlinear</td>
<td>Tests exploration ability</td>
</tr>
<tr class="odd">
<td>Scheduling</td>
<td>Discrete</td>
<td>Suited for BCO variants</td>
</tr>
<tr class="even">
<td>Neural tuning</td>
<td>Continuous</td>
<td>Parallelizable</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-58" class="level4">
<h4 class="anchored" data-anchor-id="complexity-58">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N \times \text{iterations})\)</span></li>
<li>Space: <span class="math inline">\(O(N)\)</span></li>
<li>Convergence: Stochastic but stable for large populations</li>
</ul>
<p>Bee Colony Optimization captures the power of cooperation, a swarm of simple agents, each with limited insight, finding order and intelligence through shared discovery.</p>
</section>
</section>
<section id="hill-climbing" class="level3">
<h3 class="anchored" data-anchor-id="hill-climbing">960. Hill Climbing</h3>
<p>Hill Climbing is one of the simplest optimization algorithms. It mimics how a climber ascends a hill by always taking a step toward higher ground (better solutions). Despite its simplicity, it’s the foundation of many local search and metaheuristic algorithms.</p>
<section id="what-problem-are-we-solving-59" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-59">What Problem Are We Solving?</h4>
<p>Hill Climbing is used for optimization without gradients, when we can evaluate the quality of a solution but not compute its derivatives. It’s especially useful for:</p>
<ul>
<li>Discrete or combinatorial optimization</li>
<li>Heuristic search problems</li>
<li>Feature selection</li>
<li>Parameter tuning</li>
</ul>
<p>However, it may get stuck at local optima, points that seem best locally but are not globally optimal.</p>
</section>
<section id="the-core-idea-29" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-29">The Core Idea</h4>
<p>At each step, Hill Climbing evaluates neighboring solutions and moves to the one with the best improvement. If no neighbor is better, it stops, assuming it has reached a peak (local optimum).</p>
</section>
<section id="mathematical-formulation-8" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-8">Mathematical Formulation</h4>
<p>Let <span class="math inline">\(f(x)\)</span> be the fitness or objective function we want to maximize. We start with an initial solution <span class="math inline">\(x_0\)</span> and iterate:</p>
<ol type="1">
<li>Generate a neighbor <span class="math inline">\(x'\)</span> near <span class="math inline">\(x_t\)</span>.</li>
<li>If <span class="math inline">\(f(x') &gt; f(x_t)\)</span>, move there: <span class="math display">\[
x_{t+1} = x'
\]</span> else stop.</li>
<li>Repeat until no improvement is found.</li>
</ol>
<p>Formally:</p>
<p><span class="math display">\[
x_{t+1} =
\begin{cases}
x', &amp; \text{if } f(x') &gt; f(x_t),\\
x_t, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
</section>
<section id="how-it-works-plain-language-18" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-18">How It Works (Plain Language)</h4>
<p>Imagine you are standing in fog on a hill. You can’t see far, but you can feel the slope. You take small steps uphill, always toward increasing elevation. Eventually, you stop when all nearby directions go downhill, you’ve reached a peak (though maybe not the highest one).</p>
</section>
<section id="step-by-step-summary-25" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-25">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start with a random solution</td>
</tr>
<tr class="even">
<td>2</td>
<td>Generate a neighboring solution</td>
</tr>
<tr class="odd">
<td>3</td>
<td>If neighbor is better, move there</td>
</tr>
<tr class="even">
<td>4</td>
<td>Repeat until no better neighbor exists</td>
</tr>
</tbody>
</table>
<p>Variants add randomness or restart from new positions to escape local optima.</p>
</section>
<section id="example-35" class="level4">
<h4 class="anchored" data-anchor-id="example-35">Example</h4>
<p>Maximize <span class="math inline">\(f(x) = -x^2 + 5x\)</span> over <span class="math inline">\(x \in [0, 5]\)</span></p>
<p>Start with <span class="math inline">\(x = 1\)</span> Neighbor step: <span class="math inline">\(x' = x + 0.1\)</span></p>
<p>Compute: <span class="math display">\[
f(x=1) = 4, \quad f(x'=1.1) = 4.39
\]</span> Move uphill. Continue until slope becomes negative, near <span class="math inline">\(x = 2.5\)</span>, where <span class="math inline">\(f(x)\)</span> peaks.</p>
</section>
<section id="tiny-code-9" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-9">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> <span class="op">-</span>x2 <span class="op">+</span> <span class="dv">5</span><span class="op">*</span>x</span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> random.uniform(<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a>    x_new <span class="op">=</span> x <span class="op">+</span> random.choice([<span class="op">-</span>step, step])</span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;=</span> x_new <span class="op">&lt;=</span> <span class="dv">5</span> <span class="kw">and</span> f(x_new) <span class="op">&gt;</span> f(x):</span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x_new</span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best x:"</span>, x, <span class="st">"f(x):"</span>, f(x))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb111"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Start with x = random value</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="co">// Loop: generate neighbor x_new</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co">// If f(x_new) &gt; f(x), move to x_new</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="co">// Stop when no better neighbor found</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-59" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-59">Why It Matters</h4>
<ul>
<li><p>Foundation for local search and stochastic optimization.</p></li>
<li><p>Works when gradient is unknown or non-differentiable.</p></li>
<li><p>Forms the base for advanced methods:</p>
<ul>
<li>Simulated Annealing (adds temperature-based randomness)</li>
<li>Tabu Search (adds memory)</li>
<li>Genetic Algorithms (adds population-based exploration)</li>
</ul></li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-58" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-58">A Gentle Proof (Why It Works)</h4>
<p>If the search space is finite and each step improves <span class="math inline">\(f(x)\)</span>, the algorithm must terminate at a local optimum since there’s a finite number of states. Convergence is guaranteed, but not necessarily to the global optimum.</p>
<p>Formally, since <span class="math inline">\(f(x_{t+1}) &gt; f(x_t)\)</span> and <span class="math inline">\(f\)</span> is bounded above, <span class="math display">\[
\lim_{t \to \infty} (f(x_{t+1}) - f(x_t)) = 0
\]</span> thus Hill Climbing reaches a stable point.</p>
</section>
<section id="try-it-yourself-59" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-59">Try It Yourself</h4>
<ol type="1">
<li>Use Hill Climbing to maximize <span class="math inline">\(f(x) = \sin(x)\)</span> over <span class="math inline">\([0, 2\pi]\)</span>.</li>
<li>Add random restarts to escape local maxima.</li>
<li>Try stochastic Hill Climbing, sometimes accept worse solutions.</li>
<li>Compare to Simulated Annealing.</li>
<li>Visualize steps on a 2D surface like <span class="math inline">\(f(x, y) = \sin(x)\cos(y)\)</span>.</li>
</ol>
</section>
<section id="test-cases-59" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-59">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Function</th>
<th>Domain</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(-x^2 + 5x\)</span></td>
<td>Continuous</td>
<td>Smooth single peak</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sin(x)\)</span></td>
<td>Multi-modal</td>
<td>Needs random restarts</td>
</tr>
<tr class="odd">
<td>Rastrigin</td>
<td>Multi-modal</td>
<td>Local optima test</td>
</tr>
<tr class="even">
<td>Traveling Salesman</td>
<td>Discrete</td>
<td>Neighborhood swap heuristic</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-59" class="level4">
<h4 class="anchored" data-anchor-id="complexity-59">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T)\)</span> (T = iterations)</li>
<li>Space: <span class="math inline">\(O(1)\)</span></li>
<li>Convergence: deterministic, local</li>
</ul>
<p>Hill Climbing is the purest form of search, step by step, always upward, until no higher ground remains. From this humble method, the modern landscape of metaheuristics begins.</p>
</section>
</section>
</section>
<section id="section-97.-reinforcement-learning" class="level1">
<h1>Section 97. Reinforcement Learning</h1>
<section id="monte-carlo-control" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-control">961. Monte Carlo Control</h3>
<p>Monte Carlo Control is a cornerstone algorithm in reinforcement learning (RL) that estimates the optimal policy by sampling complete episodes and averaging the observed returns. It learns from experience, no model of the environment required, by repeating trial and error across many episodes.</p>
<section id="what-problem-are-we-solving-60" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-60">What Problem Are We Solving?</h4>
<p>We want to learn how to act in an environment to maximize expected cumulative reward without knowing the underlying transition probabilities or reward model.</p>
<p>Given:</p>
<ul>
<li>A set of states <span class="math inline">\(S\)</span></li>
<li>A set of actions <span class="math inline">\(A\)</span></li>
<li>A reward function <span class="math inline">\(R(s, a)\)</span> (unknown)</li>
<li>Discount factor <span class="math inline">\(\gamma\)</span></li>
</ul>
<p>Monte Carlo Control learns:</p>
<ul>
<li>The action-value function <span class="math inline">\(Q(s,a)\)</span>, expected return after taking <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span> and following policy <span class="math inline">\(\pi\)</span></li>
<li>The optimal policy <span class="math inline">\(\pi^*(s) = \arg\max_a Q(s, a)\)</span></li>
</ul>
</section>
<section id="the-core-idea-30" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-30">The Core Idea</h4>
<p>Monte Carlo methods estimate <span class="math inline">\(Q(s,a)\)</span> by running complete episodes and averaging returns that follow each <span class="math inline">\((s,a)\)</span> pair. Then, the policy is improved greedily with respect to the estimated <span class="math inline">\(Q\)</span>. Repeated alternation of evaluation and improvement converges to the optimal policy.</p>
</section>
<section id="mathematical-formulation-9" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-9">Mathematical Formulation</h4>
<ol type="1">
<li><p>Return definition <span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
\]</span></p></li>
<li><p>Action-value update <span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha [G_t - Q(s,a)]
\]</span> where <span class="math inline">\(G_t\)</span> is the return following first occurrence of <span class="math inline">\((s,a)\)</span>.</p></li>
<li><p>Policy improvement <span class="math display">\[
\pi(s) = \arg\max_a Q(s,a)
\]</span></p></li>
<li><p>Exploration control (ε-greedy):</p></li>
</ol>
<p><span class="math display">\[
\pi(a|s) =
\begin{cases}
1 - \varepsilon + \dfrac{\varepsilon}{|A(s)|}, &amp; \text{if } a = \arg\max_a Q(s,a),\\
\dfrac{\varepsilon}{|A(s)|}, &amp; \text{otherwise.}
\end{cases}
\]</span></p>
</section>
<section id="how-it-works-plain-language-19" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-19">How It Works (Plain Language)</h4>
<p>Think of a player learning a game (like blackjack) by playing it over and over:</p>
<ul>
<li>Every full game (episode) ends with a score (total reward).</li>
<li>The player records what moves led to what outcomes.</li>
<li>Over time, average results for each move form a reliable estimate of its true value.</li>
<li>The player gradually shifts to moves that give higher average returns.</li>
</ul>
</section>
<section id="step-by-step-summary-26" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-26">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(Q(s,a)\)</span> arbitrarily</td>
</tr>
<tr class="even">
<td>2</td>
<td>Initialize a policy <span class="math inline">\(\pi\)</span> (e.g., random or ε-greedy)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Generate an episode following <span class="math inline">\(\pi\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Compute returns <span class="math inline">\(G_t\)</span> for all visited <span class="math inline">\((s,a)\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update <span class="math inline">\(Q(s,a)\)</span> by averaging returns</td>
</tr>
<tr class="even">
<td>6</td>
<td>Improve <span class="math inline">\(\pi\)</span> greedily w.r.t. updated <span class="math inline">\(Q\)</span></td>
</tr>
<tr class="odd">
<td>7</td>
<td>Repeat for many episodes</td>
</tr>
</tbody>
</table>
</section>
<section id="example-36" class="level4">
<h4 class="anchored" data-anchor-id="example-36">Example</h4>
<p>Suppose an agent plays blackjack. Each round (episode), it records the sequence of state-action pairs and the final outcome (+1 win, -1 lose, 0 draw). Over many rounds, it estimates the value of each <span class="math inline">\((s,a)\)</span>, e.g., “hit at 15” or “stand at 18”, and adjusts its strategy accordingly.</p>
</section>
<section id="tiny-code-10" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-10">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Environment (example): simplified blackjack</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="st">"hit"</span>, <span class="st">"stand"</span>]</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> {}</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>returns <span class="op">=</span> {}</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy(s, eps<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> random.random() <span class="op">&lt;</span> eps:</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(actions)</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: Q.get((s, a), <span class="dv">0</span>))</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a>    episode <span class="op">=</span> []</span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a>    state <span class="op">=</span> random.randint(<span class="dv">4</span>, <span class="dv">21</span>)</span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb112-17"><a href="#cb112-17" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> policy(state)</span>
<span id="cb112-18"><a href="#cb112-18" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> random.choice([<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>])  <span class="co"># placeholder for environment response</span></span>
<span id="cb112-19"><a href="#cb112-19" aria-hidden="true" tabindex="-1"></a>        episode.append((state, a, reward))</span>
<span id="cb112-20"><a href="#cb112-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="fl">0.3</span>:  <span class="co"># terminal</span></span>
<span id="cb112-21"><a href="#cb112-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb112-22"><a href="#cb112-22" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="bu">min</span>(<span class="dv">21</span>, state <span class="op">+</span> random.choice([<span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>]))</span>
<span id="cb112-23"><a href="#cb112-23" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb112-24"><a href="#cb112-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (s, a, r) <span class="kw">in</span> <span class="bu">reversed</span>(episode):</span>
<span id="cb112-25"><a href="#cb112-25" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> r <span class="op">+</span> <span class="fl">0.9</span> <span class="op">*</span> G</span>
<span id="cb112-26"><a href="#cb112-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">any</span>(x[<span class="dv">0</span>] <span class="op">==</span> s <span class="kw">and</span> x[<span class="dv">1</span>] <span class="op">==</span> a <span class="cf">for</span> x <span class="kw">in</span> episode[:<span class="op">-</span><span class="dv">1</span>]):</span>
<span id="cb112-27"><a href="#cb112-27" aria-hidden="true" tabindex="-1"></a>            returns.setdefault((s,a), []).append(G)</span>
<span id="cb112-28"><a href="#cb112-28" aria-hidden="true" tabindex="-1"></a>            Q[(s,a)] <span class="op">=</span> <span class="bu">sum</span>(returns[(s,a)]) <span class="op">/</span> <span class="bu">len</span>(returns[(s,a)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-60" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-60">Why It Matters</h4>
<ul>
<li>Model-free: learns directly from experience without needing transition probabilities.</li>
<li>Simple but powerful: the basis for many RL methods like SARSA, Q-learning, and Actor–Critic.</li>
<li>Exploration-friendly: ε-greedy ensures all actions are tried enough times.</li>
<li>Theoretical guarantee: with infinite exploration, <span class="math inline">\(Q \to Q^*\)</span>.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-59" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-59">A Gentle Proof (Why It Works)</h4>
<p>By the Law of Large Numbers, the empirical average of returns for <span class="math inline">\((s,a)\)</span> converges to its expected value under policy <span class="math inline">\(\pi\)</span>: <span class="math display">\[
E[G_t | S_t = s, A_t = a] = q_\pi(s,a)
\]</span> Successive policy improvement ensures: <span class="math display">\[
q_{\pi_{k+1}}(s,a) \ge q_{\pi_k}(s,a)
\]</span> Thus, repeated alternation of evaluation and improvement converges to <span class="math inline">\(\pi^*\)</span>.</p>
</section>
<section id="try-it-yourself-60" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-60">Try It Yourself</h4>
<ol type="1">
<li>Implement Monte Carlo control for a gridworld or blackjack.</li>
<li>Compare first-visit vs every-visit Monte Carlo updates.</li>
<li>Visualize how <span class="math inline">\(Q(s,a)\)</span> stabilizes over episodes.</li>
<li>Experiment with different <span class="math inline">\(\varepsilon\)</span> for exploration.</li>
<li>Try decaying <span class="math inline">\(\varepsilon_t\)</span> for late-stage exploitation.</li>
</ol>
</section>
<section id="test-cases-60" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-60">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Reward Signal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Blackjack</td>
<td>Episodic</td>
<td>+1 win, -1 lose</td>
</tr>
<tr class="even">
<td>Gridworld</td>
<td>Finite horizon</td>
<td>Step cost and goal reward</td>
</tr>
<tr class="odd">
<td>Tic-Tac-Toe</td>
<td>Discrete actions</td>
<td>+1 win, -1 lose, 0 draw</td>
</tr>
<tr class="even">
<td>Maze navigation</td>
<td>Continuous</td>
<td>Sparse terminal reward</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-60" class="level4">
<h4 class="anchored" data-anchor-id="complexity-60">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N \times T)\)</span> per episode (N states, T steps per episode)</li>
<li>Space: <span class="math inline">\(O(|S||A|)\)</span></li>
<li>Convergence: guaranteed with infinite sampling under <span class="math inline">\(\varepsilon\)</span>-greedy policy</li>
</ul>
<p>Monte Carlo Control embodies the spirit of learning from experience, no models, no equations of motion, just repeated play and careful averaging until knowledge emerges.</p>
</section>
</section>
<section id="temporal-difference-td-learning" class="level3">
<h3 class="anchored" data-anchor-id="temporal-difference-td-learning">962. Temporal Difference (TD) Learning</h3>
<p>Temporal Difference (TD) Learning is a reinforcement learning algorithm that blends ideas from Monte Carlo methods and Dynamic Programming. It learns directly from experience, step by step, by updating value estimates using predictions of future estimates rather than waiting for full episodes to finish.</p>
<section id="what-problem-are-we-solving-61" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-61">What Problem Are We Solving?</h4>
<p>Monte Carlo control waits until the end of each episode to update its value estimates. This can be slow or infeasible in continuing tasks.</p>
<p>TD learning solves this by <em>bootstrapping</em>, updating estimates based partly on existing estimates:</p>
<ul>
<li>Learn state values (<span class="math inline">\(V(s)\)</span>) or action values (<span class="math inline">\(Q(s,a)\)</span>)</li>
<li>Without knowing the environment model</li>
<li>During (not after) episodes</li>
</ul>
<p>It’s used in:</p>
<ul>
<li>Game playing (e.g., TD-Gammon)</li>
<li>Online prediction</li>
<li>Robot control</li>
<li>Financial forecasting</li>
</ul>
</section>
<section id="the-core-idea-31" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-31">The Core Idea</h4>
<p>TD learning updates a value function based on the difference between:</p>
<ul>
<li>The predicted return at one time step, and</li>
<li>The better-informed prediction at the next step.</li>
</ul>
<p>That difference is called the temporal difference error.</p>
</section>
<section id="mathematical-formulation-10" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-10">Mathematical Formulation</h4>
<ol type="1">
<li><p>Value update rule (TD(0)): <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\]</span> where</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate</li>
<li><span class="math inline">\(\gamma\)</span> is the discount factor</li>
</ul></li>
<li><p>The term <span class="math display">\[
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\]</span> is the TD error, how surprising the next observation is.</p></li>
<li><p>The same idea extends to Q-values: <span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
\]</span></p></li>
</ol>
</section>
<section id="how-it-works-plain-language-20" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-20">How It Works (Plain Language)</h4>
<p>Imagine predicting a movie’s rating as you watch it. After every scene, you adjust your expectation based on how the movie is going so far, without waiting for the credits to roll.</p>
<p>That’s TD learning: you refine your predictions as new evidence arrives, using your <em>current predictions</em> as stepping stones.</p>
</section>
<section id="step-by-step-summary-27" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-27">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(V(s)\)</span> or <span class="math inline">\(Q(s,a)\)</span> arbitrarily</td>
</tr>
<tr class="even">
<td>2</td>
<td>Start an episode with state <span class="math inline">\(S_0\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Choose action <span class="math inline">\(A_t\)</span> (ε-greedy)</td>
</tr>
<tr class="even">
<td>4</td>
<td>Observe reward <span class="math inline">\(R_{t+1}\)</span> and next state <span class="math inline">\(S_{t+1}\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Compute TD error <span class="math inline">\(\delta_t\)</span></td>
</tr>
<tr class="even">
<td>6</td>
<td>Update <span class="math inline">\(V(S_t)\)</span> or <span class="math inline">\(Q(S_t, A_t)\)</span></td>
</tr>
<tr class="odd">
<td>7</td>
<td>Repeat for all time steps</td>
</tr>
</tbody>
</table>
</section>
<section id="example-37" class="level4">
<h4 class="anchored" data-anchor-id="example-37">Example</h4>
<p>Let <span class="math inline">\(V(A)=0.5\)</span>, <span class="math inline">\(\gamma=0.9\)</span>, <span class="math inline">\(\alpha=0.1\)</span> You move from state A to B, get reward <span class="math inline">\(R=1\)</span>, and <span class="math inline">\(V(B)=0.6\)</span>.</p>
<p>Compute: <span class="math display">\[
\delta = R + \gamma V(B) - V(A) = 1 + 0.9 \times 0.6 - 0.5 = 1.04
\]</span> Update: <span class="math display">\[
V(A) \leftarrow 0.5 + 0.1 \times 1.04 = 0.604
\]</span></p>
</section>
<section id="tiny-code-11" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-11">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> {s: <span class="fl">0.0</span> <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)}</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>alpha, gamma <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.9</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> random.choice(<span class="bu">list</span>(V.keys()))</span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>        next_s <span class="op">=</span> random.choice(<span class="bu">list</span>(V.keys()))</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>        V[s] <span class="op">+=</span> alpha <span class="op">*</span> (r <span class="op">+</span> gamma <span class="op">*</span> V[next_s] <span class="op">-</span> V[s])</span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> next_s</span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(V)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>C (Sketch)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb114"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co">// For each step:</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a><span class="co">//   delta = R + gamma * V[next_s] - V[s];</span></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a><span class="co">//   V[s] += alpha * delta;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-61" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-61">Why It Matters</h4>
<ul>
<li>Online learning, updates happen as experience unfolds.</li>
<li>Efficient, no need to store full episodes.</li>
<li>Bootstrapping, learns from both experience and its own predictions.</li>
<li>Foundation for advanced algorithms like SARSA, Q-learning, and Actor–Critic.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-60" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-60">A Gentle Proof (Why It Works)</h4>
<p>By the contraction property of Bellman operators, repeated TD updates converge to the true value function <span class="math inline">\(V_\pi(s)\)</span> under a fixed policy <span class="math inline">\(\pi\)</span>. Each update moves <span class="math inline">\(V(s)\)</span> closer to the expected discounted return: <span class="math display">\[
E[V(S_t)] \to v_\pi(s)
\]</span> As long as every state is visited infinitely often and <span class="math inline">\(\alpha_t\)</span> satisfies standard conditions (<span class="math inline">\(\sum \alpha_t = \infty\)</span>, <span class="math inline">\(\sum \alpha_t^2 &lt; \infty\)</span>).</p>
</section>
<section id="try-it-yourself-61" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-61">Try It Yourself</h4>
<ol type="1">
<li>Implement TD(0) for a random-walk environment.</li>
<li>Compare learning curves for different <span class="math inline">\(\alpha\)</span>.</li>
<li>Visualize how <span class="math inline">\(V(s)\)</span> converges to true values.</li>
<li>Extend to TD(λ), multi-step backup averaging.</li>
<li>Compare with Monte Carlo estimates.</li>
</ol>
</section>
<section id="test-cases-61" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-61">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Walk</td>
<td>Classic example</td>
<td>Smooth convergence</td>
</tr>
<tr class="even">
<td>Gridworld</td>
<td>State transitions</td>
<td>Online update visualization</td>
</tr>
<tr class="odd">
<td>Tic-Tac-Toe</td>
<td>Predict game outcomes</td>
<td>Model-free</td>
</tr>
<tr class="even">
<td>Robot path</td>
<td>Continuous control</td>
<td>Real-time learning</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-61" class="level4">
<h4 class="anchored" data-anchor-id="complexity-61">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T)\)</span> per episode (T = steps)</li>
<li>Space: <span class="math inline">\(O(|S|)\)</span> or <span class="math inline">\(O(|S||A|)\)</span></li>
<li>Convergence: guaranteed for small <span class="math inline">\(\alpha\)</span> under on-policy exploration</li>
</ul>
<p>Temporal Difference Learning teaches the power of bootstrapped prediction, learning not by looking back after everything is over, but by predicting, updating, and improving as you go.</p>
</section>
</section>
<section id="sarsa-on-policy-temporal-difference-learning" class="level3">
<h3 class="anchored" data-anchor-id="sarsa-on-policy-temporal-difference-learning">963. SARSA (On-Policy Temporal Difference Learning)</h3>
<p>SARSA (State–Action–Reward–State–Action) is a classic on-policy reinforcement learning algorithm. It extends Temporal Difference (TD) learning to directly estimate action values <span class="math inline">\(Q(s,a)\)</span> rather than just state values. The name comes from the five elements used in each update: <span class="math inline">\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\)</span>.</p>
<section id="what-problem-are-we-solving-62" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-62">What Problem Are We Solving?</h4>
<p>We want to learn the optimal policy that maximizes long-term reward, but unlike Monte Carlo methods, we don’t need to wait for the episode to finish. Unlike Q-learning, SARSA updates using the action actually taken by the current policy, not the greedy one, making it “on-policy.”</p>
<p>It’s used for:</p>
<ul>
<li>Online control and navigation tasks</li>
<li>Robot and autonomous vehicle control</li>
<li>Any environment where exploration must be safe or gradual</li>
</ul>
</section>
<section id="the-core-idea-32" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-32">The Core Idea</h4>
<p>SARSA updates the value of the current state-action pair using:</p>
<ul>
<li>The immediate reward <span class="math inline">\(R_{t+1}\)</span></li>
<li>The predicted value of the <em>next</em> state-action pair under the current policy</li>
</ul>
<p>It learns while following the policy it’s evaluating, balancing exploration (via ε-greedy) and exploitation (choosing best actions).</p>
</section>
<section id="mathematical-formulation-11" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-11">Mathematical Formulation</h4>
<ol type="1">
<li><p>TD update rule: <span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
\]</span></p></li>
<li><p>Action selection (ε-greedy):</p></li>
</ol>
<p><span class="math display">\[
A_t =
\begin{cases}
\arg\max_a Q(S_t, a), &amp; \text{with probability } 1 - \varepsilon,\\
\text{random action}, &amp; \text{with probability } \varepsilon.
\end{cases}
\]</span></p>
</section>
<section id="how-it-works-plain-language-21" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-21">How It Works (Plain Language)</h4>
<p>Imagine teaching a robot to walk. Each move changes its position (state) and gives feedback (reward). The robot updates its understanding of which move was good or bad, based on what actually happened, not on some hypothetical “best” move it didn’t take. That’s SARSA: learning <em>from experience following your own policy</em>.</p>
</section>
<section id="step-by-step-summary-28" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-28">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(Q(s,a)\)</span> arbitrarily</td>
</tr>
<tr class="even">
<td>2</td>
<td>Start at initial state <span class="math inline">\(S_0\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Choose <span class="math inline">\(A_0\)</span> from <span class="math inline">\(S_0\)</span> using ε-greedy</td>
</tr>
<tr class="even">
<td>4</td>
<td>Repeat for each step:</td>
</tr>
<tr class="odd">
<td></td>
<td>a. Take <span class="math inline">\(A_t\)</span>, observe <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>b. Choose next action <span class="math inline">\(A_{t+1}\)</span> (ε-greedy)</td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Update <span class="math inline">\(Q(S_t, A_t)\)</span> using TD rule</td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Set <span class="math inline">\(S_t \leftarrow S_{t+1}\)</span>, <span class="math inline">\(A_t \leftarrow A_{t+1}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="example-38" class="level4">
<h4 class="anchored" data-anchor-id="example-38">Example</h4>
<p>Suppose <span class="math inline">\(Q(s,a)\)</span> is initialized to zero. At time <span class="math inline">\(t\)</span>:</p>
<ul>
<li><span class="math inline">\(S_t = s_1\)</span>, <span class="math inline">\(A_t = \text{right}\)</span>, reward <span class="math inline">\(R_{t+1} = 1\)</span></li>
<li>Next state <span class="math inline">\(S_{t+1} = s_2\)</span>, action <span class="math inline">\(A_{t+1} = \text{up}\)</span></li>
<li><span class="math inline">\(\alpha = 0.1\)</span>, <span class="math inline">\(\gamma = 0.9\)</span></li>
</ul>
<p>Then: <span class="math display">\[
Q(s_1, \text{right}) \leftarrow 0 + 0.1 [1 + 0.9 Q(s_2, \text{up}) - 0] = 0.1
\]</span></p>
</section>
<section id="tiny-code-12" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-12">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> {}</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>]</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="st">"left"</span>, <span class="st">"right"</span>]</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>alpha, gamma, eps <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy(s):</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> random.random() <span class="op">&lt;</span> eps:</span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(actions)</span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: Q.get((s,a), <span class="dv">0</span>))</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> random.choice(states)</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> policy(s)</span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a>        s_next <span class="op">=</span> random.choice(states)</span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a>        a_next <span class="op">=</span> policy(s_next)</span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> Q.get((s,a), <span class="dv">0</span>)</span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>        q_next <span class="op">=</span> Q.get((s_next,a_next), <span class="dv">0</span>)</span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a>        Q[(s,a)] <span class="op">=</span> q <span class="op">+</span> alpha <span class="op">*</span> (r <span class="op">+</span> gamma <span class="op">*</span> q_next <span class="op">-</span> q)</span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a>        s, a <span class="op">=</span> s_next, a_next</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-62" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-62">Why It Matters</h4>
<ul>
<li>On-policy control, learns safely using the same policy it acts with.</li>
<li>Smoothly transitions from exploration to exploitation.</li>
<li>Proven convergence under decaying <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(\alpha\)</span>.</li>
<li>Serves as a foundation for Expected SARSA, n-step SARSA, and TD(λ).</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-61" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-61">A Gentle Proof (Why It Works)</h4>
<p>For a fixed policy <span class="math inline">\(\pi\)</span>, the SARSA update approximates the Bellman equation: <span class="math display">\[
Q_\pi(S_t, A_t) = E[R_{t+1} + \gamma Q_\pi(S_{t+1}, A_{t+1})]
\]</span> Each update step is a stochastic approximation to this expectation. Given sufficient exploration and diminishing learning rate, <span class="math inline">\(Q \to Q_\pi\)</span>, and greedy improvement leads to <span class="math inline">\(\pi^*\)</span>.</p>
</section>
<section id="try-it-yourself-62" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-62">Try It Yourself</h4>
<ol type="1">
<li>Implement SARSA in a gridworld (like OpenAI Gym’s CliffWalking).</li>
<li>Compare performance with Q-learning (off-policy).</li>
<li>Test different ε values to see exploration effects.</li>
<li>Try n-step SARSA for faster convergence.</li>
<li>Visualize learned <span class="math inline">\(Q(s,a)\)</span> heatmaps.</li>
</ol>
</section>
<section id="test-cases-62" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-62">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 34%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CliffWalking</td>
<td>Classic on-policy test</td>
<td>Avoids cliff with safe policy</td>
</tr>
<tr class="even">
<td>Gridworld</td>
<td>Deterministic</td>
<td>Good for visualization</td>
</tr>
<tr class="odd">
<td>Taxi-v3</td>
<td>Discrete navigation</td>
<td>Requires exploration</td>
</tr>
<tr class="even">
<td>FrozenLake</td>
<td>Stochastic</td>
<td>Highlights ε-greedy balance</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-62" class="level4">
<h4 class="anchored" data-anchor-id="complexity-62">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(|S||A|)\)</span> per update step</li>
<li>Space: <span class="math inline">\(O(|S||A|)\)</span></li>
<li>Convergence: guaranteed under GLIE (greedy in the limit with infinite exploration)</li>
</ul>
<p>SARSA shows the essence of learning while doing, each decision refines the policy that made it, blending action and reflection into one continuous loop of improvement.</p>
</section>
</section>
<section id="q-learning-off-policy-temporal-difference-control" class="level3">
<h3 class="anchored" data-anchor-id="q-learning-off-policy-temporal-difference-control">964. Q-Learning (Off-Policy Temporal Difference Control)</h3>
<p>Q-Learning is one of the most influential reinforcement learning algorithms. It learns the optimal action-value function directly, even while following a different (exploratory) behavior policy. Unlike SARSA, which learns from the actions it <em>actually</em> takes, Q-Learning learns from the <em>best possible</em> actions it <em>could</em> take, making it an off-policy method.</p>
<section id="what-problem-are-we-solving-63" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-63">What Problem Are We Solving?</h4>
<p>We want to find the optimal policy <span class="math inline">\(\pi^*(s)\)</span> that maximizes cumulative reward in an unknown environment. Q-Learning does this without requiring a model of the environment’s dynamics.</p>
<p>It is used in:</p>
<ul>
<li>Games (e.g., AlphaGo’s early versions)</li>
<li>Navigation and control</li>
<li>Autonomous decision-making systems</li>
<li>Continuous adaptation tasks</li>
</ul>
</section>
<section id="the-core-idea-33" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-33">The Core Idea</h4>
<p>At each step, Q-Learning updates the estimate of <span class="math inline">\(Q(s,a)\)</span> toward the <em>best possible</em> next action’s value, not necessarily the one taken by the current policy.</p>
<p>This makes Q-Learning off-policy:</p>
<ul>
<li>The behavior policy (exploration) decides what to do.</li>
<li>The target policy (greedy) decides what the learner aims for.</li>
</ul>
</section>
<section id="mathematical-formulation-12" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-12">Mathematical Formulation</h4>
<ol type="1">
<li><p>Update rule: <span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \big[ R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \big]
\]</span></p></li>
<li><p>Greedy target: The term <span class="math inline">\(\max_{a'} Q(S_{t+1}, a')\)</span> represents the best estimated future return from the next state.</p></li>
<li><p>Action selection (ε-greedy):</p></li>
</ol>
<p><span class="math display">\[
A_t =
\begin{cases}
\arg\max_a Q(S_t, a), &amp; \text{with probability } 1 - \varepsilon,\\
\text{random action}, &amp; \text{with probability } \varepsilon.
\end{cases}
\]</span></p>
</section>
<section id="how-it-works-plain-language-22" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-22">How It Works (Plain Language)</h4>
<p>Think of Q-Learning as learning the <em>map of rewards</em> in an environment. Every time you move, you update what you believe the best future payoff is, not just from what you did, but from what you <em>could have done better</em>. Over many steps, this map converges to the true optimal reward landscape.</p>
</section>
<section id="step-by-step-summary-29" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-29">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(Q(s,a)\)</span> arbitrarily</td>
</tr>
<tr class="even">
<td>2</td>
<td>For each episode:</td>
</tr>
<tr class="odd">
<td></td>
<td>a. Start from initial state <span class="math inline">\(S_0\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>b. Choose action <span class="math inline">\(A_t\)</span> using ε-greedy</td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Take action, observe <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Update <span class="math inline">\(Q(S_t, A_t)\)</span> using TD rule</td>
</tr>
<tr class="odd">
<td></td>
<td>e. Set <span class="math inline">\(S_t \leftarrow S_{t+1}\)</span> and repeat until terminal</td>
</tr>
</tbody>
</table>
</section>
<section id="example-39" class="level4">
<h4 class="anchored" data-anchor-id="example-39">Example</h4>
<p>Suppose:</p>
<ul>
<li><span class="math inline">\(S_t = s_1\)</span>, <span class="math inline">\(A_t = \text{right}\)</span></li>
<li>Reward <span class="math inline">\(R_{t+1} = 1\)</span>, next state <span class="math inline">\(S_{t+1} = s_2\)</span></li>
<li><span class="math inline">\(\max_a Q(s_2, a) = 4\)</span></li>
<li><span class="math inline">\(\alpha = 0.1\)</span>, <span class="math inline">\(\gamma = 0.9\)</span>, <span class="math inline">\(Q(s_1, \text{right}) = 2\)</span></li>
</ul>
<p>Then: <span class="math display">\[
Q(s_1, \text{right}) \leftarrow 2 + 0.1 [1 + 0.9 \times 4 - 2] = 2 + 0.1 \times 3.6 = 2.36
\]</span></p>
</section>
<section id="tiny-code-13" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-13">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> {}</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>]</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="st">"left"</span>, <span class="st">"right"</span>]</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>alpha, gamma, eps <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span></span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy(s):</span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> random.random() <span class="op">&lt;</span> eps:</span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(actions)</span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: Q.get((s,a), <span class="dv">0</span>))</span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-13"><a href="#cb116-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb116-14"><a href="#cb116-14" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> random.choice(states)</span>
<span id="cb116-15"><a href="#cb116-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb116-16"><a href="#cb116-16" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> policy(s)</span>
<span id="cb116-17"><a href="#cb116-17" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb116-18"><a href="#cb116-18" aria-hidden="true" tabindex="-1"></a>        s_next <span class="op">=</span> random.choice(states)</span>
<span id="cb116-19"><a href="#cb116-19" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> Q.get((s,a), <span class="dv">0</span>)</span>
<span id="cb116-20"><a href="#cb116-20" aria-hidden="true" tabindex="-1"></a>        q_next_max <span class="op">=</span> <span class="bu">max</span>([Q.get((s_next,a2), <span class="dv">0</span>) <span class="cf">for</span> a2 <span class="kw">in</span> actions])</span>
<span id="cb116-21"><a href="#cb116-21" aria-hidden="true" tabindex="-1"></a>        Q[(s,a)] <span class="op">=</span> q <span class="op">+</span> alpha <span class="op">*</span> (r <span class="op">+</span> gamma <span class="op">*</span> q_next_max <span class="op">-</span> q)</span>
<span id="cb116-22"><a href="#cb116-22" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> s_next</span>
<span id="cb116-23"><a href="#cb116-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="fl">0.2</span>:  <span class="co"># terminate</span></span>
<span id="cb116-24"><a href="#cb116-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-63" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-63">Why It Matters</h4>
<ul>
<li>Model-free, learns directly from experience.</li>
<li>Off-policy, can learn optimal policy while exploring randomly.</li>
<li>Guaranteed convergence to <span class="math inline">\(Q^*\)</span> under certain conditions (Watkins &amp; Dayan, 1992).</li>
<li>Forms the foundation for Deep Q-Networks (DQN) and modern deep RL.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-62" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-62">A Gentle Proof (Why It Works)</h4>
<p>For a deterministic learning rate <span class="math inline">\(\alpha_t\)</span> and sufficient exploration, Q-Learning approximates the Bellman optimality equation: <span class="math display">\[
Q^*(s,a) = E[R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a')]
\]</span> Since the Bellman operator is a contraction mapping, repeated application guarantees convergence: <span class="math display">\[
| Q_{t+1} - Q^* |*\infty \le \gamma | Q_t - Q^* |*\infty
\]</span> Hence <span class="math inline">\(Q_t \to Q^*\)</span> as <span class="math inline">\(t \to \infty\)</span>.</p>
</section>
<section id="try-it-yourself-63" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-63">Try It Yourself</h4>
<ol type="1">
<li>Implement Q-Learning for the FrozenLake environment.</li>
<li>Compare learning curves with SARSA.</li>
<li>Tune <span class="math inline">\(\varepsilon\)</span> for exploration and convergence speed.</li>
<li>Add a decaying <span class="math inline">\(\alpha_t\)</span> to stabilize learning.</li>
<li>Visualize the learned <span class="math inline">\(Q\)</span>-table or policy heatmap.</li>
</ol>
</section>
<section id="test-cases-63" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-63">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FrozenLake</td>
<td>Discrete grid</td>
<td>Off-policy advantage</td>
</tr>
<tr class="even">
<td>Taxi-v3</td>
<td>Navigation</td>
<td>Faster convergence than SARSA</td>
</tr>
<tr class="odd">
<td>CliffWalking</td>
<td>Risky terrain</td>
<td>SARSA safer, Q-learning bolder</td>
</tr>
<tr class="even">
<td>Gridworld</td>
<td>Small testbed</td>
<td>Perfect for Q-table visualization</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-63" class="level4">
<h4 class="anchored" data-anchor-id="complexity-63">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(|S||A|)\)</span> per update</li>
<li>Space: <span class="math inline">\(O(|S||A|)\)</span></li>
<li>Convergence: guaranteed under GLIE and Robbins–Monro learning rate conditions</li>
</ul>
<p>Q-Learning is the heart of modern reinforcement learning, a learner that imagines better futures and improves itself toward them, one step at a time.</p>
</section>
</section>
<section id="double-q-learning" class="level3">
<h3 class="anchored" data-anchor-id="double-q-learning">965. Double Q-Learning</h3>
<p>Double Q-Learning refines standard Q-Learning by solving one of its biggest weaknesses: overestimation bias. In regular Q-Learning, the same values are used both to select and to evaluate actions, this tends to overrate some actions, especially in noisy environments. Double Q-Learning fixes this by maintaining two separate value estimators, which keep each other honest.</p>
<section id="what-problem-are-we-solving-64" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-64">What Problem Are We Solving?</h4>
<p>Standard Q-Learning uses the same <span class="math inline">\(Q\)</span> function to both:</p>
<ol type="1">
<li>Choose the best next action via <span class="math inline">\(\max_a Q(S', a)\)</span></li>
<li>Evaluate that chosen action’s value</li>
</ol>
<p>This self-referential step can produce optimistic overestimates, which make learning unstable or slow.</p>
<p>Double Q-Learning reduces that bias by decoupling <em>action selection</em> and <em>action evaluation.</em></p>
</section>
<section id="the-core-idea-34" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-34">The Core Idea</h4>
<p>Use two value functions, <span class="math inline">\(Q^A\)</span> and <span class="math inline">\(Q^B\)</span>, that take turns learning from each other.</p>
<ul>
<li>One function (<span class="math inline">\(Q^A\)</span>) chooses the next action (selection)</li>
<li>The other (<span class="math inline">\(Q^B\)</span>) evaluates that action (evaluation)</li>
</ul>
<p>By alternating updates, the system learns more accurate and stable estimates of true <span class="math inline">\(Q^*\)</span>.</p>
</section>
<section id="mathematical-formulation-13" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-13">Mathematical Formulation</h4>
<ol type="1">
<li>Maintain two estimators, <span class="math inline">\(Q^A\)</span> and <span class="math inline">\(Q^B\)</span></li>
<li>With equal probability, update one of them at each step:</li>
</ol>
<p>If updating <span class="math inline">\(Q^A\)</span>: <span class="math display">\[
Q^A(S_t, A_t) \leftarrow Q^A(S_t, A_t) + \alpha [R_{t+1} + \gamma Q^B(S_{t+1}, \arg\max_a Q^A(S_{t+1}, a)) - Q^A(S_t, A_t)]
\]</span></p>
<p>If updating <span class="math inline">\(Q^B\)</span>: <span class="math display">\[
Q^B(S_t, A_t) \leftarrow Q^B(S_t, A_t) + \alpha [R_{t+1} + \gamma Q^A(S_{t+1}, \arg\max_a Q^B(S_{t+1}, a)) - Q^B(S_t, A_t)]
\]</span></p>
</section>
<section id="how-it-works-plain-language-23" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-23">How It Works (Plain Language)</h4>
<p>Imagine two friends, Alice and Bob, both trying to estimate how good each action is. Alice picks the best-looking action according to <em>her</em> table, but Bob gives the score. Then next time, Bob picks and Alice scores. Because each one checks the other’s optimism, their shared knowledge becomes more reliable.</p>
</section>
<section id="step-by-step-summary-30" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-30">Step-by-Step Summary</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize <span class="math inline">\(Q^A\)</span> and <span class="math inline">\(Q^B\)</span> arbitrarily</td>
</tr>
<tr class="even">
<td>2</td>
<td>For each episode:</td>
</tr>
<tr class="odd">
<td></td>
<td>a. Choose <span class="math inline">\(A_t\)</span> using ε-greedy over <span class="math inline">\((Q^A + Q^B)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>b. Take action <span class="math inline">\(A_t\)</span>, observe <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Randomly choose which <span class="math inline">\(Q\)</span> to update</td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Use one for action selection, the other for evaluation</td>
</tr>
<tr class="odd">
<td></td>
<td>e. Repeat until episode ends</td>
</tr>
</tbody>
</table>
</section>
<section id="example-40" class="level4">
<h4 class="anchored" data-anchor-id="example-40">Example</h4>
<p>Suppose:</p>
<ul>
<li><span class="math inline">\(S_t = s_1\)</span>, <span class="math inline">\(A_t = \text{right}\)</span></li>
<li><span class="math inline">\(R_{t+1} = 2\)</span>, <span class="math inline">\(S_{t+1} = s_2\)</span></li>
<li><span class="math inline">\(\gamma = 0.9\)</span>, <span class="math inline">\(\alpha = 0.1\)</span></li>
<li><span class="math inline">\(Q^A(s_1,\text{right}) = 1.5\)</span></li>
<li><span class="math inline">\(Q^A(s_2,\text{up}) = 2.0\)</span>, <span class="math inline">\(Q^B(s_2,\text{up}) = 1.8\)</span></li>
</ul>
<p>If we update <span class="math inline">\(Q^A\)</span>: <span class="math display">\[
Q^A(s_1,\text{right}) \leftarrow 1.5 + 0.1 [2 + 0.9 \times Q^B(s_2, \arg\max_a Q^A(s_2,a)) - 1.5]
\]</span> Since <span class="math inline">\(\arg\max_a Q^A(s_2,a)\)</span> is “up,” <span class="math display">\[
Q^A(s_1,\text{right}) = 1.5 + 0.1 [2 + 0.9 \times 1.8 - 1.5] = 1.5 + 0.1 \times 2.12 = 1.712
\]</span></p>
</section>
<section id="tiny-code-14" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-14">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>Q_A, Q_B <span class="op">=</span> {}, {}</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>]</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="st">"left"</span>, <span class="st">"right"</span>]</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>alpha, gamma, eps <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span></span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy(s):</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>    Q_total <span class="op">=</span> {a: Q_A.get((s,a),<span class="dv">0</span>) <span class="op">+</span> Q_B.get((s,a),<span class="dv">0</span>) <span class="cf">for</span> a <span class="kw">in</span> actions}</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> random.random() <span class="op">&lt;</span> eps:</span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(actions)</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: Q_total[a])</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> random.choice(states)</span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> policy(s)</span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a>        s_next <span class="op">=</span> random.choice(states)</span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="fl">0.5</span>:</span>
<span id="cb117-21"><a href="#cb117-21" aria-hidden="true" tabindex="-1"></a>            a_next <span class="op">=</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: Q_A.get((s_next,a),<span class="dv">0</span>))</span>
<span id="cb117-22"><a href="#cb117-22" aria-hidden="true" tabindex="-1"></a>            Q_A[(s,a)] <span class="op">=</span> Q_A.get((s,a),<span class="dv">0</span>) <span class="op">+</span> alpha <span class="op">*</span> (</span>
<span id="cb117-23"><a href="#cb117-23" aria-hidden="true" tabindex="-1"></a>                r <span class="op">+</span> gamma <span class="op">*</span> Q_B.get((s_next,a_next),<span class="dv">0</span>) <span class="op">-</span> Q_A.get((s,a),<span class="dv">0</span>))</span>
<span id="cb117-24"><a href="#cb117-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb117-25"><a href="#cb117-25" aria-hidden="true" tabindex="-1"></a>            a_next <span class="op">=</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: Q_B.get((s_next,a),<span class="dv">0</span>))</span>
<span id="cb117-26"><a href="#cb117-26" aria-hidden="true" tabindex="-1"></a>            Q_B[(s,a)] <span class="op">=</span> Q_B.get((s,a),<span class="dv">0</span>) <span class="op">+</span> alpha <span class="op">*</span> (</span>
<span id="cb117-27"><a href="#cb117-27" aria-hidden="true" tabindex="-1"></a>                r <span class="op">+</span> gamma <span class="op">*</span> Q_A.get((s_next,a_next),<span class="dv">0</span>) <span class="op">-</span> Q_B.get((s,a),<span class="dv">0</span>))</span>
<span id="cb117-28"><a href="#cb117-28" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> s_next</span>
<span id="cb117-29"><a href="#cb117-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="fl">0.2</span>:</span>
<span id="cb117-30"><a href="#cb117-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-64" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-64">Why It Matters</h4>
<ul>
<li>Reduces overestimation, more stable learning curves than Q-Learning</li>
<li>Converges more smoothly in stochastic environments</li>
<li>Foundation for Double DQN, a deep variant widely used in modern RL</li>
<li>Encourages better value calibration in uncertain domains</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-63" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-63">A Gentle Proof (Why It Works)</h4>
<p>In Q-Learning, <span class="math inline">\(\max_a Q(S',a)\)</span> tends to overestimate because of sampling noise. Double Q-Learning breaks that coupling: <span class="math display">\[
E[Q^B(S', \arg\max_a Q^A(S',a))] \le E[\max_a Q^A(S',a)]
\]</span> Thus, bias is reduced while retaining consistency with the Bellman optimality equation. The method still converges to <span class="math inline">\(Q^*\)</span> under standard conditions.</p>
</section>
<section id="try-it-yourself-64" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-64">Try It Yourself</h4>
<ol type="1">
<li>Run both Q-Learning and Double Q-Learning on the same environment.</li>
<li>Plot value estimates, notice Q-Learning’s optimistic bias.</li>
<li>Tune learning rate <span class="math inline">\(\alpha\)</span> and discount <span class="math inline">\(\gamma\)</span>.</li>
<li>Extend to Double DQN using neural networks.</li>
<li>Try stochastic reward functions to highlight bias effects.</li>
</ol>
</section>
<section id="test-cases-64" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-64">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 32%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CliffWalking</td>
<td>Noisy terrain</td>
<td>More stable than Q-Learning</td>
</tr>
<tr class="even">
<td>FrozenLake</td>
<td>Stochastic transitions</td>
<td>Less variance</td>
</tr>
<tr class="odd">
<td>Taxi-v3</td>
<td>Sparse rewards</td>
<td>Smoother convergence</td>
</tr>
<tr class="even">
<td>Gridworld</td>
<td>Simple grid</td>
<td>Easier visualization of Q_A vs Q_B</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-64" class="level4">
<h4 class="anchored" data-anchor-id="complexity-64">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(2|S||A|)\)</span> (dual Q-tables)</li>
<li>Space: <span class="math inline">\(O(2|S||A|)\)</span></li>
<li>Convergence: unbiased, stable for small <span class="math inline">\(\alpha\)</span> and persistent exploration</li>
</ul>
<p>Double Q-Learning is like learning from a partner, each half of the system cross-checks the other, turning optimism into balance and convergence into confidence.</p>
</section>
</section>
<section id="deep-q-network-dqn" class="level3">
<h3 class="anchored" data-anchor-id="deep-q-network-dqn">966. Deep Q-Network (DQN)</h3>
<p>Deep Q-Network (DQN) extends classical Q-Learning by using a neural network to approximate the action-value function <span class="math inline">\(Q(s, a)\)</span>. Instead of storing a table for every state-action pair, DQN generalizes across large or continuous state spaces, enabling reinforcement learning to work on high-dimensional inputs like images, games, and sensor data.</p>
<section id="what-problem-are-we-solving-65" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-65">What Problem Are We Solving?</h4>
<p>Traditional Q-Learning breaks down when:</p>
<ul>
<li>The state space is too large to store <span class="math inline">\(Q(s,a)\)</span> explicitly</li>
<li>States are continuous (e.g., robot positions, pixels)</li>
<li>Function approximation is needed</li>
</ul>
<p>DQN addresses this by learning <span class="math inline">\(Q_\theta(s, a)\)</span>, a parameterized function approximated by a deep neural network with weights <span class="math inline">\(\theta\)</span>.</p>
<p>Applications include:</p>
<ul>
<li>Atari game playing (DeepMind, 2015)</li>
<li>Autonomous control systems</li>
<li>Decision-making from raw sensory inputs</li>
</ul>
</section>
<section id="the-core-idea-35" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-35">The Core Idea</h4>
<p>Approximate the optimal Q-function <span class="math inline">\(Q^*(s,a)\)</span> with a neural network: <span class="math display">\[
Q(s,a;\theta) \approx Q^*(s,a)
\]</span></p>
<p>Then apply the Q-Learning update rule, but instead of updating a single entry in a table, minimize the mean-squared error between the current prediction and the target: <span class="math display">\[
L(\theta) = \mathbb{E}\Big[ \big( y - Q(s,a;\theta) \big)^2 \Big]
\]</span> where the target value is: <span class="math display">\[
y = R + \gamma \max_{a'} Q(s',a';\theta^-)
\]</span></p>
<p><span class="math inline">\(\theta^-\)</span> are the parameters of a target network, updated periodically for stability.</p>
</section>
<section id="how-it-works-plain-language-24" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-24">How It Works (Plain Language)</h4>
<p>DQN learns how good each action is by <em>predicting future rewards</em> with a neural network. It observes transitions <span class="math inline">\((s, a, r, s')\)</span>, stores them, and samples random mini-batches to train. This “experience replay” helps it avoid overfitting to recent experiences and stabilize learning.</p>
</section>
<section id="step-by-step-summary-31" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-31">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize replay buffer <span class="math inline">\(D\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Initialize Q-network with random weights <span class="math inline">\(\theta\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Initialize target network with weights <span class="math inline">\(\theta^- = \theta\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>For each episode:</td>
</tr>
<tr class="odd">
<td></td>
<td>a. Observe state <span class="math inline">\(s\)</span>, choose action <span class="math inline">\(a\)</span> via ε-greedy on <span class="math inline">\(Q(s,a;\theta)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>b. Execute <span class="math inline">\(a\)</span>, observe reward <span class="math inline">\(r\)</span> and next state <span class="math inline">\(s'\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Store <span class="math inline">\((s,a,r,s')\)</span> in replay buffer</td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Sample random batch from <span class="math inline">\(D\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td>e. Compute target <span class="math inline">\(y = r + \gamma \max_{a'} Q(s',a';\theta^-)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>f.&nbsp;Minimize loss <span class="math inline">\((y - Q(s,a;\theta))^2\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td>g. Periodically update <span class="math inline">\(\theta^- \leftarrow \theta\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="mathematical-formulation-14" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-14">Mathematical Formulation</h4>
<ol type="1">
<li><p>Loss function <span class="math display">\[
L(\theta) = \mathbb{E}*{(s,a,r,s') \sim D} \Big[ \big( R + \gamma \max*{a'} Q(s',a';\theta^-) - Q(s,a;\theta) \big)^2 \Big]
\]</span></p></li>
<li><p>Gradient descent update <span class="math display">\[
\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta)
\]</span></p></li>
<li><p>Target network update <span class="math display">\[
\theta^- \leftarrow \theta \text{ every } C \text{ steps.}
\]</span></p></li>
</ol>
</section>
<section id="example-41" class="level4">
<h4 class="anchored" data-anchor-id="example-41">Example</h4>
<p>In Atari “Breakout”:</p>
<ul>
<li>Input: 84×84 grayscale game frames</li>
<li>Output: predicted Q-values for 4 possible actions</li>
<li>Reward: +1 when the ball breaks a brick, 0 otherwise The network learns to move the paddle optimally to maximize cumulative score.</li>
</ul>
</section>
<section id="tiny-code-15" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-15">Tiny Code</h4>
<p>Python (simplified DQN skeleton)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, torch.nn <span class="im">as</span> nn, torch.optim <span class="im">as</span> optim, random</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DQN(nn.Module):</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_states, n_actions):</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(n_states, <span class="dv">128</span>), nn.ReLU(),</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, n_actions)</span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a>q_net <span class="op">=</span> DQN(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a>target_net <span class="op">=</span> DQN(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a>target_net.load_state_dict(q_net.state_dict())</span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(q_net.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb118-19"><a href="#cb118-19" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb118-20"><a href="#cb118-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-21"><a href="#cb118-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(batch):</span>
<span id="cb118-22"><a href="#cb118-22" aria-hidden="true" tabindex="-1"></a>    s, a, r, s_next <span class="op">=</span> batch</span>
<span id="cb118-23"><a href="#cb118-23" aria-hidden="true" tabindex="-1"></a>    q_values <span class="op">=</span> q_net(torch.tensor(s, dtype<span class="op">=</span>torch.float32))</span>
<span id="cb118-24"><a href="#cb118-24" aria-hidden="true" tabindex="-1"></a>    next_q <span class="op">=</span> target_net(torch.tensor(s_next, dtype<span class="op">=</span>torch.float32)).<span class="bu">max</span>(<span class="dv">1</span>)[<span class="dv">0</span>].detach()</span>
<span id="cb118-25"><a href="#cb118-25" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.tensor(r) <span class="op">+</span> gamma <span class="op">*</span> next_q</span>
<span id="cb118-26"><a href="#cb118-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> ((q_values.gather(<span class="dv">1</span>, torch.tensor(a).unsqueeze(<span class="dv">1</span>)).squeeze() <span class="op">-</span> y)<span class="dv">2</span>).mean()</span>
<span id="cb118-27"><a href="#cb118-27" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()<span class="op">;</span> loss.backward()<span class="op">;</span> optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-65" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-65">Why It Matters</h4>
<ul>
<li>Scales to high-dimensional inputs (e.g., images, sensors).</li>
<li>Experience replay breaks correlation between consecutive samples.</li>
<li>Target network prevents feedback loops and instability.</li>
<li>Major milestone: Atari games achieved superhuman performance using only pixels as input.</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-64" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-64">A Gentle Proof (Why It Works)</h4>
<p>The Bellman optimality operator: <span class="math display">\[
\mathcal{T}Q(s,a) = \mathbb{E}[R + \gamma \max_{a'} Q(s',a')]
\]</span> is a contraction mapping in <span class="math inline">\(|\cdot|_\infty\)</span>. By minimizing <span class="math inline">\(L(\theta)\)</span>, DQN seeks to approximate the fixed point <span class="math inline">\(Q^*\)</span> of <span class="math inline">\(\mathcal{T}\)</span>. Target networks and replay buffers stabilize this iterative approximation, ensuring convergence in expectation.</p>
</section>
<section id="try-it-yourself-65" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-65">Try It Yourself</h4>
<ol type="1">
<li>Implement DQN for CartPole-v1 (OpenAI Gym).</li>
<li>Try different architectures, linear vs convolutional.</li>
<li>Compare with tabular Q-Learning performance.</li>
<li>Tune buffer size, batch size, and target update rate.</li>
<li>Visualize training reward over time.</li>
</ol>
</section>
<section id="test-cases-65" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-65">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CartPole-v1</td>
<td>Classic benchmark</td>
<td>Quick convergence</td>
</tr>
<tr class="even">
<td>MountainCar-v0</td>
<td>Sparse rewards</td>
<td>Needs exploration</td>
</tr>
<tr class="odd">
<td>Atari Breakout</td>
<td>Visual input</td>
<td>Deep CNN required</td>
</tr>
<tr class="even">
<td>LunarLander-v2</td>
<td>Continuous state</td>
<td>Sensitive to hyperparameters</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-65" class="level4">
<h4 class="anchored" data-anchor-id="complexity-65">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(B \times E)\)</span> (batch size × episodes)</li>
<li>Space: <span class="math inline">\(O(|D|)\)</span> (replay buffer size)</li>
<li>Convergence: approximate; improved with Double DQN and Dueling DQN</li>
</ul>
<p>DQN marked the birth of Deep Reinforcement Learning, a fusion of neural representation and temporal learning that allows machines to learn complex behaviors directly from pixels and experience.</p>
</section>
</section>
<section id="reinforce-policy-gradient-by-sampling" class="level3">
<h3 class="anchored" data-anchor-id="reinforce-policy-gradient-by-sampling">967. REINFORCE (Policy Gradient by Sampling)</h3>
<p>REINFORCE is one of the simplest and most fundamental policy gradient algorithms. Instead of learning a value function like Q-Learning, it directly learns the policy, that is, how to act. By adjusting its parameters to increase the probability of rewarding actions, REINFORCE captures the essence of learning from experience.</p>
<section id="what-problem-are-we-solving-66" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-66">What Problem Are We Solving?</h4>
<p>Value-based methods (like Q-Learning or DQN) approximate <span class="math inline">\(Q(s,a)\)</span> and act greedily. But in many problems:</p>
<ul>
<li>The action space is continuous</li>
<li>Deterministic policies perform poorly</li>
<li>Stochastic exploration is essential</li>
</ul>
<p>REINFORCE solves this by optimizing a parameterized stochastic policy <span class="math inline">\(\pi_\theta(a|s)\)</span>, without needing to learn <span class="math inline">\(Q(s,a)\)</span>.</p>
<p>It is ideal for:</p>
<ul>
<li>Continuous control (robotics, game agents)</li>
<li>Policy optimization in environments with stochastic transitions</li>
<li>Learning with parameterized actions (e.g., probabilities, torques, velocities)</li>
</ul>
</section>
<section id="the-core-idea-36" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-36">The Core Idea</h4>
<p>Maximize the expected cumulative reward: <span class="math display">\[
J(\theta) = \mathbb{E}*{\pi*\theta}[R]
\]</span></p>
<p>Using the log-derivative trick: <span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}*{\pi*\theta}\big[ \nabla_\theta \log \pi_\theta(a|s) , G_t \big]
\]</span></p>
<p>where <span class="math inline">\(G_t\)</span> is the return (total discounted reward from time <span class="math inline">\(t\)</span>).</p>
<p>The update rule becomes: <span class="math display">\[
\theta \leftarrow \theta + \alpha , G_t , \nabla_\theta \log \pi_\theta(a_t|s_t)
\]</span></p>
<p>This moves the policy parameters <span class="math inline">\(\theta\)</span> in the direction that increases the log-probability of actions that led to higher rewards.</p>
</section>
<section id="how-it-works-plain-language-25" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-25">How It Works (Plain Language)</h4>
<p>Imagine a player who tries different strategies, receives scores, and remembers which choices led to better outcomes. REINFORCE simply nudges the probabilities of those actions to make them more likely in the future. Over time, the agent learns which actions tend to yield higher rewards, directly shaping its behavior.</p>
</section>
<section id="step-by-step-summary-32" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-32">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 2%">
<col style="width: 63%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize policy parameters <span class="math inline">\(\theta\)</span> randomly</td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>For each episode:</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>a. Run the policy <span class="math inline">\(\pi_\theta(a                                                                    | s)\)</span> to generate a trajectory <span class="math inline">\((s_0,a_0,r_1,s_1,...)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>b. Compute the return <span class="math inline">\(G_t = r_{t+1} + \gamma r_{t+2} + \dots\)</span> for each time step</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Update parameters: <span class="math inline">\(\theta \leftarrow \theta + \alpha , G_t , \nabla_\theta \log \pi_\theta(a_t | s_t)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Repeat until convergence</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="mathematical-formulation-15" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-15">Mathematical Formulation</h4>
<ol type="1">
<li><p>Expected return <span class="math display">\[
J(\theta) = \mathbb{E}*{\pi*\theta}\Big[ \sum_{t=0}^T \gamma^t R_t \Big]
\]</span></p></li>
<li><p>Policy gradient theorem <span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}*{\pi*\theta}\Big[ \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - b_t) \Big]
\]</span></p></li>
<li><p>Baseline term <span class="math inline">\(b_t\)</span> Subtracting a baseline (often a value estimate) reduces variance without changing the expectation.</p></li>
</ol>
</section>
<section id="example-42" class="level4">
<h4 class="anchored" data-anchor-id="example-42">Example</h4>
<p>Suppose:</p>
<ul>
<li><p><span class="math inline">\(\pi_\theta(a|s)\)</span> is a softmax policy: <span class="math display">\[
\pi_\theta(a|s) = \frac{e^{\theta^\top f(s,a)}}{\sum_b e^{\theta^\top f(s,b)}}
\]</span></p></li>
<li><p>Reward <span class="math inline">\(R_t = +1\)</span> for good actions, <span class="math inline">\(-1\)</span> for bad. Then REINFORCE increases <span class="math inline">\(\theta\)</span> for actions with positive <span class="math inline">\(G_t\)</span>, and decreases it otherwise.</p></li>
</ul>
</section>
<section id="tiny-code-16" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-16">Tiny Code</h4>
<p>Python (with softmax policy)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> np.exp(x <span class="op">-</span> np.<span class="bu">max</span>(x))</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> e <span class="op">/</span> e.<span class="bu">sum</span>()</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">2</span>)  <span class="co"># parameter vector</span></span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>alpha, gamma <span class="op">=</span> <span class="fl">0.01</span>, <span class="fl">0.99</span></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>    states, actions, rewards <span class="op">=</span> [], [], []</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> softmax(theta)</span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> np.random.choice(<span class="bu">len</span>(probs), p<span class="op">=</span>probs)</span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> a <span class="op">==</span> <span class="dv">1</span> <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># example reward</span></span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true" tabindex="-1"></a>        states.append(s)<span class="op">;</span> actions.append(a)<span class="op">;</span> rewards.append(r)</span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(rewards))):</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> rewards[t] <span class="op">+</span> gamma <span class="op">*</span> G</span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true" tabindex="-1"></a>        grad_log <span class="op">=</span> np.zeros_like(theta)</span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true" tabindex="-1"></a>        grad_log[actions[t]] <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> softmax(theta)[actions[t]]</span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">+=</span> alpha <span class="op">*</span> G <span class="op">*</span> grad_log</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-66" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-66">Why It Matters</h4>
<ul>
<li>Direct policy optimization, no Q-tables needed</li>
<li>Supports continuous actions (unlike Q-learning)</li>
<li>Simple and general, basis for all policy gradient methods</li>
<li>Works with neural networks (e.g., in actor–critic models)</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-65" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-65">A Gentle Proof (Why It Works)</h4>
<p>Using the log-derivative identity: <span class="math display">\[
\nabla_\theta \pi_\theta(a|s) = \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)
\]</span></p>
<p>Then: <span class="math display">\[
\nabla_\theta J(\theta) = \sum_s d_\pi(s) \sum_a Q^\pi(s,a) \nabla_\theta \log \pi_\theta(a|s)
\]</span></p>
<p>The gradient is unbiased but high-variance; the baseline <span class="math inline">\(b_t\)</span> term reduces this variance without biasing the result.</p>
</section>
<section id="try-it-yourself-66" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-66">Try It Yourself</h4>
<ol type="1">
<li>Implement REINFORCE for CartPole-v1.</li>
<li>Compare with Actor–Critic methods, notice variance differences.</li>
<li>Add a value function baseline for variance reduction.</li>
<li>Use a neural network for <span class="math inline">\(\pi_\theta(a|s)\)</span>.</li>
<li>Plot the total reward vs.&nbsp;episode count.</li>
</ol>
</section>
<section id="test-cases-66" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-66">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 33%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CartPole-v1</td>
<td>Discrete control</td>
<td>Converges slowly but steadily</td>
</tr>
<tr class="even">
<td>MountainCarContinuous</td>
<td>Continuous actions</td>
<td>Needs good normalization</td>
</tr>
<tr class="odd">
<td>LunarLander-v2</td>
<td>Complex rewards</td>
<td>Sensitive to learning rate</td>
</tr>
<tr class="even">
<td>Pendulum-v1</td>
<td>Continuous torque control</td>
<td>Requires baseline</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-66" class="level4">
<h4 class="anchored" data-anchor-id="complexity-66">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T)\)</span> per episode (trajectory length)</li>
<li>Space: <span class="math inline">\(O(T)\)</span> (store rewards and gradients)</li>
<li>Convergence: Unbiased but high-variance; slow without baseline</li>
</ul>
<p>REINFORCE shows the simplest truth in reinforcement learning, increase the probability of what worked. Every successful action leaves a small gradient trail toward better behavior.</p>
</section>
</section>
<section id="actorcritic-value-guided-policy-update" class="level3">
<h3 class="anchored" data-anchor-id="actorcritic-value-guided-policy-update">968. Actor–Critic (Value-Guided Policy Update)</h3>
<p>Actor–Critic algorithms blend two ideas, the actor learns <em>what to do</em>, while the critic learns <em>how good</em> that decision was. This fusion of policy gradient and value estimation makes learning faster and more stable than pure policy-based (like REINFORCE) or pure value-based (like Q-Learning) methods.</p>
<section id="what-problem-are-we-solving-67" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-67">What Problem Are We Solving?</h4>
<p>REINFORCE learns directly from entire episodes, which causes:</p>
<ul>
<li>High variance (updates depend on long-term rewards)</li>
<li>Slow learning (no intermediate feedback)</li>
</ul>
<p>Actor–Critic solves this by adding a critic that estimates the value function <span class="math inline">\(V_\phi(s)\)</span>, providing the actor with real-time feedback at every step. It bridges the gap between Monte Carlo and temporal-difference learning.</p>
</section>
<section id="the-core-idea-37" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-37">The Core Idea</h4>
<p>We maintain two networks:</p>
<ul>
<li>Actor: the policy <span class="math inline">\(\pi_\theta(a|s)\)</span> that decides what to do</li>
<li>Critic: the value function <span class="math inline">\(V_\phi(s)\)</span> that judges how good the current state (or action) is</li>
</ul>
<p>The actor updates in the direction suggested by the critic’s feedback: <span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E} \big[ \nabla_\theta \log \pi_\theta(a_t|s_t) , \delta_t \big]
\]</span> where the TD error (advantage) is: <span class="math display">\[
\delta_t = R_{t+1} + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
\]</span></p>
<p>The critic learns to minimize the mean squared TD error: <span class="math display">\[
L(\phi) = \big( \delta_t \big)^2
\]</span></p>
</section>
<section id="how-it-works-plain-language-26" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-26">How It Works (Plain Language)</h4>
<p>Think of the actor as an explorer and the critic as a coach. The actor tries different actions, and the critic immediately says “that was better/worse than expected.” The actor then adjusts its probabilities accordingly, while the critic keeps refining its sense of value.</p>
</section>
<section id="step-by-step-summary-33" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-33">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 91%">
<col style="width: 4%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize actor parameters <span class="math inline">\(\theta\)</span> and critic parameters <span class="math inline">\(\phi\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>For each episode:</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>a. Observe state <span class="math inline">\(s_t\)</span>, choose action <span class="math inline">\(a_t \sim \pi_\theta(a_t                                     | s_t)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>b. Execute <span class="math inline">\(a_t\)</span>, observe <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(s_{t+1}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Compute TD error: <span class="math inline">\(\delta_t = R_{t+1} + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Update critic: <span class="math inline">\(\phi \leftarrow \phi + \beta , \delta_t , \nabla_\phi V_\phi(s_t)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>e. Update actor: <span class="math inline">\(\theta \leftarrow \theta + \alpha , \delta_t , \nabla_\theta \log \pi_\theta(a_t | s_t)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>f.&nbsp;Repeat until done</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="mathematical-formulation-16" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-16">Mathematical Formulation</h4>
<ol type="1">
<li><p>Actor update <span class="math display">\[
\theta \leftarrow \theta + \alpha , \delta_t , \nabla_\theta \log \pi_\theta(a_t|s_t)
\]</span></p></li>
<li><p>Critic update <span class="math display">\[
\phi \leftarrow \phi + \beta , \delta_t , \nabla_\phi V_\phi(s_t)
\]</span></p></li>
<li><p>TD error (advantage) <span class="math display">\[
\delta_t = R_{t+1} + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
\]</span></p></li>
</ol>
</section>
<section id="example-43" class="level4">
<h4 class="anchored" data-anchor-id="example-43">Example</h4>
<p>Consider a robot learning to walk:</p>
<ul>
<li>The actor controls muscle activations (policy)</li>
<li>The critic predicts future stability (value) If a step improves balance, the critic’s <span class="math inline">\(\delta_t\)</span> is positive, rewarding those actions. If the robot stumbles, <span class="math inline">\(\delta_t\)</span> becomes negative, and the actor reduces those action probabilities.</li>
</ul>
</section>
<section id="tiny-code-17" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-17">Tiny Code</h4>
<p>Python (simplified actor–critic skeleton)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, torch.nn <span class="im">as</span> nn, torch.optim <span class="im">as</span> optim</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Actor(nn.Module):</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_states, n_actions):</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(n_states, <span class="dv">128</span>), nn.ReLU(),</span>
<span id="cb120-9"><a href="#cb120-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, n_actions), nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb120-10"><a href="#cb120-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb120-11"><a href="#cb120-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb120-12"><a href="#cb120-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-13"><a href="#cb120-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Critic(nn.Module):</span>
<span id="cb120-14"><a href="#cb120-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_states):</span>
<span id="cb120-15"><a href="#cb120-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb120-16"><a href="#cb120-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(nn.Linear(n_states, <span class="dv">128</span>), nn.ReLU(), nn.Linear(<span class="dv">128</span>, <span class="dv">1</span>))</span>
<span id="cb120-17"><a href="#cb120-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb120-18"><a href="#cb120-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb120-19"><a href="#cb120-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-20"><a href="#cb120-20" aria-hidden="true" tabindex="-1"></a>actor, critic <span class="op">=</span> Actor(<span class="dv">4</span>, <span class="dv">2</span>), Critic(<span class="dv">4</span>)</span>
<span id="cb120-21"><a href="#cb120-21" aria-hidden="true" tabindex="-1"></a>opt_a <span class="op">=</span> optim.Adam(actor.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb120-22"><a href="#cb120-22" aria-hidden="true" tabindex="-1"></a>opt_c <span class="op">=</span> optim.Adam(critic.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb120-23"><a href="#cb120-23" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb120-24"><a href="#cb120-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-25"><a href="#cb120-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(s, a, r, s_next):</span>
<span id="cb120-26"><a href="#cb120-26" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> torch.tensor(s, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb120-27"><a href="#cb120-27" aria-hidden="true" tabindex="-1"></a>    s_next <span class="op">=</span> torch.tensor(s_next, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb120-28"><a href="#cb120-28" aria-hidden="true" tabindex="-1"></a>    v_s <span class="op">=</span> critic(s)</span>
<span id="cb120-29"><a href="#cb120-29" aria-hidden="true" tabindex="-1"></a>    v_next <span class="op">=</span> critic(s_next).detach()</span>
<span id="cb120-30"><a href="#cb120-30" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> v_next <span class="op">-</span> v_s</span>
<span id="cb120-31"><a href="#cb120-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># critic update</span></span>
<span id="cb120-32"><a href="#cb120-32" aria-hidden="true" tabindex="-1"></a>    loss_c <span class="op">=</span> delta.<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="cb120-33"><a href="#cb120-33" aria-hidden="true" tabindex="-1"></a>    opt_c.zero_grad()<span class="op">;</span> loss_c.backward()<span class="op">;</span> opt_c.step()</span>
<span id="cb120-34"><a href="#cb120-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># actor update</span></span>
<span id="cb120-35"><a href="#cb120-35" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> actor(s)</span>
<span id="cb120-36"><a href="#cb120-36" aria-hidden="true" tabindex="-1"></a>    log_prob <span class="op">=</span> torch.log(probs[a])</span>
<span id="cb120-37"><a href="#cb120-37" aria-hidden="true" tabindex="-1"></a>    loss_a <span class="op">=</span> <span class="op">-</span>log_prob <span class="op">*</span> delta.detach()</span>
<span id="cb120-38"><a href="#cb120-38" aria-hidden="true" tabindex="-1"></a>    opt_a.zero_grad()<span class="op">;</span> loss_a.backward()<span class="op">;</span> opt_a.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-67" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-67">Why It Matters</h4>
<ul>
<li>Lower variance than REINFORCE (thanks to the critic)</li>
<li>Online learning, can update after each step instead of full episodes</li>
<li>Foundation for advanced methods like A2C, PPO, DDPG, and SAC</li>
<li>Balances exploration and exploitation through continuous feedback</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-66" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-66">A Gentle Proof (Why It Works)</h4>
<p>The policy gradient theorem with a baseline <span class="math inline">\(b(s)=V(s)\)</span> gives: <span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}*{\pi*\theta} \big[ \nabla_\theta \log \pi_\theta(a|s) (Q(s,a) - V(s)) \big]
\]</span></p>
<p>The critic approximates <span class="math inline">\(V(s)\)</span>, so <span class="math inline">\(Q(s,a) - V(s) \approx \delta_t\)</span>. Thus, the actor update uses the TD error <span class="math inline">\(\delta_t\)</span> as an unbiased estimator of advantage.</p>
</section>
<section id="try-it-yourself-67" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-67">Try It Yourself</h4>
<ol type="1">
<li>Implement Actor–Critic for CartPole-v1.</li>
<li>Compare performance with REINFORCE, notice smoother learning.</li>
<li>Try continuous actions using Gaussian policy.</li>
<li>Add entropy regularization to encourage exploration.</li>
<li>Experiment with learning rates <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</li>
</ol>
</section>
<section id="test-cases-67" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-67">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 26%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CartPole-v1</td>
<td>Discrete control</td>
<td>Faster convergence than REINFORCE</td>
</tr>
<tr class="even">
<td>Pendulum-v1</td>
<td>Continuous control</td>
<td>Works with Gaussian policies</td>
</tr>
<tr class="odd">
<td>LunarLander-v2</td>
<td>Complex dynamics</td>
<td>Stable learning with small variance</td>
</tr>
<tr class="even">
<td>MountainCar-v0</td>
<td>Sparse rewards</td>
<td>Needs patience and exploration</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-67" class="level4">
<h4 class="anchored" data-anchor-id="complexity-67">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(T)\)</span> per episode (TD updates each step)</li>
<li>Space: <span class="math inline">\(O(|\theta| + |\phi|)\)</span></li>
<li>Convergence: faster and smoother than REINFORCE, but sensitive to critic bias</li>
</ul>
<p>Actor–Critic is like a two-part mind, the actor acts with intuition, the critic evaluates with reason, and together they converge toward mastery.</p>
</section>
</section>
<section id="ppo-proximal-policy-optimization" class="level3">
<h3 class="anchored" data-anchor-id="ppo-proximal-policy-optimization">969. PPO (Proximal Policy Optimization)</h3>
<p>Proximal Policy Optimization (PPO) is one of the most popular and stable policy gradient algorithms. It improves upon Actor–Critic methods by carefully controlling how much the policy can change at each update, preventing destructive steps that destabilize training.</p>
<section id="what-problem-are-we-solving-68" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-68">What Problem Are We Solving?</h4>
<p>In policy gradient and actor–critic methods, large policy updates can cause:</p>
<ul>
<li>Instability: new policies deviate too much from previous ones</li>
<li>Collapse: overfitting to noisy advantages</li>
<li>Catastrophic forgetting: good behaviors are suddenly lost</li>
</ul>
<p>PPO solves this by enforcing a <em>trust region</em>, it limits how far the new policy can move from the old one while still improving.</p>
</section>
<section id="the-core-idea-38" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-38">The Core Idea</h4>
<p>PPO optimizes a clipped surrogate objective: <span class="math display">\[
L^{CLIP}(\theta) = \mathbb{E}_t \Big[ \min\big( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \big) \Big]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span> is the probability ratio</li>
<li><span class="math inline">\(A_t\)</span> is the advantage estimate</li>
<li><span class="math inline">\(\epsilon\)</span> (e.g., 0.1–0.3) defines the clipping range</li>
</ul>
<p>The “min” expression ensures that when <span class="math inline">\(r_t(\theta)\)</span> moves too far from 1, the gradient is cut off, keeping updates safe and stable.</p>
</section>
<section id="how-it-works-plain-language-27" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-27">How It Works (Plain Language)</h4>
<p>Think of PPO as telling the actor:</p>
<blockquote class="blockquote">
<p>“You can improve the policy, but only a little at a time.”</p>
</blockquote>
<p>It prevents the actor from “jumping off a cliff” by limiting how much the probability of chosen actions can change. This controlled adjustment keeps learning steady even in complex environments.</p>
</section>
<section id="step-by-step-summary-34" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-34">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 95%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Collect experience <span class="math inline">\((s_t, a_t, r_t, s_{t+1})\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Estimate the advantage <span class="math inline">\(A_t = R_t + \gamma V(s_{t+1}) - V(s_t)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Compute the probability ratio <span class="math inline">\(r_t(\theta)\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Update policy by maximizing the clipped objective <span class="math inline">\(L^{CLIP}(\theta)\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update value function <span class="math inline">\(V_\phi(s)\)</span> by minimizing squared error</td>
</tr>
<tr class="even">
<td>6</td>
<td>Repeat for several epochs over the same batch (mini-batch updates)</td>
</tr>
</tbody>
</table>
</section>
<section id="mathematical-formulation-17" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-17">Mathematical Formulation</h4>
<ol type="1">
<li><p>Policy loss (clipped objective): <span class="math display">\[
L^{CLIP}(\theta) = \mathbb{E}\Big[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)\Big]
\]</span></p></li>
<li><p>Value loss: <span class="math display">\[
L^{V}(\phi) = \mathbb{E}\Big[ (V_\phi(s_t) - R_t)^2 \Big]
\]</span></p></li>
<li><p>Entropy regularization (optional): <span class="math display">\[
L^{S} = \mathbb{E}[ -\beta , H(\pi_\theta(\cdot|s_t)) ]
\]</span></p></li>
<li><p>Combined objective: <span class="math display">\[
L = L^{CLIP} - c_1 L^{V} + c_2 L^{S}
\]</span></p></li>
</ol>
</section>
<section id="example-44" class="level4">
<h4 class="anchored" data-anchor-id="example-44">Example</h4>
<p>Suppose the old policy predicted: <span class="math display">\[
\pi_{\text{old}}(a|s) = 0.5, \quad A_t = +2
\]</span> If the new policy predicts <span class="math inline">\(\pi_\theta(a|s) = 0.8\)</span>, then <span class="math inline">\(r_t = 1.6\)</span>, and since it exceeds <span class="math inline">\((1+\epsilon)\)</span>, the gradient is clipped, ensuring stable improvement.</p>
</section>
<section id="tiny-code-18" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-18">Tiny Code</h4>
<p>Python (simplified PPO training loop)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, torch.nn <span class="im">as</span> nn, torch.optim <span class="im">as</span> optim</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ActorCritic(nn.Module):</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_states, n_actions):</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shared <span class="op">=</span> nn.Sequential(nn.Linear(n_states, <span class="dv">64</span>), nn.Tanh())</span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actor <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">64</span>, n_actions), nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.critic <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.shared(x)</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.actor(h), <span class="va">self</span>.critic(h)</span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a>env_n_states, env_n_actions <span class="op">=</span> <span class="dv">4</span>, <span class="dv">2</span></span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ActorCritic(env_n_states, env_n_actions)</span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-19"><a href="#cb121-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppo_update(batch):</span>
<span id="cb121-20"><a href="#cb121-20" aria-hidden="true" tabindex="-1"></a>    s, a, r, s_next, old_logprob, adv <span class="op">=</span> batch</span>
<span id="cb121-21"><a href="#cb121-21" aria-hidden="true" tabindex="-1"></a>    probs, values <span class="op">=</span> model(torch.tensor(s, dtype<span class="op">=</span>torch.float32))</span>
<span id="cb121-22"><a href="#cb121-22" aria-hidden="true" tabindex="-1"></a>    dist <span class="op">=</span> torch.distributions.Categorical(probs)</span>
<span id="cb121-23"><a href="#cb121-23" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> dist.log_prob(torch.tensor(a))</span>
<span id="cb121-24"><a href="#cb121-24" aria-hidden="true" tabindex="-1"></a>    ratio <span class="op">=</span> torch.exp(logprob <span class="op">-</span> torch.tensor(old_logprob))</span>
<span id="cb121-25"><a href="#cb121-25" aria-hidden="true" tabindex="-1"></a>    clipped <span class="op">=</span> torch.clamp(ratio, <span class="dv">1</span> <span class="op">-</span> epsilon, <span class="dv">1</span> <span class="op">+</span> epsilon) <span class="op">*</span> torch.tensor(adv)</span>
<span id="cb121-26"><a href="#cb121-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">min</span>(ratio <span class="op">*</span> torch.tensor(adv), clipped).mean()</span>
<span id="cb121-27"><a href="#cb121-27" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()<span class="op">;</span> loss.backward()<span class="op">;</span> optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-68" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-68">Why It Matters</h4>
<ul>
<li>Stability: avoids catastrophic updates</li>
<li>Efficiency: multiple epochs of mini-batch optimization</li>
<li>Simplicity: easy to implement (no complex constraints like TRPO)</li>
<li>Performance: state-of-the-art in robotics, games, and control</li>
</ul>
<p>PPO is widely used in:</p>
<ul>
<li>OpenAI Gym environments</li>
<li>Robotics (MuJoCo, Isaac Gym)</li>
<li>Game agents (Atari, StarCraft, Dota 2)</li>
<li>Sim-to-real control transfer</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-67" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-67">A Gentle Proof (Why It Works)</h4>
<p>The key to PPO’s success is its surrogate objective bound. When the policy update is small (<span class="math inline">\(r_t \approx 1\)</span>), PPO behaves like a standard policy gradient. When updates grow too large, the clipping term caps the objective, limiting step size. This behaves like a <em>soft trust region</em>, ensuring monotonic improvement in expected return.</p>
</section>
<section id="try-it-yourself-68" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-68">Try It Yourself</h4>
<ol type="1">
<li>Implement PPO for CartPole-v1 or LunarLander-v2.</li>
<li>Compare clipped vs unclipped updates, observe stability difference.</li>
<li>Tune <span class="math inline">\(\epsilon\)</span> between 0.1–0.3 for best results.</li>
<li>Plot policy ratio <span class="math inline">\(r_t\)</span> over time to visualize constraint effects.</li>
<li>Extend to continuous actions using Gaussian distributions.</li>
</ol>
</section>
<section id="test-cases-68" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-68">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 49%">
</colgroup>
<thead>
<tr class="header">
<th>Environment</th>
<th>Description</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CartPole-v1</td>
<td>Discrete control</td>
<td>Fast and stable learning</td>
</tr>
<tr class="even">
<td>LunarLander-v2</td>
<td>Sparse rewards</td>
<td>Smooth training curve</td>
</tr>
<tr class="odd">
<td>Hopper-v2 (MuJoCo)</td>
<td>Continuous control</td>
<td>Strong performance</td>
</tr>
<tr class="even">
<td>Humanoid-v2</td>
<td>High-dimensional</td>
<td>Scales well with mini-batch updates</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-68" class="level4">
<h4 class="anchored" data-anchor-id="complexity-68">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(E \times B)\)</span> (epochs × batch size)</li>
<li>Space: <span class="math inline">\(O(B)\)</span> (stored trajectories)</li>
<li>Convergence: stable, monotonic improvement</li>
</ul>
<p>PPO is the gold standard of modern policy optimization, simple to implement, robust to hyperparameters, and the reason reinforcement learning scaled beyond the lab.</p>
</section>
</section>
<section id="ddpg-sac-continuous-action-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="ddpg-sac-continuous-action-reinforcement-learning">970. DDPG / SAC (Continuous Action Reinforcement Learning)</h3>
<p>When environments require continuous actions, like steering a car or controlling a robotic arm, discrete algorithms such as Q-learning or PPO need adaptation. DDPG (Deep Deterministic Policy Gradient) and SAC (Soft Actor–Critic) are two powerful actor–critic frameworks designed for these continuous domains, balancing precision, stability, and exploration.</p>
<section id="what-problem-are-we-solving-69" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-69">What Problem Are We Solving?</h4>
<p>Most reinforcement learning algorithms assume discrete actions (e.g., move left or right). But real-world control problems often need continuous control (e.g., throttle = 0.73). For such cases:</p>
<ul>
<li>Q-learning cannot directly apply (it requires <span class="math inline">\(\max_a Q(s,a)\)</span>).</li>
<li>Policy gradients (like REINFORCE) are too noisy.</li>
<li>PPO struggles with fine-grained precision.</li>
</ul>
<p>DDPG and SAC solve this by combining value-based learning (for stability) and policy-based learning (for flexibility).</p>
</section>
</section>
<section id="deep-deterministic-policy-gradient-ddpg" class="level3">
<h3 class="anchored" data-anchor-id="deep-deterministic-policy-gradient-ddpg">1. Deep Deterministic Policy Gradient (DDPG)</h3>
<p>DDPG is an off-policy, deterministic actor–critic algorithm. It uses two networks:</p>
<ul>
<li>Actor: outputs a deterministic action <span class="math inline">\(a = \mu_\theta(s)\)</span></li>
<li>Critic: evaluates it via <span class="math inline">\(Q_\phi(s, a)\)</span></li>
</ul>
<section id="core-update-rules" class="level4">
<h4 class="anchored" data-anchor-id="core-update-rules">Core Update Rules</h4>
<ol type="1">
<li><p>Critic update (TD error): <span class="math display">\[
L(\phi) = \big(Q_\phi(s_t, a_t) - y_t\big)^2
\]</span> where <span class="math display">\[
y_t = r_t + \gamma Q_{\phi'}(s_{t+1}, \mu_{\theta'}(s_{t+1}))
\]</span> (The primed networks are <em>target networks</em> for stability.)</p></li>
<li><p>Actor update (policy gradient): <span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}\big[ \nabla_a Q_\phi(s,a) \big|*{a=\mu*\theta(s)} \nabla_\theta \mu_\theta(s) \big]
\]</span></p></li>
</ol>
</section>
<section id="key-features" class="level4">
<h4 class="anchored" data-anchor-id="key-features">Key Features</h4>
<ul>
<li>Deterministic actions <span class="math inline">\(\to\)</span> stable updates</li>
<li>Target networks <span class="math inline">\(\to\)</span> prevent divergence</li>
<li>Experience replay <span class="math inline">\(\to\)</span> sample efficiency</li>
<li>Noise (like Ornstein–Uhlenbeck) <span class="math inline">\(\to\)</span> exploration in continuous space</li>
</ul>
</section>
<section id="tiny-code-ddpg-skeleton" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-ddpg-skeleton">Tiny Code (DDPG skeleton)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, torch.nn <span class="im">as</span> nn, torch.optim <span class="im">as</span> optim</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Actor(nn.Module):</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, s_dim, a_dim):</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(s_dim, <span class="dv">256</span>), nn.ReLU(),</span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, a_dim), nn.Tanh())</span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, s): <span class="cf">return</span> <span class="va">self</span>.net(s)</span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-11"><a href="#cb122-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Critic(nn.Module):</span>
<span id="cb122-12"><a href="#cb122-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, s_dim, a_dim):</span>
<span id="cb122-13"><a href="#cb122-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb122-14"><a href="#cb122-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb122-15"><a href="#cb122-15" aria-hidden="true" tabindex="-1"></a>            nn.Linear(s_dim <span class="op">+</span> a_dim, <span class="dv">256</span>), nn.ReLU(),</span>
<span id="cb122-16"><a href="#cb122-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">1</span>))</span>
<span id="cb122-17"><a href="#cb122-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, s, a): <span class="cf">return</span> <span class="va">self</span>.net(torch.cat([s,a], dim<span class="op">=-</span><span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-69" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-69">Why It Matters</h4>
<ul>
<li>Excellent for robotic control, autonomous vehicles, and game physics.</li>
<li>Handles high-dimensional continuous actions smoothly.</li>
<li>Foundation for more advanced variants like TD3 and SAC.</li>
</ul>
</section>
</section>
<section id="soft-actorcritic-sac" class="level3">
<h3 class="anchored" data-anchor-id="soft-actorcritic-sac">2. Soft Actor–Critic (SAC)</h3>
<p>SAC extends DDPG with a maximum entropy principle, encouraging exploration and robustness. It learns stochastic policies rather than deterministic ones, favoring both reward and entropy.</p>
<section id="objective" class="level4">
<h4 class="anchored" data-anchor-id="objective">Objective</h4>
<p>Maximize the expected return with entropy: <span class="math display">\[
J(\pi) = \sum_t \mathbb{E}_{(s_t,a_t)\sim\pi}\big[ r(s_t,a_t) + \alpha , \mathcal{H}(\pi(\cdot|s_t)) \big]
\]</span></p>
<p>Here, <span class="math inline">\(\alpha\)</span> controls the exploration–exploitation tradeoff:</p>
<ul>
<li>Large <span class="math inline">\(\alpha\)</span> → more random exploration</li>
<li>Small <span class="math inline">\(\alpha\)</span> → more focused on reward</li>
</ul>
</section>
<section id="policy-update" class="level4">
<h4 class="anchored" data-anchor-id="policy-update">Policy Update</h4>
<p>SAC minimizes the soft policy loss: <span class="math display">\[
L_\pi(\theta) = \mathbb{E}\big[ \alpha \log \pi_\theta(a_t|s_t) - Q_\phi(s_t, a_t) \big]
\]</span></p>
<p>and the critic loss: <span class="math display">\[
L_Q(\phi) = \big(Q_\phi(s_t,a_t) - (r_t + \gamma V_{\bar{\phi}}(s_{t+1}))\big)^2
\]</span></p>
<p>with the soft value function: <span class="math display">\[
V_{\bar{\phi}}(s) = \mathbb{E}*{a\sim\pi*\theta}\big[ Q_\phi(s,a) - \alpha \log \pi_\theta(a|s) \big]
\]</span></p>
</section>
<section id="how-it-works-plain-language-28" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-28">How It Works (Plain Language)</h4>
<p>SAC doesn’t just seek <em>good</em> actions; it prefers many good options. By rewarding <em>uncertainty</em> (entropy), it avoids getting stuck in narrow, brittle behaviors. The result: smooth, stable, and exploratory control.</p>
</section>
<section id="step-by-step-summary-35" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-summary-35">Step-by-Step Summary</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Collect experiences <span class="math inline">\((s,a,r,s')\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>Update critic with soft target <span class="math inline">\(r + \gamma (Q - \alpha \log \pi)\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td>Update actor to maximize <span class="math inline">\(Q - \alpha \log \pi\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td>Adjust <span class="math inline">\(\alpha\)</span> automatically to maintain target entropy</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Repeat using replay buffer samples</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-sac-skeleton" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-sac-skeleton">Tiny Code (SAC skeleton)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> soft_q_loss(s, a, r, s_next, done, actor, critic, target, alpha):</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>        a_next, logp <span class="op">=</span> actor.sample(s_next)</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>        q_target <span class="op">=</span> r <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> done) <span class="op">*</span> gamma <span class="op">*</span> (target(s_next, a_next) <span class="op">-</span> alpha <span class="op">*</span> logp)</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> critic(s, a)</span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((q <span class="op">-</span> q_target)<span class="dv">2</span>).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-70" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-70">Why It Matters</h4>
<ul>
<li>More stable than DDPG (thanks to entropy term)</li>
<li>Better exploration, avoids premature convergence</li>
<li>Works well in stochastic, high-dimensional tasks</li>
<li>Automatic entropy tuning simplifies hyperparameters</li>
</ul>
<p>SAC often outperforms all other continuous-control methods on benchmarks like HalfCheetah, Walker2D, and Ant.</p>
</section>
<section id="comparison-table" class="level4">
<h4 class="anchored" data-anchor-id="comparison-table">Comparison Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 31%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>DDPG</th>
<th>SAC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Policy type</td>
<td>Deterministic</td>
<td>Stochastic</td>
</tr>
<tr class="even">
<td>Exploration</td>
<td>Noise process</td>
<td>Entropy term</td>
</tr>
<tr class="odd">
<td>Stability</td>
<td>Moderate</td>
<td>Very high</td>
</tr>
<tr class="even">
<td>Sample efficiency</td>
<td>High</td>
<td>High</td>
</tr>
<tr class="odd">
<td>Entropy tuning</td>
<td>Manual</td>
<td>Automatic</td>
</tr>
<tr class="even">
<td>Typical domains</td>
<td>Robotics, simulators</td>
<td>Robotics, physics, control</td>
</tr>
</tbody>
</table>
</section>
<section id="try-it-yourself-69" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-69">Try It Yourself</h4>
<ol type="1">
<li>Train both DDPG and SAC on Pendulum-v1.</li>
<li>Compare learning curves, SAC will be smoother.</li>
<li>Adjust <span class="math inline">\(\alpha\)</span> to see entropy’s effect on exploration.</li>
<li>Extend to Humanoid-v2 for large-scale control.</li>
<li>Experiment with replay buffer size and target update rate.</li>
</ol>
</section>
<section id="test-cases-69" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-69">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Environment</th>
<th>Algorithm</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pendulum-v1</td>
<td>DDPG</td>
<td>Works, but sensitive to noise</td>
</tr>
<tr class="even">
<td>Hopper-v2</td>
<td>SAC</td>
<td>Stable, continuous learning</td>
</tr>
<tr class="odd">
<td>Ant-v2</td>
<td>SAC</td>
<td>Smooth convergence</td>
</tr>
<tr class="even">
<td>Reacher-v2</td>
<td>DDPG</td>
<td>May overfit or oscillate</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-69" class="level4">
<h4 class="anchored" data-anchor-id="complexity-69">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(N)\)</span> per batch (two networks updated per step)</li>
<li>Space: <span class="math inline">\(O(|\theta| + |\phi|)\)</span> (actor and critic params)</li>
<li>Convergence: SAC &gt; DDPG in stability and exploration</li>
</ul>
<p>In short: DDPG aims for precision; SAC aims for stability. Together, they define the modern standard for continuous control learning, where the agent doesn’t just choose actions, but <em>flows</em> through them.</p>
</section>
</section>
</section>
<section id="section-98.-approximation-and-online-algorithms" class="level1">
<h1>Section 98. Approximation and Online Algorithms</h1>
<section id="greedy-set-cover-ln-n-approximation" class="level3">
<h3 class="anchored" data-anchor-id="greedy-set-cover-ln-n-approximation">971. Greedy Set Cover (ln n-Approximation)</h3>
<p>The Set Cover Problem asks: given a universe of elements and a collection of sets that cover them, what is the smallest number of sets needed to cover everything? This problem is NP-hard, but a simple greedy algorithm gives a logarithmic-factor approximation, one of the most elegant results in approximation algorithms.</p>
<section id="what-problem-are-we-solving-70" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-70">What Problem Are We Solving?</h4>
<p>Formally: Let the universe be <span class="math display">\[
U = {e_1, e_2, \dots, e_n}
\]</span> and a family of subsets <span class="math display">\[
S = {S_1, S_2, \dots, S_m}, \quad S_i \subseteq U.
\]</span></p>
<p>We want to choose the smallest subcollection <span class="math inline">\(C \subseteq S\)</span> such that <span class="math display">\[
\bigcup_{S_i \in C} S_i = U.
\]</span></p>
<p>This appears in:</p>
<ul>
<li>Sensor placement (cover all regions)</li>
<li>Feature selection</li>
<li>Test suite minimization</li>
<li>Network monitoring</li>
</ul>
<p>Exact solutions are exponential in <span class="math inline">\(m\)</span>, so we seek a good approximation instead.</p>
</section>
<section id="the-core-idea-39" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-39">The Core Idea</h4>
<p>Pick the set that covers the largest number of uncovered elements at each step. Repeat until every element is covered.</p>
<p>Although greedy may not find the perfect solution, it guarantees a solution no worse than a factor of <span class="math inline">\(\ln n\)</span> times the optimal.</p>
</section>
<section id="step-by-step-example-4" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-4">Step-by-Step Example</h4>
<p>Suppose <span class="math inline">\(U = {1,2,3,4,5,6}\)</span> and <span class="math display">\[
S_1 = {1,2,3}, \quad S_2 = {2,4}, \quad S_3 = {3,4,5}, \quad S_4 = {5,6}.
\]</span></p>
<p>Greedy steps:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Iteration</th>
<th>Sets Available</th>
<th>Chosen Set</th>
<th>Newly Covered</th>
<th>Covered So Far</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>all</td>
<td><span class="math inline">\(S_1\)</span></td>
<td>{1,2,3}</td>
<td>{1,2,3}</td>
</tr>
<tr class="even">
<td>2</td>
<td>remaining</td>
<td><span class="math inline">\(S_3\)</span></td>
<td>{4,5}</td>
<td>{1,2,3,4,5}</td>
</tr>
<tr class="odd">
<td>3</td>
<td>remaining</td>
<td><span class="math inline">\(S_4\)</span></td>
<td>{6}</td>
<td>{1,2,3,4,5,6}</td>
</tr>
</tbody>
</table>
<p>Total sets used = 3 (optimal is 2 or 3). Still within the <span class="math inline">\(\ln n\)</span> bound.</p>
</section>
<section id="algorithm-pseudocode" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode">Algorithm (Pseudocode)</h4>
<pre><code>GreedySetCover(U, S):
    C = ∅
    while U not empty:
        pick S_i in S that covers the largest number of uncovered elements
        C = C ∪ {S_i}
        U = U \ S_i
    return C</code></pre>
</section>
<section id="mathematical-guarantee" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-guarantee">Mathematical Guarantee</h4>
<p>Let OPT be the optimal number of sets. Then the greedy algorithm selects at most <span class="math display">\[
H_n = 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} \le \ln n + 1
\]</span> times OPT.</p>
<p>So: <span class="math display">\[
|C_{\text{greedy}}| \le (\ln n + 1) , |C_{\text{opt}}|.
\]</span></p>
<p>This is the best possible unless <span class="math inline">\(P = NP\)</span>.</p>
</section>
<section id="tiny-code-19" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-19">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_set_cover(universe, sets):</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> <span class="bu">set</span>(universe)</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>    cover <span class="op">=</span> []</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> U:</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>        best <span class="op">=</span> <span class="bu">max</span>(sets, key<span class="op">=</span><span class="kw">lambda</span> s: <span class="bu">len</span>(U <span class="op">&amp;</span> s))</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>        cover.append(best)</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>        U <span class="op">-=</span> best</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cover</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> {<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>}</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> [{<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>}, {<span class="dv">2</span>,<span class="dv">4</span>}, {<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>}, {<span class="dv">5</span>,<span class="dv">6</span>}]</span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> greedy_set_cover(U, S)</span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cover:"</span>, C)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-71" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-71">Why It Matters</h4>
<ul>
<li>Greedy set cover is a template for many covering and selection problems.</li>
<li>It achieves the tight logarithmic approximation bound.</li>
<li>It’s easy to implement and works well in practice.</li>
</ul>
<p>Applications include:</p>
<ul>
<li>Selecting minimal training examples</li>
<li>Reducing redundancy in feature sets</li>
<li>Building minimal monitoring systems</li>
</ul>
</section>
<section id="try-it-yourself-70" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-70">Try It Yourself</h4>
<ol type="1">
<li>Generate random subsets and test how many sets greedy picks vs.&nbsp;optimal (for small <span class="math inline">\(n\)</span>).</li>
<li>Visualize coverage after each step.</li>
<li>Implement the weighted version, prefer sets with higher “value per cost” ratio.</li>
<li>Test on document summarization: sentences as sets, words as elements.</li>
</ol>
</section>
<section id="test-cases-70" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-70">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 31%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>Universe</th>
<th>Sets</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>{1,2,3,4}</td>
<td>{{1,2}, {2,3}, {3,4}}</td>
<td>Picks 3 sets</td>
</tr>
<tr class="even">
<td>{1,2,3,4,5,6}</td>
<td>{{1,2,3}, {3,4,5}, {5,6}}</td>
<td>Picks 3 sets</td>
</tr>
<tr class="odd">
<td>Random subsets (n=100)</td>
<td>,</td>
<td>Achieves near-optimal ln(n) ratio</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-70" class="level4">
<h4 class="anchored" data-anchor-id="complexity-70">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(nm)\)</span> (each iteration scans all sets)</li>
<li>Space: <span class="math inline">\(O(n+m)\)</span></li>
<li>Approximation ratio: <span class="math inline">\(\ln n + 1\)</span></li>
</ul>
<p>The greedy set cover is a small miracle of simplicity, a one-line heuristic that quietly reaches the best provable bound for one of computer science’s hardest problems.</p>
</section>
</section>
<section id="vertex-cover-approximation-double-matching-heuristic" class="level3">
<h3 class="anchored" data-anchor-id="vertex-cover-approximation-double-matching-heuristic">972. Vertex Cover Approximation (Double-Matching Heuristic)</h3>
<p>The Vertex Cover Problem asks for the smallest set of vertices that touch every edge in a graph. It’s a cornerstone NP-hard problem, but surprisingly, a very simple algorithm achieves a 2-approximation, meaning it’s at most twice as large as the optimal solution.</p>
<section id="what-problem-are-we-solving-71" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-71">What Problem Are We Solving?</h4>
<p>Given an undirected graph <span class="math display">\[
G = (V, E)
\]</span> find the smallest subset of vertices <span class="math display">\[
C \subseteq V
\]</span> such that for every edge <span class="math inline">\((u, v) \in E\)</span>, at least one of <span class="math inline">\(u\)</span> or <span class="math inline">\(v\)</span> is in <span class="math inline">\(C\)</span>.</p>
<p>In other words, all edges are “covered” by selecting enough vertices.</p>
<p>Applications include:</p>
<ul>
<li>Network monitoring (every link has a watcher)</li>
<li>Facility placement (each connection served by one site)</li>
<li>Resource allocation and security systems</li>
</ul>
</section>
<section id="the-core-idea-40" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-40">The Core Idea</h4>
<p>Pick any uncovered edge <span class="math inline">\((u,v)\)</span>, add both its endpoints to the cover, then remove all edges incident to either <span class="math inline">\(u\)</span> or <span class="math inline">\(v\)</span>. Repeat until no edges remain.</p>
<p>This simple greedy-like approach guarantees that: <span class="math display">\[
|C_{\text{approx}}| \le 2 , |C_{\text{opt}}|
\]</span></p>
</section>
<section id="step-by-step-example-5" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-5">Step-by-Step Example</h4>
<p>Graph:</p>
<pre><code>Edges = {(1,2), (2,3), (3,4), (4,5)}</code></pre>
<p>Algorithm:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 17%">
<col style="width: 22%">
<col style="width: 33%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Picked Edge</th>
<th>Added Vertices</th>
<th>Remaining Edges</th>
<th>Current Cover</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>(1,2)</td>
<td>{1,2}</td>
<td>(3,4), (4,5)</td>
<td>{1,2}</td>
</tr>
<tr class="even">
<td>2</td>
<td>(3,4)</td>
<td>{3,4}</td>
<td>(4,5) already covered</td>
<td>{1,2,3,4}</td>
</tr>
</tbody>
</table>
<p>Final cover: {1,2,3,4}. Optimal cover is {2,4} (size 2), so ratio = 4 / 2 = 2, perfectly within the bound.</p>
</section>
<section id="algorithm-pseudocode-1" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode-1">Algorithm (Pseudocode)</h4>
<pre><code>ApproximateVertexCover(G):
    C = ∅
    while E not empty:
        pick any edge (u, v) ∈ E
        C = C ∪ {u, v}
        remove all edges incident to u or v
    return C</code></pre>
</section>
<section id="mathematical-guarantee-1" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-guarantee-1">Mathematical Guarantee</h4>
<p>Let OPT be the minimum vertex cover. The algorithm picks both endpoints for each selected edge, but since any edge must be incident to at least one vertex in OPT, the number of chosen edges ≤ |OPT|, and hence:</p>
<p><span class="math display">\[
|C| \le 2 |OPT|
\]</span></p>
<p>So the algorithm is a 2-approximation.</p>
</section>
<section id="tiny-code-20" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-20">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vertex_cover_approx(graph):</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>    cover <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>    edges <span class="op">=</span> <span class="bu">set</span>(graph)</span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> edges:</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>        u, v <span class="op">=</span> edges.pop()</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>        cover.update([u, v])</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>        edges <span class="op">=</span> {e <span class="cf">for</span> e <span class="kw">in</span> edges <span class="cf">if</span> u <span class="kw">not</span> <span class="kw">in</span> e <span class="kw">and</span> v <span class="kw">not</span> <span class="kw">in</span> e}</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cover</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a>edges <span class="op">=</span> {(<span class="dv">1</span>,<span class="dv">2</span>), (<span class="dv">2</span>,<span class="dv">3</span>), (<span class="dv">3</span>,<span class="dv">4</span>), (<span class="dv">4</span>,<span class="dv">5</span>)}</span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vertex_cover_approx(edges))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-72" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-72">Why It Matters</h4>
<ul>
<li>Fast, simple, and provably near-optimal</li>
<li>Basis for numerous network optimization heuristics</li>
<li>Helps design better bounds for other graph problems (like set cover, clique cover, etc.)</li>
<li>Forms part of “primal–dual” frameworks in combinatorial optimization</li>
</ul>
</section>
<section id="try-it-yourself-71" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-71">Try It Yourself</h4>
<ol type="1">
<li>Generate random graphs and compare to optimal vertex cover via brute force (for <span class="math inline">\(n &lt; 10\)</span>).</li>
<li>Implement weighted vertex cover (prefer lower-cost vertices).</li>
<li>Visualize the graph, mark edges as “covered” after each iteration.</li>
<li>Compare with matching-based approximation (maximum matching gives same factor).</li>
</ol>
</section>
<section id="test-cases-71" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-71">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Graph</th>
<th>Approx Cover</th>
<th>Optimal</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Path (1–2–3–4–5)</td>
<td>{1,2,3,4}</td>
<td>{2,4}</td>
<td>2.0</td>
</tr>
<tr class="even">
<td>Triangle (1–2–3–1)</td>
<td>{1,2}</td>
<td>{1,2}</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td>Star (1–{2,3,4,5})</td>
<td>{1,2}</td>
<td>{1}</td>
<td>2.0</td>
</tr>
<tr class="even">
<td>Random 6-node graph</td>
<td>~2×</td>
<td>~1×</td>
<td>≤ 2.0</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-71" class="level4">
<h4 class="anchored" data-anchor-id="complexity-71">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(|E|)\)</span></li>
<li>Space: <span class="math inline">\(O(|V|)\)</span></li>
<li>Approximation ratio: ≤ 2</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-68" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-68">A Gentle Proof (Why It Works)</h4>
<p>Each chosen edge <span class="math inline">\((u,v)\)</span> contributes two vertices to the cover. In any optimal cover, at least one of <span class="math inline">\(u\)</span> or <span class="math inline">\(v\)</span> must appear, hence each edge contributes at least one “token” toward OPT. Since we may pick both, we overcount by at most a factor of 2.</p>
</section>
<section id="summary-table" class="level4">
<h4 class="anchored" data-anchor-id="summary-table">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Algorithm Type</td>
<td>Greedy / Matching-based</td>
</tr>
<tr class="even">
<td>Guarantee</td>
<td>2× optimal</td>
</tr>
<tr class="odd">
<td>Determinism</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Works On</td>
<td>Unweighted graphs</td>
</tr>
<tr class="odd">
<td>Extension</td>
<td>Weighted version via LP rounding</td>
</tr>
</tbody>
</table>
<p>The vertex cover approximation is a perfect example of beauty in simplicity, one small doubling step that brings a hard combinatorial puzzle within reach of practicality.</p>
</section>
</section>
<section id="traveling-salesman-approximation-mst-based-2-approximation" class="level3">
<h3 class="anchored" data-anchor-id="traveling-salesman-approximation-mst-based-2-approximation">973. Traveling Salesman Approximation (MST-based 2-Approximation)</h3>
<p>The Traveling Salesman Problem (TSP) asks for the shortest possible tour that visits all cities exactly once and returns to the start. It’s one of the most famous NP-hard problems in combinatorial optimization, yet, when distances satisfy the triangle inequality, there’s a simple and elegant 2-approximation algorithm based on the Minimum Spanning Tree (MST).</p>
<section id="what-problem-are-we-solving-72" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-72">What Problem Are We Solving?</h4>
<p>Given a complete weighted graph <span class="math display">\[
G = (V, E)
\]</span> where <span class="math inline">\(w(u,v)\)</span> is the distance (cost) between vertices <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>, find a cycle that visits all vertices exactly once with minimal total cost.</p>
<p>Formally:</p>
<p><span class="math display">\[
\min_{\pi} \sum_{i=1}^{n} w(\pi_i, \pi_{i+1})
\]</span> where <span class="math inline">\(\pi\)</span> is a permutation of the vertices and <span class="math inline">\(\pi_{n+1} = \pi_1\)</span>.</p>
<p>When the distances satisfy the triangle inequality:</p>
<p><span class="math display">\[
w(u,v) \le w(u,x) + w(x,v),
\]</span></p>
<p>we can approximate the optimal tour using MST doubling and traversal.</p>
</section>
<section id="the-core-idea-41" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-41">The Core Idea</h4>
<ol type="1">
<li>Compute an MST of the graph (minimum cost to connect all nodes).</li>
<li>Traverse the MST in preorder (depth-first traversal).</li>
<li>Shortcut repeated vertices using triangle inequality.</li>
</ol>
<p>The resulting tour’s total cost ≤ 2 × optimal.</p>
</section>
<section id="step-by-step-example-6" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-6">Step-by-Step Example</h4>
<p>Consider a complete graph of 4 cities:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Edge</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(A,B)</td>
<td>1</td>
</tr>
<tr class="even">
<td>(A,C)</td>
<td>3</td>
</tr>
<tr class="odd">
<td>(A,D)</td>
<td>4</td>
</tr>
<tr class="even">
<td>(B,C)</td>
<td>2</td>
</tr>
<tr class="odd">
<td>(B,D)</td>
<td>5</td>
</tr>
<tr class="even">
<td>(C,D)</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Step 1: Build MST Edges in MST = {(A,B), (B,C), (C,D)} Total MST cost = 1 + 2 + 3 = 6.</p>
<p>Step 2: Traverse preorder: A → B → C → D → A Raw traversal cost = 8.</p>
<p>Step 3: Shortcut repeats if necessary (none here). Final TSP tour = 8 ≤ 2 × 6 = 12 (within bound).</p>
</section>
<section id="algorithm-pseudocode-2" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode-2">Algorithm (Pseudocode)</h4>
<pre><code>ApproxTSPviaMST(G):
    T = MinimumSpanningTree(G)
    tour = PreorderTraversal(T)
    return tour with shortcuts</code></pre>
</section>
<section id="mathematical-guarantee-2" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-guarantee-2">Mathematical Guarantee</h4>
<p>Let OPT be the cost of the optimal TSP tour.</p>
<ul>
<li>The MST has cost ≤ OPT (since removing one edge from the optimal tour yields a spanning tree).</li>
<li>Traversing the MST twice gives total cost ≤ 2 × MST ≤ 2 × OPT.</li>
<li>Shortcutting (triangle inequality) never increases cost.</li>
</ul>
<p>Thus:</p>
<p><span class="math display">\[
\text{Cost}*{\text{approx}} \le 2 \times \text{Cost}*{\text{OPT}}.
\]</span></p>
</section>
<section id="tiny-code-21" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-21">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tsp_mst_approx(G):</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>    T <span class="op">=</span> nx.minimum_spanning_tree(G)</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>    preorder <span class="op">=</span> <span class="bu">list</span>(nx.dfs_preorder_nodes(T, source<span class="op">=</span><span class="bu">list</span>(T.nodes)[<span class="dv">0</span>]))</span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>    tour <span class="op">=</span> preorder <span class="op">+</span> [preorder[<span class="dv">0</span>]]  <span class="co"># complete the cycle</span></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>    cost <span class="op">=</span> <span class="bu">sum</span>(G[tour[i]][tour[i<span class="op">+</span><span class="dv">1</span>]][<span class="st">'weight'</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(tour)<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tour, cost</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-73" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-73">Why It Matters</h4>
<ul>
<li>Simple yet effective for metric TSPs (triangle inequality holds).</li>
<li>Foundation for better heuristics (Christofides, 1.5-approximation).</li>
<li>Applies in routing, network cabling, robot motion, delivery logistics, etc.</li>
</ul>
<p>The MST heuristic captures a “skeleton” of efficient connectivity, doubling it ensures coverage, and shortcutting cleans up redundancy.</p>
</section>
<section id="try-it-yourself-72" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-72">Try It Yourself</h4>
<ol type="1">
<li>Generate random 2D points and compute Euclidean distances.</li>
<li>Compare MST-based TSP cost to brute-force optimal (for <span class="math inline">\(n \le 10\)</span>).</li>
<li>Visualize MST vs.&nbsp;resulting TSP path.</li>
<li>Extend to Christofides Algorithm (add minimum matching on odd-degree vertices).</li>
</ol>
</section>
<section id="test-cases-72" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-72">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Graph Type</th>
<th>Approximation</th>
<th>Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4-city Euclidean</td>
<td>≤ 2×</td>
<td>Matches bound</td>
</tr>
<tr class="even">
<td>10 random cities</td>
<td>≤ 2×</td>
<td>Works consistently</td>
</tr>
<tr class="odd">
<td>Metric graph</td>
<td>≤ 2×</td>
<td>Guaranteed bound</td>
</tr>
<tr class="even">
<td>Non-metric graph</td>
<td>&gt; 2×</td>
<td>Bound not guaranteed</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-72" class="level4">
<h4 class="anchored" data-anchor-id="complexity-72">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(E \log V)\)</span> for MST + <span class="math inline">\(O(V)\)</span> for traversal</li>
<li>Space: <span class="math inline">\(O(V)\)</span></li>
<li>Approximation ratio: ≤ 2</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-69" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-69">A Gentle Proof (Why It Works)</h4>
<p>Let <span class="math inline">\(T\)</span> be the MST, and let <span class="math inline">\(H\)</span> be the Euler tour obtained by doubling each edge in <span class="math inline">\(T\)</span>. Since <span class="math inline">\(T\)</span> is connected, <span class="math inline">\(H\)</span> visits every vertex at least once and has cost <span class="math inline">\(2w(T)\)</span>. By shortcutting repeated vertices (using triangle inequality), we get a Hamiltonian cycle whose cost ≤ <span class="math inline">\(2w(T)\)</span>. As <span class="math inline">\(w(T) \le w(\text{OPT})\)</span>, we conclude:</p>
<p><span class="math display">\[
w(\text{tour}) \le 2 w(\text{OPT}).
\]</span></p>
</section>
<section id="summary-table-1" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-1">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Algorithm Type</td>
<td>MST-based heuristic</td>
</tr>
<tr class="even">
<td>Approximation Ratio</td>
<td>≤ 2</td>
</tr>
<tr class="odd">
<td>Requires</td>
<td>Triangle inequality</td>
</tr>
<tr class="even">
<td>Determinism</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Extension</td>
<td>Christofides (1.5× OPT)</td>
</tr>
</tbody>
</table>
<p>The MST-based TSP approximation is the perfect balance between mathematical beauty and computational practicality, it doubles what you need, then prunes what you don’t.</p>
</section>
</section>
<section id="k-center-approximation-farthest-point-heuristic" class="level3">
<h3 class="anchored" data-anchor-id="k-center-approximation-farthest-point-heuristic">974. k-Center Approximation (Farthest-Point Heuristic)</h3>
<p>The k-Center Problem asks: given a set of points and a distance metric, how can we pick <em>k</em> centers so that the maximum distance from any point to its nearest center is as small as possible? It’s a classic clustering and facility-location problem, NP-hard, but solvable within a 2-approximation using a simple farthest-point heuristic.</p>
<section id="what-problem-are-we-solving-73" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-73">What Problem Are We Solving?</h4>
<p>Given a set of points <span class="math inline">\(V\)</span> and a distance function <span class="math inline">\(d(u,v)\)</span>, select <span class="math inline">\(k\)</span> centers <span class="math inline">\(C = {c_1, c_2, \dots, c_k}\)</span> to minimize the maximum distance from any point to its nearest center:</p>
<p><span class="math display">\[
r^* = \min_{C \subseteq V, |C|=k} \max_{v \in V} \min_{c \in C} d(v, c)
\]</span></p>
<p>The goal is to make sure no point is too far from a chosen center.</p>
<p>This models problems like:</p>
<ul>
<li>Placing hospitals to minimize the farthest patient distance</li>
<li>Designing data centers for minimal latency</li>
<li>Building cell towers with guaranteed coverage radius</li>
</ul>
</section>
<section id="the-core-idea-42" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-42">The Core Idea</h4>
<ol type="1">
<li>Start with an arbitrary point as the first center.</li>
<li>Repeatedly choose the farthest point from all current centers.</li>
<li>Stop after <span class="math inline">\(k\)</span> centers have been chosen.</li>
</ol>
<p>This greedy selection ensures every new center captures the worst-served region so far. The result is at most twice as far as the optimal radius.</p>
</section>
<section id="step-by-step-example-7" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-example-7">Step-by-Step Example</h4>
<p>Points along a line: <span class="math inline">\(V = {0, 1, 2, 5, 8, 11}\)</span> and <span class="math inline">\(k = 2\)</span>.</p>
<p>Algorithm:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 24%">
<col style="width: 17%">
<col style="width: 31%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Chosen Centers</th>
<th>Farthest Point</th>
<th>Distance to Closest Center</th>
<th>Centers After Step</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>pick arbitrary → {0}</td>
<td>farthest = 11</td>
<td>distance = 11</td>
<td>{0, 11}</td>
</tr>
<tr class="even">
<td>2</td>
<td>stop (2 centers)</td>
<td>,</td>
<td>,</td>
<td>final = {0, 11}</td>
</tr>
</tbody>
</table>
<p>Radius achieved: Every point ≤ 5.5 from nearest center. Optimal radius: 5 → so ratio = 1.1 ≤ 2.</p>
</section>
<section id="algorithm-pseudocode-3" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode-3">Algorithm (Pseudocode)</h4>
<pre><code>GreedyKCenter(V, k):
    pick any point v as first center
    C = {v}
    while |C| &lt; k:
        select point x in V that maximizes min_{c in C} d(x, c)
        add x to C
    return C</code></pre>
</section>
<section id="mathematical-guarantee-3" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-guarantee-3">Mathematical Guarantee</h4>
<p>Let <span class="math inline">\(r^*\)</span> be the optimal radius. At every step, the algorithm picks a point at least <span class="math inline">\(r^*\)</span> away from existing centers, ensuring that the chosen centers are all at least <span class="math inline">\(r^*\)</span> apart. When <span class="math inline">\(k\)</span> centers are selected, every remaining point is within <span class="math inline">\(2r^*\)</span>.</p>
<p>So:</p>
<p><span class="math display">\[
r_{\text{greedy}} \le 2r^*
\]</span></p>
</section>
<section id="tiny-code-22" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-22">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_center(points, k):</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>    centers <span class="op">=</span> [points[<span class="dv">0</span>]]</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(centers) <span class="op">&lt;</span> k:</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>        dist <span class="op">=</span> [<span class="bu">min</span>(np.linalg.norm(p <span class="op">-</span> c) <span class="cf">for</span> c <span class="kw">in</span> centers) <span class="cf">for</span> p <span class="kw">in</span> points]</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>        next_center <span class="op">=</span> points[np.argmax(dist)]</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>        centers.append(next_center)</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(centers)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-74" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-74">Why It Matters</h4>
<ul>
<li>One of the simplest 2-approximation algorithms in combinatorial optimization</li>
<li>Scales linearly with input size, ideal for clustering large datasets</li>
<li>Intuitive and geometric, repeatedly covers the most distant area</li>
<li>Foundation for facility location, graph partitioning, and distributed clustering</li>
</ul>
</section>
<section id="try-it-yourself-73" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-73">Try It Yourself</h4>
<ol type="1">
<li>Generate random 2D points and run the algorithm for different <span class="math inline">\(k\)</span>.</li>
<li>Plot the centers, they should spread out uniformly.</li>
<li>Compare to K-means (minimizes average distance, not max distance).</li>
<li>Observe how adding one more center dramatically reduces the maximum radius.</li>
<li>Test on graph distances instead of Euclidean distances.</li>
</ol>
</section>
<section id="test-cases-73" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-73">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>k</th>
<th>Approx Radius</th>
<th>Optimal Radius</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Line [0–10]</td>
<td>2</td>
<td>5.5</td>
<td>5.0</td>
<td>1.1</td>
</tr>
<tr class="even">
<td>Grid 3×3</td>
<td>3</td>
<td>1.9</td>
<td>1.0</td>
<td>≤ 2</td>
</tr>
<tr class="odd">
<td>Random 50 pts</td>
<td>5</td>
<td>~2× optimal</td>
<td>consistent</td>
<td></td>
</tr>
<tr class="even">
<td>City graph</td>
<td>4</td>
<td>≤ 2×</td>
<td>holds for metric graphs</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-73" class="level4">
<h4 class="anchored" data-anchor-id="complexity-73">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(k |V|^2)\)</span> (can be optimized using distance caching)</li>
<li>Space: <span class="math inline">\(O(|V|)\)</span></li>
<li>Approximation ratio: ≤ 2</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-70" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-70">A Gentle Proof (Why It Works)</h4>
<p>Let <span class="math inline">\(C^*\)</span> be the optimal centers with radius <span class="math inline">\(r^*\)</span>. When the greedy algorithm chooses centers, every new center must lie in a distinct ball of radius <span class="math inline">\(r^*\)</span> around an optimal center. Thus, after <span class="math inline">\(k\)</span> choices, all optimal clusters are represented. Every remaining point is within at most two radii of a chosen center.</p>
<p><span class="math display">\[
r_{\text{greedy}} \le 2r^*
\]</span></p>
</section>
<section id="summary-table-2" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-2">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Algorithm Type</td>
<td>Greedy (farthest-point)</td>
</tr>
<tr class="even">
<td>Objective</td>
<td>Minimize max distance</td>
</tr>
<tr class="odd">
<td>Approximation Ratio</td>
<td>≤ 2</td>
</tr>
<tr class="even">
<td>Metric Required</td>
<td>Yes (triangle inequality)</td>
</tr>
<tr class="odd">
<td>Use Cases</td>
<td>Coverage, clustering, facility placement</td>
</tr>
</tbody>
</table>
<p>The farthest-point heuristic captures the essence of coverage problems, expand evenly, reach farthest first, and guarantee every point has a home within twice the best possible distance.</p>
</section>
</section>
<section id="online-paging-lru-least-recently-used" class="level3">
<h3 class="anchored" data-anchor-id="online-paging-lru-least-recently-used">975. Online Paging (LRU – Least Recently Used)</h3>
<p>The Online Paging Problem models how an operating system or cache manages a limited amount of fast memory (cache) while serving a sequence of requests. Since future requests are unknown, the algorithm must make decisions on the fly about which item to evict, a core example of an online algorithm.</p>
<section id="what-problem-are-we-solving-74" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-74">What Problem Are We Solving?</h4>
<p>We have:</p>
<ul>
<li>A cache that holds up to <span class="math inline">\(k\)</span> pages.</li>
<li>A sequence of page requests <span class="math inline">\(p_1, p_2, \dots, p_n\)</span>.</li>
<li>Each request costs 1 if the page is not in cache (a <em>miss</em>), and 0 if it is (a <em>hit</em>).</li>
</ul>
<p>When the cache is full and a new page is requested, we must evict one to make space, but we don’t know future requests.</p>
<p>Goal: Minimize the total number of cache misses.</p>
</section>
<section id="the-core-idea-lru-strategy" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-lru-strategy">The Core Idea (LRU Strategy)</h4>
<p>Least Recently Used (LRU) evicts the page that hasn’t been accessed for the longest time. It relies on the temporal locality assumption: pages used recently are likely to be used again soon.</p>
<p>LRU keeps a record of access order and always removes the oldest entry.</p>
</section>
<section id="example-45" class="level4">
<h4 class="anchored" data-anchor-id="example-45">Example</h4>
<p>Cache size <span class="math inline">\(k = 3\)</span> Request sequence: <code>A, B, C, A, D, B, A, E</code></p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 12%">
<col style="width: 20%">
<col style="width: 27%">
<col style="width: 18%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Request</th>
<th>Cache before</th>
<th>Action</th>
<th>Cache after</th>
<th>Miss/Hit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>A</td>
<td>{}</td>
<td>Load A</td>
<td>{A}</td>
<td>Miss</td>
</tr>
<tr class="even">
<td>2</td>
<td>B</td>
<td>{A}</td>
<td>Load B</td>
<td>{A,B}</td>
<td>Miss</td>
</tr>
<tr class="odd">
<td>3</td>
<td>C</td>
<td>{A,B}</td>
<td>Load C</td>
<td>{A,B,C}</td>
<td>Miss</td>
</tr>
<tr class="even">
<td>4</td>
<td>A</td>
<td>{A,B,C}</td>
<td>Hit</td>
<td>{A,B,C}</td>
<td>Hit</td>
</tr>
<tr class="odd">
<td>5</td>
<td>D</td>
<td>{A,B,C}</td>
<td>Evict B (oldest)</td>
<td>{A,C,D}</td>
<td>Miss</td>
</tr>
<tr class="even">
<td>6</td>
<td>B</td>
<td>{A,C,D}</td>
<td>Evict C</td>
<td>{A,D,B}</td>
<td>Miss</td>
</tr>
<tr class="odd">
<td>7</td>
<td>A</td>
<td>{A,D,B}</td>
<td>Hit</td>
<td>{A,D,B}</td>
<td>Hit</td>
</tr>
<tr class="even">
<td>8</td>
<td>E</td>
<td>{A,D,B}</td>
<td>Evict D</td>
<td>{A,B,E}</td>
<td>Miss</td>
</tr>
</tbody>
</table>
<p>Total: 8 requests, 6 misses, 2 hits.</p>
</section>
<section id="algorithm-pseudocode-4" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode-4">Algorithm (Pseudocode)</h4>
<pre><code>LRU(CacheSize, Requests):
    Cache = empty list
    for page in Requests:
        if page in Cache:
            move page to front (most recent)
        else:
            if len(Cache) == CacheSize:
                remove last element (least recent)
            insert page at front</code></pre>
</section>
<section id="mathematical-guarantee-4" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-guarantee-4">Mathematical Guarantee</h4>
<p>The competitive ratio of an online algorithm compares its performance to the optimal offline algorithm (which knows the future).</p>
<p>For LRU:</p>
<p><span class="math display">\[
\text{Competitive ratio} = k
\]</span></p>
<p>Meaning: LRU’s total cost is at most <span class="math inline">\(k\)</span> times the cost of the optimal offline algorithm (denoted OPT).</p>
</section>
<section id="tiny-code-23" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-23">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lru(cache_size, requests):</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>    cache <span class="op">=</span> deque()</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>    hits <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> page <span class="kw">in</span> requests:</span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> page <span class="kw">in</span> cache:</span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>            cache.remove(page)</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a>            cache.appendleft(page)</span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a>            hits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(cache) <span class="op">==</span> cache_size:</span>
<span id="cb134-13"><a href="#cb134-13" aria-hidden="true" tabindex="-1"></a>                cache.pop()</span>
<span id="cb134-14"><a href="#cb134-14" aria-hidden="true" tabindex="-1"></a>            cache.appendleft(page)</span>
<span id="cb134-15"><a href="#cb134-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hits, <span class="bu">len</span>(requests) <span class="op">-</span> hits</span>
<span id="cb134-16"><a href="#cb134-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-17"><a href="#cb134-17" aria-hidden="true" tabindex="-1"></a>reqs <span class="op">=</span> [<span class="st">'A'</span>,<span class="st">'B'</span>,<span class="st">'C'</span>,<span class="st">'A'</span>,<span class="st">'D'</span>,<span class="st">'B'</span>,<span class="st">'A'</span>,<span class="st">'E'</span>]</span>
<span id="cb134-18"><a href="#cb134-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lru(<span class="dv">3</span>, reqs))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-75" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-75">Why It Matters</h4>
<ul>
<li>LRU is used in CPU caches, OS memory management, web caching, and databases.</li>
<li>Demonstrates online algorithm design, decision-making without future knowledge.</li>
<li>Key for studying competitive analysis and worst-case guarantees.</li>
</ul>
</section>
<section id="try-it-yourself-74" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-74">Try It Yourself</h4>
<ol type="1">
<li>Compare LRU, FIFO, and Random Replacement.</li>
<li>Simulate sequences with repeated vs.&nbsp;random access patterns.</li>
<li>Increase cache size <span class="math inline">\(k\)</span>, observe diminishing misses.</li>
<li>Measure competitive ratio experimentally.</li>
</ol>
</section>
<section id="test-cases-74" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-74">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Sequence</th>
<th>Cache Size</th>
<th>Algorithm</th>
<th>Misses</th>
<th>Hits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A,B,C,A,B,C</td>
<td>2</td>
<td>LRU</td>
<td>4</td>
<td>2</td>
</tr>
<tr class="even">
<td>A,B,C,A,D,B</td>
<td>3</td>
<td>LRU</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="odd">
<td>Random</td>
<td>4</td>
<td>LRU</td>
<td>≈ k×OPT</td>
<td>,</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-74" class="level4">
<h4 class="anchored" data-anchor-id="complexity-74">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(nk)\)</span> naive, or <span class="math inline">\(O(n)\)</span> with hash map + linked list</li>
<li>Space: <span class="math inline">\(O(k)\)</span></li>
<li>Competitive ratio: ≤ <span class="math inline">\(k\)</span></li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-71" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-71">A Gentle Proof (Why It Works)</h4>
<p>In any sequence, consider the last <span class="math inline">\(k\)</span> distinct pages accessed before a miss, those must all be in the optimal cache as well. Since LRU only evicts the least recently used page, it can be at most <span class="math inline">\(k\)</span> times worse than OPT.</p>
<p>Formally: <span class="math display">\[
\text{Cost(LRU)} \le k \cdot \text{Cost(OPT)}
\]</span></p>
</section>
<section id="summary-table-3" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-3">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Type</td>
<td>Online, deterministic</td>
</tr>
<tr class="even">
<td>Competitive Ratio</td>
<td>≤ k</td>
</tr>
<tr class="odd">
<td>Cache Policy</td>
<td>Least Recently Used</td>
</tr>
<tr class="even">
<td>Works Well On</td>
<td>Locality-based sequences</td>
</tr>
<tr class="odd">
<td>Used In</td>
<td>OS, CPU cache, databases</td>
</tr>
</tbody>
</table>
<p>LRU is the timeless balance between intuition and theory, an algorithm that forgets just enough of the past to keep up with an unpredictable future.</p>
</section>
</section>
<section id="online-matching-ranking-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="online-matching-ranking-algorithm">976. Online Matching (Ranking Algorithm)</h3>
<p>The Online Bipartite Matching Problem models situations where one side of a bipartite graph (say, users) arrives one by one, and we must decide immediately which resource (say, server or ad slot) to match them with, without knowing future arrivals. The Ranking Algorithm achieves a competitive ratio of <span class="math inline">\((1 - 1/e)\)</span>, the best possible for randomized algorithms in this setting.</p>
<section id="what-problem-are-we-solving-75" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-75">What Problem Are We Solving?</h4>
<p>We are given a bipartite graph <span class="math display">\[
G = (U, V, E)
\]</span> where <span class="math inline">\(U\)</span> (offline vertices) are known in advance, and <span class="math inline">\(V\)</span> (online vertices) arrive one by one.</p>
<p>When each vertex <span class="math inline">\(v \in V\)</span> arrives:</p>
<ul>
<li>We see its incident edges <span class="math inline">\((u,v)\)</span> for all <span class="math inline">\(u \in U\)</span>.</li>
<li>We must immediately and irrevocably choose whether to match <span class="math inline">\(v\)</span> with an unmatched <span class="math inline">\(u\)</span>.</li>
</ul>
<p>Goal: Maximize the total number of matches made by the end.</p>
</section>
<section id="the-core-idea-ranking-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-ranking-algorithm">The Core Idea (Ranking Algorithm)</h4>
<p>The Ranking Algorithm is a randomized strategy introduced by Karp, Vazirani, and Vazirani (1990). It assigns a random ranking to the offline vertices once at the start and uses it to break ties consistently.</p>
<p>Algorithm outline:</p>
<ol type="1">
<li><p>Randomly permute the offline vertices <span class="math inline">\(U\)</span>.</p></li>
<li><p>For each arriving vertex <span class="math inline">\(v \in V\)</span>:</p>
<ul>
<li>Among all currently available neighbors, match <span class="math inline">\(v\)</span> to the highest-ranked (lowest-numbered) one.</li>
</ul></li>
</ol>
<p>This simple strategy achieves a <span class="math inline">\((1 - 1/e) ≈ 0.632\)</span> competitive ratio, provably optimal for this problem.</p>
</section>
<section id="example-46" class="level4">
<h4 class="anchored" data-anchor-id="example-46">Example</h4>
<p>Let <span class="math inline">\(U = {A, B, C}\)</span> and <span class="math inline">\(V = {1, 2, 3}\)</span> arrive in order.</p>
<p>Edges:</p>
<ul>
<li>1 connects to {A, B}</li>
<li>2 connects to {B, C}</li>
<li>3 connects to {A, C}</li>
</ul>
<p>Random ranking of <span class="math inline">\(U\)</span>: A=1, B=2, C=3</p>
<p>Step-by-step:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Arrival</th>
<th>Neighbors</th>
<th>Free?</th>
<th>Picked Match</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>{A, B}</td>
<td>all free</td>
<td>A (highest rank)</td>
</tr>
<tr class="even">
<td>2</td>
<td>{B, C}</td>
<td>all free</td>
<td>B</td>
</tr>
<tr class="odd">
<td>3</td>
<td>{A, C}</td>
<td>A used, C free</td>
<td>C</td>
</tr>
</tbody>
</table>
<p>Final matching = {(1,A), (2,B), (3,C)} (perfect match).</p>
</section>
<section id="algorithm-pseudocode-5" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode-5">Algorithm (Pseudocode)</h4>
<pre><code>RankingMatching(U, V, E):
    assign random rank π(u) to each u in U
    for each arriving v in V:
        N = {u ∈ U | (u,v) ∈ E and u unmatched}
        if N ≠ ∅:
            pick u in N with smallest π(u)
            match (u,v)</code></pre>
</section>
<section id="mathematical-guarantee-5" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-guarantee-5">Mathematical Guarantee</h4>
<p>Let OPT denote the number of matches the optimal offline algorithm can make (knowing the full sequence).</p>
<p>Then:</p>
<p><span class="math display">\[
E[\text{matches(Ranking)}] \ge (1 - 1/e) \cdot \text{OPT}
\]</span></p>
<p>This bound is tight for all randomized online algorithms under adversarial arrival.</p>
</section>
<section id="tiny-code-24" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-24">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> online_matching_ranking(U, edges, arrivals):</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>    rank <span class="op">=</span> {u: i <span class="cf">for</span> i, u <span class="kw">in</span> <span class="bu">enumerate</span>(random.sample(U, <span class="bu">len</span>(U)))}</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>    matched <span class="op">=</span> {}</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> arrivals:</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>        candidates <span class="op">=</span> [u <span class="cf">for</span> u <span class="kw">in</span> U <span class="cf">if</span> (u, v) <span class="kw">in</span> edges <span class="kw">and</span> u <span class="kw">not</span> <span class="kw">in</span> matched.values()]</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> candidates:</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>            chosen <span class="op">=</span> <span class="bu">min</span>(candidates, key<span class="op">=</span><span class="kw">lambda</span> u: rank[u])</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>            matched[v] <span class="op">=</span> chosen</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> matched</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-76" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-76">Why It Matters</h4>
<ul>
<li>Fundamental in online advertising, task assignment, resource allocation.</li>
<li>Balances randomization and greediness to achieve provable guarantees.</li>
<li>Foundational model for competitive analysis and online learning algorithms.</li>
</ul>
</section>
<section id="try-it-yourself-75" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-75">Try It Yourself</h4>
<ol type="1">
<li>Construct a bipartite graph where multiple matches are possible.</li>
<li>Simulate arrivals in different orders.</li>
<li>Compare Ranking vs.&nbsp;Greedy (match to any available).</li>
<li>Measure average matches over random rankings.</li>
<li>Observe that Ranking consistently beats Greedy in adversarial sequences.</li>
</ol>
</section>
<section id="test-cases-75" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-75">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Scenario</th>
<th>Algorithm</th>
<th>Matches</th>
<th>Competitive Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple 3×3 complete</td>
<td>Ranking</td>
<td>3</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>Adversarial order</td>
<td>Ranking</td>
<td>0.63×OPT</td>
<td>≈ (1 - 1/e)</td>
</tr>
<tr class="odd">
<td>Random graph</td>
<td>Ranking</td>
<td>≥ 0.63×OPT</td>
<td>consistent</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-75" class="level4">
<h4 class="anchored" data-anchor-id="complexity-75">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(|E|)\)</span></li>
<li>Space: <span class="math inline">\(O(|U| + |V|)\)</span></li>
<li>Competitive Ratio: <span class="math inline">\((1 - 1/e)\)</span></li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-72" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-72">A Gentle Proof (Why It Works)</h4>
<p>Random ranking gives each offline vertex an equal chance to be available when its best possible online partner arrives. Using probabilistic analysis, the expected fraction of matched vertices satisfies:</p>
<p><span class="math display">\[
\frac{dM}{dx} = 1 - M, \quad \Rightarrow \quad M = 1 - e^{-x}
\]</span></p>
<p>At full capacity (<span class="math inline">\(x=1\)</span>), <span class="math inline">\(M = 1 - 1/e\)</span>. Thus, expected performance is <span class="math inline">\((1 - 1/e)\)</span> times optimal.</p>
</section>
<section id="summary-table-4" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-4">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Type</td>
<td>Online randomized</td>
</tr>
<tr class="even">
<td>Competitive Ratio</td>
<td><span class="math inline">\((1 - 1/e)\)</span></td>
</tr>
<tr class="odd">
<td>Deterministic Version</td>
<td>Greedy (worse: 0.5×OPT)</td>
</tr>
<tr class="even">
<td>Used In</td>
<td>Online ads, resource allocation</td>
</tr>
<tr class="odd">
<td>Key Idea</td>
<td>Pre-rank offline side once</td>
</tr>
</tbody>
</table>
<p>The Ranking Algorithm is a gem of online optimization, simple randomness at the start yields deep robustness against any future.</p>
</section>
</section>
<section id="online-knapsack-ratio-based-acceptance" class="level3">
<h3 class="anchored" data-anchor-id="online-knapsack-ratio-based-acceptance">977. Online Knapsack (Ratio-Based Acceptance)</h3>
<p>The Online Knapsack Problem models real-time decision-making under capacity constraints: items arrive one by one, each with a value and weight, and you must decide immediately whether to accept it, without knowing what comes next. This problem captures ad auctions, job scheduling, and real-time resource allocation under uncertainty.</p>
<section id="what-problem-are-we-solving-76" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-76">What Problem Are We Solving?</h4>
<p>We are given:</p>
<ul>
<li>A knapsack of capacity <span class="math inline">\(W\)</span></li>
<li>A sequence of items <span class="math inline">\((v_i, w_i)\)</span> arriving one at a time where <span class="math inline">\(v_i\)</span> is the value, and <span class="math inline">\(w_i\)</span> is the weight.</li>
</ul>
<p>For each item:</p>
<ul>
<li>If accepted, it consumes <span class="math inline">\(w_i\)</span> capacity.</li>
<li>The decision is irrevocable, we cannot remove items later.</li>
</ul>
<p>Goal: Maximize the total value of accepted items while keeping total weight <span class="math inline">\(\le W\)</span>.</p>
<p>This is the online version of the classical 0/1 knapsack problem.</p>
</section>
<section id="the-core-idea-greedy-ratio-threshold" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-greedy-ratio-threshold">The Core Idea (Greedy Ratio Threshold)</h4>
<p>Without future knowledge, we can’t plan perfectly. Instead, we use a threshold strategy based on the value-to-weight ratio:</p>
<p><span class="math display">\[
\rho_i = \frac{v_i}{w_i}
\]</span></p>
<p>Only accept items whose ratio <span class="math inline">\(\rho_i\)</span> is above a dynamic threshold, which is gradually lowered as the knapsack fills.</p>
</section>
<section id="algorithm-intuition" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-intuition">Algorithm Intuition</h4>
<p>Think of the knapsack as being filled gradually. At first, we are picky, we only take high-value items. As capacity decreases, we lower the threshold and accept lower-value ones.</p>
<p>If we define <span class="math inline">\(x\)</span> as the fraction of capacity filled, then a common threshold rule is:</p>
<p><span class="math display">\[
\rho(x) = \rho_{\max} e^{x-1}
\]</span></p>
<p>An arriving item with <span class="math inline">\(\rho_i \ge \rho(x)\)</span> is accepted.</p>
<p>This ensures we use capacity smoothly while retaining near-optimal value.</p>
</section>
<section id="example-47" class="level4">
<h4 class="anchored" data-anchor-id="example-47">Example</h4>
<p>Capacity <span class="math inline">\(W = 10\)</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 17%">
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Item</th>
<th>Value (<span class="math inline">\(v_i\)</span>)</th>
<th>Weight (<span class="math inline">\(w_i\)</span>)</th>
<th>Ratio (<span class="math inline">\(v_i/w_i\)</span>)</th>
<th>Decision</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>20</td>
<td>4</td>
<td>5.0</td>
<td>accept</td>
</tr>
<tr class="even">
<td>2</td>
<td>10</td>
<td>4</td>
<td>2.5</td>
<td>reject (too low)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>15</td>
<td>3</td>
<td>5.0</td>
<td>accept</td>
</tr>
<tr class="even">
<td>4</td>
<td>8</td>
<td>4</td>
<td>2.0</td>
<td>reject</td>
</tr>
<tr class="odd">
<td>5</td>
<td>12</td>
<td>3</td>
<td>4.0</td>
<td>accept (enough space left)</td>
</tr>
</tbody>
</table>
<p>Total value = 20 + 15 + 12 = 47 Total weight = 4 + 3 + 3 = 10</p>
</section>
<section id="algorithm-pseudocode-6" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-pseudocode-6">Algorithm (Pseudocode)</h4>
<pre><code>OnlineKnapsack(W, items):
    used = 0
    accepted = []
    for (v, w) in items:
        rho = v / w
        x = used / W
        threshold = rho_max * exp(x - 1)
        if used + w &lt;= W and rho &gt;= threshold:
            accepted.append((v, w))
            used += w
    return accepted</code></pre>
</section>
<section id="competitive-ratio" class="level4">
<h4 class="anchored" data-anchor-id="competitive-ratio">Competitive Ratio</h4>
<p>Let OPT be the value obtained by the optimal offline algorithm.</p>
<p>For continuous arrivals and differentiable threshold function, the exponential threshold rule achieves a competitive ratio of <span class="math inline">\((1 - 1/e)\)</span>, the same as the online matching problem.</p>
<p>Formally:</p>
<p><span class="math display">\[
\frac{E[\text{ALG}]}{\text{OPT}} \ge 1 - \frac{1}{e} \approx 0.632
\]</span></p>
</section>
<section id="tiny-code-25" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-25">Tiny Code</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> online_knapsack(items, W, rho_max):</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>    used, value <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v, w <span class="kw">in</span> items:</span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>        rho <span class="op">=</span> v <span class="op">/</span> w</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> used <span class="op">/</span> W</span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> rho_max <span class="op">*</span> math.exp(x <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> used <span class="op">+</span> w <span class="op">&lt;=</span> W <span class="kw">and</span> rho <span class="op">&gt;=</span> threshold:</span>
<span id="cb138-10"><a href="#cb138-10" aria-hidden="true" tabindex="-1"></a>            used <span class="op">+=</span> w</span>
<span id="cb138-11"><a href="#cb138-11" aria-hidden="true" tabindex="-1"></a>            value <span class="op">+=</span> v</span>
<span id="cb138-12"><a href="#cb138-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> value</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-77" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-77">Why It Matters</h4>
<ul>
<li>Real-world systems (ads, cloud jobs, CPU time) must make allocation decisions immediately.</li>
<li>Demonstrates the power of exponential thresholds in online optimization.</li>
<li>Connects deeply to competitive analysis, prophet inequalities, and mechanism design.</li>
</ul>
</section>
<section id="try-it-yourself-76" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-76">Try It Yourself</h4>
<ol type="1">
<li>Simulate 100 random items with weights and values.</li>
<li>Compare your online rule to the offline optimum (sorted by ratio).</li>
<li>Adjust the decay function <span class="math inline">\(\rho(x)\)</span> to see its impact.</li>
<li>Observe that exponential decay achieves consistently near-optimal results.</li>
</ol>
</section>
<section id="test-cases-76" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-76">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Items</th>
<th>W</th>
<th>Algorithm</th>
<th>Competitive Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random uniform</td>
<td>10</td>
<td>Exponential threshold</td>
<td>≈ 0.63×OPT</td>
</tr>
<tr class="even">
<td>Ad stream</td>
<td>100</td>
<td>Ratio-based</td>
<td>0.6–0.65</td>
</tr>
<tr class="odd">
<td>Constant values</td>
<td>20</td>
<td>Any policy</td>
<td>≈ OPT</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-76" class="level4">
<h4 class="anchored" data-anchor-id="complexity-76">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(n)\)</span> (single pass)</li>
<li>Space: <span class="math inline">\(O(1)\)</span> (only store current fill ratio)</li>
<li>Competitive Ratio: <span class="math inline">\((1 - 1/e)\)</span></li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-73" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-73">A Gentle Proof (Why It Works)</h4>
<p>Let <span class="math inline">\(f(x)\)</span> denote the total expected value at fill fraction <span class="math inline">\(x\)</span>. Differentiating with respect to <span class="math inline">\(x\)</span> under the exponential threshold rule:</p>
<p><span class="math display">\[
\frac{df}{dx} = \rho_{\max} e^{x-1}
\]</span></p>
<p>Integrating over <span class="math inline">\(x \in [0,1]\)</span> gives:</p>
<p><span class="math display">\[
f(1) = \rho_{\max} (1 - 1/e)
\]</span></p>
<p>Thus, the total expected value reaches <span class="math inline">\((1 - 1/e)\)</span> of the best possible.</p>
</section>
<section id="summary-table-5" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-5">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Type</td>
<td>Online, ratio-based</td>
</tr>
<tr class="even">
<td>Competitive Ratio</td>
<td><span class="math inline">\((1 - 1/e)\)</span></td>
</tr>
<tr class="odd">
<td>Decision Rule</td>
<td>Threshold on <span class="math inline">\(v_i/w_i\)</span></td>
</tr>
<tr class="even">
<td>Used In</td>
<td>Ad auctions, scheduling, dynamic pricing</td>
</tr>
<tr class="odd">
<td>Optimality</td>
<td>Tight for adversarial input</td>
</tr>
</tbody>
</table>
<p>The Online Knapsack problem embodies the art of tradeoff, balancing greed and patience, maximizing value before the future unfolds.</p>
</section>
</section>
<section id="competitive-ratio-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="competitive-ratio-evaluation">978. Competitive Ratio Evaluation</h3>
<p>The Competitive Ratio is the central tool used to measure the performance of online algorithms, algorithms that must make decisions <em>without knowing the future</em>. It provides a formal way to compare an online algorithm’s result to that of the optimal offline algorithm (which knows the entire input sequence in advance).</p>
<section id="what-problem-are-we-solving-77" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-77">What Problem Are We Solving?</h4>
<p>In online problems, data arrives sequentially, and the algorithm must react instantly.</p>
<p>Examples include:</p>
<ul>
<li>Online paging (LRU): evicting pages without future access info</li>
<li>Online matching (Ranking): assigning ads to users in real time</li>
<li>Online knapsack: deciding whether to accept new items</li>
<li>Online scheduling: allocating machines to jobs as they arrive</li>
</ul>
<p>Since no future knowledge exists, we cannot talk about “optimality” in the traditional sense. Instead, we measure <em>how close</em> the online algorithm can get to the offline optimum.</p>
</section>
<section id="definition" class="level4">
<h4 class="anchored" data-anchor-id="definition">Definition</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(\text{ALG}(I)\)</span> = cost (or profit) of the online algorithm on input <span class="math inline">\(I\)</span></li>
<li><span class="math inline">\(\text{OPT}(I)\)</span> = cost (or profit) of the optimal offline algorithm (knows the future)</li>
</ul>
<p>Then the competitive ratio <span class="math inline">\(c\)</span> is defined as:</p>
<p><span class="math display">\[
c = \sup_I \frac{\text{ALG}(I)}{\text{OPT}(I)}
\]</span></p>
<p>for minimization problems, or equivalently</p>
<p><span class="math display">\[
c = \inf_I \frac{\text{ALG}(I)}{\text{OPT}(I)}
\]</span></p>
<p>for maximization problems.</p>
</section>
<section id="intuitive-meaning" class="level4">
<h4 class="anchored" data-anchor-id="intuitive-meaning">Intuitive Meaning</h4>
<ul>
<li>If <span class="math inline">\(c = 1\)</span>, the online algorithm is <em>as good as</em> the offline one, perfectly optimal.</li>
<li>If <span class="math inline">\(c = 2\)</span>, it performs at worst twice as bad.</li>
<li>If <span class="math inline">\(c = (1 - 1/e)\)</span>, it achieves about 63% of the offline optimum, common in randomized algorithms like online matching or knapsack.</li>
</ul>
<p>Thus, the closer <span class="math inline">\(c\)</span> is to 1, the better the algorithm.</p>
</section>
<section id="example-1-paging-lru" class="level4">
<h4 class="anchored" data-anchor-id="example-1-paging-lru">Example 1 – Paging (LRU)</h4>
<p>For a cache of size <span class="math inline">\(k\)</span>: <span class="math display">\[
\text{Cost(LRU)} \le k \cdot \text{Cost(OPT)}
\]</span></p>
<p>So the competitive ratio is <span class="math inline">\(k\)</span>.</p>
<p>LRU can perform <span class="math inline">\(k\)</span> times worse than the optimal offline cache, but no deterministic algorithm can do better.</p>
</section>
<section id="example-2-online-matching-ranking" class="level4">
<h4 class="anchored" data-anchor-id="example-2-online-matching-ranking">Example 2 – Online Matching (Ranking)</h4>
<p><span class="math display">\[
E[\text{ALG}] \ge (1 - 1/e) \cdot \text{OPT}
\]</span></p>
<p>So the competitive ratio is <span class="math inline">\((1 - 1/e) \approx 0.632\)</span>.</p>
<p>This is provably optimal for randomized online matching algorithms.</p>
</section>
<section id="example-3-ski-rental-problem" class="level4">
<h4 class="anchored" data-anchor-id="example-3-ski-rental-problem">Example 3 – Ski Rental Problem</h4>
<p>You can rent skis each day for cost <span class="math inline">\(r\)</span>, or buy them once for cost <span class="math inline">\(b\)</span>.</p>
<p>Optimal strategy:</p>
<ul>
<li>Rent until total rental cost equals <span class="math inline">\(b\)</span>, then buy.</li>
</ul>
<p>Competitive ratio:</p>
<p><span class="math display">\[
c = \frac{2b - r}{b} \approx 2 - \frac{r}{b}
\]</span></p>
<p>This gives a 2-competitive deterministic bound.</p>
</section>
<section id="algorithm-evaluation-framework" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-evaluation-framework">Algorithm (Evaluation Framework)</h4>
<p>To evaluate an algorithm’s competitive ratio:</p>
<ol type="1">
<li>Define the input space <span class="math inline">\(\mathcal{I}\)</span> (all possible request sequences).</li>
<li>Define <span class="math inline">\(\text{ALG}(I)\)</span> and <span class="math inline">\(\text{OPT}(I)\)</span> for each <span class="math inline">\(I\)</span>.</li>
<li>Compute the worst-case ratio across all inputs: <span class="math display">\[
c = \max_I \frac{\text{ALG}(I)}{\text{OPT}(I)}
\]</span></li>
<li>Optionally, compute expected competitive ratio for randomized algorithms.</li>
</ol>
</section>
<section id="tiny-code-simulation-example" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simulation-example">Tiny Code (Simulation Example)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> competitive_ratio(alg_func, opt_func, inputs):</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>    worst_ratio <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> I <span class="kw">in</span> inputs:</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a>        alg <span class="op">=</span> alg_func(I)</span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a>        opt <span class="op">=</span> opt_func(I)</span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>        ratio <span class="op">=</span> alg <span class="op">/</span> opt <span class="cf">if</span> opt <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a>        worst_ratio <span class="op">=</span> <span class="bu">max</span>(worst_ratio, ratio)</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> worst_ratio</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This framework can simulate multiple inputs to find the empirical ratio.</p>
</section>
<section id="why-it-matters-78" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-78">Why It Matters</h4>
<ul>
<li>The competitive ratio allows theoretical guarantees for <em>uncertain environments</em>.</li>
<li>Helps compare algorithms fairly without assuming probabilistic inputs.</li>
<li>Bridges theory with systems: LRU, scheduling, caching, and dynamic pricing all rely on it.</li>
</ul>
</section>
<section id="try-it-yourself-77" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-77">Try It Yourself</h4>
<ol type="1">
<li>Implement online paging, knapsack, or matching.</li>
<li>Simulate adversarial inputs to find the worst case.</li>
<li>Compute the ratio <span class="math inline">\(\text{ALG}/\text{OPT}\)</span>.</li>
<li>See how randomization improves the average ratio.</li>
</ol>
</section>
<section id="test-cases-77" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-77">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Algorithm</th>
<th>Competitive Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Paging</td>
<td>LRU</td>
<td><span class="math inline">\(k\)</span></td>
</tr>
<tr class="even">
<td>Online Matching</td>
<td>Ranking</td>
<td><span class="math inline">\(1 - 1/e\)</span></td>
</tr>
<tr class="odd">
<td>Ski Rental</td>
<td>Rent-then-buy</td>
<td><span class="math inline">\(2\)</span></td>
</tr>
<tr class="even">
<td>Online Knapsack</td>
<td>Ratio threshold</td>
<td><span class="math inline">\(1 - 1/e\)</span></td>
</tr>
<tr class="odd">
<td>Load Balancing</td>
<td>Greedy</td>
<td><span class="math inline">\(2 - 1/m\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-77" class="level4">
<h4 class="anchored" data-anchor-id="complexity-77">Complexity</h4>
<p>Evaluating the ratio often requires:</p>
<ul>
<li>Offline optimum via dynamic programming or linear programming</li>
<li>Online run via simulation</li>
<li>Worst-case search across generated input sequences</li>
</ul>
<p>Time complexity depends on the offline algorithm (often <span class="math inline">\(O(n^2)\)</span> or worse).</p>
</section>
<section id="a-gentle-proof-the-big-idea" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-the-big-idea">A Gentle Proof (The Big Idea)</h4>
<p>For any online algorithm <span class="math inline">\(\text{ALG}\)</span>, the ratio bounds its <em>relative inefficiency</em>:</p>
<p><span class="math display">\[
\forall I, \quad \text{ALG}(I) \le c \cdot \text{OPT}(I)
\]</span></p>
<p>If <span class="math inline">\(c\)</span> is small, <span class="math inline">\(\text{ALG}\)</span> is nearly optimal even against an adversary. For randomized algorithms, the guarantee is in <em>expectation</em>:</p>
<p><span class="math display">\[
E[\text{ALG}(I)] \le c \cdot \text{OPT}(I)
\]</span></p>
</section>
<section id="summary-table-6" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-6">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Metric</td>
<td><span class="math inline">\(\text{ALG}/\text{OPT}\)</span> ratio</td>
</tr>
<tr class="even">
<td>Purpose</td>
<td>Quantify online inefficiency</td>
</tr>
<tr class="odd">
<td>Deterministic Bound</td>
<td>Worst-case ratio</td>
</tr>
<tr class="even">
<td>Randomized Bound</td>
<td>Expected ratio</td>
</tr>
<tr class="odd">
<td>Ideal Value</td>
<td>1 (perfect)</td>
</tr>
<tr class="even">
<td>Common Values</td>
<td>2, <span class="math inline">\(1 - 1/e\)</span>, <span class="math inline">\(k\)</span></td>
</tr>
</tbody>
</table>
<p>The competitive ratio is the philosopher’s compass of online algorithms, it doesn’t promise certainty, but measures <em>how gracefully</em> we face the unknown.</p>
</section>
</section>
<section id="ptas-and-fptas-schemes-polynomial-time-approximation" class="level3">
<h3 class="anchored" data-anchor-id="ptas-and-fptas-schemes-polynomial-time-approximation">979. PTAS and FPTAS Schemes (Polynomial-Time Approximation)</h3>
<p>Not all optimization problems can be solved efficiently. Some are NP-hard, meaning no known algorithm can find an exact solution in polynomial time. But in many real-world scenarios, we don’t need the perfect answer, just a <em>good enough</em> one. That’s where approximation schemes come in: algorithms that get arbitrarily close to the optimal solution, within controllable precision.</p>
<section id="what-problem-are-we-solving-78" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-78">What Problem Are We Solving?</h4>
<p>Given an optimization problem (usually NP-hard), we want an algorithm that produces a solution whose value is within a small fraction <span class="math inline">\(\varepsilon\)</span> of the optimal one.</p>
<p>For a minimization problem, the goal is:</p>
<p><span class="math display">\[
\frac{\text{ALG}}{\text{OPT}} \le 1 + \varepsilon
\]</span></p>
<p>For a maximization problem, the goal is:</p>
<p><span class="math display">\[
\frac{\text{ALG}}{\text{OPT}} \ge 1 - \varepsilon
\]</span></p>
<p>Here, <span class="math inline">\(\varepsilon &gt; 0\)</span> is a user-chosen error tolerance.</p>
</section>
<section id="the-big-picture" class="level4">
<h4 class="anchored" data-anchor-id="the-big-picture">The Big Picture</h4>
<p>There are two main types:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 53%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Definition</th>
<th>Running Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PTAS (Polynomial-Time Approximation Scheme)</td>
<td>For any <span class="math inline">\(\varepsilon &gt; 0\)</span>, runs in time polynomial in <span class="math inline">\(n\)</span>, but possibly exponential in <span class="math inline">\(1/\varepsilon\)</span>.</td>
<td><span class="math inline">\(O(n^{f(1/\varepsilon)})\)</span></td>
</tr>
<tr class="even">
<td>FPTAS (Fully Polynomial-Time Approximation Scheme)</td>
<td>Runs in time polynomial in both <span class="math inline">\(n\)</span> and <span class="math inline">\(1/\varepsilon\)</span>.</td>
<td><span class="math inline">\(O(\text{poly}(n, 1/\varepsilon))\)</span></td>
</tr>
</tbody>
</table>
<p>So FPTAS ⊂ PTAS, it’s a stricter and more efficient subclass.</p>
</section>
<section id="example-1-knapsack-problem" class="level4">
<h4 class="anchored" data-anchor-id="example-1-knapsack-problem">Example 1 – Knapsack Problem</h4>
<p>For the classic 0/1 Knapsack Problem, we can use an FPTAS by scaling values.</p>
<p>Original problem:</p>
<p><span class="math display">\[
\max \sum_i v_i x_i \quad \text{s.t.} \quad \sum_i w_i x_i \le W, \quad x_i \in {0,1}
\]</span></p>
<p>We scale down item values by a factor <span class="math inline">\(K = \frac{\varepsilon \cdot V_{\max}}{n}\)</span> and round them:</p>
<p><span class="math display">\[
v_i' = \left\lfloor \frac{v_i}{K} \right\rfloor
\]</span></p>
<p>Then run the dynamic programming algorithm on these scaled values, which now have a smaller range. The result guarantees:</p>
<p><span class="math display">\[
(1 - \varepsilon) \cdot \text{OPT} \le \text{ALG} \le \text{OPT}
\]</span></p>
<p>with runtime:</p>
<p><span class="math display">\[
O\left( \frac{n^3}{\varepsilon} \right)
\]</span></p>
</section>
<section id="example-2-euclidean-traveling-salesman-problem-tsp" class="level4">
<h4 class="anchored" data-anchor-id="example-2-euclidean-traveling-salesman-problem-tsp">Example 2 – Euclidean Traveling Salesman Problem (TSP)</h4>
<p>For points in a plane (Euclidean metric), Arora (1996) developed a PTAS. By cleverly subdividing the plane into grids and limiting the paths crossing each boundary, it achieves:</p>
<p><span class="math display">\[
\text{ALG} \le (1 + \varepsilon) \cdot \text{OPT}
\]</span></p>
<p>with runtime:</p>
<p><span class="math display">\[
O(n (\log n)^{O(1/\varepsilon)})
\]</span></p>
<p>This is a landmark result, TSP is NP-hard, yet admits near-optimal solutions efficiently in geometry.</p>
</section>
<section id="example-3-scheduling-jobs-on-machines" class="level4">
<h4 class="anchored" data-anchor-id="example-3-scheduling-jobs-on-machines">Example 3 – Scheduling Jobs on Machines</h4>
<p>Problem: Assign <span class="math inline">\(n\)</span> jobs to <span class="math inline">\(m\)</span> identical machines to minimize the makespan (max load). A PTAS can be built by:</p>
<ol type="1">
<li>Scheduling the largest <span class="math inline">\(k = O(1/\varepsilon^2)\)</span> jobs optimally by enumeration.</li>
<li>Scheduling the rest greedily.</li>
</ol>
<p>This gives:</p>
<p><span class="math display">\[
\text{ALG} \le (1 + \varepsilon) \cdot \text{OPT}
\]</span></p>
<p>in polynomial time.</p>
</section>
<section id="algorithm-generic-scheme" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-generic-scheme">Algorithm (Generic Scheme)</h4>
<pre><code>PTAS(problem, ε):
    for each possible simplified configuration (bounded by f(1/ε)):
        solve reduced subproblem optimally
    return best found solution</code></pre>
<p>FPTAS uses a scaling trick to reduce numerical precision so dynamic programming remains polynomial in <span class="math inline">\(1/ε\)</span>.</p>
</section>
<section id="tiny-code-fptas-for-knapsack" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-fptas-for-knapsack">Tiny Code (FPTAS for Knapsack)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knapsack_fptas(values, weights, W, ε):</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(values)</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>    vmax <span class="op">=</span> <span class="bu">max</span>(values)</span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> ε <span class="op">*</span> vmax <span class="op">/</span> n</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>    scaled <span class="op">=</span> [<span class="bu">int</span>(v <span class="op">/</span> K) <span class="cf">for</span> v <span class="kw">in</span> values]</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a>    Vsum <span class="op">=</span> <span class="bu">sum</span>(scaled)</span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a>    dp <span class="op">=</span> [<span class="bu">float</span>(<span class="st">'inf'</span>)] <span class="op">*</span> (Vsum <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a>    dp[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb141-10"><a href="#cb141-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-11"><a href="#cb141-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb141-12"><a href="#cb141-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> v <span class="kw">in</span> <span class="bu">range</span>(Vsum, scaled[i] <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb141-13"><a href="#cb141-13" aria-hidden="true" tabindex="-1"></a>            dp[v] <span class="op">=</span> <span class="bu">min</span>(dp[v], dp[v <span class="op">-</span> scaled[i]] <span class="op">+</span> weights[i])</span>
<span id="cb141-14"><a href="#cb141-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-15"><a href="#cb141-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> <span class="bu">range</span>(Vsum, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb141-16"><a href="#cb141-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> dp[v] <span class="op">&lt;=</span> W:</span>
<span id="cb141-17"><a href="#cb141-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> v <span class="op">*</span> K</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-79" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-79">Why It Matters</h4>
<ul>
<li>PTAS and FPTAS let us solve <em>impossible</em> problems <em>practically</em>.</li>
<li>They offer a tradeoff between speed and accuracy.</li>
<li>Widely used in optimization, operations research, AI planning, and data science.</li>
<li>They provide the bridge between theory (NP-hardness) and practice (good solutions fast).</li>
</ul>
</section>
<section id="try-it-yourself-78" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-78">Try It Yourself</h4>
<ol type="1">
<li>Run the FPTAS on small knapsack instances.</li>
<li>Change <span class="math inline">\(\varepsilon\)</span> and observe how runtime and quality change.</li>
<li>Compare with the exact DP solution.</li>
<li>See how small <span class="math inline">\(\varepsilon\)</span> quickly increases computation.</li>
</ol>
</section>
<section id="test-cases-78" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-78">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Scheme</th>
<th>Guarantee</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Knapsack</td>
<td>Scaling DP</td>
<td><span class="math inline">\((1 - \varepsilon)\)</span></td>
<td>FPTAS</td>
</tr>
<tr class="even">
<td>Euclidean TSP</td>
<td>Arora (1996)</td>
<td><span class="math inline">\((1 + \varepsilon)\)</span></td>
<td>PTAS</td>
</tr>
<tr class="odd">
<td>Job Scheduling</td>
<td>Enumeration + Greedy</td>
<td><span class="math inline">\((1 + \varepsilon)\)</span></td>
<td>PTAS</td>
</tr>
<tr class="even">
<td>Bin Packing</td>
<td>Rounding + First-Fit</td>
<td><span class="math inline">\((1 + \varepsilon)\)</span></td>
<td>PTAS</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-78" class="level4">
<h4 class="anchored" data-anchor-id="complexity-78">Complexity</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 47%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Scheme</th>
<th>Time</th>
<th>Example Problem</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PTAS</td>
<td><span class="math inline">\(O(n^{f(1/\varepsilon)})\)</span></td>
<td>TSP</td>
</tr>
<tr class="even">
<td>FPTAS</td>
<td><span class="math inline">\(O(\text{poly}(n, 1/\varepsilon))\)</span></td>
<td>Knapsack</td>
</tr>
<tr class="odd">
<td>Constant Approximation</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td>Greedy Set Cover</td>
</tr>
</tbody>
</table>
</section>
<section id="a-gentle-proof-why-it-works-74" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-74">A Gentle Proof (Why It Works)</h4>
<p>Approximation schemes rely on rounding and bounding. Let <span class="math inline">\(\text{OPT}\)</span> be the optimal value, and <span class="math inline">\(\text{ALG}\)</span> the approximate one.</p>
<p>If each item’s value is rounded to within <span class="math inline">\((1 - \varepsilon)\)</span> of the true value:</p>
<p><span class="math display">\[
\text{ALG} \ge (1 - \varepsilon) \cdot \text{OPT}
\]</span></p>
<p>Since rounding errors compound linearly, not exponentially, precision scales gracefully with <span class="math inline">\(\varepsilon\)</span>, maintaining polynomial runtime.</p>
</section>
<section id="summary-table-7" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-7">Summary Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 48%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>PTAS</th>
<th>FPTAS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>Arbitrary <span class="math inline">\(\varepsilon\)</span></td>
<td>Arbitrary <span class="math inline">\(\varepsilon\)</span></td>
</tr>
<tr class="even">
<td>Time</td>
<td>Poly(<span class="math inline">\(n\)</span>), exp(<span class="math inline">\(1/\varepsilon\)</span>)</td>
<td>Poly(<span class="math inline">\(n, 1/\varepsilon\)</span>)</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td><span class="math inline">\((1 \pm \varepsilon)\)</span></td>
<td><span class="math inline">\((1 \pm \varepsilon)\)</span></td>
</tr>
<tr class="even">
<td>Example</td>
<td>Euclidean TSP</td>
<td>Knapsack</td>
</tr>
<tr class="odd">
<td>Use Case</td>
<td>Theoretical &amp; geometric</td>
<td>Practical numeric</td>
</tr>
</tbody>
</table>
<p>Approximation schemes remind us that perfect is often impractical, but almost perfect can be beautifully efficient.</p>
</section>
</section>
<section id="primaldual-method-approximate-combinatorial-optimization" class="level3">
<h3 class="anchored" data-anchor-id="primaldual-method-approximate-combinatorial-optimization">980. Primal–Dual Method (Approximate Combinatorial Optimization)</h3>
<p>The Primal–Dual Method is a powerful framework for designing approximation algorithms for NP-hard problems. Instead of solving optimization problems exactly, it builds both the primal and dual linear programs together, maintaining feasible or nearly feasible solutions, and stopping when a balance is reached between cost and coverage.</p>
<section id="what-problem-are-we-solving-79" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-79">What Problem Are We Solving?</h4>
<p>Many combinatorial optimization problems can be expressed as linear programs (LPs).</p>
<p>A primal LP might look like:</p>
<p><span class="math display">\[
\min c^T x \quad \text{s.t.} \quad A x \ge b, ; x \ge 0
\]</span></p>
<p>Its dual LP is:</p>
<p><span class="math display">\[
\max b^T y \quad \text{s.t.} \quad A^T y \le c, ; y \ge 0
\]</span></p>
<p>The Primal–Dual Method constructs solutions for both simultaneously, maintaining relationships between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> so that their costs remain close.</p>
<p>This gives approximation guarantees even when we can’t solve the LP exactly.</p>
</section>
<section id="intuitive-idea" class="level4">
<h4 class="anchored" data-anchor-id="intuitive-idea">Intuitive Idea</h4>
<p>Think of the primal and dual as two players:</p>
<ul>
<li>The primal player selects elements (edges, sets, facilities) to satisfy constraints.</li>
<li>The dual player raises prices or penalties for unsatisfied constraints.</li>
</ul>
<p>The algorithm iteratively raises dual variables until some constraint becomes “tight,” then adds the corresponding primal variable (e.g., selects an edge, opens a facility). This process repeats until all constraints are satisfied.</p>
<p>The ratio between total primal cost and total dual value gives the approximation factor.</p>
</section>
<section id="example-vertex-cover-problem" class="level4">
<h4 class="anchored" data-anchor-id="example-vertex-cover-problem">Example – Vertex Cover Problem</h4>
<p>Goal: Choose the smallest set of vertices covering all edges in a graph <span class="math inline">\(G = (V, E)\)</span>.</p>
<p>Primal (minimization):</p>
<p><span class="math display">\[
\begin{aligned}
\text{minimize} \quad &amp; \sum_{v \in V} x_v \
\text{subject to} \quad &amp; x_u + x_v \ge 1 \quad \forall (u,v) \in E \
&amp; x_v \ge 0
\end{aligned}
\]</span></p>
<p>Dual (maximization):</p>
<p><span class="math display">\[
\begin{aligned}
\text{maximize} \quad &amp; \sum_{(u,v) \in E} y_{uv} \
\text{subject to} \quad &amp; \sum_{(u,v): v \in (u,v)} y_{uv} \le 1 \quad \forall v \in V \
&amp; y_{uv} \ge 0
\end{aligned}
\]</span></p>
<p>Algorithm (Primal–Dual):</p>
<ol type="1">
<li>Initialize all <span class="math inline">\(y_{uv} = 0\)</span> and all edges uncovered.</li>
<li>Increase <span class="math inline">\(y_{uv}\)</span> for uncovered edges until some vertex constraint becomes tight (sum of incident <span class="math inline">\(y_{uv} = 1\)</span>).</li>
<li>Add that vertex to the cover (set <span class="math inline">\(x_v = 1\)</span>).</li>
<li>Repeat until all edges are covered.</li>
</ol>
<p>At the end:</p>
<ul>
<li>Dual cost = total raised <span class="math inline">\(y_{uv}\)</span></li>
<li>Primal cost = number of selected vertices</li>
</ul>
<p>This algorithm yields a 2-approximation, since each edge can “charge” at most twice.</p>
</section>
<section id="example-set-cover-problem" class="level4">
<h4 class="anchored" data-anchor-id="example-set-cover-problem">Example – Set Cover Problem</h4>
<p>Sets <span class="math inline">\(S_1, S_2, \dots, S_m\)</span> with costs <span class="math inline">\(c_i\)</span>, need to cover all elements <span class="math inline">\(U\)</span>.</p>
<p>Primal:</p>
<p><span class="math display">\[
\min \sum_i c_i x_i \quad \text{s.t.} \quad \sum_{i: e \in S_i} x_i \ge 1, ; x_i \ge 0
\]</span></p>
<p>Dual:</p>
<p><span class="math display">\[
\max \sum_{e \in U} y_e \quad \text{s.t.} \quad \sum_{e \in S_i} y_e \le c_i, ; y_e \ge 0
\]</span></p>
<p>Algorithm Sketch:</p>
<ol type="1">
<li>Start with <span class="math inline">\(y_e = 0\)</span>.</li>
<li>Uniformly raise <span class="math inline">\(y_e\)</span> for uncovered elements until some set constraint becomes tight (<span class="math inline">\(\sum_{e \in S_i} y_e = c_i\)</span>).</li>
<li>Pick that set <span class="math inline">\(S_i\)</span> into the cover.</li>
<li>Repeat until all elements are covered.</li>
</ol>
<p>Approximation ratio: <span class="math display">\[
H_n = 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} = O(\log n)
\]</span> Same as the greedy algorithm.</p>
</section>
<section id="tiny-code-simplified-vertex-cover-example" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified-vertex-cover-example">Tiny Code (Simplified Vertex Cover Example)</h4>
<p>Python</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> primal_dual_vertex_cover(edges, n):</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> {e: <span class="dv">0</span> <span class="cf">for</span> e <span class="kw">in</span> edges}</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>    cover <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> edges:</span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a>        (u, v) <span class="op">=</span> edges.pop()</span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> u <span class="kw">not</span> <span class="kw">in</span> cover <span class="kw">and</span> v <span class="kw">not</span> <span class="kw">in</span> cover:</span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a>            cover.add(u)</span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a>            cover.add(v)</span>
<span id="cb142-9"><a href="#cb142-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cover</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This simple primal-dual interpretation of “cover both endpoints” yields a 2-approximation.</p>
</section>
<section id="why-it-matters-80" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-80">Why It Matters</h4>
<ul>
<li><p>Offers a constructive, combinatorial way to derive approximation bounds.</p></li>
<li><p>Avoids solving LPs directly while still exploiting LP structure.</p></li>
<li><p>Yields many of the best-known results for NP-hard problems:</p>
<ul>
<li>2-approximation for Vertex Cover</li>
<li><span class="math inline">\(O(\log n)\)</span> for Set Cover</li>
<li>Constant-factor approximations for Facility Location and Steiner Tree</li>
</ul></li>
</ul>
</section>
<section id="try-it-yourself-79" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-79">Try It Yourself</h4>
<ol type="1">
<li>Derive primal and dual forms of the Facility Location problem.</li>
<li>Implement a primal-dual algorithm that raises client “prices.”</li>
<li>Track when facility opening constraints become tight.</li>
<li>Compare the solution cost to the LP relaxation.</li>
</ol>
</section>
<section id="test-cases-79" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-79">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Problem</th>
<th>Approximation</th>
<th>Method</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Vertex Cover</td>
<td>2</td>
<td>Tightness-based primal-dual</td>
</tr>
<tr class="even">
<td>Set Cover</td>
<td><span class="math inline">\(O(\log n)\)</span></td>
<td>Dual raising</td>
</tr>
<tr class="odd">
<td>Facility Location</td>
<td>1.61</td>
<td>Jain–Vazirani primal-dual</td>
</tr>
<tr class="even">
<td>Steiner Tree</td>
<td>2</td>
<td>Edge-growing</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-79" class="level4">
<h4 class="anchored" data-anchor-id="complexity-79">Complexity</h4>
<ul>
<li>Time: <span class="math inline">\(O(|E| + |V|)\)</span> for graphs, or polynomial for LP-structured problems</li>
<li>Space: <span class="math inline">\(O(|V|)\)</span></li>
<li>Approximation Ratio: Problem-dependent, usually constant or logarithmic</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-75" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-75">A Gentle Proof (Why It Works)</h4>
<p>From weak duality:</p>
<p><span class="math display">\[
b^T y \le c^T x
\]</span></p>
<p>At each step, the dual value grows until a constraint becomes tight, ensuring:</p>
<p><span class="math display">\[
c^T x \le \alpha \cdot b^T y
\]</span></p>
<p>for some <span class="math inline">\(\alpha\)</span>, the approximation factor. This holds because each primal constraint is satisfied once its corresponding dual variable stops increasing.</p>
<p>For example, in Vertex Cover, <span class="math inline">\(\alpha = 2\)</span> since each edge can contribute to two vertices.</p>
</section>
<section id="summary-table-8" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-8">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Builds primal and dual LPs together</td>
</tr>
<tr class="even">
<td>Strategy</td>
<td>Raise duals until primal constraint tight</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td><span class="math inline">\(\text{cost(ALG)} \le \alpha \cdot \text{OPT}\)</span></td>
</tr>
<tr class="even">
<td>Common <span class="math inline">\(\alpha\)</span></td>
<td>2 (VC), <span class="math inline">\(O(\log n)\)</span> (SC)</td>
</tr>
<tr class="odd">
<td>Advantage</td>
<td>No need to solve LP directly</td>
</tr>
</tbody>
</table>
<p>The Primal–Dual Method is the quiet backbone of approximation theory, balancing two mirrors of the same problem until their reflection yields near-optimal beauty.</p>
</section>
</section>
</section>
<section id="section-99.-fairness-causal-inference-and-robust-optimization" class="level1">
<h1>Section 99. Fairness, Causal Inference, and Robust Optimization</h1>
<section id="reweighting-for-fairness" class="level3">
<h3 class="anchored" data-anchor-id="reweighting-for-fairness">981. Reweighting for Fairness</h3>
<p>Reweighting is a simple but powerful algorithmic strategy for reducing bias in machine learning models. It adjusts the <em>importance weights</em> of training samples so that different demographic or sensitive groups contribute <em>equally</em> to the learning process.</p>
<p>This forms the foundation of <em>pre-processing fairness methods</em>, adjusting data before training to correct imbalances in representation or outcomes.</p>
<section id="what-problem-are-we-solving-80" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-80">What Problem Are We Solving?</h4>
<p>Real-world datasets often reflect <em>historical bias</em>. For example, a dataset for loan approval might contain fewer positive examples for a particular group due to systemic discrimination. Training directly on such data leads to unfair predictions.</p>
<p>We want a method that preserves accuracy while making model outcomes more equitable across groups (e.g., gender, race, or age).</p>
</section>
<section id="basic-idea" class="level4">
<h4 class="anchored" data-anchor-id="basic-idea">Basic Idea</h4>
<p>If data is biased, certain combinations of group membership and label are underrepresented. Reweighting aims to correct that imbalance.</p>
<p>Each instance gets a weight:</p>
<p><span class="math display">\[
w(x, a, y) = \frac{P(A=a) , P(Y=y)}{P(A=a, Y=y)}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(A\)</span> = protected attribute (e.g., gender)</li>
<li><span class="math inline">\(Y\)</span> = true label (e.g., loan approved)</li>
<li><span class="math inline">\(P(A=a, Y=y)\)</span> = joint distribution in the observed data</li>
<li><span class="math inline">\(P(A=a) P(Y=y)\)</span> = product of marginals, what we’d expect if <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span> were independent</li>
</ul>
<p>This adjustment <em>breaks the dependency</em> between sensitive attributes and labels in the training data.</p>
</section>
<section id="how-it-works-plain-language-29" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-plain-language-29">How It Works (Plain Language)</h4>
<ol type="1">
<li>Count how often each pair <span class="math inline">\((A=a, Y=y)\)</span> appears in the data.</li>
<li>Compute expected frequencies under independence: <span class="math inline">\(P(A=a) P(Y=y)\)</span>.</li>
<li>Compute reweighting factor <span class="math inline">\(w(a, y)\)</span> = ratio of expected to actual frequency.</li>
<li>Apply these weights when training the model, each example’s loss contribution is scaled by its weight.</li>
</ol>
<p>This ensures that: <span class="math display">\[
A \perp Y \quad \text{(statistical independence enforced in expectation)}
\]</span></p>
</section>
<section id="example-48" class="level4">
<h4 class="anchored" data-anchor-id="example-48">Example</h4>
<p>Suppose:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Group</th>
<th>Label = 1</th>
<th>Label = 0</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A=Male</td>
<td>700</td>
<td>300</td>
<td>1000</td>
</tr>
<tr class="even">
<td>A=Female</td>
<td>300</td>
<td>700</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>Marginals:</p>
<ul>
<li><span class="math inline">\(P(A=\text{Male}) = P(A=\text{Female}) = 0.5\)</span></li>
<li><span class="math inline">\(P(Y=1) = 0.5\)</span>, <span class="math inline">\(P(Y=0) = 0.5\)</span></li>
</ul>
<p>Then the ideal joint distribution under independence is <span class="math inline">\(0.25\)</span> for each <span class="math inline">\((A,Y)\)</span> combination. Observed probabilities:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>(A,Y)</th>
<th>Observed</th>
<th>Expected</th>
<th>Weight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Male,1)</td>
<td>0.35</td>
<td>0.25</td>
<td>0.714</td>
</tr>
<tr class="even">
<td>(Male,0)</td>
<td>0.15</td>
<td>0.25</td>
<td>1.667</td>
</tr>
<tr class="odd">
<td>(Female,1)</td>
<td>0.15</td>
<td>0.25</td>
<td>1.667</td>
</tr>
<tr class="even">
<td>(Female,0)</td>
<td>0.35</td>
<td>0.25</td>
<td>0.714</td>
</tr>
</tbody>
</table>
<p>During training, each instance is weighted accordingly. This makes male and female contributions balanced with respect to positive and negative outcomes.</p>
</section>
<section id="tiny-code-26" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-26">Tiny Code</h4>
<p>Python (using sklearn)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils.class_weight <span class="im">import</span> compute_sample_weight</span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Example data</span></span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([<span class="st">'M'</span>, <span class="st">'M'</span>, <span class="st">'F'</span>, <span class="st">'F'</span>, <span class="st">'M'</span>, <span class="st">'F'</span>])</span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute reweighting factors</span></span>
<span id="cb143-10"><a href="#cb143-10" aria-hidden="true" tabindex="-1"></a>unique_A, unique_Y <span class="op">=</span> np.unique(A), np.unique(Y)</span>
<span id="cb143-11"><a href="#cb143-11" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.zeros(<span class="bu">len</span>(A))</span>
<span id="cb143-12"><a href="#cb143-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-13"><a href="#cb143-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> unique_A:</span>
<span id="cb143-14"><a href="#cb143-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> y <span class="kw">in</span> unique_Y:</span>
<span id="cb143-15"><a href="#cb143-15" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where((A <span class="op">==</span> a) <span class="op">&amp;</span> (Y <span class="op">==</span> y))[<span class="dv">0</span>]</span>
<span id="cb143-16"><a href="#cb143-16" aria-hidden="true" tabindex="-1"></a>        pa <span class="op">=</span> np.mean(A <span class="op">==</span> a)</span>
<span id="cb143-17"><a href="#cb143-17" aria-hidden="true" tabindex="-1"></a>        py <span class="op">=</span> np.mean(Y <span class="op">==</span> y)</span>
<span id="cb143-18"><a href="#cb143-18" aria-hidden="true" tabindex="-1"></a>        pay <span class="op">=</span> <span class="bu">len</span>(idx) <span class="op">/</span> <span class="bu">len</span>(A)</span>
<span id="cb143-19"><a href="#cb143-19" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> (pa <span class="op">*</span> py) <span class="op">/</span> pay</span>
<span id="cb143-20"><a href="#cb143-20" aria-hidden="true" tabindex="-1"></a>        weights[idx] <span class="op">=</span> w</span>
<span id="cb143-21"><a href="#cb143-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-22"><a href="#cb143-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Train weighted model</span></span>
<span id="cb143-23"><a href="#cb143-23" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression()</span>
<span id="cb143-24"><a href="#cb143-24" aria-hidden="true" tabindex="-1"></a>clf.fit(np.ones((<span class="bu">len</span>(A), <span class="dv">1</span>)), Y, sample_weight<span class="op">=</span>weights)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-81" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-81">Why It Matters</h4>
<ul>
<li>It reduces bias before model training (pre-processing).</li>
<li>Works with any classifier that supports weighted samples.</li>
<li>Maintains interpretability and control, easy to explain and audit.</li>
<li>Forms the foundation for more advanced methods like Adversarial Debiasing or Fair Reweighting (Zafar et al.).</li>
</ul>
</section>
<section id="try-it-yourself-80" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-80">Try It Yourself</h4>
<ol type="1">
<li>Compute reweighting factors for your dataset.</li>
<li>Compare model accuracy before and after reweighting.</li>
<li>Evaluate fairness metrics (e.g., demographic parity difference, equal opportunity).</li>
<li>Tune <span class="math inline">\(\varepsilon\)</span> thresholds for acceptable fairness–accuracy tradeoff.</li>
</ol>
</section>
<section id="test-cases-80" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-80">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Fairness Metric</th>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adult Income</td>
<td>Demographic Parity</td>
<td>0.23</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>COMPAS</td>
<td>Equal Opportunity</td>
<td>0.18</td>
<td>0.07</td>
</tr>
<tr class="odd">
<td>Loan Approval</td>
<td>Statistical Parity</td>
<td>0.20</td>
<td>0.04</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-80" class="level4">
<h4 class="anchored" data-anchor-id="complexity-80">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Count group-label frequencies</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(k)\)</span></td>
</tr>
<tr class="even">
<td>Compute weights</td>
<td><span class="math inline">\(O(k)\)</span></td>
<td><span class="math inline">\(O(k)\)</span></td>
</tr>
<tr class="odd">
<td>Weighted training</td>
<td>Depends on model</td>
<td>,</td>
</tr>
</tbody>
</table>
<p>Usually negligible overhead compared to training time.</p>
</section>
<section id="a-gentle-proof-why-it-works-76" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-76">A Gentle Proof (Why It Works)</h4>
<p>The fairness correction relies on making the joint <span class="math inline">\((A, Y)\)</span> distribution factorize into marginals:</p>
<p><span class="math display">\[
P(A, Y) = P(A)P(Y)
\]</span></p>
<p>By assigning weights:</p>
<p><span class="math display">\[
w(a, y) = \frac{P(A=a)P(Y=y)}{P(A=a, Y=y)}
\]</span></p>
<p>the weighted empirical distribution <span class="math inline">\(\hat{P}_w\)</span> satisfies:</p>
<p><span class="math display">\[
\hat{P}_w(A=a, Y=y) = P(A=a) P(Y=y)
\]</span></p>
<p>Hence, any model minimizing weighted loss learns under a fairer, balanced distribution.</p>
</section>
<section id="summary-table-9" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-9">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Goal</td>
<td>Remove dependency between <span class="math inline">\(A\)</span> (group) and <span class="math inline">\(Y\)</span> (label)</td>
</tr>
<tr class="even">
<td>Formula</td>
<td><span class="math inline">\(w(a,y) = \frac{P(A=a) P(Y=y)}{P(A=a, Y=y)}\)</span></td>
</tr>
<tr class="odd">
<td>Type</td>
<td>Pre-processing fairness</td>
</tr>
<tr class="even">
<td>Advantage</td>
<td>Simple, model-agnostic</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Demographic parity (approximate)</td>
</tr>
</tbody>
</table>
<p>Reweighting is fairness through <em>rebalancing</em>: not changing the world, just changing the lens through which the model sees it.</p>
</section>
</section>
<section id="demographic-parity-constraint" class="level3">
<h3 class="anchored" data-anchor-id="demographic-parity-constraint">982. Demographic Parity Constraint</h3>
<p>Demographic Parity (DP), also called Statistical Parity, is one of the most fundamental fairness criteria in machine learning. It ensures that <em>predicted outcomes</em> are independent of <em>sensitive attributes</em> such as gender, race, or age.</p>
<p>In simple terms: the model should give positive outcomes at the same rate for all groups.</p>
<section id="what-problem-are-we-solving-81" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-81">What Problem Are We Solving?</h4>
<p>Even when models are accurate, they can produce biased predictions. For example:</p>
<ul>
<li>A loan approval model may approve 80% of male applicants but only 40% of female applicants.</li>
<li>A hiring algorithm may favor younger candidates, even with equal qualifications.</li>
</ul>
<p>Demographic Parity seeks to eliminate these differences by constraining the model to produce equal acceptance rates across groups.</p>
</section>
<section id="the-fairness-condition" class="level4">
<h4 class="anchored" data-anchor-id="the-fairness-condition">The Fairness Condition</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(A\)</span> = sensitive attribute (e.g., gender)</li>
<li><span class="math inline">\(\hat{Y}\)</span> = model’s predicted label (e.g., approved = 1, denied = 0)</li>
</ul>
<p>Then Demographic Parity requires:</p>
<p><span class="math display">\[
P(\hat{Y} = 1 \mid A = 0) = P(\hat{Y} = 1 \mid A = 1)
\]</span></p>
<p>That is, the probability of a positive outcome should be the same, regardless of <span class="math inline">\(A\)</span>. In practice, we relax it slightly to allow a tolerance <span class="math inline">\(\varepsilon\)</span>:</p>
<p><span class="math display">\[
\left| P(\hat{Y} = 1 \mid A = 0) - P(\hat{Y} = 1 \mid A = 1) \right| \le \varepsilon
\]</span></p>
</section>
<section id="intuitive-example" class="level4">
<h4 class="anchored" data-anchor-id="intuitive-example">Intuitive Example</h4>
<p>Suppose we train a classifier on job applications. Outcomes before applying DP constraint:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Group</th>
<th>Positive Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Male</td>
<td>0.70</td>
</tr>
<tr class="even">
<td>Female</td>
<td>0.45</td>
</tr>
</tbody>
</table>
<p>DP requires adjusting the model or thresholds so both groups achieve roughly equal positive rates (say 0.57 ± 0.02).</p>
<p>We can do this by:</p>
<ul>
<li>Changing decision thresholds per group, or</li>
<li>Adding a constraint in the loss function during training.</li>
</ul>
</section>
<section id="loss-with-demographic-parity-penalty" class="level4">
<h4 class="anchored" data-anchor-id="loss-with-demographic-parity-penalty">Loss with Demographic Parity Penalty</h4>
<p>The standard empirical loss is:</p>
<p><span class="math display">\[
L = \frac{1}{n} \sum_i \ell(f(x_i), y_i)
\]</span></p>
<p>We add a fairness penalty:</p>
<p><span class="math display">\[
L_{\text{fair}} = L + \lambda , \left| , E[\hat{Y} | A = 0] - E[\hat{Y} | A = 1] , \right|
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> controls the tradeoff between accuracy and fairness. A higher <span class="math inline">\(\lambda\)</span> enforces stronger fairness at the cost of model performance.</p>
</section>
<section id="tiny-code-fair-logistic-regression-with-dp-penalty" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-fair-logistic-regression-with-dp-penalty">Tiny Code (Fair Logistic Regression with DP Penalty)</h4>
<p>Python (simplified)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FairLogReg(nn.Module):</span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d):</span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> nn.Linear(d, <span class="dv">1</span>)</span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.w(x))</span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-13"><a href="#cb144-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demographic_parity_penalty(pred, A):</span>
<span id="cb144-14"><a href="#cb144-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mean positive rate per group</span></span>
<span id="cb144-15"><a href="#cb144-15" aria-hidden="true" tabindex="-1"></a>    p0 <span class="op">=</span> pred[A <span class="op">==</span> <span class="dv">0</span>].mean()</span>
<span id="cb144-16"><a href="#cb144-16" aria-hidden="true" tabindex="-1"></a>    p1 <span class="op">=</span> pred[A <span class="op">==</span> <span class="dv">1</span>].mean()</span>
<span id="cb144-17"><a href="#cb144-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.<span class="bu">abs</span>(p0 <span class="op">-</span> p1)</span>
<span id="cb144-18"><a href="#cb144-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-19"><a href="#cb144-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb144-20"><a href="#cb144-20" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> FairLogReg(d<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb144-21"><a href="#cb144-21" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb144-22"><a href="#cb144-22" aria-hidden="true" tabindex="-1"></a>λ <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb144-23"><a href="#cb144-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-24"><a href="#cb144-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, y, A <span class="kw">in</span> data_loader:</span>
<span id="cb144-25"><a href="#cb144-25" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model(x).squeeze()</span>
<span id="cb144-26"><a href="#cb144-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.BCELoss()(pred, y)</span>
<span id="cb144-27"><a href="#cb144-27" aria-hidden="true" tabindex="-1"></a>    dp_penalty <span class="op">=</span> demographic_parity_penalty(pred, A)</span>
<span id="cb144-28"><a href="#cb144-28" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> loss <span class="op">+</span> λ <span class="op">*</span> dp_penalty</span>
<span id="cb144-29"><a href="#cb144-29" aria-hidden="true" tabindex="-1"></a>    opt.zero_grad()</span>
<span id="cb144-30"><a href="#cb144-30" aria-hidden="true" tabindex="-1"></a>    total_loss.backward()</span>
<span id="cb144-31"><a href="#cb144-31" aria-hidden="true" tabindex="-1"></a>    opt.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This model learns while actively penalizing group-level imbalance in prediction rates.</p>
</section>
<section id="why-it-matters-82" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-82">Why It Matters</h4>
<ul>
<li>Prevents discrimination <em>in outcomes</em>, not just in labels.</li>
<li>Serves as a first-line fairness constraint in many systems (e.g., credit scoring, advertising).</li>
<li>Forms the foundation of other criteria like Equalized Odds and Predictive Parity.</li>
<li>Works well when group membership is known and relevant for fairness analysis.</li>
</ul>
</section>
<section id="try-it-yourself-81" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-81">Try It Yourself</h4>
<ol type="1">
<li>Train a standard logistic regression model.</li>
<li>Measure <span class="math inline">\(P(\hat{Y}=1 \mid A=0)\)</span> and <span class="math inline">\(P(\hat{Y}=1 \mid A=1)\)</span>.</li>
<li>Add a fairness penalty (or threshold adjustment).</li>
<li>Observe how accuracy and fairness trade off.</li>
<li>Tune <span class="math inline">\(\lambda\)</span> to balance fairness and utility.</li>
</ol>
</section>
<section id="test-cases-81" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-81">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 35%">
<col style="width: 10%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Metric</th>
<th>Before</th>
<th>After DP Constraint</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adult Income</td>
<td><span class="math inline">\(\Delta P(\hat{Y}=1)\)</span></td>
<td>0.24</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>COMPAS</td>
<td><span class="math inline">\(\Delta P(\hat{Y}=1)\)</span></td>
<td>0.18</td>
<td>0.07</td>
</tr>
<tr class="odd">
<td>Bank Marketing</td>
<td><span class="math inline">\(\Delta P(\hat{Y}=1)\)</span></td>
<td>0.21</td>
<td>0.03</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-81" class="level4">
<h4 class="anchored" data-anchor-id="complexity-81">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Compute group means</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(k)\)</span></td>
</tr>
<tr class="even">
<td>Add penalty to loss</td>
<td><span class="math inline">\(O(1)\)</span></td>
<td>,</td>
</tr>
<tr class="odd">
<td>Training cost</td>
<td>Slightly above baseline</td>
<td>,</td>
</tr>
</tbody>
</table>
<p>Fairness enforcement adds negligible computational overhead.</p>
</section>
<section id="a-gentle-proof-why-it-works-77" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-77">A Gentle Proof (Why It Works)</h4>
<p>If <span class="math inline">\(\hat{Y}\)</span> is trained to minimize</p>
<p><span class="math display">\[
L_{\text{fair}} = L + \lambda , | E[\hat{Y}|A=0] - E[\hat{Y}|A=1] |
\]</span></p>
<p>then at equilibrium:</p>
<p><span class="math display">\[
E[\hat{Y}|A=0] \approx E[\hat{Y}|A=1]
\]</span></p>
<p>which implies approximate independence:</p>
<p><span class="math display">\[
\hat{Y} \perp A
\]</span></p>
<p>Thus, group membership no longer influences outcomes, the core fairness condition for Demographic Parity.</p>
</section>
<section id="summary-table-10" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-10">Summary Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 54%">
<col style="width: 20%">
<col style="width: 5%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fairness Type</td>
<td>Demographic Parity (Statistical Parity)</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Mathematical Form</td>
<td><span class="math inline">\(P(\hat{Y}=1                                     | A=0) = P(\hat{Y}=1 | A=1)\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Implementation</td>
<td>Penalty or post-processing</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Strength</td>
<td>Group-level equality</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Limitation</td>
<td>May reduce accuracy or ignore label correlations</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Demographic Parity is fairness in its purest form, it doesn’t ask <em>why</em> outcomes differ, only that they <em>shouldn’t</em> differ at all.</p>
</section>
</section>
<section id="equalized-odds" class="level3">
<h3 class="anchored" data-anchor-id="equalized-odds">983. Equalized Odds</h3>
<p>Equalized Odds (EO) is a fairness criterion that goes deeper than <em>Demographic Parity</em>. Instead of just equalizing overall prediction rates, it demands equality conditioned on the true outcome, ensuring that a model is <em>equally accurate</em> (and equally mistaken) across demographic groups.</p>
<p>It’s a fairness definition that focuses on error balance, not just outcome rates.</p>
<section id="what-problem-are-we-solving-82" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-82">What Problem Are We Solving?</h4>
<p>Demographic Parity ensures equal positive rates but ignores whether those predictions are correct. A model could trivially satisfy DP by flipping coins, giving random approvals equally across groups. That’s fair in form, but not in truth.</p>
<p>Equalized Odds fixes this by enforcing that both <em>true positive rates</em> and <em>false positive rates</em> are equal across groups.</p>
</section>
<section id="the-fairness-condition-1" class="level4">
<h4 class="anchored" data-anchor-id="the-fairness-condition-1">The Fairness Condition</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(A\)</span> = sensitive attribute (e.g., gender)</li>
<li><span class="math inline">\(Y\)</span> = true label</li>
<li><span class="math inline">\(\hat{Y}\)</span> = predicted label</li>
</ul>
<p>Then Equalized Odds requires:</p>
<p><span class="math display">\[
P(\hat{Y} = 1 \mid Y = y, A = 0) = P(\hat{Y} = 1 \mid Y = y, A = 1)
\quad \text{for } y \in {0, 1}
\]</span></p>
<p>That is:</p>
<ul>
<li>Equal true positive rates (TPR) across groups</li>
<li>Equal false positive rates (FPR) across groups</li>
</ul>
<p>In practice, we measure: <span class="math display">\[
\text{TPR gap} = | P(\hat{Y}=1|Y=1,A=0) - P(\hat{Y}=1|Y=1,A=1) |
\]</span> <span class="math display">\[
\text{FPR gap} = | P(\hat{Y}=1|Y=0,A=0) - P(\hat{Y}=1|Y=0,A=1) |
\]</span></p>
<p>Both should be small, ideally below a threshold <span class="math inline">\(\varepsilon\)</span>.</p>
</section>
<section id="intuitive-example-1" class="level4">
<h4 class="anchored" data-anchor-id="intuitive-example-1">Intuitive Example</h4>
<p>Suppose we build a medical diagnosis model.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Group</th>
<th>True Positive Rate</th>
<th>False Positive Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Male</td>
<td>0.85</td>
<td>0.10</td>
</tr>
<tr class="even">
<td>Female</td>
<td>0.70</td>
<td>0.05</td>
</tr>
</tbody>
</table>
<p>The model is better at catching positives for males. Equalized Odds would require adjustments (e.g., shifting decision thresholds) so both groups have approximately equal TPR and FPR, say 0.78 and 0.07 respectively.</p>
</section>
<section id="implementation-strategies" class="level4">
<h4 class="anchored" data-anchor-id="implementation-strategies">Implementation Strategies</h4>
<p>There are three main ways to enforce Equalized Odds:</p>
<ol type="1">
<li><p>Pre-processing Modify data or sample weights so that errors are balanced across groups (e.g., reweighting, resampling).</p></li>
<li><p>In-processing Add fairness regularizers to the loss: <span class="math display">\[
L_{\text{fair}} = L + \lambda_1 | \text{TPR}_0 - \text{TPR}_1 | + \lambda_2 | \text{FPR}_0 - \text{FPR}_1 |
\]</span></p></li>
<li><p>Post-processing Adjust decision thresholds per group after training to equalize error rates (Hardt et al., 2016).</p></li>
</ol>
</section>
<section id="tiny-code-threshold-adjustment-for-eo" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-threshold-adjustment-for-eo">Tiny Code (Threshold Adjustment for EO)</h4>
<p>Python (post-processing)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> equalized_odds_thresholds(y_true, y_pred, A):</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>    thresholds <span class="op">=</span> {}</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> a <span class="kw">in</span> np.unique(A):</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>        yg <span class="op">=</span> y_pred[A <span class="op">==</span> a]</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>        tg <span class="op">=</span> y_true[A <span class="op">==</span> a]</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute best threshold to equalize TPR/FPR</span></span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>        best_t, best_gap <span class="op">=</span> <span class="fl">0.5</span>, <span class="fl">1.0</span></span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">101</span>):</span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a>            pred_bin <span class="op">=</span> (yg <span class="op">&gt;=</span> t).astype(<span class="bu">int</span>)</span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>            tpr <span class="op">=</span> np.mean(pred_bin[tg <span class="op">==</span> <span class="dv">1</span>])</span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a>            fpr <span class="op">=</span> np.mean(pred_bin[tg <span class="op">==</span> <span class="dv">0</span>])</span>
<span id="cb145-14"><a href="#cb145-14" aria-hidden="true" tabindex="-1"></a>            gap <span class="op">=</span> <span class="bu">abs</span>(tpr <span class="op">-</span> fpr)</span>
<span id="cb145-15"><a href="#cb145-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> gap <span class="op">&lt;</span> best_gap:</span>
<span id="cb145-16"><a href="#cb145-16" aria-hidden="true" tabindex="-1"></a>                best_t, best_gap <span class="op">=</span> t, gap</span>
<span id="cb145-17"><a href="#cb145-17" aria-hidden="true" tabindex="-1"></a>        thresholds[a] <span class="op">=</span> best_t</span>
<span id="cb145-18"><a href="#cb145-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> thresholds</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This computes per-group thresholds that minimize error imbalance.</p>
</section>
<section id="why-it-matters-83" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-83">Why It Matters</h4>
<ul>
<li>Ensures equal treatment in correctness, not just outcome rates.</li>
<li>Ideal for high-stakes domains like healthcare, justice, or hiring.</li>
<li>Avoids “fair but useless” models (unlike Demographic Parity).</li>
<li>Provides a tradeoff between fairness and accuracy that’s more ethically meaningful.</li>
</ul>
</section>
<section id="try-it-yourself-82" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-82">Try It Yourself</h4>
<ol type="1">
<li>Train any binary classifier and record predictions per group.</li>
<li>Compute TPR and FPR for each group.</li>
<li>Adjust thresholds or weights to minimize both gaps.</li>
<li>Compare model fairness (EO gaps) and accuracy before and after.</li>
</ol>
</section>
<section id="test-cases-82" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-82">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Metric</th>
<th>Before</th>
<th>After Equalized Odds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>COMPAS</td>
<td>TPR gap</td>
<td>0.22</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>COMPAS</td>
<td>FPR gap</td>
<td>0.18</td>
<td>0.04</td>
</tr>
<tr class="odd">
<td>Adult Income</td>
<td>TPR gap</td>
<td>0.15</td>
<td>0.03</td>
</tr>
<tr class="even">
<td>Adult Income</td>
<td>FPR gap</td>
<td>0.12</td>
<td>0.02</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-82" class="level4">
<h4 class="anchored" data-anchor-id="complexity-82">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Threshold search</td>
<td><span class="math inline">\(O(k \cdot n)\)</span></td>
<td><span class="math inline">\(O(k)\)</span></td>
</tr>
<tr class="even">
<td>Compute rates</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="odd">
<td>Training penalty (in-processing)</td>
<td>Minor</td>
<td>,</td>
</tr>
</tbody>
</table>
<p>Here <span class="math inline">\(k\)</span> is the number of candidate thresholds.</p>
</section>
<section id="a-gentle-proof-why-it-works-78" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-78">A Gentle Proof (Why It Works)</h4>
<p>Let <span class="math inline">\(\hat{Y}_A\)</span> denote predictions per group. By adjusting thresholds to make <span class="math inline">\(P(\hat{Y}=1|Y=y,A=a)\)</span> equal across <span class="math inline">\(A\)</span>, we enforce:</p>
<p><span class="math display">\[
\hat{Y} \perp A \mid Y
\]</span></p>
<p>This conditional independence expresses fairness: the model’s errors no longer depend on group membership once true outcomes are known.</p>
</section>
<section id="summary-table-11" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-11">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fairness Type</td>
<td>Equalized Odds</td>
</tr>
<tr class="even">
<td>Condition</td>
<td><span class="math inline">\(\hat{Y} \perp A \mid Y\)</span></td>
</tr>
<tr class="odd">
<td>Constraints</td>
<td>Equal TPR and FPR across groups</td>
</tr>
<tr class="even">
<td>Implementation</td>
<td>Pre-, In-, or Post-processing</td>
</tr>
<tr class="odd">
<td>Benefit</td>
<td>Ensures equal <em>error behavior</em></td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>May require group-specific thresholds</td>
</tr>
</tbody>
</table>
<p>Equalized Odds is fairness with awareness, not pretending everyone’s the same, but ensuring everyone is <em>treated equally in being right or wrong.</em></p>
</section>
</section>
<section id="adversarial-debiasing" class="level3">
<h3 class="anchored" data-anchor-id="adversarial-debiasing">984. Adversarial Debiasing</h3>
<p>Adversarial Debiasing is one of the most elegant and powerful methods to achieve fairness in machine learning. It uses the same philosophy that drives Generative Adversarial Networks (GANs): two models compete, one tries to make accurate predictions, while the other tries to detect unfair bias. Through this tension, the predictor learns to <em>hide</em> information about sensitive attributes, resulting in fairer outcomes.</p>
<section id="what-problem-are-we-solving-83" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-83">What Problem Are We Solving?</h4>
<p>Many learning algorithms, even when trained on reweighted or balanced data, still leak information about sensitive attributes (<span class="math inline">\(A\)</span>), such as gender or race. The model might not use <span class="math inline">\(A\)</span> explicitly, but it can <em>infer</em> it from correlated features (like zip code or occupation).</p>
<p>We want to train a predictor <span class="math inline">\(f_\theta(x)\)</span> that:</p>
<ol type="1">
<li>Predicts the target <span class="math inline">\(Y\)</span> accurately, and</li>
<li>Produces predictions <span class="math inline">\(\hat{Y}\)</span> that contain as little information about <span class="math inline">\(A\)</span> as possible.</li>
</ol>
</section>
<section id="the-core-idea-43" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-43">The Core Idea</h4>
<p>We build two networks (or modules):</p>
<ol type="1">
<li><p>Predictor (Main model) Learns <span class="math inline">\(f_\theta(x)\)</span> to predict the true label <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Adversary (Fairness discriminator) Learns <span class="math inline">\(g_\phi(\hat{Y})\)</span> (or sometimes <span class="math inline">\(g_\phi(h)\)</span> using internal representations) to predict the sensitive attribute <span class="math inline">\(A\)</span> from the predictor’s output.</p></li>
</ol>
<p>The predictor is trained to <em>minimize prediction loss</em> while simultaneously <em>maximizing the adversary’s loss</em>, making it hard for the adversary to recover <span class="math inline">\(A\)</span>.</p>
</section>
<section id="the-objective" class="level4">
<h4 class="anchored" data-anchor-id="the-objective">The Objective</h4>
<p>Let <span class="math inline">\(L_y\)</span> be the prediction loss (e.g., cross-entropy for <span class="math inline">\(Y\)</span>), and <span class="math inline">\(L_a\)</span> the adversary loss (for predicting <span class="math inline">\(A\)</span>).</p>
<p>We optimize a min–max objective:</p>
<p><span class="math display">\[
\min_{\theta} \max_{\phi} , \big( L_y(f_\theta(x), y) - \lambda L_a(g_\phi(f_\theta(x)), a) \big)
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> controls the fairness–accuracy tradeoff.</p>
<ul>
<li>The predictor wants to minimize <span class="math inline">\(L_y\)</span> and maximize <span class="math inline">\(L_a\)</span> (fool the adversary).</li>
<li>The adversary wants to minimize <span class="math inline">\(L_a\)</span> (predict <span class="math inline">\(A\)</span> as well as possible).</li>
</ul>
<p>When the system reaches equilibrium: <span class="math display">\[
\hat{Y} \perp A
\]</span> i.e., predictions no longer carry information about group membership.</p>
</section>
<section id="tiny-code-adversarial-debiasing-framework" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-adversarial-debiasing-framework">Tiny Code (Adversarial Debiasing Framework)</h4>
<p>PyTorch-style pseudocode</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictor model (main task)</span></span>
<span id="cb146-6"><a href="#cb146-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Predictor(nn.Module):</span>
<span id="cb146-7"><a href="#cb146-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d):</span>
<span id="cb146-8"><a href="#cb146-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb146-9"><a href="#cb146-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(nn.Linear(d, <span class="dv">16</span>), nn.ReLU(), nn.Linear(<span class="dv">16</span>, <span class="dv">1</span>), nn.Sigmoid())</span>
<span id="cb146-10"><a href="#cb146-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-11"><a href="#cb146-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb146-12"><a href="#cb146-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb146-13"><a href="#cb146-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-14"><a href="#cb146-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Adversary model (fairness discriminator)</span></span>
<span id="cb146-15"><a href="#cb146-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Adversary(nn.Module):</span>
<span id="cb146-16"><a href="#cb146-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb146-17"><a href="#cb146-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb146-18"><a href="#cb146-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(nn.Linear(<span class="dv">1</span>, <span class="dv">8</span>), nn.ReLU(), nn.Linear(<span class="dv">8</span>, <span class="dv">1</span>), nn.Sigmoid())</span>
<span id="cb146-19"><a href="#cb146-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-20"><a href="#cb146-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred):</span>
<span id="cb146-21"><a href="#cb146-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(y_pred)</span>
<span id="cb146-22"><a href="#cb146-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-23"><a href="#cb146-23" aria-hidden="true" tabindex="-1"></a>predictor <span class="op">=</span> Predictor(d<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb146-24"><a href="#cb146-24" aria-hidden="true" tabindex="-1"></a>adversary <span class="op">=</span> Adversary()</span>
<span id="cb146-25"><a href="#cb146-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-26"><a href="#cb146-26" aria-hidden="true" tabindex="-1"></a>opt_pred <span class="op">=</span> optim.Adam(predictor.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb146-27"><a href="#cb146-27" aria-hidden="true" tabindex="-1"></a>opt_adv <span class="op">=</span> optim.Adam(adversary.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb146-28"><a href="#cb146-28" aria-hidden="true" tabindex="-1"></a>λ <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb146-29"><a href="#cb146-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-30"><a href="#cb146-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x, y, a <span class="kw">in</span> data_loader:</span>
<span id="cb146-31"><a href="#cb146-31" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> predictor(x)</span>
<span id="cb146-32"><a href="#cb146-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-33"><a href="#cb146-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train adversary</span></span>
<span id="cb146-34"><a href="#cb146-34" aria-hidden="true" tabindex="-1"></a>    a_pred <span class="op">=</span> adversary(y_pred.detach())</span>
<span id="cb146-35"><a href="#cb146-35" aria-hidden="true" tabindex="-1"></a>    loss_a <span class="op">=</span> nn.BCELoss()(a_pred, a.<span class="bu">float</span>())</span>
<span id="cb146-36"><a href="#cb146-36" aria-hidden="true" tabindex="-1"></a>    opt_adv.zero_grad()</span>
<span id="cb146-37"><a href="#cb146-37" aria-hidden="true" tabindex="-1"></a>    loss_a.backward()</span>
<span id="cb146-38"><a href="#cb146-38" aria-hidden="true" tabindex="-1"></a>    opt_adv.step()</span>
<span id="cb146-39"><a href="#cb146-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-40"><a href="#cb146-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train predictor</span></span>
<span id="cb146-41"><a href="#cb146-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> predictor(x)</span>
<span id="cb146-42"><a href="#cb146-42" aria-hidden="true" tabindex="-1"></a>    a_pred <span class="op">=</span> adversary(y_pred)</span>
<span id="cb146-43"><a href="#cb146-43" aria-hidden="true" tabindex="-1"></a>    loss_y <span class="op">=</span> nn.BCELoss()(y_pred, y.<span class="bu">float</span>())</span>
<span id="cb146-44"><a href="#cb146-44" aria-hidden="true" tabindex="-1"></a>    loss_total <span class="op">=</span> loss_y <span class="op">-</span> λ <span class="op">*</span> nn.BCELoss()(a_pred, a.<span class="bu">float</span>())</span>
<span id="cb146-45"><a href="#cb146-45" aria-hidden="true" tabindex="-1"></a>    opt_pred.zero_grad()</span>
<span id="cb146-46"><a href="#cb146-46" aria-hidden="true" tabindex="-1"></a>    loss_total.backward()</span>
<span id="cb146-47"><a href="#cb146-47" aria-hidden="true" tabindex="-1"></a>    opt_pred.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This loop alternates between strengthening the adversary and weakening its influence on the predictor, balancing fairness and accuracy dynamically.</p>
</section>
<section id="why-it-matters-84" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-84">Why It Matters</h4>
<ul>
<li>Learns representation-level fairness, deeper than reweighting or thresholding.</li>
<li>Works even when sensitive information is implicit or correlated.</li>
<li>Applicable to text, vision, tabular, or graph data.</li>
<li>Enables flexible tradeoff via <span class="math inline">\(\lambda\)</span>.</li>
<li>Inspired by information theory: reduce mutual information between <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(A\)</span>.</li>
</ul>
</section>
<section id="try-it-yourself-83" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-83">Try It Yourself</h4>
<ol type="1">
<li><p>Start with a dataset like Adult Income (with <code>sex</code> as <span class="math inline">\(A\)</span>).</p></li>
<li><p>Train a baseline classifier, measure demographic disparity.</p></li>
<li><p>Add an adversarial head to predict <span class="math inline">\(A\)</span>.</p></li>
<li><p>Tune <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li><span class="math inline">\(\lambda = 0\)</span> → maximum accuracy, no fairness.</li>
<li><span class="math inline">\(\lambda &gt; 1\)</span> → strong fairness, possible accuracy drop.</li>
</ul></li>
<li><p>Observe tradeoffs in fairness metrics.</p></li>
</ol>
</section>
<section id="test-cases-83" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-83">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 28%">
<col style="width: 9%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Metric</th>
<th>Before</th>
<th>After Adversarial Debiasing</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adult Income</td>
<td>Demographic Parity</td>
<td>0.23</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>COMPAS</td>
<td>Equalized Odds Gap</td>
<td>0.21</td>
<td>0.07</td>
</tr>
<tr class="odd">
<td>German Credit</td>
<td>TPR Gap</td>
<td>0.19</td>
<td>0.06</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-83" class="level4">
<h4 class="anchored" data-anchor-id="complexity-83">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predictor update</td>
<td><span class="math inline">\(O(n d)\)</span></td>
<td><span class="math inline">\(O(d)\)</span></td>
</tr>
<tr class="even">
<td>Adversary update</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td>Slightly slower than standard training</td>
<td>,</td>
</tr>
</tbody>
</table>
<p>Training alternates between two optimizers, typically doubles runtime, but remains polynomial.</p>
</section>
<section id="a-gentle-proof-why-it-works-79" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-79">A Gentle Proof (Why It Works)</h4>
<p>At equilibrium of the min–max game: <span class="math display">\[
\nabla_\phi L_a = 0, \quad \nabla_\theta (L_y - \lambda L_a) = 0
\]</span></p>
<p>This implies the adversary cannot extract information about <span class="math inline">\(A\)</span> from <span class="math inline">\(\hat{Y}\)</span>. Hence the mutual information <span class="math inline">\(I(\hat{Y}; A)\)</span> is minimized: <span class="math display">\[
I(\hat{Y}; A) \approx 0
\]</span></p>
<p>so predictions become <em>statistically independent</em> of the sensitive attribute, the formal definition of fairness under demographic parity.</p>
</section>
<section id="summary-table-12" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-12">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fairness Type</td>
<td>In-processing (Adversarial)</td>
</tr>
<tr class="even">
<td>Objective</td>
<td><span class="math inline">\(\min_\theta \max_\phi (L_y - \lambda L_a)\)</span></td>
</tr>
<tr class="odd">
<td>Mechanism</td>
<td>Adversary tries to detect bias; predictor hides it</td>
</tr>
<tr class="even">
<td>Key Idea</td>
<td>Fairness through confusion</td>
</tr>
<tr class="odd">
<td>Strength</td>
<td>Learns fair latent representations</td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>Requires adversary tuning and joint stability</td>
</tr>
</tbody>
</table>
<p>Adversarial Debiasing is fairness by competition, two networks locked in a game where justice emerges from balance.</p>
</section>
</section>
<section id="causal-dag-discovery" class="level3">
<h3 class="anchored" data-anchor-id="causal-dag-discovery">985. Causal DAG Discovery</h3>
<p>Causal Directed Acyclic Graph (DAG) Discovery is the process of uncovering cause-and-effect relationships from data. Unlike correlation-based learning, causal discovery tries to answer <em>what happens if we intervene</em>, going beyond prediction into the structure of reality itself.</p>
<section id="what-problem-are-we-solving-84" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-84">What Problem Are We Solving?</h4>
<p>Machine learning models often find associations:</p>
<ul>
<li>“Smoking and yellow teeth are correlated.” But only causal analysis can tell us:</li>
<li>“Smoking causes yellow teeth.”</li>
</ul>
<p>Causal discovery formalizes this by identifying a graph structure <span class="math inline">\(G = (V, E)\)</span> over variables <span class="math inline">\(V\)</span>, where edges <span class="math inline">\(X_i \to X_j\)</span> represent direct causal influence.</p>
</section>
<section id="causal-graph-basics" class="level4">
<h4 class="anchored" data-anchor-id="causal-graph-basics">Causal Graph Basics</h4>
<p>A causal DAG is a directed acyclic graph where:</p>
<ul>
<li>Each node represents a variable.</li>
<li>Each edge represents a causal relation (<span class="math inline">\(X_i \to X_j\)</span> means <span class="math inline">\(X_i\)</span> directly causes <span class="math inline">\(X_j\)</span>).</li>
<li>No directed cycles exist (no feedback loops).</li>
</ul>
<p>We assume data follows the Causal Markov Condition: <span class="math display">\[
P(X_1, \dots, X_n) = \prod_i P(X_i \mid \text{Pa}(X_i))
\]</span> where <span class="math inline">\(\text{Pa}(X_i)\)</span> are the parents (direct causes) of <span class="math inline">\(X_i\)</span> in the graph.</p>
</section>
<section id="two-major-approaches" class="level4">
<h4 class="anchored" data-anchor-id="two-major-approaches">Two Major Approaches</h4>
<ol type="1">
<li><p>Constraint-Based (Conditional Independence)</p>
<ul>
<li>Tests statistical independence among variables.</li>
<li>Builds edges consistent with those tests.</li>
<li>Example: PC Algorithm (Peter–Clark).</li>
</ul></li>
<li><p>Score-Based (Optimization)</p>
<ul>
<li>Assigns a score to each DAG based on how well it fits the data (e.g., BIC score).</li>
<li>Searches over DAG space to maximize score.</li>
<li>Example: GES (Greedy Equivalence Search).</li>
</ul></li>
</ol>
</section>
<section id="the-pc-algorithm-plain-overview" class="level4">
<h4 class="anchored" data-anchor-id="the-pc-algorithm-plain-overview">The PC Algorithm (Plain Overview)</h4>
<p>Input: dataset with <span class="math inline">\(n\)</span> variables. Goal: build DAG capturing causal dependencies.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 96%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start with a fully connected undirected graph.</td>
</tr>
<tr class="even">
<td>2</td>
<td>For each pair <span class="math inline">\((X_i, X_j)\)</span>, test if they are conditionally independent given some subset <span class="math inline">\(S\)</span> of other variables.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>If independent, remove the edge between them.</td>
</tr>
<tr class="even">
<td>4</td>
<td>Orient remaining edges using logical rules (e.g., collider rules).</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Output resulting partially directed DAG (CPDAG).</td>
</tr>
</tbody>
</table>
<p>This process combines statistics and logic to infer causal directions.</p>
</section>
<section id="tiny-code-pc-algorithm-simplified" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-pc-algorithm-simplified">Tiny Code (PC Algorithm Simplified)</h4>
<p>Python pseudocode (using partial correlations)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> pearsonr</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cond_independent(x, y, cond, data, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> cond:</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a>        r, _ <span class="op">=</span> pearsonr(data[:, x], data[:, y])</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simple linear regression residual method</span></span>
<span id="cb147-10"><a href="#cb147-10" aria-hidden="true" tabindex="-1"></a>        Xc <span class="op">=</span> data[:, cond]</span>
<span id="cb147-11"><a href="#cb147-11" aria-hidden="true" tabindex="-1"></a>        beta_x <span class="op">=</span> np.linalg.lstsq(Xc, data[:, x], rcond<span class="op">=</span><span class="va">None</span>)[<span class="dv">0</span>]</span>
<span id="cb147-12"><a href="#cb147-12" aria-hidden="true" tabindex="-1"></a>        beta_y <span class="op">=</span> np.linalg.lstsq(Xc, data[:, y], rcond<span class="op">=</span><span class="va">None</span>)[<span class="dv">0</span>]</span>
<span id="cb147-13"><a href="#cb147-13" aria-hidden="true" tabindex="-1"></a>        rx <span class="op">=</span> data[:, x] <span class="op">-</span> Xc <span class="op">@</span> beta_x</span>
<span id="cb147-14"><a href="#cb147-14" aria-hidden="true" tabindex="-1"></a>        ry <span class="op">=</span> data[:, y] <span class="op">-</span> Xc <span class="op">@</span> beta_y</span>
<span id="cb147-15"><a href="#cb147-15" aria-hidden="true" tabindex="-1"></a>        r, _ <span class="op">=</span> pearsonr(rx, ry)</span>
<span id="cb147-16"><a href="#cb147-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">abs</span>(r) <span class="op">&lt;</span> alpha</span>
<span id="cb147-17"><a href="#cb147-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-18"><a href="#cb147-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pc_algorithm(data):</span>
<span id="cb147-19"><a href="#cb147-19" aria-hidden="true" tabindex="-1"></a>    n_vars <span class="op">=</span> data.shape[<span class="dv">1</span>]</span>
<span id="cb147-20"><a href="#cb147-20" aria-hidden="true" tabindex="-1"></a>    adj <span class="op">=</span> np.ones((n_vars, n_vars)) <span class="op">-</span> np.eye(n_vars)</span>
<span id="cb147-21"><a href="#cb147-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(n_vars <span class="op">-</span> <span class="dv">2</span>):</span>
<span id="cb147-22"><a href="#cb147-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (i, j) <span class="kw">in</span> itertools.combinations(<span class="bu">range</span>(n_vars), <span class="dv">2</span>):</span>
<span id="cb147-23"><a href="#cb147-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> adj[i, j]:</span>
<span id="cb147-24"><a href="#cb147-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> cond <span class="kw">in</span> itertools.combinations([k <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_vars) <span class="cf">if</span> k <span class="kw">not</span> <span class="kw">in</span> [i, j]], l):</span>
<span id="cb147-25"><a href="#cb147-25" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> cond_independent(i, j, cond, data):</span>
<span id="cb147-26"><a href="#cb147-26" aria-hidden="true" tabindex="-1"></a>                        adj[i, j] <span class="op">=</span> adj[j, i] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb147-27"><a href="#cb147-27" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">break</span></span>
<span id="cb147-28"><a href="#cb147-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> adj</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This simplified version removes edges that show conditional independence.</p>
</section>
<section id="why-it-matters-85" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-85">Why It Matters</h4>
<ul>
<li>Reveals <em>mechanistic</em> understanding, not just correlation.</li>
<li>Essential for reasoning under intervention: “What if we change <span class="math inline">\(X\)</span>?”</li>
<li>Foundation for fairness-aware models and causal inference in AI.</li>
<li>Helps prevent spurious associations that mislead decisions.</li>
</ul>
</section>
<section id="try-it-yourself-84" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-84">Try It Yourself</h4>
<ol type="1">
<li>Generate synthetic data: <span class="math display">\[
X \to Y, \quad Z = X + \epsilon, \quad Y = X + Z + \text{noise}
\]</span></li>
<li>Run a causal discovery algorithm (e.g., PC or GES).</li>
<li>Check if it recovers the original causal directions.</li>
<li>Compare with correlations, note where they differ.</li>
</ol>
</section>
<section id="test-cases-84" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-84">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 32%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>True Causal Structure</th>
<th>Discovered Edges (PC)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Synthetic (3 vars)</td>
<td><span class="math inline">\(X \to Y \to Z\)</span></td>
<td><span class="math inline">\(X \to Y\)</span>, <span class="math inline">\(Y \to Z\)</span></td>
</tr>
<tr class="even">
<td>Linear Gaussian</td>
<td><span class="math inline">\(A \to B\)</span>, <span class="math inline">\(A \to C\)</span></td>
<td><span class="math inline">\(A \to B\)</span>, <span class="math inline">\(A \to C\)</span></td>
</tr>
<tr class="odd">
<td>Nonlinear</td>
<td><span class="math inline">\(X \to Y\)</span>, <span class="math inline">\(Y \not\to X\)</span></td>
<td>Partial DAG (direction ambiguous)</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-84" class="level4">
<h4 class="anchored" data-anchor-id="complexity-84">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Independence tests</td>
<td><span class="math inline">\(O(n^k)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr class="even">
<td>Edge orientation</td>
<td><span class="math inline">\(O(n^2)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(k\)</span> is the maximum conditioning set size (typically small). Causal discovery scales poorly with dimensionality but can be optimized using sparsity assumptions.</p>
</section>
<section id="a-gentle-proof-why-it-works-80" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-80">A Gentle Proof (Why It Works)</h4>
<p>Under the Causal Markov Condition and Faithfulness Assumption:</p>
<ul>
<li>If <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are conditionally independent given <span class="math inline">\(S\)</span>, then there is no direct edge between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>.</li>
</ul>
<p>Thus, constraint-based methods can recover the Markov equivalence class of the true DAG. That is, all DAGs that encode the same conditional independencies.</p>
</section>
<section id="summary-table-13" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-13">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Causal Inference</td>
</tr>
<tr class="even">
<td>Goal</td>
<td>Discover causal structure (DAG) from data</td>
</tr>
<tr class="odd">
<td>Key Assumptions</td>
<td>Markov + Faithfulness</td>
</tr>
<tr class="even">
<td>Main Methods</td>
<td>PC (constraint), GES (score), NOTEARS (continuous)</td>
</tr>
<tr class="odd">
<td>Output</td>
<td>Causal DAG or CPDAG</td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>Cannot always orient all edges uniquely</td>
</tr>
</tbody>
</table>
<p>Causal DAG Discovery teaches algorithms <em>why</em> things happen, not just <em>when</em> they co-occur, turning data into cause, not coincidence.</p>
</section>
</section>
<section id="propensity-score-matching" class="level3">
<h3 class="anchored" data-anchor-id="propensity-score-matching">986. Propensity Score Matching</h3>
<p>Propensity Score Matching (PSM) is a cornerstone technique in causal inference, a way to simulate randomized experiments from observational data. It balances treatment and control groups by matching samples that have similar probabilities of receiving treatment, given observed covariates.</p>
<p>The key idea: if two individuals have the same <em>propensity</em> to receive a treatment, any difference in outcomes between them can be attributed to the treatment itself.</p>
<section id="what-problem-are-we-solving-85" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-85">What Problem Are We Solving?</h4>
<p>When treatment assignment isn’t random, comparing treated vs.&nbsp;untreated groups directly gives biased results. For example, in a medical study:</p>
<ul>
<li>Healthier patients may be more likely to receive a treatment.</li>
<li>So the observed outcome difference reflects both the treatment <em>and</em> their health status.</li>
</ul>
<p>We need a way to control for confounding variables <span class="math inline">\(X\)</span> to estimate the true <em>causal effect</em> of treatment <span class="math inline">\(T\)</span> on outcome <span class="math inline">\(Y\)</span>.</p>
</section>
<section id="the-core-idea-44" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-44">The Core Idea</h4>
<p>Define the propensity score as the probability of receiving treatment given the covariates: <span class="math display">\[
e(x) = P(T = 1 \mid X = x)
\]</span></p>
<p>The method proceeds in three steps:</p>
<ol type="1">
<li>Estimate <span class="math inline">\(e(x)\)</span>, usually with logistic regression or an ML classifier.</li>
<li>Match treated and untreated samples with similar <span class="math inline">\(e(x)\)</span>.</li>
<li>Compare outcomes <span class="math inline">\(Y\)</span> between matched pairs to estimate treatment effect.</li>
</ol>
<p>Once matched, the treated and control groups should be <em>statistically indistinguishable</em> in terms of <span class="math inline">\(X\)</span>.</p>
</section>
<section id="mathematical-foundation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-foundation">Mathematical Foundation</h4>
<p>Under the Strong Ignorability Assumption: <span class="math display">\[
Y(0), Y(1) \perp T \mid X, \quad 0 &lt; e(X) &lt; 1
\]</span></p>
<p>the average treatment effect (ATE) can be estimated as: <span class="math display">\[
\text{ATE} = E[Y(1) - Y(0)] = E_T\left[\frac{Y T}{e(X)} - \frac{Y (1 - T)}{1 - e(X)}\right]
\]</span></p>
<p>After matching, we compute the average treatment effect on the treated (ATT): <span class="math display">\[
\text{ATT} = E[Y(1) - Y(0) \mid T = 1]
\]</span></p>
<p>where <span class="math inline">\(Y(0)\)</span> for treated units is replaced by outcomes of matched control units.</p>
</section>
<section id="tiny-code-propensity-score-matching" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-propensity-score-matching">Tiny Code (Propensity Score Matching)</h4>
<p>Python example using logistic regression and nearest-neighbor matching</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> NearestNeighbors</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> propensity_score_matching(X, T, Y):</span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: estimate propensity scores</span></span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LogisticRegression()</span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a>    model.fit(X, T)</span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> model.predict_proba(X)[:, <span class="dv">1</span>]</span>
<span id="cb148-10"><a href="#cb148-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-11"><a href="#cb148-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: match treated to control by nearest propensity</span></span>
<span id="cb148-12"><a href="#cb148-12" aria-hidden="true" tabindex="-1"></a>    treated_idx <span class="op">=</span> np.where(T <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb148-13"><a href="#cb148-13" aria-hidden="true" tabindex="-1"></a>    control_idx <span class="op">=</span> np.where(T <span class="op">==</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb148-14"><a href="#cb148-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-15"><a href="#cb148-15" aria-hidden="true" tabindex="-1"></a>    nbrs <span class="op">=</span> NearestNeighbors(n_neighbors<span class="op">=</span><span class="dv">1</span>).fit(e[control_idx].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb148-16"><a href="#cb148-16" aria-hidden="true" tabindex="-1"></a>    _, match_idx <span class="op">=</span> nbrs.kneighbors(e[treated_idx].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb148-17"><a href="#cb148-17" aria-hidden="true" tabindex="-1"></a>    matched_control <span class="op">=</span> control_idx[match_idx.flatten()]</span>
<span id="cb148-18"><a href="#cb148-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-19"><a href="#cb148-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: compute ATT</span></span>
<span id="cb148-20"><a href="#cb148-20" aria-hidden="true" tabindex="-1"></a>    att <span class="op">=</span> np.mean(Y[treated_idx] <span class="op">-</span> Y[matched_control])</span>
<span id="cb148-21"><a href="#cb148-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> att, e</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-86" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-86">Why It Matters</h4>
<ul>
<li>Simulates randomization when true experiments are impossible.</li>
<li>Reduces selection bias by balancing covariates.</li>
<li>Works well in economics, medicine, and social science.</li>
<li>Provides interpretable causal estimates.</li>
</ul>
<p>But note: it cannot fix <em>hidden confounding</em>, only observed variables are adjusted.</p>
</section>
<section id="try-it-yourself-85" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-85">Try It Yourself</h4>
<ol type="1">
<li>Use a dataset with treatment <span class="math inline">\(T\)</span>, outcome <span class="math inline">\(Y\)</span>, and covariates <span class="math inline">\(X\)</span>.</li>
<li>Estimate propensity scores using logistic regression.</li>
<li>Match each treated unit to one control with similar <span class="math inline">\(e(x)\)</span>.</li>
<li>Compute the mean difference in outcomes.</li>
<li>Check covariate balance before and after matching.</li>
</ol>
</section>
<section id="test-cases-85" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-85">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 9%">
<col style="width: 20%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Method</th>
<th>Estimated ATT</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Synthetic Linear</td>
<td>PSM</td>
<td>+2.1</td>
<td>True effect = +2</td>
</tr>
<tr class="even">
<td>Lalonde</td>
<td>PSM</td>
<td>+1.6</td>
<td>Matches experimental benchmark</td>
</tr>
<tr class="odd">
<td>IHDP</td>
<td>PSM</td>
<td>+3.3</td>
<td>Close to true causal value</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-85" class="level4">
<h4 class="anchored" data-anchor-id="complexity-85">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Propensity model</td>
<td><span class="math inline">\(O(n d)\)</span></td>
<td><span class="math inline">\(O(d)\)</span></td>
</tr>
<tr class="even">
<td>Matching</td>
<td><span class="math inline">\(O(n \log n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr class="odd">
<td>ATT computation</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(d\)</span> = number of covariates. Overall, efficient and scalable for moderate datasets.</p>
</section>
<section id="a-gentle-proof-why-it-works-81" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-81">A Gentle Proof (Why It Works)</h4>
<p>The propensity score is a balancing score, given <span class="math inline">\(e(X)\)</span>, treatment assignment is independent of covariates: <span class="math display">\[
T \perp X \mid e(X)
\]</span> Therefore, matching on <span class="math inline">\(e(X)\)</span> ensures the treated and control groups are comparable, just as in randomized experiments.</p>
<p>By conditioning on <span class="math inline">\(e(X)\)</span> instead of full <span class="math inline">\(X\)</span>, PSM compresses high-dimensional adjustment into a single scalar, the probability of treatment.</p>
</section>
<section id="summary-table-14" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-14">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Causal Inference</td>
</tr>
<tr class="even">
<td>Key Assumption</td>
<td>No unobserved confounders</td>
</tr>
<tr class="odd">
<td>Core Step</td>
<td>Match on <span class="math inline">\(P(T=1 \mid X)\)</span></td>
</tr>
<tr class="even">
<td>Output</td>
<td>Average Treatment Effect (ATE/ATT)</td>
</tr>
<tr class="odd">
<td>Benefit</td>
<td>Mimics randomized control</td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>Sensitive to model misspecification</td>
</tr>
</tbody>
</table>
<p>Propensity Score Matching bridges the gap between <em>data we have</em> and <em>experiments we wish we’d run</em>, balancing worlds to reveal cause beneath correlation.</p>
</section>
</section>
<section id="instrumental-variable-estimation" class="level3">
<h3 class="anchored" data-anchor-id="instrumental-variable-estimation">987. Instrumental Variable Estimation</h3>
<p>Instrumental Variable (IV) Estimation is a classic method in causal inference used when there are unobserved confounders, variables that affect both the treatment and the outcome, making ordinary regression biased. It introduces an external variable, called an instrument, that influences the treatment but has <em>no direct effect</em> on the outcome except through that treatment.</p>
<p>This approach is essential in econometrics, epidemiology, and policy analysis when randomization is impossible.</p>
<section id="what-problem-are-we-solving-86" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-86">What Problem Are We Solving?</h4>
<p>Suppose we want to estimate the causal effect of treatment <span class="math inline">\(T\)</span> on outcome <span class="math inline">\(Y\)</span>. A simple linear model might look like: <span class="math display">\[
Y = \beta T + \gamma X + \epsilon
\]</span></p>
<p>If <span class="math inline">\(T\)</span> is correlated with the error term <span class="math inline">\(\epsilon\)</span> (due to hidden confounders), then ordinary least squares (OLS) gives a biased estimate of <span class="math inline">\(\beta\)</span>.</p>
<p>We need an external source of variation in <span class="math inline">\(T\)</span> that is not related to <span class="math inline">\(\epsilon\)</span>.</p>
</section>
<section id="the-core-idea-45" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-45">The Core Idea</h4>
<p>We find an instrumental variable <span class="math inline">\(Z\)</span> satisfying:</p>
<ol type="1">
<li>Relevance: <span class="math inline">\(Z\)</span> is correlated with treatment <span class="math inline">\(T\)</span> <span class="math display">\[\text{Cov}(Z, T) \neq 0\]</span></li>
<li>Exogeneity: <span class="math inline">\(Z\)</span> affects <span class="math inline">\(Y\)</span> only through <span class="math inline">\(T\)</span> <span class="math display">\[Z \perp \epsilon\]</span></li>
</ol>
<p>Then, <span class="math inline">\(Z\)</span> serves as a <em>natural experiment</em>, providing variation in <span class="math inline">\(T\)</span> that is uncorrelated with the confounders.</p>
<p>The key causal relationship becomes: <span class="math display">\[
Z \to T \to Y
\]</span> and no direct edge from <span class="math inline">\(Z\)</span> to <span class="math inline">\(Y\)</span>.</p>
</section>
<section id="two-stage-least-squares-2sls" class="level4">
<h4 class="anchored" data-anchor-id="two-stage-least-squares-2sls">Two-Stage Least Squares (2SLS)</h4>
<p>The standard procedure for IV estimation is Two-Stage Least Squares (2SLS).</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 90%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stage 1</td>
<td>Regress <span class="math inline">\(T\)</span> on <span class="math inline">\(Z\)</span> and covariates <span class="math inline">\(X\)</span>: <span class="math display">\[T = \pi_0 + \pi_1 Z + \pi_2 X + \nu\]</span></td>
</tr>
<tr class="even">
<td>Stage 2</td>
<td>Regress <span class="math inline">\(Y\)</span> on the predicted <span class="math inline">\(\hat{T}\)</span> from Stage 1: <span class="math display">\[Y = \beta_0 + \beta_1 \hat{T} + \beta_2 X + \eta\]</span></td>
</tr>
</tbody>
</table>
<p>The coefficient <span class="math inline">\(\beta_1\)</span> is the causal effect of <span class="math inline">\(T\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>Intuitively, <span class="math inline">\(\hat{T}\)</span> is the “clean” part of <span class="math inline">\(T\)</span> explained by <span class="math inline">\(Z\)</span>, stripped of confounding influence.</p>
</section>
<section id="mathematical-derivation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-derivation">Mathematical Derivation</h4>
<p>In the simple case without covariates: <span class="math display">\[
Y = \beta T + \epsilon
\]</span> and <span class="math inline">\(Z\)</span> is the instrument.</p>
<p>The IV estimator is given by: <span class="math display">\[
\hat{\beta}_{IV} = \frac{\text{Cov}(Z, Y)}{\text{Cov}(Z, T)}
\]</span></p>
<p>This ratio measures how much <span class="math inline">\(Y\)</span> changes for each change in <span class="math inline">\(T\)</span> induced by the instrument <span class="math inline">\(Z\)</span>.</p>
</section>
<section id="tiny-code-2sls-example" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-2sls-example">Tiny Code (2SLS Example)</h4>
<p>Python example using NumPy</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> iv_estimate(Y, T, Z):</span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stage 1: Predict treatment using instrument</span></span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> np.column_stack((np.ones(<span class="bu">len</span>(Z)), Z))</span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>    beta_stage1 <span class="op">=</span> np.linalg.inv(Z.T <span class="op">@</span> Z) <span class="op">@</span> Z.T <span class="op">@</span> T</span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>    T_hat <span class="op">=</span> Z <span class="op">@</span> beta_stage1</span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stage 2: Predict outcome using predicted treatment</span></span>
<span id="cb149-10"><a href="#cb149-10" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.column_stack((np.ones(<span class="bu">len</span>(T_hat)), T_hat))</span>
<span id="cb149-11"><a href="#cb149-11" aria-hidden="true" tabindex="-1"></a>    beta_stage2 <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> Y</span>
<span id="cb149-12"><a href="#cb149-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_stage2[<span class="dv">1</span>]  <span class="co"># causal coefficient</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example use case:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.random.randn(<span class="dv">1000</span>)</span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>Z <span class="op">+</span> np.random.randn(<span class="dv">1000</span>)</span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> <span class="dv">3</span><span class="op">*</span>T <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>np.random.randn(<span class="dv">1000</span>)</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a>beta_iv <span class="op">=</span> iv_estimate(Y, T, Z)</span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(beta_iv)  <span class="co"># ≈ 3 (true causal effect)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-87" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-87">Why It Matters</h4>
<ul>
<li><p>Solves endogeneity problems (when treatment correlates with noise).</p></li>
<li><p>Enables causal inference without full randomization.</p></li>
<li><p>Forms the basis of natural experiments, e.g.:</p>
<ul>
<li>Using draft lotteries as instruments for military service.</li>
<li>Using distance to college as an instrument for education.</li>
<li>Using rainfall as an instrument for agricultural production.</li>
</ul></li>
</ul>
</section>
<section id="try-it-yourself-86" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-86">Try It Yourself</h4>
<ol type="1">
<li>Choose a dataset where treatment <span class="math inline">\(T\)</span> is endogenous (correlated with confounders).</li>
<li>Identify an external variable <span class="math inline">\(Z\)</span> that affects <span class="math inline">\(T\)</span> but not <span class="math inline">\(Y\)</span> directly.</li>
<li>Run both OLS and IV (2SLS) regressions.</li>
<li>Compare estimates, IV should correct for bias.</li>
<li>Check instrument strength (e.g., first-stage F-statistic &gt; 10).</li>
</ol>
</section>
<section id="test-cases-86" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-86">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 25%">
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Instrument</th>
<th>True Effect</th>
<th>OLS Estimate</th>
<th>IV Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Education → Wage</td>
<td>Distance to college</td>
<td>+0.10</td>
<td>+0.18</td>
<td>+0.11</td>
</tr>
<tr class="even">
<td>Smoking → Birthweight</td>
<td>Cigarette tax</td>
<td>-200</td>
<td>-80</td>
<td>-190</td>
</tr>
<tr class="odd">
<td>Alcohol → Accidents</td>
<td>Beer tax</td>
<td>+1.5</td>
<td>+0.7</td>
<td>+1.4</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-86" class="level4">
<h4 class="anchored" data-anchor-id="complexity-86">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stage 1 regression</td>
<td><span class="math inline">\(O(n d^2)\)</span></td>
<td><span class="math inline">\(O(d^2)\)</span></td>
</tr>
<tr class="even">
<td>Stage 2 regression</td>
<td><span class="math inline">\(O(n d^2)\)</span></td>
<td><span class="math inline">\(O(d^2)\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(O(n d^2)\)</span></td>
<td><span class="math inline">\(O(d)\)</span></td>
</tr>
</tbody>
</table>
<p>Efficient and scalable for moderate-dimensional linear models.</p>
</section>
<section id="a-gentle-proof-why-it-works-82" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-82">A Gentle Proof (Why It Works)</h4>
<p>Since <span class="math inline">\(Z\)</span> is independent of <span class="math inline">\(\epsilon\)</span>, we can take expectations: <span class="math display">\[
E[Z Y] = E[Z (\beta T + \epsilon)] = \beta E[Z T]
\]</span> thus: <span class="math display">\[
\beta = \frac{E[Z Y]}{E[Z T]}
\]</span> which proves that <span class="math inline">\(\hat{\beta}_{IV}\)</span> consistently estimates the true causal effect.</p>
<p>This holds even when <span class="math inline">\(T\)</span> is correlated with <span class="math inline">\(\epsilon\)</span>, as long as <span class="math inline">\(Z\)</span> is not.</p>
</section>
<section id="summary-table-15" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-15">Summary Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Causal Inference (Endogeneity)</td>
</tr>
<tr class="even">
<td>Key Idea</td>
<td>Use exogenous variable <span class="math inline">\(Z\)</span> as a proxy for randomization</td>
</tr>
<tr class="odd">
<td>Core Equation</td>
<td><span class="math inline">\(\hat{\beta}_{IV} = \frac{\text{Cov}(Z, Y)}{\text{Cov}(Z, T)}\)</span></td>
</tr>
<tr class="even">
<td>Algorithm</td>
<td>Two-Stage Least Squares (2SLS)</td>
</tr>
<tr class="odd">
<td>Strength</td>
<td>Handles unobserved confounding</td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>Requires strong, valid instrument</td>
</tr>
</tbody>
</table>
<p>Instrumental Variable Estimation is the art of finding natural randomness in the world, turning everyday variation into experiments nature already ran for us.</p>
</section>
</section>
<section id="robust-optimization" class="level3">
<h3 class="anchored" data-anchor-id="robust-optimization">988. Robust Optimization</h3>
<p>Robust Optimization (RO) is a framework for making decisions that remain effective under uncertainty. Instead of assuming perfect knowledge of parameters, RO prepares for <em>the worst plausible case</em>, ensuring that a solution performs well even when reality deviates from the model.</p>
<p>It is widely used in finance, operations research, and machine learning to guard against estimation errors, data noise, and adversarial uncertainty.</p>
<section id="what-problem-are-we-solving-87" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-87">What Problem Are We Solving?</h4>
<p>Many optimization problems assume exact parameters, but in practice, coefficients, costs, or probabilities may be uncertain. For example:</p>
<ul>
<li>In portfolio optimization, expected returns are estimated from noisy data.</li>
<li>In supply chains, demand and cost forecasts fluctuate daily.</li>
<li>In machine learning, loss surfaces can shift across domains.</li>
</ul>
<p>Traditional optimization may fail catastrophically when inputs differ from expectations. Robust optimization explicitly models uncertainty and protects solutions against it.</p>
</section>
<section id="the-core-idea-46" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-46">The Core Idea</h4>
<p>Let’s start with a standard linear optimization problem:</p>
<p><span class="math display">\[
\min_x ; c^T x \quad \text{s.t.} \quad A x \le b
\]</span></p>
<p>In robust optimization, we treat coefficients as uncertain within specified <em>uncertainty sets</em>. For example, each coefficient <span class="math inline">\(a_{ij}\)</span> may vary within a known range: <span class="math display">\[
a_{ij} \in [\bar{a}*{ij} - \Delta*{ij}, ; \bar{a}*{ij} + \Delta*{ij}]
\]</span></p>
<p>Then we require the constraint to hold for all possible realizations: <span class="math display">\[
A x \le b, \quad \forall A \in \mathcal{U}
\]</span></p>
<p>where <span class="math inline">\(\mathcal{U}\)</span> is the uncertainty set (e.g., intervals, ellipsoids, or polyhedra).</p>
<p>This converts the problem into a min–max form: <span class="math display">\[
\min_x ; \max_{u \in \mathcal{U}} f(x, u)
\]</span></p>
<p>We seek a solution <span class="math inline">\(x^*\)</span> that minimizes the <em>worst-case loss</em>.</p>
</section>
<section id="types-of-uncertainty-sets" class="level4">
<h4 class="anchored" data-anchor-id="types-of-uncertainty-sets">Types of Uncertainty Sets</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 28%">
<col style="width: 43%">
<col style="width: 10%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Example</th>
<th>Mathematical Form</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Box</td>
<td>Each parameter varies within fixed bounds</td>
<td><span class="math inline">\(\mathcal{U} = {a:                                             | a_i - \bar{a}_i | \le \Delta_i}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Ellipsoidal</td>
<td>Uncertainty correlated across parameters</td>
<td><span class="math inline">\(\mathcal{U} = {a: (a - \bar{a})^T Q^{-1}(a - \bar{a}) \le 1}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Polyhedral</td>
<td>Constraints define feasible region</td>
<td><span class="math inline">\(\mathcal{U} = {a: F a \le g}\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Each choice yields a different computational complexity and conservativeness.</p>
</section>
<section id="example-robust-linear-program" class="level4">
<h4 class="anchored" data-anchor-id="example-robust-linear-program">Example: Robust Linear Program</h4>
<p>Nominal problem: <span class="math display">\[
\min_x ; c^T x \quad \text{s.t.} \quad A x \le b
\]</span></p>
<p>Robust version (box uncertainty): <span class="math display">\[
A = \bar{A} + \Delta, \quad |\Delta_{ij}| \le \rho_{ij}
\]</span></p>
<p>Then the robust constraint becomes: <span class="math display">\[
\bar{a}*i^T x + \sum_j \rho*{ij} |x_j| \le b_i
\]</span></p>
<p>This yields a convex reformulation that can be solved using standard LP or conic solvers.</p>
</section>
<section id="tiny-code-robust-lp-example" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-robust-lp-example">Tiny Code (Robust LP Example)</h4>
<p>Python example using <code>cvxpy</code></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxpy <span class="im">as</span> cp</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Nominal data</span></span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="dv">7</span>])</span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>], [<span class="fl">0.3</span>, <span class="fl">0.1</span>]])</span>
<span id="cb151-9"><a href="#cb151-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-10"><a href="#cb151-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision variable</span></span>
<span id="cb151-11"><a href="#cb151-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> cp.Variable(<span class="dv">2</span>, nonneg<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb151-12"><a href="#cb151-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-13"><a href="#cb151-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Robust constraints</span></span>
<span id="cb151-14"><a href="#cb151-14" aria-hidden="true" tabindex="-1"></a>constraints <span class="op">=</span> []</span>
<span id="cb151-15"><a href="#cb151-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(A.shape[<span class="dv">0</span>]):</span>
<span id="cb151-16"><a href="#cb151-16" aria-hidden="true" tabindex="-1"></a>    constraints.append(A[i] <span class="op">@</span> x <span class="op">+</span> np.<span class="bu">sum</span>(rho[i] <span class="op">*</span> cp.<span class="bu">abs</span>(x)) <span class="op">&lt;=</span> b[i])</span>
<span id="cb151-17"><a href="#cb151-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-18"><a href="#cb151-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Objective</span></span>
<span id="cb151-19"><a href="#cb151-19" aria-hidden="true" tabindex="-1"></a>objective <span class="op">=</span> cp.Minimize(c <span class="op">@</span> x)</span>
<span id="cb151-20"><a href="#cb151-20" aria-hidden="true" tabindex="-1"></a>problem <span class="op">=</span> cp.Problem(objective, constraints)</span>
<span id="cb151-21"><a href="#cb151-21" aria-hidden="true" tabindex="-1"></a>problem.solve()</span>
<span id="cb151-22"><a href="#cb151-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-23"><a href="#cb151-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Robust solution:"</span>, x.value)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This ensures constraints hold even when coefficients deviate by ±ρ.</p>
</section>
<section id="why-it-matters-88" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-88">Why It Matters</h4>
<ul>
<li>Guarantees feasibility under uncertainty.</li>
<li>Prevents overfitting to a single estimate.</li>
<li>Useful in adversarial ML, finance, logistics, and resource allocation.</li>
<li>Generalizes to robust regression, robust SVM, and robust neural training.</li>
</ul>
<p>In machine learning, for instance, robust optimization underpins adversarial training: <span class="math display">\[
\min_\theta \max_{\delta \in \mathcal{U}} L(f_\theta(x + \delta), y)
\]</span> which ensures the model performs well even under worst-case perturbations.</p>
</section>
<section id="try-it-yourself-87" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-87">Try It Yourself</h4>
<ol type="1">
<li>Create a simple LP model <span class="math inline">\(\min c^T x\)</span> subject to <span class="math inline">\(A x \le b\)</span>.</li>
<li>Add box uncertainty on <span class="math inline">\(A\)</span>.</li>
<li>Compare the nominal and robust solutions.</li>
<li>Increase uncertainty magnitude <span class="math inline">\(\rho\)</span>, observe how <span class="math inline">\(x^*\)</span> becomes more conservative.</li>
</ol>
</section>
<section id="test-cases-87" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-87">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 14%">
<col style="width: 33%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Uncertainty Set</th>
<th>Robust vs.&nbsp;Nominal Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Portfolio optimization</td>
<td>Ellipsoidal</td>
<td>Slightly higher risk-adjusted return</td>
<td>Less sensitive to estimation noise</td>
</tr>
<tr class="even">
<td>Supply chain</td>
<td>Box</td>
<td>Higher cost, fewer stockouts</td>
<td>Reliable under demand uncertainty</td>
</tr>
<tr class="odd">
<td>Scheduling</td>
<td>Polyhedral</td>
<td>Stable schedule</td>
<td>Resistant to delay variations</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-87" class="level4">
<h4 class="anchored" data-anchor-id="complexity-87">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LP (nominal)</td>
<td><span class="math inline">\(O(n^3)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr class="even">
<td>Robust LP</td>
<td><span class="math inline">\(O(n^3)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr class="odd">
<td>Robust SOCP</td>
<td><span class="math inline">\(O(n^3)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
</tbody>
</table>
<p>The complexity remains polynomial when <span class="math inline">\(\mathcal{U}\)</span> is convex (box or ellipsoidal).</p>
</section>
<section id="a-gentle-proof-why-it-works-83" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-83">A Gentle Proof (Why It Works)</h4>
<p>Let the uncertain constraint be: <span class="math display">\[
a^T x \le b, \quad \forall a \in \mathcal{U}
\]</span> Then for a convex uncertainty set <span class="math inline">\(\mathcal{U}\)</span>, the <em>worst-case</em> <span class="math inline">\(a\)</span> occurs at the boundary of <span class="math inline">\(\mathcal{U}\)</span>, turning the infinite constraints into finite deterministic constraints.</p>
<p>For box uncertainty: <span class="math display">\[
\max_{|a_i - \bar{a}_i| \le \rho_i} a^T x = \bar{a}^T x + \sum_i \rho_i |x_i|
\]</span> This converts the problem into a convex one, a robust version of the nominal LP.</p>
</section>
<section id="summary-table-16" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-16">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Optimization under uncertainty</td>
</tr>
<tr class="even">
<td>Key Idea</td>
<td>Protect against worst-case input</td>
</tr>
<tr class="odd">
<td>Typical Form</td>
<td><span class="math inline">\(\min_x \max_{u \in \mathcal{U}} f(x, u)\)</span></td>
</tr>
<tr class="even">
<td>Methods</td>
<td>Box, Ellipsoidal, Polyhedral RO</td>
</tr>
<tr class="odd">
<td>Benefit</td>
<td>Reliable, conservative solutions</td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>May be overly cautious</td>
</tr>
</tbody>
</table>
<p>Robust Optimization transforms uncertainty from a threat into a design principle, a way to plan for chaos, not fear it.</p>
</section>
</section>
<section id="distributionally-robust-optimization" class="level3">
<h3 class="anchored" data-anchor-id="distributionally-robust-optimization">989. Distributionally Robust Optimization</h3>
<p>Distributionally Robust Optimization (DRO) is a powerful generalization of robust optimization that prepares not just for worst-case <em>parameters</em>, but for worst-case <em>distributions</em> of uncertainty. Instead of assuming that data come from a known probability distribution, DRO assumes the true distribution may lie within an <em>ambiguity set</em> around a nominal estimate, and seeks decisions that perform best under the worst plausible distribution.</p>
<p>This bridges classical stochastic optimization and adversarial learning, and has deep ties to modern machine learning, fairness, and generalization.</p>
<section id="what-problem-are-we-solving-88" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-88">What Problem Are We Solving?</h4>
<p>In many real-world tasks, we train or optimize under data drawn from an unknown distribution <span class="math inline">\(P\)</span>, but we only have a finite empirical sample <span class="math inline">\(\hat{P}*n\)</span>. A standard stochastic optimization problem is: <span class="math display">\[
\min*{x} ; E_P[L(x, \xi)]
\]</span></p>
<p>But <span class="math inline">\(\hat{P}_n\)</span> is just an estimate of <span class="math inline">\(P\)</span>. If the training data are biased, incomplete, or nonstationary, the solution may perform poorly on future data.</p>
<p>DRO addresses this by considering all distributions <em>close</em> to <span class="math inline">\(\hat{P}*n\)</span>, and minimizing the worst-case expected loss: <span class="math display">\[
\min*{x} ; \max_{Q \in \mathcal{P}} E_Q[L(x, \xi)]
\]</span></p>
<p>where <span class="math inline">\(\mathcal{P}\)</span> is an ambiguity set of plausible distributions.</p>
</section>
<section id="the-core-idea-47" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-47">The Core Idea</h4>
<p>The ambiguity set <span class="math inline">\(\mathcal{P}\)</span> defines how much deviation from the nominal distribution is allowed. Common choices include:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 51%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Definition</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\phi\)</span>-divergence</td>
<td><span class="math inline">\(\mathcal{P} = { Q : D_\phi(Q \Vert \hat{P}) \le \rho }\)</span></td>
<td>Includes KL, <span class="math inline">\(\chi^2\)</span>, and total variation</td>
</tr>
<tr class="even">
<td>Wasserstein ball</td>
<td><span class="math inline">\(\mathcal{P} = { Q : W(Q, \hat{P}) \le \epsilon }\)</span></td>
<td>Distance in probability metric space</td>
</tr>
<tr class="odd">
<td>Moment-based</td>
<td><span class="math inline">\(\mathcal{P} = { Q : E_Q[\xi] = \mu, \text{Var}_Q[\xi] \le \Sigma }\)</span></td>
<td>Controls moments rather than shape</td>
</tr>
</tbody>
</table>
<p>Then, DRO finds a decision <span class="math inline">\(x^*\)</span> that minimizes <em>expected loss</em> under the worst distribution <span class="math inline">\(Q\)</span> within <span class="math inline">\(\mathcal{P}\)</span>.</p>
</section>
<section id="mathematical-form" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-form">Mathematical Form</h4>
<p>Given a loss function <span class="math inline">\(L(x, \xi)\)</span> and ambiguity set <span class="math inline">\(\mathcal{P}\)</span>: <span class="math display">\[
\min_x ; \max_{Q \in \mathcal{P}} E_Q[L(x, \xi)]
\]</span></p>
<p>The inner maximization over <span class="math inline">\(Q\)</span> represents an adversarial “nature” choosing the worst-case distribution within <span class="math inline">\(\mathcal{P}\)</span>.</p>
<p>Under mild convexity assumptions, this problem is dually equivalent to a regularized empirical risk minimization problem: <span class="math display">\[
\min_x ; E_{\hat{P}}[L(x, \xi)] + \lambda , \Omega(x)
\]</span> where <span class="math inline">\(\Omega(x)\)</span> is a regularizer depending on the ambiguity type.</p>
<p>Thus, DRO unifies robustness and regularization, a core insight in modern ML.</p>
</section>
<section id="example-dro-with-wasserstein-distance" class="level4">
<h4 class="anchored" data-anchor-id="example-dro-with-wasserstein-distance">Example: DRO with Wasserstein Distance</h4>
<p>The Wasserstein ambiguity set is defined as: <span class="math display">\[
\mathcal{P} = { Q : W(Q, \hat{P}_n) \le \epsilon }
\]</span> where <span class="math inline">\(W\)</span> is the optimal transport (Earth mover’s) distance.</p>
<p>The DRO objective becomes: <span class="math display">\[
\min_x \max_{W(Q, \hat{P}_n) \le \epsilon} E_Q[L(x, \xi)]
\]</span></p>
<p>This has a dual formulation: <span class="math display">\[
\min_x ; E_{\hat{P}*n}[L(x, \xi)] + \epsilon \cdot | \nabla_x L(x, \xi) |
\]</span> which resembles <em>adversarial training</em> in neural networks.</p>
</section>
<section id="tiny-code-wasserstein-dro-example" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-wasserstein-dro-example">Tiny Code (Wasserstein DRO Example)</h4>
<p>Python example using convex loss</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wasserstein_dro_loss(L, grad_L, x, epsilon):</span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Approximate DRO regularized loss."""</span></span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a>    empirical_loss <span class="op">=</span> np.mean(L(x))</span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a>    grad_norm <span class="op">=</span> np.linalg.norm(np.mean(grad_L(x)), <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> empirical_loss <span class="op">+</span> epsilon <span class="op">*</span> grad_norm</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This simple structure is analogous to: <span class="math display">\[
\text{Loss}_{DRO} = \text{EmpiricalLoss} + \epsilon \times \text{GradientNorm}
\]</span> — a robustified loss against distributional shifts.</p>
</section>
<section id="why-it-matters-89" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-89">Why It Matters</h4>
<ul>
<li>Provides robust generalization beyond training data.</li>
<li>Protects against data shift, bias, and sampling noise.</li>
<li>Bridges optimization and statistical learning theory.</li>
<li>Interpretable as adversarial regularization in deep learning.</li>
</ul>
<p>In machine learning, DRO yields models that perform consistently across different domains, crucial for fairness, out-of-distribution generalization, and reliable AI.</p>
</section>
<section id="try-it-yourself-88" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-88">Try It Yourself</h4>
<ol type="1">
<li>Train a linear regression model on a biased dataset.</li>
<li>Compute a Wasserstein-based DRO objective.</li>
<li>Tune <span class="math inline">\(\epsilon\)</span>, observe that small <span class="math inline">\(\epsilon\)</span> improves robustness, large <span class="math inline">\(\epsilon\)</span> yields overly conservative solutions.</li>
<li>Compare test performance under domain shift.</li>
</ol>
</section>
<section id="test-cases-88" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-88">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 14%">
<col style="width: 24%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Ambiguity Set</th>
<th>Effect</th>
<th>Outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logistic regression</td>
<td>KL-divergence</td>
<td>Regularizes weights</td>
<td>Improved generalization</td>
</tr>
<tr class="even">
<td>Portfolio optimization</td>
<td>Wasserstein</td>
<td>Hedge against shocks</td>
<td>Reduced volatility</td>
</tr>
<tr class="odd">
<td>Image classification</td>
<td>Wasserstein</td>
<td>Adversarial robustness</td>
<td>Stronger against FGSM/PGD attacks</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-88" class="level4">
<h4 class="anchored" data-anchor-id="complexity-88">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Inner distributional optimization</td>
<td><span class="math inline">\(O(n)\)</span> – <span class="math inline">\(O(n^2)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr class="even">
<td>Dual formulation</td>
<td><span class="math inline">\(O(d^3)\)</span></td>
<td><span class="math inline">\(O(d^2)\)</span></td>
</tr>
<tr class="odd">
<td>Gradient-based updates</td>
<td><span class="math inline">\(O(n d)\)</span></td>
<td><span class="math inline">\(O(d)\)</span></td>
</tr>
</tbody>
</table>
<p>Modern DRO solvers leverage convex duality or stochastic gradient methods for scalability.</p>
</section>
<section id="a-gentle-proof-why-it-works-84" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-84">A Gentle Proof (Why It Works)</h4>
<p>By strong duality, for many <span class="math inline">\(\mathcal{P}\)</span>, the inner supremum admits a closed-form expression: <span class="math display">\[
\max_{Q \in \mathcal{P}} E_Q[L(x, \xi)] = E_{\hat{P}_n}[L(x, \xi)] + \text{robust penalty}
\]</span> where the penalty corresponds to the <em>worst perturbation</em> consistent with <span class="math inline">\(\mathcal{P}\)</span>.</p>
<p>This means DRO implicitly regularizes the solution to remain stable under data perturbations, connecting to Tikhonov and adversarial regularization.</p>
</section>
<section id="summary-table-17" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-17">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Optimization under distributional uncertainty</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Minimize loss under worst plausible data distribution</td>
</tr>
<tr class="odd">
<td>Key Form</td>
<td><span class="math inline">\(\min_x \max_{Q \in \mathcal{P}} E_Q[L(x, \xi)]\)</span></td>
</tr>
<tr class="even">
<td>Common Sets</td>
<td>Wasserstein, KL, <span class="math inline">\(\chi^2\)</span>, Moment-based</td>
</tr>
<tr class="odd">
<td>Dual Form</td>
<td>Regularized empirical risk</td>
</tr>
<tr class="even">
<td>Applications</td>
<td>ML robustness, fairness, finance</td>
</tr>
<tr class="odd">
<td>Limitation</td>
<td>Can be overly conservative if <span class="math inline">\(\mathcal{P}\)</span> too large</td>
</tr>
</tbody>
</table>
<p>Distributionally Robust Optimization is the mathematical heart of trustworthy AI, ensuring our models don’t just fit the data they see, but also withstand the worlds they haven’t yet encountered.</p>
</section>
</section>
<section id="counterfactual-fairness" class="level3">
<h3 class="anchored" data-anchor-id="counterfactual-fairness">990. Counterfactual Fairness</h3>
<p>Counterfactual Fairness is a fairness criterion grounded in causal reasoning. It asks: <em>Would this decision have been the same if the individual had belonged to a different demographic group, all else being equal?</em></p>
<p>Rather than relying on correlations between protected attributes (like race or gender) and outcomes, counterfactual fairness uses causal models to reason about how changing sensitive attributes would affect predictions.</p>
<section id="what-problem-are-we-solving-89" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-89">What Problem Are We Solving?</h4>
<p>Machine learning models often encode biases from historical data. Even if sensitive attributes are excluded, proxies or correlated variables can still leak bias.</p>
<p>For example:</p>
<ul>
<li>A credit scoring model may discriminate based on postal codes correlated with race.</li>
<li>A hiring model may favor certain universities that correlate with socioeconomic status.</li>
</ul>
<p>We want models that make the same decision for the same person, even if we <em>hypothetically change</em> their protected attribute, that is, under a <em>counterfactual world</em>.</p>
</section>
<section id="the-core-idea-48" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-48">The Core Idea</h4>
<p>Let:</p>
<ul>
<li><span class="math inline">\(A\)</span>: protected attribute (e.g., gender, race)</li>
<li><span class="math inline">\(X\)</span>: observed features (e.g., education, income)</li>
<li><span class="math inline">\(Y\)</span>: outcome (e.g., loan approval)</li>
<li><span class="math inline">\(\hat{Y}\)</span>: model prediction</li>
</ul>
<p>A model is counterfactually fair if: <span class="math display">\[
P(\hat{Y}*{A \leftarrow a}(U) = y \mid X = x, A = a) = P(\hat{Y}*{A \leftarrow a'}(U) = y \mid X = x, A = a)
\]</span> for all possible values <span class="math inline">\(a, a'\)</span> of the protected attribute.</p>
<p>Here, <span class="math inline">\(\hat{Y}_{A \leftarrow a}(U)\)</span> represents the <em>counterfactual prediction</em> if we were to intervene and set <span class="math inline">\(A = a\)</span> in the causal model, keeping all other latent factors <span class="math inline">\(U\)</span> fixed.</p>
<p>Intuitively:</p>
<blockquote class="blockquote">
<p>A model is fair if the prediction for an individual would not change in a hypothetical world where only their sensitive attribute were different.</p>
</blockquote>
</section>
<section id="causal-graph-perspective" class="level4">
<h4 class="anchored" data-anchor-id="causal-graph-perspective">Causal Graph Perspective</h4>
<p>The causal structure is modeled using a Structural Causal Model (SCM):</p>
<p><span class="math display">\[
A \to X \to \hat{Y}, \quad A \to \hat{Y}
\]</span></p>
<p>Counterfactual fairness involves removing or blocking the <em>unfair causal paths</em> from <span class="math inline">\(A\)</span> to <span class="math inline">\(\hat{Y}\)</span> that do not pass through legitimate mediators.</p>
<p>For example:</p>
<ul>
<li>Path <span class="math inline">\(A \to X \to \hat{Y}\)</span> (e.g., education) may be acceptable.</li>
<li>Direct path <span class="math inline">\(A \to \hat{Y}\)</span> (e.g., race influencing prediction) is <em>unfair</em>.</li>
</ul>
</section>
<section id="example-scenario" class="level4">
<h4 class="anchored" data-anchor-id="example-scenario">Example Scenario</h4>
<p>Hiring decision model Variables:</p>
<ul>
<li><span class="math inline">\(A\)</span>: gender</li>
<li><span class="math inline">\(X\)</span>: years of experience, education</li>
<li><span class="math inline">\(\hat{Y}\)</span>: predicted suitability</li>
</ul>
<p>If gender affects education (due to structural inequality) but not intrinsic ability, we may wish to remove the direct effect <span class="math inline">\(A \to \hat{Y}\)</span> but keep the mediated path <span class="math inline">\(A \to X \to \hat{Y}\)</span>.</p>
<p>We simulate the counterfactual:</p>
<ul>
<li>Keep latent ability <span class="math inline">\(U\)</span> constant.</li>
<li>Change <span class="math inline">\(A\)</span> from male to female.</li>
<li>Recompute <span class="math inline">\(\hat{Y}\)</span> under the modified graph.</li>
</ul>
<p>If <span class="math inline">\(\hat{Y}\)</span> changes, unfairness exists.</p>
</section>
<section id="tiny-code-simplified-counterfactual-simulation" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-simplified-counterfactual-simulation">Tiny Code (Simplified Counterfactual Simulation)</h4>
<p>Python pseudocode using DoWhy</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dowhy</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dowhy <span class="im">import</span> CausalModel</span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example data with bias</span></span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"A"</span>: [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],        <span class="co"># gender</span></span>
<span id="cb153-7"><a href="#cb153-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"X"</span>: [<span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">10</span>],    <span class="co"># experience</span></span>
<span id="cb153-8"><a href="#cb153-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Y"</span>: [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]         <span class="co"># hiring decision</span></span>
<span id="cb153-9"><a href="#cb153-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb153-10"><a href="#cb153-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-11"><a href="#cb153-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CausalModel(</span>
<span id="cb153-12"><a href="#cb153-12" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>data,</span>
<span id="cb153-13"><a href="#cb153-13" aria-hidden="true" tabindex="-1"></a>    treatment<span class="op">=</span><span class="st">"A"</span>,</span>
<span id="cb153-14"><a href="#cb153-14" aria-hidden="true" tabindex="-1"></a>    outcome<span class="op">=</span><span class="st">"Y"</span>,</span>
<span id="cb153-15"><a href="#cb153-15" aria-hidden="true" tabindex="-1"></a>    common_causes<span class="op">=</span>[<span class="st">"X"</span>]</span>
<span id="cb153-16"><a href="#cb153-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb153-17"><a href="#cb153-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-18"><a href="#cb153-18" aria-hidden="true" tabindex="-1"></a>identified_estimand <span class="op">=</span> model.identify_effect()</span>
<span id="cb153-19"><a href="#cb153-19" aria-hidden="true" tabindex="-1"></a>estimate <span class="op">=</span> model.estimate_effect(identified_estimand, method_name<span class="op">=</span><span class="st">"backdoor.linear_regression"</span>)</span>
<span id="cb153-20"><a href="#cb153-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-21"><a href="#cb153-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated causal effect of gender:"</span>, estimate.value)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You can then perform a do-intervention (<code>do(A=0)</code> vs.&nbsp;<code>do(A=1)</code>) to test whether <span class="math inline">\(\hat{Y}\)</span> changes under the counterfactual scenario.</p>
</section>
<section id="why-it-matters-90" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-90">Why It Matters</h4>
<ul>
<li>Ensures fairness grounded in causality, not mere correlation.</li>
<li>Detects hidden discrimination that persists even after removing <span class="math inline">\(A\)</span> from features.</li>
<li>Encourages interpretable fairness definitions aligned with legal and ethical reasoning.</li>
</ul>
<p>Applications include:</p>
<ul>
<li>Credit risk and loan approvals</li>
<li>Hiring and promotion models</li>
<li>Recidivism risk assessment</li>
<li>Healthcare prioritization</li>
</ul>
</section>
<section id="try-it-yourself-89" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-89">Try It Yourself</h4>
<ol type="1">
<li>Build a causal graph of your dataset (e.g., using domain knowledge).</li>
<li>Identify paths from <span class="math inline">\(A\)</span> to <span class="math inline">\(\hat{Y}\)</span>.</li>
<li>Remove or neutralize unfair paths.</li>
<li>Test whether <span class="math inline">\(\hat{Y}\)</span> changes when you intervene on <span class="math inline">\(A\)</span>.</li>
<li>Train a model using fair representations or invariant features.</li>
</ol>
</section>
<section id="test-cases-89" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-89">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 22%">
<col style="width: 30%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Protected Attribute</th>
<th>Technique</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Hiring</td>
<td>Gender</td>
<td>Path blocking (A → Y)</td>
<td>Equalized predictions</td>
</tr>
<tr class="even">
<td>Lending</td>
<td>Race</td>
<td>Counterfactual reweighting</td>
<td>Reduced bias</td>
</tr>
<tr class="odd">
<td>Healthcare</td>
<td>Age</td>
<td>Causal adjustment</td>
<td>Stable outcomes across groups</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-89" class="level4">
<h4 class="anchored" data-anchor-id="complexity-89">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Time</th>
<th>Space</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Causal graph construction</td>
<td><span class="math inline">\(O(n^2)\)</span></td>
<td><span class="math inline">\(O(n^2)\)</span></td>
</tr>
<tr class="even">
<td>Intervention computation</td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr class="odd">
<td>Counterfactual simulation</td>
<td><span class="math inline">\(O(m n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
</tbody>
</table>
<p>Most modern frameworks (like DoWhy, EconML, or CausalML) automate these steps efficiently.</p>
</section>
<section id="a-gentle-proof-why-it-works-85" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-85">A Gentle Proof (Why It Works)</h4>
<p>Under a structural causal model: <span class="math display">\[
\hat{Y} = f(A, X, U)
\]</span> where <span class="math inline">\(U\)</span> are latent background variables independent of <span class="math inline">\(A\)</span>.</p>
<p>If <span class="math inline">\(\hat{Y}\)</span> is counterfactually fair, then: <span class="math display">\[
f(a, X_{A \leftarrow a}, U) = f(a', X_{A \leftarrow a'}, U)
\]</span> for all <span class="math inline">\(a, a'\)</span> and fixed <span class="math inline">\(U\)</span>.</p>
<p>This implies <span class="math inline">\(\hat{Y}\)</span> depends only on <em>legitimate features</em> and not on <span class="math inline">\(A\)</span> or its unfair descendants.</p>
</section>
<section id="summary-table-18" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-18">Summary Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 85%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Causal fairness in decision systems</td>
</tr>
<tr class="even">
<td>Key Idea</td>
<td>Prediction invariant under counterfactual change in sensitive attribute</td>
</tr>
<tr class="odd">
<td>Formal Criterion</td>
<td><span class="math inline">\(P(\hat{Y}*{A \leftarrow a} = y \mid X, A=a) = P(\hat{Y}*{A \leftarrow a'} = y \mid X, A=a)\)</span></td>
</tr>
<tr class="even">
<td>Implementation</td>
<td>Structural Causal Models + do-calculus</td>
</tr>
<tr class="odd">
<td>Benefit</td>
<td>Detects and corrects hidden bias</td>
</tr>
<tr class="even">
<td>Limitation</td>
<td>Requires causal knowledge and assumptions</td>
</tr>
</tbody>
</table>
<p>Counterfactual Fairness is the gold standard of algorithmic fairness, it asks not whether two groups are treated equally <em>on average</em>, but whether each <em>individual</em> would be treated the same in every possible world.</p>
</section>
</section>
</section>
<section id="section-100.-ai-planning-search-and-learning-systems" class="level1">
<h1>Section 100. AI Planning, Search and Learning Systems</h1>
<section id="breadth-first-search-bfs" class="level3">
<h3 class="anchored" data-anchor-id="breadth-first-search-bfs">991. Breadth-First Search (BFS)</h3>
<p>Breadth-First Search (BFS) is one of the most fundamental graph traversal algorithms. It explores a graph level by level, discovering all vertices at distance 1 before distance 2, and so on. This makes BFS ideal for finding the shortest path in unweighted graphs or performing systematic exploration of networks, trees, or state spaces.</p>
<section id="what-problem-are-we-solving-90" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-90">What Problem Are We Solving?</h4>
<p>We want to traverse or search through a graph, visiting all reachable vertices from a given start node.</p>
<p>Given a graph <span class="math inline">\(G = (V, E)\)</span> and a starting node <span class="math inline">\(s\)</span>, BFS finds:</p>
<ul>
<li>All nodes reachable from <span class="math inline">\(s\)</span>.</li>
<li>The shortest distance <span class="math inline">\(d(s, v)\)</span> for each vertex <span class="math inline">\(v\)</span>.</li>
<li>The parent or predecessor relationships (to reconstruct paths).</li>
</ul>
<p>Formally, BFS explores vertices in increasing order of distance from the source.</p>
</section>
<section id="the-core-idea-49" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-49">The Core Idea</h4>
<ol type="1">
<li>Maintain a queue of vertices to explore.</li>
<li>Start with the source node and mark it as visited.</li>
<li>Repeatedly dequeue a node, visit all its unvisited neighbors, and enqueue them.</li>
<li>Continue until the queue is empty.</li>
</ol>
<p>The algorithm guarantees that each vertex is visited once, and edges are relaxed in breadth-first order.</p>
</section>
<section id="algorithm" class="level4">
<h4 class="anchored" data-anchor-id="algorithm">Algorithm</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 95%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize queue <span class="math inline">\(Q\)</span> with start node <span class="math inline">\(s\)</span>.</td>
</tr>
<tr class="even">
<td>2</td>
<td>Mark <span class="math inline">\(s\)</span> as visited and set <span class="math inline">\(d(s) = 0\)</span>.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>While <span class="math inline">\(Q\)</span> not empty:</td>
</tr>
<tr class="even">
<td></td>
<td>&nbsp;&nbsp;a. Pop vertex <span class="math inline">\(u\)</span> from <span class="math inline">\(Q\)</span>.</td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;b. For each neighbor <span class="math inline">\(v\)</span> of <span class="math inline">\(u\)</span>:</td>
</tr>
<tr class="even">
<td></td>
<td>&nbsp;&nbsp;&nbsp;&nbsp;i. If <span class="math inline">\(v\)</span> not visited: mark visited, set <span class="math inline">\(d(v) = d(u) + 1\)</span>, enqueue <span class="math inline">\(v\)</span>.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-classic-bfs" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-classic-bfs">Tiny Code (Classic BFS)</h4>
<p>C Example</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb154"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdio.h&gt;</span></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdbool.h&gt;</span></span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#define N </span><span class="dv">100</span></span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> adj<span class="op">[</span>N<span class="op">][</span>N<span class="op">];</span>   <span class="co">// adjacency matrix</span></span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a><span class="dt">bool</span> visited<span class="op">[</span>N<span class="op">];</span></span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> queue<span class="op">[</span>N<span class="op">];</span></span>
<span id="cb154-8"><a href="#cb154-8" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> front <span class="op">=</span> <span class="dv">0</span><span class="op">,</span> rear <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb154-9"><a href="#cb154-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-10"><a href="#cb154-10" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> enqueue<span class="op">(</span><span class="dt">int</span> v<span class="op">)</span> <span class="op">{</span> queue<span class="op">[</span>rear<span class="op">++]</span> <span class="op">=</span> v<span class="op">;</span> <span class="op">}</span></span>
<span id="cb154-11"><a href="#cb154-11" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> dequeue<span class="op">()</span> <span class="op">{</span> <span class="cf">return</span> queue<span class="op">[</span>front<span class="op">++];</span> <span class="op">}</span></span>
<span id="cb154-12"><a href="#cb154-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-13"><a href="#cb154-13" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> bfs<span class="op">(</span><span class="dt">int</span> start<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb154-14"><a href="#cb154-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> i<span class="op">++)</span> visited<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="kw">false</span><span class="op">;</span></span>
<span id="cb154-15"><a href="#cb154-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-16"><a href="#cb154-16" aria-hidden="true" tabindex="-1"></a>    visited<span class="op">[</span>start<span class="op">]</span> <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb154-17"><a href="#cb154-17" aria-hidden="true" tabindex="-1"></a>    enqueue<span class="op">(</span>start<span class="op">);</span></span>
<span id="cb154-18"><a href="#cb154-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-19"><a href="#cb154-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span>front <span class="op">&lt;</span> rear<span class="op">)</span> <span class="op">{</span></span>
<span id="cb154-20"><a href="#cb154-20" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> u <span class="op">=</span> dequeue<span class="op">();</span></span>
<span id="cb154-21"><a href="#cb154-21" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">"</span><span class="sc">%d</span><span class="st"> "</span><span class="op">,</span> u<span class="op">);</span></span>
<span id="cb154-22"><a href="#cb154-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> v <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> v <span class="op">&lt;</span> n<span class="op">;</span> v<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb154-23"><a href="#cb154-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>adj<span class="op">[</span>u<span class="op">][</span>v<span class="op">]</span> <span class="op">&amp;&amp;</span> <span class="op">!</span>visited<span class="op">[</span>v<span class="op">])</span> <span class="op">{</span></span>
<span id="cb154-24"><a href="#cb154-24" aria-hidden="true" tabindex="-1"></a>                visited<span class="op">[</span>v<span class="op">]</span> <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb154-25"><a href="#cb154-25" aria-hidden="true" tabindex="-1"></a>                enqueue<span class="op">(</span>v<span class="op">);</span></span>
<span id="cb154-26"><a href="#cb154-26" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb154-27"><a href="#cb154-27" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb154-28"><a href="#cb154-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb154-29"><a href="#cb154-29" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Python Example</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bfs(graph, start):</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>    visited <span class="op">=</span> <span class="bu">set</span>([start])</span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a>    queue <span class="op">=</span> deque([start])</span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> []</span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> queue:</span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> queue.popleft()</span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a>        order.append(u)</span>
<span id="cb155-11"><a href="#cb155-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> v <span class="kw">in</span> graph[u]:</span>
<span id="cb155-12"><a href="#cb155-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb155-13"><a href="#cb155-13" aria-hidden="true" tabindex="-1"></a>                visited.add(v)</span>
<span id="cb155-14"><a href="#cb155-14" aria-hidden="true" tabindex="-1"></a>                queue.append(v)</span>
<span id="cb155-15"><a href="#cb155-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> order</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example usage:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> {</span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'A'</span>: [<span class="st">'B'</span>, <span class="st">'C'</span>],</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'B'</span>: [<span class="st">'A'</span>, <span class="st">'D'</span>, <span class="st">'E'</span>],</span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'C'</span>: [<span class="st">'A'</span>, <span class="st">'F'</span>],</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'D'</span>: [<span class="st">'B'</span>],</span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'E'</span>: [<span class="st">'B'</span>, <span class="st">'F'</span>],</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'F'</span>: [<span class="st">'C'</span>, <span class="st">'E'</span>]</span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-10"><a href="#cb156-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bfs(G, <span class="st">'A'</span>))  <span class="co"># ['A', 'B', 'C', 'D', 'E', 'F']</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-91" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-91">Why It Matters</h4>
<ul>
<li>Shortest paths in unweighted graphs.</li>
<li>Connectivity checks in graphs and networks.</li>
<li>Component detection in undirected graphs.</li>
<li>Layered search in AI planning or puzzles.</li>
</ul>
<p>BFS is the foundation for many algorithms:</p>
<ul>
<li>Dijkstra (with weights)</li>
<li>Ford–Fulkerson (augmenting paths)</li>
<li>Edmonds–Karp (max flow)</li>
<li>Topological order detection in DAGs</li>
</ul>
</section>
<section id="try-it-yourself-90" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-90">Try It Yourself</h4>
<ol type="1">
<li>Implement BFS using an adjacency list and verify traversal order.</li>
<li>Compute distance of every vertex from the source.</li>
<li>Use BFS to find the shortest path in an unweighted maze.</li>
<li>Modify BFS to count connected components in an undirected graph.</li>
</ol>
</section>
<section id="test-cases-90" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-90">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Graph</th>
<th>Start</th>
<th>Expected Order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A–B–C–D</td>
<td>A</td>
<td>A, B, C, D</td>
</tr>
<tr class="even">
<td>Triangle A–B–C–A</td>
<td>B</td>
<td>B, A, C</td>
</tr>
<tr class="odd">
<td>Star (1 connected to 2,3,4)</td>
<td>1</td>
<td>1, 2, 3, 4</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-90" class="level4">
<h4 class="anchored" data-anchor-id="complexity-90">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(V + E)\)</span></td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(V)\)</span> (queue + visited array)</td>
</tr>
</tbody>
</table>
<p>Each vertex and edge is processed once, giving linear time in graph size.</p>
</section>
<section id="a-gentle-proof-why-it-works-86" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-86">A Gentle Proof (Why It Works)</h4>
<p>At every iteration, BFS dequeues the vertex with minimum distance from the source. Since it visits neighbors in order of increasing distance, the first time a vertex is discovered corresponds to the shortest path from the source.</p>
<p>Formally, by induction:</p>
<ul>
<li>Base: <span class="math inline">\(d(s) = 0\)</span>.</li>
<li>Step: if <span class="math inline">\(u\)</span> has correct distance <span class="math inline">\(d(u)\)</span>, all unvisited neighbors get <span class="math inline">\(d(u) + 1\)</span>, preserving order.</li>
</ul>
<p>Hence, BFS produces correct shortest distances in unweighted graphs.</p>
</section>
<section id="summary-table-19" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-19">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Graph traversal</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Explore neighbors layer by layer</td>
</tr>
<tr class="odd">
<td>Data Structure</td>
<td>Queue (FIFO)</td>
</tr>
<tr class="even">
<td>Guarantee</td>
<td>Shortest path in unweighted graphs</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(V + E)\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(V)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Search, connectivity, shortest paths</td>
</tr>
</tbody>
</table>
<p>Breadth-First Search is the algorithmic heartbeat of exploration, calm, orderly, and fair: visiting every neighbor before wandering deeper into the graph’s unknown.</p>
</section>
</section>
<section id="depth-first-search-dfs" class="level3">
<h3 class="anchored" data-anchor-id="depth-first-search-dfs">992. Depth-First Search (DFS)</h3>
<p>Depth-First Search (DFS) explores a graph by going as deep as possible along one branch before backtracking. It’s the opposite of BFS, instead of expanding breadth-first layers, DFS dives downward through edges, uncovering hidden structures like paths, cycles, and connectivity patterns.</p>
<section id="what-problem-are-we-solving-91" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-91">What Problem Are We Solving?</h4>
<p>Given a graph <span class="math inline">\(G = (V, E)\)</span> and a starting vertex <span class="math inline">\(s\)</span>, we want to explore all vertices reachable from <span class="math inline">\(s\)</span>. Unlike BFS, which guarantees shortest paths, DFS focuses on complete exploration, ideal for:</p>
<ul>
<li>Detecting connected components</li>
<li>Checking cycles in graphs</li>
<li>Generating topological orderings</li>
<li>Solving maze traversal and pathfinding tasks</li>
</ul>
</section>
<section id="the-core-idea-50" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-50">The Core Idea</h4>
<p>DFS uses a stack (explicit or recursive) to remember where it came from. It works recursively as:</p>
<ol type="1">
<li>Visit a vertex.</li>
<li>Recurse into each unvisited neighbor.</li>
<li>Backtrack when no unvisited neighbors remain.</li>
</ol>
<p>This “deep first” pattern ensures that every path is explored to its end before moving on.</p>
</section>
<section id="algorithm-1" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-1">Algorithm</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Mark the starting node as visited.</td>
</tr>
<tr class="even">
<td>2</td>
<td>For each neighbor <span class="math inline">\(v\)</span> of the node:</td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;a. If <span class="math inline">\(v\)</span> not visited, recurse with DFS(<span class="math inline">\(v\)</span>).</td>
</tr>
<tr class="even">
<td>3</td>
<td>Continue until all vertices reachable from the start are visited.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-classic-dfs" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-classic-dfs">Tiny Code (Classic DFS)</h4>
<p>C Example (Recursive)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb157"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdio.h&gt;</span></span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;stdbool.h&gt;</span></span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#define N </span><span class="dv">100</span></span>
<span id="cb157-5"><a href="#cb157-5" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> adj<span class="op">[</span>N<span class="op">][</span>N<span class="op">];</span></span>
<span id="cb157-6"><a href="#cb157-6" aria-hidden="true" tabindex="-1"></a><span class="dt">bool</span> visited<span class="op">[</span>N<span class="op">];</span></span>
<span id="cb157-7"><a href="#cb157-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-8"><a href="#cb157-8" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> dfs<span class="op">(</span><span class="dt">int</span> v<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb157-9"><a href="#cb157-9" aria-hidden="true" tabindex="-1"></a>    visited<span class="op">[</span>v<span class="op">]</span> <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb157-10"><a href="#cb157-10" aria-hidden="true" tabindex="-1"></a>    printf<span class="op">(</span><span class="st">"</span><span class="sc">%d</span><span class="st"> "</span><span class="op">,</span> v<span class="op">);</span></span>
<span id="cb157-11"><a href="#cb157-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> u <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> u <span class="op">&lt;</span> n<span class="op">;</span> u<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb157-12"><a href="#cb157-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>adj<span class="op">[</span>v<span class="op">][</span>u<span class="op">]</span> <span class="op">&amp;&amp;</span> <span class="op">!</span>visited<span class="op">[</span>u<span class="op">])</span> <span class="op">{</span></span>
<span id="cb157-13"><a href="#cb157-13" aria-hidden="true" tabindex="-1"></a>            dfs<span class="op">(</span>u<span class="op">,</span> n<span class="op">);</span></span>
<span id="cb157-14"><a href="#cb157-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb157-15"><a href="#cb157-15" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb157-16"><a href="#cb157-16" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Python Example</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfs(graph, start, visited<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visited <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>        visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>    visited.add(start)</span>
<span id="cb158-5"><a href="#cb158-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> graph[start]:</span>
<span id="cb158-6"><a href="#cb158-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb158-7"><a href="#cb158-7" aria-hidden="true" tabindex="-1"></a>            dfs(graph, v, visited)</span>
<span id="cb158-8"><a href="#cb158-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> visited</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example usage:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> {</span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'A'</span>: [<span class="st">'B'</span>, <span class="st">'C'</span>],</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'B'</span>: [<span class="st">'A'</span>, <span class="st">'D'</span>, <span class="st">'E'</span>],</span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'C'</span>: [<span class="st">'A'</span>, <span class="st">'F'</span>],</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'D'</span>: [<span class="st">'B'</span>],</span>
<span id="cb159-6"><a href="#cb159-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'E'</span>: [<span class="st">'B'</span>, <span class="st">'F'</span>],</span>
<span id="cb159-7"><a href="#cb159-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'F'</span>: [<span class="st">'C'</span>, <span class="st">'E'</span>]</span>
<span id="cb159-8"><a href="#cb159-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb159-9"><a href="#cb159-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-10"><a href="#cb159-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dfs(G, <span class="st">'A'</span>))  <span class="co"># {'A', 'B', 'D', 'E', 'F', 'C'}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-92" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-92">Why It Matters</h4>
<p>DFS underlies many advanced algorithms:</p>
<ul>
<li>Cycle detection in directed or undirected graphs</li>
<li>Topological sorting in DAGs</li>
<li>Finding articulation points and bridges</li>
<li>Tarjan’s SCC algorithm for strongly connected components</li>
<li>Backtracking in puzzles (Sudoku, N-Queens, pathfinding)</li>
</ul>
<p>In trees, DFS is the natural traversal for pre-order, in-order, and post-order searches.</p>
</section>
<section id="try-it-yourself-91" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-91">Try It Yourself</h4>
<ol type="1">
<li>Modify DFS to record discovery and finishing times.</li>
<li>Use DFS to detect whether a graph is connected.</li>
<li>Apply DFS on a directed graph and perform topological sorting.</li>
<li>Visualize the DFS recursion tree, mark back edges and forward edges.</li>
</ol>
</section>
<section id="test-cases-91" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-91">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Graph</th>
<th>Start</th>
<th>Expected Order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A–B–C</td>
<td>A</td>
<td>A, B, C</td>
</tr>
<tr class="even">
<td>Triangle (A–B–C–A)</td>
<td>B</td>
<td>B, A, C</td>
</tr>
<tr class="odd">
<td>Star (1→2,3,4)</td>
<td>1</td>
<td>1, 2, 3, 4 (depth order may vary)</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-91" class="level4">
<h4 class="anchored" data-anchor-id="complexity-91">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(V + E)\)</span></td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(V)\)</span> (stack or recursion depth)</td>
</tr>
</tbody>
</table>
<p>Each vertex and edge is visited once. Space complexity equals recursion depth, which in the worst case can reach <span class="math inline">\(V\)</span>.</p>
</section>
<section id="a-gentle-proof-why-it-works-87" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-87">A Gentle Proof (Why It Works)</h4>
<p>DFS explores along a path until no unvisited neighbor remains, then backtracks. Each edge <span class="math inline">\((u, v)\)</span> is considered exactly once when <span class="math inline">\(v\)</span> is first discovered. By induction, DFS guarantees every reachable vertex is visited exactly once.</p>
</section>
<section id="variants-3" class="level4">
<h4 class="anchored" data-anchor-id="variants-3">Variants</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Variant</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Recursive DFS</td>
<td>Simpler, uses call stack</td>
</tr>
<tr class="even">
<td>Iterative DFS</td>
<td>Explicit stack, avoids recursion limit</td>
</tr>
<tr class="odd">
<td>Modified DFS</td>
<td>Tracks parent edges or discovery times</td>
</tr>
<tr class="even">
<td>DFS Forest</td>
<td>Collection of DFS trees for disconnected graphs</td>
</tr>
</tbody>
</table>
</section>
<section id="summary-table-20" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-20">Summary Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Graph traversal</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Explore deeply before backtracking</td>
</tr>
<tr class="odd">
<td>Data Structure</td>
<td>Stack or recursion</td>
</tr>
<tr class="even">
<td>Guarantee</td>
<td>Full exploration of connected component</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(V + E)\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(V)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Topological sort, SCC, pathfinding, cycle detection</td>
</tr>
</tbody>
</table>
<p>Depth-First Search is the explorer’s mindset, fearless, methodical, and curious, diving deep into one path until it has seen all that lies beneath.</p>
</section>
</section>
<section id="a-search" class="level3">
<h3 class="anchored" data-anchor-id="a-search">993. A* Search</h3>
<p>A* (pronounced “A-star”) is one of the most influential search algorithms in artificial intelligence. It combines the cost so far with a heuristic estimate of the cost to go, balancing efficiency and optimality. A* elegantly generalizes Dijkstra’s algorithm and greedy best-first search, and it’s used everywhere from game AI to robotics to route planning.</p>
<section id="what-problem-are-we-solving-92" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-92">What Problem Are We Solving?</h4>
<p>We want to find the shortest path between a start node and a goal node in a weighted graph. Each edge has a cost <span class="math inline">\(c(u, v)\)</span>, and we also have a heuristic function <span class="math inline">\(h(v)\)</span> estimating the remaining cost from <span class="math inline">\(v\)</span> to the goal.</p>
<p>If we only used actual cost <span class="math inline">\(g(v)\)</span> → Dijkstra’s algorithm. If we only used heuristic <span class="math inline">\(h(v)\)</span> → Greedy best-first search. A* combines both: <span class="math display">\[
f(v) = g(v) + h(v)
\]</span> The algorithm always expands the node with the smallest total estimated cost <span class="math inline">\(f(v)\)</span>.</p>
</section>
<section id="the-core-idea-51" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-51">The Core Idea</h4>
<ol type="1">
<li><p>Maintain two sets:</p>
<ul>
<li>Open (frontier): nodes to explore</li>
<li>Closed: already visited nodes</li>
</ul></li>
<li><p>Start from the source node with <span class="math inline">\(g(s) = 0\)</span>.</p></li>
<li><p>While the open set is not empty:</p>
<ul>
<li><p>Pick node <span class="math inline">\(u\)</span> with smallest <span class="math inline">\(f(u) = g(u) + h(u)\)</span>.</p></li>
<li><p>If <span class="math inline">\(u\)</span> is the goal, reconstruct path and stop.</p></li>
<li><p>For each neighbor <span class="math inline">\(v\)</span> of <span class="math inline">\(u\)</span>:</p>
<ul>
<li>Compute tentative cost <span class="math inline">\(g'(v) = g(u) + c(u, v)\)</span>.</li>
<li>If <span class="math inline">\(g'(v) &lt; g(v)\)</span>, update and record parent.</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="algorithm-2" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-2">Algorithm</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize open set with start node <span class="math inline">\(s\)</span>.</td>
</tr>
<tr class="even">
<td>2</td>
<td>Set <span class="math inline">\(g(s) = 0\)</span>, <span class="math inline">\(f(s) = h(s)\)</span>.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>While open set not empty:</td>
</tr>
<tr class="even">
<td></td>
<td>&nbsp;&nbsp;a. Choose node <span class="math inline">\(u\)</span> with smallest <span class="math inline">\(f(u)\)</span>.</td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;b. If <span class="math inline">\(u\)</span> is goal → return path.</td>
</tr>
<tr class="even">
<td></td>
<td>&nbsp;&nbsp;c.&nbsp;For each neighbor <span class="math inline">\(v\)</span> of <span class="math inline">\(u\)</span>: update <span class="math inline">\(g(v)\)</span> and <span class="math inline">\(f(v)\)</span>.</td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;d.&nbsp;Move <span class="math inline">\(u\)</span> to closed set.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb160"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> heapq</span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> a_star(graph, start, goal, h):</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>    open_set <span class="op">=</span> [(<span class="dv">0</span>, start)]</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a>    came_from <span class="op">=</span> {}</span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> {start: <span class="dv">0</span>}</span>
<span id="cb160-7"><a href="#cb160-7" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> {start: h(start)}</span>
<span id="cb160-8"><a href="#cb160-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-9"><a href="#cb160-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> open_set:</span>
<span id="cb160-10"><a href="#cb160-10" aria-hidden="true" tabindex="-1"></a>        _, u <span class="op">=</span> heapq.heappop(open_set)</span>
<span id="cb160-11"><a href="#cb160-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> u <span class="op">==</span> goal:</span>
<span id="cb160-12"><a href="#cb160-12" aria-hidden="true" tabindex="-1"></a>            path <span class="op">=</span> [u]</span>
<span id="cb160-13"><a href="#cb160-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> u <span class="kw">in</span> came_from:</span>
<span id="cb160-14"><a href="#cb160-14" aria-hidden="true" tabindex="-1"></a>                u <span class="op">=</span> came_from[u]</span>
<span id="cb160-15"><a href="#cb160-15" aria-hidden="true" tabindex="-1"></a>                path.append(u)</span>
<span id="cb160-16"><a href="#cb160-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="bu">list</span>(<span class="bu">reversed</span>(path))</span>
<span id="cb160-17"><a href="#cb160-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-18"><a href="#cb160-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> v, cost <span class="kw">in</span> graph[u]:</span>
<span id="cb160-19"><a href="#cb160-19" aria-hidden="true" tabindex="-1"></a>            g_new <span class="op">=</span> g[u] <span class="op">+</span> cost</span>
<span id="cb160-20"><a href="#cb160-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> g <span class="kw">or</span> g_new <span class="op">&lt;</span> g[v]:</span>
<span id="cb160-21"><a href="#cb160-21" aria-hidden="true" tabindex="-1"></a>                g[v] <span class="op">=</span> g_new</span>
<span id="cb160-22"><a href="#cb160-22" aria-hidden="true" tabindex="-1"></a>                f[v] <span class="op">=</span> g_new <span class="op">+</span> h(v)</span>
<span id="cb160-23"><a href="#cb160-23" aria-hidden="true" tabindex="-1"></a>                came_from[v] <span class="op">=</span> u</span>
<span id="cb160-24"><a href="#cb160-24" aria-hidden="true" tabindex="-1"></a>                heapq.heappush(open_set, (f[v], v))</span>
<span id="cb160-25"><a href="#cb160-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> {</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'A'</span>: [(<span class="st">'B'</span>, <span class="dv">1</span>), (<span class="st">'C'</span>, <span class="dv">3</span>)],</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'B'</span>: [(<span class="st">'D'</span>, <span class="dv">3</span>)],</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'C'</span>: [(<span class="st">'D'</span>, <span class="dv">1</span>)],</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'D'</span>: []</span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="kw">lambda</span> x: {<span class="st">'A'</span>: <span class="dv">3</span>, <span class="st">'B'</span>: <span class="dv">2</span>, <span class="st">'C'</span>: <span class="dv">1</span>, <span class="st">'D'</span>: <span class="dv">0</span>}[x]</span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a_star(graph, <span class="st">'A'</span>, <span class="st">'D'</span>, h))  <span class="co"># ['A', 'C', 'D']</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-93" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-93">Why It Matters</h4>
<ul>
<li><p>Finds optimal path if heuristic <span class="math inline">\(h\)</span> is admissible (<span class="math inline">\(h(v) \le h^*(v)\)</span>, true cost).</p></li>
<li><p>Efficiently prunes paths unlikely to lead to the goal.</p></li>
<li><p>Powers algorithms in:</p>
<ul>
<li>Game pathfinding (e.g., grid maps, navigation meshes)</li>
<li>Robotics motion planning</li>
<li>Route optimization (maps, logistics)</li>
<li>Natural language parsing and AI planning</li>
</ul></li>
</ul>
</section>
<section id="try-it-yourself-92" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-92">Try It Yourself</h4>
<ol type="1">
<li><p>Implement A* on a 2D grid with obstacles.</p></li>
<li><p>Test with heuristics:</p>
<ul>
<li>Manhattan distance</li>
<li>Euclidean distance</li>
<li>Zero heuristic (reduces to Dijkstra)</li>
</ul></li>
<li><p>Visualize the expansion of the search frontier.</p></li>
<li><p>Experiment with overestimating heuristics (non-admissible), see when optimality breaks.</p></li>
</ol>
</section>
<section id="test-cases-92" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-92">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Graph</th>
<th>Heuristic</th>
<th>Expected Path</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A–B–C (unit cost)</td>
<td><span class="math inline">\(h=0\)</span></td>
<td>A → B → C</td>
</tr>
<tr class="even">
<td>Grid (Manhattan)</td>
<td>Admissible</td>
<td>Shortest geometric path</td>
</tr>
<tr class="odd">
<td>Random graph</td>
<td>Overestimate</td>
<td>May skip optimal path</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-92" class="level4">
<h4 class="anchored" data-anchor-id="complexity-92">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(E \log V)\)</span> (using priority queue)</td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(V)\)</span> (stores <span class="math inline">\(g\)</span>, <span class="math inline">\(f\)</span>, open/closed sets)</td>
</tr>
</tbody>
</table>
<p>Efficiency depends heavily on heuristic accuracy, a good <span class="math inline">\(h\)</span> drastically reduces explored nodes.</p>
</section>
<section id="a-gentle-proof-why-it-works-88" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-88">A Gentle Proof (Why It Works)</h4>
<p>If <span class="math inline">\(h(v)\)</span> is admissible (never overestimates true cost) and consistent (<span class="math inline">\(h(u) \le c(u,v) + h(v)\)</span>), then A* always expands nodes in order of increasing true cost to the goal.</p>
<p>Therefore, the first time the goal node is removed from the open set, the shortest path has been found.</p>
</section>
<section id="summary-table-21" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-21">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Informed graph search</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Minimize total cost <span class="math inline">\(f = g + h\)</span></td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Optimal if <span class="math inline">\(h\)</span> admissible</td>
</tr>
<tr class="even">
<td>Data Structure</td>
<td>Priority queue (min-heap)</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(E \log V)\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(V)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Pathfinding, robotics, AI planning</td>
</tr>
</tbody>
</table>
<p>A* is the perfect blend of logic and intuition, a search that not only knows where it’s been, but also has a sense of where it should go next.</p>
</section>
</section>
<section id="iterative-deepening-a-ida" class="level3">
<h3 class="anchored" data-anchor-id="iterative-deepening-a-ida">994. Iterative Deepening A* (IDA*)</h3>
<p>Iterative Deepening A* combines the space efficiency of Depth-First Search with the optimality and heuristic power of A<em>. It’s particularly useful when the graph or state space is too large for A</em> to fit in memory, yet we still need optimal solutions guided by heuristics.</p>
<section id="what-problem-are-we-solving-93" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-93">What Problem Are We Solving?</h4>
<p>Standard A* stores all explored nodes in memory, which can grow exponentially. For large problems (like puzzles or planning tasks), this becomes infeasible.</p>
<p>IDA* solves this by performing iterative depth-first searches, each limited by an increasing cost threshold. Instead of exploring by depth, we explore by estimated total cost <span class="math inline">\(f(n) = g(n) + h(n)\)</span>, repeating the process with larger limits until the goal is reached.</p>
</section>
<section id="the-core-idea-52" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-52">The Core Idea</h4>
<ol type="1">
<li>Start with an initial threshold equal to <span class="math inline">\(h(start)\)</span>.</li>
<li>Perform a Depth-First Search, but prune any node where <span class="math inline">\(f(n) &gt; threshold\)</span>.</li>
<li>If the goal is not found, increase the threshold to the smallest pruned value and repeat.</li>
</ol>
<p>This process ensures that the first time the goal is reached, it has the minimum possible <span class="math inline">\(f\)</span> value, guaranteeing optimality.</p>
</section>
<section id="algorithm-3" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-3">Algorithm</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize threshold = <span class="math inline">\(h(start)\)</span>.</td>
</tr>
<tr class="even">
<td>2</td>
<td>While true: perform depth-limited DFS using <span class="math inline">\(f(n) = g(n) + h(n)\)</span>.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>If goal found, return path.</td>
</tr>
<tr class="even">
<td>4</td>
<td>If no path within limit, increase threshold to next higher <span class="math inline">\(f\)</span> encountered.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example-1" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-1">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ida_star(start, goal, h, neighbors):</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> search(path, g, bound):</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>        node <span class="op">=</span> path[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>        f <span class="op">=</span> g <span class="op">+</span> h(node)</span>
<span id="cb162-5"><a href="#cb162-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f <span class="op">&gt;</span> bound:</span>
<span id="cb162-6"><a href="#cb162-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> f</span>
<span id="cb162-7"><a href="#cb162-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> node <span class="op">==</span> goal:</span>
<span id="cb162-8"><a href="#cb162-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> path</span>
<span id="cb162-9"><a href="#cb162-9" aria-hidden="true" tabindex="-1"></a>        min_bound <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb162-10"><a href="#cb162-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> next_node, cost <span class="kw">in</span> neighbors(node):</span>
<span id="cb162-11"><a href="#cb162-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> next_node <span class="kw">not</span> <span class="kw">in</span> path:</span>
<span id="cb162-12"><a href="#cb162-12" aria-hidden="true" tabindex="-1"></a>                path.append(next_node)</span>
<span id="cb162-13"><a href="#cb162-13" aria-hidden="true" tabindex="-1"></a>                t <span class="op">=</span> search(path, g <span class="op">+</span> cost, bound)</span>
<span id="cb162-14"><a href="#cb162-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">isinstance</span>(t, <span class="bu">list</span>):</span>
<span id="cb162-15"><a href="#cb162-15" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">return</span> t</span>
<span id="cb162-16"><a href="#cb162-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> t <span class="op">&lt;</span> min_bound:</span>
<span id="cb162-17"><a href="#cb162-17" aria-hidden="true" tabindex="-1"></a>                    min_bound <span class="op">=</span> t</span>
<span id="cb162-18"><a href="#cb162-18" aria-hidden="true" tabindex="-1"></a>                path.pop()</span>
<span id="cb162-19"><a href="#cb162-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> min_bound</span>
<span id="cb162-20"><a href="#cb162-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-21"><a href="#cb162-21" aria-hidden="true" tabindex="-1"></a>    bound <span class="op">=</span> h(start)</span>
<span id="cb162-22"><a href="#cb162-22" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> [start]</span>
<span id="cb162-23"><a href="#cb162-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb162-24"><a href="#cb162-24" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> search(path, <span class="dv">0</span>, bound)</span>
<span id="cb162-25"><a href="#cb162-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(t, <span class="bu">list</span>):</span>
<span id="cb162-26"><a href="#cb162-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> t</span>
<span id="cb162-27"><a href="#cb162-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> t <span class="op">==</span> <span class="bu">float</span>(<span class="st">'inf'</span>):</span>
<span id="cb162-28"><a href="#cb162-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb162-29"><a href="#cb162-29" aria-hidden="true" tabindex="-1"></a>        bound <span class="op">=</span> t</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb163"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> {</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'A'</span>: [(<span class="st">'B'</span>, <span class="dv">1</span>), (<span class="st">'C'</span>, <span class="dv">4</span>)],</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'B'</span>: [(<span class="st">'D'</span>, <span class="dv">1</span>)],</span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'C'</span>: [(<span class="st">'D'</span>, <span class="dv">1</span>)],</span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'D'</span>: []</span>
<span id="cb163-6"><a href="#cb163-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb163-7"><a href="#cb163-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neighbors(n): <span class="cf">return</span> graph.get(n, [])</span>
<span id="cb163-8"><a href="#cb163-8" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="kw">lambda</span> x: {<span class="st">'A'</span>: <span class="dv">3</span>, <span class="st">'B'</span>: <span class="dv">2</span>, <span class="st">'C'</span>: <span class="dv">1</span>, <span class="st">'D'</span>: <span class="dv">0</span>}[x]</span>
<span id="cb163-9"><a href="#cb163-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ida_star(<span class="st">'A'</span>, <span class="st">'D'</span>, h, <span class="kw">lambda</span> n: graph.get(n, [])))  <span class="co"># ['A', 'B', 'D']</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-94" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-94">Why It Matters</h4>
<ul>
<li><p>Memory-efficient A*: uses <span class="math inline">\(O(d)\)</span> space (depth only).</p></li>
<li><p>Guarantees optimal solution if heuristic is admissible.</p></li>
<li><p>Used in domains like:</p>
<ul>
<li>15-puzzle and sliding puzzles</li>
<li>Robot motion planning</li>
<li>AI game search</li>
<li>Large-scale route finding where A* cannot fit in RAM</li>
</ul></li>
</ul>
</section>
<section id="try-it-yourself-93" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-93">Try It Yourself</h4>
<ol type="1">
<li>Use IDA* on an <span class="math inline">\(8 \times 8\)</span> grid with obstacles.</li>
<li>Compare memory usage and number of nodes explored vs A*.</li>
<li>Experiment with heuristic tightness (e.g., Manhattan vs Euclidean).</li>
<li>Implement cutoff visualization, show how thresholds expand per iteration.</li>
</ol>
</section>
<section id="test-cases-93" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-93">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 28%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Graph</th>
<th>Heuristic</th>
<th>Expected Path</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A–B–D (unit cost)</td>
<td>Admissible</td>
<td>A → B → D</td>
</tr>
<tr class="even">
<td>Branching with C</td>
<td><span class="math inline">\(h(C) &gt; h^*(C)\)</span></td>
<td>Still finds A → B → D</td>
</tr>
<tr class="odd">
<td>Grid world</td>
<td>Manhattan distance</td>
<td>Optimal path found gradually</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-93" class="level4">
<h4 class="anchored" data-anchor-id="complexity-93">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(b^{d})\)</span> worst case (like DFS)</td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(d)\)</span> (depth of recursion)</td>
</tr>
</tbody>
</table>
<p>Although it may revisit nodes across iterations, IDA* saves exponential memory compared to A*.</p>
</section>
<section id="a-gentle-proof-why-it-works-89" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-89">A Gentle Proof (Why It Works)</h4>
<p>Each iteration explores all paths with <span class="math inline">\(f(n) \le\)</span> current threshold. Since thresholds grow in increasing order of <span class="math inline">\(f\)</span>, the first time the goal’s <span class="math inline">\(f\)</span> value is reached, it must be minimal. Thus, IDA* maintains both completeness and optimality, provided <span class="math inline">\(h\)</span> is admissible.</p>
</section>
<section id="summary-table-22" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-22">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Heuristic search with limited memory</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Iteratively deepen cost threshold <span class="math inline">\(f = g + h\)</span></td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Optimal if heuristic admissible</td>
</tr>
<tr class="even">
<td>Data Structure</td>
<td>Recursive DFS</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(b^{d})\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(d)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Puzzle solving, pathfinding, planning</td>
</tr>
</tbody>
</table>
<p>IDA* searches patiently and wisely, like climbing a mountain in steady, careful thresholds, knowing that each step higher brings you closer to the summit with the least effort possible.</p>
</section>
</section>
<section id="uniform-cost-search-ucs" class="level3">
<h3 class="anchored" data-anchor-id="uniform-cost-search-ucs">995. Uniform Cost Search (UCS)</h3>
<p>Uniform Cost Search (UCS) is a classic algorithm for finding the least-cost path between nodes when all edge costs are non-negative. It is essentially Dijkstra’s algorithm framed as a search problem, expanding the least-cost node first, ensuring optimality without needing any heuristic.</p>
<section id="what-problem-are-we-solving-94" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-94">What Problem Are We Solving?</h4>
<p>Given a weighted graph <span class="math inline">\(G = (V, E)\)</span> where each edge <span class="math inline">\((u, v)\)</span> has a non-negative cost <span class="math inline">\(c(u, v)\)</span>, find the path from a start node <span class="math inline">\(s\)</span> to a goal node <span class="math inline">\(g\)</span> that minimizes the total path cost: <span class="math display">\[
C(s, g) = \sum_{(u, v) \in \text{path}} c(u, v)
\]</span></p>
<p>Unlike BFS (which assumes unit costs), UCS generalizes to arbitrary positive costs.</p>
</section>
<section id="the-core-idea-53" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-53">The Core Idea</h4>
<ul>
<li>Always expand the node with the smallest known cumulative cost <span class="math inline">\(g(n)\)</span>.</li>
<li>Keep track of the best cost found so far for each node.</li>
<li>Once a goal node is dequeued from the frontier, the cost is guaranteed to be minimal.</li>
</ul>
</section>
<section id="algorithm-4" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-4">Algorithm</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 93%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize priority queue with start node <span class="math inline">\((s, 0)\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>While queue not empty:</td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;a. Pop node <span class="math inline">\(u\)</span> with smallest cost <span class="math inline">\(g(u)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td>&nbsp;&nbsp;b. If <span class="math inline">\(u\)</span> is the goal → return path</td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;c.&nbsp;For each neighbor <span class="math inline">\(v\)</span>:</td>
</tr>
<tr class="even">
<td></td>
<td>&nbsp;&nbsp;&nbsp;&nbsp;Compute <span class="math inline">\(g'(v) = g(u) + c(u, v)\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td>&nbsp;&nbsp;&nbsp;&nbsp;If <span class="math inline">\(v\)</span> not visited or cheaper path found → update and push</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example-2" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-2">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> heapq</span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> uniform_cost_search(graph, start, goal):</span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a>    pq <span class="op">=</span> [(<span class="dv">0</span>, start, [])]</span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a>    visited <span class="op">=</span> {}</span>
<span id="cb164-6"><a href="#cb164-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> pq:</span>
<span id="cb164-7"><a href="#cb164-7" aria-hidden="true" tabindex="-1"></a>        cost, node, path <span class="op">=</span> heapq.heappop(pq)</span>
<span id="cb164-8"><a href="#cb164-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> node <span class="op">==</span> goal:</span>
<span id="cb164-9"><a href="#cb164-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> path <span class="op">+</span> [node], cost</span>
<span id="cb164-10"><a href="#cb164-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> node <span class="kw">in</span> visited <span class="kw">and</span> visited[node] <span class="op">&lt;=</span> cost:</span>
<span id="cb164-11"><a href="#cb164-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb164-12"><a href="#cb164-12" aria-hidden="true" tabindex="-1"></a>        visited[node] <span class="op">=</span> cost</span>
<span id="cb164-13"><a href="#cb164-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> neighbor, edge_cost <span class="kw">in</span> graph[node]:</span>
<span id="cb164-14"><a href="#cb164-14" aria-hidden="true" tabindex="-1"></a>            heapq.heappush(pq, (cost <span class="op">+</span> edge_cost, neighbor, path <span class="op">+</span> [node]))</span>
<span id="cb164-15"><a href="#cb164-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span>, <span class="bu">float</span>(<span class="st">'inf'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> {</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'A'</span>: [(<span class="st">'B'</span>, <span class="dv">1</span>), (<span class="st">'C'</span>, <span class="dv">5</span>)],</span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'B'</span>: [(<span class="st">'C'</span>, <span class="dv">2</span>), (<span class="st">'D'</span>, <span class="dv">4</span>)],</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'C'</span>: [(<span class="st">'D'</span>, <span class="dv">1</span>)],</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'D'</span>: []</span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>path, cost <span class="op">=</span> uniform_cost_search(graph, <span class="st">'A'</span>, <span class="st">'D'</span>)</span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(path, cost)  <span class="co"># ['A', 'B', 'C', 'D'], 4</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-95" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-95">Why It Matters</h4>
<ul>
<li><p>Finds optimal paths in weighted graphs without requiring heuristics.</p></li>
<li><p>Forms the foundation of A* (where heuristics are added).</p></li>
<li><p>Useful in domains such as:</p>
<ul>
<li>Navigation and logistics</li>
<li>Network routing</li>
<li>Planning and scheduling</li>
<li>AI pathfinding when heuristic not available</li>
</ul></li>
</ul>
</section>
<section id="try-it-yourself-94" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-94">Try It Yourself</h4>
<ol type="1">
<li>Modify UCS to stop after a specific depth.</li>
<li>Compare UCS with Dijkstra’s algorithm, note the identical frontier logic.</li>
<li>Use UCS in a grid world where different terrains have different traversal costs.</li>
<li>Visualize how UCS expands nodes in increasing cost order.</li>
</ol>
</section>
<section id="test-cases-94" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-94">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Graph</th>
<th>Start</th>
<th>Goal</th>
<th>Expected Path</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A–B–C (1, 2)</td>
<td>A</td>
<td>C</td>
<td>A → B → C</td>
<td>3</td>
</tr>
<tr class="even">
<td>A–C (5), A–B–C (1, 2)</td>
<td>A</td>
<td>C</td>
<td>A → B → C</td>
<td>3</td>
</tr>
<tr class="odd">
<td>Weighted grid</td>
<td>(0,0)</td>
<td>(2,2)</td>
<td>Minimal-cost route</td>
<td>varies</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-94" class="level4">
<h4 class="anchored" data-anchor-id="complexity-94">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(E \log V)\)</span></td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(V)\)</span></td>
</tr>
</tbody>
</table>
<p>UCS explores nodes in increasing cost order, similar to Dijkstra. In dense graphs, memory usage can grow as it keeps multiple paths to nodes before pruning.</p>
</section>
<section id="a-gentle-proof-why-it-works-90" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-90">A Gentle Proof (Why It Works)</h4>
<p>UCS is optimal because it expands nodes in order of non-decreasing path cost <span class="math inline">\(g(n)\)</span>. When a goal node <span class="math inline">\(g\)</span> is selected for expansion, no cheaper path to <span class="math inline">\(g\)</span> can exist in the queue. This follows from the monotonicity of edge costs (<span class="math inline">\(c(u, v) \ge 0\)</span>).</p>
</section>
<section id="summary-table-23" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-23">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Uninformed cost-based search</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Expand node with lowest cumulative cost</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Optimal if edge costs non-negative</td>
</tr>
<tr class="even">
<td>Data Structure</td>
<td>Priority queue (min-heap)</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(E \log V)\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(V)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Pathfinding, routing, planning</td>
</tr>
</tbody>
</table>
<p>Uniform Cost Search is steady and patient, it never guesses, never rushes. It simply follows the path of least resistance, one minimal cost at a time, until it reaches the goal with certainty.</p>
</section>
</section>
<section id="monte-carlo-tree-search-mcts" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-tree-search-mcts">996. Monte Carlo Tree Search (MCTS)</h3>
<p>Monte Carlo Tree Search (MCTS) is the algorithmic heart of modern game-playing AI. It combines random sampling, incremental tree building, and statistical reasoning to make near-optimal decisions without exhaustive search. MCTS was famously used in AlphaGo, AlphaZero, and other intelligent agents that learn by exploring possibilities through simulation.</p>
<section id="what-problem-are-we-solving-95" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-95">What Problem Are We Solving?</h4>
<p>We want to find the best move or action in a large search space (like in Go, Chess, or planning tasks) where the full game tree is too vast to explore completely.</p>
<p>MCTS avoids full expansion by repeatedly simulating random playouts to estimate the value of actions.</p>
<p>Each iteration involves four steps: Selection, Expansion, Simulation, Backpropagation.</p>
</section>
<section id="the-core-idea-54" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-54">The Core Idea</h4>
<p>Each node in the tree represents a state. The algorithm grows the tree asymmetrically, focusing more on promising branches.</p>
<p>At each iteration:</p>
<ol type="1">
<li><p>Selection: Traverse the tree from root using a <em>tree policy</em> (like UCT) until a node is not fully expanded. Select child <span class="math inline">\(i\)</span> that maximizes: <span class="math display">\[
UCT(i) = \bar{X}_i + c \sqrt{\frac{\ln N}{n_i}}
\]</span> where <span class="math inline">\(\bar{X}_i\)</span> is average reward, <span class="math inline">\(n_i\)</span> visits, and <span class="math inline">\(N\)</span> total visits of parent.</p></li>
<li><p>Expansion: Add one or more unexplored children.</p></li>
<li><p>Simulation: Run a random rollout (playout) from that new node until the game ends. Record the outcome (win/loss/score).</p></li>
<li><p>Backpropagation: Update visit counts and rewards along the path back to the root.</p></li>
</ol>
<p>Over time, estimates converge, and the root’s best child corresponds to the best move.</p>
</section>
<section id="algorithm-outline" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-outline">Algorithm Outline</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start from root (current game state).</td>
</tr>
<tr class="even">
<td>2</td>
<td>Repeat N iterations:</td>
</tr>
<tr class="odd">
<td></td>
<td>a. Select promising node using UCT.</td>
</tr>
<tr class="even">
<td></td>
<td>b. Expand by generating a new child.</td>
</tr>
<tr class="odd">
<td></td>
<td>c.&nbsp;Simulate a random game from that node.</td>
</tr>
<tr class="even">
<td></td>
<td>d.&nbsp;Backpropagate results up the tree.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Return the child with the highest average reward.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example-3" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-3">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math, random</span>
<span id="cb166-2"><a href="#cb166-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-3"><a href="#cb166-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Node:</span>
<span id="cb166-4"><a href="#cb166-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state, parent<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb166-5"><a href="#cb166-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state <span class="op">=</span> state</span>
<span id="cb166-6"><a href="#cb166-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parent <span class="op">=</span> parent</span>
<span id="cb166-7"><a href="#cb166-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.children <span class="op">=</span> []</span>
<span id="cb166-8"><a href="#cb166-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.visits <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb166-9"><a href="#cb166-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb166-10"><a href="#cb166-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-11"><a href="#cb166-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> uct(node, c<span class="op">=</span><span class="fl">1.414</span>):</span>
<span id="cb166-12"><a href="#cb166-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> node.visits <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb166-13"><a href="#cb166-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb166-14"><a href="#cb166-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> node.value <span class="op">/</span> node.visits <span class="op">+</span> c <span class="op">*</span> math.sqrt(math.log(node.parent.visits) <span class="op">/</span> node.visits)</span>
<span id="cb166-15"><a href="#cb166-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-16"><a href="#cb166-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mcts(root, iter_limit, get_moves, simulate, make_move):</span>
<span id="cb166-17"><a href="#cb166-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(iter_limit):</span>
<span id="cb166-18"><a href="#cb166-18" aria-hidden="true" tabindex="-1"></a>        node <span class="op">=</span> root</span>
<span id="cb166-19"><a href="#cb166-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Selection</span></span>
<span id="cb166-20"><a href="#cb166-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> node.children <span class="kw">and</span> <span class="bu">all</span>(c.visits <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">for</span> c <span class="kw">in</span> node.children):</span>
<span id="cb166-21"><a href="#cb166-21" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> <span class="bu">max</span>(node.children, key<span class="op">=</span>uct)</span>
<span id="cb166-22"><a href="#cb166-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expansion</span></span>
<span id="cb166-23"><a href="#cb166-23" aria-hidden="true" tabindex="-1"></a>        moves <span class="op">=</span> get_moves(node.state)</span>
<span id="cb166-24"><a href="#cb166-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> moves:</span>
<span id="cb166-25"><a href="#cb166-25" aria-hidden="true" tabindex="-1"></a>            move <span class="op">=</span> random.choice(moves)</span>
<span id="cb166-26"><a href="#cb166-26" aria-hidden="true" tabindex="-1"></a>            new_state <span class="op">=</span> make_move(node.state, move)</span>
<span id="cb166-27"><a href="#cb166-27" aria-hidden="true" tabindex="-1"></a>            child <span class="op">=</span> Node(new_state, parent<span class="op">=</span>node)</span>
<span id="cb166-28"><a href="#cb166-28" aria-hidden="true" tabindex="-1"></a>            node.children.append(child)</span>
<span id="cb166-29"><a href="#cb166-29" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> child</span>
<span id="cb166-30"><a href="#cb166-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulation</span></span>
<span id="cb166-31"><a href="#cb166-31" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> simulate(node.state)</span>
<span id="cb166-32"><a href="#cb166-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb166-33"><a href="#cb166-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> node:</span>
<span id="cb166-34"><a href="#cb166-34" aria-hidden="true" tabindex="-1"></a>            node.visits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb166-35"><a href="#cb166-35" aria-hidden="true" tabindex="-1"></a>            node.value <span class="op">+=</span> result</span>
<span id="cb166-36"><a href="#cb166-36" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> node.parent</span>
<span id="cb166-37"><a href="#cb166-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(root.children, key<span class="op">=</span><span class="kw">lambda</span> n: n.value <span class="op">/</span> n.visits)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-96" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-96">Why It Matters</h4>
<ul>
<li><p>Balances exploration (try new moves) and exploitation (favor known good moves).</p></li>
<li><p>Scales well to extremely large search spaces.</p></li>
<li><p>Used in:</p>
<ul>
<li>Game AI (Go, Chess, Hex, Shogi, video games)</li>
<li>Planning and robotics</li>
<li>Reinforcement learning hybrids (AlphaZero)</li>
</ul></li>
</ul>
</section>
<section id="try-it-yourself-95" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-95">Try It Yourself</h4>
<ol type="1">
<li>Implement MCTS on Tic-Tac-Toe or Connect Four.</li>
<li>Change exploration constant <span class="math inline">\(c\)</span> to see trade-off behavior.</li>
<li>Compare pure random playouts vs heuristic-guided playouts.</li>
<li>Add time limits instead of iteration limits.</li>
</ol>
</section>
<section id="test-cases-95" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-95">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Domain</th>
<th>Action Space</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tic-Tac-Toe</td>
<td>Small</td>
<td>Converges fast</td>
</tr>
<tr class="even">
<td>Go</td>
<td>Massive</td>
<td>Needs 100k+ simulations</td>
</tr>
<tr class="odd">
<td>Grid navigation</td>
<td>Continuous</td>
<td>Requires state abstraction</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-95" class="level4">
<h4 class="anchored" data-anchor-id="complexity-95">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(N \times D)\)</span> (iterations × depth)</td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(N)\)</span> (tree nodes)</td>
</tr>
</tbody>
</table>
<p>Each iteration grows the tree gradually. Performance depends on how good the simulation policy is.</p>
</section>
<section id="a-gentle-proof-why-it-works-91" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-91">A Gentle Proof (Why It Works)</h4>
<p>With enough iterations, MCTS converges to the optimal minimax value, since all actions are explored infinitely often and their value estimates approach expected returns by the law of large numbers.</p>
<p>The UCT rule ensures logarithmic balance between exploration and exploitation.</p>
</section>
<section id="summary-table-24" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-24">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Monte Carlo sampling in tree search</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Grow search tree by simulated play</td>
</tr>
<tr class="odd">
<td>Exploration</td>
<td>Upper Confidence Bound (UCT)</td>
</tr>
<tr class="even">
<td>Guarantee</td>
<td>Converges to optimal policy asymptotically</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(ND)\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(N)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Games, planning, RL hybrids</td>
</tr>
</tbody>
</table>
<p>MCTS learns not by seeing every path, but by <em>sampling wisely</em>. Each simulation is a glimpse of the future, and together, they guide the search toward mastery.</p>
</section>
</section>
<section id="minimax-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="minimax-algorithm">997. Minimax Algorithm</h3>
<p>The Minimax algorithm is the foundation of game-playing AI. It models decision-making between two opponents, one trying to maximize the score, the other trying to minimize it. Minimax explores the game tree and assumes both players play optimally.</p>
<p>It’s used in Chess, Tic-Tac-Toe, Checkers, and any deterministic, turn-based, zero-sum game.</p>
<section id="what-problem-are-we-solving-96" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-96">What Problem Are We Solving?</h4>
<p>We want to choose the best possible move in a two-player game where each move alternates between maximizing and minimizing outcomes.</p>
<p>For a game tree with alternating turns:</p>
<ul>
<li>MAX tries to maximize the evaluation function.</li>
<li>MIN tries to minimize it.</li>
</ul>
<p>The final outcome is determined by recursively applying these opposing objectives down the tree.</p>
</section>
<section id="the-core-idea-55" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-55">The Core Idea</h4>
<p>At each node (game state):</p>
<ul>
<li>If it’s MAX’s turn, choose the move with maximum value among children.</li>
<li>If it’s MIN’s turn, choose the move with minimum value among children.</li>
</ul>
<p>The value of a node is computed recursively:</p>
<p><span class="math display">\[
V(s) =
\begin{cases}
\text{Utility}(s), &amp; \text{if } s \text{ is terminal},\\
\max_{a \in \text{Actions}(s)} V(\text{Result}(s,a)), &amp; \text{if } s \text{ is MAX turn},\\
\min_{a \in \text{Actions}(s)} V(\text{Result}(s,a)), &amp; \text{if } s \text{ is MIN turn.}
\end{cases}
\]</span></p>
<p>This yields an optimal strategy under perfect play.</p>
</section>
<section id="algorithm-5" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-5">Algorithm</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Build or simulate the game tree to a fixed depth or until terminal states.</td>
</tr>
<tr class="even">
<td>2</td>
<td>Assign evaluation scores to terminal nodes (win/loss/draw or heuristic).</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Propagate scores upward: alternate max/min at each level.</td>
</tr>
<tr class="even">
<td>4</td>
<td>The root’s best move is the child with the highest propagated score.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example-4" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-4">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb167"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> minimax(state, depth, maximizing_player, evaluate, get_moves, make_move):</span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> depth <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="kw">not</span> get_moves(state):</span>
<span id="cb167-3"><a href="#cb167-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> evaluate(state)</span>
<span id="cb167-4"><a href="#cb167-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-5"><a href="#cb167-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> maximizing_player:</span>
<span id="cb167-6"><a href="#cb167-6" aria-hidden="true" tabindex="-1"></a>        max_eval <span class="op">=</span> <span class="bu">float</span>(<span class="st">'-inf'</span>)</span>
<span id="cb167-7"><a href="#cb167-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> move <span class="kw">in</span> get_moves(state):</span>
<span id="cb167-8"><a href="#cb167-8" aria-hidden="true" tabindex="-1"></a>            new_state <span class="op">=</span> make_move(state, move)</span>
<span id="cb167-9"><a href="#cb167-9" aria-hidden="true" tabindex="-1"></a>            <span class="bu">eval</span> <span class="op">=</span> minimax(new_state, depth <span class="op">-</span> <span class="dv">1</span>, <span class="va">False</span>, evaluate, get_moves, make_move)</span>
<span id="cb167-10"><a href="#cb167-10" aria-hidden="true" tabindex="-1"></a>            max_eval <span class="op">=</span> <span class="bu">max</span>(max_eval, <span class="bu">eval</span>)</span>
<span id="cb167-11"><a href="#cb167-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> max_eval</span>
<span id="cb167-12"><a href="#cb167-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb167-13"><a href="#cb167-13" aria-hidden="true" tabindex="-1"></a>        min_eval <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb167-14"><a href="#cb167-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> move <span class="kw">in</span> get_moves(state):</span>
<span id="cb167-15"><a href="#cb167-15" aria-hidden="true" tabindex="-1"></a>            new_state <span class="op">=</span> make_move(state, move)</span>
<span id="cb167-16"><a href="#cb167-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">eval</span> <span class="op">=</span> minimax(new_state, depth <span class="op">-</span> <span class="dv">1</span>, <span class="va">True</span>, evaluate, get_moves, make_move)</span>
<span id="cb167-17"><a href="#cb167-17" aria-hidden="true" tabindex="-1"></a>            min_eval <span class="op">=</span> <span class="bu">min</span>(min_eval, <span class="bu">eval</span>)</span>
<span id="cb167-18"><a href="#cb167-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> min_eval</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example (Tic-Tac-Toe heuristic):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(state):</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="op">==</span> <span class="st">'win'</span>: <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> state <span class="op">==</span> <span class="st">'lose'</span>: <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-97" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-97">Why It Matters</h4>
<ul>
<li>Captures perfect rational play in deterministic games.</li>
<li>Provides the foundation for Alpha–Beta pruning and MCTS.</li>
<li>Shows the connection between search and strategic reasoning.</li>
</ul>
<p>Used in:</p>
<ul>
<li>Turn-based games (Chess, Checkers, Connect Four)</li>
<li>Simple planning with adversarial dynamics</li>
</ul>
</section>
<section id="try-it-yourself-96" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-96">Try It Yourself</h4>
<ol type="1">
<li>Implement minimax for Tic-Tac-Toe.</li>
<li>Add a heuristic evaluator for non-terminal positions.</li>
<li>Compare performance with and without pruning.</li>
<li>Visualize the game tree expansion.</li>
</ol>
</section>
<section id="test-cases-96" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-96">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Game</th>
<th>Depth</th>
<th>Behavior</th>
<th>Outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tic-Tac-Toe</td>
<td>full</td>
<td>Optimal play</td>
<td>Draw</td>
</tr>
<tr class="even">
<td>Connect Four</td>
<td>4</td>
<td>Approx optimal</td>
<td>Competitive play</td>
</tr>
<tr class="odd">
<td>Simplified Chess</td>
<td>2</td>
<td>Greedy play</td>
<td>Suboptimal but valid</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-96" class="level4">
<h4 class="anchored" data-anchor-id="complexity-96">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(b^d)\)</span></td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(bd)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(b\)</span> is branching factor and <span class="math inline">\(d\)</span> is search depth.</p>
</section>
<section id="a-gentle-proof-why-it-works-92" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-92">A Gentle Proof (Why It Works)</h4>
<p>Because both players are assumed optimal, each node’s value represents the best achievable outcome for the current player, assuming the opponent also plays optimally.</p>
<p>By backward induction, this yields the Nash equilibrium of the two-player deterministic game.</p>
</section>
<section id="summary-table-25" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-25">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Adversarial search</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Alternate max and min decisions</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Optimal for perfect play</td>
</tr>
<tr class="even">
<td>Data Structure</td>
<td>Game tree</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(b^d)\)</span></td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(bd)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Board games, strategic planning</td>
</tr>
</tbody>
</table>
<p>Minimax plays the role of a perfect strategist, it doesn’t react emotionally, doesn’t gamble, just follows logic to its inevitable conclusion: play perfectly, or be outplayed.</p>
</section>
</section>
<section id="alphabeta-pruning" class="level3">
<h3 class="anchored" data-anchor-id="alphabeta-pruning">998. Alpha–Beta Pruning</h3>
<p>Alpha–Beta Pruning is an optimization of the Minimax algorithm. It prunes away branches that cannot affect the final decision, allowing us to search deeper without changing the result.</p>
<p>In essence, it tells the computer: “Don’t explore that move, it can’t possibly change the outcome.”</p>
<section id="what-problem-are-we-solving-97" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-97">What Problem Are We Solving?</h4>
<p>Minimax explores every node in the game tree, even when some moves are already provably worse. Alpha–Beta pruning avoids this by keeping track of two bounds:</p>
<ul>
<li>α (alpha), the best value the maximizer can guarantee so far.</li>
<li>β (beta), the best value the minimizer can guarantee so far.</li>
</ul>
<p>If at any point α ≥ β, we stop exploring that branch, it can’t influence the decision.</p>
</section>
<section id="the-core-idea-56" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-56">The Core Idea</h4>
<p>While traversing the tree:</p>
<ul>
<li>MAX nodes update α = max(α, value).</li>
<li>MIN nodes update β = min(β, value).</li>
<li>If α ≥ β, prune the rest of that branch (cutoff).</li>
</ul>
<p>This reduces the number of evaluated nodes while preserving correctness.</p>
</section>
<section id="algorithm-6" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-6">Algorithm</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Initialize α = −∞, β = +∞.</td>
</tr>
<tr class="even">
<td>2</td>
<td>Recursively evaluate game tree as in Minimax.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>For MAX nodes, update α; for MIN nodes, update β.</td>
</tr>
<tr class="even">
<td>4</td>
<td>If α ≥ β, prune remaining children.</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Return α or β depending on player type.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example-5" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-5">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> alphabeta(state, depth, alpha, beta, maximizing, evaluate, get_moves, make_move):</span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> depth <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="kw">not</span> get_moves(state):</span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> evaluate(state)</span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-5"><a href="#cb169-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> maximizing:</span>
<span id="cb169-6"><a href="#cb169-6" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="bu">float</span>(<span class="st">'-inf'</span>)</span>
<span id="cb169-7"><a href="#cb169-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> move <span class="kw">in</span> get_moves(state):</span>
<span id="cb169-8"><a href="#cb169-8" aria-hidden="true" tabindex="-1"></a>            new_state <span class="op">=</span> make_move(state, move)</span>
<span id="cb169-9"><a href="#cb169-9" aria-hidden="true" tabindex="-1"></a>            value <span class="op">=</span> <span class="bu">max</span>(value, alphabeta(new_state, depth<span class="op">-</span><span class="dv">1</span>, alpha, beta, <span class="va">False</span>, evaluate, get_moves, make_move))</span>
<span id="cb169-10"><a href="#cb169-10" aria-hidden="true" tabindex="-1"></a>            alpha <span class="op">=</span> <span class="bu">max</span>(alpha, value)</span>
<span id="cb169-11"><a href="#cb169-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> alpha <span class="op">&gt;=</span> beta:  <span class="co"># Beta cutoff</span></span>
<span id="cb169-12"><a href="#cb169-12" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb169-13"><a href="#cb169-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> value</span>
<span id="cb169-14"><a href="#cb169-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb169-15"><a href="#cb169-15" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb169-16"><a href="#cb169-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> move <span class="kw">in</span> get_moves(state):</span>
<span id="cb169-17"><a href="#cb169-17" aria-hidden="true" tabindex="-1"></a>            new_state <span class="op">=</span> make_move(state, move)</span>
<span id="cb169-18"><a href="#cb169-18" aria-hidden="true" tabindex="-1"></a>            value <span class="op">=</span> <span class="bu">min</span>(value, alphabeta(new_state, depth<span class="op">-</span><span class="dv">1</span>, alpha, beta, <span class="va">True</span>, evaluate, get_moves, make_move))</span>
<span id="cb169-19"><a href="#cb169-19" aria-hidden="true" tabindex="-1"></a>            beta <span class="op">=</span> <span class="bu">min</span>(beta, value)</span>
<span id="cb169-20"><a href="#cb169-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> beta <span class="op">&lt;=</span> alpha:  <span class="co"># Alpha cutoff</span></span>
<span id="cb169-21"><a href="#cb169-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb169-22"><a href="#cb169-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> value</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> alphabeta(start_state, <span class="dv">4</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>), <span class="bu">float</span>(<span class="st">'inf'</span>), <span class="va">True</span>, evaluate, get_moves, make_move)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-98" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-98">Why It Matters</h4>
<ul>
<li>Same optimal result as Minimax, but with far fewer node evaluations.</li>
<li>Enables deeper searches in complex games.</li>
<li>Forms the basis for Chess, Checkers, and Go engines.</li>
</ul>
<p>It’s the standard search core of classic game AI.</p>
</section>
<section id="try-it-yourself-97" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-97">Try It Yourself</h4>
<ol type="1">
<li>Compare the number of nodes expanded by Minimax vs Alpha–Beta.</li>
<li>Implement move ordering, see how pruning improves dramatically.</li>
<li>Apply it to Tic-Tac-Toe, Connect Four, or small Chess subsets.</li>
<li>Add a transposition table to skip repeated states.</li>
</ol>
</section>
<section id="test-cases-97" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-97">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Game</th>
<th>Depth</th>
<th>Algorithm</th>
<th>Nodes Evaluated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tic-Tac-Toe</td>
<td>6</td>
<td>Minimax</td>
<td>~550</td>
</tr>
<tr class="even">
<td>Tic-Tac-Toe</td>
<td>6</td>
<td>Alpha–Beta</td>
<td>~80</td>
</tr>
<tr class="odd">
<td>Connect Four</td>
<td>6</td>
<td>Alpha–Beta</td>
<td>~40% fewer nodes</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-97" class="level4">
<h4 class="anchored" data-anchor-id="complexity-97">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td><span class="math inline">\(O(b^{m})\)</span> best-case <span class="math inline">\(O(b^{d})\)</span> worst-case</td>
</tr>
<tr class="even">
<td>Space</td>
<td><span class="math inline">\(O(bd)\)</span></td>
</tr>
</tbody>
</table>
<p>where:</p>
<ul>
<li><span class="math inline">\(b\)</span> = branching factor</li>
<li><span class="math inline">\(d\)</span> = search depth</li>
<li><span class="math inline">\(m\)</span> = effective depth after pruning (depends on move order)</li>
</ul>
</section>
<section id="a-gentle-proof-why-it-works-93" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-93">A Gentle Proof (Why It Works)</h4>
<p>If α ≥ β at a node, it means the current player has already found a better move elsewhere, and this branch cannot improve the outcome. Thus pruning is safe, the optimal value remains unchanged.</p>
<p>Formally, Alpha–Beta preserves the backward induction property of Minimax.</p>
</section>
<section id="summary-table-26" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-26">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Optimized adversarial search</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Prune branches that can’t affect outcome</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Identical result to Minimax</td>
</tr>
<tr class="even">
<td>Key Variables</td>
<td>α (best for MAX), β (best for MIN)</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td><span class="math inline">\(O(b^m)\)</span> (with pruning)</td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td><span class="math inline">\(O(bd)\)</span></td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Game engines, decision-making AI</td>
</tr>
</tbody>
</table>
<p>Alpha–Beta pruning is Minimax made efficient, a master strategist that knows when not to waste time thinking about moves that don’t matter.</p>
</section>
</section>
<section id="strips-planning" class="level3">
<h3 class="anchored" data-anchor-id="strips-planning">999. STRIPS Planning</h3>
<p>STRIPS (Stanford Research Institute Problem Solver) is a foundational algorithm in AI planning, designed to automatically generate sequences of actions that lead from an initial state to a desired goal.</p>
<p>It was developed in the early 1970s and remains the conceptual backbone of modern planners and symbolic reasoning systems.</p>
<section id="what-problem-are-we-solving-98" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-98">What Problem Are We Solving?</h4>
<p>We want an algorithm that plans actions to achieve a goal, not just reactively act.</p>
<p>Given:</p>
<ul>
<li><p>An initial state (facts about the world)</p></li>
<li><p>A goal (conditions to be made true)</p></li>
<li><p>A set of actions, each with:</p>
<ul>
<li>Preconditions (what must be true to apply the action)</li>
<li>Add effects (facts made true after action)</li>
<li>Delete effects (facts made false after action)</li>
</ul></li>
</ul>
<p>STRIPS systematically searches for a sequence of actions that transform the initial state into one satisfying the goal.</p>
</section>
<section id="the-core-idea-57" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-57">The Core Idea</h4>
<p>STRIPS works through symbolic state-space search, applying operators that change world facts.</p>
<p>Each action is defined as:</p>
<p><span class="math display">\[
\text{Action}(A) = \langle \text{Pre}(A), \text{Add}(A), \text{Del}(A) \rangle
\]</span></p>
<p>When applied to a state <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[
\text{Result}(S, A) = (S - \text{Del}(A)) \cup \text{Add}(A)
\]</span></p>
<p>The planner uses forward search (from start to goal) or backward search (from goal to start) to find a valid plan, a sequence of actions <span class="math inline">\(\langle A_1, A_2, ..., A_n \rangle\)</span> that leads to the goal.</p>
</section>
<section id="example-49" class="level4">
<h4 class="anchored" data-anchor-id="example-49">Example</h4>
<p>Let’s say we want to move a robot from Room1 to Room2:</p>
<ul>
<li>Initial state: In(Room1)</li>
<li>Goal: In(Room2)</li>
<li>Action: Move(Room1, Room2)</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Action</th>
<th>Preconditions</th>
<th>Add Effects</th>
<th>Delete Effects</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Move(x, y)</td>
<td>In(x)</td>
<td>In(y)</td>
<td>In(x)</td>
</tr>
</tbody>
</table>
<p>Forward search applies this operator:</p>
<p><span class="math display">\[
S_0 = { In(Room1) }
\]</span> <span class="math display">\[
Move(Room1, Room2):\quad S_1 = (S_0 - { In(Room1) }) \cup { In(Room2) } = { In(Room2) }
\]</span></p>
<p>Goal achieved.</p>
</section>
<section id="algorithm-outline-1" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-outline-1">Algorithm Outline</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Represent initial state, goal, and actions using predicates.</td>
</tr>
<tr class="even">
<td>2</td>
<td>Start from the initial state.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Choose applicable actions (whose preconditions are true).</td>
</tr>
<tr class="even">
<td>4</td>
<td>Apply action to generate successor states.</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Continue until the goal condition is satisfied.</td>
</tr>
</tbody>
</table>
<p>Can use BFS, DFS, or heuristic-based search (like A*).</p>
</section>
<section id="tiny-code-python-example-6" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-6">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> deque</span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> strips_plan(initial, goal, actions):</span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a>    queue <span class="op">=</span> deque([(initial, [])])</span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> queue:</span>
<span id="cb171-6"><a href="#cb171-6" aria-hidden="true" tabindex="-1"></a>        state, plan <span class="op">=</span> queue.popleft()</span>
<span id="cb171-7"><a href="#cb171-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> goal <span class="op">&lt;=</span> state:  <span class="co"># all goal facts satisfied</span></span>
<span id="cb171-8"><a href="#cb171-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> plan</span>
<span id="cb171-9"><a href="#cb171-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, pre, add, delete <span class="kw">in</span> actions:</span>
<span id="cb171-10"><a href="#cb171-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pre <span class="op">&lt;=</span> state:</span>
<span id="cb171-11"><a href="#cb171-11" aria-hidden="true" tabindex="-1"></a>                new_state <span class="op">=</span> (state <span class="op">-</span> delete) <span class="op">|</span> add</span>
<span id="cb171-12"><a href="#cb171-12" aria-hidden="true" tabindex="-1"></a>                queue.append((new_state, plan <span class="op">+</span> [name]))</span>
<span id="cb171-13"><a href="#cb171-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb171-14"><a href="#cb171-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-15"><a href="#cb171-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb171-16"><a href="#cb171-16" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [</span>
<span id="cb171-17"><a href="#cb171-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Move(Room1, Room2)"</span>, {<span class="st">"In(Room1)"</span>}, {<span class="st">"In(Room2)"</span>}, {<span class="st">"In(Room1)"</span>}),</span>
<span id="cb171-18"><a href="#cb171-18" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb171-19"><a href="#cb171-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-20"><a href="#cb171-20" aria-hidden="true" tabindex="-1"></a>plan <span class="op">=</span> strips_plan({<span class="st">"In(Room1)"</span>}, {<span class="st">"In(Room2)"</span>}, actions)</span>
<span id="cb171-21"><a href="#cb171-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(plan)  <span class="co"># ['Move(Room1, Room2)']</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-it-matters-99" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-99">Why It Matters</h4>
<ul>
<li>Introduced formal symbolic planning, the ability for machines to reason about <em>what must be done</em>, not just <em>what to do now</em>.</li>
<li>Basis for PDDL (Planning Domain Definition Language), automated theorem proving, and robot motion planning.</li>
<li>Still used conceptually in modern AI planners and reinforcement learning with symbolic models.</li>
</ul>
</section>
<section id="try-it-yourself-98" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-98">Try It Yourself</h4>
<ol type="1">
<li>Model a block-stacking problem (Block A on B, B on C, etc.).</li>
<li>Implement backward search (regression) planning.</li>
<li>Add multiple actions and test branching.</li>
<li>Compare STRIPS planning with A* search on same domain.</li>
</ol>
</section>
<section id="test-cases-98" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-98">Test Cases</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 42%">
<col style="width: 21%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Initial</th>
<th>Goal</th>
<th>Plan</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Move</td>
<td>In(Room1)</td>
<td>In(Room2)</td>
<td>[Move(Room1, Room2)]</td>
</tr>
<tr class="even">
<td>Blocks</td>
<td>On(A, Table), Clear(B)</td>
<td>On(A, B)</td>
<td>[Pick(A), Stack(A, B)]</td>
</tr>
<tr class="odd">
<td>Logistics</td>
<td>In(Truck, Depot), At(Package, Depot)</td>
<td>Delivered(Package)</td>
<td>[Load, Drive, Unload]</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-98" class="level4">
<h4 class="anchored" data-anchor-id="complexity-98">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td>Exponential in number of actions</td>
</tr>
<tr class="even">
<td>Space</td>
<td>Exponential in state representation size</td>
</tr>
</tbody>
</table>
</section>
<section id="a-gentle-proof-why-it-works-94" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-94">A Gentle Proof (Why It Works)</h4>
<p>STRIPS is sound and complete for domains expressible in propositional logic: If a plan exists that satisfies the goal, STRIPS will eventually find it, because all applicable actions are systematically explored.</p>
<p>It encodes the world as logical transitions, ensuring each step preserves consistency.</p>
</section>
<section id="summary-table-27" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-27">Summary Table</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Symbolic AI planning</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Search using logical action operators</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Sound and complete (finite domains)</td>
</tr>
<tr class="even">
<td>Data Structure</td>
<td>States as sets of predicates</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td>Exponential</td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td>Exponential</td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Robotics, planning, automated reasoning</td>
</tr>
</tbody>
</table>
<p>STRIPS marked the dawn of AI as reasoning, machines thinking in terms of <em>what must be true</em> and <em>what must change</em> to make the world match a goal.</p>
</section>
</section>
<section id="hierarchical-task-network-htn-planning" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-task-network-htn-planning">1000. Hierarchical Task Network (HTN) Planning</h3>
<p>Hierarchical Task Network (HTN) Planning extends classical STRIPS planning by adding <em>structure</em> to how plans are formed. Instead of searching through flat, atomic actions, HTN organizes them into tasks, some abstract (high-level) and others primitive (executable). It mirrors how humans think: <em>“To travel, first pack, then go to the airport, then fly.”</em></p>
<section id="what-problem-are-we-solving-99" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving-99">What Problem Are We Solving?</h4>
<p>Classical STRIPS is good at exploring all possible actions, but it doesn’t scale well or encode knowledge about <em>how tasks are usually done</em>. HTN Planning introduces hierarchical decomposition, letting planners break complex goals into manageable subtasks guided by domain knowledge.</p>
<p>Given:</p>
<ul>
<li><p>An initial state</p></li>
<li><p>A set of tasks, where</p>
<ul>
<li>Primitive tasks are executable actions</li>
<li>Compound tasks are decomposed into subtasks using methods</li>
</ul></li>
</ul>
<p>The planner recursively refines the main goal task into smaller actions until only primitive steps remain.</p>
</section>
<section id="the-core-idea-58" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-58">The Core Idea</h4>
<p>Each method tells the planner <em>how</em> to achieve a compound task, depending on the current context.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(T\)</span> = set of tasks</li>
<li><span class="math inline">\(M\)</span> = set of methods</li>
<li><span class="math inline">\(A\)</span> = set of primitive actions</li>
</ul>
<p>A method <span class="math inline">\(m\)</span> is defined as:</p>
<p><span class="math display">\[
m = \langle \text{name}(m), \text{task}(m), \text{subtasks}(m), \text{preconditions}(m) \rangle
\]</span></p>
<p>The planner starts from a top-level task <span class="math inline">\(t_0\)</span>, recursively decomposes it using applicable methods, and continues until the plan consists entirely of executable actions.</p>
</section>
<section id="example-50" class="level4">
<h4 class="anchored" data-anchor-id="example-50">Example</h4>
<p>Let’s plan Deliver(Package, Destination):</p>
<p>Initial state: <code>At(Truck, Depot), At(Package, Depot)</code></p>
<p>Goal task: <code>Deliver(Package, Destination)</code></p>
<p>Methods:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Preconditions</th>
<th>Subtasks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deliver-by-truck</td>
<td>At(Truck, x), At(Package, x)</td>
<td>[Load(Package, Truck), Drive(Truck, x, Destination), Unload(Package, Truck)]</td>
</tr>
</tbody>
</table>
<p>Primitive actions (STRIPS-style):</p>
<ul>
<li><code>Load(p, t)</code>: Pre {At(p, x), At(t, x)}, Add {In(p, t)}, Del {At(p, x)}</li>
<li><code>Drive(t, x, y)</code>: Pre {At(t, x)}, Add {At(t, y)}, Del {At(t, x)}</li>
<li><code>Unload(p, t)</code>: Pre {In(p, t), At(t, y)}, Add {At(p, y)}, Del {In(p, t)}</li>
</ul>
<p>The decomposition leads to the final executable plan:</p>
<ol type="1">
<li>Load(Package, Truck)</li>
<li>Drive(Truck, Depot, Destination)</li>
<li>Unload(Package, Truck)</li>
</ol>
</section>
<section id="algorithm-outline-2" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-outline-2">Algorithm Outline</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 96%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Start with a task network (list of tasks to achieve).</td>
</tr>
<tr class="even">
<td>2</td>
<td>Pick a task: if primitive, check preconditions and execute; if compound, find an applicable method.</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Replace the compound task with its subtasks.</td>
</tr>
<tr class="even">
<td>4</td>
<td>Repeat until only primitive actions remain.</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Execute primitive actions to reach the goal.</td>
</tr>
</tbody>
</table>
</section>
<section id="tiny-code-python-example-7" class="level4">
<h4 class="anchored" data-anchor-id="tiny-code-python-example-7">Tiny Code (Python Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> htn_plan(tasks, methods, actions, state):</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> tasks:</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> []</span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>    task <span class="op">=</span> tasks[<span class="dv">0</span>]</span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>    rest <span class="op">=</span> tasks[<span class="dv">1</span>:]</span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Primitive task</span></span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> task <span class="kw">in</span> actions:</span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a>        pre, add, delete <span class="op">=</span> actions[task]</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pre <span class="op">&lt;=</span> state:</span>
<span id="cb172-10"><a href="#cb172-10" aria-hidden="true" tabindex="-1"></a>            new_state <span class="op">=</span> (state <span class="op">-</span> delete) <span class="op">|</span> add</span>
<span id="cb172-11"><a href="#cb172-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> [task] <span class="op">+</span> htn_plan(rest, methods, actions, new_state)</span>
<span id="cb172-12"><a href="#cb172-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb172-13"><a href="#cb172-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb172-14"><a href="#cb172-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compound task</span></span>
<span id="cb172-15"><a href="#cb172-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, goal, subtasks, pre <span class="kw">in</span> methods:</span>
<span id="cb172-16"><a href="#cb172-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> goal <span class="op">==</span> task <span class="kw">and</span> pre <span class="op">&lt;=</span> state:</span>
<span id="cb172-17"><a href="#cb172-17" aria-hidden="true" tabindex="-1"></a>            new_tasks <span class="op">=</span> subtasks <span class="op">+</span> rest</span>
<span id="cb172-18"><a href="#cb172-18" aria-hidden="true" tabindex="-1"></a>            plan <span class="op">=</span> htn_plan(new_tasks, methods, actions, state)</span>
<span id="cb172-19"><a href="#cb172-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> plan:</span>
<span id="cb172-20"><a href="#cb172-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> plan</span>
<span id="cb172-21"><a href="#cb172-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Example domain setup:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> {</span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Load"</span>: ({<span class="st">"At(Package, Depot)"</span>, <span class="st">"At(Truck, Depot)"</span>}, {<span class="st">"In(Package, Truck)"</span>}, {<span class="st">"At(Package, Depot)"</span>}),</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Drive"</span>: ({<span class="st">"At(Truck, Depot)"</span>}, {<span class="st">"At(Truck, Destination)"</span>}, {<span class="st">"At(Truck, Depot)"</span>}),</span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Unload"</span>: ({<span class="st">"In(Package, Truck)"</span>, <span class="st">"At(Truck, Destination)"</span>}, {<span class="st">"At(Package, Destination)"</span>}, {<span class="st">"In(Package, Truck)"</span>})</span>
<span id="cb173-5"><a href="#cb173-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb173-6"><a href="#cb173-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-7"><a href="#cb173-7" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> [</span>
<span id="cb173-8"><a href="#cb173-8" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Deliver-by-truck"</span>, <span class="st">"Deliver"</span>, [<span class="st">"Load"</span>, <span class="st">"Drive"</span>, <span class="st">"Unload"</span>], {<span class="st">"At(Truck, Depot)"</span>, <span class="st">"At(Package, Depot)"</span>})</span>
<span id="cb173-9"><a href="#cb173-9" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb173-10"><a href="#cb173-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-11"><a href="#cb173-11" aria-hidden="true" tabindex="-1"></a>plan <span class="op">=</span> htn_plan([<span class="st">"Deliver"</span>], methods, actions, {<span class="st">"At(Truck, Depot)"</span>, <span class="st">"At(Package, Depot)"</span>})</span>
<span id="cb173-12"><a href="#cb173-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(plan)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Output:</p>
<pre><code>$$'Load', 'Drive', 'Unload']</code></pre>
</section>
<section id="why-it-matters-100" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-100">Why It Matters</h4>
<ul>
<li>Encodes domain knowledge explicitly, how tasks are normally decomposed.</li>
<li>Enables scalable planning for complex systems (robotics, logistics, simulation).</li>
<li>Bridges the gap between symbolic planning and real-world procedural knowledge.</li>
</ul>
<p>Used in:</p>
<ul>
<li>Robotics (multi-step manipulation tasks)</li>
<li>Game AI (unit control, story planning)</li>
<li>Simulation engines (multi-agent coordination)</li>
</ul>
</section>
<section id="try-it-yourself-99" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-99">Try It Yourself</h4>
<ol type="1">
<li>Build an HTN planner for a household robot.</li>
<li>Add methods for <em>CookMeal</em>, <em>CleanRoom</em>, <em>MakeTea</em>.</li>
<li>Visualize decomposition trees.</li>
<li>Compare plan lengths with and without hierarchical guidance.</li>
</ol>
</section>
<section id="test-cases-99" class="level4">
<h4 class="anchored" data-anchor-id="test-cases-99">Test Cases</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Domain</th>
<th>Task</th>
<th>Plan</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logistics</td>
<td>Deliver(Package)</td>
<td>[Load, Drive, Unload]</td>
</tr>
<tr class="even">
<td>Household</td>
<td>MakeTea</td>
<td>[BoilWater, AddTea, Pour]</td>
</tr>
<tr class="odd">
<td>Game AI</td>
<td>AttackEnemy</td>
<td>[MoveToTarget, Aim, Fire]</td>
</tr>
</tbody>
</table>
</section>
<section id="complexity-99" class="level4">
<h4 class="anchored" data-anchor-id="complexity-99">Complexity</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Measure</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Time</td>
<td>Exponential (depends on branching of methods)</td>
</tr>
<tr class="even">
<td>Space</td>
<td>Linear in decomposition depth</td>
</tr>
</tbody>
</table>
</section>
<section id="a-gentle-proof-why-it-works-95" class="level4">
<h4 class="anchored" data-anchor-id="a-gentle-proof-why-it-works-95">A Gentle Proof (Why It Works)</h4>
<p>HTN preserves soundness: if every method is valid (its subtasks achieve its parent’s goal), then the entire plan is valid.</p>
<p>It’s not <em>complete</em> (it may miss valid plans if no applicable methods exist), but it’s far more <em>practical</em>, guiding search with human-like structure.</p>
</section>
<section id="summary-table-28" class="level4">
<h4 class="anchored" data-anchor-id="summary-table-28">Summary Table</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Framework</td>
<td>Hierarchical symbolic planning</td>
</tr>
<tr class="even">
<td>Core Idea</td>
<td>Decompose complex tasks into simpler subtasks</td>
</tr>
<tr class="odd">
<td>Guarantee</td>
<td>Sound but not complete</td>
</tr>
<tr class="even">
<td>Data Structure</td>
<td>Task network</td>
</tr>
<tr class="odd">
<td>Time Complexity</td>
<td>Exponential</td>
</tr>
<tr class="even">
<td>Space Complexity</td>
<td>Linear</td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>Robotics, simulation, game AI, workflow automation</td>
</tr>
</tbody>
</table>
<p>HTN planning closes our 1000-algorithm journey, from sorting numbers to organizing intelligent behavior. It shows how structure, knowledge, and recursion combine to turn reasoning into action, and action into purpose.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../books/en-us/list-9.html" class="pagination-link" aria-label="Chapter 9. Systems, Databases and Distributed Algorithms">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Chapter 9. Systems, Databases and Distributed Algorithms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>