[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Algorithms",
    "section": "",
    "text": "Roadmap\nThe Little Book of Algorithms is a multi-volume project. Each volume has a clear sequence of chapters, and each chapter has three levels of depth (L0 beginner intuition, L1 practical techniques, L2 advanced systems/theory). This roadmap outlines the plan for development and publication.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "The Little Book of Algorithms",
    "section": "Goals",
    "text": "Goals\n\nEstablish a consistent layered structure across all chapters.\nProvide runnable implementations in Python, C, Go, Erlang, and Lean.\nEnsure Quarto build supports HTML, PDF, EPUB, and LaTeX.\nDeliver both pedagogy (L0) and production insights (L2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#volumes",
    "href": "index.html#volumes",
    "title": "The Little Book of Algorithms",
    "section": "Volumes",
    "text": "Volumes\n\nVolume I - Structures Linéaires\n\nChapter 0 - Foundations\nChapter 1 - Numbers\nChapter 2 - Arrays\nChapter 3 - Strings\nChapter 4 - Linked Lists\nChapter 5 - Stacks & Queues\n\n\n\nVolume II - Algorithmes Fondamentaux\n\nChapter 6 - Searching\nChapter 7 - Selection\nChapter 8 - Sorting\nChapter 9 - Amortized Analysis\n\n\n\nVolume III - Structures Hiérarchiques\n\nChapter 10 - Tree Fundamentals\nChapter 11 - Heaps & Priority Queues\nChapter 12 - Binary Search Trees\nChapter 13 - Balanced Trees & Ordered Maps\nChapter 14 - Range Queries\nChapter 15 - Vector Databases\n\n\n\nVolume IV - Paradigmes Algorithmiques\n\nChapter 16 - Divide-and-Conquer\nChapter 17 - Greedy\nChapter 18 - Dynamic Programming\nChapter 19 - Backtracking & Search\n\n\n\nVolume V - Graphes et Complexité\n\nChapter 20 - Graph Basics\nChapter 21 - DAGs & SCC\nChapter 22 - Shortest Paths\nChapter 23 - Flows & Matchings\nChapter 24 - Tree Algorithms\nChapter 25 - Complexity & Limits\nChapter 26 - External & Cache-Oblivious\nChapter 27 - Probabilistic & Streaming\nChapter 28 - Engineering",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#milestones",
    "href": "index.html#milestones",
    "title": "The Little Book of Algorithms",
    "section": "Milestones",
    "text": "Milestones\n\nComplete detailed outlines for all chapters (L0, L1, L2).\nWrite draft text for all L0 sections (intuition, analogies, simple examples).\nExpand each chapter with L1 content (implementations, correctness arguments, exercises).\nAdd L2 content (systems insights, proofs, optimizations, advanced references).\nDevelop and test runnable code in src/ across Python, C, Go, Erlang, and Lean.\nIntegrate diagrams, figures, and visual explanations.\nFinalize Quarto build setup for HTML, PDF, and EPUB.\nRelease first public edition (HTML + PDF).\nAdd LaTeX build, refine EPUB, and polish cross-references.\nPublish on GitHub Pages and archive DOI.\nGather feedback, refine explanations, and expand exercises/problem sets.\nLong-term: maintain as a living reference with continuous updates and companion volumes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#deliverables",
    "href": "index.html#deliverables",
    "title": "The Little Book of Algorithms",
    "section": "Deliverables",
    "text": "Deliverables\n\nQuarto project with 29 chapters (00–28).\nMulti-language reference implementations.\nLearning matrix in README for navigation.\nROADMAP.md (this file) to track progress.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#long-term-vision",
    "href": "index.html#long-term-vision",
    "title": "The Little Book of Algorithms",
    "section": "Long-term Vision",
    "text": "Long-term Vision\n\nMaintain the repository as a living reference.\nExtend with exercises, problem sets, and quizzes.\nBuild a dependency map across volumes for prerequisites.\nConnect to companion “Little Book” series (linear algebra, calculus, probability).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "Chapter 1. Numbers",
    "section": "",
    "text": "1.1 Representation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#representation",
    "href": "chapter_1.html#representation",
    "title": "Chapter 1. Numbers",
    "section": "",
    "text": "1.1 L0. Decimal and Binary Basics\nA number representation is a way of writing numbers using symbols and positional rules. Humans typically use decimal notation, while computers rely on binary because it aligns with the two-state nature of electronic circuits. Understanding both systems is the first step in connecting mathematical intuition with machine computation.\n\nNumbers in Everyday Life\nHumans work with the decimal system (base 10), which uses digits 0 through 9. Each position in a number has a place value that is a power of 10.\n\\[\n427 = 4 \\times 10^2 + 2 \\times 10^1 + 7 \\times 10^0\n\\]\nThis principle of positional notation is the same idea used in other bases.\n\n\nNumbers in Computers\nComputers, however, operate in binary (base 2). A binary digit (bit) can only be 0 or 1, matching the two stable states of electronic circuits (off/on). Each binary place value represents a power of 2.\n\\[\n1011_2 = 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0 = 11_{10}\n\\]\nJust like in decimal where \\(9 + 1 = 10\\), in binary \\(1 + 1 = 10_2\\).\n\n\nConversion Between Decimal and Binary\nTo convert from decimal to binary, repeatedly divide the number by 2 and record the remainders. Then read the remainders from bottom to top.\nExample: Convert \\(42_{10}\\) into binary.\n\n42 ÷ 2 = 21 remainder 0\n21 ÷ 2 = 10 remainder 1\n10 ÷ 2 = 5 remainder 0\n5 ÷ 2 = 2 remainder 1\n2 ÷ 2 = 1 remainder 0\n1 ÷ 2 = 0 remainder 1\n\nReading upward: \\(101010_2\\).\nTo convert from binary to decimal, expand into powers of 2 and sum:\n\\[\n101010_2 = 1 \\times 2^5 + 0 \\times 2^4 + 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 0 \\times 2^0 = 42_{10}\n\\]\n\n\nWorked Example (Python)\nn = 42\nprint(\"Decimal:\", n)\nprint(\"Binary :\", bin(n))   # 0b101010\n\n# binary literal in Python\nb = 0b101010\nprint(\"Binary literal:\", b)\n\n# converting binary string to decimal\nprint(\"From binary '1011':\", int(\"1011\", 2))\nOutput:\nDecimal: 42\nBinary : 0b101010\nBinary literal: 42\nFrom binary '1011': 11\n\n\nWhy It Matters\n\nAll information inside a computer — numbers, text, images, programs — reduces to binary representation.\nDecimal and binary conversions are the first bridge between human-friendly math and machine-level data.\nUnderstanding binary is essential for debugging, low-level programming, and algorithms that depend on bit operations.\n\n\n\nExercises\n\nWrite the decimal number 19 in binary.\nConvert the binary number 10101₂ into decimal.\nShow the repeated division steps to convert 27 into binary.\nVerify in Python that 0b111111 equals 63.\nExplain why computers use binary instead of decimal.\n\n\n\n\n1.1 L1. Beyond Binary: Octal, Hex, and Two’s Complement\nNumbers are not always written in base-10 or even in base-2. For efficiency and compactness, programmers often use octal (base-8) and hexadecimal (base-16). At the same time, negative numbers must be represented reliably; modern computers use two’s complement for this purpose.\n\nOctal and Hexadecimal\nOctal and hex are simply alternate numeral systems.\n\nOctal (base 8): digits 0–7.\nHexadecimal (base 16): digits 0–9 plus A–F.\n\nWhy they matter:\n\nHex is concise: one hex digit = 4 binary bits.\nOctal was historically convenient: one octal digit = 3 binary bits (useful on early 12-, 24-, or 36-bit machines).\n\nFor example, the number 42 is written as:\n\n\n\nDecimal\nBinary\nOctal\nHex\n\n\n\n\n42\n101010\n52\n2A\n\n\n\n\n\nTwo’s Complement\nTo represent negative numbers, we cannot just “stick a minus sign” in memory. Instead, binary uses two’s complement:\n\nChoose a fixed bit-width (say 8 bits).\nFor a negative number -x, compute 2^bits - x.\nStore the result as an ordinary binary integer.\n\nExample with 8 bits:\n\n+5 → 00000101\n-5 → 11111011\n-1 → 11111111\n\nWhy two’s complement is powerful:\n\nAddition and subtraction “just work” with the same circuitry for signed and unsigned.\nThere is only one representation of zero.\n\n\n\nWorking Example (Python)\n# Decimal 42 in different bases\nn = 42\nprint(\"Decimal:\", n)\nprint(\"Binary :\", bin(n))\nprint(\"Octal  :\", oct(n))\nprint(\"Hex    :\", hex(n))\n\n# Two's complement for -5 in 8 bits\ndef to_twos_complement(x: int, bits: int = 8) -&gt; str:\n    if x &gt;= 0:\n        return format(x, f\"0{bits}b\")\n    return format((1 &lt;&lt; bits) + x, f\"0{bits}b\")\n\nprint(\"+5:\", to_twos_complement(5, 8))\nprint(\"-5:\", to_twos_complement(-5, 8))\nOutput:\nDecimal: 42\nBinary : 0b101010\nOctal  : 0o52\nHex    : 0x2a\n+5: 00000101\n-5: 11111011\n\n\nWhy It Matters\n\nProgrammer convenience: Hex makes binary compact and human-readable.\nHardware design: Two’s complement ensures arithmetic circuits are simple and unified.\nDebugging: Memory dumps, CPU registers, and network packets are usually shown in hex.\n\n\n\nExercises\n\nConvert 100 into binary, octal, and hex.\nWrite -7 in 8-bit two’s complement.\nVerify that 0xFF is equal to 255.\nParse the bitstring \"11111001\" as an 8-bit two’s complement number.\nExplain why engineers prefer two’s complement over “sign-magnitude” representation.\n\n\n\n\n1.1 L2. Floating-Point and Precision Issues\nNot all numbers are integers. To approximate fractions, scientific notation, and very large or very small values, computers use floating-point representation. The de-facto standard is IEEE-754, which defines how real numbers are encoded, how special values are handled, and what precision guarantees exist.\n\nStructure of Floating-Point Numbers\nA floating-point value is composed of three fields:\n\nSign bit (s) — indicates positive (0) or negative (1).\nExponent (e) — determines the scale or “magnitude.”\nMantissa / significand (m) — contains the significant digits.\n\nThe value is interpreted as:\n\\[\n(-1)^s \\times 1.m \\times 2^{(e - \\text{bias})}\n\\]\nExample: IEEE-754 single precision (32 bits)\n\n1 sign bit\n8 exponent bits (bias = 127)\n23 mantissa bits\n\n\n\nExact vs Approximate Representation\nSome numbers are represented exactly:\n\n1.0 has a clean binary form.\n\nOthers cannot be represented precisely:\n\n0.1 in decimal is a repeating fraction in binary, so the closest approximation is stored.\n\nPython example:\na = 0.1 + 0.2\nprint(\"0.1 + 0.2 =\", a)\nprint(\"Equal to 0.3?\", a == 0.3)\nOutput:\n0.1 + 0.2 = 0.30000000000000004\nEqual to 0.3? False\n\n\nSpecial Values\nIEEE-754 reserves encodings for special cases:\n\n\n\nSign\nExponent\nMantissa\nMeaning\n\n\n\n\n0/1\nall 1s\n0\n+∞ / −∞\n\n\n0/1\nall 1s\nnonzero\nNaN (Not a Number)\n\n\n0/1\nall 0s\nnonzero\nDenormals (gradual underflow)\n\n\n\nExamples:\n\nDivision by zero produces infinity: 1.0 / 0.0 = inf.\n0.0 / 0.0 yields NaN, which propagates in computations.\nDenormals allow gradual precision near zero.\n\n\n\nArbitrary Precision\nLanguages like Python and libraries like GMP provide arbitrary-precision arithmetic:\n\nIntegers (int) can grow as large as memory allows.\nDecimal libraries (decimal.Decimal in Python) allow exact decimal arithmetic.\nThese are slower, but essential for cryptography, symbolic computation, and finance.\n\n\n\nWorked Example (Python)\nimport math\n\nprint(\"Infinity:\", 1.0 / 0.0)\nprint(\"NaN:\", 0.0 / 0.0)\n\nprint(\"Is NaN?\", math.isnan(float('nan')))\nprint(\"Is Inf?\", math.isinf(float('inf')))\n\n# Arbitrary precision integer\nbig = 2200\nprint(\"2200 =\", big)\n\n\nWhy It Matters\n\nRounding surprises: Many decimal fractions cannot be represented exactly.\nError propagation: Repeated arithmetic may accumulate tiny inaccuracies.\nSpecial values: NaN and infinity must be handled carefully.\nDomain correctness: Cryptography, finance, and symbolic algebra require exact precision.\n\n\n\nExercises\n\nWrite down the IEEE-754 representation (sign, exponent, mantissa) of 1.0.\nExplain why 0.1 is not exactly representable in binary.\nTest in Python whether float('nan') == float('nan'). What happens, and why?\nFind the smallest positive number you can add to 1.0 before it changes (machine epsilon).\nWhy is arbitrary precision slower but critical in some applications?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#basic-operations",
    "href": "chapter_1.html#basic-operations",
    "title": "Chapter 1. Numbers",
    "section": "1.2 Basic Operations",
    "text": "1.2 Basic Operations\n\n1.2 L0. Addition, Subtraction, Multiplication, Division\nAn arithmetic operation combines numbers to produce a new number. At this level we focus on four basics: addition, subtraction, multiplication, and division—first with decimal intuition, then a peek at how the same ideas look in binary. Mastering these is essential before moving to algorithms that build on them.\n\nIntuition: place value + carrying/borrowing\nAll four operations are versions of combining place values (ones, tens, hundreds …; or in binary: ones, twos, fours …).\n\nAddition: add column by column; if a column exceeds the base, carry 1 to the next column.\nSubtraction: subtract column by column; if a column is too small, borrow 1 from the next column.\nMultiplication: repeated addition; multiply by each digit and shift (place value), then add partial results.\nDivision: repeated subtraction or sharing; find how many times a number “fits,” track the remainder.\n\nThese rules are identical in any base. Only the place values change.\n\n\nDecimal examples (by hand)\n\nAddition (carry)\n\n   478\n + 259\n ----\n   737    (8+9=17 → write 7, carry 1; 7+5+1=13 → write 3, carry 1; 4+2+1=7)\n\nSubtraction (borrow)\n\n   503\n -  78\n ----\n   425    (3-8 borrow → 13-8=5; 0 became -1 so borrow from 5 → 9-7=2; 4 stays 4)\n\nMultiplication (partial sums)\n\n   214\n ×   3\n ----\n   642    (214*3 = 642)\n\nLong division (quotient + remainder)\n\n  47 ÷ 5 → 9 remainder 2   (because 5*9 = 45, leftover 2)\n\n\nBinary peek (same rules, base 2)\n\nAdd rules: 0+0=0, 0+1=1, 1+0=1, 1+1=10₂ (write 0, carry 1)\nSubtract rules: 0−0=0, 1−0=1, 1−1=0, 0−1 → borrow (becomes 10₂−1=1, borrow 1)\n\nExample: \\(1011₂ + 0110₂\\)\n   1011\n + 0110\n ------\n  10001   (1+0=1; 1+1=0 carry1; 0+1+carry=0 carry1; 1+0+carry=0 carry1 → carry out)\n\n\nWorked examples (Python)\n# Basic arithmetic with integers\na, b = 478, 259\nprint(\"a+b =\", a + b)      # 737\nprint(\"a-b =\", a - b)      # 219\nprint(\"a*b =\", a * b)      # 123,  478*259 = 123,  ... actually compute:\nprint(\"47//5 =\", 47 // 5)  # integer division -&gt; 9\nprint(\"47%5  =\", 47 % 5)   # remainder -&gt; 2\n\n# Show carry/borrow intuition using binary strings\nx, y = 0b1011, 0b0110\ns = x + y\nprint(\"x+y (binary):\", bin(x), \"+\", bin(y), \"=\", bin(s))\n\n# Small helper: manual long division that returns (quotient, remainder)\ndef long_divide(n: int, d: int):\n    if d == 0:\n        raise ZeroDivisionError(\"division by zero\")\n    q = n // d\n    r = n % d\n    return q, r\n\nprint(\"long_divide(47,5):\", long_divide(47, 5))  # (9, 2)\n\nNote: // is integer division in Python; % is the remainder. For now we focus on integers (no decimals).\n\n\n\nWhy it matters\n\nEvery higher-level algorithm (searching, hashing, cryptography, numeric methods) relies on these operations.\nUnderstanding carry/borrow makes binary arithmetic and bit-level reasoning feel natural.\nKnowing integer division and remainder is vital for base conversions, hashing (mod), and many algorithmic patterns.\n\n\n\nExercises\n\nCompute by hand, then verify in Python:\n\n\\(326 + 589\\)\n\\(704 - 259\\)\n\\(38 \\times 12\\)\n\\(123 \\div 7\\) (give quotient and remainder)\n\nIn binary, add \\(10101₂ + 111₍₂₎\\). Show carries.\nWrite a short Python snippet that prints the quotient and remainder for n=200 divided by d=23.\nConvert your remainder into a sentence: “200 = 23 × (quotient) + (remainder)”.\nChallenge: Multiply \\(19 \\times 23\\) by hand using partial sums; then check with Python.\n\n\n\n\n1.2 L1. Division, Modulo, and Efficiency\nBeyond the simple four arithmetic operations, programmers need to think about division with remainder, the modulo operator, and how efficient these operations are on real machines. Addition and subtraction are almost always “constant time,” but division can be slower, and understanding modulo is essential for algorithms like hashing, cryptography, and scheduling.\n\nInteger Division and Modulo\nFor integers, division produces both a quotient and a remainder.\n\nMathematical definition: for integers \\(n, d\\) with \\(d \\neq 0\\),\n\\[\nn = d \\times q + r, \\quad 0 \\leq r &lt; |d|\n\\]\nwhere \\(q\\) is the quotient, \\(r\\) the remainder.\nProgramming notation (Python):\n\nn // d → quotient\nn % d → remainder\n\n\nExamples:\n\n47 // 5 = 9, 47 % 5 = 2 because \\(47 = 5 \\times 9 + 2\\).\n23 // 7 = 3, 23 % 7 = 2 because \\(23 = 7 \\times 3 + 2\\).\n\n\n\n\nn\nd\nn // d\nn % d\n\n\n\n\n47\n5\n9\n2\n\n\n23\n7\n3\n2\n\n\n100\n9\n11\n1\n\n\n\n\n\nModulo in Algorithms\nThe modulo operation is a workhorse in programming:\n\nHashing: To map a large integer into a table of size m, use key % m.\nCyclic behavior: To loop back after 7 days in a week: (day + shift) % 7.\nCryptography: Modular arithmetic underlies RSA, Diffie–Hellman, and many number-theoretic algorithms.\n\n\n\nEfficiency Considerations\n\nAddition and subtraction: generally 1 CPU cycle.\nMultiplication: slightly more expensive, but still fast on modern hardware.\nDivision and modulo: slower, often an order of magnitude more costly than multiplication.\n\nPractical tricks:\n\nIf d is a power of two, n % d can be computed by a bitmask.\n\nExample: n % 8 == n & 7 (since 8 = 2³).\n\nSome compilers automatically optimize modulo when the divisor is constant.\n\n\n\nWorked Example (Python)\n# Quotient and remainder\nn, d = 47, 5\nprint(\"Quotient:\", n // d)  # 9\nprint(\"Remainder:\", n % d)  # 2\n\n# Identity check: n == d*q + r\nq, r = divmod(n, d)  # built-in tuple return\nprint(\"Check:\", d*q + r == n)\n\n# Modulo for cyclic behavior: days of week\ndays = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nstart = 5  # Saturday\nshift = 4\nfuture_day = days[(start + shift) % 7]\nprint(\"Start Saturday + 4 days =\", future_day)\n\n# Optimization: power-of-two modulo with bitmask\nfor n in [5, 12, 20]:\n    print(f\"{n} % 8 = {n % 8}, bitmask {n & 7}\")\nOutput:\nQuotient: 9\nRemainder: 2\nCheck: True\nStart Saturday + 4 days = Wed\n5 % 8 = 5, bitmask 5\n12 % 8 = 4, bitmask 4\n20 % 8 = 4, bitmask 4\n\n\nWhy It Matters\n\nReal programs rely heavily on modulo for indexing, hashing, and wrap-around logic.\nDivision is computationally more expensive; knowing when to replace it with bit-level operations improves performance.\nModular arithmetic introduces a new “world” where numbers wrap around — the foundation of many advanced algorithms.\n\n\n\nExercises\n\nCompute by hand and confirm in Python:\n\n100 // 9 and 100 % 9\n123 // 11 and 123 % 11\n\nWrite a function that simulates a clock: given hour and shift, return the new hour (24-hour cycle).\nProve the identity: for any integers n and d,\nn == d * (n // d) + (n % d)\nby trying with random values.\nShow how to replace n % 16 with a bitwise operation. Why does it work?\nChallenge: Write a short Python function to check if a number is divisible by 7 using only % and //.\n\n\n\n\n1.2 L2. Fast Arithmetic Algorithms\nWhen numbers grow large, the naïve methods for multiplication and division become too slow. On paper, long multiplication takes \\(O(n^2)\\) steps for \\(n\\)-digit numbers. Computers face the same issue: multiplying two very large integers digit by digit can be expensive. Fast arithmetic algorithms reduce this cost, using clever divide-and-conquer techniques or transformations into other domains.\n\nMultiplication Beyond the School Method\nNaïve long multiplication\n\nTreats an \\(n\\)-digit number as a sequence of digits.\nEach digit of one number multiplies every digit of the other.\nComplexity: \\(O(n^2)\\).\nWorks fine for small integers, but too slow for cryptography or big-number libraries.\n\nKaratsuba’s Algorithm\n\nDiscovered in 1960 by Anatoly Karatsuba.\nIdea: split numbers into halves and reduce multiplications.\nComplexity: \\(O(n^{\\log_2 3}) \\approx O(n^{1.585})\\).\nRecursive strategy:\n\nFor numbers \\(x = x_1 \\cdot B^m + x_0\\), \\(y = y_1 \\cdot B^m + y_0\\).\nCompute 3 multiplications instead of 4:\n\n\\(z_0 = x_0 y_0\\)\n\\(z_2 = x_1 y_1\\)\n\\(z_1 = (x_0+x_1)(y_0+y_1) - z_0 - z_2\\)\n\nResult: \\(z_2 \\cdot B^{2m} + z_1 \\cdot B^m + z_0\\).\n\n\nFFT-based Multiplication (Schönhage–Strassen and successors)\n\nRepresent numbers as polynomials of their digits.\nMultiply polynomials efficiently using Fast Fourier Transform.\nComplexity: near \\(O(n \\log n)\\).\nUsed in modern big-integer libraries (e.g. GNU MP, Java’s BigInteger).\n\n\n\nDivision Beyond Long Division\n\nNaïve long division: \\(O(n^2)\\) for \\(n\\)-digit dividend.\nNewton’s method for reciprocal: approximate \\(1/d\\) using Newton–Raphson iterations, then multiply by \\(n\\).\nComplexity: tied to multiplication — if multiplication is fast, so is division.\n\n\n\nModular Exponentiation\nFast arithmetic also matters in modular contexts (cryptography).\n\nCompute \\(a^b \\bmod m\\) efficiently.\nSquare-and-multiply (binary exponentiation):\n\nWrite \\(b\\) in binary.\nFor each bit: square result, multiply if bit=1.\nComplexity: \\(O(\\log b)\\) multiplications.\n\n\n\n\nWorked Example (Python)\n# Naïve multiplication\ndef naive_mul(x: int, y: int) -&gt; int:\n    return x * y  # Python already uses fast methods internally\n\n# Karatsuba multiplication (recursive, simplified)\ndef karatsuba(x: int, y: int) -&gt; int:\n    # base case\n    if x &lt; 10 or y &lt; 10:\n        return x * y\n    # split numbers\n    n = max(x.bit_length(), y.bit_length())\n    m = n // 2\n    high1, low1 = divmod(x, 1 &lt;&lt; m)\n    high2, low2 = divmod(y, 1 &lt;&lt; m)\n    z0 = karatsuba(low1, low2)\n    z2 = karatsuba(high1, high2)\n    z1 = karatsuba(low1 + high1, low2 + high2) - z0 - z2\n    return (z2 &lt;&lt; (2*m)) + (z1 &lt;&lt; m) + z0\n\n# Modular exponentiation (square-and-multiply)\ndef modexp(a: int, b: int, m: int) -&gt; int:\n    result = 1\n    base = a % m\n    exp = b\n    while exp &gt; 0:\n        if exp & 1:\n            result = (result * base) % m\n        base = (base * base) % m\n        exp &gt;&gt;= 1\n    return result\n\n# Demo\nprint(\"Karatsuba(1234, 5678) =\", karatsuba(1234, 5678))\nprint(\"pow(7, 128, 13) =\", modexp(7, 128, 13))  # fast modular exponentiation\nOutput:\nKaratsuba(1234, 5678) = 7006652\npow(7, 128, 13) = 3\n\n\nWhy It Matters\n\nCryptography: RSA requires multiplying and dividing integers with thousands of digits.\nComputer algebra systems: symbolic computation depends on fast polynomial/integer arithmetic.\nBig data / simulation: arbitrary precision needed when floats are not exact.\n\n\n\nExercises\n\nMultiply 31415926 × 27182818 using:\n\nPython’s *\nYour Karatsuba implementation. Compare results.\n\nImplement modexp(a, b, m) for \\(a=5, b=117, m=19\\). Confirm with Python’s built-in pow(a, b, m).\nExplain why Newton’s method for division depends on fast multiplication.\nResearch: what is the current fastest known multiplication algorithm for large integers?\nChallenge: Modify Karatsuba to print intermediate z0, z1, z2 values for small inputs to visualize the recursion.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#properties",
    "href": "chapter_1.html#properties",
    "title": "Chapter 1. Numbers",
    "section": "1.3 Properties",
    "text": "1.3 Properties\n\n1.3 L0 — Simple Number Properties\nNumbers have patterns that help us reason about algorithms without heavy mathematics. At this level we focus on basic properties: even vs odd, divisibility, and remainders. These ideas show up everywhere—from loop counters to data structure layouts.\n\nEven and Odd\nA number is even if it ends with digit 0, 2, 4, 6, or 8 in decimal, and odd otherwise.\n\nIn binary, checking parity is even easier: the last bit tells the story.\n\n…0 → even\n…1 → odd\n\n\nExample in Python:\ndef is_even(n: int) -&gt; bool:\n    return n % 2 == 0\n\nprint(is_even(10))  # True\nprint(is_even(7))   # False\n\n\nDivisibility\nWe often ask: does one number divide another?\n\na is divisible by b if there exists some integer k with a = b * k.\nIn code: a % b == 0.\n\nExamples:\n\n12 is divisible by 3 → 12 % 3 == 0.\n14 is not divisible by 5 → 14 % 5 == 4.\n\n\n\nRemainders and Modular Thinking\nWhen dividing, the remainder is what’s left over.\n\nExample: 17 // 5 = 3, remainder 2.\nModular arithmetic wraps around like a clock:\n\n(17 % 5) = 2 → same as “2 o’clock after going 17 steps around a 5-hour clock.”\n\n\nThis “wrap-around” view is central in array indexing, hashing, and cryptography later on.\n\n\nWhy It Matters\n\nAlgorithms: Parity checks decide branching (e.g., even-odd optimizations).\nData structures: Array indices often wrap around using %.\nEveryday: Calendars cycle days of the week; remainders formalize that.\n\n\n\nExercises\n\nWrite a function that returns \"even\" or \"odd\" for a given number.\nCheck if 91 is divisible by 7.\nCompute the remainder of 100 divided by 9.\nUse % to simulate a 7-day week: if today is day 5 (Saturday) and you add 10 days, what day is it?\nFind the last digit of 2^15 without computing the full number (hint: check the remainder mod 10).\n\n\n\n\n1.3 L1 — Classical Number Theory Tools\nBeyond simple parity and divisibility, algorithms often need deeper number properties. At this level we introduce a few “toolkit” ideas from elementary number theory: greatest common divisor (GCD), least common multiple (LCM), and modular arithmetic identities. These are lightweight but powerful concepts that show up in algorithm design, cryptography, and optimization.\n\nGreatest Common Divisor (GCD)\nThe GCD of two numbers is the largest number that divides both.\n\nExample: gcd(20, 14) = 2.\nWhy useful: GCD simplifies fractions, ensures ratios are reduced, and appears in algorithm correctness proofs.\n\nEuclid’s Algorithm: Instead of trial division, we can compute GCD quickly:\ngcd(a, b) = gcd(b, a % b)\nThis repeats until b = 0, at which point a is the answer.\nPython example:\ndef gcd(a: int, b: int) -&gt; int:\n    while b:\n        a, b = b, a % b\n    return a\n\nprint(gcd(20, 14))  # 2\n\n\nLeast Common Multiple (LCM)\nThe LCM of two numbers is the smallest positive number divisible by both.\n\nExample: lcm(12, 18) = 36.\nConnection to GCD:\nlcm(a, b) = (a * b) // gcd(a, b)\n\nThis is useful in scheduling, periodic tasks, and synchronization problems.\n\n\nModular Arithmetic Identities\nRemainders behave predictably under operations:\n\nAddition: (a + b) % m = ((a % m) + (b % m)) % m\nMultiplication: (a * b) % m = ((a % m) * (b % m)) % m\n\nExample:\n\n(123 + 456) % 7 = (123 % 7 + 456 % 7) % 7\nThis property lets us work with small remainders instead of huge numbers, key in cryptography and hashing.\n\n\n\nWhy It Matters\n\nAlgorithms: GCD ensures efficiency in fraction reduction, graph algorithms, and number-theoretic algorithms.\nSystems: LCM models periodicity, e.g., aligning CPU scheduling intervals.\nCryptography: Modular arithmetic underpins secure communication (RSA, Diffie-Hellman).\nPractical programming: Modular identities simplify computations with limited ranges (hash tables, cyclic arrays).\n\n\n\nExercises\n\nCompute gcd(252, 198) by hand using Euclid’s algorithm.\nWrite a function that returns the LCM of two numbers. Test it on (12, 18).\nShow that (37 + 85) % 12 equals ((37 % 12) + (85 % 12)) % 12.\nReduce the fraction 84/126 using GCD.\nFind the smallest day d such that d is a multiple of both 12 and 18 (hint: LCM).\n\n\n\n\n1.3 L2 — Advanced Number Theory in Algorithms\nAt this level, we move beyond everyday divisibility and Euclid’s algorithm. Modern algorithms frequently rely on deep number theory to achieve efficiency. Topics such as modular inverses, Euler’s totient function, and primality tests are crucial foundations for cryptography, randomized algorithms, and competitive programming.\n\nModular Inverses\nThe modular inverse of a number a (mod m) is an integer x such that:\n(a * x) % m = 1\n\nExample: the inverse of 3 modulo 7 is 5, because (3*5) % 7 = 15 % 7 = 1.\nExistence: an inverse exists if and only if gcd(a, m) = 1.\nComputation: using the Extended Euclidean Algorithm.\n\nThis is the backbone of modular division and is heavily used in cryptography (RSA), hash functions, and matrix inverses mod p.\n\n\nEuler’s Totient Function (φ)\nThe function φ(n) counts the number of integers between 1 and n that are coprime to n.\n\nExample: φ(9) = 6 because {1, 2, 4, 5, 7, 8} are coprime to 9.\nKey property (Euler’s theorem):\na^φ(n) ≡ 1 (mod n)     if gcd(a, n) = 1\nSpecial case: Fermat’s Little Theorem — for prime p,\na^(p-1) ≡ 1 (mod p)\n\nThis result is central in modular exponentiation and cryptosystems like RSA.\n\n\nPrimality Testing\nDetermining if a number is prime is easy for small inputs but hard for large ones. Efficient algorithms are essential:\n\nTrial division: works only for small n.\nFermat primality test: uses Fermat’s Little Theorem to detect composites, but can be fooled by Carmichael numbers.\nMiller–Rabin test: a probabilistic algorithm widely used in practice (cryptographic key generation).\nAKS primality test: a deterministic polynomial-time method (theoretical importance).\n\nExample intuition:\n\nFor large n, we don’t check all divisors; we test properties of a^k mod n for random bases a.\n\n\n\nWhy It Matters\n\nCryptography: Public-key systems depend on modular inverses, Euler’s theorem, and large primes.\nAlgorithms: Modular inverses simplify solving equations in modular arithmetic (e.g., Chinese Remainder Theorem applications).\nPractical Computing: Randomized primality tests (like Miller–Rabin) balance correctness and efficiency in real-world systems.\n\n\n\nExercises\n\nFind the modular inverse of 7 modulo 13.\nCompute φ(10) and verify Euler’s theorem for a = 3.\nUse Fermat’s test to check whether 341 is prime. (Hint: try a = 2.)\nImplement modular inverse using the Extended Euclidean Algorithm.\nResearch: why do cryptographic protocols prefer Miller–Rabin over AKS, even though AKS is deterministic?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#overflow-precision",
    "href": "chapter_1.html#overflow-precision",
    "title": "Chapter 1. Numbers",
    "section": "1.4 Overflow & Precision",
    "text": "1.4 Overflow & Precision\n\n1.4 L0 - When Numbers Get Too Big or Too Small\nNumbers inside a computer are stored with a fixed number of bits. This means they can only represent values up to a certain limit. If a calculation produces a result larger than this limit, the value “wraps around,” much like the digits on an odometer rolling over after 999 to 000. This phenomenon is called overflow. Similarly, computers often cannot represent all decimal fractions exactly, leading to tiny errors called precision loss.\n\nDeep Dive\n\nInteger Overflow\n\nA computer uses a fixed number of bits (commonly 8, 16, 32, or 64) to store integers.\nAn 8-bit unsigned integer can represent values from 0 to 255. Adding 1 to 255 causes the value to wrap back to 0.\nSigned integers use two’s complement representation. For an 8-bit signed integer, the range is −128 to +127. Adding 1 to 127 makes it overflow to −128.\n\nExample in binary:\n11111111₂ (255) + 1 = 00000000₂ (0)\n01111111₂ (+127) + 1 = 10000000₂ (−128)\nFloating-Point Precision\n\nDecimal fractions like 0.1 cannot always be represented exactly in binary.\nAs a result, calculations may accumulate tiny errors.\nFor example, repeatedly adding 0.1 may not exactly equal 1.0 due to precision limits.\n\n\n\n\nExample\n# Integer overflow simulation with 8-bit values\ndef add_8bit(a, b):\n    result = (a + b) % 256  # simulate wraparound\n    return result\n\nprint(add_8bit(250, 10))   # 260 wraps to 4\nprint(add_8bit(255, 1))    # wraps to 0\n\n# Floating-point precision issue\nx = 0.1 + 0.2\nprint(x)           # Expected 0.3, but gives 0.30000000000000004\nprint(x == 0.3)    # False\n\n\nWhy It Matters\n\nUnexpected results: A calculation may suddenly produce a negative number or wrap around to zero.\nReal-world impact:\n\nVideo games may show scores jumping strangely if counters overflow.\nBanking or financial systems must avoid losing cents due to floating-point errors.\nEngineers and scientists rely on careful handling of precision to ensure correct simulations.\n\nFoundation for algorithms: Understanding overflow and precision prepares you for later topics like hashing, cryptography, and numerical analysis.\n\n\n\nExercises\n\nSimulate a 4-bit unsigned integer system. What happens if you start at 14 and keep adding 1?\nIn Python, try adding 0.1 to itself ten times. Does it equal exactly 1.0? Why or why not?\nWrite a function that checks if an 8-bit signed integer addition would overflow.\nImagine you are programming a digital clock that uses 2 digits for minutes (00–59). What happens when the value goes from 59 to 60? How would you handle this?\n\n\n\n\n1.4 L1 - Detecting and Managing Overflow in Real Programs\nComputers don’t do math in the abstract. Integers live in fixed-width registers; floats follow IEEE-754. Robust software accounts for these limits up front: choose the right representation, detect overflow, and compare floats safely. The following sections explain how these issues show up in practice and how to design around them.\n\nDeep Dive\n\n1) Integer arithmetic in practice\nFixed width means wraparound at \\(2^{n}\\). Unsigned wrap is modular arithmetic; signed overflow (two’s complement) can flip signs. Developers often discover this the hard way when a counter suddenly goes negative or wraps back to zero in production logs.\nBit width & ranges This table reminds us of the hard limits baked into hardware. Once the range is exceeded, the value doesn’t “grow bigger”—it wraps.\n\n\n\n\n\n\n\n\nWidth\nSigned range\nUnsigned range\n\n\n\n\n32\n−2,147,483,648 … 2,147,483,647\n0 … 4,294,967,295\n\n\n64\n−9,223,372,036,854,775,808 … 9,223,372,036,854,775,807\n0 … 18,446,744,073,709,551,615\n\n\n\nOverflow semantics by language Each language makes slightly different promises. This matters if you’re writing cross-language services or reading binary data across APIs.\n\n\n\n\n\n\n\n\n\nLanguage\nSigned overflow\nUnsigned overflow\nNotes\n\n\n\n\nC/C++\nUB (undefined)\nModular wrap\nUse UBSan/-fsanitize=undefined; widen types or check before add.\n\n\nRust\nTraps in debug; defined APIs\nwrapping_add, checked_add, saturating_add\nMake intent explicit.\n\n\nJava/Kotlin\nWraps (two’s complement)\nN/A (only signed types)\nUse Math.addExact to trap.\n\n\nC#\nWraps by default; checked to trap\nchecked/unchecked blocks\ndecimal type for money.\n\n\nPython\nArbitrary precision\nArbitrary precision\nSimulates fixed width if needed.\n\n\n\nA quick lesson: “wrap” may be safe in crypto or hashing, but it’s usually a bug in counters or indices. Always decide what you want up front.\n\n\n2) Floating-point you can depend on\nIEEE-754 doubles have ~15–16 decimal digits and huge dynamic range, but not exact decimal fractions. Think of floats as convenient approximations. They are perfect for physics simulations, but brittle when used to represent cents in a bank account.\nWhere precision is lost These examples show why “0.1 + 0.2 != 0.3” isn’t a joke—it’s a direct consequence of binary storage.\n\nScale mismatch: \\(1\\text{e}16 + 1 = 1\\text{e}16\\). The tiny +1 gets lost.\nCancellation: subtracting nearly equal numbers deletes significant digits.\nDecimal fractions (0.1) are repeating in binary.\n\nComparing floats Never compare with ==. Instead, use a mixed relative + absolute check:\n\\[\n|x-y| \\le \\max(\\text{rel}\\cdot\\max(|x|,|y|),\\ \\text{abs})\n\\]\nThis makes comparisons robust whether you’re near zero or far away.\nRounding modes (when you explicitly care) Most of the time you don’t think about rounding—hardware defaults to “round to nearest, ties to even.” But when writing financial systems or interval arithmetic, you want to control it.\n\n\n\n\n\n\n\nMode\nTypical use\n\n\n\n\nRound to nearest, ties to even (default)\nGeneral numeric work; minimizes bias\n\n\nToward 0 / ±∞\nBounds, interval arithmetic, conservative estimates\n\n\n\nHaving explicit rounding modes is like having a steering wheel—you don’t always turn, but you’re glad it’s there when the road curves.\nSummation strategies The order of addition matters for floats. These options give you a menu of accuracy vs. speed.\n\n\n\n\n\n\n\n\n\nMethod\nError\nCost\nWhen to use\n\n\n\n\nNaïve left-to-right\nWorst\nLow\nNever for sensitive sums\n\n\nPairwise / tree\nBetter\nMed\nParallel reductions, “good default”\n\n\nKahan (compensated)\nBest\nHigher\nFinancial-ish aggregates, small vectors\n\n\n\nYou don’t need Kahan everywhere, but knowing it exists keeps you from blaming “mystery bugs” on hardware.\nRepresentation choices Sometimes the best answer is to avoid floats entirely. Money is the classic example.\n\n\n\n\n\n\n\nUse case\nRecommended representation\n\n\n\n\nCurrency, invoicing\nFixed-point (e.g., cents as int64) or decimal/BigDecimal\n\n\nScientific compute\nfloat64, compensated sums, stable algorithms\n\n\nIDs, counters\nuint64/int64, detect overflow on boundaries\n\n\n\n\n\n\nCode (Python—portable patterns)\n# 32-bit checked add (raises on overflow)\ndef add_i32_checked(a: int, b: int) -&gt; int:\n    s = a + b\n    if s &lt; -2_147_483_648 or s &gt; 2_147_483_647:\n        raise OverflowError(\"int32 overflow\")\n    return s\n\n# Simulate 32-bit wrap (intentional modular arithmetic)\ndef add_i32_wrapping(a: int, b: int) -&gt; int:\n    s = (a + b) & 0xFFFFFFFF\n    return s - 0x100000000 if s & 0x80000000 else s\n\n# Relative+absolute epsilon float compare\ndef almost_equal(x: float, y: float, rel=1e-12, abs_=1e-12) -&gt; bool:\n    return abs(x - y) &lt;= max(rel * max(abs(x), abs(y)), abs_)\n\n# Kahan (compensated) summation\ndef kahan_sum(xs):\n    s = 0.0\n    c = 0.0\n    for x in xs:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\n# Fixed-point cents (safe for ~±9e16 cents with int64)\ndef dollars_to_cents(d: str) -&gt; int:\n    whole, _, frac = d.partition(\".\")\n    frac = (frac + \"00\")[:2]\n    return int(whole) * 100 + int(frac)\n\ndef cents_to_dollars(c: int) -&gt; str:\n    sign = \"-\" if c &lt; 0 else \"\"\n    c = abs(c)\n    return f\"{sign}{c//100}.{c%100:02d}\"\nThese examples are in Python for clarity, but the same ideas exist in every major language.\n\n\nWhy it matters\n\nReliability: Silent wrap or float drift becomes data corruption under load or over time.\nInteroperability: Services in different languages disagree on overflow; define and document your contracts.\nReproducibility: Deterministic numerics (same inputs → same bits) depend on summation order, rounding, and libraries.\nSecurity: UB-triggered overflows can turn into exploitable states.\n\nThis is why “it worked on my laptop” is not enough. You want to be sure it works on every platform, every time.\n\n\nExercises\n\nOverflow policy: For a metrics pipeline, decide where to use checked, wrapping, and saturating addition—and justify each with failure modes.\nULP probe: Find the smallest \\(\\epsilon\\) such that 1.0 + ε != 1.0 in your language; explain how it relates to machine epsilon.\nSummation bake-off: Sum the first 1M terms of the harmonic series with naïve, pairwise, and Kahan methods; compare results and timings.\nFixed-point ledger: Implement deposit/transfer/withdraw using int64 cents; prove no rounding loss for two-decimal currencies.\nBoundary tests: Write property tests that add_i32_checked raises on {INT_MAX,1} and {INT_MIN,-1}, and equals modular add where documented.\nCross-lang contract: Specify a JSON schema for monetary amounts and counters that avoids float types; include examples and edge cases.\n\nGreat — let’s rework 1.4 Overflow & Precision (L2) into a friendlier deep dive, using the same pattern: structured sections, tables, and added “bridge” sentences that guide the reader through complex, low-level material. This version should be dense enough to teach internals, but smooth enough to read without feeling like a spec sheet.\n\n\n\n1.4 L2. Under the Hood\nAt the lowest level, overflow and precision aren’t abstract concepts—they are consequences of how CPUs, compilers, and libraries actually implement arithmetic. Understanding these details makes debugging easier and gives you control over performance, reproducibility, and correctness.\n\nDeep Dive\n\n1) Hardware semantics\nCPUs implement integer and floating-point arithmetic directly in silicon. When the result doesn’t fit, different flags or traps are triggered.\n\nStatus flags (integers): Most architectures (x86, ARM, RISC-V) set overflow, carry, and zero flags after arithmetic. These flags drive branch instructions like jo (“jump if overflow”).\nFloating-point control: The FPU or SIMD unit maintains exception flags (inexact, overflow, underflow, invalid, divide-by-zero). These rarely trap by default; they silently set flags until explicitly checked.\n\nArchitectural view\n\n\n\n\n\n\n\n\n\nArch\nInteger overflow\nFP behavior\nDeveloper hooks\n\n\n\n\nx86-64\nWraparound in 2’s complement; OF/CF bits set\nIEEE-754; flags in MXCSR\njo/jno, fenv.h\n\n\nARM64\nWraparound; NZCV flags\nIEEE-754; exception bits\ncondition codes, feset*\n\n\nRISC-V\nWraparound; OV/CF optional\nIEEE-754; status regs\nCSRs, trap handlers\n\n\n\nKnowing what the CPU does lets you choose: rely on hardware wrap, trap explicitly, or add software checks.\n\n\n2) Compiler and language layers\nEven if hardware sets flags, your language may ignore them. Compilers often optimize based on the language spec.\n\nC/C++: Signed overflow is undefined behavior—the optimizer assumes it never happens, which can remove safety checks you thought were there.\nRust: Catches overflow in debug builds, then forces you to pick: checked_add, wrapping_add, or saturating_add.\nJVM languages (Java, Kotlin, Scala): Always wrap, hiding UB but forcing you to detect overflow yourself.\n.NET (C#, F#): Defaults to wrapping; you can enable checked contexts to trap.\nPython: Emulates unbounded integers, but sometimes simulates C-like behavior for low-level modules.\n\nThese choices aren’t arbitrary—they reflect trade-offs between speed, safety, and backward compatibility.\n\n\n3) Precision management in floating point\nFloating-point has more than just rounding errors. Engineers deal with gradual underflow, denormals, and fused operations.\n\nSubnormals: Numbers smaller than ~2.2e-308 in double precision become “denormalized,” losing precision but extending the range toward zero. Many CPUs handle these slowly.\nFlush-to-zero: Some systems skip subnormals entirely, treating them as zero to boost speed. Great for graphics; risky for scientific code.\nFMA (fused multiply-add): Computes (a*b + c) with one rounding, often improving precision and speed. However, it can break reproducibility across machines that do/don’t use FMA.\n\nPrecision events\n\n\n\n\n\n\n\n\nEvent\nWhat happens\nWhy it matters\n\n\n\n\nOverflow\nBecomes ±Inf\nDetectable via isinf, often safe\n\n\nUnderflow\nBecomes 0 or subnormal\nPerformance hit, possible accuracy loss\n\n\nInexact\nResult rounded\nHappens constantly; only matters if flagged\n\n\nInvalid\nNaN produced\nDivision 0/0, sqrt(-1), etc.\n\n\n\nWhen performance bugs show up in HPC or ML code, denormals and FMAs are often the hidden cause.\n\n\n4) Debugging and testing tools\nLow-level correctness requires instrumentation. Fortunately, toolchains give you options.\n\nSanitizers: -fsanitize=undefined (Clang/GCC) traps on signed overflow.\nValgrind / perf counters: Can catch denormal slowdowns.\nUnit-test utilities: Rust’s assert_eq!(checked_add(…)), Python’s math.isclose, Java’s BigDecimal reference checks.\nReproducibility flags: -ffast-math (fast but non-deterministic), vs. -frounding-math (strict).\n\nTesting with multiple compilers and settings reveals assumptions you didn’t know you had.\n\n\n5) Strategies in production systems\nWhen deploying real systems, you pick policies that match domain needs.\n\nDatabases: Use DECIMAL(p,s) to store fixed-point, preventing float drift in sums.\nFinancial systems: Explicit fixed-point types (cents as int64) + saturating logic on overflow.\nGraphics / ML: Accept float32 imprecision; gain throughput with fused ops and flush-to-zero.\nLow-level kernels: Exploit modular wraparound deliberately for hash tables, checksums, and crypto.\n\nPolicy menu\n\n\n\n\n\n\n\nScenario\nStrategy\n\n\n\n\nMoney transfers\nFixed-point, saturating arithmetic\n\n\nPhysics sim\nfloat64, stable integrators, compensated summation\n\n\nHashing / RNG\nEmbrace wraparound modular math\n\n\nCritical counters\nuint64 with explicit overflow trap\n\n\n\nThinking in policies avoids one-off hacks. Document “why” once, then apply consistently.\n\n\n\nCode Examples\nC (wrap vs check)\n#include &lt;stdint.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;limits.h&gt;\n\nbool add_checked_i32(int32_t a, int32_t b, int32_t *out) {\n    if ((b &gt; 0 && a &gt; INT32_MAX - b) ||\n        (b &lt; 0 && a &lt; INT32_MIN - b)) {\n        return false; // overflow\n    }\n    *out = a + b;\n    return true;\n}\nRust (explicit intent)\nfn demo() {\n    let x: i32 = i32::MAX;\n    println!(\"{:?}\", x.wrapping_add(1));   // wrap\n    println!(\"{:?}\", x.checked_add(1));    // None\n    println!(\"{:?}\", x.saturating_add(1)); // clamp\n}\nPython (reproducibility check)\nimport math\n\ndef ulp_diff(a: float, b: float) -&gt; int:\n    # Compares floats in terms of ULPs\n    import struct\n    ai = struct.unpack('!q', struct.pack('!d', a))[0]\n    bi = struct.unpack('!q', struct.pack('!d', b))[0]\n    return abs(ai - bi)\n\nprint(ulp_diff(1.0, math.nextafter(1.0, 2.0)))  # 1\nThese snippets show how different languages force you to state your policy, rather than relying on “whatever the hardware does.”\n\n\nWhy it matters\n\nPerformance: Understanding denormals and FMAs can save orders of magnitude in compute-heavy workloads.\nCorrectness: Database money columns or counters in billing systems can silently corrupt without fixed-point or overflow checks.\nPortability: Code that relies on UB may “work” on GCC Linux but fail on Clang macOS.\nSecurity: Integer overflow bugs (e.g., buffer length miscalculation) remain a classic vulnerability class.\n\nIn short, overflow and precision are not “just math”—they are systems-level contracts that must be understood and enforced.\n\n\nExercises\n\nCompiler behavior: Write a C function that overflows int32_t. Compile with and without -fsanitize=undefined. What changes?\nFMA investigation: Run a dot-product with and without -ffast-math. Measure result differences across compilers.\nDenormal trap: Construct a loop multiplying by 1e-308. Time it with flush-to-zero enabled vs disabled.\nPolicy design: For an in-memory database, define rules for counters, timestamps, and currency columns. Which use wrapping, which use fixed-point, which trap?\nCross-language test: Implement add_checked_i32 in C, Rust, and Python. Run edge-case inputs (INT_MAX, INT_MIN). Compare semantics.\nULP meter: Write a function in your language to compute ULP distance between two floats. Use it to compare rounding differences between platforms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "Chapter 2. Arrays",
    "section": "",
    "text": "2.1 Static Arrays",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#static-arrays",
    "href": "chapter_2.html#static-arrays",
    "title": "Chapter 2. Arrays",
    "section": "",
    "text": "2.2 L0 — Arrays That Grow\nA dynamic array is like a container that can expand and shrink as needed. Unlike static arrays, which must know their size in advance, a dynamic array adapts as elements are added or removed. You can think of it as a bookshelf where new shelves appear automatically when space runs out. The underlying idea is simple: keep the benefits of fast index-based access, while adding flexibility to change the size.\n\nDeep Dive\nA dynamic array begins with a fixed amount of space called its capacity. When the number of elements (the length) exceeds this capacity, the array grows. This is usually done by allocating a new, larger block of memory and copying the old elements into it. After this, new elements can be added until the new capacity is filled, at which point the process repeats.\nDespite this resizing process, the key properties remain:\n\nFast access and update: Elements can still be reached instantly using an index.\nAppend flexibility: New elements can be added at the end without worrying about fixed size.\nOccasional resizing cost: Most appends are quick, but when resizing happens, it takes longer because all elements must be copied.\n\nThe performance picture is intuitive:\n\n\n\n\n\n\n\n\nOperation\nTime Complexity (Typical)\nNotes\n\n\n\n\nAccess element\nO(1)\nIndex maps directly to position\n\n\nUpdate element\nO(1)\nReplace value in place\n\n\nAppend element\nO(1) amortized\nOccasionally O(n) when resizing occurs\n\n\nPop element\nO(1)\nRemove from end\n\n\nInsert/Delete\nO(n)\nElements must be shifted\n\n\n\nDynamic arrays therefore trade predictability for flexibility. The occasional slow operation is outweighed by the ability to grow and shrink on demand, which makes them useful for most real-world tasks where the number of elements is not known in advance.\n\n\nWorked Example\n# Create a dynamic array using Python's built-in list\narr = []\n\n# Append elements (array grows automatically)\nfor i in range(5):\n    arr.append((i + 1) * 10)\n\nprint(\"Array after appending:\", arr)\n\n# Access and update elements\nprint(\"Element at index 2:\", arr[2])\narr[2] = 99\nprint(\"Updated array:\", arr)\n\n# Remove last element\nlast = arr.pop()\nprint(\"Removed element:\", last)\nprint(\"Array after pop:\", arr)\n\n# Traverse array\nfor i in range(len(arr)):\n    print(f\"Index {i}: {arr[i]}\")\nThis short program shows how a dynamic array in Python resizes automatically with append and shrinks with pop. Access and updates remain instant, while resizing happens invisibly when more space is needed.\n\n\nWhy it matters\nDynamic arrays combine efficiency and flexibility. They allow programs to handle unknown or changing amounts of data without predefining sizes. They form the backbone of lists in high-level languages, balancing performance with usability. They also illustrate the idea of amortized cost: most operations are fast, but occasional expensive operations are averaged out over time.\n\n\nExercises\n\nCreate an array and append numbers 1 through 10. Print the final array.\nReplace the 3rd element with a new value.\nRemove the last two elements and print the result.\nWrite a procedure that traverses a dynamic array and computes the average of its elements.\nExplain why appending one element might sometimes be much slower than appending another, even though both look the same in code.\n\n\n\n\n2.1 L1 — Static Arrays in Practice\nStatic arrays are one of the simplest and most reliable ways of storing data. They are defined as collections of elements laid out in a fixed-size, contiguous block of memory. Unlike dynamic arrays, their size is determined at creation and cannot be changed later. This property makes them predictable, efficient, and easy to reason about, but also less flexible when dealing with varying amounts of data.\n\nDeep Dive\nAt the heart of static arrays is their memory layout. When an array is created, the program reserves a continuous region of memory large enough to hold all its elements. Each element is stored right next to the previous one. This design allows very fast access because the position of any element can be computed directly:\naddress_of(arr[i]) = base_address + (i × element_size)\nNo searching or scanning is required, only simple arithmetic. This is why reading or writing to an element at a given index is considered O(1) — constant time regardless of the array size.\nThe trade-offs emerge when considering insertion or deletion. Because elements are tightly packed, inserting a new element in the middle requires shifting all the subsequent elements by one position. Deleting works the same way in reverse. These operations are therefore O(n), linear in the size of the array.\nThe cost summary is straightforward:\n\n\n\nOperation\nTime Complexity\nNotes\n\n\n\n\nAccess element\nO(1)\nDirect index calculation\n\n\nUpdate element\nO(1)\nReplace in place\n\n\nTraverse\nO(n)\nVisit each element once\n\n\nInsert/Delete\nO(n)\nShifting elements required\n\n\n\n\nTrade-offs.\nStatic arrays excel when you know the size in advance. They guarantee fast access and compact memory usage because there is no overhead for resizing or metadata. However, they lack flexibility. If the array is too small, you must allocate a larger one and copy all elements over. If it is too large, memory is wasted. This is why languages like Python provide dynamic lists by default, while static arrays are used in performance-critical or resource-constrained contexts.\n\n\nUse cases.\n\nBuffers: Fixed-size areas for network packets or hardware input.\nLookup tables: Precomputed constants or small ranges of values (e.g., ASCII character tables).\nStatic configuration data: Tables known at compile-time, where resizing is unnecessary.\n\n\n\nPitfalls.\nProgrammers must be careful of two common issues:\n\nOut-of-bounds errors: Trying to access an index outside the valid range, leading to exceptions (in safe languages) or undefined behavior (in low-level languages).\nSizing problems: Underestimating leads to crashes, overestimating leads to wasted memory.\n\nStatic arrays are common in many programming environments. In Python, the array module provides a fixed-type sequence that behaves more like a C-style array. Libraries like NumPy also provide fixed-shape arrays that offer efficient memory usage and fast computations. In C and C++, arrays are part of the language itself, and they form the foundation of higher-level containers like std::vector.\n\n\n\nWorked Example\nimport array\n\n# Create a static array of integers (type 'i' = signed int)\narr = array.array('i', [0] * 5)\n\n# Fill the array with values\nfor i in range(len(arr)):\n    arr[i] = (i + 1) * 10\n\n# Access and update elements\nprint(\"Element at index 2:\", arr[2])\narr[2] = 99\nprint(\"Updated element at index 2:\", arr[2])\n\n# Traverse the array\nprint(\"All elements:\")\nfor i in range(len(arr)):\n    print(f\"Index {i}: {arr[i]}\")\n\n# Demonstrating the limitation: trying to insert beyond capacity\ntry:\n    arr.insert(5, 60)  # This technically works in Python's array, but resizes internally\n    print(\"Inserted new element:\", arr)\nexcept Exception as e:\n    print(\"Error inserting into static array:\", e)\nThis code illustrates the strengths and weaknesses of static arrays. Access and updates are immediate, and traversal is simple. But the notion of a “fixed size” means that insertion and deletion are costly or, in some languages, unsupported.\n\n\nWhy it matters\nStatic arrays are the building blocks of data structures. They teach the trade-off between speed and flexibility. They remind us that memory is finite and that how data is laid out in memory directly impacts performance. Whether writing Python code, using NumPy, or implementing algorithms in C, understanding static arrays makes it easier to reason about cost, predict behavior, and avoid common errors.\n\n\nExercises\n\nCreate an array of size 8 and fill it with even numbers from 2 to 16. Then access the 4th element directly.\nUpdate the middle element of a fixed-size array with a new value.\nWrite a procedure to traverse an array and find the maximum element.\nExplain why inserting a new value into the beginning of a static array requires shifting every other element.\nGive two examples of real-world systems where fixed-size arrays are a natural fit.\n\n\n\n\n2.1 L2 — Static Arrays and the System Beneath\nStatic arrays are more than just a collection of values; they are a direct window into how computers store and access data. At the advanced level, understanding static arrays means looking at memory models, cache behavior, compiler optimizations, and the role of arrays in operating systems and production libraries. This perspective is critical for building high-performance software and for avoiding subtle, system-level bugs.\n\nDeep Dive\nAt the lowest level, a static array is a contiguous block of memory. When an array is declared, the compiler calculates the required size as length × element_size and reserves that many bytes. Each element is addressed by simple arithmetic:\naddress_of(arr[i]) = base_address + (i × element_size)\nThis is why access and updates are constant time. The difference between static arrays and dynamically allocated ones often comes down to where the memory lives. Arrays declared inside a function may live on the stack, offering fast allocation and automatic cleanup. Larger arrays or arrays whose size isn’t known at compile time are allocated on the heap, requiring runtime management via calls such as malloc and free.\nThe cache hierarchy makes arrays especially efficient. Because elements are contiguous, accessing arr[i] loads not just one element but also its neighbors into a cache line (often 64 bytes). This property, known as spatial locality, means that scanning through an array is very fast. Prefetchers in modern CPUs exploit this by pulling in upcoming cache lines before they are needed. However, irregular access patterns (e.g., striding by 17) can defeat prefetching and lead to performance drops.\nAlignment and padding further influence performance. On most systems, integers must start at addresses divisible by 4, and doubles at addresses divisible by 8. If the compiler cannot guarantee alignment, it may add padding bytes to enforce it. Misaligned accesses can cause slowdowns or even hardware faults on strict architectures.\nDifferent programming languages expose these behaviors differently. In C, a declaration like int arr[10]; on the stack creates exactly 40 bytes on a 32-bit system. In contrast, malloc(10 * sizeof(int)) allocates memory on the heap. In C++, std::array&lt;int, 10&gt; is a safer wrapper around C arrays, while std::vector&lt;int&gt; adds resizing at the cost of indirection and metadata. In Fortran and NumPy, multidimensional arrays can be stored in column-major order rather than row-major, which changes how indices map to addresses and affects iteration performance.\nThe operating system kernel makes heavy use of static arrays. For example, Linux defines fixed-size arrays in structures like task_struct for file descriptors, and uses arrays in page tables for managing memory mappings. Static arrays provide predictability and remove the need for runtime memory allocation in performance-critical or security-sensitive code.\nFrom a performance profiling standpoint, arrays reveal fundamental trade-offs. Shifting elements during insertion or deletion requires copying bytes across memory, and the cost grows linearly with the number of elements. Compilers attempt to optimize loops over arrays with vectorization, turning element-wise operations into SIMD instructions. They may also apply loop unrolling or bounds-check elimination (BCE) when it can be proven that indices remain safe.\nStatic arrays also carry risks. In C and C++, accessing out-of-bounds memory leads to undefined behavior, often exploited in buffer overflow attacks. Languages like Java or Python mitigate this with runtime bounds checks, but at the expense of some performance.\nAt this level, static arrays should be seen not only as a data structure but as a fundamental contract between code, compiler, and hardware.\nWorked Example (C)\n#include &lt;stdio.h&gt;\n\nint main() {\n    // Static array of 8 integers allocated on the stack\n    int arr[8];\n\n    // Initialize array\n    for (int i = 0; i &lt; 8; i++) {\n        arr[i] = (i + 1) * 10;\n    }\n\n    // Access and update element\n    printf(\"Element at index 3: %d\\n\", arr[3]);\n    arr[3] = 99;\n    printf(\"Updated element at index 3: %d\\n\", arr[3]);\n\n    // Traverse with cache-friendly pattern\n    int sum = 0;\n    for (int i = 0; i &lt; 8; i++) {\n        sum += arr[i];\n    }\n    printf(\"Sum of array: %d\\n\", sum);\n\n    // Dangerous: Uncommenting would cause undefined behavior\n    // printf(\"%d\\n\", arr[10]);\n\n    return 0;\n}\nThis C program demonstrates how static arrays live on the stack, how indexing works, and why out-of-bounds access is dangerous. On real hardware, iterating sequentially benefits from spatial locality, making the traversal very fast compared to random access.\n\n\nWhy it matters\nStatic arrays are the substrate upon which much of computing is built. They are simple in abstraction but complex in practice, touching compilers, operating systems, and hardware. Understanding them is essential for:\n\nWriting cache-friendly and high-performance code.\nAvoiding security vulnerabilities like buffer overflows.\nAppreciating why higher-level data structures behave the way they do.\nBuilding intuition for memory layout, alignment, and the interaction between code and the CPU.\n\nArrays are not just “collections of values” — they are the foundation of efficient data processing.\n\n\nExercises\n\nIn C, declare a static array of size 16 and measure how long it takes to sum its elements sequentially versus accessing them in steps of 4. Explain the performance difference.\nExplain why iterating over a 2D array row by row is faster in C than column by column.\nConsider a struct with mixed types (e.g., char, int, double). Predict where padding bytes will be inserted if placed inside an array.\nResearch and describe how the Linux kernel uses static arrays in managing processes or memory.\nDemonstrate with code how accessing beyond the end of a static array in C can cause undefined behavior, and explain why this is a serious risk in system programming.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#dynamic-arrays",
    "href": "chapter_2.html#dynamic-arrays",
    "title": "Chapter 2. Arrays",
    "section": "2.2 Dynamic Arrays",
    "text": "2.2 Dynamic Arrays\n\n2.2 L0 — Arrays That Grow\nA dynamic array is like a container that can expand and shrink as needed. Unlike static arrays, which must know their size in advance, a dynamic array adapts as elements are added or removed. You can think of it as a bookshelf where new shelves appear automatically when space runs out. The underlying idea is simple: keep the benefits of fast index-based access, while adding flexibility to change the size.\n\nDeep Dive\nA dynamic array begins with a fixed amount of space called its capacity. When the number of elements (the length) exceeds this capacity, the array grows. This is usually done by allocating a new, larger block of memory and copying the old elements into it. After this, new elements can be added until the new capacity is filled, at which point the process repeats.\nDespite this resizing process, the key properties remain:\n\nFast access and update: Elements can still be reached instantly using an index.\nAppend flexibility: New elements can be added at the end without worrying about fixed size.\nOccasional resizing cost: Most appends are quick, but when resizing happens, it takes longer because all elements must be copied.\n\nThe performance picture is intuitive:\n\n\n\n\n\n\n\n\nOperation\nTime Complexity (Typical)\nNotes\n\n\n\n\nAccess element\nO(1)\nIndex maps directly to position\n\n\nUpdate element\nO(1)\nReplace value in place\n\n\nAppend element\nO(1) amortized\nOccasionally O(n) when resizing occurs\n\n\nPop element\nO(1)\nRemove from end\n\n\nInsert/Delete\nO(n)\nElements must be shifted\n\n\n\nDynamic arrays therefore trade predictability for flexibility. The occasional slow operation is outweighed by the ability to grow and shrink on demand, which makes them useful for most real-world tasks where the number of elements is not known in advance.\n\n\nWorked Example\n# Create a dynamic array using Python's built-in list\narr = []\n\n# Append elements (array grows automatically)\nfor i in range(5):\n    arr.append((i + 1) * 10)\n\nprint(\"Array after appending:\", arr)\n\n# Access and update elements\nprint(\"Element at index 2:\", arr[2])\narr[2] = 99\nprint(\"Updated array:\", arr)\n\n# Remove last element\nlast = arr.pop()\nprint(\"Removed element:\", last)\nprint(\"Array after pop:\", arr)\n\n# Traverse array\nfor i in range(len(arr)):\n    print(f\"Index {i}: {arr[i]}\")\nThis short program shows how a dynamic array in Python resizes automatically with append and shrinks with pop. Access and updates remain instant, while resizing happens invisibly when more space is needed.\n\n\nWhy it matters\nDynamic arrays combine efficiency and flexibility. They allow programs to handle unknown or changing amounts of data without predefining sizes. They form the backbone of lists in high-level languages, balancing performance with usability. They also illustrate the idea of amortized cost: most operations are fast, but occasional expensive operations are averaged out over time.\n\n\nExercises\n\nCreate an array and append numbers 1 through 10. Print the final array.\nReplace the 3rd element with a new value.\nRemove the last two elements and print the result.\nWrite a procedure that traverses a dynamic array and computes the average of its elements.\nExplain why appending one element might sometimes be much slower than appending another, even though both look the same in code.\n\n\n\n\n2.2 L1 — Dynamic Arrays in Practice\nDynamic arrays extend the idea of static arrays by making size flexible. They allow adding or removing elements without knowing the total number in advance. Under the hood, this flexibility is achieved through careful memory management: the array is stored in a contiguous block, but when more space is needed, a larger block is allocated, and all elements are copied over. This mechanism balances speed with adaptability and is the reason why dynamic arrays are the default sequence type in many languages.\n\nDeep Dive\nA dynamic array starts with a certain capacity, often larger than the initial number of elements. When the number of stored elements exceeds capacity, the array is resized. The common strategy is to double the capacity. For example, an array of capacity 4 that becomes full will reallocate to capacity 8. All existing elements are copied into the new block, and the old memory is freed.\nThis strategy makes appending efficient on average. While an individual resize costs O(n) because of the copying, most appends are O(1). Across a long sequence of operations, the total cost averages out — this is called amortized analysis.\nDynamic arrays retain the key advantages of static arrays:\n\nContiguous storage means fast random access with O(1) time.\nUpdates are also O(1) because they overwrite existing slots.\n\nThe challenges appear with other operations:\n\nInsertions or deletions in the middle require shifting elements, making them O(n).\nResizing events create temporary latency spikes, especially when arrays are large.\n\nA clear summary:\n\n\n\nOperation\nTime Complexity\nNotes\n\n\n\n\nAccess element\nO(1)\nDirect index calculation\n\n\nUpdate element\nO(1)\nReplace value in place\n\n\nAppend element\nO(1) amortized\nOccasional O(n) when resizing\n\n\nPop element\nO(1)\nRemove from end\n\n\nInsert/Delete\nO(n)\nShifting elements required\n\n\n\n\nTrade-offs.\nDynamic arrays sacrifice predictability for convenience. Resizing causes performance spikes, but the doubling strategy keeps the average cost low. Over-allocation wastes some memory, but it reduces the frequency of resizes. The key is that this trade-off is usually favorable in practice.\n\n\nUse cases.\nDynamic arrays are well-suited for:\n\nLists whose size is not known in advance.\nWorkloads dominated by appending and reading values.\nGeneral-purpose data structures in high-level programming languages.\n\n\n\nLanguage implementations.\n\nPython: list is a dynamic array, using an over-allocation strategy to reduce frequent resizes.\nC++: std::vector doubles its capacity when needed, invalidating pointers/references after reallocation.\nJava: ArrayList grows by about 1.5× when full, trading memory efficiency for fewer copies.\n\n\n\nPitfalls.\n\nIn languages with pointers or references, resizes can invalidate existing references.\nLarge arrays may cause noticeable latency during reallocation.\nMiddle insertions and deletions remain inefficient compared to linked structures.\n\n\n\n\nWorked Example\n# Demonstrate dynamic array behavior using Python's list\narr = []\n\n# Append elements to trigger resizing internally\nfor i in range(12):\n    arr.append(i)\n    print(f\"Appended {i}, length = {len(arr)}\")\n\n# Access and update\nprint(\"Element at index 5:\", arr[5])\narr[5] = 99\nprint(\"Updated element at index 5:\", arr[5])\n\n# Insert in the middle (expensive operation)\narr.insert(6, 123)\nprint(\"Array after middle insert:\", arr)\n\n# Pop elements\narr.pop()\nprint(\"Array after pop:\", arr)\nThis example illustrates appending, updating, inserting, and popping. While Python hides the resizing, the cost is there: occasionally the list must allocate more space and copy its contents.\n\n\nWhy it matters\nDynamic arrays balance flexibility and performance. They demonstrate the principle of amortized complexity, showing how expensive operations can be smoothed out over time. They also highlight trade-offs between memory usage and speed. Understanding them explains why high-level lists perform well in everyday coding but also where they can fail under stress.\n\n\nExercises\n\nCreate a dynamic array and append the numbers 1 to 20. Measure how many times resizing would have occurred if the growth factor were 2.\nInsert an element into the middle of a large array and explain why this operation is slower than appending at the end.\nWrite a procedure to remove all odd numbers from a dynamic array.\nCompare Python’s list, Java’s ArrayList, and C++’s std::vector in terms of growth strategy.\nExplain why references to elements of a std::vector may become invalid after resizing.\n\n\n\n\n2.2 L2 — Dynamic Arrays Under the Hood\nDynamic arrays reveal how high-level flexibility is built on top of low-level memory management. While they appear as resizable containers, underneath they are carefully engineered to balance performance, memory efficiency, and safety. Understanding their internals sheds light on allocators, cache behavior, and the risks of pointer invalidation.\n\nDeep Dive\nDynamic arrays rely on heap allocation. When first created, they reserve a contiguous memory block with some capacity. As elements are appended and the array fills, the implementation must allocate a new, larger block, copy all existing elements, and free the old block.\nMost implementations use a geometric growth strategy, often doubling the capacity when space runs out. Some use a factor smaller than two, such as 1.5×, to reduce memory waste. The trade-off is between speed and efficiency:\n\nLarger growth factors reduce the number of costly reallocations.\nSmaller growth factors waste less memory but increase resize frequency.\n\nThis leads to an amortized O(1) cost for append. Each resize is expensive, but they happen infrequently enough that the average cost remains constant across many operations.\nHowever, resizes have side effects:\n\nPointer invalidation: In C++ std::vector, any reference, pointer, or iterator into the old memory becomes invalid after reallocation.\nLatency spikes: Copying thousands or millions of elements in one step can stall a program, especially in real-time or low-latency systems.\nAllocator fragmentation: Repeated growth and shrink cycles can fragment the heap, reducing performance in long-running systems.\n\nCache efficiency is one of the strengths of dynamic arrays. Because elements are stored contiguously, traversals are cache-friendly, and prefetchers can load entire blocks into cache lines. But reallocations can disrupt locality temporarily, as the array may move to a new region of memory.\nDifferent languages implement dynamic arrays with variations:\n\nPython lists use over-allocation with a small growth factor (~12.5% to 25% extra). This minimizes wasted memory while keeping amortized costs stable.\nC++ std::vector typically doubles its capacity when needed. Developers can call reserve() to preallocate memory and avoid repeated reallocations.\nJava ArrayList grows by ~1.5×, balancing heap usage with resize frequency.\n\nDynamic arrays also face risks:\n\nIf resizing logic is incorrect, buffer overflows may occur.\nAttackers can exploit repeated growth/shrink cycles to cause denial-of-service via frequent allocations.\nVery large allocations can fail outright if memory is exhausted.\n\nFrom a profiling perspective, workloads matter. Append-heavy patterns perform extremely well due to amortization. Insert-heavy or middle-delete workloads perform poorly because of element shifting. Allocator-aware optimizations, like pre-reserving capacity, can dramatically improve performance.\n\n\nWorked Example (C++)\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    std::vector&lt;int&gt; v;\n    v.reserve(4);  // reserve space for 4 elements to reduce reallocations\n\n    for (int i = 0; i &lt; 12; i++) {\n        v.push_back(i * 10);\n        std::cout &lt;&lt; \"Appended \" &lt;&lt; i*10\n                  &lt;&lt; \", size = \" &lt;&lt; v.size()\n                  &lt;&lt; \", capacity = \" &lt;&lt; v.capacity() &lt;&lt; std::endl;\n    }\n\n    // Access and update\n    std::cout &lt;&lt; \"Element at index 5: \" &lt;&lt; v[5] &lt;&lt; std::endl;\n    v[5] = 99;\n    std::cout &lt;&lt; \"Updated element at index 5: \" &lt;&lt; v[5] &lt;&lt; std::endl;\n\n    // Demonstrate invalidation risk\n    int* ptr = &v[0];\n    v.push_back(12345); // may reallocate and move data\n    std::cout &lt;&lt; \"Old pointer may now be invalid: \" &lt;&lt; *ptr &lt;&lt; std::endl; // UB if reallocated\n}\nThis program shows how std::vector manages capacity. The output reveals how capacity grows as more elements are appended. The pointer invalidation example highlights a subtle but critical risk: after a resize, old addresses into the array are no longer safe.\n\n\nWhy it matters\nDynamic arrays expose the tension between abstraction and reality. They appear simple, but internally they touch almost every layer of the system: heap allocators, caches, compiler optimizations, and safety checks. They are essential for understanding how high-level languages achieve both usability and performance, and they illustrate real-world engineering trade-offs between speed, memory, and safety.\n\n\nExercises\n\nIn C++, measure the capacity growth of a std::vector&lt;int&gt; as you append 1,000 elements. Plot size vs capacity.\nExplain why a program that repeatedly appends and deletes elements might fragment the heap over time.\nCompare the growth strategies of Python list, C++ std::vector, and Java ArrayList. Which wastes more memory? Which minimizes resize cost?\nWrite a program that appends 1 million integers to a dynamic array and then times the traversal. Compare it with inserting 1 million integers at the beginning.\nShow how reserve() in std::vector or ensureCapacity() in Java ArrayList can eliminate costly reallocation spikes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#slices-views",
    "href": "chapter_2.html#slices-views",
    "title": "Chapter 2. Arrays",
    "section": "2.3 Slices & Views",
    "text": "2.3 Slices & Views\n\n2.3 L0 — Looking Through a Window\nA slice or view is a way to look at part of an array without creating a new one. Instead of copying data, a slice points to the same underlying elements, just with its own start and end boundaries. This makes working with subarrays fast and memory-efficient. You can think of a slice as a window into a longer row of boxes, showing only the portion you care about.\n\nDeep Dive\nWhen you take a slice, you don’t get a new array filled with copied elements. Instead, you get a new “view” that remembers where in the original array it starts and stops. This is useful because:\n\nNo copying means creating a slice is very fast.\nShared storage means changes in the slice also affect the original array (in languages like Go, Rust, or NumPy).\nReduced scope means you can focus on a part of the array without carrying the entire structure.\n\nKey properties of slices:\n\nThey refer to the same memory as the original array.\nThey have their own length (number of elements visible).\nThey may also carry a capacity, which limits how far they can expand into the original array.\n\nIn Python, list slicing (arr[2:5]) creates a new list with copies of the elements. This is not a true view. By contrast, NumPy arrays, Go slices, and Rust slices provide real views — updates to the slice affect the original array.\nA summary:\n\n\n\nFeature\nSlice/View\nNew Array (Copy)\n\n\n\n\nMemory usage\nShares existing storage\nAllocates new storage\n\n\nCreation cost\nO(1)\nO(n) for copied elements\n\n\nUpdates\nAffect original array\nIndependent\n\n\nSafety\nRisk of aliasing issues\nNo shared changes\n\n\n\nSlices are especially valuable when working with large datasets, where copying would be too expensive.\n\n\nWorked Example\n# Python slicing creates a copy, but useful to illustrate concept\narr = [10, 20, 30, 40, 50]\n\n# Slice of middle part\nsub = arr[1:4]\nprint(\"Original array:\", arr)\nprint(\"Slice (copy in Python):\", sub)\n\n# Modifying the slice does not affect the original (Python behavior)\nsub[0] = 99\nprint(\"Modified slice:\", sub)\nprint(\"Original array unchanged:\", arr)\n\n# In contrast, NumPy arrays behave like true views\nimport numpy as np\narr_np = np.array([10, 20, 30, 40, 50])\nsub_np = arr_np[1:4]\nsub_np[0] = 99\nprint(\"NumPy slice reflects back:\", arr_np)\nThis example shows the difference: Python lists create a copy, while NumPy slices act as views and affect the original.\n\n\nWhy it matters\nSlices let you work with subsets of data without wasting memory or time copying. They are critical in systems and scientific computing where performance matters. They also highlight the idea of aliasing: when two names refer to the same data. Understanding slices teaches you when changes propagate and when they don’t, which helps avoid surprising bugs.\n\n\nExercises\n\nCreate an array of 10 numbers. Take a slice of the middle 5 elements and print them.\nUpdate the first element in your slice and describe what happens to the original array in your chosen language.\nCompare slicing behavior in Python and NumPy: which one copies, which one shares?\nExplain why slicing a very large dataset is more efficient than copying it.\nThink of a real-world analogy where two people share the same resource but only see part of it. How does this relate to slices?\n\n\n\n\n2.3 L1 — Slices in Practice\nSlices provide a practical way to work with subarrays efficiently. Instead of copying data into a new structure, a slice acts as a lightweight reference to part of an existing array. This gives programmers flexibility to manipulate sections of data without paying the cost of duplication, while still preserving the familiar indexing model of arrays.\n\nDeep Dive\nAt the implementation level, a slice is typically represented by a small structure that stores:\n\nA pointer to the first element in the slice.\nThe slice’s length (how many elements it can access).\nOptionally, its capacity (how far the slice can grow into the backing array).\n\nIndexing into a slice works just like indexing into an array:\nslice[i] → base_address + i × element_size\nThe complexity model stays consistent:\n\nSlice creation: O(1) when implemented as a view, O(n) if the language copies elements.\nAccess/update: O(1), just like arrays.\nTraversal: O(k), proportional to the slice’s length.\n\nThis design makes slices efficient but introduces trade-offs. With true views, the slice and the original array share memory. Updates made through one are visible through the other. This can be extremely useful but also dangerous, as it introduces the possibility of unintended side effects. Languages that prioritize safety (like Python lists) avoid this by returning a copy instead of a view.\nThe balance is clear:\n\nViews (Go, Rust, NumPy): fast and memory-efficient, but require discipline to avoid aliasing bugs.\nCopies (Python lists): safer, but slower and more memory-intensive for large arrays.\n\nA summary of behaviors:\n\n\n\n\n\n\n\n\n\nLanguage/Library\nSlice Behavior\nShared Updates\nNotes\n\n\n\n\nGo\nView\nYes\nBacked by (ptr, len, cap) triple\n\n\nRust\nView\nYes\nSafe with borrow checker (mutable/immutable)\n\n\nPython list\nCopy\nNo\nSafer but memory-expensive\n\n\nNumPy array\nView\nYes\nBasis of efficient scientific computing\n\n\nC/C++\nManual pointer\nYes\nNo built-in slice type; must manage manually\n\n\n\n\nUse cases.\n\nProcessing large datasets in segments without copying.\nImplementing algorithms like sliding windows, partitions, or block-based iteration.\nSharing views of arrays across functions for modular design without allocating new memory.\n\n\n\nPitfalls.\n\nIn languages with views, careless updates can corrupt the original array unexpectedly.\nIn Go and C++, extending a slice/view beyond its capacity causes runtime errors or undefined behavior.\nIn Python, forgetting that slices are copies can lead to performance issues in large-scale workloads.\n\n\n\n\nWorked Example\n# Demonstrating copy slices vs view slices in Python and NumPy\n\n# Python list slicing creates a copy\narr = [1, 2, 3, 4, 5]\nsub = arr[1:4]\nsub[0] = 99\nprint(\"Python original:\", arr)  # unchanged\nprint(\"Python slice (copy):\", sub)\n\n# NumPy slicing creates a view\nimport numpy as np\narr_np = np.array([1, 2, 3, 4, 5])\nsub_np = arr_np[1:4]\nsub_np[0] = 99\nprint(\"NumPy original (affected):\", arr_np)\nprint(\"NumPy slice (view):\", sub_np)\nThis example shows the key difference: Python lists copy, while NumPy provides true views. The choice reflects different design priorities: safety in Python’s core data structures versus performance in numerical computing.\n\n\nWhy it matters\nSlices make programs more efficient and expressive. They eliminate unnecessary copying, speed up algorithms that work on subranges, and support modular programming by passing references instead of duplicating data. At the same time, they expose important design trade-offs between safety and performance. Understanding slices provides insight into how modern languages manage memory efficiently while protecting against common errors.\n\n\nExercises\n\nIn Go, create an array of 10 elements and take a slice of the middle 5. Update the slice and observe the effect on the array.\nIn Python, slice a list of 1 million numbers and explain the performance cost compared to slicing a NumPy array of the same size.\nWrite a procedure that accepts a slice and doubles each element. Test with both a copy-based language (Python lists) and a view-based language (NumPy or Go).\nExplain why passing slices to functions is more memory-efficient than passing entire arrays.\nDiscuss a scenario where slice aliasing could lead to unintended bugs in a large program.\n\n\n\n\n2.3 L2 — Slices and Views in Systems\nSlices are not just convenient programming shortcuts; they represent a powerful abstraction that ties language semantics to hardware realities. At this level, slices expose details about memory layout, lifetime, and compiler optimizations. They are central to performance-critical systems because they allow efficient access to subsets of data without copying, while also demanding careful handling to avoid aliasing bugs and unsafe memory access.\n\nDeep Dive\nA slice is typically represented internally as a triple:\n\nA pointer to the first element,\nA length describing how many elements are visible,\nA capacity showing how far the slice may extend into the backing array.\n\nIndexing into a slice is still O(1), but the compiler inserts bounds checks to prevent invalid access. In performance-sensitive code, compilers often apply bounds-check elimination (BCE) when they can prove that loop indices remain within safe limits. This allows slices to combine safety with near-native performance.\nSlices are non-owning references. They do not manage memory themselves but instead depend on the underlying array. In languages like Rust, the borrow checker enforces lifetimes to prevent dangling slices. In C and C++, however, programmers must manually ensure that the backing array outlives the slice, or risk undefined behavior.\nBecause slices share memory, they introduce aliasing. Multiple slices can point to overlapping regions of the same array. This can lead to subtle bugs if two parts of a program update the same region concurrently. In multithreaded contexts, mutable aliasing without synchronization can cause data races. Some systems adopt copy-on-write strategies to reduce risks, but this adds overhead.\nFrom a performance perspective, slices preserve contiguity, which is ideal for cache locality and prefetching. Sequential traversal is cache-friendly, but strided access (e.g., every 3rd element) can defeat hardware prefetchers, reducing efficiency. Languages like NumPy exploit strides explicitly, enabling both dense and sparse-like views without copying.\nLanguage designs differ in how they handle slices:\n\nGo uses (ptr, len, cap). Appending to a slice may allocate a new array if capacity is exceeded, silently detaching it from the original backing storage.\nRust distinguishes &[T] for immutable and &mut [T] for mutable slices, with the compiler enforcing safe borrowing rules.\nC/C++ provide no built-in slice type, so developers rely on raw pointers and manual length tracking. This is flexible but error-prone.\nNumPy supports advanced slicing: views with strides, broadcasting rules, and multidimensional slices for scientific computing.\n\nCompilers also optimize slice-heavy code:\n\nVectorization transforms element-wise loops into SIMD instructions when slices are contiguous.\nEscape analysis determines whether slices can stay stack-allocated or must be promoted to the heap.\n\nSystem-level use cases highlight the importance of slices:\n\nZero-copy I/O: network and file system buffers are exposed as slices into larger memory regions.\nMemory-mapped files: slices map directly to disk pages, enabling efficient processing of large datasets.\nGPU programming: CUDA and OpenCL kernels operate on slices of device memory, avoiding transfers.\n\nThese applications show why slices are not just a programming convenience but a core tool for bridging high-level logic with low-level performance.\n\n\nWorked Example (Go)\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    arr := [6]int{10, 20, 30, 40, 50, 60}\n    s := arr[1:4] // slice referencing elements 20, 30, 40\n\n    fmt.Println(\"Original array:\", arr)\n    fmt.Println(\"Slice view:\", s)\n\n    // Update through slice\n    s[0] = 99\n    fmt.Println(\"After update via slice, array:\", arr)\n\n    // Demonstrate capacity\n    fmt.Println(\"Slice length:\", len(s), \"capacity:\", cap(s))\n\n    // Appending beyond slice capacity reallocates\n    s = append(s, 70, 80)\n    fmt.Println(\"Slice after append:\", s)\n    fmt.Println(\"Array after append (unchanged):\", arr)\n}\nThis example illustrates Go’s slice model. The slice s initially shares storage with arr. Updates propagate to the array. However, when appending exceeds the slice’s capacity, Go allocates a new backing array, breaking the link with the original. This behavior is efficient but can surprise developers if not understood.\n\n\nWhy it matters\nSlices embody key system concepts: pointer arithmetic, memory ownership, cache locality, and aliasing. They explain how languages achieve zero-copy abstractions while balancing safety and performance. They also highlight risks such as dangling references and silent reallocations. Mastery of slices is essential for building efficient algorithms, avoiding memory errors, and reasoning about system-level performance.\n\n\nExercises\n\nIn Go, create an array of 8 integers and take overlapping slices. Modify one slice and observe effects on the other. Explain why this happens.\nIn Rust, attempt to create two mutable slices of the same array region. Explain why the borrow checker rejects it.\nIn C, simulate a slice using a pointer and a length. Show what happens if the backing array is freed while the slice is still in use.\nIn NumPy, create a 2D array and take a strided slice (every second row). Explain why performance is worse than contiguous slicing.\nCompare how Python, Go, and Rust enforce (or fail to enforce) safety when working with slices.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#multidimensional-arrays",
    "href": "chapter_2.html#multidimensional-arrays",
    "title": "Chapter 2. Arrays",
    "section": "2.4 Multidimensional Arrays",
    "text": "2.4 Multidimensional Arrays\n\n2.4 L0 — Tables and Grids\nA multidimensional array is an extension of the simple array idea. Instead of storing data in a single row, a multidimensional array organizes elements in a grid, table, or cube. The most common example is a two-dimensional array, which looks like a table with rows and columns. Each position in the grid is identified by two coordinates: one for the row and one for the column. This structure is useful for representing spreadsheets, images, game boards, and mathematical matrices.\n\nDeep Dive\nYou can think of a multidimensional array as an array of arrays. A two-dimensional array is a list where each element is itself another list. For example, a 3×3 table contains 3 rows, each of which has 3 columns. Accessing an element requires specifying both coordinates: arr[row][col].\nEven though we visualize multidimensional arrays as grids, in memory they are still stored as a single continuous sequence. To find an element, the program computes its position using a formula. In a 2D array with n columns, the element at (row, col) is located at:\nindex = row × n + col\nThis mapping allows direct access in constant time, just like with 1D arrays.\nCommon operations are:\n\nCreation: decide dimensions and initialize with values.\nAccess: specify row and column to retrieve an element.\nUpdate: change the value at a given coordinate.\nTraversal: visit elements row by row or column by column.\n\nA quick summary:\n\n\n\nOperation\nDescription\nCost\n\n\n\n\nAccess element\nGet value at (row, col)\nO(1)\n\n\nUpdate element\nReplace value at (row, col)\nO(1)\n\n\nTraverse array\nVisit all elements\nO(n×m)\n\n\n\nMultidimensional arrays introduce an important detail: traversal order. In many languages (like C and Python’s NumPy), arrays are stored in row-major order, which means all elements of the first row are laid out contiguously, then the second row, and so on. Others, like Fortran, use column-major order. This difference affects performance in more advanced topics, but at this level, the key idea is that access is still fast and predictable.\n\n\nWorked Example\n# Create a 2D array (3x3 table) using list of lists\ntable = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Access element in second row, third column\nprint(\"Element at (1, 2):\", table[1][2])  # prints 6\n\n# Update element\ntable[0][0] = 99\nprint(\"Updated table:\", table)\n\n# Traverse row by row\nprint(\"Row traversal:\")\nfor row in table:\n    for val in row:\n        print(val, end=\" \")\n    print()\nThis example shows how to build and use a 2D array in Python. It looks like a table, with easy access via coordinates.\n\n\nWhy it matters\nMultidimensional arrays provide a natural way to represent structured data like matrices, grids, and images. They allow algorithms to work directly with two-dimensional or higher-dimensional information without flattening everything into one long row. This makes programs easier to write, read, and reason about.\n\n\nExercises\n\nCreate a 3×3 array with numbers 1 through 9 and print it in a table format.\nAccess the element at row 2, column 3 and describe how you found it.\nChange the center element of a 3×3 array to 0.\nWrite a loop to compute the sum of all values in a 4×4 array.\nExplain why accessing (row, col) in a 2D array is still O(1) even though the data is stored in a single sequence in memory.\n\n\n\n\n2.4 L1 — Multidimensional Arrays in Practice\nMultidimensional arrays are powerful because they extend the linear model of arrays into grids, tables, and higher dimensions. At a practical level, they are still stored in memory as a flattened linear block. What changes is the indexing formula: instead of a single index, we use multiple coordinates that the system translates into one offset.\n\nDeep Dive\nThe most common form is a 2D array. In memory, the elements are laid out row by row (row-major) or column by column (column-major).\n\nRow-major (C, NumPy default): elements of each row are contiguous.\nColumn-major (Fortran, MATLAB): elements of each column are contiguous.\n\nFor a 2D array with num_cols columns, the element at (row, col) in row-major order is located at:\nindex = row × num_cols + col\nFor column-major order with num_rows rows, the formula is:\nindex = col × num_rows + row\nThis distinction matters when traversing. Accessing elements in the memory’s natural order (row by row for row-major, column by column for column-major) is cache-friendly. Traversing in the opposite order forces the program to jump around in memory, leading to slower performance.\nExtending to 3D and higher is straightforward. For a 3D array with (layers, rows, cols) in row-major order:\nindex = layer × (rows × cols) + row × cols + col\nComplexity remains consistent:\n\nAccess/update: O(1) using index calculation.\nTraversal: O(n × m) for 2D, O(n × m × k) for 3D.\n\nTrade-offs:\n\nContiguous multidimensional arrays provide excellent performance for predictable workloads (e.g., matrix operations).\nResizing is costly because the entire block must be reallocated.\nJagged arrays (arrays of arrays) provide flexibility but lose memory contiguity, reducing cache performance.\n\nUse cases:\n\nStoring images (pixels as grids).\nMathematical matrices in scientific computing.\nGame boards and maps.\nTables in database-like structures.\n\nDifferent languages implement multidimensional arrays differently:\n\nPython lists: nested lists simulate 2D arrays but are jagged and fragmented in memory.\nNumPy: provides true multidimensional arrays stored contiguously in row-major (default) or column-major order.\nC/C++: support both contiguous multidimensional arrays (int arr[rows][cols];) and pointer-based arrays of arrays.\nJava: uses arrays of arrays (jagged by default).\n\n\n\nWorked Example\n# Comparing list of lists vs NumPy arrays\n# List of lists (jagged)\ntable = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nprint(\"Element at (2, 1):\", table[2][1])  # 8\n\n# NumPy array (true contiguous 2D array)\nimport numpy as np\nmatrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(\"Element at (2, 1):\", matrix[2,1])  # 8\n\n# Traversal in row-major order\nfor row in range(matrix.shape[0]):\n    for col in range(matrix.shape[1]):\n        val = matrix[row, col]  # efficient in NumPy\nThe Python list-of-lists behaves like a table, but each row may live separately in memory. NumPy, on the other hand, stores data contiguously, enabling much faster iteration and vectorized operations.\n\n\nWhy it matters\nMultidimensional arrays are central to real-world applications, from graphics and simulations to data science and machine learning. They highlight how physical memory layout (row-major vs column-major) interacts with algorithm design. Understanding them allows developers to choose between safety, flexibility, and performance, depending on the problem.\n\n\nExercises\n\nWrite a procedure to sum all values in a 5×5 array by traversing row by row.\nFor a 3×3 NumPy array, access element (2,1) and explain how its memory index is calculated in row-major order.\nCreate a jagged array (rows of different lengths) in Python. Show how traversal differs from a true 2D array.\nExplain why traversing a NumPy array by rows is faster than by columns.\nWrite a formula for computing the linear index of (i,j,k) in a 3D array stored in row-major order.\n\n\n\n\n2.4 L2 — Multidimensional Arrays and System Realities\nMultidimensional arrays are not only a logical abstraction but also a system-level structure that interacts with memory layout, caches, and compilers. At this level, understanding how they are stored, accessed, and optimized is essential for building high-performance code in scientific computing, graphics, and data-intensive systems.\n\nDeep Dive\nA multidimensional array is stored either as a contiguous linear block or as an array of pointers (jagged array). In the contiguous layout, elements follow one another in memory according to a linearization formula. In row-major order (C, NumPy), a 2D element at (row, col) is:\nindex = row × num_cols + col\nIn column-major order (Fortran, MATLAB), the formula is:\nindex = col × num_rows + row\nThis difference has deep performance consequences. In row-major layout, traversing row by row is cache-friendly because consecutive elements are contiguous. Traversing column by column introduces large strides, which can cause cache and TLB misses. In column-major arrays, the reverse holds true.\n\nCache and performance.\nWhen an array is traversed sequentially in its natural memory order, cache lines are used efficiently and hardware prefetchers work well. Strided access, such as reading every k-th column in a row-major layout, prevents prefetchers from predicting the access pattern and leads to performance drops. For large arrays, this can mean the difference between processing gigabytes per second and megabytes per second.\n\n\nAlignment and padding.\nCompilers and libraries often align rows to cache line or SIMD vector boundaries. For example, a 64-byte cache line may cause padding to be inserted so that each row begins on a boundary. In parallel systems, this prevents false sharing when multiple threads process different rows. However, padding increases memory footprint.\n\n\nLanguage-level differences.\n\nC/C++: contiguous 2D arrays (int arr[rows][cols]) guarantee row-major layout. Jagged arrays (array of pointers) sacrifice locality but allow uneven row sizes.\nFortran/MATLAB: column-major ordering dominates scientific computing, influencing algorithms in BLAS and LAPACK.\nNumPy: stores strides explicitly, enabling flexible slicing and arbitrary views. Strided slices can represent transposed matrices without copying.\n\n\n\nOptimizations.\n\nLoop tiling/blocking: partition loops into smaller blocks that fit into cache, maximizing reuse.\nSIMD-friendly layouts: structure-of-arrays (SoA) improves vectorization compared to array-of-structures (AoS).\nMatrix multiplication kernels: carefully designed to exploit cache hierarchy, prefetching, and SIMD registers.\n\n\n\nSystem-level use cases.\n\nImage processing: images stored as row-major arrays, with pixels in contiguous scanlines. Efficient filters process them row by row.\nGPU computing: memory coalescing requires threads in a warp to access contiguous memory regions; array layout directly affects throughput.\nDatabases: columnar storage uses column-major arrays, enabling fast scans and aggregation queries.\n\n\n\nPitfalls.\n\nTraversing in the “wrong” order can cause performance cliffs.\nLarge index calculations may overflow if not handled carefully.\nPorting algorithms between row-major and column-major languages can introduce subtle bugs.\n\nProfiling. Practical analysis involves comparing traversal patterns, cache miss rates, and vectorization efficiency. Modern compilers can eliminate redundant bounds checks and auto-vectorize well-structured loops, but poor layout or order can block these optimizations.\n\n\n\nWorked Example (C)\n#include &lt;stdio.h&gt;\n#define ROWS 4\n#define COLS 4\n\nint main() {\n    int arr[ROWS][COLS];\n\n    // Fill the array\n    for (int r = 0; r &lt; ROWS; r++) {\n        for (int c = 0; c &lt; COLS; c++) {\n            arr[r][c] = r * COLS + c;\n        }\n    }\n\n    // Row-major traversal (cache-friendly in C)\n    int sum_row = 0;\n    for (int r = 0; r &lt; ROWS; r++) {\n        for (int c = 0; c &lt; COLS; c++) {\n            sum_row += arr[r][c];\n        }\n    }\n\n    // Column traversal (less efficient in row-major)\n    int sum_col = 0;\n    for (int c = 0; c &lt; COLS; c++) {\n        for (int r = 0; r &lt; ROWS; r++) {\n            sum_col += arr[r][c];\n        }\n    }\n\n    printf(\"Row traversal sum: %d\\n\", sum_row);\n    printf(\"Column traversal sum: %d\\n\", sum_col);\n    return 0;\n}\nThis program highlights traversal order. On large arrays, row-major traversal is much faster in C because of cache-friendly memory access, while column traversal may cause frequent cache misses.\n\n\nWhy it matters\nMultidimensional arrays sit at the heart of performance-critical applications. Their memory layout determines how well algorithms interact with CPU caches, vector units, and GPUs. Understanding row-major vs column-major, stride penalties, and cache-aware traversal allows developers to write software that scales from toy programs to high-performance computing systems.\n\n\nExercises\n\nIn C, create a 1000×1000 matrix and measure the time difference between row-major and column traversal. Explain the results.\nIn NumPy, take a 2D array and transpose it. Use .strides to confirm that the transposed array is a view, not a copy.\nWrite the linear index formula for a 4D array (a,b,c,d) in row-major order.\nExplain how false sharing could occur when two threads update adjacent rows of a large array.\nCompare the impact of row-major vs column-major layout in matrix multiplication performance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#sparse-arrays",
    "href": "chapter_2.html#sparse-arrays",
    "title": "Chapter 2. Arrays",
    "section": "2.5 Sparse Arrays",
    "text": "2.5 Sparse Arrays\n\n2.5 L0 — Sparse Arrays as Empty Parking Lots\nA sparse array is a way of storing data when most of the positions are empty. Instead of recording every slot like in a dense array, a sparse array only remembers the places that hold actual values. You can think of a huge parking lot with only a few cars parked: a dense array writes down every spot, empty or not, while a sparse array just writes down the locations of the cars.\n\nDeep Dive\nDense arrays are straightforward: every position has a value, even if it is zero or unused. This makes access simple and fast, but wastes memory if most positions are empty. Sparse arrays solve this by storing only the useful entries.\nThere are many ways to represent a sparse array:\n\nDictionary/Map: store index → value pairs, ignoring empty slots.\nCoordinate list (COO): keep two lists, one for indices and one for values.\nRun-length encoding: store stretches of empty values as counts, followed by the next filled value.\n\nThe key idea is to save memory at the cost of more complex indexing. Access is no longer just arithmetic (arr[i]) but requires looking up in the chosen structure.\nComparison:\n\n\n\n\n\n\n\n\n\nRepresentation\nMemory Use\nAccess Speed\nGood For\n\n\n\n\nDense array\nHigh\nO(1)\nData with many filled elements\n\n\nSparse (map)\nLow\nO(1) average\nFew filled elements, random access\n\n\nSparse (list)\nVery low\nO(n)\nVery small number of entries\n\n\n\n\n\nWorked Example\n# Dense representation: wastes memory for mostly empty data\ndense = [0] * 20\ndense[3] = 10\ndense[15] = 25\nprint(\"Dense array:\", dense)\n\n# Sparse representation using dictionary\nsparse = {3: 10, 15: 25}\nprint(\"Sparse array:\", sparse)\n\n# Access value\nprint(\"Value at index 3:\", sparse.get(3, 0))\nprint(\"Value at index 7:\", sparse.get(7, 0))  # default to 0 for missing\nThis shows how a sparse dictionary only records the positions that matter, while the dense version allocates space for all 20 slots.\n\n\nWhy it matters\nSparse arrays are crucial when working with large data where most entries are empty. They save memory and make it possible to process huge datasets that would not fit into memory as dense arrays. They also appear in real-world systems like machine learning (feature vectors), scientific computing (matrices with few non-zero entries), and search engines (posting lists).\n\n\nExercises\n\nRepresent a sparse array of size 1000 with only 3 non-zero values at indices 2, 500, and 999.\nWrite a procedure to count the number of non-empty values in a sparse array.\nAccess an index that does not exist in the sparse array and explain what should be returned.\nCompare the memory used by a dense array of 1000 zeros and a sparse representation with 3 values.\nThink of a real-world example (outside programming) where recording only the “non-empty” spots is more efficient than listing everything.\n\n\n\n\n2.5 L1 — Sparse Arrays in Practice\nSparse arrays become important when dealing with very large datasets where only a few positions hold non-zero values. Instead of allocating memory for every element, practical implementations use compact structures to track only the occupied indices. This saves memory, but requires trade-offs in access speed and update complexity.\n\nDeep Dive\nThere are several practical ways to represent sparse arrays:\n\nDictionary/Hash Map\n\nStore index → value pairs.\nVery fast random access and updates (average O(1)).\nMemory overhead is higher because of hash structures.\n\nCoordinate List (COO)\n\nKeep two parallel arrays: one for indices, one for values.\nCompact, easy to construct, but access is O(n).\nGood for static data with few updates.\n\nCompressed Sparse Row (CSR) / Compressed Sparse Column (CSC)\n\nWidely used for sparse matrices.\nUse three arrays: values, column indices, and row pointers (or vice versa).\nExtremely efficient for matrix-vector operations.\nPoor at dynamic updates, since compression must be rebuilt.\n\nRun-Length Encoding (RLE)\n\nStore runs of zeros as counts, followed by non-zero entries.\nBest for sequences with long stretches of emptiness.\n\n\nA comparison:\n\n\n\n\n\n\n\n\n\nFormat\nMemory Use\nAccess Speed\nBest For\n\n\n\n\nDictionary\nHigher per-entry\nO(1) avg\nDynamic updates, unpredictable indices\n\n\nCOO\nVery low\nO(n)\nStatic, small sparse sets\n\n\nCSR/CSC\nCompact\nO(1) row scan, O(log n) col lookup\nLinear algebra, scientific computing\n\n\nRLE\nVery compact\nSequential O(n), random slower\nTime-series with long zero runs\n\n\n\n\nTrade-offs:\n\nDense arrays are fast but waste memory.\nSparse arrays save memory but access/update complexity varies.\nChoice of structure depends on workload (frequent random access vs batch computation).\n\n\n\nUse cases:\n\nMachine learning: sparse feature vectors in text classification or recommender systems.\nGraph algorithms: adjacency matrices for sparse graphs.\nSearch engines: inverted index posting lists.\nScientific computing: storing large sparse matrices for simulations.\n\n\n\n\nWorked Example\n# Sparse array using Python dictionary\nsparse = {2: 10, 100: 50, 999: 7}\n\n# Accessing\nprint(\"Value at 100:\", sparse.get(100, 0))\nprint(\"Value at 3 (missing):\", sparse.get(3, 0))\n\n# Inserting new value\nsparse[500] = 42\n\n# Traversing non-empty values\nfor idx, val in sparse.items():\n    print(f\"Index {idx} → {val}\")\nFor dense vs sparse comparison:\ndense = [0] * 1000\ndense[2], dense[100], dense[999] = 10, 50, 7\nprint(\"Dense uses 1000 slots, sparse uses\", len(sparse), \"entries\")\n\n\nWhy it matters\nSparse arrays strike a balance between memory efficiency and performance. They let you work with massive datasets that would otherwise be impossible to store in memory. They also demonstrate the importance of choosing the right representation for the problem: a dictionary for dynamic updates, CSR for scientific kernels, or RLE for compressed logs.\n\n\nExercises\n\nRepresent a sparse array of length 1000 with values at indices 2, 100, and 999 using:\n\na dictionary, and\ntwo parallel lists (indices, values).\n\nWrite a procedure that traverses only non-empty entries and prints them.\nExplain why inserting a value in CSR format is more expensive than in a dictionary-based representation.\nCompare memory usage of a dense array of length 1000 with only 5 non-zero entries against its sparse dictionary form.\nGive two real-world scenarios where CSR is preferable to dictionary-based sparse arrays.\n\n\n\n\n2.5 L2 — Sparse Arrays and Compressed Layouts in Systems\nSparse arrays are not only about saving memory; they embody deep design choices about compression, cache use, and hardware acceleration. At this level, the question is not “should I store zeros or not,” but “which representation balances memory, access speed, and computational efficiency for the workload?”\n\nDeep Dive\nSeveral compressed storage formats exist, each tuned to different needs:\n\nCOO (Coordinate List): Store parallel arrays for row indices, column indices, and values. Flexible and simple, but inefficient for repeated access because lookups require scanning.\nCSR (Compressed Sparse Row): Use three arrays: values, col_indices, and row_ptr to mark boundaries. Accessing all elements of a row is O(1), while finding a specific column in a row is O(log n) or linear. Excellent for sparse matrix-vector multiplication (SpMV).\nCSC (Compressed Sparse Column): Similar to CSR, but optimized for column operations.\nDIA (Diagonal): Only store diagonals in banded matrices. Extremely memory-efficient for PDE solvers.\nELL (Ellpack/Itpack): Store each row padded to the same length, enabling SIMD and GPU vectorization. Works well when rows have similar numbers of nonzeros.\nHYB (Hybrid, CUDA): Combines ELL for regular rows and COO for irregular cases. Used in GPU-accelerated sparse libraries.\n\n\nPerformance and Complexity.\n\nDictionaries/maps: O(1) average access, but higher overhead per entry.\nCOO: O(n) lookups, better for incremental construction.\nCSR/CSC: excellent for batch operations, poor for insertions.\nELL/DIA: high throughput on SIMD/GPU hardware but inflexible.\n\nSparse matrix-vector multiplication (SpMV) illustrates trade-offs. With CSR:\ny[row] = Σ values[k] * x[col_indices[k]]  \nwhere row_ptr guides which elements belong to each row. The cost is proportional to the number of nonzeros, but performance is limited by memory bandwidth and irregular access to x.\n\n\nCache and alignment.\nCompressed formats improve locality for sequential access but introduce irregular memory access patterns when multiplying or searching. Strided iteration can align with cache lines, but pointer-heavy layouts fragment memory. Padding (in ELL) improves SIMD alignment but wastes space.\n\n\nLanguage and library implementations.\n\nPython SciPy: csr_matrix, csc_matrix, coo_matrix, dia_matrix.\nC++: Eigen and Armadillo expose CSR and CSC; Intel MKL provides highly optimized kernels.\nCUDA/cuSPARSE: Hybrid ELL + COO kernels tuned for GPUs.\n\n\n\nSystem-level use cases.\n\nLarge-scale PDE solvers and finite element methods.\nGraph algorithms (PageRank, shortest paths) using sparse adjacency matrices.\nInverted indices in search engines (postings lists).\nFeature vectors in machine learning (bag-of-words, recommender systems).\n\n\n\nPitfalls.\n\nInsertion is expensive in compressed formats (requires shifting or rebuilding).\nConverting between formats (e.g., COO ↔︎ CSR) can dominate runtime if done repeatedly.\nA poor choice of format (e.g., using ELL for irregular sparsity) can waste memory or block vectorization.\n\n\n\nOptimization and profiling.\n\nBenchmark SpMV across formats and measure achieved bandwidth.\nProfile cache misses and TLB behavior in irregular workloads.\nOn GPUs, measure coalesced vs scattered memory access to judge format suitability.\n\n\n\nWorked Example (Python with SciPy)\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\n# Dense 5x5 with many zeros\ndense = np.array([\n    [1, 0, 0, 0, 2],\n    [0, 0, 3, 0, 0],\n    [4, 0, 0, 5, 0],\n    [0, 6, 0, 0, 0],\n    [0, 0, 0, 7, 8]\n])\n\n# Convert to CSR\nsparse = csr_matrix(dense)\n\nprint(\"CSR data array:\", sparse.data)\nprint(\"CSR indices:\", sparse.indices)\nprint(\"CSR indptr:\", sparse.indptr)\n\n# Sparse matrix-vector multiplication\nx = np.array([1, 2, 3, 4, 5])\ny = sparse @ x\nprint(\"Result of SpMV:\", y)\nThis example shows how a dense matrix with many zeros can be stored efficiently in CSR. Only nonzeros are stored, and SpMV avoids unnecessary multiplications.\n\n\n\nWhy it matters\nSparse array formats are the backbone of scientific computing, machine learning, and search engines. Choosing the right format determines whether a computation runs in seconds or hours. At scale, cache efficiency, memory bandwidth, and vectorization potential matter as much as algorithmic complexity. Sparse arrays teach the critical lesson that representation is performance.\n\n\nExercises\n\nImplement COO and CSR representations of the same sparse matrix and compare memory usage.\nWrite a small CSR-based SpMV routine and measure its speed against a dense implementation.\nExplain why ELL format is efficient on GPUs but wasteful on highly irregular graphs.\nIn SciPy, convert a csr_matrix to csc_matrix and back. Measure the cost for large matrices.\nGiven a graph with 1M nodes and 10M edges, explain why adjacency lists and CSR are more practical than dense matrices.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#prefix-sums-scans",
    "href": "chapter_2.html#prefix-sums-scans",
    "title": "Chapter 2. Arrays",
    "section": "2.6 Prefix Sums & Scans",
    "text": "2.6 Prefix Sums & Scans\n\n2.6 L0 — Running Totals\nA prefix sum, also called a scan, is a way of turning a sequence into running totals. Instead of just producing one final sum, we produce an array where each position shows the sum of all earlier elements. It is like keeping a receipt tape at the checkout: each item is added in order, and you see the growing total after each step.\n\nDeep Dive\nPrefix sums are simple but powerful. Given an array [a0, a1, a2, …, an-1], the prefix sum array [p0, p1, p2, …, pn-1] is defined as:\n\nInclusive scan:\npi = a0 + a1 + … + ai\nExclusive scan:\npi = a0 + a1 + … + ai-1\n(with p0 = 0 by convention).\n\nExample with array [1, 2, 3, 4]:\n\n\n\nIndex\nOriginal\nInclusive\nExclusive\n\n\n\n\n0\n1\n1\n0\n\n\n1\n2\n3\n1\n\n\n2\n3\n6\n3\n\n\n3\n4\n10\n6\n\n\n\nPrefix sums are built in a single pass, left to right. This is O(n) in time, requiring an extra array of length n to store results.\nOnce constructed, prefix sums allow fast range queries. For any subarray between indices i and j, the sum is:\nsum(i..j) = prefix[j] - prefix[i-1]\nThis reduces what would be O(n) work into O(1) time per query.\nPrefix sums also generalize beyond addition: they can be built with multiplication, min, max, or any associative operation.\n\n\nWorked Example\narr = [1, 2, 3, 4, 5]\n\n# Inclusive prefix sum\ninclusive = []\nrunning = 0\nfor x in arr:\n    running += x\n    inclusive.append(running)\nprint(\"Inclusive scan:\", inclusive)\n\n# Exclusive prefix sum\nexclusive = [0]\nrunning = 0\nfor x in arr[:-1]:\n    running += x\n    exclusive.append(running)\nprint(\"Exclusive scan:\", exclusive)\n\n# Range query using prefix sums\ni, j = 1, 3  # sum from index 1 to 3 (2+3+4)\nrange_sum = inclusive[j] - (inclusive[i-1] if i &gt; 0 else 0)\nprint(\"Range sum (1..3):\", range_sum)\nThis program shows inclusive and exclusive scans, and how to use them to answer range queries quickly.\n\n\nWhy it matters\nPrefix sums transform repeated work into reusable results. They make range queries efficient, reduce algorithmic complexity, and appear in countless applications: histograms, text processing, probability distributions, and parallel computing. They also introduce the idea of trading extra storage for faster queries, a common algorithmic technique.\n\n\nExercises\n\nCompute the prefix sum of [1, 2, 3, 4, 5] by hand.\nShow the difference between inclusive and exclusive prefix sums for [5, 10, 15].\nUse a prefix sum to find the sum of elements from index 2 to 4 in [3, 6, 9, 12, 15].\nGiven a prefix sum array [2, 5, 9, 14], reconstruct the original array.\nExplain why prefix sums are more efficient than computing each subarray sum from scratch when handling many queries.\n\n\n\n\n2.6 L1 — Prefix Sums in Practice\nPrefix sums are a versatile tool for speeding up algorithms that involve repeated range queries. Instead of recalculating sums over and over, we preprocess the array once to create cumulative totals. This preprocessing costs O(n), but it allows each query to be answered in O(1).\n\nDeep Dive\nA prefix sum array is built by scanning the original array from left to right:\nprefix[i] = prefix[i-1] + arr[i]\nThis produces the inclusive scan. The exclusive scan shifts everything rightward, leaving prefix[0] = 0 and excluding the current element.\nThe choice between inclusive and exclusive depends on application:\n\nInclusive is easier for direct cumulative totals.\nExclusive is more natural when answering range queries.\n\nOnce built, prefix sums enable efficient operations:\n\nRange queries: sum(i..j) = prefix[j] - prefix[i-1].\nReconstruction: the original array can be recovered with arr[i] = prefix[i] - prefix[i-1].\nGeneralization: the same idea works for multiplication (cumulative product), logical OR/AND, or even min/max. The key requirement is that the operation is associative.\n\n\nTrade-offs:\n\nBuilding prefix sums requires O(n) extra memory.\nIf only a few queries are needed, recomputing directly may be simpler.\nFor many queries, the preprocessing overhead is worthwhile.\n\n\n\nUse cases:\n\nFast range-sum queries in databases or competitive programming.\nCumulative frequencies in histograms.\nSubstring analysis in text algorithms (e.g., number of vowels in a range).\nProbability and statistics: cumulative distribution functions.\n\n\n\nLanguage implementations:\n\nPython: itertools.accumulate, numpy.cumsum.\nC++: std::partial_sum from &lt;numeric&gt;.\nJava: custom loop, or stream reductions.\n\n\n\nPitfalls:\n\nConfusing inclusive vs exclusive scans often leads to off-by-one errors.\nFor large datasets, cumulative sums may overflow fixed-width integers.\n\n\n\n\nWorked Example\nimport itertools\nimport numpy as np\n\narr = [2, 4, 6, 8, 10]\n\n# Inclusive prefix sum using Python loop\ninclusive = []\nrunning = 0\nfor x in arr:\n    running += x\n    inclusive.append(running)\nprint(\"Inclusive prefix sum:\", inclusive)\n\n# Exclusive prefix sum\nexclusive = [0]\nrunning = 0\nfor x in arr[:-1]:\n    running += x\n    exclusive.append(running)\nprint(\"Exclusive prefix sum:\", exclusive)\n\n# NumPy cumsum (inclusive)\nnp_inclusive = np.cumsum(arr)\nprint(\"NumPy inclusive scan:\", np_inclusive)\n\n# Range query using prefix sums\ni, j = 1, 3  # indices 1..3 → 4+6+8\nrange_sum = inclusive[j] - (inclusive[i-1] if i &gt; 0 else 0)\nprint(\"Range sum (1..3):\", range_sum)\n\n# Recover original array from prefix sums\nreconstructed = [inclusive[0]] + [inclusive[i] - inclusive[i-1] for i in range(1, len(inclusive))]\nprint(\"Reconstructed array:\", reconstructed)\nThis example demonstrates building prefix sums by hand, using built-in libraries, answering queries, and reconstructing the original array.\n\n\nWhy it matters\nPrefix sums reduce repeated work into reusable results. They transform O(n) queries into O(1), making algorithms faster and more scalable. They are a foundational idea in algorithm design, connecting to histograms, distributions, and dynamic programming.\n\n\nExercises\n\nBuild both inclusive and exclusive prefix sums for [5, 10, 15, 20].\nUse prefix sums to compute the sum of elements from index 2 to 4 in [1, 3, 5, 7, 9].\nGiven a prefix sum array [3, 8, 15, 24], reconstruct the original array.\nWrite a procedure that computes cumulative products (scan with multiplication).\nExplain why prefix sums are more useful when answering hundreds of queries instead of just one.\n\n\n\n\n2.6 L2 — Prefix Sums and Parallel Scans\nPrefix sums seem simple, but at scale they become a central systems primitive. They serve as the backbone of parallel algorithms, GPU kernels, and high-performance libraries. At this level, the focus shifts from “what is a prefix sum” to “how can we compute it efficiently across thousands of cores, with minimal synchronization and maximal throughput?”\n\nDeep Dive\nSequential algorithm. The simple prefix sum is O(n):\nprefix[0] = arr[0]\nfor i in 1..n-1:\n    prefix[i] = prefix[i-1] + arr[i]\nEfficient for single-threaded contexts, but inherently sequential because each value depends on the one before it.\nParallel algorithms. Two key approaches dominate:\n\nHillis–Steele scan (1986):\n\nIterative doubling method.\nAt step k, each thread adds the value from 2^k positions behind.\nO(n log n) work, O(log n) depth. Simple but not work-efficient.\n\nBlelloch scan (1990):\n\nWork-efficient, O(n) total operations, O(log n) depth.\nTwo phases:\n\nUp-sweep (reduce): build a tree of partial sums.\nDown-sweep: propagate sums back down to compute prefix results.\n\nWidely used in GPU libraries.\n\n\n\nHardware performance.\n\nCache-aware scans: memory locality matters for large arrays. Blocking and tiling reduce cache misses.\nSIMD vectorization: multiple prefix elements are computed in parallel inside CPU vector registers.\nGPUs: scans are implemented at warp and block levels, with CUDA providing primitives like thrust::inclusive_scan. Warp shuffles (__shfl_up_sync) allow efficient intra-warp scans without shared memory.\n\n\n\nMemory and synchronization.\n\nIn-place scans reduce memory use but complicate parallelization.\nExclusive vs inclusive variants require careful handling of initial values.\nSynchronization overhead and false sharing are common risks in multithreaded CPU scans.\nDistributed scans (MPI) require combining partial results from each node, then adjusting local scans with offsets.\n\n\n\nLibraries and implementations.\n\nC++ TBB: parallel_scan supports both exclusive and inclusive.\nCUDA Thrust: inclusive_scan, exclusive_scan for GPU workloads.\nOpenMP: provides #pragma omp parallel for reduction but true scans require more explicit handling.\nMPI: MPI_Scan and MPI_Exscan provide distributed prefix sums.\n\n\n\nSystem-level use cases.\n\nParallel histogramming: count frequencies in parallel, prefix sums to compute cumulative counts.\nRadix sort: scans partition data into buckets efficiently.\nStream compaction: filter elements while maintaining order.\nGPU memory allocation: prefix sums assign disjoint output positions to threads.\nDatabase indexing: scans help build offsets for columnar data storage.\n\n\n\nPitfalls.\n\nRace conditions when threads update overlapping memory.\nLoad imbalance in irregular workloads (e.g., skewed distributions).\nWrong handling of inclusive vs exclusive leads to subtle bugs in partitioning algorithms.\n\n\n\nProfiling and optimization.\n\nBenchmark sequential vs parallel scan on arrays of size 10^6 or 10^9.\nCompare scalability with 2, 4, 8, … cores.\nMeasure GPU kernel efficiency at warp, block, and grid levels.\n\n\n\n\nWorked Example (CUDA Thrust)\n#include &lt;thrust/device_vector.h&gt;\n#include &lt;thrust/scan.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    thrust::device_vector&lt;int&gt; data{1, 2, 3, 4, 5};\n\n    // Inclusive scan\n    thrust::inclusive_scan(data.begin(), data.end(), data.begin());\n    std::cout &lt;&lt; \"Inclusive scan: \";\n    for (int x : data) std::cout &lt;&lt; x &lt;&lt; \" \";\n    std::cout &lt;&lt; std::endl;\n\n    // Exclusive scan\n    thrust::device_vector&lt;int&gt; data2{1, 2, 3, 4, 5};\n    thrust::exclusive_scan(data2.begin(), data2.end(), data2.begin(), 0);\n    std::cout &lt;&lt; \"Exclusive scan: \";\n    for (int x : data2) std::cout &lt;&lt; x &lt;&lt; \" \";\n    std::cout &lt;&lt; std::endl;\n}\nThis program offloads prefix sum computation to the GPU. With thousands of threads, even huge arrays can be scanned in milliseconds.\n\n\nWhy it matters\nPrefix sums are a textbook example of how a simple algorithm scales into a building block of parallel computing. They are used in compilers, graphics, search engines, and machine learning systems. They show how rethinking algorithms for hardware (CPU caches, SIMD, GPUs, distributed clusters) leads to new designs.\n\n\nExercises\n\nImplement the Hillis–Steele scan for an array of length 16 and show each step.\nImplement the Blelloch scan in pseudocode and explain how the up-sweep and down-sweep phases work.\nBenchmark a sequential prefix sum vs an OpenMP parallel scan on 10^7 elements.\nIn CUDA, implement an exclusive scan at the warp level using shuffle instructions.\nExplain how prefix sums are used in stream compaction (removing zeros from an array while preserving order).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#deep-dive-18",
    "href": "chapter_2.html#deep-dive-18",
    "title": "Chapter 2. Arrays",
    "section": "Deep Dive",
    "text": "Deep Dive\n\n2.1 Static Arrays\n\nMemory alignment and padding in C and assembly.\nArray indexing formulas compiled into machine code.\nPage tables and kernel use of fixed-size arrays (task_struct, inode).\nVectorization of loops over static arrays (SSE/AVX).\nBounds checking elimination in high-level languages.\n\n\n\n2.2 Dynamic Arrays\n\nGrowth factor experiments: doubling vs 1.5× vs incremental.\nProfiling Python’s list growth strategy (measure capacity jumps).\nAmortized vs worst-case complexity: proofs with actual benchmarks.\nReallocation latency spikes in low-latency systems.\nComparing std::vector::reserve vs default growth.\nMemory fragmentation in long-running programs.\n\n\n\n2.3 Slices & Views\n\nSlice metadata structure in Go (ptr, len, cap).\nRust borrow checker rules for &[T] vs &mut [T].\nNumPy stride tricks: transpose as a view, not a copy.\nPerformance gap: traversing contiguous vs strided slices.\nCache/TLB impact of strided access (e.g., step=16).\nFalse sharing when two threads use overlapping slices.\n\n\n\n2.4 Multidimensional Arrays\n\nRow-major vs column-major benchmarks: traverse order timing.\nLinear index formulas for N-dimensional arrays.\nLoop tiling/blocking for matrix multiplication.\nStructure of Arrays (SoA) vs Array of Structures (AoS).\nFalse sharing and padding in multi-threaded traversal.\nBLAS/LAPACK optimizations and cache-aware kernels.\nGPU coalesced memory access in 2D/3D arrays.\n\n\n\n2.5 Sparse Arrays & Compressed Layouts\n\nCOO, CSR, CSC: hands-on with memory footprint and iteration cost.\nComparing dictionary-based vs CSR-based sparse vectors.\nParallel SpMV benchmarks on CPU vs GPU.\nDIA and ELL formats: why they shine in structured sparsity.\nHybrid GPU formats (HYB: ELL + COO).\nSearch engine inverted indices as sparse structures.\nSparse arrays in ML: bag-of-words and embeddings.\n\n\n\n2.6 Prefix Sums & Scans\n\nInclusive vs exclusive scans: correctness pitfalls.\nHillis–Steele vs Blelloch scans: step count vs work efficiency.\nCache-friendly prefix sums on CPUs (blocked scans).\nSIMD prefix sum using AVX intrinsics.\nCUDA warp shuffle scans (__shfl_up_sync).\nMPI distributed scans across clusters.\nStream compaction via prefix sums (remove zeros in O(n)).\nRadix sort built from parallel scans.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#lab",
    "href": "chapter_2.html#lab",
    "title": "Chapter 2. Arrays",
    "section": "LAB",
    "text": "LAB\n\n2.1 Static Arrays\n\nLAB 1: Implement fixed-size arrays in C and Python, compare access/update speeds.\nLAB 2: Explore how static arrays are used in Linux kernel (task_struct, page tables).\nLAB 3: Disassemble a simple loop over a static array and inspect the generated assembly.\nLAB 4: Benchmark cache effects: sequential vs random access in a large static array.\n\n\n\n2.2 Dynamic Arrays\n\nLAB 1: Implement your own dynamic array in C (with doubling strategy).\nLAB 2: Benchmark Python’s list growth by tracking capacity changes while appending.\nLAB 3: Compare growth factors: doubling vs 1.5× vs fixed increments.\nLAB 4: Stress test reallocation cost by appending millions of elements, measure latency spikes.\nLAB 5: Use std::vector::reserve in C++ and compare performance vs default growth.\n\n\n\n2.3 Slices & Views\n\nLAB 1: In Go, experiment with slice creation, capacity, and append — observe when new arrays are allocated.\nLAB 2: In Rust, create overlapping slices and see how the borrow checker enforces safety.\nLAB 3: In Python, compare slicing a list vs slicing a NumPy array — demonstrate copy vs view behavior.\nLAB 4: Benchmark stride slicing in NumPy (arr[::16]) and explain performance drop.\nLAB 5: Demonstrate aliasing bugs when two slices share the same underlying array.\n\n\n\n2.4 Multidimensional Arrays\n\nLAB 1: Write code to traverse a 1000×1000 array row by row vs column by column, measure performance.\nLAB 2: Implement your own 2D array in C using both contiguous memory and array-of-pointers, compare speed.\nLAB 3: Use NumPy to confirm row-major order with .strides, then create a column-major array and compare.\nLAB 4: Implement a tiled matrix multiplication in C/NumPy and measure cache improvement.\nLAB 5: Experiment with SoA vs AoS layouts for a struct of 3 floats (x,y,z). Measure iteration performance.\n\n\n\n2.5 Sparse Arrays & Compressed Layouts\n\nLAB 1: Implement sparse arrays with Python dict vs dense lists, compare memory usage.\nLAB 2: Build COO and CSR representations for the same matrix, print memory layout.\nLAB 3: Benchmark dense vs CSR matrix-vector multiplication.\nLAB 4: Use SciPy’s csr_matrix and csc_matrix, run queries, compare performance.\nLAB 5: Implement a simple search engine inverted index as a sparse array of word→docID list.\n\n\n\n2.6 Prefix Sums & Scans\n\nLAB 1: Write inclusive and exclusive prefix sums in Python.\nLAB 2: Benchmark prefix sums for answering 1000 range queries vs naive summation.\nLAB 3: Implement Blelloch scan in C/NumPy and visualize the up-sweep/down-sweep steps.\nLAB 4: Implement prefix sums on GPU (CUDA/Thrust), compare speed to CPU.\nLAB 5: Use prefix sums for stream compaction: remove zeros from an array while preserving order.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "Chapter 3. Strings",
    "section": "",
    "text": "3.1 Representation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#representation",
    "href": "chapter_3.html#representation",
    "title": "Chapter 3. Strings",
    "section": "",
    "text": "3.1 L0 — Necklace of Characters\nA string is a sequence of characters. Each character can be a letter, number, symbol, or even whitespace. Strings are used to represent text, from simple names and messages to entire documents. In programming, a string is usually enclosed in quotes to tell the computer where it begins and ends. Strings are fundamental because almost every program interacts with text: input from users, output on screens, or data stored in files.\n\nDeep Dive\nStrings look simple but have a few important properties that beginners must understand.\n\nIn many languages such as Python, strings cannot be changed after creation. If a program “changes” a string, what really happens is the creation of a new one. This is why appending repeatedly can be costly.\n\nEach character in a string has a position, starting at 0. Accessing is direct: word[0] gives the first character. Slicing extracts portions: word[1:4] creates a substring.\n\n\n\nString\nIndex Positions\n\n\n\n\n\"HELLO\"\nH=0, E=1, L=2, L=3, O=4\n\n\n\nNegative indices count backward: word[-1] gives the last character.\n\nStrings can be combined or repeated. Concatenation uses the + operator; repetition uses *. Example: \"Hi\" + \"!\" → \"Hi!\", \"Na\" * 4 → \"NaNaNaNa\".\n\nQuotes (' or \") mark strings. Escape characters handle special cases: \"\\n\" is newline, \"\\t\" is tab. Triple quotes allow multi-line text.\n\nComputers store strings as numbers. ASCII is a small table of 128 characters (A–Z, digits, symbols). Unicode is a universal table covering all writing systems. Modern programs use Unicode (usually UTF-8 encoding), allowing text like \"こんにちは\" or \"😊\".\n\n\nWorked Example (Python)\n# Defining different kinds of strings\ns1 = \"Hello\"\ns2 = 'World'\ns3 = \"\"\"This is \na multi-line string.\"\"\"\n\n# Indexing\nprint(s1[0])      # H\nprint(s1[-1])     # o\n\n# Slicing\nprint(s1[1:4])    # ell\n\n# Concatenation and repetition\ngreeting = s1 + \" \" + s2\nprint(greeting)   # Hello World\n\nlaugh = \"Ha\" * 3\nprint(laugh)      # HaHaHa\n\n# Escapes\nquote = \"She said: \\\"Yes!\\\"\"\nprint(quote)      # She said: \"Yes!\"\n\n# Unicode characters\nemoji = \"Smile 😊\"\nprint(emoji)      # Smile 😊\n\n\nWhy it matters\nStrings are everywhere. They store names, labels, and text messages. They carry commands and configurations in programs. They are the backbone of web pages, logs, and data files. Understanding immutability avoids performance mistakes. Knowing slicing and concatenation makes manipulation easier. Being aware of Unicode prevents bugs in multilingual applications. Strings look simple but are a core abstraction every developer must master.\n\n\nExercises\n\nTake the word \"COMPUTER\" and print the first, middle, and last character.\nSlice the string \"PROGRAMMING\" to extract \"GRAM\".\nConcatenate \"DATA\" and \"BASE\" into one string without using +.\nRepeat the string \"Na\" four times, then add \" Batman!\" at the end.\nCreate a multi-line string containing your name, age, and favorite color.\nFind the difference between word[0] and word[-1] in \"PYTHON\".\nWrite a string that contains both single and double quotes.\nShow that \"abc\" * 0 results in an empty string.\nPrint the Unicode string for \"😊\" alongside a normal word.\nExplain in one sentence why immutability matters when working with strings.\n\n\n\n\n3.1 L1 — Encodings & Efficiency\nStrings in modern programming languages are more than just “text.” They are data structures designed with specific trade-offs in memory layout, immutability, and performance. At this level, understanding how strings behave internally allows programmers to write faster, safer, and more maintainable code.\n\nOverview/Definition\nA string is immutable in many languages, meaning once created, it cannot be changed. This design improves safety and makes strings easier to reason about, but it also affects performance. Operations like concatenation, slicing, and encoding conversions create new strings under the hood. Intermediate programmers must understand how these costs accumulate, when interning optimizes memory usage, and how encodings like UTF-8 and UTF-16 impact representation.\n\n\nDeep Dive\n\nStrings cannot be modified in place in Python, Java, or Go. If \"hello\" is changed to \"hallo\", the original object remains untouched, and a new one is created. This has two main consequences:\n\nSafe sharing across code without fear of accidental modification.\nExtra memory and CPU costs when building new strings repeatedly.\n\n\nLanguages often reuse identical string values. For example, Python keeps a single \"foo\" literal in memory if used multiple times. This is called interning.\n\nAutomatic interning happens for identifiers and some literals.\nManual interning (sys.intern()) can reduce memory in applications with repeated keys (like symbol tables or parsers).\n\n\nStrings store characters differently across languages:\n\nUTF-8: Variable-width, 1–4 bytes per character. Compact for ASCII-heavy text.\nUTF-16: Mostly 2 bytes per character, but uses surrogate pairs for others.\nUTF-32: Fixed 4 bytes per character, simple but wasteful.\n\n\n\nEncoding\nExample Text \"A😊\"\nStorage Size\n\n\n\n\nASCII\nnot representable\n—\n\n\nUTF-8\n0x41 0xF0 0x9F 0x98 0x8A\n5 bytes\n\n\nUTF-16\n0x0041 0xD83D 0xDE0A\n6 bytes\n\n\nUTF-32\n0x00000041 0x0001F60A\n8 bytes\n\n\n\nThis matters because operations like slicing or indexing depend on how characters are encoded.\n\nConcatenation Costs\n\nIn Python, a + b builds a new string by copying both inputs.\nRepeated concatenation inside loops leads to quadratic performance.\nSolution: accumulate pieces in a list and join once.\n\nExample of inefficient vs efficient approach:\n# Inefficient\nresult = \"\"\nfor i in range(1000):\n    result += str(i)\n\n# Efficient\nparts = []\nfor i in range(1000):\n    parts.append(str(i))\nresult = \"\".join(parts)\nSlicing and Substrings\n\nIn Python, s[2:5] creates a new string, copying data.\nIn Java before version 7u6, substrings shared the same underlying array (risking memory leaks if a small substring referenced a huge array). After 7u6, substring copies data to avoid this issue.\nIn C++, std::string_view allows non-owning references to substrings without copying.\n\nEncoding and Decoding\n\nStrings are abstract characters; at some point, they must become bytes.\n.encode() turns a Python str into bytes using a given encoding.\n.decode() reverses the process.\nPitfalls: decoding with the wrong encoding leads to errors or corrupted text.\n\nPerformance Pitfalls in Practice\n\nParsing logs or JSON with millions of lines can cause memory spikes if string copies accumulate.\nUnicode introduces complexity: slicing \"😊\" may split it in half if treated as bytes instead of characters.\nBenchmarking is essential when dealing with high-throughput text pipelines.\n\nCross-Language Comparisons\n\n\n\n\n\n\n\n\nLanguage\nRepresentation\nNotes\n\n\n\n\nPython\nUnicode (PEP 393 flexible storage)\nOptimized for compactness.\n\n\nJava\nUTF-16, immutable String\nUses StringBuilder for mutable ops.\n\n\nC++\nstd::string, string_view\nSSO (small string optimization) avoids heap allocation for short strings.\n\n\nGo\nUTF-8 encoded immutable slices\nstring is a read-only []byte.\n\n\n\n\n\nWorked Example (Python)\n# Interning demonstration\nimport sys\n\na = \"hello\"\nb = \"hello\"\nprint(a is b)  # True, both refer to the same interned literal\n\n# Manual interning\nx = sys.intern(\"repeated_key\")\ny = sys.intern(\"repeated_key\")\nprint(x is y)  # True\n\n# Encoding and decoding\ntext = \"Café\"\nencoded = text.encode(\"utf-8\")\nprint(encoded)  # b'Caf\\xc3\\xa9'\ndecoded = encoded.decode(\"utf-8\")\nprint(decoded)  # Café\n\n# Concatenation efficiency\nwords = [\"alpha\", \"beta\", \"gamma\"]\njoined = \"-\".join(words)\nprint(joined)  # alpha-beta-gamma\n\n\nWhy it matters\nIntermediate-level understanding of string representation prevents subtle but costly mistakes. Knowing that strings are immutable avoids slow concatenation loops. Interning saves memory in large-scale text-heavy systems. Choosing the right encoding prevents bugs when handling international text. Understanding substring behavior helps avoid hidden memory leaks. These details directly impact performance, correctness, and reliability in production systems.\n\n\nExercises\n\nWrite code that shows the difference in performance between concatenating strings in a loop and using join.\nDemonstrate string interning: show two identical literals referencing the same object.\nEncode and decode a string containing accented characters using UTF-8.\nTake a long string and slice it into two halves; explain why slicing creates a new object in Python.\nCompare memory usage between UTF-8 and UTF-16 for a text that contains only ASCII characters.\nInvestigate what happens when decoding a UTF-8 byte sequence with ASCII encoding.\nShow why \"😊\"[0] in Python gives a character but in UTF-8 byte arrays it spans multiple bytes.\nExplain the difference between Java String and StringBuilder in terms of mutability.\nWrite code that demonstrates the effect of sys.intern() on repeated keys.\nResearch and explain how std::string_view in C++ avoids copies.\n\n\n\n\n3.1 L2 — Internals & Systems\nStrings are central to modern software, but their internal design depends heavily on language, runtime, and operating system. At this level, understanding low-level representation, OS interaction, and hardware acceleration becomes critical.\nStrings are immutable in most high-level languages, but under the hood they are arrays of bytes or characters managed by the runtime. Different languages make different trade-offs: memory layout, encoding choice, and substring handling. The operating system provides system calls for moving string data between memory and devices. Hardware accelerates common operations like copying and comparison. Production libraries extend functionality for correctness and speed.\n\nDeep Dive\n\nLow-Level Representations\nC strings are arrays of char terminated by '\\0'. A missing terminator causes buffer overflows. C++ std::string stores length and data, with small string optimization (SSO) to keep short strings inline. Python uses PEP 393: compact Unicode with 1, 2, or 4 bytes per character depending on the highest code point. Java String is UTF-16, compressed if all characters fit in Latin-1. Go strings are immutable slices of bytes; Rust distinguishes String (owned) and &str (borrowed).\n\n\n\nLanguage\nRepresentation\nNotes\n\n\n\n\nC\nchar * + '\\0'\nSimple but unsafe.\n\n\nC++\nstd::string, SSO\nShort strings inline.\n\n\nPython\nFlexible Unicode\nAdapts width per string.\n\n\nJava\nUTF-16, compressed\nImmutable.\n\n\nGo\nUTF-8, slice view\nImmutable.\n\n\nRust\nString / &str\nUTF-8 guaranteed.\n\n\n\n\n\nOS-Level Considerations\nStrings occupy stack or heap memory. Large strings cause fragmentation. Garbage-collected languages like Java and Python reclaim unused strings automatically. System calls (read, write) exchange raw bytes with the OS. In C, forgetting '\\0' causes overruns. At boundaries (sockets, files), encoding mismatches are a common source of bugs.\n\n\nHardware-Level Considerations\nPerformance depends on cache alignment. A string aligned to cache lines is scanned faster. Libraries like glibc use SIMD instructions for memcpy, memcmp, and strlen, processing multiple bytes per CPU instruction. Large-scale search and comparison rely on vectorized instructions. Misaligned memory leads to extra CPU cycles.\n\n\nAdvanced Encoding Issues\nUnicode normalization ensures two strings that look identical are stored in the same form. NFC and NFD differ in how accents are represented. UTF-16 surrogate pairs represent code points beyond 0xFFFF. Grapheme clusters (like “👨‍👩‍👧‍👦”) span multiple code points but behave as one character for users. Security issues arise: Unicode confusables trick users (а in Cyrillic vs a in Latin), and null characters may bypass string checks in C.\n\n\nProduction Libraries & Techniques\nICU provides collation, normalization, and locale-sensitive operations. RE2 avoids regex backtracking vulnerabilities. Hyperscan accelerates string matching using SIMD. String views or slices in C++ and Rust enable zero-copy substrings.\n\n\nPerformance Engineering\nConcatenation cost is linear in total length. Substrings may leak memory if they reference a large parent buffer (Java pre-7u6). Copy semantics avoid leaks but cost memory. Benchmarking is necessary to choose the right trade-off. Immutability simplifies reasoning but requires builders (StringBuilder, bytes.Buffer) for efficiency.\n\n\n\nWorked Example (Python)\nimport sys\nimport unicodedata\nimport time\n\n# --- Representation ---\nascii_text = \"Hello\"\nunicode_text = \"こんにちは😊\"\nprint(\"ASCII:\", len(ascii_text), \"chars,\", len(ascii_text.encode(\"utf-8\")), \"bytes\")\nprint(\"Unicode:\", len(unicode_text), \"chars,\", len(unicode_text.encode(\"utf-8\")), \"bytes\")\n\n# --- Encoding/Decoding ---\ndata = \"Café\"\nutf8_bytes = data.encode(\"utf-8\")\nprint(\"UTF-8 bytes:\", utf8_bytes)\nprint(\"Decoded back:\", utf8_bytes.decode(\"utf-8\"))\n\n# --- Interning ---\na = \"hello\"\nb = \"hello\"\nprint(\"a is b:\", a is b)\nx = sys.intern(\"repeated_key\")\ny = sys.intern(\"repeated_key\")\nprint(\"x is y:\", x is y)\n\n# --- Concatenation benchmark ---\nN = 20000\nstart = time.time()\ns = \"\"\nfor i in range(N):\n    s += \"x\"\nprint(\"Naive concat time:\", round(time.time() - start, 4), \"s\")\n\nstart = time.time()\nparts = [\"x\" for _ in range(N)]\ns = \"\".join(parts)\nprint(\"Join concat time:\", round(time.time() - start, 4), \"s\")\n\n# --- Unicode normalization ---\ns1 = \"café\"          # composed\ns2 = \"cafe\\u0301\"    # decomposed\nprint(\"Raw equal:\", s1 == s2)\nprint(\"NFC equal:\", unicodedata.normalize(\"NFC\", s1) == unicodedata.normalize(\"NFC\", s2))\n\n# --- Surrogate pairs & grapheme clusters ---\nsmile = \"😊\"\nprint(\"UTF-16 units:\", len(smile.encode(\"utf-16\")) // 2)\nfamily = \"👨‍👩‍👧‍👦\"\nprint(\"Family code points:\", len(family), \"rendered:\", family)\n\n# --- Unicode confusables ---\nlatin_a = \"a\"\ncyrillic_a = \"а\"  # visually similar\nprint(\"Latin a == Cyrillic a:\", latin_a == cyrillic_a)\nprint(\"Latin ord:\", ord(latin_a), \"Cyrillic ord:\", ord(cyrillic_a))\n\n\nWhy it matters\nKnowing internals prevents bugs and security issues. Buffer overflows from missing terminators break systems. Misunderstood Unicode can cause errors in databases or user interfaces. Cache alignment and SIMD accelerate processing of huge text datasets. Normalization avoids mismatched text values. Confusables create attack vectors. Production systems depend on engineers understanding these low-level details.\n\n\nExercises\n\nExplain why a string with a missing terminator can cause reading beyond its memory.\nCompare how many bytes the string \"Hello\" uses in UTF-8, UTF-16, and UTF-32.\nShow two visually identical strings that are not equal because of different Unicode code points.\nDemonstrate normalization making two unequal strings equal.\nMeasure and compare performance between naive concatenation and buffered concatenation.\nExplain how surrogate pairs represent characters beyond 0xFFFF.\nConstruct a grapheme cluster (like a family emoji) and count its code points versus user-perceived characters.\nShow how substring references can create memory leaks if the original large string is kept alive.\nInvestigate how cache alignment could affect the speed of scanning a very large string.\nDesign a test case that reveals a security risk using Unicode confusables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#operations",
    "href": "chapter_3.html#operations",
    "title": "Chapter 3. Strings",
    "section": "3.2 Operations",
    "text": "3.2 Operations\n\n3.2 L0 — Everyday String Manipulations\nStrings are not only stored but also actively used and modified. Beginners often start with everyday tasks: changing text to uppercase, trimming spaces, searching for words, and building sentences. These operations are simple yet powerful, and they form the foundation of text processing in any program.\nString operations are ways to manipulate or examine text. They include changing case, removing whitespace, searching for substrings, splitting sentences into words, joining words into sentences, and formatting messages. These actions are essential for handling user input, displaying results, and working with text files.\n\nDeep Dive\n\nCase Conversion\nText often needs to be converted. Uppercase makes words stand out (\"hello\" → \"HELLO\"). Lowercase is useful for comparisons (\"Yes\" vs \"yes\"). Case conversion helps normalize text before processing.\n\n\nTrimming Whitespace\nWhitespace before or after text causes bugs when comparing values. \" hello \" is not the same as \"hello\". Trimming removes unwanted spaces, tabs, or newlines.\n\n\nReplacing Substrings\nSometimes one part of text must change. \"cat\".replace(\"c\", \"h\") becomes \"hat\". Replacing works for single characters or words.\n\n\nSearching in Strings\nFinding a substring answers whether \"dog\" appears inside \"hotdog\". Many languages return the index where the substring begins. If not found, they return a special value (like -1 or None).\n\n\n\nOperation\nExample\nResult\n\n\n\n\nContains\n\"hello\" in \"say hello\"\nTrue\n\n\nIndex\n\"abc\".find(\"b\")\n1\n\n\nNot found\n\"abc\".find(\"z\")\n-1 (or equivalent)\n\n\n\n\n\nSplitting and Joining\nSplitting breaks text into parts: \"one two three\" → [\"one\", \"two\", \"three\"]. Joining does the reverse: [\"a\",\"b\",\"c\"] → \"a-b-c\". They are inverse operations.\n\n\nFormatting\nCombining variables with text creates messages. \"Hello \" + name works, but formatting systems are clearer: \"Hello, {name}!\". Formatting ensures readable and consistent text output.\n\n\n\nWorked Example (Python)\n# Case conversion\nword = \"Hello\"\nprint(word.upper())   # HELLO\nprint(word.lower())   # hello\n\n# Trimming whitespace\ntext = \"   data  \"\nprint(\"Before:\", repr(text))\nprint(\"After:\", repr(text.strip()))\n\n# Replacing substrings\nanimal = \"cat\"\nprint(animal.replace(\"c\", \"h\"))  # hat\n\n# Searching\nsentence = \"the quick brown fox\"\nprint(\"fox\" in sentence)        # True\nprint(sentence.find(\"quick\"))   # 4\nprint(sentence.find(\"dog\"))     # -1\n\n# Splitting and joining\nline = \"one two three\"\nparts = line.split(\" \")\nprint(parts)  # ['one', 'two', 'three']\njoined = \"-\".join(parts)\nprint(joined)  # one-two-three\n\n# Formatting\nname = \"Alice\"\nage = 30\nprint(f\"My name is {name}, I am {age} years old.\")\n\n\nWhy it matters\nText is everywhere: names, messages, configuration files, logs. Programs must read, search, and reshape strings constantly. Converting case enables reliable comparisons. Trimming prevents subtle errors with extra spaces. Splitting and joining support structured data handling. Formatting improves readability for users. These basic operations turn raw text into structured, usable information.\n\n\nExercises\n\nConvert the string \"python\" to uppercase and lowercase.\nRemove extra spaces from \"   hello world   \".\nReplace \"dog\" with \"cat\" in the sentence \"the dog barked\".\nCheck if \"apple\" is inside \"pineapple\".\nFind the position of \"moon\" in \"the moon rises\".\nSplit \"red,green,blue\" into parts and then join them back with \"|\".\nCount how many times the word \"the\" appears in \"the cat and the dog\".\nFormat a string that says \"My favorite number is X\", where X is any integer.\nCombine a list of words into a sentence with spaces in between.\nExplain in one sentence why trimming whitespace is important when processing user input.\n\nDo you want me to continue with 3.2 L1 in the same structured style, adding intermediate concepts like regex basics and efficient searching?\n\n\n\n3.2 L1 — Patterns & Practical Tricks\nStrings are not only for simple manipulations. In real programs, text must be searched, parsed, and transformed efficiently. At this level, the focus shifts from basic usage to techniques that handle larger inputs, avoid performance traps, and apply structured text processing.\nIntermediate string operations expand beyond uppercase, trimming, and splitting. They involve efficient searching, counting, and pattern matching. Regular expressions become a tool for flexible matching. Developers also need to parse structured data such as CSV and JSON, where correctness and performance both matter.\n\nDeep Dive\n\nEfficient Searching\nMost languages provide methods like find, index, startswith, and endswith. These run in linear time but are implemented with optimizations in native code. Checking prefixes and suffixes avoids scanning the entire string.\n\n\nCounting and Replacing\nCounting how many times a substring occurs is essential for statistics or validation. Replacing substrings can transform logs or templates. Efficiency matters: repeated replacements in large text can be costly.\n\n\n\nOperation\nExample Input\nResult\n\n\n\n\nStartswith\n\"hello\".startswith(\"he\")\nTrue\n\n\nEndswith\n\"world\".endswith(\"ld\")\nTrue\n\n\nCount substring\n\"banana\".count(\"na\")\n2\n\n\nReplace\n\"2025-09\".replace(\"-\", \"/\")\n\"2025/09\"\n\n\n\n\n\nRegular Expressions (Regex)\nRegex allows powerful pattern matching with symbols:\n\n. matches any character.\n* means repeat zero or more times.\n+ means one or more times.\n[abc] matches any of the listed characters.\n\nExample:\n\nPattern [0-9]+ matches any sequence of digits.\nPattern \\w+@\\w+\\.\\w+ matches simple email addresses.\n\nRegex engines vary: some use backtracking (flexible but can be slow), others use DFA/NFA (linear-time but less expressive).\n\n\nParsing Structured Text\nMany formats (CSV, JSON, logs) are line-based text. Splitting on commas or spaces is not enough for robust parsing, but for simple cases splitting and trimming work. Intermediate developers should know the limitations of naive parsing and when to use libraries.\n\n\nPerformance Pitfalls\nNaive string concatenation inside loops can be quadratic in cost. Multiple find/replace calls on large inputs can also degrade performance. The best practice is to buffer results or use streaming parsers.\n\n\n\nWorked Example (Python)\nimport re\n\ntext = \"user: alice, email: alice@example.com; user: bob, email: bob@example.org\"\n\n# Efficient searching\nprint(text.startswith(\"user\"))      # True\nprint(text.endswith(\"org\"))         # True\nprint(text.find(\"bob\"))             # position of 'bob'\n\n# Counting and replacing\nfruit = \"banana\"\nprint(fruit.count(\"na\"))            # 2\nprint(fruit.replace(\"na\", \"NA\"))    # baNANA\n\n# Regex: find all email addresses\nemails = re.findall(r\"\\w+@\\w+\\.\\w+\", text)\nprint(emails)  # ['alice@example.com', 'bob@example.org']\n\n# Regex: extract all numbers from a string\ndata = \"Order 123, item 456, total 789\"\nnumbers = re.findall(r\"[0-9]+\", data)\nprint(numbers)  # ['123', '456', '789']\n\n# Parsing structured text (simple CSV line)\nline = \"apple, banana, cherry\"\nparts = [p.strip() for p in line.split(\",\")]\nprint(parts)  # ['apple', 'banana', 'cherry']\n\n# Performance demo: avoid naive concatenation\nN = 10000\n# Slow\ns = \"\"\nfor i in range(N):\n    s += \"x\"\nprint(\"Naive length:\", len(s))\n\n# Fast\ns = \"\".join([\"x\" for _ in range(N)])\nprint(\"Join length:\", len(s))\n\n\nWhy it matters\nIntermediate operations turn raw text into structured data. Searching and counting enable validation and analysis. Regex provides flexible pattern extraction, but misuse can harm performance. Parsing prepares text for further computation. Avoiding pitfalls like naive concatenation ensures scalable code. These techniques are essential for handling logs, configurations, user inputs, and datasets in real-world applications.\n\n\nExercises (Easy → Hard)\n\nCheck if the string \"algorithm\" starts with \"algo\" and ends with \"rithm\".\nCount how many times \"the\" appears in \"the cat and the dog and the theater\".\nReplace every \"-\" in \"2025-09-10\" with \"/\".\nExtract all numbers from the text \"room 101, floor 5, building 42\".\nWrite a regex to match any word that starts with \"a\" and ends with \"e\".\nParse \"red, green, blue , yellow\" into a clean list of color names without spaces.\nDemonstrate why repeated concatenation in a loop is slower than using a buffer or join.\nWrite a regex to match simple phone numbers like \"123-456-7890\".\nExplain why splitting CSV by commas fails if values contain quotes (e.g., \"apple, \"banana,grape\", cherry\").\nDesign a regex that extracts domain names from email addresses.\n\n\n\n\n3.2 L2 — Algorithms & Systems\nAdvanced string operations rely on specialized algorithms and system support. Searching and matching must be efficient on gigabytes of text. Regex engines balance flexibility with safety. Compilers tokenize source code into symbols using string scanning. And at the system boundary, encoding and locale awareness become critical for correct communication.\n\nDeep Dive\n\nString Search Algorithms\nThe naive search scans one character at a time, which is simple but inefficient for large text.\n\nKnuth–Morris–Pratt (KMP) preprocesses the pattern to avoid redundant comparisons.\nBoyer–Moore skips ahead using knowledge of mismatched characters, performing sublinear searches in practice.\nThese algorithms form the backbone of text editors, search utilities, and compilers.\n\n\n\nRegex Engine Internals\nRegex engines are either backtracking-based (Perl, Python) or automata-based (RE2, Rust).\n\nBacktracking engines support rich features but risk exponential slowdowns.\nDFA/NFA-based engines guarantee linear-time execution but restrict advanced patterns.\nJIT compilation of regex (like in Java or .NET) generates machine code for speed.\n\n\n\nTokenization and Lexical Analysis\nCompilers and interpreters must scan source code into tokens. This uses deterministic finite automata (DFAs) built from regex-like rules. For example, recognizing identifiers, keywords, and numbers all rely on string scanning.\n\n\nUnicode-Aware Searching\nSearching in Unicode text requires more than simple byte comparison. Case folding (making case-insensitive comparisons) differs by locale. Grapheme clusters must be considered as user-visible units, not code points.\n\n\nSystem Boundaries\nStrings cross boundaries when written to files, sockets, or system calls. Encodings must match between sender and receiver. Misalignment causes corrupted output or runtime errors. Databases and network APIs rely on normalized, correctly encoded strings.\n\n\nPerformance Considerations\nSubstring extraction may copy data (safe but costly) or reference parent buffers (fast but risky). SIMD instructions accelerate scanning large strings in modern CPUs. Benchmarks show naive operations become bottlenecks at scale, requiring optimized algorithms.\n\n\n\nWorked Example (Python)\nimport re\nimport time\n\n# --- Naive search ---\ndef naive_search(text, pattern):\n    for i in range(len(text) - len(pattern) + 1):\n        if text[i:i+len(pattern)] == pattern:\n            return i\n    return -1\n\n# --- KMP search ---\ndef kmp_table(pattern):\n    table = [0] * len(pattern)\n    j = 0\n    for i in range(1, len(pattern)):\n        while j &gt; 0 and pattern[i] != pattern[j]:\n            j = table[j-1]\n        if pattern[i] == pattern[j]:\n            j += 1\n            table[i] = j\n    return table\n\ndef kmp_search(text, pattern):\n    table = kmp_table(pattern)\n    j = 0\n    for i in range(len(text)):\n        while j &gt; 0 and text[i] != pattern[j]:\n            j = table[j-1]\n        if text[i] == pattern[j]:\n            j += 1\n        if j == len(pattern):\n            return i - j + 1\n    return -1\n\n# --- Performance test ---\ntext = \"a\" * 100000 + \"b\"\npattern = \"a\" * 1000 + \"b\"\n\nstart = time.time()\nprint(\"Naive:\", naive_search(text, pattern))\nprint(\"Naive time:\", round(time.time() - start, 4), \"s\")\n\nstart = time.time()\nprint(\"KMP:\", kmp_search(text, pattern))\nprint(\"KMP time:\", round(time.time() - start, 4), \"s\")\n\n# --- Regex engines ---\ndata = \"user: alice@example.com id: 42\"\nemails = re.findall(r\"\\w+@\\w+\\.\\w+\", data)\nprint(\"Regex found:\", emails)\n\n# --- Unicode case folding ---\ns1 = \"Straße\"\ns2 = \"STRASSE\"\nprint(\"Casefold equal:\", s1.casefold() == s2.casefold())\n\n# --- Grapheme cluster awareness ---\nfamily = \"👨‍👩‍👧‍👦\"\nprint(\"Code points:\", len(family))     # length in code points\nprint(\"Rendered:\", family)             # perceived as one emoji\n\n\nWhy it matters\nAt scale, naive methods fail. Efficient algorithms like KMP and Boyer–Moore make search practical in editors, search engines, and compilers. Regex engines must be chosen carefully: flexible backtracking engines can crash under malicious input, while DFA-based engines provide safety. Unicode awareness ensures correctness in multilingual systems. At system boundaries, encoding mismatches corrupt data. Performance engineering determines whether systems handle megabytes or terabytes of text reliably.\n\n\nExercises\n\nDescribe how the naive substring search works and why it can be inefficient.\nCompare the number of steps needed to search \"aaaaab\" in \"a\"*1000 + \"b\" using naive vs KMP.\nExplain the key idea behind Boyer–Moore that makes it faster than naive search.\nShow why some regex patterns can lead to exponential backtracking.\nWrite a regex that matches valid identifiers (letters, digits, underscores, not starting with a digit).\nExplain how compilers use finite automata to tokenize code.\nCompare string equality with and without Unicode case folding (\"Straße\" vs \"STRASSE\").\nShow how a grapheme cluster (like family emoji) differs from code point count.\nExplain why substring references in some languages (e.g., Java before 7u6) could cause memory leaks.\nDesign a benchmark plan to measure the impact of SIMD acceleration on string scanning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#comparison",
    "href": "chapter_3.html#comparison",
    "title": "Chapter 3. Strings",
    "section": "3.3 Comparison",
    "text": "3.3 Comparison\n\n3.3 L0 — Equality & Ordering\nComparing strings is one of the simplest but most important operations in programming. Equality checks determine if two pieces of text are the same, while ordering lets us sort words alphabetically. These operations are intuitive but have precise rules that beginners must understand.\n\nOverview/Definition\nTwo strings are equal if they contain exactly the same sequence of characters in the same order. Ordering compares strings lexicographically, meaning character by character from left to right, similar to how words are sorted in a dictionary. These rules allow programs to check conditions, filter text, and sort lists.\n\n\nDeep Dive\n\nEquality\nEquality is strict: \"cat\" equals \"cat\" but not \"Cat\". Whitespace matters too, so \"hello \" is not the same as \"hello\". Computers check equality by comparing each character in order until a difference is found or both end.\n\n\nInequality\nIf two strings differ in length or characters, they are not equal. The result is simply true or false.\nLexicographic Ordering Ordering compares based on the numerical values of characters. \"apple\" comes before \"banana\" because 'a' is less than 'b'. If the first characters are equal, the comparison moves to the next.\n\n\n\nComparison\nExample\nResult\n\n\n\n\nEqual\n\"cat\" == \"cat\"\nTrue\n\n\nCase-sensitive\n\"cat\" == \"Cat\"\nFalse\n\n\nShorter vs longer\n\"car\" &lt; \"cart\"\nTrue\n\n\nAlphabetical\n\"apple\" &lt; \"banana\"\nTrue\n\n\n\n\n\nSorting\nSorting a list of strings uses lexicographic rules. The result is the same as dictionary order in English when restricted to simple lowercase letters.\n\n\n\nWorked Example (Python)\n# Equality\nprint(\"cat\" == \"cat\")     # True\nprint(\"cat\" == \"Cat\")     # False\nprint(\"hello \" == \"hello\")# False\n\n# Inequality\nprint(\"dog\" != \"cat\")     # True\n\n# Lexicographic ordering\nprint(\"apple\" &lt; \"banana\") # True\nprint(\"zebra\" &gt; \"yak\")    # True\nprint(\"car\" &lt; \"cart\")     # True\n\n# Sorting a list\nwords = [\"banana\", \"apple\", \"cherry\"]\nprint(sorted(words))      # ['apple', 'banana', 'cherry']\n\n\nWhy it matters\nEquality and ordering are the foundation of text handling. Programs must compare passwords, search for keywords, or sort lists of names. Without correct comparison, search engines would mis-rank results and databases would fail to find matches. Even small differences like uppercase vs lowercase or extra spaces can change outcomes. Understanding these rules prevents errors in everyday programming.\n\n\nExercises\n\nCheck if \"dog\" equals \"Dog\".\nCompare \"sun\" and \"moon\" and decide which comes first alphabetically.\nShow why \"car\" is less than \"cart\".\nSort the list [\"pear\", \"apple\", \"orange\"] in dictionary order.\nExplain why \"hello \" and \"hello\" are not equal.\nCompare \"Zoo\" and \"apple\" and explain why uppercase letters can affect order.\nDemonstrate that two strings must have the same length and characters to be equal.\nGiven a list of names, describe how to remove duplicates using string equality.\nExplain what happens if two strings are equal up to the length of the shorter one (e.g., \"abc\" vs \"abcd\").\nDesign a procedure to sort a list of mixed short and long strings alphabetically.\n\n\n\n\n3.3 L1 — Collation & Locales\nComparing strings gets more complicated once you move beyond simple equality and dictionary order. Real-world applications must deal with case sensitivity, cultural rules, and mixed content like numbers and letters. At this level, programmers learn practical techniques for handling comparisons in everyday systems.\nCollation is the set of rules that define how strings are compared and sorted. While lexicographic order works for plain ASCII, different languages and contexts require more sophisticated rules. For example, \"ä\" may be treated as equal to \"a\" in one locale but as a separate letter in another. Intermediate-level comparison also introduces the idea of ignoring case and whitespace when appropriate.\n\nDeep Dive\n\nCase Sensitivity\nBy default, string comparison is case-sensitive: \"cat\" ≠ \"Cat\". Case-insensitive comparison requires converting both sides to the same case or using a locale-aware function.\n\n\nLocales and Cultural Rules\nDifferent languages define alphabetical order differently.\n\nIn German, \"ä\" often sorts like \"ae\".\nIn Swedish, \"ä\" is a separate letter after \"z\".\nIn English, \"apple\" &lt; \"Banana\" because uppercase and lowercase letters use different numeric codes, but in many systems, case-insensitive comparison is preferred.\n\n\n\nNumbers in Strings\nLexicographic order can be confusing with numbers. \"file10\" comes before \"file2\" because \"1\" sorts before \"2\". Natural sorting treats numbers as whole values so \"file2\" &lt; \"file10\".\n\n\nDatabases and Collation\nDatabases define collations to decide how text is compared and sorted. A collation can be case-sensitive (CS) or case-insensitive (CI). It can also be accent-sensitive (AS) or accent-insensitive (AI). This ensures consistent results across queries.\n\n\n\nComparison Type\nExample\nResult\n\n\n\n\nCase-sensitive\n\"cat\" == \"Cat\"\nFalse\n\n\nCase-insensitive\n\"cat\" ~ \"Cat\"\nTrue\n\n\nLocale (German)\n\"äpfel\" vs \"apfel\"\nEqual or ordered together\n\n\nNatural sorting\n\"file2\" &lt; \"file10\"\nTrue\n\n\n\n\n\n\nWorked Example (Python)\nimport locale\nimport re\n\n# Case-sensitive vs case-insensitive\nprint(\"cat\" == \"Cat\")            # False\nprint(\"cat\".lower() == \"Cat\".lower())  # True\n\n# Locale-aware comparison\nlocale.setlocale(locale.LC_COLLATE, \"de_DE.UTF-8\")  # German\nprint(locale.strcoll(\"äpfel\", \"apfel\"))  # May treat ä ~ ae\n\nlocale.setlocale(locale.LC_COLLATE, \"sv_SE.UTF-8\")  # Swedish\nprint(locale.strcoll(\"ä\", \"z\"))  # ä sorted after z\n\n# Natural sorting with numbers\nfiles = [\"file2\", \"file10\", \"file1\"]\nprint(sorted(files))   # Lexicographic: ['file1', 'file10', 'file2']\nfiles.sort(key=lambda x: [int(n) if n.isdigit() else n for n in re.split(r'(\\d+)', x)])\nprint(files)           # Natural sort: ['file1', 'file2', 'file10']\n\n\nWhy it matters\nCollation and locale rules affect almost every application that processes human text. Sorting names in a contact list, comparing file names, or filtering database queries all rely on comparison rules. Incorrect handling leads to confusing results for users, especially in multilingual contexts. Natural sorting improves readability when working with numbered data. Database collations ensure consistency in queries and reports.\n\n\nExercises\n\nCompare \"cat\" and \"Cat\" both case-sensitive and case-insensitive.\nExplain why \"file10\" comes before \"file2\" in lexicographic order.\nShow how converting both strings to lowercase helps with case-insensitive comparison.\nDescribe how \"ä\" is treated differently in German vs Swedish ordering.\nGiven a list of filenames (file1, file2, file10), sort them in natural order.\nExplain how a database collation can make \"résumé\" equal to \"resume\".\nShow how case-insensitive and accent-insensitive comparisons change the equality of \"café\" and \"CAFE\".\nCreate a rule set for sorting words that mix numbers and letters, like \"a2\", \"a10\", \"a3\".\nDemonstrate how incorrect collation could mis-sort a list of international names.\nDesign an algorithm for locale-aware string comparison that handles both case and accent differences.\n\n\n\n\n3.3 L2 — Deep Comparisons\nAt the advanced level, comparing strings requires knowledge of Unicode, normalization, system-level optimizations, and specialized libraries. Equality and ordering are no longer trivial: they must handle multiple scripts, cultural rules, and performance constraints at scale.\nDeep comparison of strings involves more than checking characters one by one. Unicode introduces multiple ways to represent the same text. Collation rules vary across locales. Large-scale systems need efficient comparison functions that work correctly with international text while remaining fast enough for billions of operations.\n\nDeep Dive\n\nUnicode Normalization\nThe same character can be represented in different ways. For example, \"é\" can be stored as a single code point (U+00E9) or as 'e' plus a combining accent (U+0065 U+0301). Normalization ensures consistent representation. NFC (composed) and NFD (decomposed) are the most common forms.\n\n\nCase Folding\nCase-insensitive comparisons must use case folding, which is more complex than simply converting to lowercase. For instance, the German \"ß\" becomes \"ss\" when folded.\n\n\nCollation with ICU\nThe International Components for Unicode (ICU) library provides robust comparison and sorting. It supports locale-sensitive rules, accent sensitivity, and custom collation. For example, \"résumé\" may equal \"resume\" in accent-insensitive mode.\n\n\nSystem-Level Implementations\nAt low levels, comparison often reduces to optimized memory functions. Functions like memcmp compare bytes quickly, using CPU instructions that process multiple bytes at once. Short-circuit evaluation stops at the first mismatch. Cache alignment improves throughput when scanning long strings.\n\n\nSecurity Concerns\nUnicode confusables (characters that look alike, such as Latin \"a\" and Cyrillic \"а\") can fool comparison routines if normalization is not enforced. Attackers exploit this in phishing and injection attacks.\n\n\n\n\n\n\n\n\nChallenge\nExample\nSolution\n\n\n\n\nMultiple representations\n\"é\" vs \"e\\u0301\"\nNormalize (NFC/NFD)\n\n\nCase folding\n\"Straße\" vs \"STRASSE\"\nUnicode-aware case folding\n\n\nCollation differences\n\"ä\" in German vs Swedish\nICU locale rules\n\n\nConfusables\n\"a\" vs \"а\"\nSecurity checks, mapping\n\n\n\n\n\n\nWorked Example (Python)\nimport unicodedata\nimport locale\n\n# Unicode normalization\ns1 = \"café\"          # composed\ns2 = \"cafe\\u0301\"    # decomposed\nprint(\"Raw equal:\", s1 == s2)\nprint(\"NFC equal:\", unicodedata.normalize(\"NFC\", s1) == unicodedata.normalize(\"NFC\", s2))\nprint(\"NFD equal:\", unicodedata.normalize(\"NFD\", s1) == unicodedata.normalize(\"NFD\", s2))\n\n# Case folding\nprint(\"ß\".casefold() == \"ss\")   # True\n\n# Locale-sensitive ordering\nlocale.setlocale(locale.LC_COLLATE, \"fr_FR.UTF-8\")  # French\nwords = [\"école\", \"eclair\", \"étude\"]\nprint(sorted(words, key=locale.strxfrm))  # Locale-aware sort\n\n# Confusables\nlatin_a = \"a\"\ncyrillic_a = \"а\"  # visually similar\nprint(\"Equal?\", latin_a == cyrillic_a)\nprint(\"Latin:\", ord(latin_a), \"Cyrillic:\", ord(cyrillic_a))\n\n# System-level demonstration (byte comparison)\nb1 = b\"apple\"\nb2 = b\"apricot\"\nprint(\"memcmp style:\", (b1 &gt; b2) - (b1 &lt; b2))  # -1, 0, or 1\n\n\nWhy it matters\nDeep string comparison underpins databases, search engines, and operating systems. Normalization ensures data consistency across storage and transfer. Case folding avoids incorrect matches in multilingual systems. ICU collation makes applications usable across cultures. Optimized byte comparisons keep performance acceptable at scale. Security requires detecting confusables to prevent spoofing. Without these techniques, global systems would be unreliable and vulnerable.\n\n\nExercises\n\nGive an example where two visually identical strings are unequal because of different Unicode encodings.\nNormalize two strings with accents and show they become equal.\nExplain why \"ß\" must be compared to \"ss\" in case-insensitive matching.\nSort the words \"école\", \"eclair\", \"étude\" using French collation rules.\nShow why \"a\" (Latin) and \"а\" (Cyrillic) are dangerous in string comparison.\nDescribe how memcmp works at the system level to compare two byte sequences.\nExplain how cache alignment improves performance when comparing long strings.\nDemonstrate the difference between accent-sensitive and accent-insensitive comparison with \"café\" vs \"cafe\".\nPropose a method to detect and mitigate Unicode confusables in user input.\nDesign a benchmark to compare normalized vs non-normalized string comparison in a large dataset.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#patterns",
    "href": "chapter_3.html#patterns",
    "title": "Chapter 3. Strings",
    "section": "3.4 Patterns",
    "text": "3.4 Patterns\n\n3.4 L0 — Simple Search\nStrings are often used to find and replace text. Beginners need to understand how to look for substrings, replace parts of text, and use simple wildcards. These operations are the first step toward more advanced pattern matching.\n\nOverview/Definition\nA search operation checks if a smaller string exists inside a larger one. If found, it returns the position or a confirmation. Replace operations swap one substring for another. Wildcards act as placeholders, matching characters in flexible ways. These basics allow programs to search, clean, and transform text.\n\n\nDeep Dive\n\nFinding Substrings\nThe simplest search asks: does \"dog\" appear inside \"hotdog\"? Most systems return the starting index of the match. If not found, a special value is returned.\n\n\nReplacing Text\nReplacement takes an old value and swaps it with a new one. \"the sky is blue\" with \"blue\" → \"red\" becomes \"the sky is red\". Replacement is useful for cleaning or formatting text.\n\n\nWildcards (Intuitive View)\nWildcards are simple symbols that represent “any character.” A * can mean “any sequence,” and a ? can mean “one character.” Example: pattern \"ca*\" matches \"cat\", \"car\", and \"castle\". Wildcards are simpler than full regular expressions but build the same intuition.\n\n\n\nOperation\nExample Input\nResult\n\n\n\n\nContains\n\"cat\" in \"concatenate\"\nTrue\n\n\nFind index\n\"hello\".find(\"lo\")\n3\n\n\nReplace\n\"2025-09-10\".replace(\"-\", \"/\")\n\"2025/09/10\"\n\n\nWildcard\nPattern \"ca*\" vs \"castle\"\nMatch\n\n\n\n\n\n\nWorked Example (Python)\n# Searching substrings\nsentence = \"the quick brown fox\"\nprint(\"quick\" in sentence)       # True\nprint(sentence.find(\"fox\"))      # 16\nprint(sentence.find(\"dog\"))      # -1 (not found)\n\n# Replacing text\ndate = \"2025-09-10\"\nprint(date.replace(\"-\", \"/\"))    # 2025/09/10\n\n# Simple wildcard matching (manual example)\ndef matches(pattern, text):\n    if pattern == \"*\":  # match everything\n        return True\n    if pattern.endswith(\"*\"):\n        return text.startswith(pattern[:-1])\n    if pattern.startswith(\"*\"):\n        return text.endswith(pattern[1:])\n    return pattern == text\n\nprint(matches(\"ca*\", \"cat\"))      # True\nprint(matches(\"ca*\", \"castle\"))   # True\nprint(matches(\"*ing\", \"running\")) # True\nprint(matches(\"*at\", \"cat\"))      # True\n\n\nWhy it matters\nSearching and replacing text is one of the most common tasks in programming. From finding a word in a document, to cleaning up user input, to formatting data for storage, these operations are essential. Wildcards introduce the idea of flexible matching, which prepares learners for regular expressions and more advanced pattern recognition.\n\n\nExercises\n\nCheck if the word \"sun\" appears in the sentence \"the sun sets\".\nFind the position of \"moon\" in \"the moon rises\".\nReplace \"morning\" with \"evening\" in \"good morning\".\nShow that searching for \"dog\" in \"hotdog\" succeeds.\nReplace all dashes in \"2025-09-10\" with slashes.\nWrite a rule with * that matches both \"cat\" and \"castle\".\nCreate a wildcard pattern that matches any word ending in \"ing\".\nExplain why a failed search must return a special value (not just 0).\nDemonstrate how searching for \"a\" in \"banana\" should return multiple matches.\nDescribe how wildcards can be extended into full regular expressions.\n\n\n\n\n3.4 L1 — Regular Expressions in Practice\nAt the intermediate level, string pattern matching expands from simple wildcards to regular expressions (regex). Regex provides a concise and powerful way to describe patterns in text. It is widely used in programming for searching, extracting, and validating structured data.\nA regular expression is a sequence of symbols that defines a search pattern. Unlike simple wildcards, regex can express detailed rules such as “find all numbers,” “match words ending in ing,” or “validate an email address.” Regex is supported in almost every major programming language.\n\nDeep Dive\n\nCore Symbols\n\n. matches any single character.\n* means zero or more repetitions.\n+ means one or more repetitions.\n? means zero or one occurrence.\n[abc] matches one character from the set a, b, or c.\n[0-9] matches any digit.\n^ matches the beginning of a string; $ matches the end.\n\n\n\nExamples of Patterns\n\n[0-9]+ matches sequences of digits like \"123\".\n\\w+ matches words made of letters, digits, and underscores.\n\\w+@\\w+\\.\\w+ matches simple email addresses.\n^Hello matches any string starting with \"Hello\".\n\n\n\nGreedy vs Lazy Matching\nRegex quantifiers (*, +) are greedy by default: they try to match as much as possible. Adding ? makes them lazy, matching the smallest possible part.\n\nGreedy: pattern \".*\" on \"abc\" → \"abc\".\nLazy: pattern \".*?\" on \"abc\" → \"a\".\n\n\n\nPractical Uses\nRegex is used for:\n\nExtracting structured data from logs.\nValidating input like emails or phone numbers.\nSearching and replacing complex text patterns.\nParsing configuration files or text protocols.\n\n\n\n\nWorked Example (Python)\nimport re\n\ntext = \"Order 123, item 456, email: alice@example.com\"\n\n# Match digits\nnumbers = re.findall(r\"[0-9]+\", text)\nprint(\"Numbers:\", numbers)  # ['123', '456']\n\n# Match words\nwords = re.findall(r\"\\w+\", text)\nprint(\"Words:\", words)\n\n# Match email addresses\nemails = re.findall(r\"\\w+@\\w+\\.\\w+\", text)\nprint(\"Emails:\", emails)\n\n# Anchors\nprint(bool(re.match(r\"^Order\", text)))   # True (starts with Order)\nprint(bool(re.search(r\"com$\", text)))   # True (ends with com)\n\n# Greedy vs lazy\nsample = \"&lt;tag&gt;content&lt;/tag&gt;&lt;tag&gt;other&lt;/tag&gt;\"\ngreedy = re.findall(r\"&lt;tag&gt;.*&lt;/tag&gt;\", sample)\nlazy   = re.findall(r\"&lt;tag&gt;.*?&lt;/tag&gt;\", sample)\nprint(\"Greedy:\", greedy)  # ['&lt;tag&gt;content&lt;/tag&gt;&lt;tag&gt;other&lt;/tag&gt;']\nprint(\"Lazy:\", lazy)      # ['&lt;tag&gt;content&lt;/tag&gt;', '&lt;tag&gt;other&lt;/tag&gt;']\n\n\nWhy it matters\nRegex provides a compact language for describing patterns in text. Without it, tasks like extracting emails, validating formats, or parsing logs would require many lines of manual code. Regex is also standardized across languages, so learning it once provides a universal tool. However, careless use can create unreadable code or performance problems, so disciplined practice is essential.\n\n\nExercises\n\nWrite a regex that matches any sequence of digits.\nMatch all words in \"The quick brown fox\".\nFind all substrings that start with \"a\" and end with \"e\".\nMatch an email address of the form \"name@domain.com\".\nMatch any string that begins with \"http://\" or \"https://\".\nExtract all numbers from \"Invoice: #123, #456, #789\".\nDemonstrate the difference between greedy and lazy matching with \"&lt;tag&gt;text&lt;/tag&gt;&lt;tag&gt;more&lt;/tag&gt;\".\nWrite a regex to validate phone numbers like \"123-456-7890\".\nExplain why using regex to parse complex formats (like HTML) can be unreliable.\nCreate a regex that matches words containing exactly three vowels.\n\n\n\n\n3.4 L2 — Engines & Optimizations\nRegular expressions are powerful, but at scale their performance and safety depend on the design of the engine. Advanced systems need regex engines that are predictable, fast, and safe against denial-of-service attacks. At this level, the focus is on internals, optimizations, and production libraries.\nRegex engines are implementations of pattern matching. Some use backtracking, exploring many paths but risking exponential slowdowns. Others use deterministic finite automata (DFA) or non-deterministic automata (NFA), which guarantee linear runtime but reduce flexibility. Modern libraries combine these with JIT compilation or SIMD acceleration for speed. Production systems must choose the right engine for correctness and reliability.\n\nDeep Dive\n\nEngine Architectures\nBacktracking engines (Perl, Python, PCRE) allow advanced features like backreferences but may run in exponential time on crafted inputs. DFA/NFA engines (RE2, Rust, grep) convert patterns to automata that run in linear time. JIT-based engines (Java, .NET) compile regex into machine code for faster execution.\n\n\nGreedy vs Lazy at Engine Level\nQuantifiers like * and + are greedy by default. Engines track multiple possibilities with stacks or state machines. Lazy quantifiers stop early, but the engine still evaluates alternatives internally.\n\n\nPerformance Optimizations\nCompiled regex patterns are faster than compiling on every use. Anchors (^, $) and character classes ([0-9]) reduce search space. Avoiding pathological expressions like (a+)+ prevents catastrophic backtracking. SIMD instructions scan text in chunks, speeding up low-level matching.\n\n\nAdvanced Libraries\n\nRE2 (Google): guarantees linear runtime, disallows constructs that cause exponential behavior.\nHyperscan (Intel): supports multi-pattern regex with SIMD acceleration, used in intrusion detection.\nICU: supports Unicode-aware regex and locale-sensitive operations.\n\n\n\nSystem-Level Integration\nRegex engines are embedded in compilers, text editors, search tools, and security systems. Multi-pattern matching is essential for scanning logs, detecting spam, and monitoring network traffic.\n\n\nSecurity Concerns\nPoorly written regex can create ReDoS (Regex Denial of Service). Attackers send inputs that trigger exponential backtracking, freezing systems. For example, pattern (a+)+b against \"aaaaaaa…\" takes excessive time. Safe libraries (RE2, Hyperscan) prevent this by design.\n\n\n\n\n\n\n\n\nEngine Type\nStrengths\nWeaknesses\n\n\n\n\nBacktracking\nFull regex features, flexible\nRisk of exponential runtime (ReDoS)\n\n\nDFA/NFA\nLinear runtime, predictable\nNo backreferences, fewer features\n\n\nJIT Hybrid\nVery fast, compiled to machine code\nStill vulnerable to bad patterns\n\n\n\n\n\n\nWorked Example (Python)\nimport re, time\n\n# Backtracking danger: catastrophic regex\npattern = re.compile(r\"(a+)+b\")\ntext = \"a\" * 25 + \"b\"   # works\nprint(\"Match:\", bool(pattern.match(text)))\n\n# Problematic input: missing final 'b'\ntext_bad = \"a\" * 25     # exponential backtracking\nstart = time.time()\nprint(\"Bad match:\", bool(pattern.match(text_bad)))\nprint(\"Time:\", round(time.time() - start, 4), \"seconds\")\n\n# Greedy vs lazy\nhtml = \"&lt;tag&gt;first&lt;/tag&gt;&lt;tag&gt;second&lt;/tag&gt;\"\ngreedy = re.findall(r\"&lt;tag&gt;.*&lt;/tag&gt;\", html)\nlazy   = re.findall(r\"&lt;tag&gt;.*?&lt;/tag&gt;\", html)\nprint(\"Greedy:\", greedy)\nprint(\"Lazy:\", lazy)\n\n# Precompiled pattern efficiency\nwords = \"error warning info debug error warning\".split()\np = re.compile(r\"error|warning\")\nmatches = [w for w in words if p.match(w)]\nprint(\"Matches:\", matches)\n\n# Unicode-aware regex\ntext = \"café resume résumé\"\nutf8_words = re.findall(r\"\\w+\", text)\nprint(\"UTF-8 words:\", utf8_words)\n\n# Simulated natural sorting with regex\nfiles = [\"file1\", \"file10\", \"file2\"]\nsplitter = re.compile(r'(\\d+)')\nfiles.sort(key=lambda x: [int(n) if n.isdigit() else n for n in splitter.split(x)])\nprint(\"Natural sort:\", files)\n\n\nWhy it matters\nRegex is a double-edged sword. It compresses complex logic into concise expressions, but the wrong engine or pattern can cripple systems. Backtracking engines offer flexibility but can be exploited. DFA-based engines guarantee performance but restrict features. Production libraries like RE2 and Hyperscan provide safe, optimized solutions for real workloads. Security, speed, and predictability all depend on understanding regex internals.\n\n\nExercises\n\nExplain the difference between greedy and lazy matching.\nShow why (a+)+b can cause problems on inputs like \"aaaaaa\".\nCompare backtracking and DFA approaches to regex.\nWrite a regex to match numbers at the start of a string (^123).\nExplain why precompiling a regex can improve performance.\nDescribe how RE2 avoids catastrophic backtracking.\nShow why regex should not be used for parsing HTML.\nDesign a regex for matching IPv4 addresses and explain performance concerns.\nCompare the runtime of regex matching with and without SIMD acceleration in theory.\nPropose a strategy to defend against Regex Denial of Service in a production system.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#applications",
    "href": "chapter_3.html#applications",
    "title": "Chapter 3. Strings",
    "section": "3.5 Applications",
    "text": "3.5 Applications\n\n3.5 L0 — Everyday Uses\nStrings are used in almost every program. Beginners usually encounter them when printing messages, saving text to files, or checking simple inputs. These small applications show how strings connect code to real-world tasks.\nEveryday string applications include formatting output, working with text files, validating simple input, and combining small pieces of data into readable results. These tasks look simple but form the foundation of real-world software, from command-line tools to web apps.\n\nDeep Dive\n\nText Formatting\nPrograms must often display results with both words and numbers. Concatenation creates readable sentences: \"Hello, \" + name. Many languages also support templates: \"Hello, {name}\".\n\n\nFile Reading Basics\nA file is just text stored on disk. Reading one line returns a string. Splitting turns the line into words. Writing strings saves output. Example: reading \"Alice 25\" and splitting it into a name and an age.\n\n\nInput Validation (Simple)\nNot all inputs are valid. Checking that a string is not empty prevents errors. Minimum length ensures requirements are met (like passwords). Character checks confirm only letters or digits are entered.\n\n\nBasic Data Handling\nStrings often need simple processing: counting characters, trimming spaces, or extracting parts. \"John Doe\" can be split into \"John\" and \"Doe\". Small pieces of text are often joined into a readable result.\n\n\n\nEveryday Task\nExample Input\nExample Output\n\n\n\n\nGreeting\n\"Alice\"\n\"Hello, Alice!\"\n\n\nTrim spaces\n\" hello \"\n\"hello\"\n\n\nCount characters\n\"banana\"\n6\n\n\nSave and load message\n\"Note: call Bob\"\nWritten and reloaded\n\n\n\n\n\n\nWorked Example (Python)\n# Formatting text\nname = \"Alice\"\nage = 25\nprint(f\"Hello, {name}! You are {age} years old.\")\n\n# File basics (write then read)\nwith open(\"note.txt\", \"w\") as f:\n    f.write(\"Remember to buy milk\")\n\nwith open(\"note.txt\", \"r\") as f:\n    content = f.read()\nprint(\"File content:\", content)\n\n# Input validation\nuser_input = \"   hello123   \"\ncleaned = user_input.strip()\nif cleaned and cleaned.isalnum():\n    print(\"Valid input:\", cleaned)\nelse:\n    print(\"Invalid input\")\n\n# Data handling\nfull_name = \"John Doe\"\nfirst, last = full_name.split()\nprint(\"First name:\", first)\nprint(\"Last name:\", last)\n\n# Joining pieces\nwords = [\"data\", \"structures\", \"are\", \"fun\"]\nsentence = \" \".join(words)\nprint(sentence)\n\n\nWhy it matters\nEveryday string handling bridges user interaction and program logic. Without formatting, programs would only output raw numbers. Without validation, programs would accept broken or unsafe input. Without file reading and writing, no program could save notes or logs. These basics show how strings connect software to people and data.\n\n\nExercises\n\nPrint \"Hello, NAME!\" using a variable for the name.\nCount the number of characters in \"banana\".\nSave the string \"study algorithms\" to a file, then read it back.\nRemove extra spaces from \"   world   \" and print the cleaned string.\nCheck if \"abc123\" contains only letters and digits.\nSplit \"Alice Johnson\" into first and last names.\nJoin [\"red\", \"green\", \"blue\"] into \"red,green,blue\".\nCreate a greeting that includes both a name and an age.\nValidate that a password is at least 8 characters long.\nWrite a program that asks for a note, saves it to a file, and then reloads and prints it.\n\n\n\n\n3.5 L1 — Engineering Contexts\nAs programs grow, string handling shifts from small tasks to structured data and system integration. Strings become the glue for configurations, logs, and filtering. These applications demand clarity, consistency, and efficiency.\n\nOverview/Definition\nStrings in engineering contexts are used to represent structured formats, record system activity, and enable searching and filtering. They provide a common medium for both humans and machines, balancing readability with machine-parsable structure.\n\n\nDeep Dive\n\nConfiguration Formats\nConfiguration files define how systems run. Common formats like JSON, YAML, or INI are all string-based. Parsing these strings gives structured values. For example, \"timeout\": 30 in JSON is a string representation of a setting, later converted into a number.\n\n\nLogs and Monitoring\nSystems record events as text logs. A log line might be \"2025-09-10 12:00:01 ERROR Connection failed\". These strings are later parsed for monitoring and debugging. Consistency in string format is critical.\n\n\nSearching and Filtering\nLarge systems must sift through text quickly. Searching log files for \"ERROR\" highlights problems. Filtering configuration values based on keywords is another common task. Case-insensitive matching or regex-based extraction makes this robust.\n\n\nData Exchange\nAPIs and network protocols often transmit structured text. JSON payloads are strings at the boundary between systems. Converting between string representation and structured data is a common responsibility of engineering code.\n\n\n\n\n\n\n\n\nContext\nExample String\nPurpose\n\n\n\n\nConfiguration\n{\"host\":\"localhost\",\"port\":8080}\nSystem settings\n\n\nLog line\n2025-09-10 ERROR Disk full\nMonitoring & debugging\n\n\nSearch/filter\nFind \"ERROR\" in a log file\nIdentify issues\n\n\nAPI payload\n{\"user\":\"alice\",\"id\":42}\nData exchange between services\n\n\n\n\n\n\nWorked Example (Python)\nimport json\nimport re\n\n# Configuration parsing (JSON as string)\nconfig_str = '{\"host\":\"localhost\",\"port\":8080}'\nconfig = json.loads(config_str)\nprint(\"Host:\", config[\"host\"], \"Port:\", config[\"port\"])\n\n# Logging and filtering\nlogs = [\n    \"2025-09-10 12:00:01 INFO  Server started\",\n    \"2025-09-10 12:01:22 ERROR Connection failed\",\n    \"2025-09-10 12:02:45 WARN  Disk almost full\"\n]\nerrors = [line for line in logs if \"ERROR\" in line]\nprint(\"Error logs:\", errors)\n\n# Case-insensitive search\nquery = \"warn\"\nmatches = [line for line in logs if query.lower() in line.lower()]\nprint(\"Warnings:\", matches)\n\n# Extracting structured data from logs with regex\npattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})\\s+\\S+\\s+(ERROR|WARN|INFO)\")\nfor log in logs:\n    match = pattern.search(log)\n    if match:\n        date, level = match.groups()\n        print(f\"Date: {date}, Level: {level}\")\n\n\nWhy it matters\nStrings in configurations allow flexible system tuning without recompiling. Logs provide the primary record of system behavior and failures. Searching and filtering turn raw logs into actionable insights. APIs and protocols rely on structured strings to communicate. Mastery of these applications makes code more reliable and systems more transparent.\n\n\nExercises\n\nParse a JSON string that contains a username and print the value.\nFind all log lines that contain \"ERROR\".\nPerform a case-insensitive search for \"warn\" in a list of log lines.\nExtract the date from a log entry of the form \"YYYY-MM-DD ...\".\nParse a configuration string to extract host and port values.\nWrite a filter that selects only \"INFO\" level log lines.\nConvert a dictionary of settings into a JSON string.\nExplain why consistent formatting in logs is important for monitoring systems.\nCreate a regex that extracts both the date and log level from a log entry.\nDesign a simple log query: given a list of log strings and a keyword, return all lines that match the keyword.\n\n\n\n\n3.5 L2 — Large-Scale Systems\nWhen systems handle massive amounts of text, strings are no longer just for messages, logs, or configs. They become the raw material of search engines, data compression, natural language processing, and security enforcement. At this scale, efficiency, correctness, and safety are critical.\nStrings in large-scale systems must be stored compactly, searched efficiently, processed for meaning, and sanitized for safety. Specialized algorithms and libraries turn raw text into structured, searchable, and secure data.\n\nDeep Dive\n\nSearch Engines\nFull-text search systems use inverted indexes: instead of storing text linearly, they map each word to the documents it appears in. Searching \"cat\" means looking up \"cat\" in the index rather than scanning every file. Ranking algorithms (like TF-IDF or BM25) then score relevance.\n\n\nData Compression\nStrings consume storage. Compression algorithms like Huffman coding, gzip, or dictionary-based schemes reduce size. For example, \"banana banana banana\" can be compressed by storing \"banana\" once and pointing to it three times. Compression reduces storage costs and speeds up network transfer.\n\n\nNatural Language Processing (NLP)\nRaw strings must be tokenized (split into words), normalized (case folding, stemming), and sometimes embedded into vectors. \"running\" → \"run\". Preprocessing ensures machine learning models treat text consistently.\n\n\nSecurity\nStrings are also attack vectors. SQL injection, XSS, and command injection happen when untrusted strings are executed as code. Escaping or sanitizing input prevents this. Unicode tricks (confusables, hidden nulls) are used in phishing and bypass attacks. Secure systems enforce normalization and validation before processing.\n\n\n\n\n\n\n\n\nDomain\nString Role\nTechniques\n\n\n\n\nSearch engine\nFind text in billions of docs\nInverted index, ranking\n\n\nCompression\nReduce text storage and transfer cost\nHuffman, gzip, dictionary\n\n\nNLP\nPrepare text for analysis\nTokenization, stemming\n\n\nSecurity\nPrevent malicious string usage\nEscaping, validation\n\n\n\n\n\n\nWorked Example (Python)\nimport re\nimport zlib\nfrom collections import defaultdict\n\n# --- Search Engine Simulation ---\ndocuments = {\n    1: \"the quick brown fox\",\n    2: \"the lazy dog\",\n    3: \"the fox jumped over the dog\"\n}\n\n# Build inverted index\nindex = defaultdict(list)\nfor doc_id, text in documents.items():\n    for word in text.split():\n        index[word].append(doc_id)\n\nprint(\"Index for 'fox':\", index[\"fox\"])  # [1, 3]\n\n# --- Compression Example ---\ntext = \"banana banana banana\"\ncompressed = zlib.compress(text.encode(\"utf-8\"))\ndecompressed = zlib.decompress(compressed).decode(\"utf-8\")\nprint(\"Original:\", len(text), \"bytes\")\nprint(\"Compressed:\", len(compressed), \"bytes\")\nprint(\"Decompressed:\", decompressed)\n\n# --- NLP Tokenization & Normalization ---\nraw = \"Running runs runner\"\ntokens = raw.lower().split()\nstems = [re.sub(r\"(ing|s|er)$\", \"\", t) for t in tokens]  # naive stemming\nprint(\"Tokens:\", tokens)\nprint(\"Stems:\", stems)\n\n# --- Security Example ---\nunsafe_input = \"'; DROP TABLE users; --\"\nsafe_query = \"SELECT * FROM accounts WHERE name = ?\"\nprint(\"Unsafe input:\", unsafe_input)\nprint(\"Safe query template:\", safe_query)\n\n\nWhy it matters\nLarge-scale string handling powers web search, messaging apps, compression in files and networks, and text-based AI systems. Without inverted indexes, search engines would be unusably slow. Without compression, storage and bandwidth would be wasteful. Without normalization, machine learning models would misinterpret text. Without security checks, attackers could exploit untrusted strings to damage systems. Strings at scale are the backbone of modern computing.\n\n\nExercises\n\nExplain how an inverted index speeds up searching compared to scanning all documents.\nShow how compression reduces storage for repetitive strings like \"abc abc abc\".\nTokenize \"Cats are running\" into lowercase words.\nNormalize \"Running\" into its root form \"run\".\nDescribe why \"café\" and \"cafe\" should be normalized before search.\nGive an example of SQL injection using a string input.\nExplain how escaping special characters prevents injection attacks.\nCompare the storage savings between raw and compressed text for a long repeated string.\nPropose how to detect Unicode confusables in user names.\nDesign a small experiment to measure search speed with and without an inverted index.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#deep-dive-15",
    "href": "chapter_3.html#deep-dive-15",
    "title": "Chapter 3. Strings",
    "section": "Deep Dive",
    "text": "Deep Dive\n\n3.1 Representation\n\nL0:\n\nImmutability in everyday languages (intuitive).\nIndexing and slicing basics.\nConcatenation and repetition.\nASCII vs Unicode (conceptual).\n\nL1:\n\nImmutability and its performance costs.\nString interning and memory reuse.\nEncoding differences: UTF-8, UTF-16, UTF-32.\nSlicing behavior across languages (Python vs Java vs C++).\nEfficient concatenation (join, builders, buffers).\n\nL2:\n\nLanguage-specific internals: C char*, C++ std::string + SSO, Python PEP 393, Java String, Go slices, Rust String/&str.\nOS-level considerations: heap vs stack, fragmentation, system call boundaries.\nHardware-level: cache alignment, SIMD optimization in libc.\nUnicode normalization (NFC, NFD, case folding).\nSecurity issues: confusables, null injection.\nICU, RE2, Hyperscan as production-grade libraries.\n\n\n\n\n3.2 Operations\n\nL0:\n\nCase conversion (upper, lower).\nTrimming whitespace.\nBasic search (find, in).\nSplitting and joining.\nSimple string formatting.\n\nL1:\n\nEfficient searching (startswith, endswith).\nCounting substrings and replacing.\nRegular expression basics (., *, +, []).\nParsing structured text (CSV, JSON).\nPerformance pitfalls (naive concatenation, multiple scans).\n\nL2:\n\nString search algorithms: Naive, KMP, Boyer–Moore.\nRegex engine internals (backtracking vs DFA/NFA).\nTokenization in compilers (lexical analysis with automata).\nUnicode-aware searching and case folding.\nSystem boundary handling (sockets, files, encodings).\nSIMD-accelerated substring search.\n\n\n\n\n3.3 Comparison\n\nL0:\n\nEquality checks (case-sensitive, whitespace).\nLexicographic ordering.\nSorting by dictionary order.\n\nL1:\n\nCase-insensitive comparisons.\nLocale-aware collation (German vs Swedish).\nNatural sorting with numbers (file2 &lt; file10).\nDatabase collations (CI/CS, AI/AS).\n\nL2:\n\nUnicode normalization for equality.\nCase folding vs lowercase.\nICU collation internals.\nSystem-level optimizations (memcmp, short-circuit).\nCache-aware comparison for large text.\nDetecting and handling Unicode confusables.\n\n\n\n\n3.4 Patterns\n\nL0:\n\nSimple substring search.\nReplace operations.\nWildcards (*, ?) as intuitive pattern matching.\n\nL1:\n\nRegex syntax: quantifiers, anchors, character classes.\nGreedy vs lazy matching.\nRegex for input validation (emails, phones).\nExtracting structured data from logs.\n\nL2:\n\nRegex engine architectures: backtracking, DFA/NFA, hybrid JIT.\nPerformance optimizations (precompiled regex, anchors, SIMD).\nProduction libraries: RE2, Hyperscan, ICU.\nSystem integration: compilers, search tools, IDS.\nSecurity: catastrophic backtracking, ReDoS.\n\n\n\n\n3.5 Applications\n\nL0:\n\nSimple greetings and message formatting.\nReading and writing small text files.\nInput validation (non-empty, length, alphanumeric).\nJoining/extracting small strings.\n\nL1:\n\nConfigurations (INI, JSON, YAML).\nLogs and monitoring pipelines.\nSearching and filtering text data.\nAPIs and network payloads as strings.\n\nL2:\n\nSearch engines with inverted indexes.\nText compression (Huffman, gzip, dictionary).\nNatural language preprocessing (tokenization, stemming).\nSecurity concerns: injections, normalization, Unicode attacks.\nHigh-scale performance engineering for string-heavy systems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab",
    "href": "chapter_3.html#lab",
    "title": "Chapter 3. Strings",
    "section": "LAB",
    "text": "LAB\n\n3.1 Representation\n\nLAB 1: String Encodings in Practice\n\nRepresent the same text (\"A😊\") in UTF-8, UTF-16, and UTF-32.\nMeasure memory size in each encoding.\nShow how slicing works differently in each.\n\nLAB 2: String Interning and Memory Efficiency\n\nCreate repeated strings with and without interning.\nMeasure memory usage.\nDiscuss practical use cases (symbol tables, compilers).\n\nLAB 3: Kernel-Level String Operations\n\nExplore C char* strings with strlen and buffer overflows.\nCompare to higher-level language safety (Python, Java).\n\n\n\n\n3.2 Operations\n\nLAB 4: Concatenation Cost Benchmark\n\nCompare loop concatenation (+) vs buffered approaches (join, builders).\nMeasure time for small, medium, and large inputs.\n\nLAB 5: Regex Basics Explorer\n\nImplement small regex patterns ([0-9]+, \\w+) across different languages.\nShow differences in syntax and output.\n\nLAB 6: CSV Parsing Pitfalls\n\nTry splitting \"apple,\"banana,grape\",cherry\" naively.\nShow incorrect results.\nContrast with proper CSV parsers.\n\n\n\n\n3.3 Comparison\n\nLAB 7: Locale-Aware Sorting\n\nSort a list with \"ä\", \"z\", \"apple\".\nShow German vs Swedish locale differences.\n\nLAB 8: Unicode Normalization Demo\n\nCompare \"café\" vs \"cafe\\u0301\".\nShow why normalization (NFC, NFD) matters for equality.\n\nLAB 9: Confusable Characters Security Check\n\nCompare Latin \"a\" vs Cyrillic \"а\".\nBuild a detector for confusable Unicode characters.\n\n\n\n\n3.4 Patterns\n\nLAB 10: Substring Search Algorithms\n\nImplement Naive, KMP, and Boyer–Moore search.\nBenchmark them on long repetitive text.\n\nLAB 11: Regex Engine Catastrophe\n\nUse (a+)+b on long input without a b.\nMeasure how exponential backtracking freezes the engine.\nContrast with RE2 (linear-time safe).\n\nLAB 12: Greedy vs Lazy Matching\n\nRun regex \"&lt;tag&gt;.*&lt;/tag&gt;\" vs \"&lt;tag&gt;.*?&lt;/tag&gt;\" on HTML-like text.\nShow difference between greedy and lazy quantifiers.\n\n\n\n\n3.5 Applications\n\nLAB 13: Build a Mini Inverted Index\n\nTake a small set of documents.\nBuild an index mapping words to documents.\nQuery the index for \"fox\", \"dog\", etc.\n\nLAB 14: Compression of Repetitive Strings\n\nImplement a simple dictionary compressor.\nCompare original vs compressed sizes for \"banana banana banana\".\n\nLAB 15: Injection Attack Simulation\n\nBuild a simple query string with unsanitized input.\nShow how malicious input (\"'; DROP TABLE users; --\") alters behavior.\nFix it with escaping or parameter binding.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-1-string-encodings-in-practice",
    "href": "chapter_3.html#lab-1-string-encodings-in-practice",
    "title": "Chapter 3. Strings",
    "section": "LAB 1: String Encodings in Practice",
    "text": "LAB 1: String Encodings in Practice\n\nGoal\nUnderstand how different encodings (UTF-8, UTF-16, UTF-32) represent the same text. Learn how memory size changes depending on encoding, and why this matters for storage and transmission.\n\n\nSetup\nYou can use any language that provides access to string encodings (Python, Go, C, Java). Python is a good choice because its encode() method supports multiple encodings.\nText sample:\n\"A😊\"\nThis combines a simple ASCII character (A) with a multi-byte emoji (😊).\n\n\nStep-by-Step\n\nRepresent the string in memory\n\nCreate the string \"A😊\".\nPrint its length in characters.\nPrint its length in bytes when encoded in UTF-8, UTF-16, and UTF-32.\n\nInspect the raw byte sequences\n\nEncode the string in each format.\nPrint the resulting byte arrays.\nWrite them in hexadecimal for clarity.\n\nCompare differences\n\nNotice that UTF-8 is variable-length (1 byte for A, 4 bytes for 😊).\nUTF-16 uses 2 bytes for A, 4 bytes for 😊 (surrogate pair).\nUTF-32 uses 4 bytes for both.\n\nReflect on trade-offs\n\nUTF-8 is compact for ASCII-heavy text (English).\nUTF-16 is a balance for languages with many non-ASCII characters.\nUTF-32 is simple but wastes space.\n\n\n\n\nExample (Python)\ntext = \"A😊\"\nprint(\"Characters:\", len(text))\n\n# UTF-8\nutf8_bytes = text.encode(\"utf-8\")\nprint(\"UTF-8 length:\", len(utf8_bytes), \"bytes\")\nprint(\"UTF-8 bytes:\", utf8_bytes.hex())\n\n# UTF-16\nutf16_bytes = text.encode(\"utf-16\")\nprint(\"UTF-16 length:\", len(utf16_bytes), \"bytes\")\nprint(\"UTF-16 bytes:\", utf16_bytes.hex())\n\n# UTF-32\nutf32_bytes = text.encode(\"utf-32\")\nprint(\"UTF-32 length:\", len(utf32_bytes), \"bytes\")\nprint(\"UTF-32 bytes:\", utf32_bytes.hex())\nExpected output (approximate, may differ by platform endianness):\nCharacters: 2\nUTF-8 length: 5 bytes\nUTF-8 bytes: 41f09f988a\nUTF-16 length: 6 bytes\nUTF-16 bytes: fffe41003dd89a\nUTF-32 length: 8 bytes\nUTF-32 bytes: fffe0000410001f60a\n\n\nExpected Results\n\n\n\nEncoding\nBytes for \"A\"\nBytes for \"😊\"\nTotal Bytes\n\n\n\n\nUTF-8\n1\n4\n5\n\n\nUTF-16\n2\n4 (surrogate)\n6\n\n\nUTF-32\n4\n4\n8\n\n\n\n\n\nWhy it matters\n\nEncoding affects storage cost and speed.\nA file with millions of ASCII characters will be 2× bigger in UTF-16 than UTF-8.\nEmojis or non-Latin scripts can make UTF-8 larger than UTF-16.\nChoosing the wrong encoding can waste storage, slow down transmission, or even corrupt data if misinterpreted.\n\n\n\nExercises\n\nEncode the string \"Hello\" in UTF-8, UTF-16, UTF-32. Compare sizes.\nRepeat with \"こんにちは\" (Japanese). Which encoding is most efficient?\nInspect the raw bytes of \"💡\" and explain why it requires surrogate pairs in UTF-16.\nWrite a program that takes any string and reports its size in multiple encodings.\nExplain why UTF-32 is rarely used for storage despite its simplicity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-2-string-interning-and-memory-efficiency",
    "href": "chapter_3.html#lab-2-string-interning-and-memory-efficiency",
    "title": "Chapter 3. Strings",
    "section": "LAB 2: String Interning and Memory Efficiency",
    "text": "LAB 2: String Interning and Memory Efficiency\n\nGoal\nUnderstand how string interning reduces memory usage by reusing identical string objects. Learn when interning happens automatically, when it must be explicit, and why it matters in systems with many repeated strings.\n\n\nSetup\nUse a language that supports string interning (Python, Java, C#, Go with symbol tables, etc.). We’ll use Python because it demonstrates both automatic interning (for short literals) and manual interning (sys.intern()).\nTest dataset: a large list of repeated keys (e.g., \"user123\", \"error\", \"warning\").\n\n\nStep-by-Step\n\nAutomatic Interning of Literals\n\nDefine two identical string literals.\nCompare them with == (value equality) and is (identity).\nObserve that they may point to the same object in memory.\n\nNon-Interned Strings\n\nBuild identical strings at runtime (e.g., \"ab\" + \"cd\").\nCheck identity with is.\nThey may not be the same object, even if values match.\n\nExplicit Interning\n\nUse sys.intern() on repeated dynamic strings.\nCompare memory addresses before and after.\n\nMemory Benchmark\n\nCreate a list of repeated values (e.g., 100k \"error\" strings).\nMeasure memory usage with and without interning.\n\nDiscussion\n\nInterning saves memory by keeping one copy of repeated values.\nIt speeds up dictionary lookups (symbols, keywords, tokens).\nBut it increases memory if used on unique or rarely repeated strings.\n\n\n\n\nExample (Python)\nimport sys\n\n# 1. Automatic interning\na = \"hello\"\nb = \"hello\"\nprint(\"a == b:\", a == b)   # True (values equal)\nprint(\"a is b:\", a is b)   # True (same object due to interning)\n\n# 2. Non-interned strings\nx = \"\".join([\"he\", \"llo\"])\ny = \"\".join([\"he\", \"llo\"])\nprint(\"x == y:\", x == y)   # True (values equal)\nprint(\"x is y:\", x is y)   # False (different objects)\n\n# 3. Manual interning\nx_interned = sys.intern(x)\ny_interned = sys.intern(y)\nprint(\"x_interned is y_interned:\", x_interned is y_interned)  # True\n\n# 4. Memory efficiency demo\nwords = [\"error\"] * 100000\nunique_words = [sys.intern(\"error\") for _ in range(100000)]\nprint(\"Normal list length:\", len(words))\nprint(\"Interned list length:\", len(unique_words))\nprint(\"Normal memory ids (first 3):\", [id(w) for w in words[:3]])\nprint(\"Interned memory ids (first 3):\", [id(w) for w in unique_words[:3]])\n\n\nExpected Results\n\nString literals often share the same memory address.\nDynamically built identical strings are separate objects unless interned.\nInterned strings reuse the same memory, reducing duplication.\n\n\n\n\n\n\n\n\n\nCase\nIdentity (is)\nMemory effect\n\n\n\n\n\"hello\" vs \"hello\"\nTrue\nInterned automatically.\n\n\n\"\".join([\"he\",\"llo\"])\nFalse\nDifferent objects in memory.\n\n\nsys.intern() applied\nTrue\nBoth point to one shared object.\n\n\n\n\n\nWhy it matters\n\nCompilers and interpreters use interning for keywords, identifiers, and symbols.\nDatabases and search engines save memory when handling millions of repeated strings.\nPerformance improves because identity comparison (is) is faster than value comparison.\nOveruse of interning on unique strings wastes memory and CPU cycles.\n\n\n\nExercises\n\nCreate two identical literals and prove they point to the same memory location.\nBuild two identical strings dynamically and show they are different objects.\nApply interning to the dynamic strings and prove they now share memory.\nCreate 10,000 repeated \"apple\" strings with and without interning. Compare identity checks.\nExplain why interning improves dictionary lookups.\nShow an example where interning increases memory use (hint: many unique strings).\nCompare interned vs non-interned strings in terms of lookup speed in a dictionary of 100k keys.\nDescribe how interning is used in language runtimes (e.g., symbol tables in compilers).\nPropose a scenario in large-scale systems (like log processing) where interning saves significant memory.\nExplain why interning must be combined with garbage collection to avoid memory leaks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-3-kernel-level-string-operations",
    "href": "chapter_3.html#lab-3-kernel-level-string-operations",
    "title": "Chapter 3. Strings",
    "section": "LAB 3: Kernel-Level String Operations",
    "text": "LAB 3: Kernel-Level String Operations\n\nGoal\nExplore how strings are represented at the C level using char*. Learn how null terminators ('\\0') define string boundaries, how buffer overflows occur, and why higher-level languages add safety.\n\n\nSetup\nThis lab uses C (or C-like pseudocode). It can be run with gcc or clang. We’ll demonstrate:\n\nHow C stores strings in memory.\nWhat happens when the null terminator is missing.\nWhy functions like strcpy can cause buffer overflows.\n\n\n\nStep-by-Step\n\nString Representation in C\n\nCreate a C string with char s[] = \"hello\";.\nPrint its characters in a loop.\nShow that memory includes '\\0' at the end.\n\nEffect of Null Terminator\n\nCreate a buffer without '\\0'.\nPass it to printf(\"%s\").\nObserve how the function reads past the intended end, printing garbage or crashing.\n\nBuffer Overflow Risk\n\nCreate a small buffer (e.g., 5 bytes).\nCopy a larger string into it with strcpy.\nShow that memory beyond the buffer is overwritten.\n\nSafer Alternatives\n\nReplace strcpy with strncpy or snprintf.\nShow how they limit copies and prevent overflows.\n\n\n\n\nExample (C)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nint main() {\n    // 1. String representation\n    char s[] = \"hello\";\n    printf(\"String: %s\\n\", s);\n    for (int i = 0; i &lt; sizeof(s); i++) {\n        printf(\"Byte %d: %d\\n\", i, s[i]);\n    }\n\n    // 2. Missing null terminator\n    char bad[5] = {'h','e','l','l','o'}; // no '\\0'\n    printf(\"Bad string (no null): %s\\n\", bad); // prints garbage\n\n    // 3. Buffer overflow\n    char small[5];\n    strcpy(small, \"overflow!\"); // too long, unsafe\n    printf(\"Overflowed string: %s\\n\", small);\n\n    // 4. Safe alternative\n    char safe[5];\n    strncpy(safe, \"safe\", sizeof(safe) - 1);\n    safe[sizeof(safe) - 1] = '\\0'; // ensure null terminator\n    printf(\"Safe string: %s\\n\", safe);\n\n    return 0;\n}\n\n\nExpected Results\n\nThe string \"hello\" occupies 6 bytes: 5 characters + 1 null terminator.\nWithout '\\0', printf continues reading memory until it finds a random null.\nstrcpy into a small buffer overwrites adjacent memory, causing corruption or crashes.\nstrncpy keeps the buffer safe by limiting copied characters.\n\n\n\n\nCase\nBehavior\n\n\n\n\nWith '\\0'\nPrints correctly.\n\n\nWithout '\\0'\nPrints garbage or crashes.\n\n\nOverflow with strcpy\nMemory corruption.\n\n\nSafe with strncpy\nCorrect, bounded copy.\n\n\n\n\n\nWhy it matters\n\nC-level strings are unsafe by default. The programmer must manage termination and bounds.\nBuffer overflows are a leading cause of security vulnerabilities.\nKernel code, device drivers, and embedded systems often rely on raw C strings.\nHigher-level languages (Python, Java, Go, Rust) wrap strings in safe abstractions, preventing these bugs.\n\n\n\nExercises\n\nCreate a C string and print its bytes, showing the null terminator.\nRemove the null terminator from a string and observe the output.\nCopy \"abcdefgh\" into a buffer of size 5 and note the result.\nRewrite the unsafe copy using strncpy or snprintf.\nExplain why strncpy may leave out the null terminator if not handled carefully.\nInvestigate what happens if you forget to set safe[sizeof(safe)-1] = '\\0'.\nCompare memory usage of \"hello\" vs \"hello world\".\nExplain why C strings make functions like strlen O(n) instead of O(1).\nShow how buffer overflows can alter unrelated variables in memory.\nDiscuss how Rust or Go prevents these issues by design.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-4-concatenation-cost-benchmark",
    "href": "chapter_3.html#lab-4-concatenation-cost-benchmark",
    "title": "Chapter 3. Strings",
    "section": "LAB 4: Concatenation Cost Benchmark",
    "text": "LAB 4: Concatenation Cost Benchmark\n\nGoal\nMeasure the cost of string concatenation when building large strings. Understand why naive approaches (+ in loops) are inefficient, and how buffered approaches (join, builders, buffers) improve performance.\n\n\nSetup\nThis lab can be done in Python, Java, Go, or C++. Each has different idioms:\n\nPython → + vs \"\".join(list)\nJava → + vs StringBuilder\nGo → += vs strings.Builder\nC++ → + vs std::ostringstream\n\nWe’ll use Python as the demonstration language.\n\n\nStep-by-Step\n\nNaive Concatenation in Loop\n\nStart with an empty string.\nAppend characters one by one with +.\nMeasure execution time.\n\nBuffered Concatenation with List + Join\n\nStore characters in a list.\nUse \"\".join(list) to build the final string once.\nMeasure execution time.\n\nCompare Results\n\nShow how loop concatenation scales poorly (O(n²)).\nShow how buffered concatenation scales linearly (O(n)).\n\nExperiment with Sizes\n\nRun benchmarks for N = 10³, 10⁴, 10⁵ characters.\nRecord execution times.\n\nDiscussion\n\nNaive concatenation reallocates and copies strings repeatedly.\nBuffered methods allocate memory once and fill it efficiently.\nSimilar patterns exist across languages.\n\n\n\n\nExample (Python)\nimport time\n\ndef naive_concat(n):\n    s = \"\"\n    for i in range(n):\n        s += \"x\"\n    return s\n\ndef join_concat(n):\n    parts = []\n    for i in range(n):\n        parts.append(\"x\")\n    return \"\".join(parts)\n\nfor N in [1000, 10000, 50000]:\n    start = time.time()\n    naive_concat(N)\n    print(f\"Naive concat N={N}: {round(time.time() - start, 4)}s\")\n\n    start = time.time()\n    join_concat(N)\n    print(f\"Join concat N={N}: {round(time.time() - start, 4)}s\")\n    print(\"---\")\n\n\nExpected Results\n\nFor small N (1000), both methods are similar.\nFor larger N (50k+), naive concatenation slows dramatically.\n\n\n\n\nN\nNaive + (s)\nJoin (s)\n\n\n\n\n1,000\n~0.002\n~0.001\n\n\n10,000\n~0.15\n~0.01\n\n\n50,000\n~3.0+\n~0.05\n\n\n\n\n\nWhy it matters\n\nText-heavy applications (logs, report generation, data pipelines) must build large strings efficiently.\nUsing the wrong method can multiply runtime by 100× or more.\nHigh-level languages hide memory allocation, but understanding efficiency prevents hidden bottlenecks.\nSystems languages (C++, Rust, Go) provide explicit builders to avoid repeated allocations.\n\n\n\nExercises\n\nBenchmark string concatenation with + for N = 1000, 10,000, and 100,000.\nBenchmark buffered concatenation (join, StringBuilder, or equivalent).\nPlot execution time against N for both methods.\nExplain why naive concatenation is O(n²).\nDemonstrate that small strings (N &lt; 100) show no noticeable difference.\nCompare Python join with Go strings.Builder.\nCreate a function that takes a list of words and joins them with spaces efficiently.\nShow how repeated concatenation can impact log processing in a simulated workload.\nExtend the benchmark to include Unicode characters like \"😊\".\nPropose an optimization strategy for building multi-megabyte strings in a web server.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-5-regex-basics-explorer",
    "href": "chapter_3.html#lab-5-regex-basics-explorer",
    "title": "Chapter 3. Strings",
    "section": "LAB 5: Regex Basics Explorer",
    "text": "LAB 5: Regex Basics Explorer\n\nGoal\nLearn how regular expressions (regex) describe text patterns. Practice matching digits, words, and simple patterns across strings. Compare regex behavior across different languages, noting syntax similarities and differences.\n\n\nSetup\nRegex exists in almost every modern language.\n\nPython → re module\nJava → Pattern / Matcher\nGo → regexp package\nC++ → &lt;regex&gt;\nJavaScript → /pattern/ literals\n\nWe’ll demonstrate in Python for clarity, but the concepts transfer directly.\n\n\nStep-by-Step\n\nBasic Digit Matching\n\nUse [0-9]+ to match sequences of digits.\nExtract numbers from a sentence.\n\nWord Matching\n\nUse \\w+ to capture words (letters, digits, underscores).\nShow how it splits a sentence into tokens.\n\nAnchors\n\nUse ^pattern to match the start of a string.\nUse pattern$ to match the end.\n\nCharacter Classes\n\n[aeiou] matches vowels.\n[A-Z] matches uppercase letters.\n\nPractical Example: Email\n\nUse \\w+@\\w+\\.\\w+ to match simple emails.\nNote limitations (not RFC-complete, but useful).\n\n\n\n\nExample (Python)\nimport re\n\ntext = \"User Alice has email alice@example.com and id 12345.\"\n\n# 1. Digits\ndigits = re.findall(r\"[0-9]+\", text)\nprint(\"Digits:\", digits)  # ['12345']\n\n# 2. Words\nwords = re.findall(r\"\\w+\", text)\nprint(\"Words:\", words)   # ['User', 'Alice', 'has', ...]\n\n# 3. Anchors\nprint(bool(re.match(r\"User\", text)))   # True (starts with \"User\")\nprint(bool(re.search(r\"12345$\", text))) # True (ends with \"12345.\")\n\n# 4. Character classes\nvowels = re.findall(r\"[aeiou]\", text)\nprint(\"Vowels:\", vowels)\n\n# 5. Email pattern\nemails = re.findall(r\"\\w+@\\w+\\.\\w+\", text)\nprint(\"Emails:\", emails)\n\n\nExpected Results\n\n[0-9]+ extracts \"12345\".\n\\w+ splits the sentence into words.\n^User matches the beginning.\n[aeiou] finds vowels.\n\\w+@\\w+\\.\\w+ extracts \"alice@example.com\".\n\n\n\n\nPattern\nMeaning\nExample Match\n\n\n\n\n[0-9]+\nOne or more digits\n\"12345\"\n\n\n\\w+\nWord characters\n\"Alice\"\n\n\n^User\nMust start with \"User\"\n\"User Alice...\"\n\n\n[aeiou]\nAny single vowel\n\"a\", \"e\", \"i\"\n\n\n\\w+@\\w+\\.\\w+\nSimple email pattern\n\"alice@example.com\"\n\n\n\n\n\nWhy it matters\nRegex is a universal tool for working with text: extracting data, validating input, or parsing logs. It is concise and expressive, replacing dozens of manual string operations. Learning the basics of regex builds the foundation for advanced pattern matching and text processing in any language.\n\n\nExercises\n\nWrite a regex to match all digits in \"room 101 floor 5\".\nExtract all words from \"The quick brown fox\".\nMatch any string starting with \"Hello\".\nMatch all vowels in \"banana\".\nExtract all email addresses from \"bob@mail.com, alice@test.org\".\nWrite a regex that matches only uppercase words.\nFind all three-letter words in \"the cat sat on the mat\".\nShow how ^ and $ can be used to validate a string that must start and end with digits.\nExplain why \\w+@\\w+\\.\\w+ is not sufficient for real email validation.\nWrite a regex that finds all words ending in \"ing\" from a sentence.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-6-csv-parsing-pitfalls",
    "href": "chapter_3.html#lab-6-csv-parsing-pitfalls",
    "title": "Chapter 3. Strings",
    "section": "LAB 6: CSV Parsing Pitfalls",
    "text": "LAB 6: CSV Parsing Pitfalls\n\nGoal\nShow why naive string splitting fails on CSV data. Demonstrate common pitfalls like quoted fields and embedded commas. Learn why dedicated CSV parsers are needed for reliability.\n\n\nSetup\nCSV (Comma-Separated Values) looks simple but has rules:\n\nFields can be quoted with \"\nQuoted fields can contain commas\nQuotes inside fields are escaped as \"\"\n\nWe’ll use Python, since it has both naive string methods and a built-in csv module.\n\n\nStep-by-Step\n\nNaive Split Works Sometimes\n\nTake \"apple,banana,cherry\".\nSplit by ,.\nWorks fine.\n\nProblem: Quoted Fields\n\nTake \"apple,\"banana,grape\",cherry\".\nSplit by ,.\nSee how \"banana,grape\" is broken incorrectly.\n\nProblem: Escaped Quotes\n\nTake \"apple,\"he said \"\"hi\"\"\",cherry\".\nNaive split breaks the field and does not handle quotes.\n\nCorrect Parsing with CSV Module\n\nUse csv.reader to handle quotes and escapes.\nShow correct results.\n\nDiscussion\n\nNaive string splitting fails whenever quotes or embedded commas exist.\nCorrect parsers follow RFC 4180 rules.\nProduction code must use libraries, not ad hoc splitting.\n\n\n\n\nExample (Python)\nimport csv\n\n# 1. Naive split (works)\nline1 = \"apple,banana,cherry\"\nprint(\"Naive split:\", line1.split(\",\"))  # ['apple', 'banana', 'cherry']\n\n# 2. Quoted fields with commas\nline2 = 'apple,\"banana,grape\",cherry'\nprint(\"Naive split:\", line2.split(\",\"))  # ['apple', '\"banana', 'grape\"', 'cherry']\n\n# 3. Quoted fields with escaped quotes\nline3 = 'apple,\"he said \"\"hi\"\"\",cherry'\nprint(\"Naive split:\", line3.split(\",\"))  # ['apple', '\"he said \"\"hi\"\"\"', 'cherry']\n\n# 4. Correct parsing\nfor line in [line1, line2, line3]:\n    parsed = next(csv.reader([line]))\n    print(\"CSV parser:\", parsed)\n\n\nExpected Results\n\nNaive split fails on quoted fields.\nCSV parser handles commas and quotes correctly.\n\n\n\n\n\n\n\n\n\nInput String\nNaive Split\nCSV Parser\n\n\n\n\napple,banana,cherry\n['apple','banana','cherry']\n['apple','banana','cherry']\n\n\napple,\"banana,grape\",cherry\n['apple','\"banana','grape\"','cherry']\n['apple','banana,grape','cherry']\n\n\napple,\"he said \"\"hi\"\"\",cherry\n['apple','\"he said \"\"hi\"\"\"','cherry']\n['apple','he said \"hi\"','cherry']\n\n\n\n\n\nWhy it matters\nCSV looks simple but is deceptively tricky. Many production bugs come from naive parsing that breaks when encountering quotes or embedded commas. Using proper libraries ensures reliability across systems. Understanding these pitfalls prevents data corruption in analytics, ETL pipelines, and configuration parsing.\n\n\nExercises\n\nWrite code that naively splits \"a,b,c\" and verify the output.\nSplit \"a,\"b,c\",d\" naively and explain what goes wrong.\nParse the same line using a CSV library and compare results.\nShow how \"a,\"he said \"\"ok\"\"\",c\" is incorrectly parsed by naive splitting.\nWrite a regex that attempts to parse CSV and explain its limits.\nCompare parsing speed of naive split vs csv.reader.\nExplain why using regex for full CSV parsing is impractical.\nShow how a CSV parser handles an empty field (\"a,,c\").\nCreate a CSV with newline characters inside quotes and parse it correctly.\nDescribe why standards like RFC 4180 exist for CSV and what problems they solve.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-7-locale-aware-sorting",
    "href": "chapter_3.html#lab-7-locale-aware-sorting",
    "title": "Chapter 3. Strings",
    "section": "LAB 7: Locale-Aware Sorting",
    "text": "LAB 7: Locale-Aware Sorting\n\nGoal\nUnderstand how different locales affect string sorting. See why \"ä\" sorts differently in German vs Swedish, and how case sensitivity influences order. Learn why collation rules matter in real-world systems like databases and search engines.\n\n\nSetup\nUse a language that supports locale-aware collation:\n\nPython → locale module\nJava → Collator\nC++ → &lt;locale&gt; with collate\nDatabases → COLLATE clauses\n\nWe’ll demonstrate with Python.\n\n\nStep-by-Step\n\nDefault Sorting (ASCII/Unicode order)\n\nSort a list of words: [\"apple\", \"Banana\", \"äpfel\", \"zebra\"].\nObserve that uppercase letters and special characters do not sort as expected for dictionary order.\n\nGerman Collation\n\nUse \"de_DE.UTF-8\" locale.\nIn German, \"ä\" is often treated like \"ae\".\nSort the same list and observe differences.\n\nSwedish Collation\n\nUse \"sv_SE.UTF-8\" locale.\nIn Swedish, \"ä\" is a separate letter that comes after \"z\".\nCompare results with German sorting.\n\nCase Sensitivity\n\nCompare \"apple\" vs \"Apple\".\nShow how some locales ignore case, others do not.\n\nDiscussion\n\nLocale defines sorting rules for a culture or language.\nIncorrect collation leads to confusing search results or misordered lists.\nDatabases use collations to enforce consistency.\n\n\n\n\nExample (Python)\nimport locale\n\nwords = [\"apple\", \"Banana\", \"äpfel\", \"zebra\"]\n\n# 1. Default sorting\nprint(\"Default:\", sorted(words))\n\n# 2. German collation\nlocale.setlocale(locale.LC_COLLATE, \"de_DE.UTF-8\")\nprint(\"German:\", sorted(words, key=locale.strxfrm))\n\n# 3. Swedish collation\nlocale.setlocale(locale.LC_COLLATE, \"sv_SE.UTF-8\")\nprint(\"Swedish:\", sorted(words, key=locale.strxfrm))\n\n# 4. Case-insensitive comparison (normalize before sort)\ncase_insensitive = sorted(words, key=lambda w: w.lower())\nprint(\"Case-insensitive:\", case_insensitive)\n\n\nExpected Results\n\n\n\nLocale\nSorted Output\n\n\n\n\nDefault (Unicode)\n['Banana', 'apple', 'zebra', 'äpfel']\n\n\nGerman (de_DE)\n['apple', 'äpfel', 'Banana', 'zebra']\n\n\nSwedish (sv_SE)\n['apple', 'Banana', 'zebra', 'äpfel']\n\n\nCase-insensitive\n['apple', 'äpfel', 'Banana', 'zebra']\n\n\n\n\n\nWhy it matters\n\nMultilingual systems must handle cultural sorting correctly.\n\"ä\" is \"ae\" in German, but a distinct letter after \"z\" in Swedish.\nCase sensitivity can confuse users if \"Apple\" and \"apple\" appear far apart.\nDatabases and search engines must define collation rules to ensure consistency.\n\n\n\nExercises\n\nSort [\"Zoo\", \"apple\", \"Äpfel\"] in default order.\nSort the same list in German locale.\nSort the same list in Swedish locale.\nExplain why results differ between German and Swedish.\nWrite code that performs case-insensitive sorting.\nCompare how \"Résumé\" and \"resume\" sort under accent-sensitive vs accent-insensitive collation.\nDemonstrate natural sorting for file names: [\"file1\", \"file10\", \"file2\"].\nExplain how incorrect collation could break alphabetical order in a contact list.\nResearch and list collations available in your system/database.\nPropose a default collation strategy for a global web application.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-8-unicode-normalization-demo",
    "href": "chapter_3.html#lab-8-unicode-normalization-demo",
    "title": "Chapter 3. Strings",
    "section": "LAB 8: Unicode Normalization Demo",
    "text": "LAB 8: Unicode Normalization Demo\n\nGoal\nShow how visually identical strings can have different underlying Unicode encodings. Learn to use normalization (NFC, NFD) to make comparisons reliable across systems.\n\n\nSetup\nUnicode allows multiple representations of the same text:\n\nPrecomposed characters (é as U+00E9)\nDecomposed sequences (e + U+0301 combining accent)\n\nWe’ll use Python’s unicodedata module to demonstrate.\n\n\nStep-by-Step\n\nCreate Equivalent Strings\n\nDefine s1 = \"café\" (precomposed).\nDefine s2 = \"cafe\\u0301\" (decomposed).\nPrint both — they look the same.\n\nCompare Without Normalization\n\nCheck s1 == s2.\nResult is False, even though they appear identical.\n\nNormalize Strings\n\nApply NFC (Normalization Form Composed).\nApply NFD (Normalization Form Decomposed).\nCompare results again.\n\nCheck Lengths and Code Points\n\nCompare lengths of s1 and s2.\nPrint their Unicode code points.\nShow how precomposed has 1 code point for é, decomposed has 2.\n\nDiscussion\n\nNormalization ensures consistent storage and comparison.\nDatabases and search engines often normalize text before indexing.\nWithout it, identical-looking words may be treated as different.\n\n\n\n\nExample (Python)\nimport unicodedata\n\n# 1. Create equivalent strings\ns1 = \"café\"           # precomposed\ns2 = \"cafe\\u0301\"     # decomposed\n\nprint(\"s1:\", s1)\nprint(\"s2:\", s2)\nprint(\"Equal raw?:\", s1 == s2)\n\n# 2. Normalize\nnfc1 = unicodedata.normalize(\"NFC\", s1)\nnfc2 = unicodedata.normalize(\"NFC\", s2)\nprint(\"Equal NFC?:\", nfc1 == nfc2)\n\nnfd1 = unicodedata.normalize(\"NFD\", s1)\nnfd2 = unicodedata.normalize(\"NFD\", s2)\nprint(\"Equal NFD?:\", nfd1 == nfd2)\n\n# 3. Inspect lengths and code points\nprint(\"Length s1:\", len(s1))\nprint(\"Length s2:\", len(s2))\n\nprint(\"Code points s1:\", [hex(ord(c)) for c in s1])\nprint(\"Code points s2:\", [hex(ord(c)) for c in s2])\n\n\nExpected Results\n\ns1 == s2 → False (different underlying encoding).\nAfter normalization (NFC or NFD), they compare equal.\nLengths differ (len(s1) == 4, len(s2) == 5).\nCode points differ:\n\ns1: [0x63, 0x61, 0x66, 0xe9]\ns2: [0x63, 0x61, 0x66, 0x65, 0x301]\n\n\n\n\n\n\n\n\n\n\n\n\nString\nVisible Form\nEncoding Style\nLength\nCode Points\n\n\n\n\ns1\ncafé\nPrecomposed NFC\n4\nc a f é (U+00E9)\n\n\ns2\ncafé\nDecomposed NFD\n5\nc a f e + ´ (U+0301)\n\n\n\n\n\nWhy it matters\n\nIdentical-looking text can fail equality checks if not normalized.\nFile systems, databases, and APIs may use different normalization forms.\nSearch engines normalize text to avoid duplicate results.\nSecurity-sensitive systems must normalize input to avoid spoofing or bypass attacks.\n\n\n\nExercises\n\nCompare \"résumé\" written with precomposed vs decomposed accents.\nShow how string lengths differ between NFC and NFD forms.\nWrite code to normalize a list of user inputs before storing them in a database.\nExplain why \"é\" (U+00E9) is not equal to \"e\" + U+0301.\nDemonstrate normalization on Japanese text (hiragana + diacritics).\nShow how failing to normalize could cause duplicate entries in a dictionary.\nInvestigate what normalization form macOS uses by default for filenames.\nExplain why normalization is critical in international domain names.\nCreate a function that reports whether two strings are canonically equivalent.\nDesign a normalization step for a search engine pipeline.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-9-confusable-characters-security-check",
    "href": "chapter_3.html#lab-9-confusable-characters-security-check",
    "title": "Chapter 3. Strings",
    "section": "LAB 9: Confusable Characters Security Check",
    "text": "LAB 9: Confusable Characters Security Check\n\nGoal\nDemonstrate how visually identical characters can come from different scripts (Latin, Cyrillic, Greek). Show how this can fool string comparisons, and implement a basic detector for such confusables.\n\n\nSetup\nAttackers exploit Unicode confusables (e.g., Latin \"a\" vs Cyrillic \"а\") to trick users or bypass validation. This is called a homoglyph attack. Examples:\n\n\"paypal.com\" vs \"раураl.com\" (Cyrillic р and а).\n\"admin\" vs \"аdmin\" (Cyrillic а).\n\nWe’ll use Python for demonstration (unicodedata module).\n\n\nStep-by-Step\n\nCreate Confusable Strings\n\nDefine latin_a = \"a\" and cyrillic_a = \"а\" (U+0430).\nPrint them side by side.\nCompare with == to show they are different.\n\nInspect Code Points\n\nUse ord() to print Unicode code points.\nShow the numeric difference despite identical appearance.\n\nConfusable Detection\n\nWrite a function that checks if characters belong to different Unicode blocks.\nFlag potential confusables.\n\nPractical Example: Fake Domain\n\nConstruct \"раураl.com\" (Cyrillic letters).\nCompare it to \"paypal.com\".\nShow why naive string equality misses the difference.\n\nDiscussion\n\nHomoglyph attacks target login systems, domains, and usernames.\nDefenses:\n\nNormalize + restrict allowed scripts.\nUse libraries like Unicode confusables.txt.\nVisual warnings in browsers.\n\n\n\n\n\nExample (Python)\nimport unicodedata\n\n# 1. Confusable characters\nlatin_a = \"a\"\ncyrillic_a = \"а\"  # U+0430\nprint(\"Latin a:\", latin_a, \"Cyrillic a:\", cyrillic_a)\nprint(\"Equal?:\", latin_a == cyrillic_a)\n\n# 2. Inspect code points\nprint(\"Latin a code point:\", hex(ord(latin_a)))\nprint(\"Cyrillic a code point:\", hex(ord(cyrillic_a)))\n\n# 3. Simple detector\ndef detect_confusables(text):\n    scripts = {}\n    for ch in text:\n        name = unicodedata.name(ch, \"\")\n        if \"CYRILLIC\" in name:\n            scripts[ch] = \"Cyrillic\"\n        elif \"LATIN\" in name:\n            scripts[ch] = \"Latin\"\n        elif \"GREEK\" in name:\n            scripts[ch] = \"Greek\"\n    return scripts\n\nprint(\"Detection:\", detect_confusables(\"раураl\"))\n\n# 4. Fake domain example\nreal = \"paypal.com\"\nfake = \"раураl.com\"  # with Cyrillic 'р' and 'а'\nprint(\"Real domain:\", real)\nprint(\"Fake domain:\", fake)\nprint(\"Equal?:\", real == fake)\nprint(\"Fake detection:\", detect_confusables(fake))\n\n\nExpected Results\n\n\"a\" vs \"а\" look the same but are unequal.\nCode points differ:\n\nLatin a → U+0061\nCyrillic а → U+0430\n\nDetection function flags script differences.\n\"paypal.com\" ≠ \"раураl.com\", but naive eyes might miss it.\n\n\n\n\nCharacter\nVisual\nUnicode\nScript\n\n\n\n\na\na\nU+0061\nLatin\n\n\nа\na\nU+0430\nCyrillic\n\n\n\n\n\nWhy it matters\n\nSecurity systems that ignore Unicode tricks are vulnerable to phishing and spoofing.\nFake domains, usernames, or commands can bypass filters.\nNormalization does not fix confusables — script restriction or mapping is required.\nModern browsers warn users about domains mixing scripts.\n\n\n\nExercises\n\nCompare \"pаypal\" (with Cyrillic а) to \"paypal\".\nPrint Unicode code points of \"o\" vs Cyrillic \"о\".\nExtend the detect_confusables function to flag Greek letters too.\nCreate a fake username using mixed Latin and Cyrillic.\nShow why normalization (NFC/NFD) does not fix confusables.\nResearch Unicode confusables.txt and find at least 5 common homoglyphs.\nPropose a rule to allow only one script (Latin or Cyrillic) per string.\nWrite a program to detect mixed-script domains.\nExplain how browsers handle homoglyph domains.\nDesign a security policy for usernames to prevent confusable attacks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-10-substring-search-algorithms",
    "href": "chapter_3.html#lab-10-substring-search-algorithms",
    "title": "Chapter 3. Strings",
    "section": "LAB 10: Substring Search Algorithms",
    "text": "LAB 10: Substring Search Algorithms\n\nGoal\nCompare different substring search algorithms — Naive, Knuth–Morris–Pratt (KMP), and Boyer–Moore (BM). Learn how they work, when they perform well, and measure their efficiency on long texts.\n\n\nSetup\nSubstring search asks: Does pattern P occur in text T?\n\nNaive algorithm: check every position → O(n·m).\nKMP: preprocess pattern to skip comparisons → O(n + m).\nBoyer–Moore: skip ahead using heuristics → sublinear in practice.\n\nWe’ll implement these in Python for clarity.\n\n\nStep-by-Step\n\nNaive Algorithm\n\nFor each position in text, compare pattern character by character.\nSlow on long inputs with repeated matches.\n\nKMP Algorithm\n\nBuild a failure function (longest prefix-suffix table).\nSkip ahead when mismatch occurs.\nGuarantees linear time.\n\nBoyer–Moore Algorithm (Bad Character Rule)\n\nStart matching from the end of the pattern.\nOn mismatch, shift based on last occurrence of character.\nVery fast in practice, especially for large alphabets.\n\nBenchmark on Large Input\n\nGenerate a text of size ~100,000 characters.\nSearch for a small pattern.\nMeasure runtimes.\n\nDiscussion\n\nNaive is simple but inefficient.\nKMP is optimal for worst case.\nBoyer–Moore is fastest in practice for natural language.\n\n\n\n\nExample (Python)\nimport time\n\n# 1. Naive\ndef naive_search(text, pattern):\n    n, m = len(text), len(pattern)\n    for i in range(n - m + 1):\n        if text[i:i+m] == pattern:\n            return i\n    return -1\n\n# 2. KMP\ndef kmp_table(pattern):\n    m = len(pattern)\n    table = [0] * m\n    j = 0\n    for i in range(1, m):\n        while j &gt; 0 and pattern[i] != pattern[j]:\n            j = table[j - 1]\n        if pattern[i] == pattern[j]:\n            j += 1\n            table[i] = j\n    return table\n\ndef kmp_search(text, pattern):\n    n, m = len(text), len(pattern)\n    table = kmp_table(pattern)\n    j = 0\n    for i in range(n):\n        while j &gt; 0 and text[i] != pattern[j]:\n            j = table[j - 1]\n        if text[i] == pattern[j]:\n            j += 1\n            if j == m:\n                return i - m + 1\n    return -1\n\n# 3. Boyer–Moore (bad character heuristic)\ndef bm_table(pattern):\n    table = {}\n    for i, c in enumerate(pattern):\n        table[c] = i\n    return table\n\ndef bm_search(text, pattern):\n    n, m = len(text), len(pattern)\n    table = bm_table(pattern)\n    i = 0\n    while i &lt;= n - m:\n        j = m - 1\n        while j &gt;= 0 and text[i + j] == pattern[j]:\n            j -= 1\n        if j &lt; 0:\n            return i\n        shift = max(1, j - table.get(text[i + j], -1))\n        i += shift\n    return -1\n\n# Benchmark\ntext = \"a\" * 100000 + \"b\"\npattern = \"a\" * 10 + \"b\"\n\nfor name, func in [(\"Naive\", naive_search), (\"KMP\", kmp_search), (\"Boyer–Moore\", bm_search)]:\n    start = time.time()\n    pos = func(text, pattern)\n    print(f\"{name} found at {pos}, time {round(time.time()-start,4)}s\")\n\n\nExpected Results\n\nNaive: Slow for large repetitive text.\nKMP: Linear time, handles worst case well.\nBoyer–Moore: Fastest for natural text (skips ahead aggressively).\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorst Case Time\nBest Case Time\nNotes\n\n\n\n\nNaive\nO(n·m)\nO(n)\nSimple, bad for long patterns\n\n\nKMP\nO(n + m)\nO(n + m)\nGuaranteed linear time\n\n\nBoyer–Moore\nO(n·m) worst\nSublinear avg\nVery fast in practice on real text\n\n\n\n\n\nWhy it matters\n\nSearch engines, text editors, and databases rely on efficient substring search.\nNaive methods break down at scale.\nKMP shows theory guiding better algorithms.\nBoyer–Moore demonstrates practical engineering that beats naive even further.\n\n\n\nExercises\n\nImplement naive substring search in any language.\nSearch for \"abc\" in \"aaaabc\" and explain why naive repeats work.\nBuild the prefix table for \"ababaca\" in KMP.\nExplain how KMP avoids re-checking characters.\nImplement Boyer–Moore with the bad character rule.\nShow why Boyer–Moore jumps ahead more than one character on mismatches.\nBenchmark naive vs KMP vs BM on 1MB text.\nCompare results for random text vs repetitive text.\nExplain why Boyer–Moore worst-case is still O(n·m).\nPropose a hybrid search strategy for DNA sequences (small alphabet, long text).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-11-regex-engine-catastrophe-redos",
    "href": "chapter_3.html#lab-11-regex-engine-catastrophe-redos",
    "title": "Chapter 3. Strings",
    "section": "LAB 11: Regex Engine Catastrophe (ReDoS)",
    "text": "LAB 11: Regex Engine Catastrophe (ReDoS)\n\nGoal\nSee how certain regex patterns cause catastrophic backtracking and turn tiny inputs into huge runtimes (Regex Denial of Service—ReDoS). Learn safe alternatives and mitigation strategies.\n\n\nSetup\nAny language with a backtracking engine will reproduce the issue (Python/PCRE/Perl/JavaScript). An automata-based engine (e.g., RE2, Rust’s default) won’t exhibit catastrophic behavior for the same patterns.\nTest patterns that are notorious:\n\nNested, overlapping quantifiers: (a+)+b\nAmbiguous alternation with stars: (a|aa)+b\nCatastrophic HTMLish: (&lt;.+&gt;)+\n\n\n\nStep-by-Step\n\nReproduce a Fast Match\n\nPattern: (a+)+b\nInput: \"a\"*25 + \"b\"\nExpect: quick success (engine finds the trailing b).\n\nTrigger Catastrophe\n\nSame pattern: (a+)+b\nInput: \"a\"*25 (no b)\nExpect: very slow—engine explores exponential backtracking paths.\n\nMeasure Impact\n\nTime both inputs.\nIncrease input length stepwise (e.g., 15, 20, 25, 30 as).\nObserve runtime explosion for the no-b case.\n\nTry Another Problematic Pattern\n\nPattern: (a|aa)+b\nInputs: \"a\"*N+\"b\" (fast) and \"a\"*N (slow).\nNote that overlapping alternatives also trigger exponential blowups.\n\nMitigate\n\nRefactor to unambiguous patterns:\n\nReplace (a+)+b with a+ b-style constructions that avoid nested quantifiers, or use possessive/atomic quantifiers if supported (e.g., a++b, (?&gt;a+)b).\n\nConstrain with anchors and character classes to reduce search space.\nPrecompile and timeout: set execution time limits (if supported).\nUse safe engines (RE2/Hyperscan) for untrusted input.\n\n\n\n\nExample (Python)\nimport re, time\n\ndef timed_match(pattern, text, flags=0):\n    start = time.time()\n    ok = bool(re.match(pattern, text, flags))\n    dur = time.time() - start\n    return ok, dur\n\n# 1) Fast match: pattern matches because of trailing 'b'\npat = r\"(a+)+b\"\nfast_text = \"a\" * 25 + \"b\"\nok, dur = timed_match(pat, fast_text)\nprint(\"FAST   -&gt; match:\", ok, \"time:\", round(dur, 4), \"s\")\n\n# 2) Catastrophic: same pattern, missing the final 'b'\nslow_text = \"a\" * 25\nok, dur = timed_match(pat, slow_text)\nprint(\"SLOW   -&gt; match:\", ok, \"time:\", round(dur, 4), \"s\")\n\n# 3) Scale up to see blowup\nfor n in [20, 22, 24, 26, 28]:\n    txt = \"a\" * n\n    ok, dur = timed_match(pat, txt)\n    print(f\"N={n:&lt;2} -&gt; match:{ok} time:{round(dur, 4)} s\")\n\n# 4) Another problematic pattern\npat2 = r\"(a|aa)+b\"\nok, dur = timed_match(pat2, \"a\" * 25 + \"b\")\nprint(\"ALT OK -&gt; match:\", ok, \"time:\", round(dur, 4), \"s\")\n\nok, dur = timed_match(pat2, \"a\" * 25)\nprint(\"ALT SLOW-&gt; match:\", ok, \"time:\", round(dur, 4), \"s\")\n\n# 5) Safer alternative using atomic/possessive (if your engine supports it)\n# Python's 're' does not support atomic groups/possessive quantifiers.\n# Pseudocode examples for other engines:\n#   - PCRE/Java: r\"(?&gt;a+)b\" or r\"a++b\"\n# For Python, restructure logic instead of relying on engine features.\n\n\nExpected Results\n\nWith trailing b, matches are fast.\nWithout b, runtime grows rapidly as N increases.\nOverlapping alternations show the same pattern of slowdown.\nAtomic/possessive quantifiers (in engines that support them) remove backtracking and restore predictable performance.\n\n\n\n\n\n\n\n\n\n\n\nPattern\nInput\nEngine Type\nBehavior\n\n\n\n\n\n(a+)+b\na…ab\nBacktracking\nFast\n\n\n\n(a+)+b\na…a\nBacktracking\nCatastrophic (very slow)\n\n\n\n`(a\naa)+b`\na…ab\nBacktracking\nFast\n\n\n`(a\naa)+b`\na…a\nBacktracking\nCatastrophic\n\n\n(?&gt;a+)b\na…a?\nAtomic (PCRE/Java)\nPredictable, no catastrophe\n\n\n\na++b\na…a?\nPossessive\nPredictable\n\n\n\nRE2 version\nany of above\nDFA/NFA (safe)\nLinear-time, no catastrophe\n\n\n\n\n\n\nWhy it matters\n\nUntrusted input + fragile regex = DoS vector.\nWeb apps, API gateways, and log pipelines often apply regex to attacker-controlled text.\nThe fix is architectural (pick safe engines) and design-oriented (write non-ambiguous patterns, set timeouts).\n\n\n\nExercises\n\nExplain why (a+)+b is dangerous on inputs composed only of as.\nShow a timing table for (a+)+b against a^N for N = 10, 15, 20, 25.\nConstruct another catastrophic pattern using overlapping alternation (e.g., (ab|a)*b).\nRewrite (a|aa)+b into a pattern that avoids ambiguity while matching the same language.\nDescribe how atomic groups or possessive quantifiers prevent backtracking.\nPropose engine-agnostic mitigations: anchoring, limiting input length, pre-validation.\nDesign a test harness that detects suspicious regex (runtime or backtracking depth spikes).\nCompare behavior of the same pattern in a backtracking engine vs a linear-time engine (e.g., RE2).\nFor a production route-matching regex, list safeguards to prevent ReDoS in a web server.\nOutline a policy for your organization: when to allow backreferences/lookbehinds; when to mandate RE2-class engines.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-12-greedy-vs-lazy-matching",
    "href": "chapter_3.html#lab-12-greedy-vs-lazy-matching",
    "title": "Chapter 3. Strings",
    "section": "LAB 12: Greedy vs Lazy Matching",
    "text": "LAB 12: Greedy vs Lazy Matching\n\nGoal\nSee how regex quantifiers (*, +, ?) behave in greedy mode (match as much as possible) versus lazy mode (match as little as possible). Learn when greedy matching leads to over-capture and how lazy mode fixes it.\n\n\nSetup\nRegex engines default to greedy quantifiers:\n\n.* matches the longest possible string.\nAdding ? makes them lazy: .*? matches the shortest possible string.\n\nWe’ll use Python’s re module.\n\n\nStep-by-Step\n\nSimple Greedy Match\n\nPattern: &lt;tag&gt;.*&lt;/tag&gt;\nInput: \"&lt;tag&gt;one&lt;/tag&gt;&lt;tag&gt;two&lt;/tag&gt;\"\nGreedy captures from the first &lt;tag&gt; to the last &lt;/tag&gt;, swallowing too much.\n\nSwitch to Lazy\n\nPattern: &lt;tag&gt;.*?&lt;/tag&gt;\nSame input.\nLazy captures each &lt;tag&gt;…&lt;/tag&gt; pair separately, as expected.\n\nExperiment with Plus Quantifier\n\nCompare &lt;tag&gt;.+&lt;/tag&gt; vs &lt;tag&gt;.+?&lt;/tag&gt;.\n+ requires at least one character, * allows zero.\n\nRealistic Example: HTML-ish Parsing\n\nInput: \"&lt;b&gt;bold&lt;/b&gt;&lt;i&gt;italic&lt;/i&gt;\".\nGreedy regex: &lt;.*&gt; → captures everything between first &lt; and last &gt;.\nLazy regex: &lt;.*?&gt; → captures &lt;b&gt;, &lt;/b&gt;, &lt;i&gt;, &lt;/i&gt; separately.\n\nDiscussion\n\nGreedy matching is fine when only one block exists.\nLazy matching is safer for repeated structures.\nRegex is not a full parser; overuse leads to fragile code.\n\n\n\n\nExample (Python)\nimport re\n\ntext = \"&lt;tag&gt;one&lt;/tag&gt;&lt;tag&gt;two&lt;/tag&gt;\"\n\n# 1. Greedy match\ngreedy = re.findall(r\"&lt;tag&gt;.*&lt;/tag&gt;\", text)\nprint(\"Greedy:\", greedy)\n\n# 2. Lazy match\nlazy = re.findall(r\"&lt;tag&gt;.*?&lt;/tag&gt;\", text)\nprint(\"Lazy:\", lazy)\n\n# 3. Greedy vs lazy with plus quantifier\nsample = \"&lt;tag&gt;a&lt;/tag&gt;&lt;tag&gt;b&lt;/tag&gt;\"\nprint(\"Greedy +:\", re.findall(r\"&lt;tag&gt;.+&lt;/tag&gt;\", sample))\nprint(\"Lazy +?:\", re.findall(r\"&lt;tag&gt;.+?&lt;/tag&gt;\", sample))\n\n# 4. HTML-ish example\nhtml = \"&lt;b&gt;bold&lt;/b&gt;&lt;i&gt;italic&lt;/i&gt;\"\nprint(\"Greedy HTML:\", re.findall(r\"&lt;.*&gt;\", html))\nprint(\"Lazy HTML:\", re.findall(r\"&lt;.*?&gt;\", html))\n\n\nExpected Results\n\nGreedy consumes everything between first and last match.\nLazy matches each block separately.\n\n\n\n\n\n\n\n\n\nPattern\nInput\nOutput\n\n\n\n\n&lt;tag&gt;.*&lt;/tag&gt;\n&lt;tag&gt;one&lt;/tag&gt;&lt;tag&gt;two&lt;/tag&gt;\n['&lt;tag&gt;one&lt;/tag&gt;&lt;tag&gt;two&lt;/tag&gt;']\n\n\n&lt;tag&gt;.*?&lt;/tag&gt;\n&lt;tag&gt;one&lt;/tag&gt;&lt;tag&gt;two&lt;/tag&gt;\n['&lt;tag&gt;one&lt;/tag&gt;', '&lt;tag&gt;two&lt;/tag&gt;']\n\n\n&lt;.*&gt; (greedy)\n&lt;b&gt;bold&lt;/b&gt;&lt;i&gt;italic&lt;/i&gt;\n['&lt;b&gt;bold&lt;/b&gt;&lt;i&gt;italic&lt;/i&gt;']\n\n\n&lt;.*?&gt; (lazy)\n&lt;b&gt;bold&lt;/b&gt;&lt;i&gt;italic&lt;/i&gt;\n['&lt;b&gt;', '&lt;/b&gt;', '&lt;i&gt;', '&lt;/i&gt;']\n\n\n\n\n\nWhy it matters\n\nGreedy vs lazy matching changes how much text is captured.\nIncorrect choice can swallow entire documents or miss intended matches.\nUseful in log parsing, HTML scraping, and template matching.\nShows why regex is powerful but also error-prone when used without care.\n\n\n\nExercises\n\nWrite a regex to capture &lt;tag&gt;…&lt;/tag&gt; in \"&lt;tag&gt;a&lt;/tag&gt;&lt;tag&gt;b&lt;/tag&gt;\". Compare greedy vs lazy.\nShow why &lt;.*&gt; over-captures in \"&lt;p&gt;hi&lt;/p&gt;&lt;p&gt;bye&lt;/p&gt;\".\nModify &lt;.*&gt; into &lt;.*?&gt; and explain the difference.\nUse .+? to capture non-empty blocks. Show how it differs from .*?.\nWrite a regex that extracts &lt;b&gt;…&lt;/b&gt; text only.\nCompare results of &lt;div&gt;.*&lt;/div&gt; on a file with nested &lt;div&gt; tags.\nExplain why lazy quantifiers may still produce unexpected results in deeply nested structures.\nBenchmark .* vs .*? on a 1MB HTML file.\nShow how greedy vs lazy affects performance (number of backtracking steps).\nExplain why full HTML parsing should not rely solely on regex.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-13-build-a-mini-inverted-index",
    "href": "chapter_3.html#lab-13-build-a-mini-inverted-index",
    "title": "Chapter 3. Strings",
    "section": "LAB 13: Build a Mini Inverted Index",
    "text": "LAB 13: Build a Mini Inverted Index\n\nGoal\nImplement a simplified inverted index, the core data structure behind search engines. Learn how to map words to the documents they appear in, then use the index to answer queries quickly.\n\n\nSetup\nAn inverted index stores entries like:\n\"dog\" → [doc1, doc3]  \n\"cat\" → [doc2]  \nInstead of scanning every document, you look up the word in the index. We’ll implement this in Python, but the idea applies to any language or system.\n\n\nStep-by-Step\n\nPrepare Documents\n\nCreate a small collection of texts, each with an ID.\nExample:\n\n1: \"the quick brown fox\"\n2: \"the lazy dog\"\n3: \"the fox jumped over the dog\"\n\n\nTokenize Documents\n\nSplit text into lowercase words.\nRemove punctuation (simplified tokenizer).\n\nBuild Inverted Index\n\nFor each word, append the document ID to its list.\nUse a dictionary (map) of word → set of doc IDs.\n\nQuery the Index\n\nLook up a word and return all documents containing it.\nExtend to multi-word queries by intersecting sets.\n\nDiscussion\n\nInverted index allows fast search compared to scanning.\nReal search engines add ranking (TF-IDF, BM25), phrase search, and indexing optimizations.\n\n\n\n\nExample (Python)\nfrom collections import defaultdict\nimport re\n\n# 1. Documents\ndocs = {\n    1: \"the quick brown fox\",\n    2: \"the lazy dog\",\n    3: \"the fox jumped over the dog\"\n}\n\n# 2. Tokenize\ndef tokenize(text):\n    return re.findall(r\"\\w+\", text.lower())\n\n# 3. Build inverted index\nindex = defaultdict(set)\nfor doc_id, text in docs.items():\n    for word in tokenize(text):\n        index[word].add(doc_id)\n\n# 4. Query\ndef search(word):\n    return index.get(word.lower(), set())\n\ndef search_multi(words):\n    sets = [search(w) for w in words]\n    return set.intersection(*sets) if sets else set()\n\n# Example queries\nprint(\"Index for 'fox':\", index[\"fox\"])\nprint(\"Search 'dog':\", search(\"dog\"))\nprint(\"Search 'fox AND dog':\", search_multi([\"fox\", \"dog\"]))\n\n\nExpected Results\n\nIndex for \"fox\" → {1, 3}\nSearch \"dog\" → {2, 3}\nSearch \"fox AND dog\" → {3}\n\n\n\n\nWord\nDocument IDs\n\n\n\n\nthe\n{1, 2, 3}\n\n\nquick\n{1}\n\n\nbrown\n{1}\n\n\nfox\n{1, 3}\n\n\nlazy\n{2}\n\n\ndog\n{2, 3}\n\n\njumped\n{3}\n\n\nover\n{3}\n\n\n\n\n\nWhy it matters\n\nThis is the foundation of Google, Lucene, Elasticsearch and every major search system.\nInverted indexes make keyword search efficient.\nThey scale from a few documents to billions.\nConcepts here connect to IR (Information Retrieval) theory.\n\n\n\nExercises\n\nBuild an inverted index for 5 custom sentences.\nSearch for a single word and list document IDs.\nImplement AND search: return docs containing all words.\nImplement OR search: return docs containing at least one word.\nExtend the tokenizer to remove stop words like \"the\", \"and\".\nModify the index to count word frequency in each document.\nAdd a simple ranking: prefer documents with more matches.\nTest the system with queries on \"cat\" when no doc contains \"cat\".\nExplain why inverted indexes are better than scanning.\nDesign how you would scale this to 1 million documents.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-14-compression-of-repetitive-strings",
    "href": "chapter_3.html#lab-14-compression-of-repetitive-strings",
    "title": "Chapter 3. Strings",
    "section": "LAB 14: Compression of Repetitive Strings",
    "text": "LAB 14: Compression of Repetitive Strings\n\nGoal\nUnderstand how string compression reduces storage by exploiting repetition. Learn simple dictionary-based compression, compare sizes with and without compression, and see how algorithms like gzip or Huffman coding achieve savings.\n\n\nSetup\nWe’ll simulate a simple compression scheme:\n\nNaive storage: keep the string as-is.\nDictionary-based compression: replace repeated substrings with references.\nCompare results with Python’s zlib (which implements DEFLATE, combining LZ77 + Huffman coding).\n\nTest input:\n\"banana banana banana\"\n\n\nStep-by-Step\n\nUncompressed Storage\n\nMeasure length of original string in bytes.\n\nNaive Dictionary Compression\n\nStore \"banana\" once.\nRepresent repeated \"banana\"s as references.\nCalculate compressed size (dictionary + references).\n\nzlib Compression\n\nUse zlib.compress() to compress.\nCompare compressed size with original.\n\nTest with Different Inputs\n\nHighly repetitive: \"abc abc abc abc\" → compresses well.\nRandom: \"qwertyuiop\" → compresses poorly.\n\nDiscussion\n\nCompression works best when redundancy exists.\nDictionary + entropy coding are the backbone of text and file compression.\nTrade-off: compression saves space but costs CPU time.\n\n\n\n\nExample (Python)\nimport zlib\n\n# 1. Original string\ntext = \"banana banana banana\"\noriginal_size = len(text.encode(\"utf-8\"))\nprint(\"Original text:\", text)\nprint(\"Original size:\", original_size, \"bytes\")\n\n# 2. Naive dictionary compression simulation\ndictionary = {\"banana\": 1}\ntokens = [dictionary.get(word, word) for word in text.split()]\ncompressed_repr = (dictionary, tokens)\nsimulated_size = len(\"banana\") + len(tokens)  # rough size\nprint(\"Naive simulated compressed size:\", simulated_size, \"bytes\")\n\n# 3. zlib compression\ncompressed = zlib.compress(text.encode(\"utf-8\"))\nprint(\"zlib compressed size:\", len(compressed), \"bytes\")\n\n# 4. Compare with random string\nrandom_text = \"qwertyuiop\"\ncompressed_random = zlib.compress(random_text.encode(\"utf-8\"))\nprint(\"Random original size:\", len(random_text), \"bytes\")\nprint(\"Random compressed size:\", len(compressed_random), \"bytes\")\n\n\nExpected Results\n\nOriginal: ~20 bytes.\nNaive dictionary scheme: fewer bytes than original.\nzlib: compressed size significantly smaller (e.g., 20 → 17).\nRandom text: compressed size ≈ original (sometimes slightly larger due to overhead).\n\n\n\n\nInput\nOriginal Size\nCompressed Size\n\n\n\n\n\"banana banana banana\"\n20 bytes\n~17 bytes\n\n\n\"abc abc abc abc\"\n15 bytes\n~13 bytes\n\n\n\"qwertyuiop\"\n10 bytes\n~12 bytes (overhead)\n\n\n\n\n\nWhy it matters\n\nCompression underpins storage (zip files, databases, backups) and transmission (HTTP gzip, messaging).\nSaves bandwidth and storage by removing redundancy.\nShows trade-offs: repetitive text compresses well, random text does not.\nFoundation for deeper algorithms: Huffman coding, LZ77, arithmetic coding.\n\n\n\nExercises\n\nCompute original and compressed sizes of \"hello hello hello\".\nCompare compression ratio for \"aaaaa\" vs \"abcde\".\nWrite a function that stores unique words in a dictionary and replaces repeats with indexes.\nExtend your dictionary to store \"banana\" as key 1 and \"apple\" as key 2. Compress \"banana apple banana\".\nMeasure zlib compression ratio for a large string of repeated \"test\".\nGenerate random strings of length 100 and test compression. What happens?\nExplain why compression sometimes increases size on short random inputs.\nCompare zlib compression with bz2 or lzma in Python.\nExplain how compression is used in network protocols like HTTP/2.\nPropose how compression could be applied in search engine indexes (from LAB 13).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  },
  {
    "objectID": "chapter_3.html#lab-15-injection-attack-simulation",
    "href": "chapter_3.html#lab-15-injection-attack-simulation",
    "title": "Chapter 3. Strings",
    "section": "LAB 15: Injection Attack Simulation",
    "text": "LAB 15: Injection Attack Simulation\n\nGoal\nDemonstrate how insecure string handling in queries can lead to injection attacks (e.g., SQL injection). Show how malicious inputs manipulate string-based queries, and how parameterized queries prevent this.\n\n\nSetup\nAny system that builds queries with string concatenation is vulnerable. We’ll simulate SQL injection in Python:\n\nUnsafe example: concatenating user input directly into a query string.\nSafe example: using parameterized queries (? or %s).\n\nWe’ll use sqlite3 for demonstration, since it’s lightweight and built into Python.\n\n\nStep-by-Step\n\nCreate a Sample Database\n\nTable: accounts(name TEXT, balance INT).\nInsert rows: \"Alice\", 100, \"Bob\", 50.\n\nUnsafe Query\n\nTake user input for name.\nBuild query with string concatenation:\nSELECT * FROM accounts WHERE name = 'USER_INPUT';\nInput \"Alice\" → works fine.\nInput \"' OR '1'='1\" → query returns all rows (attack).\n\nSimulated SQL Injection Attack\n\nInput \"; DROP TABLE accounts; --\" → destructive query.\nIn real systems, this would delete the table.\n\nSafe Query with Parameters\n\nUse cursor.execute(\"SELECT * FROM accounts WHERE name = ?\", (user_input,)).\nInput \"' OR '1'='1\" → safely treated as a literal string, no injection.\n\nDiscussion\n\nString concatenation = dangerous.\nParameterization = safe.\nThis applies to SQL, shell commands, HTML, and beyond.\n\n\n\n\nExample (Python)\nimport sqlite3\n\n# 1. Setup\nconn = sqlite3.connect(\":memory:\")\nc = conn.cursor()\nc.execute(\"CREATE TABLE accounts (name TEXT, balance INT)\")\nc.executemany(\"INSERT INTO accounts VALUES (?, ?)\", [(\"Alice\", 100), (\"Bob\", 50)])\nconn.commit()\n\n# 2. Unsafe query\ndef unsafe_query(user_input):\n    query = f\"SELECT * FROM accounts WHERE name = '{user_input}'\"\n    print(\"Unsafe query:\", query)\n    return list(c.execute(query))\n\n# Safe query\ndef safe_query(user_input):\n    query = \"SELECT * FROM accounts WHERE name = ?\"\n    print(\"Safe query:\", query)\n    return list(c.execute(query, (user_input,)))\n\n# Normal input\nprint(\"Normal unsafe:\", unsafe_query(\"Alice\"))\n\n# Injection attack\nprint(\"Injection unsafe:\", unsafe_query(\"' OR '1'='1\"))\n\n# Safe version prevents injection\nprint(\"Safe version:\", safe_query(\"' OR '1'='1\"))\n\n\nExpected Results\n\nNormal input \"Alice\" works fine.\nInjection input \"' OR '1'='1\" returns all rows in unsafe query.\nSafe query treats it literally → no injection.\n\n\n\n\nInput\nUnsafe Query Result\nSafe Query Result\n\n\n\n\n\"Alice\"\n[(\"Alice\", 100)]\n[(\"Alice\", 100)]\n\n\n\"' OR '1'='1\"\n[(\"Alice\",100),(\"Bob\",50)]\n[] (no match)\n\n\n\n\n\nWhy it matters\n\nSQL injection is one of the most critical web vulnerabilities (OWASP Top 10).\nAttackers can dump data, bypass authentication, or destroy databases.\nThe problem is not SQL itself but string misuse.\nFix: always use parameterized queries or ORMs with safe bindings.\n\n\n\nExercises\n\nWrite a query vulnerable to injection and test with \"Alice\".\nInject \"' OR '1'='1\" and explain why it returns all rows.\nTry input \"; DROP TABLE accounts; --\" and explain what would happen in a real DB.\nRewrite your code to use parameterized queries.\nTest injection again with safe queries and verify no effect.\nExtend this idea to shell commands (os.system) and simulate command injection.\nResearch one real-world breach caused by SQL injection.\nExplain why escaping input manually (e.g., replacing quotes) is insufficient.\nDesign input validation rules that complement parameterization.\nPropose a security guideline for handling strings in queries for your organization.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3. Strings</span>"
    ]
  }
]