[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Algorithms",
    "section": "",
    "text": "Content\nA Friendly Guide from Numbers to Neural Networks\n\nDownload PDF - print-ready\nDownload EPUB - e-reader friendly\nView LaTex - .tex source\nSource code (Github) - Markdown source\nRead on GitHub Pages - view online\n\nLicensed under CC BY-NC-SA 4.0.\n\nChapter 1. Foundations of Algorithms\n\n\nWhat Is an Algorithm?\n\n\n\nMeasuring Time and Space\n\n\n\nBig-O, Big-Theta, Big-Omega\n\n\n\nAlgorithmic Paradigms (Greedy, Divide and Conquer, DP)\n\n\n\nRecurrence Relations\n\n\n\nSearching Basics\n\n\n\nSorting Basics\n\n\n\nData Structures Overview\n\n\n\nGraphs and Trees Overview\n\n\n\nAlgorithm Design Patterns\n\n\n\n\nChapter 2. Sorting and Searching\n\n\nElementary Sorting (Bubble, Insertion, Selection)\n\n\n\nDivide-and-Conquer Sorting (Merge, Quick, Heap)\n\n\n\nCounting and Distribution Sorts (Counting, Radix, Bucket)\n\n\n\nHybrid Sorts (IntroSort, Timsort)\n\n\n\nSpecial Sorts (Cycle, Gnome, Comb, Pancake)\n\n\n\nLinear and Binary Search\n\n\n\nInterpolation and Exponential Search\n\n\n\nSelection Algorithms (Quickselect, Median of Medians)\n\n\n\nRange Searching and Nearest Neighbor\n\n\n\nSearch Optimizations and Variants\n\n\n\n\nChapter 3. Data Structures in Action\n\n\nArrays, Linked Lists, Stacks, Queues\n\n\n\nHash Tables and Variants (Cuckoo, Robin Hood, Consistent)\n\n\n\nHeaps (Binary, Fibonacci, Pairing)\n\n\n\nBalanced Trees (AVL, Red-Black, Splay, Treap)\n\n\n\nSegment Trees and Fenwick Trees\n\n\n\nDisjoint Set Union (Union-Find)\n\n\n\nProbabilistic Data Structures (Bloom, Count-Min, HyperLogLog)\n\n\n\nSkip Lists and B-Trees\n\n\n\nPersistent and Functional Data Structures\n\n\n\nAdvanced Trees and Range Queries\n\n\n\n\nChapter 4. Graph Algorithms\n\n\nTraversals (DFS, BFS, Iterative Deepening)\n\n\n\nStrongly Connected Components (Tarjan, Kosaraju)\n\n\n\nShortest Paths (Dijkstra, Bellman-Ford, A*, Johnson)\n\n\n\nShortest Path Variants (0–1 BFS, Bidirectional, Heuristic A*)\n\n\n\nMinimum Spanning Trees (Kruskal, Prim, Borůvka)\n\n\n\nFlows (Ford–Fulkerson, Edmonds–Karp, Dinic)\n\n\n\nCuts (Stoer–Wagner, Karger, Gomory–Hu)\n\n\n\nMatchings (Hopcroft–Karp, Hungarian, Blossom)\n\n\n\nTree Algorithms (LCA, HLD, Centroid Decomposition)\n\n\n\nAdvanced Graph Algorithms and Tricks\n\n\n\n\nChapter 5. Dynamic Programming\n\n\nDP Basics and State Transitions\n\n\n\nClassic Problems (Knapsack, Subset Sum, Coin Change)\n\n\n\nSequence Problems (LIS, LCS, Edit Distance)\n\n\n\nMatrix and Chain Problems\n\n\n\nBitmask DP and Traveling Salesman\n\n\n\nDigit DP and SOS DP\n\n\n\nDP Optimizations (Divide & Conquer, Convex Hull Trick, Knuth)\n\n\n\nTree DP and Rerooting\n\n\n\nDP Reconstruction and Traceback\n\n\n\nMeta-DP and Optimization Templates\n\n\n\n\nChapter 6. Mathematics for Algorithms\n\n\nNumber Theory (GCD, Modular Arithmetic, CRT)\n\n\n\nPrimality and Factorization (Miller–Rabin, Pollard Rho)\n\n\n\nCombinatorics (Permutations, Combinations, Subsets)\n\n\n\nProbability and Randomized Algorithms\n\n\n\nSieve Methods and Modular Math\n\n\n\nLinear Algebra (Gaussian Elimination, LU, SVD)\n\n\n\nFFT and NTT (Fast Transforms)\n\n\n\nNumerical Methods (Newton, Simpson, Runge–Kutta)\n\n\n\nMathematical Optimization (Simplex, Gradient, Convex)\n\n\n\nAlgebraic Tricks and Transform Techniques\n\n\n\n\nChapter 7. Strings and Text Algorithms\n\n\nString Matching (KMP, Z, Rabin–Karp, Boyer–Moore)\n\n\n\nMulti-Pattern Search (Aho–Corasick)\n\n\n\nSuffix Structures (Suffix Array, Suffix Tree, LCP)\n\n\n\nPalindromes and Periodicity (Manacher)\n\n\n\nEdit Distance and Alignment\n\n\n\nCompression (Huffman, Arithmetic, LZ77, BWT)\n\n\n\nCryptographic Hashes and Checksums\n\n\n\nApproximate and Streaming Matching\n\n\n\nBioinformatics Alignment (Needleman–Wunsch, Smith–Waterman)\n\n\n\nText Indexing and Search Structures\n\n\n\n\nChapter 8. Geometry, Graphics, and Spatial Algorithms\n\n\nConvex Hull (Graham, Andrew, Chan)\n\n\n\nClosest Pair and Segment Intersection\n\n\n\nLine Sweep and Plane Sweep Algorithms\n\n\n\nDelaunay and Voronoi Diagrams\n\n\n\nPoint in Polygon and Polygon Triangulation\n\n\n\nSpatial Data Structures (KD, R-tree)\n\n\n\nRasterization and Scanline Techniques\n\n\n\nComputer Vision (Canny, Hough, SIFT)\n\n\n\nPathfinding in Space (A*, RRT, PRM)\n\n\n\nComputational Geometry Variants and Applications\n\n\n\n\nChapter 9. Systems, Databases, and Distributed Algorithms\n\n\nConcurrency Control (2PL, MVCC, OCC)\n\n\n\nLogging, Recovery, and Commit Protocols\n\n\n\nScheduling (Round Robin, EDF, Rate-Monotonic)\n\n\n\nCaching and Replacement (LRU, LFU, CLOCK)\n\n\n\nNetworking (Routing, Congestion Control)\n\n\n\nDistributed Consensus (Paxos, Raft, PBFT)\n\n\n\nLoad Balancing and Rate Limiting\n\n\n\nSearch and Indexing (Inverted, BM25, WAND)\n\n\n\nCompression and Encoding in Systems\n\n\n\nFault Tolerance and Replication\n\n\n\n\nChapter 10. AI, ML, and Optimization\n\n\nClassical ML (k-means, Naive Bayes, SVM, Decision Trees)\n\n\n\nEnsemble Methods (Bagging, Boosting, Random Forests)\n\n\n\nGradient Methods (SGD, Adam, RMSProp)\n\n\n\nDeep Learning (Backpropagation, Dropout, Normalization)\n\n\n\nSequence Models (Viterbi, Beam Search, CTC)\n\n\n\nMetaheuristics (GA, SA, PSO, ACO)\n\n\n\nReinforcement Learning (Q-learning, Policy Gradients)\n\n\n\nApproximation and Online Algorithms\n\n\n\nFairness, Causal Inference, and Robust Optimization\n\n\n\nAI Planning, Search, and Learning Systems",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Content</span>"
    ]
  },
  {
    "objectID": "books/en-us/cheatsheet.html",
    "href": "books/en-us/cheatsheet.html",
    "title": "The Cheatsheet",
    "section": "",
    "text": "Page 1. Big Picture and Complexity\nA quick reference for understanding algorithms, efficiency, and growth rates. Keep this sheet beside you as you read or code.\n\nWhat Is an Algorithm?\nAn algorithm is a clear, step-by-step process that solves a problem.\n\n\n\nProperty\nDescription\n\n\n\n\nPrecise\nEach step is unambiguous\n\n\nFinite\nMust stop after a certain number of steps\n\n\nEffective\nEvery step is doable by machine or human\n\n\nDeterministic\nSame input, same output (usually)\n\n\n\nThink of it like a recipe:\n\nInput: ingredients\nSteps: instructions\nOutput: final dish\n\n\n\nCore Qualities\n\n\n\nConcept\nQuestion to Ask\n\n\n\n\nCorrectness\nDoes it always solve the problem\n\n\nTermination\nDoes it eventually stop\n\n\nComplexity\nHow much time and space it needs\n\n\nClarity\nIs it easy to understand and implement\n\n\n\n\n\nWhy Complexity Matters\nDifferent algorithms grow differently as input size \\(n\\) increases.\n\n\n\nGrowth Rate\nExample Algorithm\nEffect When \\(n\\) Doubles\n\n\n\n\n\\(O(1)\\)\nHash lookup\nNo change\n\n\n\\(O(\\log n)\\)\nBinary search\nSlight increase\n\n\n\\(O(n)\\)\nLinear scan\nDoubled\n\n\n\\(O(n\\log n)\\)\nMerge sort\nSlightly more than 2×\n\n\n\\(O(n^2)\\)\nBubble sort\nQuadrupled\n\n\n\\(O(2^n)\\)\nSubset generation\nExplodes\n\n\n\\(O(n!)\\)\nBrute-force permutations\nUnusable beyond \\(n=10\\)\n\n\n\n\n\nMeasuring Time and Space\n\n\n\n\n\n\n\n\nMeasure\nMeaning\nExample\n\n\n\n\nTime Complexity\nNumber of operations\nLoop from 1 to \\(n\\): \\(O(n)\\)\n\n\nSpace Complexity\nMemory usage (stack, heap, data structures)\nRecursive call depth: \\(O(n)\\)\n\n\n\nSimple rules:\n\nSequential steps: sum of costs\nNested loops: product of sizes\nRecursion: use recurrence relations\n\n\n\nCommon Patterns\n\n\n\nPattern\nCost Formula\nComplexity\n\n\n\n\nSingle Loop (1 to \\(n\\))\n\\(T(n) = n\\)\n\\(O(n)\\)\n\n\nNested Loops (\\(n \\times n\\))\n\\(T(n) = n^2\\)\n\\(O(n^2)\\)\n\n\nHalving Each Step\n\\(T(n) = \\log_2 n\\)\n\\(O(\\log n)\\)\n\n\nDivide and Conquer (2 halves)\n\\(T(n) = 2T(n/2) + n\\)\n\\(O(n\\log n)\\)\n\n\n\n\n\nDoubling Rule\nRun algorithm for \\(n\\) and \\(2n\\):\n\n\n\nObservation\nLikely Complexity\n\n\n\n\nConstant time\n\\(O(1)\\)\n\n\nTime doubles\n\\(O(n)\\)\n\n\nTime quadruples\n\\(O(n^2)\\)\n\n\nTime × log factor\n\\(O(n\\log n)\\)\n\n\n\n\n\nTiny Code: Binary Search\ndef binary_search(arr, x):\n    lo, hi = 0, len(arr) - 1\n    while lo &lt;= hi:\n        mid = (lo + hi) // 2\n        if arr[mid] == x:\n            return mid\n        elif arr[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid - 1\n    return -1\nComplexity: \\[T(n) = T(n/2) + 1 \\Rightarrow O(\\log n)\\]\n\n\nCommon Pitfalls\n\n\n\n\n\n\n\nIssue\nTip\n\n\n\n\nOff-by-one error\nCheck loop bounds carefully\n\n\nInfinite loop\nEnsure termination condition is reachable\n\n\nMidpoint overflow (C/C++)\nUse mid = lo + (hi - lo) / 2\n\n\nUnsorted data in search\nBinary search only works on sorted input\n\n\n\n\n\nQuick Growth Summary\n\n\n\nType\nFormula Example\nDescription\n\n\n\n\nConstant\n\\(1\\)\nFixed time\n\n\nLogarithmic\n\\(\\log n\\)\nDivide each time\n\n\nLinear\n\\(n\\)\nStep through all items\n\n\nLinearithmic\n\\(n \\log n\\)\nSort-like complexity\n\n\nQuadratic\n\\(n^2\\)\nDouble loop\n\n\nCubic\n\\(n^3\\)\nTriple nested loops\n\n\nExponential\n\\(2^n\\)\nAll subsets\n\n\nFactorial\n\\(n!\\)\nAll permutations\n\n\n\n\n\nSimple Rule of Thumb\nTrace small examples by hand. Count steps, memory, and recursion depth. You’ll see how growth behaves before running code.\n\n\n\nPage 2. Recurrences and Master Theorem\nThis page helps you break down recursive algorithms and estimate their runtime using recurrences.\n\nWhat Is a Recurrence?\nA recurrence relation expresses a problem’s cost \\(T(n)\\) in terms of smaller subproblems.\nTypical structure:\n\\[\nT(n) = a , T\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nwhere:\n\n\\(a\\) = number of subproblems\n\\(b\\) = factor by which input shrinks\n\\(f(n)\\) = extra work per call (merge, combine, etc.)\n\n\n\nCommon Recurrences\n\n\n\nAlgorithm\nRecurrence Form\nSolution\n\n\n\n\nBinary Search\n\\(T(n)=T(n/2)+1\\)\n\\(O(\\log n)\\)\n\n\nMerge Sort\n\\(T(n)=2T(n/2)+n\\)\n\\(O(n\\log n)\\)\n\n\nQuick Sort (avg)\n\\(T(n)=2T(n/2)+O(n)\\)\n\\(O(n\\log n)\\)\n\n\nQuick Sort (worst)\n\\(T(n)=T(n-1)+O(n)\\)\n\\(O(n^2)\\)\n\n\nMatrix Multiply\n\\(T(n)=8T(n/2)+O(n^2)\\)\n\\(O(n^3)\\)\n\n\nKaratsuba\n\\(T(n)=3T(n/2)+O(n)\\)\n\\(O(n^{\\log_2 3})\\)\n\n\n\n\n\nSolving Recurrences\nThere are several methods to solve them:\n\n\n\n\n\n\n\n\nMethod\nDescription\nBest For\n\n\n\n\nIteration\nExpand step by step\nSimple recurrences\n\n\nSubstitution\nGuess and prove with induction\nVerification\n\n\nRecursion Tree\nVisualize total work per level\nDivide and conquer\n\n\nMaster Theorem\nShortcut for \\(T(n)=aT(n/b)+f(n)\\)\nStandard forms\n\n\n\n\n\nThe Master Theorem\nGiven \\[T(n) = aT(n/b) + f(n)\\]\nLet \\[n^{\\log_b a}\\] be the “critical term”\n\n\n\n\n\n\n\n\nCase\nCondition\nResult\n\n\n\n\n1\nIf \\(f(n) = O(n^{\\log_b a - \\varepsilon})\\)\n\\(T(n) = \\Theta(n^{\\log_b a})\\)\n\n\n2\nIf \\(f(n) = \\Theta(n^{\\log_b a}\\log^k n)\\)\n\\(T(n) = \\Theta(n^{\\log_b a}\\log^{k+1} n)\\)\n\n\n3\nIf \\(f(n) = \\Omega(n^{\\log_b a + \\varepsilon})\\) and regularity holds\n\\(T(n) = \\Theta(f(n))\\)\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\n\\(a\\)\n\\(b\\)\n\\(f(n)\\)\nCase\n\\(T(n)\\)\n\n\n\n\nMerge Sort\n2\n2\n\\(n\\)\n2\n\\(\\Theta(n\\log n)\\)\n\n\nBinary Search\n1\n2\n\\(1\\)\n1\n\\(\\Theta(\\log n)\\)\n\n\nStrassen Multiply\n7\n2\n\\(n^2\\)\n2\n\\(\\Theta(n^{\\log_2 7})\\)\n\n\nQuick Sort (avg)\n2\n2\n\\(n\\)\n2\n\\(\\Theta(n\\log n)\\)\n\n\n\n\n\nRecursion Tree Visualization\nBreak cost into levels:\nExample: \\(T(n)=2T(n/2)+n\\)\n\n\n\nLevel\n#Nodes\nWork per Node\nTotal Work\n\n\n\n\n0\n1\n\\(n\\)\n\\(n\\)\n\n\n1\n2\n\\(n/2\\)\n\\(n\\)\n\n\n2\n4\n\\(n/4\\)\n\\(n\\)\n\n\n…\n…\n…\n…\n\n\n\nSum across \\(\\log_2 n\\) levels:\n\\[T(n) = n \\log_2 n\\]\n\n\nTiny Code: Fast Exponentiation\nCompute \\(a^n\\) efficiently.\ndef power(a, n):\n    res = 1\n    while n &gt; 0:\n        if n % 2 == 1:\n            res *= a\n        a *= a\n        n //= 2\n    return res\nRecurrence:\n\\[T(n) = T(n/2) + O(1) \\Rightarrow O(\\log n)\\]\n\n\nIteration Method Example\nSolve \\(T(n)=T(n/2)+n\\)\nExpand:\n\\[\n\\begin{aligned}\nT(n) &= T(n/2) + n \\\n&= T(n/4) + n/2 + n \\\n&= T(n/8) + n/4 + n/2 + n \\\n&= \\ldots + n(1 + 1/2 + 1/4 + \\ldots) \\\n&= O(n)\n\\end{aligned}\n\\]\n\n\nCommon Forms\n\n\n\nForm\nResult\n\n\n\n\n\\(T(n)=T(n-1)+O(1)\\)\n\\(O(n)\\)\n\n\n\\(T(n)=T(n/2)+O(1)\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n)=2T(n/2)+O(1)\\)\n\\(O(n)\\)\n\n\n\\(T(n)=2T(n/2)+O(n)\\)\n\\(O(n\\log n)\\)\n\n\n\\(T(n)=T(n/2)+O(n)\\)\n\\(O(n)\\)\n\n\n\n\n\nQuick Checklist\n\nIdentify \\(a\\), \\(b\\), and \\(f(n)\\)\nCompare \\(f(n)\\) to \\(n^{\\log_b a}\\)\nApply correct case\nConfirm assumptions (regularity)\nState final complexity\n\nUnderstanding recurrences helps you estimate performance before coding. Always look for subproblem count, size, and merge cost.\n\n\n\nPage 3. Sorting at a Glance\nSorting is one of the most common algorithmic tasks. This page helps you quickly compare sorting methods, their complexity, stability, and when to use them.\n\nWhy Sorting Matters\nSorting organizes data so that searches, merges, and analyses become efficient. Many problems become simpler once the input is sorted.\n\n\nQuick Comparison Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nSpace\nStable\nIn-Place\nNotes\n\n\n\n\nBubble Sort\n\\(O(n)\\)\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(1)\\)\nYes\nYes\nSimple, educational\n\n\nSelection Sort\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(1)\\)\nNo\nYes\nFew swaps\n\n\nInsertion Sort\n\\(O(n)\\)\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(1)\\)\nYes\nYes\nGreat for small/partial sort\n\n\nMerge Sort\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n)\\)\nYes\nNo\nStable, divide and conquer\n\n\nQuick Sort\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n^2)\\)\n\\(O(\\log n)\\)\nNo\nYes\nFast average, in place\n\n\nHeap Sort\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(1)\\)\nNo\nYes\nNot stable\n\n\nCounting Sort\n\\(O(n+k)\\)\n\\(O(n+k)\\)\n\\(O(n+k)\\)\n\\(O(n+k)\\)\nYes\nNo\nInteger keys only\n\n\nRadix Sort\n\\(O(d(n+k))\\)\n\\(O(d(n+k))\\)\n\\(O(d(n+k))\\)\n\\(O(n+k)\\)\nYes\nNo\nSort by digits\n\n\nBucket Sort\n\\(O(n+k)\\)\n\\(O(n+k)\\)\n\\(O(n^2)\\)\n\\(O(n)\\)\nYes\nNo\nUniform distribution needed\n\n\n\n\n\nChoosing a Sorting Algorithm\n\n\n\nSituation\nBest Choice\n\n\n\n\nSmall array or nearly sorted data\nInsertion Sort\n\n\nStable required, general case\nMerge Sort or Timsort\n\n\nIn-place and fast on average\nQuick Sort\n\n\nGuarantee worst-case \\(O(n\\log n)\\)\nHeap Sort\n\n\nSmall integer keys or limited range\nCounting or Radix\n\n\nExternal sorting (large data)\nExternal Merge Sort\n\n\n\n\n\nTiny Code: Insertion Sort\nSimple and intuitive for beginners.\ndef insertion_sort(a):\n    for i in range(1, len(a)):\n        key = a[i]\n        j = i - 1\n        while j &gt;= 0 and a[j] &gt; key:\n            a[j + 1] = a[j]\n            j -= 1\n        a[j + 1] = key\n    return a\nComplexity: \\[T(n) = O(n^2)\\] average, \\[O(n)\\] best (already sorted)\n\n\nDivide and Conquer Sorts\n\nMerge Sort\nSplits list, sorts halves, merges results.\nRecurrence: \\[T(n) = 2T(n/2) + O(n) = O(n\\log n)\\]\nTiny Code:\ndef merge_sort(a):\n    if len(a) &lt;= 1:\n        return a\n    mid = len(a)//2\n    L = merge_sort(a[:mid])\n    R = merge_sort(a[mid:])\n    i = j = 0\n    res = []\n    while i &lt; len(L) and j &lt; len(R):\n        if L[i] &lt;= R[j]:\n            res.append(L[i]); i += 1\n        else:\n            res.append(R[j]); j += 1\n    res.extend(L[i:]); res.extend(R[j:])\n    return res\n\n\nQuick Sort\nPick pivot, partition, sort subarrays.\nRecurrence: \\[T(n) = T(k) + T(n-k-1) + O(n)\\] Average case: \\[O(n\\log n)\\] Worst case: \\[O(n^2)\\]\nTiny Code:\ndef quick_sort(a):\n    if len(a) &lt;= 1:\n        return a\n    pivot = a[len(a)//2]\n    left  = [x for x in a if x &lt; pivot]\n    mid   = [x for x in a if x == pivot]\n    right = [x for x in a if x &gt; pivot]\n    return quick_sort(left) + mid + quick_sort(right)\n\n\n\nStable vs Unstable\n\n\n\n\n\n\n\n\nProperty\nDescription\nExample\n\n\n\n\nStable\nEqual elements keep original order\nMerge Sort, Insertion\n\n\nUnstable\nMay reorder equal elements\nQuick, Heap\n\n\n\n\n\nVisualization Tips\n\n\n\nPattern\nDescription\n\n\n\n\nBubble\nCompare and swap adjacent\n\n\nSelection\nSelect min each pass\n\n\nInsertion\nGrow sorted region step by step\n\n\nMerge\nDivide, conquer, merge\n\n\nQuick\nPartition and recurse\n\n\nHeap\nBuild heap, extract repeatedly\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\n\nType\nCategory\nComplexity\nStable\nSpace\n\n\n\n\nSimple\nBubble, Selection\n\\(O(n^2)\\)\nVaries\n\\(O(1)\\)\n\n\nInsertion\nIncremental\n\\(O(n^2)\\)\nYes\n\\(O(1)\\)\n\n\nDivide/Conquer\nMerge, Quick\n\\(O(n\\log n)\\)\nMerge yes\nMerge no\n\n\nDistribution\nCounting, Radix\n\\(O(n+k)\\)\nYes\n\\(O(n+k)\\)\n\n\nHybrid\nTimsort, IntroSort\n\\(O(n\\log n)\\)\nYes\nVaries\n\n\n\nWhen in doubt, start with Timsort (Python) or std::sort (C++) which adapt dynamically.\n\n\n\nPage 4. Searching and Selection\nSearching means finding what you need from a collection. Selection means picking specific elements such as the smallest, largest, or k-th element. This page summarizes both.\n\nSearching Basics\n\n\n\n\n\n\n\n\n\nType\nDescription\nData Requirement\nComplexity\n\n\n\n\nLinear Search\nCheck one by one\nNone\n\\(O(n)\\)\n\n\nBinary Search\nDivide range by 2 each step\nSorted\n\\(O(\\log n)\\)\n\n\nJump Search\nSkip ahead fixed steps\nSorted\n\\(O(\\sqrt n)\\)\n\n\nInterpolation\nGuess position based on value\nSorted, uniform\n\\(O(\\log\\log n)\\) avg\n\n\nExponential\nExpand window, then binary search\nSorted\n\\(O(\\log n)\\)\n\n\n\n\n\nLinear Search\nSimple but slow for large inputs.\ndef linear_search(a, x):\n    for i, v in enumerate(a):\n        if v == x:\n            return i\n    return -1\nComplexity: \\[T(n) = O(n)\\]\n\n\nBinary Search\nFast on sorted lists.\ndef binary_search(a, x):\n    lo, hi = 0, len(a) - 1\n    while lo &lt;= hi:\n        mid = (lo + hi) // 2\n        if a[mid] == x:\n            return mid\n        elif a[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid - 1\n    return -1\nComplexity: \\[T(n) = T(n/2) + 1 \\Rightarrow O(\\log n)\\]\n\n\nBinary Search Variants\n\n\n\n\n\n\n\n\nVariant\nGoal\nReturn Value\n\n\n\n\nLower Bound\nFirst index where \\(a[i] \\ge x\\)\nPosition of first ≥ x\n\n\nUpper Bound\nFirst index where \\(a[i] &gt; x\\)\nPosition of first &gt; x\n\n\nCount Range\nupper_bound - lower_bound\nCount of \\(x\\) in sorted array\n\n\n\n\n\nCommon Binary Search Pitfalls\n\n\n\nProblem\nFix\n\n\n\n\nInfinite loop\nUpdate bounds correctly\n\n\nOff-by-one\nCheck mid inclusion carefully\n\n\nUnsuitable for unsorted data\nSort or use hash-based search\n\n\nOverflow (C/C++)\nmid = lo + (hi - lo) / 2\n\n\n\n\n\nExponential Search\nUsed for unbounded or large sorted lists.\n\nCheck positions \\(1, 2, 4, 8, ...\\) until \\(a[i] \\ge x\\)\nBinary search in last found interval\n\nComplexity: \\[O(\\log n)\\]\n\n\nSelection Problems\nFind the \\(k\\)-th smallest or largest element.\n\n\n\n\n\n\n\n\n\nTask\nExample Use Case\nAlgorithm\nComplexity\n\n\n\n\nMin / Max\nSmallest / largest element\nLinear Scan\n\\(O(n)\\)\n\n\nk-th Smallest\nOrder statistic\nQuickselect\nAvg \\(O(n)\\)\n\n\nMedian\nMiddle element\nQuickselect\nAvg \\(O(n)\\)\n\n\nTop-k Elements\nPartial sort\nHeap / Partition\n\\(O(n\\log k)\\)\n\n\nMedian of Medians\nWorst-case linear selection\nDeterministic\n\\(O(n)\\)\n\n\n\n\n\nTiny Code: Quickselect (k-th smallest)\nimport random\n\ndef quickselect(a, k):\n    if len(a) == 1:\n        return a[0]\n    pivot = random.choice(a)\n    left  = [x for x in a if x &lt; pivot]\n    mid   = [x for x in a if x == pivot]\n    right = [x for x in a if x &gt; pivot]\n\n    if k &lt; len(left):\n        return quickselect(left, k)\n    elif k &lt; len(left) + len(mid):\n        return pivot\n    else:\n        return quickselect(right, k - len(left) - len(mid))\nComplexity: Average \\(O(n)\\), Worst \\(O(n^2)\\)\n\n\nTiny Code: Lower Bound\ndef lower_bound(a, x):\n    lo, hi = 0, len(a)\n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if a[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid\n    return lo\n\n\nHash-Based Searching\nWhen order does not matter, hashing gives near constant lookup.\n\n\n\nOperation\nAverage\nWorst\n\n\n\n\nInsert\n\\(O(1)\\)\n\\(O(n)\\)\n\n\nSearch\n\\(O(1)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(1)\\)\n\\(O(n)\\)\n\n\n\nBest for large, unsorted collections.\n\n\nSummary Table\n\n\n\nScenario\nRecommended Approach\nComplexity\n\n\n\n\nSmall array\nLinear Search\n\\(O(n)\\)\n\n\nLarge, sorted array\nBinary Search\n\\(O(\\log n)\\)\n\n\nUnbounded range\nExponential Search\n\\(O(\\log n)\\)\n\n\nNeed k-th smallest element\nQuickselect\nAvg \\(O(n)\\)\n\n\nMany lookups\nHash Table\nAvg \\(O(1)\\)\n\n\n\n\n\nQuick Tips\n\nAlways check whether data is sorted before applying binary search.\nQuickselect is great when you only need the k-th element, not a full sort.\nUse hash maps for fast lookups on unsorted data.\n\n\n\n\nPage 5. Core Data Structures\nData structures organize data for efficient access and modification. Choosing the right one often makes an algorithm simple and fast.\n\nArrays and Lists\n\n\n\n\n\n\n\n\n\n\n\n\nStructure\nAccess\nSearch\nInsert End\nInsert Middle\nDelete\nNotes\n\n\n\n\nStatic Array\n\\(O(1)\\)\n\\(O(n)\\)\nN/A\n\\(O(n)\\)\n\\(O(n)\\)\nFixed size\n\n\nDynamic Array\n\\(O(1)\\)\n\\(O(n)\\)\nAmortized \\(O(1)\\)\n\\(O(n)\\)\n\\(O(n)\\)\nAuto-resizing\n\n\nLinked List (S)\n\\(O(n)\\)\n\\(O(n)\\)\n\\(O(1)\\) head\n\\(O(1)\\) if node known\n\\(O(1)\\) if node known\nSequential access\n\n\nLinked List (D)\n\\(O(n)\\)\n\\(O(n)\\)\n\\(O(1)\\) head/tail\n\\(O(1)\\) if node known\n\\(O(1)\\) if node known\nTwo-way traversal\n\n\n\n\nSingly linked lists: next pointer only\nDoubly linked lists: next and prev pointers\nDynamic arrays use doubling to grow capacity\n\n\n\nTiny Code: Dynamic Array Resize (Python-like)\ndef resize(arr, new_cap):\n    new = [None] * new_cap\n    for i in range(len(arr)):\n        new[i] = arr[i]\n    return new\nDoubling capacity keeps amortized append \\(O(1)\\).\n\n\nStacks and Queues\n\n\n\nStructure\nPush\nPop\nPeek\nNotes\n\n\n\n\nStack (LIFO)\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\nUndo operations, recursion\n\n\nQueue (FIFO)\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\nScheduling, BFS\n\n\nDeque\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\nInsert/remove both ends\n\n\n\n\n\nTiny Code: Stack\nstack = []\nstack.append(x)   # push\nx = stack.pop()   # pop\n\n\nTiny Code: Queue\nfrom collections import deque\n\nq = deque()\nq.append(x)   # enqueue\nx = q.popleft()  # dequeue\n\n\nPriority Queue (Heap)\nStores elements so the smallest (or largest) is always on top.\n\n\n\nOperation\nComplexity\n\n\n\n\nInsert\n\\(O(\\log n)\\)\n\n\nExtract min\n\\(O(\\log n)\\)\n\n\nPeek min\n\\(O(1)\\)\n\n\nBuild heap\n\\(O(n)\\)\n\n\n\nTiny Code:\nimport heapq\nheap = []\nheapq.heappush(heap, value)\nx = heapq.heappop(heap)\nHeaps are used in Dijkstra, Prim, and scheduling.\n\n\nHash Tables\n\n\n\nOperation\nAverage\nWorst\nNotes\n\n\n\n\nInsert\n\\(O(1)\\)\n\\(O(n)\\)\nHash collisions increase cost\n\n\nSearch\n\\(O(1)\\)\n\\(O(n)\\)\nGood hash + low load factor helps\n\n\nDelete\n\\(O(1)\\)\n\\(O(n)\\)\nUsually open addressing or chaining\n\n\n\nKey ideas:\n\nCompute index using hash function: index = hash(key) % capacity\nResolve collisions by chaining or probing\n\n\n\nTiny Code: Hash Map (Simplified)\ntable = [[] for _ in range(8)]\ndef put(key, value):\n    i = hash(key) % len(table)\n    for kv in table[i]:\n        if kv[0] == key:\n            kv[1] = value\n            return\n    table[i].append([key, value])\n\n\nSets\nA hash-based collection of unique elements.\n\n\n\nOperation\nAverage Complexity\n\n\n\n\nAdd\n\\(O(1)\\)\n\n\nSearch\n\\(O(1)\\)\n\n\nRemove\n\\(O(1)\\)\n\n\n\nUsed for membership checks and duplicates removal.\n\n\nUnion-Find (Disjoint Set)\nKeeps track of connected components. Two main operations:\n\nfind(x): get representative of x\nunion(a,b): merge sets of a and b\n\nWith path compression + union by rank → nearly \\(O(1)\\).\nTiny Code:\nclass DSU:\n    def __init__(self, n):\n        self.p = list(range(n))\n        self.r = [0]*n\n    def find(self, x):\n        if self.p[x] != x:\n            self.p[x] = self.find(self.p[x])\n        return self.p[x]\n    def union(self, a, b):\n        ra, rb = self.find(a), self.find(b)\n        if ra == rb: return\n        if self.r[ra] &lt; self.r[rb]: ra, rb = rb, ra\n        self.p[rb] = ra\n        if self.r[ra] == self.r[rb]:\n            self.r[ra] += 1\n\n\nSummary Table\n\n\n\nCategory\nStructure\nUse Case\n\n\n\n\nSequence\nArray, List\nOrdered data\n\n\nLIFO/FIFO\nStack, Queue\nRecursion, scheduling\n\n\nPriority\nHeap\nBest-first selection, PQ problems\n\n\nHash-based\nHash Table, Set\nFast lookups, uniqueness\n\n\nConnectivity\nUnion-Find\nGraph components, clustering\n\n\n\n\n\nQuick Tips\n\nChoose array when random access matters.\nChoose list when insertions/deletions frequent.\nChoose stack or queue for control flow.\nChoose heap for priority.\nChoose hash table for constant lookups.\nChoose DSU for disjoint sets or graph merging.\n\n\n\n\nPage 6. Graphs Quick Use\nGraphs model connections between objects. They appear everywhere: maps, networks, dependencies, and systems. This page gives you a compact view of common graph algorithms.\n\nGraph Basics\nA graph has vertices (nodes) and edges (connections).\n\n\n\nType\nDescription\n\n\n\n\nUndirected\nEdges go both ways\n\n\nDirected (Digraph)\nEdges have direction\n\n\nWeighted\nEdges carry cost or distance\n\n\nUnweighted\nAll edges cost 1\n\n\n\n\n\nRepresentations\n\n\n\n\n\n\n\n\n\nRepresentation\nSpace\nBest For\nNotes\n\n\n\n\nAdjacency List\n\\(O(V+E)\\)\nSparse graphs\nCommon in practice\n\n\nAdjacency Matrix\n\\(O(V^2)\\)\nDense graphs\nConstant-time edge lookup\n\n\nEdge List\n\\(O(E)\\)\nEdge-based algorithms\nEasy to iterate over edges\n\n\n\nAdjacency List Example (Python):\ngraph = {\n    0: [(1, 2), (2, 5)],\n    1: [(2, 1)],\n    2: []\n}\nEach tuple (neighbor, weight) represents an edge.\n\n\nTraversals\n\nBreadth-First Search (BFS)\nVisits layer by layer (good for shortest paths in unweighted graphs).\nfrom collections import deque\ndef bfs(adj, s):\n    dist = {s: 0}\n    q = deque([s])\n    while q:\n        u = q.popleft()\n        for v in adj[u]:\n            if v not in dist:\n                dist[v] = dist[u] + 1\n                q.append(v)\n    return dist\nComplexity: \\(O(V+E)\\)\n\n\nDepth-First Search (DFS)\nExplores deeply before backtracking.\ndef dfs(adj, u, visited):\n    visited.add(u)\n    for v in adj[u]:\n        if v not in visited:\n            dfs(adj, v, visited)\nComplexity: \\(O(V+E)\\)\n\n\n\nShortest Path Algorithms\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorks On\nNegative Edges\nComplexity\nNotes\n\n\n\n\nBFS\nUnweighted\nNo\n\\(O(V+E)\\)\nShortest hops\n\n\nDijkstra\nWeighted (nonneg)\nNo\n\\(O((V+E)\\log V)\\)\nUses priority queue\n\n\nBellman-Ford\nWeighted\nYes\n\\(O(VE)\\)\nDetects negative cycles\n\n\nFloyd-Warshall\nAll pairs\nYes\n\\(O(V^3)\\)\nDP approach\n\n\n\n\n\nTiny Code: Dijkstra’s Algorithm\nimport heapq\n\ndef dijkstra(adj, s):\n    INF = 1018\n    dist = [INF] * len(adj)\n    dist[s] = 0\n    pq = [(0, s)]\n    while pq:\n        d, u = heapq.heappop(pq)\n        if d != dist[u]: \n            continue\n        for v, w in adj[u]:\n            nd = d + w\n            if nd &lt; dist[v]:\n                dist[v] = nd\n                heapq.heappush(pq, (nd, v))\n    return dist\n\n\nTopological Sort (DAGs only)\nOrders nodes so every edge \\((u,v)\\) goes from earlier to later.\n\n\n\nMethod\nIdea\nComplexity\n\n\n\n\nDFS-based\nPost-order stack reversal\n\\(O(V+E)\\)\n\n\nKahn’s Algo\nRemove nodes with indegree 0\n\\(O(V+E)\\)\n\n\n\n\n\nMinimum Spanning Tree (MST)\nConnect all nodes with minimum total weight.\n\n\n\n\n\n\n\n\n\nAlgorithm\nIdea\nComplexity\nNotes\n\n\n\n\nKruskal\nSort edges, use Union-Find\n\\(O(E\\log E)\\)\nWorks well with edge list\n\n\nPrim\nGrow tree using PQ\n\\(O(E\\log V)\\)\nStarts from any vertex\n\n\n\n\n\nTiny Code: Kruskal MST\ndef kruskal(edges, n):\n    parent = list(range(n))\n    def find(x):\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n    res = 0\n    for w, u, v in sorted(edges):\n        ru, rv = find(u), find(v)\n        if ru != rv:\n            res += w\n            parent[rv] = ru\n    return res\n\n\nStrongly Connected Components (SCC)\nSubsets where every node can reach every other. Use Kosaraju or Tarjan algorithm, both \\(O(V+E)\\).\n\n\nCycle Detection\n\n\n\nGraph Type\nMethod\nNotes\n\n\n\n\nUndirected\nDFS with parent\nEdge to non-parent visited\n\n\nDirected\nDFS with color/state\nBack edge found = cycle\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nTask\nAlgorithm\nComplexity\nNotes\n\n\n\n\nVisit all nodes\nDFS / BFS\n\\(O(V+E)\\)\nTraversal\n\n\nShortest path (unweighted)\nBFS\n\\(O(V+E)\\)\nCounts edges\n\n\nShortest path (weighted)\nDijkstra\n\\(O(E\\log V)\\)\nNo negative weights\n\n\nNegative edges allowed\nBellman-Ford\n\\(O(VE)\\)\nDetects negative cycles\n\n\nAll-pairs shortest path\nFloyd-Warshall\n\\(O(V^3)\\)\nDP matrix\n\n\nMST\nKruskal / Prim\n\\(O(E\\log V)\\)\nMinimal connection cost\n\n\nDAG order\nTopological Sort\n\\(O(V+E)\\)\nOnly for DAGs\n\n\n\n\n\nQuick Tips\n\nUse BFS for shortest path in unweighted graphs.\nUse Dijkstra if weights are nonnegative.\nUse Union-Find for Kruskal MST.\nUse Topological Sort for dependency resolution.\nAlways check for negative edges before using Dijkstra.\n\n\n\n\nPage 7. Dynamic Programming Quick Use\nDynamic Programming (DP) is about solving big problems by breaking them into overlapping subproblems and reusing their solutions. This page helps you see patterns quickly.\n\nWhen to Use DP\nYou can usually apply DP if:\n\n\n\nSymptom\nMeaning\n\n\n\n\nOptimal Substructure\nBest solution uses best of subparts\n\n\nOverlapping Subproblems\nSame subresults appear again\n\n\nDecision + Recurrence\nState transitions can be defined\n\n\n\n\n\nDP Styles\n\n\n\n\n\n\n\n\nStyle\nDescription\nExample\n\n\n\n\nTop-down (Memo)\nRecursion + cache results\nFibonacci with memoization\n\n\nBottom-up (Tabular)\nIterative fill table\nKnapsack table\n\n\nSpace-optimized\nReuse previous row/state\nRolling arrays\n\n\n\n\n\nFibonacci Example\nRecurrence: \\[F(n)=F(n-1)+F(n-2),\\quad F(0)=0,F(1)=1\\]\n\nTop-down (Memoization)\ndef fib(n, memo={}):\n    if n &lt;= 1:\n        return n\n    if n not in memo:\n        memo[n] = fib(n-1, memo) + fib(n-2, memo)\n    return memo[n]\n\n\nBottom-up (Tabulation)\ndef fib(n):\n    dp = [0, 1]\n    for i in range(2, n + 1):\n        dp.append(dp[i-1] + dp[i-2])\n    return dp[n]\n\n\n\nSteps to Solve DP Problems\n\nDefine State Example: \\(dp[i]\\) = best answer for first \\(i\\) items\nDefine Transition Example: \\(dp[i]=\\max(dp[i-1], value[i]+dp[i-weight[i]])\\)\nSet Base Cases Example: \\(dp[0]=0\\)\nChoose Order Bottom-up or Top-down\nReturn Answer Often \\(dp[n]\\) or \\(dp[target]\\)\n\n\n\nCommon DP Categories\n\n\n\n\n\n\n\n\nCategory\nExample Problems\nState Form\n\n\n\n\nSequence\nLIS, LCS, Edit Distance\n\\(dp[i][j]\\) over prefixes\n\n\nSubset\nKnapsack, Subset Sum\n\\(dp[i][w]\\) capacity-based\n\n\nPartition\nPalindrome Partitioning, Equal Sum\n\\(dp[i]\\) cut-based\n\n\nGrid\nMin Path Sum, Unique Paths\n\\(dp[i][j]\\) over cells\n\n\nCounting\nCoin Change Count, Stairs\nAdd ways from subproblems\n\n\nInterval\nMatrix Chain, Burst Balloons\n\\(dp[i][j]\\) range subproblem\n\n\nBitmask\nTSP, Assignment\n\\(dp[mask][i]\\) subset states\n\n\nDigit\nCount numbers with constraint\n\\(dp[pos][tight][sum]\\) digits\n\n\nTree\nRerooting, Subtree DP\n\\(dp[u]\\) over children\n\n\n\n\n\nClassic Problems\n\n\n\n\n\n\n\n\nProblem\nState Definition\nTransition\n\n\n\n\nClimbing Stairs\n\\(dp[i]=\\) ways to reach step i\n\\(dp[i]=dp[i-1]+dp[i-2]\\)\n\n\nCoin Change (Count Ways)\n\\(dp[x]=\\) ways to make sum x\n\\(dp[x]+=dp[x-coin]\\)\n\n\n0/1 Knapsack\n\\(dp[w]=\\) max value under weight w\n\\(dp[w]=\\max(dp[w],dp[w-w_i]+v_i)\\)\n\n\nLongest Increasing Subseq.\n\\(dp[i]=\\) LIS ending at i\nif \\(a[j]&lt;a[i]\\), \\(dp[i]=dp[j]+1\\)\n\n\nEdit Distance\n\\(dp[i][j]=\\) edit cost\nmin(insert,delete,replace)\n\n\nMatrix Chain Multiplication\n\\(dp[i][j]=\\) min cost mult subchain\n\\(dp[i][j]=\\min_k(dp[i][k]+dp[k+1][j])\\)\n\n\n\n\n\nTiny Code: 0/1 Knapsack (1D optimized)\ndef knapsack(weights, values, W):\n    dp = [0]*(W+1)\n    for i in range(len(weights)):\n        for w in range(W, weights[i]-1, -1):\n            dp[w] = max(dp[w], dp[w-weights[i]] + values[i])\n    return dp[W]\n\n\nSequence Alignment Example\nEdit Distance Recurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1], & \\text{if } s[i] = t[j],\\\\\n1 + \\min(dp[i-1][j],\\ dp[i][j-1],\\ dp[i-1][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\nOptimization Techniques\n\n\n\n\n\n\n\n\nTechnique\nWhen to Use\nExample\n\n\n\n\nSpace Optimization\n2D → 1D states reuse\nKnapsack, LCS\n\n\nPrefix/Suffix Precomp\nRange aggregates\nSum/Min queries\n\n\nDivide & Conquer DP\nMonotonic decisions\nMatrix Chain\n\n\nConvex Hull Trick\nLinear transition minima\nDP on lines\n\n\nBitset DP\nLarge boolean states\nSubset sum optimization\n\n\n\n\n\nDebugging Tips\n\nPrint partial dp arrays to see progress.\nCheck base cases carefully.\nEnsure loops match transition dependencies.\nAlways confirm the recurrence before coding.\n\n\n\n\nPage 8. Mathematics for Algorithms Quick Use\nMathematics builds the foundation for algorithmic reasoning. This page collects essential formulas and methods every programmer should know.\n\nNumber Theory Essentials\n\n\n\n\n\n\n\n\nTopic\nDescription\nFormula / Idea\n\n\n\n\nGCD (Euclidean)\nGreatest common divisor\n\\(gcd(a,b)=gcd(b,a%b)\\)\n\n\nExtended GCD\nSolve \\(ax+by=gcd(a,b)\\)\nBacktrack coefficients\n\n\nLCM\nLeast common multiple\n\\(lcm(a,b)=\\frac{a\\cdot b}{gcd(a,b)}\\)\n\n\nModular Addition\nAdd under modulo M\n\\((a+b)\\bmod M\\)\n\n\nModular Multiply\nMultiply under modulo M\n\\((a\\cdot b)\\bmod M\\)\n\n\nModular Inverse\n\\(a^{-1}\\bmod M\\)\n\\(a^{M-2}\\bmod M\\) if M is prime\n\n\nModular Exponent\nFast exponentiation\nSquare and multiply\n\n\nCRT\nCombine congruences\nSolve system \\(x\\equiv a_i\\pmod{m_i}\\)\n\n\n\nTiny Code (Modular Exponentiation):\ndef modpow(a, n, M):\n    res = 1\n    while n:\n        if n & 1:\n            res = res * a % M\n        a = a * a % M\n        n &gt;&gt;= 1\n    return res\n\n\nPrimality and Factorization\n\n\n\n\n\n\n\n\n\nAlgorithm\nUse Case\nComplexity\nNotes\n\n\n\n\nTrial Division\nSmall n\n\\(O(\\sqrt{n})\\)\nSimple\n\n\nSieve of Eratosthenes\nGenerate primes\n\\(O(n\\log\\log n)\\)\nClassic prime sieve\n\n\nMiller–Rabin\nProbabilistic primality\n\\(O(k\\log^3 n)\\)\nFast for big n\n\n\nPollard Rho\nFactor composite\n\\(O(n^{1/4})\\)\nRandomized\n\n\nSieve of Atkin\nFaster variant\n\\(O(n)\\)\nComplex implementation\n\n\n\n\n\nCombinatorics\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\n\\(n! = n\\cdot(n-1)\\cdots1\\)\nFactorial\n\n\n\\(\\binom{n}{k}=\\dfrac{n!}{k!(n-k)!}\\)\nNumber of combinations\n\n\n\\(P(n,k)=\\dfrac{n!}{(n-k)!}\\)\nNumber of permutations\n\n\nPascal’s Rule: \\(\\binom{n}{k}=\\binom{n-1}{k}+\\binom{n-1}{k-1}\\)\nBuild Pascal’s Triangle\n\n\nCatalan: \\(C_n=\\dfrac{1}{n+1}\\binom{2n}{n}\\)\nParentheses counting\n\n\n\nTiny Code (nCr with factorials mod M):\ndef nCr(n, r, fact, inv):\n    return fact[n]*inv[r]%M*inv[n-r]%M\n\n\nProbability Basics\n\n\n\n\n\n\n\n\n\nConcept\nFormula or Idea\n\n\n\n\n\n\nProbability\n\\(P(A)=\\frac{\\text{favorable}}{\\text{total}}\\)\n\n\n\n\nComplement\n\\(P(\\bar{A})=1-P(A)\\)\n\n\n\n\nUnion\n\\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\)\n\n\n\n\nConditional\n\\(P(A                                         | B)=\\frac{P(A\\cap B)}{P(B)}\\)\n\n\n\n\nBayes’ Theorem\n\\(P(A                                         | B)=\\frac{P(B                | A)P(A)}{P(B)}\\)\n\n\n\n\nExpected Value\n\\(E[X]=\\sum x_iP(x_i)\\)\n\n\n\n\nVariance\n\\(Var(X)=E[X^2]-E[X]^2\\)\n\n\n\n\n\n\n\nLinear Algebra Core\n\n\n\nOperation\nFormula / Method\nComplexity\n\n\n\n\nGaussian Elimination\nSolve \\(Ax=b\\)\n\\(O(n^3)\\)\n\n\nDeterminant\nProduct of pivots\n\\(O(n^3)\\)\n\n\nMatrix Multiply\n\\((AB)*{ij}=\\sum_kA*{ik}B_{kj}\\)\n\\(O(n^3)\\)\n\n\nTranspose\n\\(A^T_{ij}=A_{ji}\\)\n\\(O(n^2)\\)\n\n\nLU Decomposition\n\\(A=LU\\) (lower, upper)\n\\(O(n^3)\\)\n\n\nCholesky\n\\(A=LL^T\\) (symmetric pos. def.)\n\\(O(n^3)\\)\n\n\nPower Method\nDominant eigenvalue estimation\niterative\n\n\n\nTiny Code (Gaussian Elimination Skeleton):\nfor i in range(n):\n    pivot = a[i][i]\n    for j in range(i, n+1):\n        a[i][j] /= pivot\n    for k in range(n):\n        if k != i:\n            ratio = a[k][i]\n            for j in range(i, n+1):\n                a[k][j] -= ratio*a[i][j]\n\n\nFast Transforms\n\n\n\nTransform\nUse Case\nComplexity\nNotes\n\n\n\n\nFFT\nPolynomial convolution\n\\(O(n\\log n)\\)\nComplex numbers\n\n\nNTT\nModular convolution\n\\(O(n\\log n)\\)\nPrime modulus\n\n\nFWT (XOR)\nXOR-based convolution\n\\(O(n\\log n)\\)\nSubset DP\n\n\n\nFFT Equation:\n\\[\nX_k = \\sum_{n=0}^{N-1} x_n e^{-2\\pi i kn/N}\n\\]\n\n\nNumerical Methods\n\n\n\n\n\n\n\n\nMethod\nPurpose\nFormula or Idea\n\n\n\n\nBisection\nRoot-finding\nMidpoint halve until \\(f(x)=0\\)\n\n\nNewton–Raphson\nFast convergence\n\\(x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}\\)\n\n\nSecant Method\nApprox derivative\n\\(x_{n+1}=x_n-f(x_n)\\frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}\\)\n\n\nSimpson’s Rule\nIntegration\n\\(\\int_a^bf(x)dx\\approx\\frac{h}{3}(f(a)+4f(m)+f(b))\\)\n\n\n\n\n\nOptimization and Calculus\n\n\n\n\n\n\n\nConcept\nFormula / Idea\n\n\n\n\nDerivative\n\\(f'(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\\)\n\n\nGradient Descent\n\\(x_{k+1}=x_k-\\eta\\nabla f(x_k)\\)\n\n\nLagrange Multipliers\n\\(\\nabla f=\\lambda\\nabla g\\)\n\n\nConvex Function\n\\(f(\\lambda x+(1-\\lambda)y)\\le\\lambda f(x)+(1-\\lambda)f(y)\\)\n\n\n\nTiny Code (Gradient Descent):\nx = x0\nfor _ in range(1000):\n    grad = df(x)\n    x -= lr * grad\n\n\nAlgebraic Tricks\n\n\n\n\n\n\n\n\n\nTopic\nFormula / Use\n\n\n\n\n\n\nExponentiation\n\\(a^n\\) via square-multiply\n\n\n\n\nPolynomial Deriv.\n\\((ax^n)' = n\\cdot a x^{n-1}\\)\n\n\n\n\nIntegration\n\\(\\int x^n dx = \\frac{x^{n+1}}{n+1}+C\\)\n\n\n\n\nMöbius Inversion\n\\(f(n)=\\sum_{d                         | n}g(d)\\implies g(n)=\\sum_{d | n}\\mu(d)\\cdot f(n/d)\\)\n\n\n\n\n\n\n\nQuick Reference Table\n\n\n\nDomain\nMust-Know Algorithm\n\n\n\n\nNumber Theory\nGCD, Mod Exp, CRT\n\n\nCombinatorics\nPascal, Factorial, Catalan\n\n\nProbability\nBayes, Expected Value\n\n\nLinear Algebra\nGaussian Elimination\n\n\nTransforms\nFFT, NTT\n\n\nOptimization\nGradient Descent\n\n\n\n\n\n\nPage 9. Strings and Text Algorithms Quick Use\nStrings are sequences of characters used in text search, matching, and transformation. This page gives quick references to classical and modern string techniques.\n\nString Fundamentals\n\n\n\n\n\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nAlphabet\nSet of symbols\n{a, b, c}\n\n\nString Length\nNumber of characters\n\"hello\" → 5\n\n\nSubstring\nContinuous part of string\n\"ell\" in \"hello\"\n\n\nSubsequence\nOrdered subset (not necessarily cont.)\n\"hlo\" from \"hello\"\n\n\nPrefix / Suffix\nStarts / ends part of string\n\"he\", \"lo\"\n\n\n\nIndexing: Most algorithms use 0-based indexing.\n\n\nString Search Overview\n\n\n\nAlgorithm\nComplexity\nDescription\n\n\n\n\nNaive Search\n\\(O(nm)\\)\nCheck all positions\n\n\nKMP\n\\(O(n+m)\\)\nPrefix-suffix skip table\n\n\nZ-Algorithm\n\\(O(n+m)\\)\nPrecompute match lengths\n\n\nRabin–Karp\n\\(O(n+m)\\) avg\nRolling hash check\n\n\nBoyer–Moore\n\\(O(n/m)\\) avg\nBackward scan, skip mismatches\n\n\n\n\n\nKMP Prefix Function\nCompute prefix-suffix matches for pattern.\n\n\n\n\n\n\n\nStep\nMeaning\n\n\n\n\n\\(pi[i]\\)\nLongest proper prefix that is also suffix for \\(pattern[0:i]\\)\n\n\n\nTiny Code:\ndef prefix_function(p):\n    pi = [0]*len(p)\n    j = 0\n    for i in range(1, len(p)):\n        while j &gt; 0 and p[i] != p[j]:\n            j = pi[j-1]\n        if p[i] == p[j]:\n            j += 1\n        pi[i] = j\n    return pi\nSearch uses pi to skip mismatches.\n\n\nZ-Algorithm\nComputes length of substring starting at i matching prefix.\n\n\n\nStep\nMeaning\n\n\n\n\n\\(Z[i]\\)\nLongest substring starting at i matching prefix\n\n\n\nUse $S = pattern + '$' + text$ to find pattern occurrences.\n\n\nRabin–Karp Rolling Hash\n\n\n\nIdea\nCompute hash for window of text, slide, compare\n\n\n\n\n\nHash Function: \\[\nh(s) = (s_0p^{n-1} + s_1p^{n-2} + \\dots + s_{n-1}) \\bmod M\n\\]\nUpdate efficiently when sliding one character.\nTiny Code:\ndef rolling_hash(s, base=257, mod=109+7):\n    h = 0\n    for ch in s:\n        h = (h*base + ord(ch)) % mod\n    return h\n\n\nAdvanced Pattern Matching\n\n\n\nAlgorithm\nUse Case\nComplexity\n\n\n\n\nBoyer–Moore\nLarge alphabet\n\\(O(n/m)\\) avg\n\n\nSunday\nLast char shift heuristic\n\\(O(n)\\) avg\n\n\nBitap\nApproximate match\n\\(O(nm/w)\\)\n\n\nAho–Corasick\nMulti-pattern search\n\\(O(n+z)\\)\n\n\n\n\n\nAho–Corasick Automaton\nBuild a trie from patterns and compute failure links.\n\n\n\nStep\nDescription\n\n\n\n\nBuild Trie\nAdd all patterns\n\n\nFailure Link\nFallback to next prefix\n\n\nOutput Link\nRecord pattern match\n\n\n\nTiny Code Sketch:\nfrom collections import deque\n\ndef build_ac(patterns):\n    trie = [{}]\n    fail = [0]\n    for pat in patterns:\n        node = 0\n        for c in pat:\n            node = trie[node].setdefault(c, len(trie))\n            if node == len(trie):\n                trie.append({})\n                fail.append(0)\n    # compute failure links\n    q = deque()\n    for c in trie[0]:\n        q.append(trie[0][c])\n    while q:\n        u = q.popleft()\n        for c, v in trie[u].items():\n            f = fail[u]\n            while f and c not in trie[f]:\n                f = fail[f]\n            fail[v] = trie[f].get(c, 0)\n            q.append(v)\n    return trie, fail\n\n\nSuffix Structures\n\n\n\n\n\n\n\n\nStructure\nPurpose\nBuild Time\n\n\n\n\nSuffix Array\nSorted list of suffix indices\n\\(O(n\\log n)\\)\n\n\nLCP Array\nLongest Common Prefix of suffix\n\\(O(n)\\)\n\n\nSuffix Tree\nTrie of suffixes\n\\(O(n)\\) (Ukkonen)\n\n\nSuffix Automaton\nMinimal DFA of substrings\n\\(O(n)\\)\n\n\n\nSuffix Array Doubling Approach:\n\nRank substrings of length \\(2^k\\)\nSort and merge using pairs of ranks\n\nLCP via Kasai’s Algorithm: \\[\nLCP[i]=\\text{common prefix of } S[SA[i]:], S[SA[i-1]:]\n\\]\n\n\nPalindrome Detection\n\n\n\nAlgorithm\nDescription\nComplexity\n\n\n\n\nManacher’s Algorithm\nLongest palindromic substring\n\\(O(n)\\)\n\n\nDP Table\nCheck substring palindrome\n\\(O(n^2)\\)\n\n\nCenter Expansion\nExpand around center\n\\(O(n^2)\\)\n\n\n\nManacher’s Core:\n\nTransform with separators (#)\nTrack radius of palindrome around each center\n\n\n\nEdit Distance Family\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\nComplexity\n\n\n\n\nLevenshtein Distance\nInsert/Delete/Replace\n\\(O(nm)\\)\n\n\nDamerau–Levenshtein\nAdd transposition\n\\(O(nm)\\)\n\n\nHirschberg\nSpace-optimized LCS\n\\(O(nm)\\) time, \\(O(n)\\) space\n\n\n\nRecurrence: \\[\ndp[i][j]=\\min\n\\begin{cases}\ndp[i-1][j]+1 \\\ndp[i][j-1]+1 \\\ndp[i-1][j-1]+(s_i\\neq t_j)\n\\end{cases}\n\\]\n\n\nCompression Techniques\n\n\n\n\n\n\n\n\nAlgorithm\nType\nIdea\n\n\n\n\nHuffman Coding\nPrefix code\nShorter codes for frequent chars\n\n\nArithmetic Coding\nRange encoding\nFractional interval representation\n\n\nLZ77 / LZ78\nDictionary-based\nReuse earlier substrings\n\n\nBWT + MTF + RLE\nBlock sorting\nGroup similar chars before coding\n\n\n\nHuffman Principle: Shorter bit strings assigned to higher frequency symbols.\n\n\nHashing and Checksums\n\n\n\nAlgorithm\nUse Case\nNotes\n\n\n\n\nCRC32\nError detection\nSimple polynomial mod\n\n\nMD5\nHash (legacy)\nNot secure\n\n\nSHA-256\nSecure hash\nCryptographic\n\n\nRolling Hash\nSubstring compare\nUsed in Rabin–Karp\n\n\n\n\n\nQuick Reference\n\n\n\nTask\nAlgorithm\nComplexity\n\n\n\n\nSingle pattern search\nKMP / Z\n\\(O(n+m)\\)\n\n\nMulti-pattern search\nAho–Corasick\n\\(O(n+z)\\)\n\n\nApproximate search\nBitap / Wu–Manber\n\\(O(kn)\\)\n\n\nSubstring queries\nSuffix Array + LCP\n\\(O(\\log n)\\)\n\n\nPalindromes\nManacher\n\\(O(n)\\)\n\n\nCompression\nHuffman / LZ77\nvariable\n\n\nEdit distance\nDP table\n\\(O(nm)\\)\n\n\n\n\n\n\nPage 10. Geometry, Graphics, and Spatial Algorithms Quick Use\nGeometry helps us solve problems about shapes, distances, and spatial relationships. This page summarizes core computational geometry techniques with simple formulas and examples.\n\nCoordinate Basics\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nFormula / Example\n\n\n\n\n\n\n\n\nPoint Distance\nDistance between \\((x_1,y_1)\\) and \\((x_2,y_2)\\)\n\\(d=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\\)\n\n\n\n\n\n\nMidpoint\nBetween two points\n\\((\\frac{x_1+x_2}{2}, \\frac{y_1+y_2}{2})\\)\n\n\n\n\n\n\nDot Product\nAngle & projection\n\\(\\vec{a}\\cdot\\vec{b}=                    | a |   | b | \\cos\\theta\\)\n\n\n\n\n\n\nCross Product (2D)\nSigned area, orientation\n\\(a\\times b = a_xb_y - a_yb_x\\)\n\n\n\n\n\n\nOrientation Test\nCCW, CW, collinear check\n\\(\\text{sign}(a\\times b)\\)\n\n\n\n\n\n\n\nTiny Code (Orientation Test):\ndef orient(a, b, c):\n    val = (b[0]-a[0])*(c[1]-a[1]) - (b[1]-a[1])*(c[0]-a[0])\n    return 0 if val == 0 else (1 if val &gt; 0 else -1)\n\n\nConvex Hull\nFind the smallest convex polygon enclosing all points.\n\n\n\nAlgorithm\nComplexity\nNotes\n\n\n\n\nGraham Scan\n\\(O(n\\log n)\\)\nSort by angle, use stack\n\n\nAndrew’s Monotone\n\\(O(n\\log n)\\)\nSort by x, build upper/lower\n\n\nJarvis March\n\\(O(nh)\\)\nWrap hull, h = hull size\n\n\nChan’s Algorithm\n\\(O(n\\log h)\\)\nOutput-sensitive hull\n\n\n\nSteps:\n\nSort points\nBuild lower hull\nBuild upper hull\nConcatenate\n\n\n\nClosest Pair of Points\nDivide-and-conquer approach.\n\n\n\nStep\nDescription\n\n\n\n\nSplit by x\nDivide points into halves\n\n\nRecurse and merge\nTrack min distance across strip\n\n\n\nComplexity: \\(O(n\\log n)\\)\nFormula: \\[\nd(p,q)=\\sqrt{(x_p-x_q)^2+(y_p-y_q)^2}\n\\]\n\n\nLine Intersection\nTwo segments \\((p_1,p_2)\\) and \\((q_1,q_2)\\) intersect if:\n\nOrientations differ\nSegments overlap on line if collinear\n\nTiny Code:\ndef intersect(p1, p2, q1, q2):\n    o1 = orient(p1, p2, q1)\n    o2 = orient(p1, p2, q2)\n    o3 = orient(q1, q2, p1)\n    o4 = orient(q1, q2, p2)\n    return o1 != o2 and o3 != o4\n\n\nPolygon Area (Shoelace Formula)\nFor vertices \\((x_i, y_i)\\) in order:\n\\[\nA=\\frac{1}{2}\\left|\\sum_{i=0}^{n-1}(x_iy_{i+1}-x_{i+1}y_i)\\right|\n\\]\nTiny Code:\ndef area(poly):\n    s = 0\n    n = len(poly)\n    for i in range(n):\n        x1, y1 = poly[i]\n        x2, y2 = poly[(i+1)%n]\n        s += x1*y2 - x2*y1\n    return abs(s)/2\n\n\nPoint in Polygon\n\n\n\nMethod\nIdea\nComplexity\n\n\n\n\nRay Casting\nCount edge crossings\n\\(O(n)\\)\n\n\nWinding Number\nTrack signed rotations\n\\(O(n)\\)\n\n\nConvex Test\nCheck all orientations\n\\(O(n)\\)\n\n\n\nRay Casting: Odd number of crossings → inside.\n\n\nRotating Calipers\nUsed for:\n\nPolygon diameter (farthest pair)\nMinimum bounding box\nWidth and antipodal pairs\n\nIdea: Sweep around convex hull using tangents. Complexity: \\(O(n)\\) after hull.\n\n\nSweep Line Techniques\n\n\n\nProblem\nMethod\nComplexity\n\n\n\n\nClosest Pair\nActive set by y\n\\(O(n\\log n)\\)\n\n\nSegment Intersection\nEvent-based sweeping\n\\(O((n+k)\\log n)\\)\n\n\nRectangle Union Area\nVertical edge events\n\\(O(n\\log n)\\)\n\n\nSkyline Problem\nMerge by height\n\\(O(n\\log n)\\)\n\n\n\nUse balanced trees or priority queues for active sets.\n\n\nCircle Geometry\n\n\n\nConcept\nFormula\n\n\n\n\nEquation\n\\((x-x_c)^2+(y-y_c)^2=r^2\\)\n\n\nTangent Length\n\\(\\sqrt{d^2-r^2}\\)\n\n\nTwo-Circle Intersection\nDistance-based geometry\n\n\n\n\n\nSpatial Data Structures\n\n\n\nStructure\nUse Case\nNotes\n\n\n\n\nKD-Tree\nNearest neighbor search\nAxis-aligned splits\n\n\nR-Tree\nRange queries\nBounding boxes hierarchy\n\n\nQuadtree\n2D recursive subdivision\nGraphics, collision detection\n\n\nOctree\n3D extension\nVolumetric partitioning\n\n\nBSP Tree\nPlanar splits\nRendering, collision\n\n\n\n\n\nRasterization and Graphics\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nNotes\n\n\n\n\nBresenham Line\nDraw line integer grid\nNo floating point\n\n\nMidpoint Circle\nCircle rasterization\nSymmetry exploitation\n\n\nScanline Fill\nPolygon fill algorithm\nSort edges, horizontal sweep\n\n\nZ-Buffer\nHidden surface removal\nPer-pixel depth comparison\n\n\nPhong Shading\nSmooth lighting\nInterpolate normals\n\n\n\n\n\nPathfinding in Space\n\n\n\nAlgorithm\nDescription\nNotes\n\n\n\n\nA*\nHeuristic shortest path\n\\(f(n)=g(n)+h(n)\\)\n\n\nTheta*\nAny-angle path\nShortcut-based\n\n\nRRT / RRT*\nRandom exploration\nRobotics planning\n\n\nPRM\nProbabilistic roadmap\nSampled graph\n\n\nVisibility Graph\nConnect visible points\nGeometric planning\n\n\n\n\n\nQuick Summary\n\n\n\nTask\nAlgorithm\nComplexity\n\n\n\n\nConvex Hull\nGraham / Andrew\n\\(O(n\\log n)\\)\n\n\nClosest Pair\nDivide and Conquer\n\\(O(n\\log n)\\)\n\n\nSegment Intersection Detection\nSweep Line\n\\(O(n\\log n)\\)\n\n\nPoint in Polygon\nRay Casting\n\\(O(n)\\)\n\n\nPolygon Area\nShoelace Formula\n\\(O(n)\\)\n\n\nNearest Neighbor Search\nKD-Tree\n\\(O(\\log n)\\)\n\n\nPathfinding\nA*\n\\(O(E\\log V)\\)\n\n\n\n\n\nTip\n\nAlways sort points for geometry preprocessing.\nUse cross product for orientation tests.\nPrefer integer arithmetic when possible to avoid floating errors.\n\n\n\n\nPage 11. Systems, Databases, and Distributed Algorithms Quick Use\nSystems and databases rely on algorithms that manage memory, concurrency, persistence, and coordination. This page gives an overview of the most important ones.\n\nConcurrency Control\nEnsures correctness when multiple transactions or threads run at once.\n\n\n\n\n\n\n\n\nMethod\nIdea\nNotes\n\n\n\n\nTwo-Phase Locking (2PL)\nAcquire locks, then release after commit\nGuarantees serializability\n\n\nStrict 2PL\nHold all locks until commit\nPrevents cascading aborts\n\n\nConservative 2PL\nLock all before execution\nDeadlock-free but less parallel\n\n\nTimestamp Ordering\nOrder by timestamps\nMay abort late transactions\n\n\nMultiversion CC (MVCC)\nReaders get snapshots\nUsed in PostgreSQL, InnoDB\n\n\nOptimistic CC (OCC)\nValidate at commit\nBest for low conflict workloads\n\n\n\n\n\nTiny Code: Timestamp Ordering\n# Simplified\nif write_ts[x] &gt; txn_ts or read_ts[x] &gt; txn_ts:\n    abort()\nelse:\n    write_ts[x] = txn_ts\nEach object tracks read and write timestamps.\n\n\nDeadlocks\nCircular waits among transactions.\nDetection | Build Wait-For Graph, detect cycle |\nPrevention | Wait-Die (old waits) / Wound-Wait (young aborts) |\nDetection Complexity: \\(O(V+E)\\)\nTiny Code (Wait-For Graph Cycle Check):\ndef has_cycle(graph):\n    visited, stack = set(), set()\n    def dfs(u):\n        visited.add(u)\n        stack.add(u)\n        for v in graph[u]:\n            if v not in visited and dfs(v): return True\n            if v in stack: return True\n        stack.remove(u)\n        return False\n    return any(dfs(u) for u in graph)\n\n\nLogging and Recovery\n\n\n\n\n\n\n\n\nTechnique\nDescription\nNotes\n\n\n\n\nWrite-Ahead Log\nLog before data\nEnsures durability\n\n\nARIES\nAnalysis, Redo, Undo phases\nIndustry standard\n\n\nCheckpointing\nSave consistent snapshot\nSpeeds recovery\n\n\nShadow Paging\nCopy-on-write updates\nSimpler but less flexible\n\n\n\nRecovery after crash:\n\nAnalysis: find active transactions\nRedo: reapply committed changes\nUndo: revert uncommitted ones\n\n\n\nIndexing\nAccelerates lookups and range queries.\n\n\n\n\n\n\n\n\nIndex Type\nDescription\nNotes\n\n\n\n\nB-Tree / B+Tree\nBalanced multiway tree\nDisk-friendly\n\n\nHash Index\nExact match only\nNo range queries\n\n\nGiST / R-Tree\nSpatial data\nBounding box hierarchy\n\n\nInverted Index\nText search\nMaps token to document list\n\n\n\nB+Tree Complexity: \\(O(\\log_B N)\\) (B = branching factor)\nTiny Code (Binary Search in Index):\ndef search(node, key):\n    i = bisect_left(node.keys, key)\n    if i &lt; len(node.keys) and node.keys[i] == key:\n        return node.values[i]\n    if node.is_leaf:\n        return None\n    return search(node.children[i], key)\n\n\nQuery Processing\n\n\n\nStep\nDescription\n\n\n\n\nParsing\nBuild abstract syntax tree\n\n\nOptimization\nReorder joins, pick indices\n\n\nExecution Plan\nChoose algorithm per operator\n\n\nExecution\nEvaluate iterators or pipelines\n\n\n\nCommon join strategies:\n\n\n\nJoin Type\nComplexity\nNotes\n\n\n\n\nNested Loop\n\\(O(nm)\\)\nSimple, slow\n\n\nHash Join\n\\(O(n+m)\\)\nBuild + probe\n\n\nSort-Merge Join\n\\(O(n\\log n+m\\log m)\\)\nSorted inputs\n\n\n\n\n\nCaching and Replacement\n\n\n\nPolicy\nDescription\nNotes\n\n\n\n\nLRU\nEvict least recently used\nSimple, temporal locality\n\n\nLFU\nEvict least frequently used\nGood for stable patterns\n\n\nARC / LIRS\nAdaptive hybrid\nHandles mixed workloads\n\n\nRandom\nRandom eviction\nSimple, fair\n\n\n\nTiny Code (LRU using OrderedDict):\nfrom collections import OrderedDict\n\nclass LRU:\n    def __init__(self, cap):\n        self.cap = cap\n        self.cache = OrderedDict()\n    def get(self, k):\n        if k not in self.cache: return -1\n        self.cache.move_to_end(k)\n        return self.cache[k]\n    def put(self, k, v):\n        if k in self.cache: self.cache.move_to_end(k)\n        self.cache[k] = v\n        if len(self.cache) &gt; self.cap: self.cache.popitem(last=False)\n\n\nDistributed Systems Core\n\n\n\nProblem\nDescription\nTypical Solution\n\n\n\n\nConsensus\nAgree on value across nodes\nPaxos, Raft\n\n\nLeader Election\nPick coordinator\nBully, Raft\n\n\nReplication\nMaintain copies\nLog replication\n\n\nPartitioning\nSplit data\nConsistent hashing\n\n\nMembership\nDetect nodes\nGossip protocols\n\n\n\n\n\nRaft Consensus (Simplified)\n\n\n\nPhase\nAction\n\n\n\n\nElection\nNodes vote, elect leader\n\n\nReplication\nLeader appends log entries\n\n\nCommitment\nOnce majority acknowledge\n\n\n\nSafety: Committed entries never change. Liveness: New leader elected on failure.\nTiny Code Sketch:\nif vote_request.term &gt; term:\n    term = vote_request.term\n    voted_for = candidate\n\n\nConsistent Hashing\nDistributes keys across nodes smoothly.\n\n\n\nStep\nDescription\n\n\n\n\nHash each node to ring\ne.g. hash(node_id)\n\n\nHash each key\nFind next node clockwise\n\n\nAdd/remove node\nOnly nearby keys move\n\n\n\nUsed in: Dynamo, Cassandra, Memcached.\n\n\nFault Tolerance Patterns\n\n\n\nPattern\nDescription\nExample\n\n\n\n\nReplication\nMultiple copies\nPrimary-backup\n\n\nCheckpointing\nSave progress periodically\nML training\n\n\nHeartbeats\nLiveness detection\nCluster managers\n\n\nRetry + Backoff\nHandle transient failures\nAPI calls\n\n\nQuorum Reads/Writes\nRequire majority agreement\nCassandra\n\n\n\n\n\nDistributed Coordination\n\n\n\nTool / Protocol\nDescription\nExample Use\n\n\n\n\nZooKeeper\nCentralized coordination\nLocks, config\n\n\nRaft\nDistributed consensus\nLog replication\n\n\nEtcd\nKey-value store on Raft\nCluster metadata\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nTopic\nAlgorithm / Concept\nComplexity\nNotes\n\n\n\n\nLocking\n2PL, MVCC, OCC\nvaries\nTransaction isolation\n\n\nDeadlock\nWait-Die, Detection\n\\(O(V+E)\\)\nGraph-based check\n\n\nRecovery\nARIES, WAL\nvaries\nCrash recovery\n\n\nIndexing\nB+Tree, Hash Index\n\\(O(\\log N)\\)\nFaster queries\n\n\nJoin\nHash / Sort-Merge\nvaries\nQuery optimization\n\n\nCache\nLRU, LFU\n\\(O(1)\\)\nData locality\n\n\nConsensus\nRaft, Paxos\n\\(O(n)\\) msg\nFault tolerance\n\n\nPartitioning\nConsistent Hashing\n\\(O(1)\\) avg\nScalability\n\n\n\n\n\nQuick Tips\n\nAlways ensure serializability in concurrency.\nUse MVCC for read-heavy workloads.\nARIES ensures durability via WAL.\nFor scalability, partition and replicate wisely.\nConsensus is required for shared state correctness.\n\n\n\n\nPage 12. Algorithms for AI, ML, and Optimization Quick Use\nThis page gathers classical algorithms that power modern AI and machine learning systems, from clustering and classification to gradient-based learning and metaheuristics.\n\nClassical Machine Learning Algorithms\n\n\n\n\n\n\n\n\n\nCategory\nAlgorithm\nCore Idea\nComplexity\n\n\n\n\nClustering\nk-Means\nAssign to nearest centroid, update centers\n\\(O(nkt)\\)\n\n\nClustering\nk-Medoids (PAM)\nRepresentative points as centers\n\\(O(k(n-k)^2)\\)\n\n\nClustering\nGaussian Mixture (EM)\nSoft assignments via probabilities\n\\(O(nkd)\\) per iter\n\n\nClassification\nNaive Bayes\nApply Bayes rule with feature independence\n\\(O(nd)\\)\n\n\nClassification\nLogistic Regression\nLinear + sigmoid activation\n\\(O(nd)\\)\n\n\nClassification\nSVM (Linear)\nMaximize margin via convex optimization\n\\(O(nd)\\) approx\n\n\nClassification\nk-NN\nVote from nearest neighbors\n\\(O(nd)\\) per query\n\n\nTrees\nDecision Tree (CART)\nRecursive splitting by impurity\n\\(O(nd\\log n)\\)\n\n\nProjection\nLDA / PCA\nFind projection maximizing variance or class separation\n\\(O(d^3)\\)\n\n\n\n\n\nTiny Code: k-Means\nimport random, math\n\ndef kmeans(points, k, iters=100):\n    centroids = random.sample(points, k)\n    for _ in range(iters):\n        groups = [[] for _ in range(k)]\n        for p in points:\n            idx = min(range(k), key=lambda i: (p[0]-centroids[i][0])2 + (p[1]-centroids[i][1])2)\n            groups[idx].append(p)\n        new_centroids = []\n        for g in groups:\n            if g:\n                x = sum(p[0] for p in g)/len(g)\n                y = sum(p[1] for p in g)/len(g)\n                new_centroids.append((x,y))\n            else:\n                new_centroids.append(random.choice(points))\n        if centroids == new_centroids: break\n        centroids = new_centroids\n    return centroids\n\n\nLinear Models\n\n\n\n\n\n\n\n\nModel\nFormula\nLoss Function\n\n\n\n\nLinear Regression\n\\(\\hat{y}=w^Tx+b\\)\nMSE: \\(\\frac{1}{n}\\sum(y-\\hat{y})^2\\)\n\n\nLogistic Regression\n\\(\\hat{y}=\\sigma(w^Tx+b)\\)\nCross-Entropy\n\n\nRidge Regression\nLinear + \\(L_2\\) penalty\n\\(L=\\text{MSE}+\\lambda|w|^2\\)\n\n\nLasso Regression\nLinear + \\(L_1\\) penalty\n\\(L=\\text{MSE}+\\lambda|w|_1\\)\n\n\n\nTiny Code (Gradient Descent for Linear Regression):\ndef train(X, y, lr=0.01, epochs=1000):\n    w = [0]*len(X[0])\n    b = 0\n    for _ in range(epochs):\n        for i in range(len(y)):\n            y_pred = sum(w[j]*X[i][j] for j in range(len(w))) + b\n            err = y_pred - y[i]\n            for j in range(len(w)):\n                w[j] -= lr * err * X[i][j]\n            b -= lr * err\n    return w, b\n\n\nDecision Trees and Ensembles\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\nNotes\n\n\n\n\nID3 / C4.5 / CART\nSplit by info gain or Gini\nRecursive, interpretable\n\n\nRandom Forest\nBagging + Decision Trees\nReduces variance\n\n\nGradient Boosting\nSequential residual fitting\nXGBoost, LightGBM, CatBoost\n\n\nAdaBoost\nWeighted weak learners\nSensitive to noise\n\n\n\nImpurity Measures:\n\nGini: \\(1-\\sum p_i^2\\)\nEntropy: \\(-\\sum p_i\\log_2p_i\\)\n\n\n\nSupport Vector Machines (SVM)\nFinds a maximum margin hyperplane.\nObjective: \\[\n\\min_{w,b} \\frac{1}{2}|w|^2 + C\\sum\\xi_i\n\\] subject to \\(y_i(w^Tx_i+b)\\ge1-\\xi_i\\)\nKernel trick enables nonlinear separation: \\[K(x_i,x_j)=\\phi(x_i)\\cdot\\phi(x_j)\\]\n\n\nNeural Network Fundamentals\n\n\n\nComponent\nDescription\n\n\n\n\nNeuron\n\\(y=\\sigma(w\\cdot x+b)\\)\n\n\nActivation\nSigmoid, ReLU, Tanh\n\n\nLoss\nMSE, Cross-Entropy\n\n\nTraining\nGradient Descent + Backprop\n\n\nOptimizers\nSGD, Adam, RMSProp\n\n\n\nForward Propagation: \\[a^{(l)} = \\sigma(W^{(l)}a^{(l-1)}+b^{(l)})\\] Backpropagation computes gradients layer by layer.\n\n\nGradient Descent Variants\n\n\n\nVariant\nIdea\nNotes\n\n\n\n\nBatch\nUse all data each step\nStable but slow\n\n\nStochastic\nUpdate per sample\nNoisy, fast\n\n\nMini-batch\nGroup updates\nCommon practice\n\n\nMomentum\nAdd velocity term\nFaster convergence\n\n\nAdam\nAdaptive moment estimates\nMost popular\n\n\n\nUpdate Rule: \\[\nw = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n\\]\n\n\nUnsupervised Learning\n\n\n\nAlgorithm\nDescription\nNotes\n\n\n\n\nPCA\nVariance-based projection\nEigen decomposition\n\n\nICA\nIndependent components\nSignal separation\n\n\nt-SNE\nPreserve local structure\nVisualization only\n\n\nAutoencoder\nNN reconstruction model\nDimensionality red.\n\n\n\nPCA Formula: Covariance \\(C=\\frac{1}{n}X^TX\\), eigenvectors of \\(C\\) are principal axes.\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\n\nModel\nDescription\nNotes\n\n\n\n\n\n\nNaive Bayes\nIndependence assumption\n\\(P(y                 | x)\\propto P(y)\\prod P(x_i | y)\\)\n\n\n\n\nHMM\nSequential hidden states\nViterbi for decoding\n\n\n\n\nMarkov Chains\nTransition probabilities\n\\(P(x_t               | x_{t-1})\\)\n\n\n\n\nGaussian Mixture\nSoft clustering\nEM algorithm\n\n\n\n\n\n\n\nOptimization and Metaheuristics\n\n\n\n\n\n\n\n\nAlgorithm\nCategory\nNotes\n\n\n\n\nGradient Descent\nConvex Opt.\nDifferentiable objectives\n\n\nNewton’s Method\nSecond-order\nUses Hessian\n\n\nSimulated Annealing\nProb. search\nEscape local minima\n\n\nGenetic Algorithm\nEvolutionary\nPopulation-based search\n\n\nPSO (Swarm)\nCollective move\nInspired by flocking behavior\n\n\nHill Climbing\nGreedy search\nLocal optimization\n\n\n\n\n\nReinforcement Learning Core\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nAgent\nLearner/decision maker\nRobot, policy\n\n\nEnvironment\nProvides states, rewards\nGame, simulation\n\n\nPolicy\nMapping state → action\n\\(\\pi(s)=a\\)\n\n\nValue Function\nExpected return\n\\(V(s)\\), \\(Q(s,a)\\)\n\n\n\nQ-Learning Update: \\[\nQ(s,a)\\leftarrow Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a')-Q(s,a))\n\\]\nTiny Code:\nQ[s][a] += alpha * (r + gamma * max(Q[s_next]) - Q[s][a])\n\n\nAI Search Algorithms\n\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\nComplexity\nNotes\n\n\n\n\nBFS\nShortest path unweighted\n\\(O(V+E)\\)\nLevel order search\n\n\nDFS\nDeep exploration\n\\(O(V+E)\\)\nBacktracking\n\n\nA* Search\nInformed, uses heuristic\n\\(O(E\\log V)\\)\n\\(f(n)=g(n)+h(n)\\)\n\n\nIDA*\nIterative deepening A*\nMemory efficient\nOptimal if \\(h\\) admissible\n\n\nBeam Search\nKeep best k states\nApproximate\nNLP decoding\n\n\n\n\n\nEvaluation Metrics\n\n\n\n\n\n\n\n\nTask\nMetric\nFormula / Meaning\n\n\n\n\nClassification\nAccuracy, Precision, Recall\n\\(\\frac{TP}{TP+FP}\\), \\(\\frac{TP}{TP+FN}\\)\n\n\nRegression\nRMSE, MAE, \\(R^2\\)\nFit and error magnitude\n\n\nClustering\nSilhouette Score\nCohesion vs separation\n\n\nRanking\nMAP, NDCG\nOrder-sensitive\n\n\n\nConfusion Matrix:\n\n\n\n\nPred +\nPred -\n\n\n\n\nActual +\nTP\nFN\n\n\nActual -\nFP\nTN\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\nCategory\nAlgorithm Example\nNotes\n\n\n\n\nClustering\nk-Means, GMM\nUnsupervised grouping\n\n\nClassification\nLogistic, SVM, Trees\nSupervised labeling\n\n\nRegression\nLinear, Ridge, Lasso\nPredict continuous value\n\n\nOptimization\nGD, Adam, Simulated Annealing\nMinimize loss\n\n\nProbabilistic\nBayes, HMM, EM\nUncertainty modeling\n\n\nReinforcement\nQ-Learning, SARSA\nReward-based learning",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Cheatsheet</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html",
    "href": "books/en-us/book.html",
    "title": "The Book",
    "section": "",
    "text": "Chapter 1. Foundations of algorithms\nLicensed under CC BY-NC-SA 4.0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-1.-foundations-of-algorithms",
    "href": "books/en-us/book.html#chapter-1.-foundations-of-algorithms",
    "title": "The Book",
    "section": "",
    "text": "1. What Is an Algorithm?\nLet’s start at the beginning. Before code, data, or performance, we need a clear idea of what an algorithm really is.\nAn algorithm is a clear, step-by-step procedure to solve a problem. Think of it like a recipe: you have inputs (ingredients), a series of steps (instructions), and an output (the finished dish).\nAt its core, an algorithm should be:\n\nPrecise: every step is well defined and unambiguous\nFinite: it finishes after a limited number of steps\nEffective: each step is simple enough to carry out\nDeterministic (usually): the same input gives the same output\n\nWhen you write an algorithm, you are describing how to get from question to answer, not just what the answer is.\n\nExample: Sum from 1 to (n)\nSuppose you want the sum of the numbers from 1 to (n).\nNatural language steps\n\nSet total = 0\nFor each i from 1 to n, add i to total\nReturn total\n\nPseudocode\nAlgorithm SumToN(n):\n    total ← 0\n    for i ← 1 to n:\n        total ← total + i\n    return total\nC code\nint sum_to_n(int n) {\n    int total = 0;\n    for (int i = 1; i &lt;= n; i++) {\n        total += i;\n    }\n    return total;\n}\n\n\nTiny Code\nTry a quick run by hand with (n = 5):\n\nstart total = 0\nadd 1 → total = 1\nadd 2 → total = 3\nadd 3 → total = 6\nadd 4 → total = 10\nadd 5 → total = 15\n\nOutput is 15.\nYou will also see this closed-form formula soon:\n\\[\n1 + 2 + 3 + \\dots + n = \\frac{n(n+1)}{2}\n\\]\n\n\nWhy It Matters\nAlgorithms are the blueprints of computation. Every program, from a calculator to an AI model, is built from algorithms. Computers are fast at following instructions. Algorithms give those instructions structure and purpose.\n\nAlgorithms are the language of problem solving.\n\n\n\nTry It Yourself\n\nWrite an algorithm to find the maximum number in a list\nWrite an algorithm to reverse a string\nDescribe your morning routine as an algorithm: list the inputs, the steps, and the final output\n\nTip: the best way to learn is to think in small, clear steps. Break a problem into simple actions you can execute one by one.\n\n\n\n2. Measuring Time and Space\nNow that you know what an algorithm is, it’s time to ask a deeper question:\n\nHow do we know if one algorithm is better than another?\n\nIt’s not enough for an algorithm to be correct. It should also be efficient. We measure efficiency in two key ways: time and space.\n\nTime Complexity\nTime measures how long an algorithm takes to run, relative to its input size. We don’t measure in seconds, because hardware speed varies. Instead, we count steps or operations.\nExample:\nfor (int i = 0; i &lt; n; i++) {\n    printf(\"Hi\\n\");\n}\nThis loop runs \\(n\\) times, so it has time complexity \\(O(n)\\). The time grows linearly with input size.\nAnother example:\nfor (int i = 0; i &lt; n; i++)\n  for (int j = 0; j &lt; n; j++)\n    printf(\"*\");\nThis runs \\(n \\times n = n^2\\) times, so it has \\(O(n^2)\\) time complexity.\nThese Big-O symbols describe how runtime grows as the input grows.\n\n\nSpace Complexity\nSpace measures how much memory an algorithm uses.\nExample:\nint sum = 0;  // O(1) space\nThis uses a constant amount of memory, regardless of input size.\nBut if we allocate an array:\nint arr[n];   // O(n) space\nThis uses space proportional to \\(n\\).\nOften, we trade time for space:\n\nUsing a hash table speeds up lookups (more memory, less time)\nUsing a streaming algorithm saves memory (less space, more time)\n\n\n\nTiny Code\nCompare two ways to compute the sum from 1 to \\(n\\):\nMethod 1: Loop\nint sum_loop(int n) {\n    int total = 0;\n    for (int i = 1; i &lt;= n; i++) total += i;\n    return total;\n}\nTime: \\(O(n)\\) Space: \\(O(1)\\)\nMethod 2: Formula\nint sum_formula(int n) {\n    return n * (n + 1) / 2;\n}\nTime: \\(O(1)\\) Space: \\(O(1)\\)\nBoth are correct, but one is faster. Analyzing time and space helps you understand why.\n\n\nWhy It Matters\nWhen data grows huge (millions or billions), small inefficiencies explode.\nAn algorithm that takes \\(O(n^2)\\) time might feel fine for 10 elements, but impossible for 1,000,000.\nMeasuring time and space helps you:\n\nPredict performance\nCompare different solutions\nOptimize intelligently\n\nIt’s your compass for navigating complexity.\n\n\nTry It Yourself\n\nWrite a simple algorithm to find the minimum in an array. Estimate its time and space complexity.\nCompare two algorithms that solve the same problem. Which one scales better?\nThink of a daily task that feels like \\(O(n)\\). Can you imagine one that’s \\(O(1)\\)?\n\nUnderstanding these measurements early makes every future algorithm more meaningful.\n\n\n\n3. Big-O, Big-Theta, Big-Omega\nNow that you can measure time and space, let’s learn the language used to describe those measurements.\nWhen we say an algorithm is \\(O(n)\\), we’re using asymptotic notation, a way to describe how an algorithm’s running time or memory grows as input size \\(n\\) increases.\nIt’s not about exact steps, but about how the cost scales for very large \\(n\\).\n\nThe Big-O (Upper Bound)\nBig-O answers the question: “How bad can it get?” It gives an upper bound on growth, the worst-case scenario.\nIf an algorithm takes at most \\(5n + 20\\) steps, we write \\(O(n)\\). We drop constants and lower-order terms because they don’t matter at scale.\nCommon Big-O notations:\n\n\n\n\n\n\n\n\n\nName\nNotation\nGrowth\nExample\n\n\n\n\nConstant\n\\(O(1)\\)\nFlat\nAccessing array element\n\n\nLogarithmic\n\\(O(\\log n)\\)\nVery slow growth\nBinary search\n\n\nLinear\n\\(O(n)\\)\nProportional\nSingle loop\n\n\nQuadratic\n\\(O(n^2)\\)\nGrows quickly\nDouble loop\n\n\nExponential\n\\(O(2^n)\\)\nExplodes\nRecursive subset generation\n\n\n\nIf your algorithm is \\(O(n)\\), doubling input size roughly doubles runtime. If it’s \\(O(n^2)\\), doubling input size makes it about four times slower.\n\n\nThe Big-Theta (Tight Bound)\nBig-Theta (\\(\\Theta\\)) gives a tight bound, when you know the algorithm’s growth from above and below.\nIf runtime is roughly \\(3n + 2\\), then \\(T(n) = \\Theta(n)\\). That means it’s both \\(O(n)\\) and \\(\\Omega(n)\\).\n\n\nThe Big-Omega (Lower Bound)\nBig-Omega (\\(\\Omega\\)) answers: “How fast can it possibly be?” It’s the best-case growth, the lower limit.\nExample:\n\nLinear search: \\(\\Omega(1)\\) if the element is at the start\n\\(O(n)\\) in the worst case if it’s at the end\n\nSo we might say:\n\\[\nT(n) = \\Omega(1),\\quad T(n) = O(n)\n\\]\n\n\nTiny Code\nLet’s see Big-O in action.\nint sum_pairs(int n) {\n    int total = 0;\n    for (int i = 0; i &lt; n; i++)        // O(n)\n        for (int j = 0; j &lt; n; j++)    // O(n)\n            total += i + j;            // O(1)\n    return total;\n}\nTotal steps ≈ \\(n \\times n = n^2\\). So \\(T(n) = O(n^2)\\).\nIf we added a constant-time operation before or after the loops, it wouldn’t matter. Constants vanish in asymptotic notation.\n\n\nWhy It Matters\nBig-O, Big-Theta, and Big-Omega let you talk precisely about performance. They are the grammar of efficiency.\nWhen you can write:\n\nAlgorithm A runs in \\(O(n \\log n)\\) time, \\(O(n)\\) space\n\nyou’ve captured its essence clearly and compared it meaningfully.\nThey help you:\n\nPredict behavior at scale\nChoose better data structures\nCommunicate efficiency in interviews and papers\n\nIt’s not about exact timing, it’s about growth.\n\n\nTry It Yourself\n\nAnalyze this code:\nfor (int i = 1; i &lt;= n; i *= 2)\n    printf(\"%d\", i);\nWhat’s the time complexity?\nWrite an algorithm that’s \\(O(n \\log n)\\) (hint: merge sort).\nIdentify the best, worst, and average-case complexities for linear search and binary search.\n\nLearning Big-O is like learning a new language, once you’re fluent, you can see how code grows before you even run it.\n\n\n\n4. Algorithmic Paradigms (Greedy, Divide and Conquer, DP)\nOnce you can measure performance, it’s time to explore how algorithms are designed. Behind every clever solution is a guiding paradigm, a way of thinking about problems.\nThree of the most powerful are:\n\nGreedy Algorithms\nDivide and Conquer\nDynamic Programming (DP)\n\nEach represents a different mindset for problem solving.\n\n1. Greedy Algorithms\nA greedy algorithm makes the best local choice at each step, hoping it leads to a global optimum.\nThink of it like:\n\n“Take what looks best right now, and don’t worry about the future.”\n\nThey are fast and simple, but not always correct. They only work when the greedy choice property holds.\nExample: Coin Change (Greedy version) Suppose you want to make 63 cents using US coins (25, 10, 5, 1). The greedy approach:\n\nTake 25 → 38 left\nTake 25 → 13 left\nTake 10 → 3 left\nTake 1 × 3\n\nThis works here, but not always (try coins 1, 3, 4 for amount 6). Simple, but not guaranteed optimal.\nCommon greedy algorithms:\n\nKruskal’s Minimum Spanning Tree\nPrim’s Minimum Spanning Tree\nDijkstra’s Shortest Path (non-negative weights)\nHuffman Coding\n\n\n\n2. Divide and Conquer\nThis is a classic paradigm. You break the problem into smaller subproblems, solve each recursively, and then combine the results.\nIt’s like splitting a task among friends, then merging their answers.\nFormally:\n\\[\nT(n) = aT\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nExamples:\n\nMerge Sort: divide the array, sort halves, merge\nQuick Sort: partition around a pivot\nBinary Search: halve the range each step\n\nElegant and powerful, but recursion overhead can add cost if poorly structured.\n\n\n3. Dynamic Programming (DP)\nDP is for problems with overlapping subproblems and optimal substructure. You solve smaller subproblems once and store the results to avoid recomputation.\nIt’s like divide and conquer with memory.\nExample: Fibonacci Naive recursion is exponential. DP with memoization is linear.\nint fib(int n) {\n    if (n &lt;= 1) return n;\n    static int memo[1000] = {0};\n    if (memo[n]) return memo[n];\n    memo[n] = fib(n-1) + fib(n-2);\n    return memo[n];\n}\nEfficient reuse, but requires insight into subproblem structure.\n\n\nTiny Code\nQuick comparison using Fibonacci:\nNaive (Divide and Conquer)\nint fib_dc(int n) {\n    if (n &lt;= 1) return n;\n    return fib_dc(n-1) + fib_dc(n-2);  // exponential\n}\nDP (Memoization)\nint fib_dp(int n, int memo[]) {\n    if (n &lt;= 1) return n;\n    if (memo[n]) return memo[n];\n    return memo[n] = fib_dp(n-1, memo) + fib_dp(n-2, memo);\n}\n\n\nWhy It Matters\nAlgorithmic paradigms give you patterns for design:\n\nGreedy: when local choices lead to a global optimum\nDivide and Conquer: when the problem splits naturally\nDynamic Programming: when subproblems overlap\n\nOnce you recognize a problem’s structure, you’ll instantly know which mindset fits best.\nThink of paradigms as templates for reasoning, not just techniques but philosophies.\n\n\nTry It Yourself\n\nWrite a greedy algorithm to make change using coins [1, 3, 4] for amount 6. Does it work?\nImplement merge sort using divide and conquer.\nSolve Fibonacci both ways (naive vs DP) and compare speeds.\nThink of a real-life task you solve greedily.\n\nLearning paradigms is like learning styles of thought. Once you know them, every problem starts to look familiar.\n\n\n\n5. Recurrence Relations\nEvery time you break a problem into smaller subproblems, you create a recurrence, a mathematical way to describe how the total cost grows.\nRecurrence relations are the backbone of analyzing recursive algorithms. They tell us how much time or space an algorithm uses, based on the cost of its subproblems.\n\nWhat Is a Recurrence?\nA recurrence relation expresses \\(T(n)\\), the total cost for input size \\(n\\), in terms of smaller instances.\nExample (Merge Sort):\n\\[\nT(n) = 2T(n/2) + O(n)\n\\]\nThat means:\n\nIt divides the problem into 2 halves (\\(2T(n/2)\\))\nMerges results in \\(O(n)\\) time\n\nYou will often see recurrences like:\n\n\\(T(n) = T(n - 1) + O(1)\\)\n\\(T(n) = 2T(n/2) + O(n)\\)\n\\(T(n) = T(n/2) + O(1)\\)\n\nEach one represents a different structure of recursion.\n\n\nExample 1: Simple Linear Recurrence\nConsider this code:\nint count_down(int n) {\n    if (n == 0) return 0;\n    return 1 + count_down(n - 1);\n}\nThis calls itself once for each smaller input:\n\\[\nT(n) = T(n - 1) + O(1)\n\\]\nSolve it:\n\\[\nT(n) = O(n)\n\\]\nBecause it runs once per level.\n\n\nExample 2: Binary Recurrence\nFor binary recursion:\nint sum_tree(int n) {\n    if (n == 1) return 1;\n    return sum_tree(n/2) + sum_tree(n/2) + 1;\n}\nHere we do two subcalls on \\(n/2\\) and a constant amount of extra work:\n\\[\nT(n) = 2T(n/2) + O(1)\n\\]\nSolve it: \\(T(n) = O(n)\\)\nWhy? Each level doubles the number of calls but halves the size. There are \\(\\log n\\) levels, and total work adds up to \\(O(n)\\).\n\n\nSolving Recurrences\nThere are several ways to solve them:\n\nSubstitution Method Guess the solution, then prove it by induction.\nRecursion Tree Method Expand the recurrence into a tree and sum the cost per level.\nMaster Theorem Use a formula when the recurrence matches:\n\\[\nT(n) = aT(n/b) + f(n)\n\\]\n\n\n\nMaster Theorem (Quick Summary)\nIf \\(T(n) = aT(n/b) + f(n)\\), then:\n\nIf \\(f(n) = O(n^{\\log_b a - \\epsilon})\\), then \\(T(n) = \\Theta(n^{\\log_b a})\\)\nIf \\(f(n) = \\Theta(n^{\\log_b a})\\), then \\(T(n) = \\Theta(n^{\\log_b a} \\log n)\\)\nIf \\(f(n) = \\Omega(n^{\\log_b a + \\epsilon})\\), and the regularity condition holds, then \\(T(n) = \\Theta(f(n))\\)\n\nExample (Merge Sort): \\(a = 2\\), \\(b = 2\\), \\(f(n) = O(n)\\)\n\\[\nT(n) = 2T(n/2) + O(n) = O(n \\log n)\n\\]\n\n\nTiny Code\nLet’s write a quick recursive sum:\nint sum_array(int arr[], int l, int r) {\n    if (l == r) return arr[l];\n    int mid = (l + r) / 2;\n    return sum_array(arr, l, mid) + sum_array(arr, mid+1, r);\n}\nRecurrence:\n\\[\nT(n) = 2T(n/2) + O(1)\n\\]\n→ \\(O(n)\\)\nIf you added merging (like in merge sort), you would get \\(+O(n)\\):\n→ \\(O(n \\log n)\\)\n\n\nWhy It Matters\nRecurrence relations let you predict the cost of recursive solutions.\nWithout them, recursion feels like magic. With them, you can quantify efficiency.\nThey are key to understanding:\n\nDivide and Conquer\nDynamic Programming\nBacktracking\n\nOnce you can set up a recurrence, solving it becomes a game of algebra and logic.\n\n\nTry It Yourself\n\nWrite a recurrence for binary search. Solve it.\nWrite a recurrence for merge sort. Solve it.\nAnalyze this function:\nvoid fun(int n) {\n    if (n &lt;= 1) return;\n    fun(n/2);\n    fun(n/3);\n    fun(n/6);\n}\nWhat’s the recurrence? Approximate the complexity.\nExpand \\(T(n) = T(n-1) + 1\\) into its explicit sum.\n\nLearning recurrences helps you see inside recursion. They turn code into equations.\n\n\n\n6. Searching Basics\nBefore we sort or optimize, we need a way to find things. Searching is one of the most fundamental actions in computing, whether it’s looking up a name, finding a key, or checking if something exists.\nA search algorithm takes a collection (array, list, tree, etc.) and a target, and returns whether the target is present (and often its position).\nLet’s begin with two foundational techniques: Linear Search and Binary Search.\n\n1. Linear Search\nLinear search is the simplest method:\n\nStart at the beginning\nCheck each element in turn\nStop if you find the target\n\nIt works on any list, sorted or not, but can be slow for large data.\nint linear_search(int arr[], int n, int key) {\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] == key) return i;\n    }\n    return -1;\n}\nExample: If arr = [2, 4, 6, 8, 10] and key = 6, it finds it at index 2.\nComplexity:\n\nTime: \\(O(n)\\)\nSpace: \\(O(1)\\)\n\nLinear search is simple and guaranteed to find the target if it exists, but slow when lists are large.\n\n\n2. Binary Search\nWhen the list is sorted, we can do much better. Binary search repeatedly divides the search space in half.\nSteps:\n\nCheck the middle element\nIf it matches, you’re done\nIf target &lt; mid, search the left half\nElse, search the right half\n\nint binary_search(int arr[], int n, int key) {\n    int low = 0, high = n - 1;\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] == key) return mid;\n        else if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\nExample: arr = [2, 4, 6, 8, 10], key = 8\n\nmid = 6 → key &gt; mid → search right half\nmid = 8 → found\n\nComplexity:\n\nTime: \\(O(\\log n)\\)\nSpace: \\(O(1)\\)\n\nBinary search is a massive improvement; doubling input only adds one extra step.\n\n\n3. Recursive Binary Search\nBinary search can also be written recursively:\nint binary_search_rec(int arr[], int low, int high, int key) {\n    if (low &gt; high) return -1;\n    int mid = (low + high) / 2;\n    if (arr[mid] == key) return mid;\n    else if (arr[mid] &gt; key) return binary_search_rec(arr, low, mid - 1, key);\n    else return binary_search_rec(arr, mid + 1, high, key);\n}\nSame logic, different structure. Both iterative and recursive forms are equally efficient.\n\n\n4. Choosing Between Them\n\n\n\nMethod\nWorks On\nTime\nSpace\nNeeds Sorting\n\n\n\n\nLinear Search\nAny list\nO(n)\nO(1)\nNo\n\n\nBinary Search\nSorted list\nO(log n)\nO(1)\nYes\n\n\n\nIf data is unsorted or very small, linear search is fine. If data is sorted and large, binary search is far superior.\n\n\nTiny Code\nCompare the steps: For \\(n = 16\\):\n\nLinear search → up to 16 comparisons\nBinary search → \\(\\log_2 16 = 4\\) comparisons\n\nThat’s a huge difference.\n\n\nWhy It Matters\nSearching is the core of information retrieval. Every database, compiler, and system relies on it.\nUnderstanding simple searches prepares you for:\n\nHash tables (constant-time lookups)\nTree searches (ordered structures)\nGraph traversals (structured exploration)\n\nIt’s not just about finding values; it’s about learning how data structure and algorithm design fit together.\n\n\nTry It Yourself\n\nWrite a linear search that returns all indices where a target appears.\nModify binary search to return the first occurrence of a target in a sorted array.\nCompare runtime on arrays of size 10, 100, 1000.\nWhat happens if you run binary search on an unsorted list?\n\nSearch is the foundation. Once you master it, you’ll recognize its patterns everywhere.\n\n\n\n7. Sorting Basics\nSorting is one of the most studied problems in computer science. Why? Because order matters. It makes searching faster, patterns clearer, and data easier to manage.\nA sorting algorithm arranges elements in a specific order (usually ascending or descending). Once sorted, many operations (like binary search, merging, or deduplication) become much simpler.\nLet’s explore the foundational sorting methods and the principles behind them.\n\n1. What Makes a Sort Algorithm\nA sorting algorithm should define:\n\nInput: a sequence of elements\nOutput: the same elements, in sorted order\nStability: keeps equal elements in the same order (important for multi-key sorts)\nIn-place: uses only a constant amount of extra space\n\nDifferent algorithms balance speed, memory, and simplicity.\n\n\n2. Bubble Sort\nIdea: repeatedly “bubble up” the largest element to the end by swapping adjacent pairs.\nvoid bubble_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        for (int j = 0; j &lt; n - i - 1; j++) {\n            if (arr[j] &gt; arr[j + 1]) {\n                int temp = arr[j];\n                arr[j] = arr[j + 1];\n                arr[j + 1] = temp;\n            }\n        }\n    }\n}\nEach pass moves the largest remaining item to its final position.\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(1)\\)\nStable: Yes\n\nSimple but inefficient for large data.\n\n\n3. Selection Sort\nIdea: repeatedly select the smallest element and put it in the correct position.\nvoid selection_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int min_idx = i;\n        for (int j = i + 1; j &lt; n; j++) {\n            if (arr[j] &lt; arr[min_idx]) min_idx = j;\n        }\n        int temp = arr[i];\n        arr[i] = arr[min_idx];\n        arr[min_idx] = temp;\n    }\n}\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(1)\\)\nStable: No\n\nFewer swaps, but still quadratic in time.\n\n\n4. Insertion Sort\nIdea: build the sorted list one item at a time, inserting each new item in the right place.\nvoid insertion_sort(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j &gt;= 0 && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nTime: \\(O(n^2)\\) (best case \\(O(n)\\) when nearly sorted)\nSpace: \\(O(1)\\)\nStable: Yes\n\nInsertion sort is great for small or nearly sorted datasets. It is often used as a base in hybrid sorts like Timsort.\n\n\n5. Comparing the Basics\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nStable\nIn-place\n\n\n\n\nBubble Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\n\n\nSelection Sort\nO(n²)\nO(n²)\nO(n²)\nNo\nYes\n\n\nInsertion Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\n\n\n\nAll three are quadratic in time, but Insertion Sort performs best on small or partially sorted data.\n\n\nTiny Code\nQuick check with arr = [5, 3, 4, 1, 2]:\nInsertion Sort (step by step)\n\nInsert 3 before 5 → [3, 5, 4, 1, 2]\nInsert 4 → [3, 4, 5, 1, 2]\nInsert 1 → [1, 3, 4, 5, 2]\nInsert 2 → [1, 2, 3, 4, 5]\n\nSorted!\n\n\nWhy It Matters\nSorting is a gateway algorithm. It teaches you about iteration, swapping, and optimization.\nEfficient sorting is critical for:\n\nPreprocessing data for binary search\nOrganizing data for analysis\nBuilding indexes and ranking systems\n\nIt’s the first step toward deeper concepts like divide and conquer and hybrid optimization.\n\n\nTry It Yourself\n\nImplement all three: bubble, selection, insertion\nTest them on arrays of size 10, 100, 1000, and note timing differences\nTry sorting an array that’s already sorted. Which one adapts best?\nModify insertion sort to sort in descending order\n\nSorting may seem simple, but it’s a cornerstone. Mastering it will shape your intuition for almost every other algorithm.\n\n\n\n8. Data Structures Overview\nAlgorithms and data structures are two sides of the same coin. An algorithm is how you solve a problem. A data structure is where you store and organize data so that your algorithm can work efficiently.\nYou can think of data structures as containers, each one shaped for specific access patterns, trade-offs, and performance needs. Choosing the right one is often the key to designing a fast algorithm.\n\n1. Why Data Structures Matter\nImagine you want to find a book quickly.\n\nIf all books are piled randomly → you must scan every one (\\(O(n)\\))\nIf they’re sorted on a shelf → you can use binary search (\\(O(\\log n)\\))\nIf you have an index or catalog → you can find it instantly (\\(O(1)\\))\n\nDifferent structures unlock different efficiencies.\n\n\n2. The Core Data Structures\nLet’s walk through the most essential ones:\n\n\n\n\n\n\n\n\n\nType\nDescription\nKey Operations\nTypical Use\n\n\n\n\nArray\nFixed-size contiguous memory\nAccess (\\(O(1)\\)), Insert/Delete (\\(O(n)\\))\nFast index access\n\n\nLinked List\nSequence of nodes with pointers\nInsert/Delete (\\(O(1)\\)), Access (\\(O(n)\\))\nDynamic sequences\n\n\nStack\nLIFO (last-in, first-out)\npush(), pop() in \\(O(1)\\)\nUndo, recursion\n\n\nQueue\nFIFO (first-in, first-out)\nenqueue(), dequeue() in \\(O(1)\\)\nScheduling, buffers\n\n\nHash Table\nKey-value pairs via hashing\nAverage \\(O(1)\\), Worst \\(O(n)\\)\nLookup, caching\n\n\nHeap\nPartially ordered tree\nInsert \\(O(\\log n)\\), Extract-Min \\(O(\\log n)\\)\nPriority queues\n\n\nTree\nHierarchical structure\nAccess \\(O(\\log n)\\) (balanced)\nSorted storage\n\n\nGraph\nNodes + edges\nTraversal \\(O(V+E)\\)\nNetworks, paths\n\n\nSet / Map\nUnique keys or key-value pairs\n\\(O(\\log n)\\) or \\(O(1)\\)\nMembership tests\n\n\n\nEach comes with trade-offs. Arrays are fast but rigid, linked lists are flexible but slower to access, and hash tables are lightning-fast but unordered.\n\n\n3. Abstract Data Types (ADTs)\nAn ADT defines what operations you can do, not how they’re implemented. For example, a Stack ADT promises:\n\npush(x)\npop()\npeek()\n\nIt can be implemented with arrays or linked lists, the behavior stays the same.\nCommon ADTs:\n\nStack\nQueue\nDeque\nPriority Queue\nMap / Dictionary\n\nThis separation of interface and implementation helps design flexible systems.\n\n\n4. The Right Tool for the Job\nChoosing the correct data structure often decides the performance of your algorithm:\n\n\n\nProblem\nGood Choice\nReason\n\n\n\n\nUndo feature\nStack\nLIFO fits history\n\n\nScheduling tasks\nQueue\nFIFO order\n\n\nDijkstra’s algorithm\nPriority Queue\nExtract smallest distance\n\n\nCounting frequencies\nHash Map\nFast key lookup\n\n\nDynamic median\nHeap + Heap\nBalance two halves\n\n\nSearch by prefix\nTrie\nFast prefix lookups\n\n\n\nGood programmers don’t just write code. They pick the right structure.\n\n\nTiny Code\nExample: comparing array vs linked list\nArray:\nint arr[5] = {1, 2, 3, 4, 5};\nprintf(\"%d\", arr[3]); // O(1)\nLinked List:\nstruct Node { int val; struct Node* next; };\nTo get the 4th element, you must traverse → \\(O(n)\\)\nDifferent structures, different access costs.\n\n\nWhy It Matters\nEvery efficient algorithm depends on the right data structure.\n\nSearching, sorting, and storing all rely on structure\nMemory layout affects cache performance\nThe wrong choice can turn \\(O(1)\\) into \\(O(n^2)\\)\n\nUnderstanding these structures is like knowing the tools in a workshop. Once you recognize their shapes, you’ll instinctively know which to grab.\n\n\nTry It Yourself\n\nImplement a stack using an array. Then implement it using a linked list.\nWrite a queue using two stacks.\nTry storing key-value pairs in a hash table (hint: mod by table size).\nCompare access times for arrays vs linked lists experimentally.\n\nData structures aren’t just storage. They are the skeletons your algorithms stand on.\n\n\n\n9. Graphs and Trees Overview\nNow that you’ve seen linear structures like arrays and linked lists, it’s time to explore nonlinear structures, graphs and trees. These are the shapes behind networks, hierarchies, and relationships.\nThey appear everywhere: family trees, file systems, maps, social networks, and knowledge graphs all rely on them.\n\n1. Trees\nA tree is a connected structure with no cycles. It’s a hierarchy, and every node (except the root) has one parent.\n\nRoot: the top node\nChild: a node directly connected below\nLeaf: a node with no children\nHeight: the longest path from root to a leaf\n\nA binary tree is one where each node has at most two children. A binary search tree (BST) keeps elements ordered:\n\nLeft child &lt; parent &lt; right child\n\nBasic operations:\n\nInsert\nSearch\nDelete\nTraverse (preorder, inorder, postorder, level-order)\n\nExample:\nstruct Node {\n    int val;\n    struct Node *left, *right;\n};\nInsert in BST:\nstruct Node* insert(struct Node* root, int val) {\n    if (!root) return newNode(val);\n    if (val &lt; root-&gt;val) root-&gt;left = insert(root-&gt;left, val);\n    else root-&gt;right = insert(root-&gt;right, val);\n    return root;\n}\n\n\n2. Common Tree Types\n\n\n\n\n\n\n\n\nType\nDescription\nUse Case\n\n\n\n\nBinary Tree\nEach node has ≤ 2 children\nGeneral hierarchy\n\n\nBinary Search Tree\nLeft &lt; Root &lt; Right\nOrdered data\n\n\nAVL / Red-Black Tree\nSelf-balancing BST\nFast search/insert\n\n\nHeap\nComplete binary tree, parent ≥ or ≤ children\nPriority queues\n\n\nTrie\nTree of characters\nPrefix search\n\n\nSegment Tree\nTree over ranges\nRange queries\n\n\nFenwick Tree\nTree with prefix sums\nEfficient updates\n\n\n\nBalanced trees keep height \\(O(\\log n)\\), guaranteeing fast operations.\n\n\n3. Graphs\nA graph generalizes the idea of trees. In graphs, nodes (vertices) can connect freely.\nA graph is a set of vertices (\\(V\\)) and edges (\\(E\\)):\n\\[\nG = (V, E)\n\\]\nDirected vs Undirected:\n\nDirected: edges have direction (A → B)\nUndirected: edges connect both ways (A, B)\n\nWeighted vs Unweighted:\n\nWeighted: each edge has a cost\nUnweighted: all edges are equal\n\nRepresentation:\n\nAdjacency Matrix: \\(n \\times n\\) matrix; entry \\((i, j) = 1\\) if edge exists\nAdjacency List: array of lists; each vertex stores its neighbors\n\nExample adjacency list:\nvector&lt;int&gt; graph[n];\ngraph[0].push_back(1);\ngraph[0].push_back(2);\n\n\n4. Common Graph Types\n\n\n\n\n\n\n\n\nGraph Type\nDescription\nExample\n\n\n\n\nUndirected\nEdges without direction\nFriendship network\n\n\nDirected\nArrows indicate direction\nWeb links\n\n\nWeighted\nEdges have costs\nRoad network\n\n\nCyclic\nContains loops\nTask dependencies\n\n\nAcyclic\nNo loops\nFamily tree\n\n\nDAG (Directed Acyclic)\nDirected, no cycles\nScheduling, compilers\n\n\nComplete\nAll pairs connected\nDense networks\n\n\nSparse\nFew edges\nReal-world graphs\n\n\n\n\n\n5. Basic Graph Operations\n\nAdd Vertex / Edge\nTraversal: Depth-First Search (DFS), Breadth-First Search (BFS)\nPath Finding: Dijkstra, Bellman-Ford\nConnectivity: Union-Find, Tarjan (SCC)\nSpanning Trees: Kruskal, Prim\n\nEach graph problem has its own flavor, from finding shortest paths to detecting cycles.\n\n\nTiny Code\nBreadth-first search (BFS):\nvoid bfs(int start, vector&lt;int&gt; graph[], int n) {\n    bool visited[n];\n    memset(visited, false, sizeof(visited));\n    queue&lt;int&gt; q;\n    visited[start] = true;\n    q.push(start);\n    while (!q.empty()) {\n        int node = q.front(); q.pop();\n        printf(\"%d \", node);\n        for (int neighbor : graph[node]) {\n            if (!visited[neighbor]) {\n                visited[neighbor] = true;\n                q.push(neighbor);\n            }\n        }\n    }\n}\nThis explores level by level, perfect for shortest paths in unweighted graphs.\n\n\nWhy It Matters\nTrees and graphs model relationships and connections, not just sequences. They are essential for:\n\nSearch engines (web graph)\nCompilers (syntax trees, dependency DAGs)\nAI (state spaces, decision trees)\nDatabases (indexes, joins, relationships)\n\nUnderstanding them unlocks an entire world of algorithms, from DFS and BFS to Dijkstra, Kruskal, and beyond.\n\n\nTry It Yourself\n\nBuild a simple binary search tree and implement inorder traversal.\nRepresent a graph with adjacency lists and print all edges.\nWrite a DFS and BFS for a small graph.\nDraw a directed graph with a cycle and detect it manually.\n\nGraphs and trees move you beyond linear thinking. They let you explore connections, not just collections.\n\n\n\n10. Algorithm Design Patterns\nBy now, you’ve seen what algorithms are and how they’re analyzed. You’ve explored searches, sorts, structures, and recursion. The next step is learning patterns, reusable strategies that guide how you build new algorithms from scratch.\nJust like design patterns in software architecture, algorithmic design patterns give structure to your thinking. Once you recognize them, many problems suddenly feel familiar.\n\n1. Brute Force\nStart simple. Try every possibility and pick the best result. Brute force is often your baseline, clear but inefficient.\nExample: Find the maximum subarray sum by checking all subarrays.\n\nTime: \\(O(n^2)\\)\nAdvantage: easy to reason about\nDisadvantage: explodes for large input\n\nSometimes, brute force helps you see the structure needed for a better approach.\n\n\n2. Divide and Conquer\nSplit the problem into smaller parts, solve each, and combine. Ideal for problems with self-similarity.\nClassic examples:\n\nMerge Sort → split and merge\nBinary Search → halve the search space\nQuick Sort → partition and sort\n\nGeneral form:\n\\[\nT(n) = aT(n/b) + f(n)\n\\]\nUse recurrence relations and the Master Theorem to analyze them.\n\n\n3. Greedy\nMake the best local decision at each step. Works only when local optimal choices lead to a global optimum.\nExamples:\n\nActivity Selection\nHuffman Coding\nDijkstra (for non-negative weights)\n\nGreedy algorithms are simple and fast when they fit.\n\n\n4. Dynamic Programming (DP)\nWhen subproblems overlap, store results and reuse them. Think recursion plus memory.\nTwo main styles:\n\nTop-Down (Memoization): recursive with caching\nBottom-Up (Tabulation): iterative filling table\n\nUsed in:\n\nFibonacci numbers\nKnapsack\nLongest Increasing Subsequence (LIS)\nMatrix Chain Multiplication\n\nDP transforms exponential recursion into polynomial time.\n\n\n5. Backtracking\nExplore all possibilities, but prune when constraints fail. It is brute force with early exits.\nPerfect for:\n\nN-Queens\nSudoku\nPermutation generation\nSubset sums\n\nBacktracking builds solutions incrementally, abandoning paths that cannot lead to a valid result.\n\n\n6. Two Pointers\nMove two indices through a sequence to find patterns or meet conditions.\nCommon use:\n\nSorted arrays (sum pairs, partitions)\nString problems (palindromes, sliding windows)\nLinked lists (slow and fast pointers)\n\nSimple, but surprisingly powerful.\n\n\n7. Sliding Window\nMaintain a window over data, expand or shrink it as needed.\nUsed for:\n\nMaximum sum subarray (Kadane’s algorithm)\nSubstrings of length \\(k\\)\nLongest substring without repeating characters\n\nHelps reduce \\(O(n^2)\\) to \\(O(n)\\) in sequence problems.\n\n\n8. Binary Search on Answer\nSometimes, the input is not sorted, but the answer space is. If you can define a function check(mid) that is monotonic (true or false changes once), you can apply binary search on possible answers.\nExamples:\n\nMinimum capacity to ship packages in D days\nSmallest feasible value satisfying a constraint\n\nPowerful for optimization under monotonic conditions.\n\n\n9. Graph-Based\nThink in terms of nodes and edges, paths and flows.\nPatterns include:\n\nBFS and DFS (exploration)\nTopological Sort (ordering)\nDijkstra and Bellman-Ford (shortest paths)\nUnion-Find (connectivity)\nKruskal and Prim (spanning trees)\n\nGraphs often reveal relationships hidden in data.\n\n\n10. Meet in the Middle\nSplit the problem into two halves, compute all possibilities for each, and combine efficiently. Used in problems where brute force \\(O(2^n)\\) is too large but \\(O(2^{n/2})\\) is manageable.\nExamples:\n\nSubset sum (divide into two halves)\nSearch problems in combinatorics\n\nA clever compromise between brute force and efficiency.\n\n\nTiny Code\nExample: Two Pointers to find a pair sum\nint find_pair_sum(int arr[], int n, int target) {\n    int i = 0, j = n - 1;\n    while (i &lt; j) {\n        int sum = arr[i] + arr[j];\n        if (sum == target) return 1;\n        else if (sum &lt; target) i++;\n        else j--;\n    }\n    return 0;\n}\nWorks in \\(O(n)\\) for sorted arrays, elegant and fast.\n\n\nWhy It Matters\nPatterns are mental shortcuts. They turn “blank page” problems into “I’ve seen this shape before.”\nOnce you recognize the structure, you can choose a suitable pattern and adapt it. This is how top coders solve complex problems under time pressure, not by memorizing algorithms, but by seeing patterns.\n\n\nTry It Yourself\n\nWrite a brute-force and a divide-and-conquer solution for maximum subarray sum. Compare speed.\nSolve the coin change problem using both greedy and DP.\nImplement N-Queens with backtracking.\nUse two pointers to find the smallest window with a given sum.\nPick a problem you’ve solved before. Can you reframe it using a different design pattern?\n\nThe more patterns you practice, the faster you will map new problems to known strategies, and the more powerful your algorithmic intuition will become.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-2.-sorting-and-searching",
    "href": "books/en-us/book.html#chapter-2.-sorting-and-searching",
    "title": "The Book",
    "section": "Chapter 2. Sorting and Searching",
    "text": "Chapter 2. Sorting and Searching\n\n11. Elementary Sorting (Bubble, Insertion, Selection)\nBefore diving into advanced sorts like mergesort or heapsort, it’s important to understand the elementary sorting algorithms , the building blocks. They’re simple, intuitive, and great for learning how sorting works under the hood.\nIn this section, we’ll cover three classics:\n\nBubble Sort - swap adjacent out-of-order pairs- Selection Sort - select the smallest element each time- Insertion Sort - insert elements one by one in order These algorithms share ( O\\(n^2\\) ) time complexity but differ in behavior and stability.\n\n\n1. Bubble Sort\nIdea: Compare adjacent pairs and swap if they’re out of order. Repeat until the array is sorted. Each pass “bubbles” the largest element to the end.\nSteps:\n\nCompare arr[j] and arr[j+1]\nSwap if arr[j] &gt; arr[j+1]\nContinue passes until no swaps are needed\n\nCode:\nvoid bubble_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int swapped = 0;\n        for (int j = 0; j &lt; n - i - 1; j++) {\n            if (arr[j] &gt; arr[j + 1]) {\n                int temp = arr[j];\n                arr[j] = arr[j + 1];\n                arr[j + 1] = temp;\n                swapped = 1;\n            }\n        }\n        if (!swapped) break;\n    }\n}\nComplexity:\n\nBest: ( O(n) ) (already sorted)- Worst: ( O\\(n^2\\) )- Space: ( O(1) )- Stable: Yes Intuition: Imagine bubbles rising , after each pass, the largest “bubble” settles at the top.\n\n\n\n2. Selection Sort\nIdea: Find the smallest element and place it at the front.\nSteps:\n\nFor each position i, find the smallest element in the remainder of the array\nSwap it with arr[i]\n\nCode:\nvoid selection_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int min_idx = i;\n        for (int j = i + 1; j &lt; n; j++) {\n            if (arr[j] &lt; arr[min_idx])\n                min_idx = j;\n        }\n        int temp = arr[i];\n        arr[i] = arr[min_idx];\n        arr[min_idx] = temp;\n    }\n}\nComplexity:\n\nBest: ( O\\(n^2\\) )- Worst: ( O\\(n^2\\) )- Space: ( O(1) )- Stable: No Intuition: Selection sort “selects” the next correct element and fixes it. It minimizes swaps but still scans all elements.\n\n\n\n3. Insertion Sort\nIdea: Build a sorted array one element at a time by inserting each new element into its correct position.\nSteps:\n\nStart from index 1\nCompare with previous elements\nShift elements greater than key to the right\nInsert key into the correct place\n\nCode:\nvoid insertion_sort(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j &gt;= 0 && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\nComplexity:\n\nBest: ( O(n) ) (nearly sorted)- Worst: ( O\\(n^2\\) )- Space: ( O(1) )- Stable: Yes Intuition: It’s like sorting cards in your hand , take the next card and slide it into the right place.\n\n\n\n4. Comparing the Three\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nStable\nIn-Place\nNotes\n\n\n\n\nBubble Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\nEarly exit possible\n\n\nSelection Sort\nO(n²)\nO(n²)\nO(n²)\nNo\nYes\nFew swaps\n\n\nInsertion Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\nGreat on small or nearly sorted data\n\n\n\n\n\nTiny Code\nLet’s see how insertion sort works on [5, 3, 4, 1, 2]:\n\nStart with 3 → insert before 5 → [3, 5, 4, 1, 2]- Insert 4 → [3, 4, 5, 1, 2]- Insert 1 → [1, 3, 4, 5, 2]- Insert 2 → [1, 2, 3, 4, 5] Sorted in five passes.\n\n\n\nWhy It Matters\nElementary sorts teach you:\n\nHow comparisons and swaps drive order- The trade-off between simplicity and efficiency- How to reason about stability and adaptability While these aren’t used for large datasets in practice, they’re used inside hybrid algorithms like Timsort and IntroSort, which switch to insertion sort for small chunks.\n\n\n\nTry It Yourself\n\nImplement all three and print the array after each pass.\nTest on arrays: already sorted, reversed, random, partially sorted.\nModify bubble sort to sort descending.\nTry insertion sort on 10,000 elements and note its behavior.\nCan you detect when the list is already sorted and stop early?\n\nStart simple. Master these patterns. They’ll be your foundation for everything from merge sort to radix sort.\n\n\n\n12. Divide-and-Conquer Sorting (Merge, Quick, Heap)\nElementary sorts are great for learning, but their (O\\(n^2\\)) runtime quickly becomes a bottleneck. To scale beyond small arrays, we need algorithms that divide problems into smaller parts, sort them independently, and combine the results.\nThis is the essence of divide and conquer , break it down, solve subproblems, merge solutions. In sorting, this approach yields some of the fastest general-purpose algorithms: Merge Sort, Quick Sort, and Heap Sort.\n\n1. Merge Sort\nIdea: Split the array in half, sort each half recursively, then merge the two sorted halves.\nMerge sort is stable, works well with linked lists, and guarantees (O\\(n \\log n\\)) time.\nSteps:\n\nDivide the array into halves\nRecursively sort each half\nMerge two sorted halves into one\n\nCode:\nvoid merge(int arr[], int l, int m, int r) {\n    int n1 = m - l + 1;\n    int n2 = r - m;\n    int L[n1], R[n2];\n    for (int i = 0; i &lt; n1; i++) L[i] = arr[l + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = arr[m + 1 + j];\n    int i = 0, j = 0, k = l;\n    while (i &lt; n1 && j &lt; n2) {\n        if (L[i] &lt;= R[j]) arr[k++] = L[i++];\n        else arr[k++] = R[j++];\n    }\n    while (i &lt; n1) arr[k++] = L[i++];\n    while (j &lt; n2) arr[k++] = R[j++];\n}\n\nvoid merge_sort(int arr[], int l, int r) {\n    if (l &lt; r) {\n        int m = (l + r) / 2;\n        merge_sort(arr, l, m);\n        merge_sort(arr, m + 1, r);\n        merge(arr, l, m, r);\n    }\n}\nComplexity:\n\nTime: (O\\(n \\log n\\)) (always)- Space: (O(n)) (temporary arrays)- Stable: Yes Merge sort is predictable, making it ideal for external sorting (like sorting data on disk).\n\n\n\n2. Quick Sort\nIdea: Pick a pivot, partition the array so smaller elements go left and larger go right, then recursively sort both sides.\nQuick sort is usually the fastest in practice due to good cache locality and low constant factors.\nSteps:\n\nChoose a pivot (often middle or random)\nPartition: move smaller elements to left, larger to right\nRecursively sort the two partitions\n\nCode:\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            i++;\n            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;\n        }\n    }\n    int tmp = arr[i + 1]; arr[i + 1] = arr[high]; arr[high] = tmp;\n    return i + 1;\n}\n\nvoid quick_sort(int arr[], int low, int high) {\n    if (low &lt; high) {\n        int pi = partition(arr, low, high);\n        quick_sort(arr, low, pi - 1);\n        quick_sort(arr, pi + 1, high);\n    }\n}\nComplexity:\n\nBest / Average: (O\\(n \\log n\\))- Worst: (O\\(n^2\\)) (bad pivot, e.g. sorted input with naive pivot)- Space: (O\\(\\log n\\)) (recursion)- Stable: No (unless modified) Quick sort is often used in standard libraries due to its efficiency in real-world workloads.\n\n\n\n3. Heap Sort\nIdea: Turn the array into a heap, repeatedly extract the largest element, and place it at the end.\nA heap is a binary tree where every parent is ≥ its children (max-heap).\nSteps:\n\nBuild a max-heap\nSwap the root (max) with the last element\nReduce heap size, re-heapify\nRepeat until sorted\n\nCode:\nvoid heapify(int arr[], int n, int i) {\n    int largest = i;\n    int l = 2 * i + 1;\n    int r = 2 * i + 2;\n    if (l &lt; n && arr[l] &gt; arr[largest]) largest = l;\n    if (r &lt; n && arr[r] &gt; arr[largest]) largest = r;\n    if (largest != i) {\n        int tmp = arr[i]; arr[i] = arr[largest]; arr[largest] = tmp;\n        heapify(arr, n, largest);\n    }\n}\n\nvoid heap_sort(int arr[], int n) {\n    for (int i = n / 2 - 1; i &gt;= 0; i--)\n        heapify(arr, n, i);\n    for (int i = n - 1; i &gt; 0; i--) {\n        int tmp = arr[0]; arr[0] = arr[i]; arr[i] = tmp;\n        heapify(arr, i, 0);\n    }\n}\nComplexity:\n\nTime: (O\\(n \\log n\\))- Space: (O(1))- Stable: No Heap sort is reliable and space-efficient but less cache-friendly than quicksort.\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nSpace\nStable\nNotes\n\n\n\n\nMerge Sort\nO(n log n)\nO(n log n)\nO(n log n)\nO(n)\nYes\nPredictable, stable\n\n\nQuick Sort\nO(n log n)\nO(n log n)\nO(n²)\nO(log n)\nNo\nFast in practice\n\n\nHeap Sort\nO(n log n)\nO(n log n)\nO(n log n)\nO(1)\nNo\nIn-place, robust\n\n\n\nEach one fits a niche:\n\nMerge Sort → stability and guarantees- Quick Sort → speed and cache performance- Heap Sort → low memory usage and simplicity\n\n\n\nTiny Code\nTry sorting [5, 1, 4, 2, 8] with merge sort:\n\nSplit → [5,1,4], [2,8]\nSort each → [1,4,5], [2,8]\nMerge → [1,2,4,5,8]\n\nEach recursive split halves the problem, yielding (O\\(\\log n\\)) depth with (O(n)) work per level.\n\n\nWhy It Matters\nDivide-and-conquer sorting is the foundation for efficient order processing. It introduces ideas you’ll reuse in:\n\nBinary search (halving)- Matrix multiplication- Fast Fourier Transform- Dynamic programming These sorts teach how recursion, partitioning, and merging combine into scalable solutions.\n\n\n\nTry It Yourself\n\nImplement merge sort, quick sort, and heap sort.\nTest all three on the same random array. Compare runtime.\nModify quick sort to use a random pivot.\nBuild a stable version of heap sort.\nVisualize merge sort’s recursion tree and merging process.\n\nMastering these sorts gives you a template for solving any divide-and-conquer problem efficiently.\n\n\n\n13. Counting and Distribution Sorts (Counting, Radix, Bucket)\nSo far, we’ve seen comparison-based sorts like merge sort and quicksort. These rely on comparing elements and are bounded by the O(n log n) lower limit for comparisons.\nBut what if you don’t need to compare elements directly , what if they’re integers or values from a limited range?\nThat’s where counting and distribution sorts come in. They exploit structure, not just order, to achieve linear-time sorting in the right conditions.\n\n1. Counting Sort\nIdea: If your elements are integers in a known range ([0, k)), you can count occurrences of each value, then reconstruct the sorted output.\nCounting sort doesn’t compare , it counts.\nSteps:\n\nFind the range of input (max value (k))\nCount occurrences in a frequency array\nConvert counts to cumulative counts\nPlace elements into their sorted positions\n\nCode:\nvoid counting_sort(int arr[], int n, int k) {\n    int count[k + 1];\n    int output[n];\n    for (int i = 0; i &lt;= k; i++) count[i] = 0;\n    for (int i = 0; i &lt; n; i++) count[arr[i]]++;\n    for (int i = 1; i &lt;= k; i++) count[i] += count[i - 1];\n    for (int i = n - 1; i &gt;= 0; i--) {\n        output[count[arr[i]] - 1] = arr[i];\n        count[arr[i]]--;\n    }\n    for (int i = 0; i &lt; n; i++) arr[i] = output[i];\n}\nExample: arr = [4, 2, 2, 8, 3, 3, 1], k = 8 → count = [0,1,2,2,1,0,0,0,1] → cumulative = [0,1,3,5,6,6,6,6,7] → sorted = [1,2,2,3,3,4,8]\nComplexity:\n\nTime: (O(n + k))- Space: (O(k))- Stable: Yes When to use:\nInput is integers- Range (k) not much larger than (n)\n\n\n\n2. Radix Sort\nIdea: Sort digits one at a time, from least significant (LSD) or most significant (MSD), using a stable sub-sort like counting sort.\nRadix sort works best when all elements have fixed-length representations (e.g., integers, strings of equal length).\nSteps (LSD method):\n\nFor each digit position (from rightmost to leftmost)\nSort all elements by that digit using a stable sort (like counting sort)\n\nCode:\nint get_max(int arr[], int n) {\n    int mx = arr[0];\n    for (int i = 1; i &lt; n; i++)\n        if (arr[i] &gt; mx) mx = arr[i];\n    return mx;\n}\n\nvoid counting_sort_digit(int arr[], int n, int exp) {\n    int output[n];\n    int count[10] = {0};\n    for (int i = 0; i &lt; n; i++)\n        count[(arr[i] / exp) % 10]++;\n    for (int i = 1; i &lt; 10; i++)\n        count[i] += count[i - 1];\n    for (int i = n - 1; i &gt;= 0; i--) {\n        int digit = (arr[i] / exp) % 10;\n        output[count[digit] - 1] = arr[i];\n        count[digit]--;\n    }\n    for (int i = 0; i &lt; n; i++)\n        arr[i] = output[i];\n}\n\nvoid radix_sort(int arr[], int n) {\n    int m = get_max(arr, n);\n    for (int exp = 1; m / exp &gt; 0; exp *= 10)\n        counting_sort_digit(arr, n, exp);\n}\nExample: arr = [170, 45, 75, 90, 802, 24, 2, 66] → sort by 1s → 10s → 100s → final = [2, 24, 45, 66, 75, 90, 170, 802]\nComplexity:\n\nTime: (O(d (n + b))), where\n\n(d): number of digits - (b): base (10 for decimal)- Space: (O(n + b))- Stable: Yes When to use:\n\nFixed-length numbers- Bounded digits (e.g., base 10 or 2)\n\n\n\n3. Bucket Sort\nIdea: Divide elements into buckets based on value ranges, sort each bucket individually, then concatenate.\nWorks best when data is uniformly distributed in a known interval.\nSteps:\n\nCreate (k) buckets for value ranges\nDistribute elements into buckets\nSort each bucket (often using insertion sort)\nMerge buckets\n\nCode:\nvoid bucket_sort(float arr[], int n) {\n    vector&lt;float&gt; buckets[n];\n    for (int i = 0; i &lt; n; i++) {\n        int idx = n * arr[i]; // assuming 0 &lt;= arr[i] &lt; 1\n        buckets[idx].push_back(arr[i]);\n    }\n    for (int i = 0; i &lt; n; i++)\n        sort(buckets[i].begin(), buckets[i].end());\n    int idx = 0;\n    for (int i = 0; i &lt; n; i++)\n        for (float val : buckets[i])\n            arr[idx++] = val;\n}\nComplexity:\n\nAverage: (O(n + k))- Worst: (O\\(n^2\\)) (if all fall in one bucket)- Space: (O(n + k))- Stable: Depends on bucket sort method When to use:\nReal numbers uniformly distributed in ([0,1))\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nStable\nType\nBest Use\n\n\n\n\nCounting Sort\nO(n + k)\nO(k)\nYes\nNon-comparison\nSmall integer range\n\n\nRadix Sort\nO(d(n + b))\nO(n + b)\nYes\nNon-comparison\nFixed-length numbers\n\n\nBucket Sort\nO(n + k) avg\nO(n + k)\nOften\nDistribution-based\nUniform floats\n\n\n\nThese algorithms achieve O(n) behavior when assumptions hold , they’re specialized but incredibly fast when applicable.\n\n\nTiny Code\nLet’s walk counting sort on arr = [4, 2, 2, 8, 3, 3, 1]:\n\nCount occurrences → [1,2,2,1,0,0,0,1]- Cumulative count → positions- Place elements → [1,2,2,3,3,4,8] Sorted , no comparisons.\n\n\n\nWhy It Matters\nDistribution sorts teach a key insight:\n\nIf you know the structure of your data, you can sort faster than comparison allows.\n\nThey show how data properties , range, distribution, digit length , can drive algorithm design.\nYou’ll meet these ideas again in:\n\nHashing (bucketing)- Indexing (range partitioning)- Machine learning (binning, histogramming)\n\n\n\nTry It Yourself\n\nImplement counting sort for integers from 0 to 100.\nExtend radix sort to sort strings by character.\nVisualize bucket sort for values between 0 and 1.\nWhat happens if you use counting sort on negative numbers? Fix it.\nCompare counting vs quick sort on small integer arrays.\n\nThese are the first glimpses of linear-time sorting , harnessing knowledge about data to break the (O\\(n \\log n\\)) barrier.\n\n\n\n14. Hybrid Sorts (IntroSort, Timsort)\nIn practice, no single sorting algorithm is perfect for all cases. Some are fast on average but fail in worst cases (like Quick Sort). Others are consistent but slow due to overhead (like Merge Sort). Hybrid sorting algorithms combine multiple techniques to get the best of all worlds , practical speed, stability, and guaranteed performance.\nTwo of the most widely used hybrids in modern systems are IntroSort and Timsort , both power the sorting functions in major programming languages.\n\n1. The Idea Behind Hybrid Sorting\nReal-world data is messy: sometimes nearly sorted, sometimes random, sometimes pathological. A smart sorting algorithm should adapt to the data.\nHybrids switch between different strategies based on:\n\nInput size- Recursion depth- Degree of order- Performance thresholds So, the algorithm “introspects” or “adapts” while running.\n\n\n\n2. IntroSort\nIntroSort (short for introspective sort) begins like Quick Sort, but when recursion gets too deep , which means Quick Sort’s worst case may be coming , it switches to Heap Sort to guarantee (O\\(n \\log n\\)) time.\nSteps:\n\nUse Quick Sort as long as recursion depth &lt; \\(2 \\log n\\)\nIf depth exceeds limit → switch to Heap Sort\nFor very small subarrays → switch to Insertion Sort\n\nThis triple combo ensures:\n\nFast average case (Quick Sort)- Guaranteed upper bound (Heap Sort)- Efficiency on small arrays (Insertion Sort) Code Sketch:\n\nvoid intro_sort(int arr[], int n) {\n    int depth_limit = 2 * log(n);\n    intro_sort_util(arr, 0, n - 1, depth_limit);\n}\n\nvoid intro_sort_util(int arr[], int begin, int end, int depth_limit) {\n    int size = end - begin + 1;\n    if (size &lt; 16) {\n        insertion_sort(arr + begin, size);\n        return;\n    }\n    if (depth_limit == 0) {\n        heap_sort_range(arr, begin, end);\n        return;\n    }\n    int pivot = partition(arr, begin, end);\n    intro_sort_util(arr, begin, pivot - 1, depth_limit - 1);\n    intro_sort_util(arr, pivot + 1, end, depth_limit - 1);\n}\nComplexity:\n\nAverage: (O\\(n \\log n\\))- Worst: (O\\(n \\log n\\))- Space: (O\\(\\log n\\))- Stable: No (depends on partition scheme) Used in:\nC++ STL’s std::sort- Many systems where performance guarantees matter\n\n\n\n3. Timsort\nTimsort is a stable hybrid combining Insertion Sort and Merge Sort. It was designed to handle real-world data, which often has runs (already sorted segments).\nDeveloped by Tim Peters (Python core dev), Timsort is now used in:\n\nPython’s sorted() and .sort()- Java’s Arrays.sort() for objects Idea:\nIdentify runs , segments already ascending or descending- Reverse descending runs (to make them ascending)- Sort small runs with Insertion Sort- Merge runs with Merge Sort Timsort adapts beautifully to partially ordered data.\n\nSteps:\n\nScan array, detect runs (sequences already sorted)\nPush runs to a stack\nMerge runs using a carefully balanced merge strategy\n\nPseudocode (simplified):\ndef timsort(arr):\n    RUN = 32\n    n = len(arr)\n\n    # Step 1: sort small chunks\n    for i in range(0, n, RUN):\n        insertion_sort(arr, i, min((i + RUN - 1), n - 1))\n\n    # Step 2: merge sorted runs\n    size = RUN\n    while size &lt; n:\n        for start in range(0, n, size * 2):\n            mid = start + size - 1\n            end = min(start + size * 2 - 1, n - 1)\n            merge(arr, start, mid, end)\n        size *= 2\nComplexity:\n\nBest: (O(n)) (already sorted data)- Average: (O\\(n \\log n\\))- Worst: (O\\(n \\log n\\))- Space: (O(n))- Stable: Yes Key Strengths:\nExcellent for real-world, partially sorted data- Stable (keeps equal keys in order)- Optimized merges (adaptive merging)\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBase Methods\nStability\nBest\nAverage\nWorst\nReal Use\n\n\n\n\nIntroSort\nQuick + Heap + Insertion\nNo\nO(n log n)\nO(n log n)\nO(n log n)\nC++ STL\n\n\nTimsort\nMerge + Insertion\nYes\nO(n)\nO(n log n)\nO(n log n)\nPython, Java\n\n\n\nIntroSort prioritizes performance guarantees. Timsort prioritizes adaptivity and stability.\nBoth show that “one size fits all” sorting doesn’t exist , great systems detect what’s going on and adapt.\n\n\nTiny Code\nSuppose we run Timsort on [1, 2, 3, 7, 6, 5, 8, 9]:\n\nDetect runs: [1,2,3], [7,6,5], [8,9]- Reverse [7,6,5] → [5,6,7]- Merge runs → [1,2,3,5,6,7,8,9] Efficient because it leverages the existing order.\n\n\n\nWhy It Matters\nHybrid sorts are the real-world heroes , they combine theory with practice. They teach an important principle:\n\nWhen one algorithm’s weakness shows up, switch to another’s strength.\n\nThese are not academic curiosities , they’re in your compiler, your browser, your OS, your database. Understanding them means you understand how modern languages optimize fundamental operations.\n\n\nTry It Yourself\n\nImplement IntroSort and test on random, sorted, and reverse-sorted arrays.\nSimulate Timsort’s run detection on nearly sorted input.\nCompare sorting speed of Insertion Sort vs Timsort for small arrays.\nAdd counters to Quick Sort and see when IntroSort should switch.\nExplore Python’s sorted() with different input shapes , guess when it uses merge vs insertion.\n\nHybrid sorts remind us: good algorithms adapt , they’re not rigid, they’re smart.\n\n\n\n15. Special Sorts (Cycle, Gnome, Comb, Pancake)\nNot all sorting algorithms follow the mainstream divide-and-conquer or distribution paradigms. Some were designed to solve niche problems, to illustrate elegant ideas, or simply to experiment with different mechanisms of ordering.\nThese special sorts, Cycle Sort, Gnome Sort, Comb Sort, and Pancake Sort, are fascinating not because they’re the fastest, but because they reveal creative ways to think about permutation, local order, and in-place operations.\n\n1. Cycle Sort\nIdea: Minimize the number of writes. Cycle sort rearranges elements into cycles, placing each value directly in its correct position. It performs exactly as many writes as there are misplaced elements, making it ideal for flash memory or systems where writes are expensive.\nSteps:\n\nFor each position i, find where arr[i] belongs (its rank).\nIf it’s not already there, swap it into position.\nContinue the cycle until the current position is correct.\nMove to the next index.\n\nCode:\nvoid cycle_sort(int arr[], int n) {\n    for (int cycle_start = 0; cycle_start &lt; n - 1; cycle_start++) {\n        int item = arr[cycle_start];\n        int pos = cycle_start;\n\n        for (int i = cycle_start + 1; i &lt; n; i++)\n            if (arr[i] &lt; item) pos++;\n\n        if (pos == cycle_start) continue;\n\n        while (item == arr[pos]) pos++;\n        int temp = arr[pos];\n        arr[pos] = item;\n        item = temp;\n\n        while (pos != cycle_start) {\n            pos = cycle_start;\n            for (int i = cycle_start + 1; i &lt; n; i++)\n                if (arr[i] &lt; item) pos++;\n            while (item == arr[pos]) pos++;\n            temp = arr[pos];\n            arr[pos] = item;\n            item = temp;\n        }\n    }\n}\nComplexity:\n\nTime: (O\\(n^2\\))- Writes: minimal (exactly n-c, where c = #cycles)- Stable: No Use Case: When minimizing writes is more important than runtime.\n\n\n\n2. Gnome Sort\nIdea: A simpler variation of insertion sort. Gnome sort moves back and forth like a “gnome” tidying flower pots: if two adjacent pots are out of order, swap and step back; otherwise, move forward.\nSteps:\n\nStart at index 1\nIf arr[i] &gt;= arr[i-1], move forward\nElse, swap and step back (if possible)\nRepeat until the end\n\nCode:\nvoid gnome_sort(int arr[], int n) {\n    int i = 1;\n    while (i &lt; n) {\n        if (i == 0 || arr[i] &gt;= arr[i - 1]) i++;\n        else {\n            int temp = arr[i]; arr[i] = arr[i - 1]; arr[i - 1] = temp;\n            i--;\n        }\n    }\n}\nComplexity:\n\nTime: (O\\(n^2\\))- Space: (O(1))- Stable: Yes Use Case: Educational simplicity. It’s a readable form of insertion logic without nested loops.\n\n\n\n3. Comb Sort\nIdea: An improvement over Bubble Sort by introducing a gap between compared elements, shrinking it gradually. By jumping farther apart early, Comb Sort helps eliminate small elements that are “stuck” near the end.\nSteps:\n\nStart with gap = n\nOn each pass, shrink gap = gap / 1.3\nCompare and swap items gap apart\nStop when gap = 1 and no swaps occur\n\nCode:\nvoid comb_sort(int arr[], int n) {\n    int gap = n;\n    int swapped = 1;\n    while (gap &gt; 1 || swapped) {\n        gap = (gap * 10) / 13;\n        if (gap == 9 || gap == 10) gap = 11;\n        if (gap &lt; 1) gap = 1;\n        swapped = 0;\n        for (int i = 0; i + gap &lt; n; i++) {\n            if (arr[i] &gt; arr[i + gap]) {\n                int temp = arr[i]; arr[i] = arr[i + gap]; arr[i + gap] = temp;\n                swapped = 1;\n            }\n        }\n    }\n}\nComplexity:\n\nAverage: (O\\(n \\log n\\))- Worst: (O\\(n^2\\))- Space: (O(1))- Stable: No Use Case: When a simple, in-place, nearly linear-time alternative to bubble sort is desired.\n\n\n\n4. Pancake Sort\nIdea: Sort an array using only one operation: flip (reversing a prefix). It’s like sorting pancakes on a plate, flip the stack so the largest pancake goes to the bottom, then repeat for the rest.\nSteps:\n\nFind the maximum unsorted element\nFlip it to the front\nFlip it again to its correct position\nReduce the unsorted portion by one\n\nCode:\nvoid flip(int arr[], int i) {\n    int start = 0;\n    while (start &lt; i) {\n        int temp = arr[start];\n        arr[start] = arr[i];\n        arr[i] = temp;\n        start++;\n        i--;\n    }\n}\n\nvoid pancake_sort(int arr[], int n) {\n    for (int curr_size = n; curr_size &gt; 1; curr_size--) {\n        int mi = 0;\n        for (int i = 1; i &lt; curr_size; i++)\n            if (arr[i] &gt; arr[mi]) mi = i;\n        if (mi != curr_size - 1) {\n            flip(arr, mi);\n            flip(arr, curr_size - 1);\n        }\n    }\n}\nComplexity:\n\nTime: (O\\(n^2\\))- Space: (O(1))- Stable: No Fun Fact: Pancake sort is the only known algorithm whose operations mimic a kitchen utensil, and inspired the Burnt Pancake Problem in combinatorics and genome rearrangement theory.\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nStable\nDistinctive Trait\n\n\n\n\nCycle Sort\nO(n²)\nO(1)\nNo\nMinimal writes\n\n\nGnome Sort\nO(n²)\nO(1)\nYes\nSimple insertion-like behavior\n\n\nComb Sort\nO(n log n) avg\nO(1)\nNo\nShrinking gap, improved bubble\n\n\nPancake Sort\nO(n²)\nO(1)\nNo\nPrefix reversals only\n\n\n\nEach highlights a different design goal:\n\nCycle: minimize writes- Gnome: simplify logic- Comb: optimize comparisons- Pancake: restrict operations\n\n\n\nTiny Code\nExample (Pancake Sort on [3, 6, 1, 9]):\n\nMax = 9 at index 3 → flip(3) → [9,1,6,3]\nflip(3) → [3,6,1,9] (9 fixed)\nMax = 6 → flip(1) → [6,3,1,9]\nflip(2) → [1,3,6,9]\n\nSorted using only flips.\n\n\nWhy It Matters\nSpecial sorts show there’s more than one way to think about ordering. They’re laboratories for exploring new ideas: minimizing swaps, limiting operations, or optimizing stability. Even if they’re not the go-to in production, they deepen your intuition about sorting mechanics.\n\n\nTry It Yourself\n\nImplement each algorithm and visualize their operations step-by-step.\nMeasure how many writes Cycle Sort performs vs. others.\nCompare Gnome and Insertion sort on nearly sorted arrays.\nModify Comb Sort’s shrink factor, how does performance change?\nWrite Pancake Sort with printouts of every flip to see the “stack” in motion.\n\nThese quirky algorithms prove that sorting isn’t just science, it’s also art and experimentation.\n\n\n\n16. Linear and Binary Search\nSearching is the process of finding a target value within a collection of data. Depending on whether the data is sorted or unsorted, you’ll use different strategies.\nIn this section, we revisit two of the most fundamental searching methods , Linear Search and Binary Search , and see how they underpin many higher-level algorithms and data structures.\n\n1. Linear Search\nIdea: Check each element one by one until you find the target. This is the simplest possible search and works on unsorted data.\nSteps:\n\nStart from index 0\nCompare arr[i] with the target\nIf match, return index\nIf end reached, return -1\n\nCode:\nint linear_search(int arr[], int n, int key) {\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] == key) return i;\n    }\n    return -1;\n}\nExample: arr = [7, 2, 4, 9, 1], key = 9\n\nCompare 7, 2, 4, then 9 → found at index 3 Complexity:\nTime: ( O(n) )- Space: ( O(1) )- Best case: ( O(1) ) (first element)- Worst case: ( O(n) ) Pros:\nWorks on any data (sorted or unsorted)- Simple to implement Cons:\nInefficient on large arrays Use it when data is small or unsorted, or when simplicity matters more than speed.\n\n\n\n2. Binary Search\nIdea: If the array is sorted, you can repeatedly halve the search space. Compare the middle element to the target , if it’s greater, search left; if smaller, search right.\nSteps:\n\nFind the midpoint\nIf arr[mid] == key, done\nIf arr[mid] &gt; key, search left\nIf arr[mid] &lt; key, search right\nRepeat until range is empty\n\nIterative Version:\nint binary_search(int arr[], int n, int key) {\n    int low = 0, high = n - 1;\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] == key) return mid;\n        else if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\nRecursive Version:\nint binary_search_rec(int arr[], int low, int high, int key) {\n    if (low &gt; high) return -1;\n    int mid = (low + high) / 2;\n    if (arr[mid] == key) return mid;\n    else if (arr[mid] &gt; key)\n        return binary_search_rec(arr, low, mid - 1, key);\n    else\n        return binary_search_rec(arr, mid + 1, high, key);\n}\nExample: arr = [1, 3, 5, 7, 9, 11], key = 7\n\nmid = 5 → key &gt; mid → move right- mid = 7 → found Complexity:\nTime: ( O\\(\\log n\\) )- Space: ( O(1) ) (iterative) or ( O\\(\\log n\\) ) (recursive)- Best case: ( O(1) ) (middle element) Requirements:\nMust be sorted- Must have random access (array, not linked list) Pros:\nVery fast for large sorted arrays- Foundation for advanced searches (e.g. interpolation, exponential) Cons:\nNeeds sorted data- Doesn’t adapt to frequent insertions/deletions\n\n\n\n3. Binary Search Variants\nBinary search is a pattern as much as a single algorithm. You can tweak it to find:\n\nFirst occurrence: move left if arr[mid] == key- Last occurrence: move right if arr[mid] == key- Lower bound: first index ≥ key- Upper bound: first index &gt; key Example (Lower Bound):\n\nint lower_bound(int arr[], int n, int key) {\n    int low = 0, high = n;\n    while (low &lt; high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid;\n    }\n    return low;\n}\nUsage: These variants power functions like std::lower_bound() in C++ and binary search trees’ lookup logic.\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorks On\nTime\nSpace\nSorted Data Needed\nNotes\n\n\n\n\nLinear Search\nAny\nO(n)\nO(1)\nNo\nBest for small/unsorted\n\n\nBinary Search\nSorted\nO(log n)\nO(1)\nYes\nFastest on ordered arrays\n\n\n\nBinary search trades simplicity for power , once your data is sorted, you unlock sublinear search.\n\n\nTiny Code\nCompare on array [2, 4, 6, 8, 10], key = 8:\n\nLinear: 4 steps- Binary: 2 steps This gap grows huge with size , for \\(n = 10^6\\), linear takes up to a million steps, binary about 20.\n\n\n\nWhy It Matters\nThese two searches form the foundation of retrieval. Linear search shows brute-force iteration; binary search shows how structure (sorted order) leads to exponential improvement.\nFrom databases to compiler symbol tables to tree lookups, this principle , divide to search faster , is everywhere.\n\n\nTry It Yourself\n\nImplement linear and binary search.\nCount comparisons for ( n = 10, 100, 1000 ).\nModify binary search to return the first occurrence of a duplicate.\nTry binary search on unsorted data , what happens?\nCombine with sorting: sort array, then search.\n\nMastering these searches builds intuition for all lookup operations , they are the gateway to efficient data retrieval.\n\n\n\n17. Interpolation and Exponential Search\nLinear and binary search work well across many scenarios, but they don’t take into account how data is distributed. When values are uniformly distributed, we can estimate where the target lies, instead of always splitting the range in half. This leads to Interpolation Search, which “jumps” close to where the value should be.\nFor unbounded or infinite lists, we can’t even know the size of the array up front , that’s where Exponential Search shines, by quickly expanding its search window before switching to binary search.\nLet’s dive into both.\n\n1. Interpolation Search\nIdea: If data is sorted and uniformly distributed, you can predict where a key might be using linear interpolation. Instead of splitting at the middle, estimate the position based on the value’s proportion in the range.\nFormula: \\[\n\\text{pos} = \\text{low} + \\frac{(key - arr[low]) \\times (high - low)}{arr[high] - arr[low]}\n\\]\nThis “guesses” where the key lies. If (key = arr[pos]), we’re done. Otherwise, adjust low or high and repeat.\nSteps:\n\nCompute estimated position pos\nCompare arr[pos] with key\nNarrow range accordingly\nRepeat while low &lt;= high and key within range\n\nCode:\nint interpolation_search(int arr[], int n, int key) {\n    int low = 0, high = n - 1;\n\n    while (low &lt;= high && key &gt;= arr[low] && key &lt;= arr[high]) {\n        if (low == high) {\n            if (arr[low] == key) return low;\n            return -1;\n        }\n        int pos = low + ((double)(key - arr[low]) * (high - low)) / (arr[high] - arr[low]);\n\n        if (arr[pos] == key)\n            return pos;\n        if (arr[pos] &lt; key)\n            low = pos + 1;\n        else\n            high = pos - 1;\n    }\n    return -1;\n}\nExample: arr = [10, 20, 30, 40, 50], key = 40 pos = 0 + ((40 - 10) * (4 - 0)) / (50 - 10) = 3 → found at index 3\nComplexity:\n\nBest: (O(1))- Average: (O\\(\\log \\log n\\)) (uniform data)- Worst: (O(n)) (non-uniform or skewed data)- Space: (O(1)) When to Use:\nData is sorted and nearly uniform- Numeric data where values grow steadily Note: Interpolation search is adaptive , faster when data is predictable, slower when data is irregular.\n\n\n\n2. Exponential Search\nIdea: When you don’t know the array size (e.g., infinite streams, linked data, files), you can’t just binary search from 0 to n-1. Exponential search finds a search range dynamically by doubling its step size until it overshoots the target, then does binary search within that range.\nSteps:\n\nIf arr[0] == key, return 0\nFind a range [bound/2, bound] such that arr[bound] &gt;= key\nPerform binary search in that range\n\nCode:\nint exponential_search(int arr[], int n, int key) {\n    if (arr[0] == key) return 0;\n    int bound = 1;\n    while (bound &lt; n && arr[bound] &lt; key)\n        bound *= 2;\n    int low = bound / 2;\n    int high = (bound &lt; n) ? bound : n - 1;\n    // Binary search in [low, high]\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] == key) return mid;\n        else if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\nExample: arr = [2, 4, 6, 8, 10, 12, 14, 16], key = 10\n\nStep: bound = 1 (4), 2 (6), 4 (10 ≥ key)- Binary search [2,4] → found Complexity:\nTime: (O\\(\\log i\\)), where (i) is index of the target- Space: (O(1))- Best: (O(1)) When to Use:\nUnbounded or streamed data- Unknown array size but sorted order\n\n\n\n3. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nData Requirement\nNotes\n\n\n\n\nLinear Search\nO(1)\nO(n)\nO(n)\nUnsorted\nWorks everywhere\n\n\nBinary Search\nO(1)\nO(log n)\nO(log n)\nSorted\nPredictable halving\n\n\nInterpolation Search\nO(1)\nO(log log n)\nO(n)\nSorted + Uniform\nAdaptive, fast on uniform data\n\n\nExponential Search\nO(1)\nO(log n)\nO(log n)\nSorted\nGreat for unknown size\n\n\n\nInterpolation improves on binary if data is smooth. Exponential shines when size is unknown.\n\n\nTiny Code\nInterpolation intuition: If your data is evenly spaced (10, 20, 30, 40, 50), the value 40 should be roughly 75% along. Instead of halving every time, we jump right near it. It’s data-aware searching.\nExponential intuition: When size is unknown, “expand until you find the wall,” then search within.\n\n\nWhy It Matters\nThese two searches show how context shapes algorithm design:\n\nDistribution (Interpolation Search)- Boundaries (Exponential Search) They teach that performance depends not only on structure (sortedness) but also metadata , how much you know about data spacing or limits.\n\nThese principles resurface in skip lists, search trees, and probabilistic indexing.\n\n\nTry It Yourself\n\nTest interpolation search on [10, 20, 30, 40, 50] , note how few steps it takes.\nTry the same on [1, 2, 4, 8, 16, 32, 64] , note slowdown.\nImplement exponential search and simulate an “infinite” array by stopping at n.\nCompare binary vs interpolation search on random vs uniform data.\nExtend exponential search to linked lists , how does complexity change?\n\nUnderstanding these searches helps you tailor lookups to the shape of your data , a key skill in algorithmic thinking.\n\n\n\n18. Selection Algorithms (Quickselect, Median of Medians)\nSometimes you don’t need to sort an entire array , you just want the k-th smallest (or largest) element. Sorting everything is overkill when you only need one specific rank. Selection algorithms solve this problem efficiently, often in linear time.\nThey’re the backbone of algorithms for median finding, percentiles, and order statistics, and they underpin operations like pivot selection in Quick Sort.\n\n1. The Selection Problem\nGiven an unsorted array of ( n ) elements and a number ( k ), find the element that would be at position ( k ) if the array were sorted.\nFor example: arr = [7, 2, 9, 4, 6], (k = 3) → Sorted = [2, 4, 6, 7, 9] → 3rd smallest = 6\nWe can solve this without sorting everything.\n\n\n2. Quickselect\nIdea: Quickselect is a selection variant of Quick Sort. It partitions the array around a pivot, but recurses only on the side that contains the k-th element.\nIt has average-case O(n) time because each partition roughly halves the search space.\nSteps:\n\nChoose a pivot (random or last element)\nPartition array into elements &lt; pivot and &gt; pivot\nLet pos be the pivot’s index after partition\nIf pos == k-1 → done\nIf pos &gt; k-1 → recurse left\nIf pos &lt; k-1 → recurse right\n\nCode:\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;\n            i++;\n        }\n    }\n    int temp = arr[i]; arr[i] = arr[high]; arr[high] = temp;\n    return i;\n}\n\nint quickselect(int arr[], int low, int high, int k) {\n    if (low == high) return arr[low];\n    int pos = partition(arr, low, high);\n    int rank = pos - low + 1;\n    if (rank == k) return arr[pos];\n    if (rank &gt; k) return quickselect(arr, low, pos - 1, k);\n    return quickselect(arr, pos + 1, high, k - rank);\n}\nExample: arr = [7, 2, 9, 4, 6], ( k = 3 )\n\nPivot = 6- Partition → [2, 4, 6, 9, 7], pos = 2- rank = 3 → found (6) Complexity:\nAverage: (O(n))- Worst: (O\\(n^2\\)) (bad pivots)- Space: (O(1))- In-place When to Use:\nFast average case- You don’t need full sorting Quickselect is used in C++’s nth_element() and many median-finding implementations.\n\n\n\n3. Median of Medians\nIdea: Guarantee worst-case ( O(n) ) time by choosing a good pivot deterministically.\nThis method ensures the pivot divides the array into reasonably balanced parts every time.\nSteps:\n\nDivide array into groups of 5\nFind the median of each group (using insertion sort)\nRecursively find the median of these medians → pivot\nPartition array around this pivot\nRecurse into the side containing the k-th element\n\nThis guarantees at least 30% of elements are eliminated each step → linear time in worst case.\nCode Sketch:\nint select_pivot(int arr[], int low, int high) {\n    int n = high - low + 1;\n    if (n &lt;= 5) {\n        insertion_sort(arr + low, n);\n        return low + n / 2;\n    }\n\n    int medians[(n + 4) / 5];\n    int i;\n    for (i = 0; i &lt; n / 5; i++) {\n        insertion_sort(arr + low + i * 5, 5);\n        medians[i] = arr[low + i * 5 + 2];\n    }\n    if (i * 5 &lt; n) {\n        insertion_sort(arr + low + i * 5, n % 5);\n        medians[i] = arr[low + i * 5 + (n % 5) / 2];\n        i++;\n    }\n    return select_pivot(medians, 0, i - 1);\n}\nYou’d then partition around pivot and recurse just like Quickselect.\nComplexity:\n\nWorst: (O(n))- Space: (O(1)) (in-place version)- Stable: No (doesn’t preserve order) Why It Matters: Median of Medians is slower in practice than Quickselect but provides theoretical guarantees , vital in real-time or critical systems.\n\n\n\n4. Special Cases\n\nMin / Max: trivial , just scan once ((O(n)))- Median: \\(k = \\lceil n/2 \\rceil\\) , can use Quickselect or Median of Medians- Top-k Elements: use partial selection or heaps (k smallest/largest) Example: To get top 5 scores from a million entries, use Quickselect to find 5th largest, then filter ≥ threshold.\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest\nAverage\nWorst\nStable\nIn-Place\nNotes\n\n\n\n\nQuickselect\nO(n)\nO(n)\nO(n²)\nNo\nYes\nFast in practice\n\n\nMedian of Medians\nO(n)\nO(n)\nO(n)\nNo\nYes\nDeterministic\n\n\nSorting\nO(n log n)\nO(n log n)\nO(n log n)\nDepends\nDepends\nOverkill for single element\n\n\n\nQuickselect is fast and simple; Median of Medians is safe and predictable.\n\n\nTiny Code\nFind 4th smallest in [9, 7, 2, 5, 4, 3]:\n\nPivot = 4 → partition [2,3,4,9,7,5]- 4 at position 2 → rank = 3 &lt; 4 → recurse right- New range [9,7,5], ( k = 1 ) → smallest = 5 Result: 5\n\n\n\nWhy It Matters\nSelection algorithms reveal a key insight:\n\nSometimes you don’t need everything , just what matters.\n\nThey form the basis for:\n\nMedian filters in signal processing- Partitioning steps in sorting- k-th order statistics- Robust statistics and quantile computation They embody a “partial work, full answer” philosophy , do exactly enough.\n\n\n\nTry It Yourself\n\nImplement Quickselect and find k-th smallest for various k.\nCompare runtime vs full sorting.\nModify Quickselect to find k-th largest.\nImplement Median of Medians pivot selection.\nUse Quickselect to find median of 1,000 random elements.\n\nMastering selection algorithms helps you reason about efficiency , you’ll learn when to stop sorting and start selecting.\n\n\n\n19. Range Searching and Nearest Neighbor\nSearching isn’t always about finding a single key. Often, you need to find all elements within a given range , or the closest match to a query point.\nThese problems are central to databases, computational geometry, and machine learning (like k-NN classification). This section introduces algorithms for range queries (e.g. find all values between L and R) and nearest neighbor searches (e.g. find the point closest to query q).\n\n1. Range Searching\nIdea: Given a set of data points (1D or multidimensional), quickly report all points within a specified range.\nIn 1D (simple arrays), range queries can be handled by binary search and prefix sums. In higher dimensions, we need trees designed for efficient spatial querying.\n\n\nA. 1D Range Query (Sorted Array)\nGoal: Find all elements in [L, R].\nSteps:\n\nUse lower bound to find first element ≥ L\nUse upper bound to find first element &gt; R\nOutput all elements in between\n\nCode (C++-style pseudo):\nint l = lower_bound(arr, arr + n, L) - arr;\nint r = upper_bound(arr, arr + n, R) - arr;\nfor (int i = l; i &lt; r; i++)\n    printf(\"%d \", arr[i]);\nTime Complexity:\n\nBinary search bounds: (O\\(\\log n\\))- Reporting results: (O(k)) where (k) = number of elements in range → Total: (O\\(\\log n + k\\))\n\n\n\nB. Prefix Sum Range Query (For sums)\nIf you just need the sum (not the actual elements), use prefix sums:\n\\[\n\\text{prefix}[i] = a_0 + a_1 + \\ldots + a_i\n\\]\nThen range sum: \\[\n\\text{sum}(L, R) = \\text{prefix}[R] - \\text{prefix}[L - 1]\n\\]\nCode:\nint prefix[n];\nprefix[0] = arr[0];\nfor (int i = 1; i &lt; n; i++)\n    prefix[i] = prefix[i - 1] + arr[i];\n\nint range_sum(int L, int R) {\n    return prefix[R] - (L &gt; 0 ? prefix[L - 1] : 0);\n}\nTime: (O(1)) per query after (O(n)) preprocessing.\nUsed in:\n\nDatabases for fast range aggregation- Fenwick trees, segment trees\n\n\n\nC. 2D Range Queries (Rectangular Regions)\nFor points ((x, y)), queries like:\n\n“Find all points where \\(L_x ≤ x ≤ R_x\\) and \\(L_y ≤ y ≤ R_y\\)”\n\nUse specialized structures:\n\nRange Trees (balanced BSTs per dimension)- Fenwick Trees / Segment Trees (for 2D arrays)- KD-Trees (spatial decomposition) Time: (O\\(\\log^2 n + k\\)) typical for 2D Space: (O\\(n \\log n\\))\n\n\n\n2. Nearest Neighbor Search\nIdea: Given a set of points, find the one closest to query (q). Distance is often Euclidean, but can be any metric.\nBrute Force: Check all points → (O(n)) per query. Too slow for large datasets.\nWe need structures that let us prune far regions fast.\n\n\nA. KD-Tree\nKD-tree = K-dimensional binary tree. Each level splits points by one coordinate, alternating axes. Used for efficient nearest neighbor search in low dimensions (2D-10D).\nConstruction:\n\nChoose axis = depth % k\nSort points by axis\nPick median → root\nRecursively build left and right\n\nQuery (Nearest Neighbor):\n\nTraverse down tree based on query position\nBacktrack , check whether hypersphere crosses splitting plane\nKeep track of best (closest) distance\n\nComplexity:\n\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\log n\\)) avg, (O(n)) worst Use Cases:\nNearest city lookup- Image / feature vector matching- Game AI spatial queries Code Sketch (2D Example):\n\nstruct Point { double x, y; };\n\ndouble dist(Point a, Point b) {\n    return sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y));\n}\n(Full KD-tree implementation omitted for brevity , idea is recursive partitioning.)\n\n\nB. Ball Tree / VP-Tree\nFor high-dimensional data, KD-trees degrade. Alternatives like Ball Trees (split by hyperspheres) or VP-Trees (Vantage Point Trees) perform better.\nThey split based on distance metrics, not coordinate axes.\n\n\nC. Approximate Nearest Neighbor (ANN)\nFor large-scale, high-dimensional data (e.g. embeddings, vectors):\n\nLocality Sensitive Hashing (LSH)- HNSW (Hierarchical Navigable Small World Graphs) These trade exactness for speed, common in:\nVector databases- Recommendation systems- AI model retrieval\n\n\n\n3. Summary\n\n\n\n\n\n\n\n\n\n\nProblem\nBrute Force\nOptimized\nTime (Query)\nNotes\n\n\n\n\n1D Range Query\nScan O(n)\nBinary Search\nO(log n + k)\nSorted data\n\n\nRange Sum\nO(n)\nPrefix Sum\nO(1)\nStatic data\n\n\n2D Range Query\nO(n)\nRange Tree\nO(log² n + k)\nSpatial filtering\n\n\nNearest Neighbor\nO(n)\nKD-Tree\nO(log n) avg\nExact, low-dim\n\n\nNearest Neighbor (high-dim)\nO(n)\nHNSW / LSH\n~O(1)\nApproximate\n\n\n\n\n\nTiny Code\nSimple 1D range query:\nint arr[] = {1, 3, 5, 7, 9, 11};\nint L = 4, R = 10;\nint l = lower_bound(arr, arr + 6, L) - arr;\nint r = upper_bound(arr, arr + 6, R) - arr;\nfor (int i = l; i &lt; r; i++)\n    printf(\"%d \", arr[i]); // 5 7 9\nOutput: 5 7 9\n\n\nWhy It Matters\nRange and nearest-neighbor queries power:\n\nDatabases (SQL range filters, BETWEEN)- Search engines (spatial indexing)- ML (k-NN classifiers, vector similarity)- Graphics / Games (collision detection, spatial queries) These are not just searches , they’re geometric lookups, linking algorithms to spatial reasoning.\n\n\n\nTry It Yourself\n\nWrite a function to return all numbers in [L, R] using binary search.\nBuild a prefix sum array and answer 5 range-sum queries in O(1).\nImplement a KD-tree for 2D points and query nearest neighbor.\nCompare brute-force vs KD-tree search on 1,000 random points.\nExplore Python’s scipy.spatial.KDTree or sklearn.neighbors.\n\nThese algorithms bridge searching with geometry and analytics, forming the backbone of spatial computation.\n\n\n\n20. Search Optimizations and Variants\nWe’ve explored the main search families , linear, binary, interpolation, exponential , each fitting a different data shape or constraint. Now let’s move one step further: optimizing search for performance and adapting it to specialized scenarios.\nThis section introduces practical variants and enhancements used in real systems, databases, and competitive programming, including jump search, fibonacci search, ternary search, and exponential + binary combinations.\n\n1. Jump Search\nIdea: If data is sorted, we can “jump” ahead by fixed steps instead of scanning linearly. It’s like hopping through the array in blocks , when you overshoot the target, you step back and linearly search that block.\nIt strikes a balance between linear and binary search , fewer comparisons without the recursion or halving of binary search.\nSteps:\n\nChoose jump size = \\(\\sqrt{n}\\)\nJump by blocks until arr[step] &gt; key\nLinear search in previous block\n\nCode:\nint jump_search(int arr[], int n, int key) {\n    int step = sqrt(n);\n    int prev = 0;\n\n    while (arr[min(step, n) - 1] &lt; key) {\n        prev = step;\n        step += sqrt(n);\n        if (prev &gt;= n) return -1;\n    }\n\n    for (int i = prev; i &lt; min(step, n); i++) {\n        if (arr[i] == key) return i;\n    }\n    return -1;\n}\nExample: arr = [1, 3, 5, 7, 9, 11, 13, 15], key = 11\n\nstep = 2- Jump 5, 7, 9, 11 → found Complexity:\nTime: (O\\(\\sqrt{n}\\))- Space: (O(1))- Works on sorted data When to Use: For moderately sized sorted lists when you want fewer comparisons but minimal overhead.\n\n\n\n2. Fibonacci Search\nIdea: Similar to binary search, but it splits the array based on Fibonacci numbers instead of midpoints. This allows using only addition and subtraction (no division), useful on hardware where division is costly.\nAlso, like binary search, it halves (roughly) the search space each iteration.\nSteps:\n\nFind the smallest Fibonacci number ≥ n\nUse it to compute probe index\nCompare and move interval accordingly\n\nCode (Sketch):\nint fibonacci_search(int arr[], int n, int key) {\n    int fibMMm2 = 0; // (m-2)'th Fibonacci\n    int fibMMm1 = 1; // (m-1)'th Fibonacci\n    int fibM = fibMMm2 + fibMMm1; // m'th Fibonacci\n\n    while (fibM &lt; n) {\n        fibMMm2 = fibMMm1;\n        fibMMm1 = fibM;\n        fibM = fibMMm2 + fibMMm1;\n    }\n\n    int offset = -1;\n    while (fibM &gt; 1) {\n        int i = min(offset + fibMMm2, n - 1);\n        if (arr[i] &lt; key) {\n            fibM = fibMMm1;\n            fibMMm1 = fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n            offset = i;\n        } else if (arr[i] &gt; key) {\n            fibM = fibMMm2;\n            fibMMm1 = fibMMm1 - fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n        } else return i;\n    }\n    if (fibMMm1 && arr[offset + 1] == key)\n        return offset + 1;\n    return -1;\n}\nComplexity:\n\nTime: (O\\(\\log n\\))- Space: (O(1))- Sorted input required Fun Fact: Fibonacci search was originally designed for tape drives , where random access is expensive, and predictable jumps matter.\n\n\n\n3. Ternary Search\nIdea: When the function or sequence is unimodal (strictly increasing then decreasing), you can locate a maximum or minimum by splitting the range into three parts instead of two.\nUsed not for discrete lookup but for optimization on sorted functions.\nSteps:\n\nDivide range into thirds\nEvaluate at two midpoints m1, m2\nEliminate one-third based on comparison\nRepeat until range is small\n\nCode:\ndouble ternary_search(double low, double high, double (*f)(double)) {\n    for (int i = 0; i &lt; 100; i++) {\n        double m1 = low + (high - low) / 3;\n        double m2 = high - (high - low) / 3;\n        if (f(m1) &lt; f(m2))\n            low = m1;\n        else\n            high = m2;\n    }\n    return (low + high) / 2;\n}\nExample: Find minimum of ( f(x) = (x-3)^2 ) between [0,10]. After iterations, converges to (x ≈ 3).\nComplexity:\n\nTime: \\(O(\\log\\text{range})\\)\nSpace: \\(O(1)\\)\nWorks for unimodal functions\n\nUsed in:\n\nMathematical optimization\nSearch-based tuning\nGame AI decision models\n\n\n\n4. Binary Search Variants (Review)\nBinary search can be tailored to answer richer queries:\n\nLower Bound: first index ≥ key- Upper Bound: first index &gt; key- Equal Range: range of all equal elements- Rotated Arrays: find element in rotated sorted array- Infinite Arrays: use exponential expansion Rotated Example: arr = [6,7,9,1,3,4], key = 3 → Find pivot, then binary search correct side.\n\n\n\n5. Combined Searches\nReal systems often chain algorithms:\n\nExponential + Binary Search → when bounds unknown- Interpolation + Linear Search → when near target- Jump + Linear Search → hybrid iteration These hybrids use context switching , pick a fast search, then fall back to simple scan in a narrowed window.\n\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nData Requirement\nSpecial Strength\n\n\n\n\nJump Search\nO(√n)\nO(1)\nSorted\nFewer comparisons\n\n\nFibonacci Search\nO(log n)\nO(1)\nSorted\nDivision-free\n\n\nTernary Search\nO(log range)\nO(1)\nUnimodal\nOptimization\n\n\nBinary Variants\nO(log n)\nO(1)\nSorted\nBound finding\n\n\nCombined Searches\nAdaptive\nO(1)\nMixed\nPractical hybrids\n\n\n\n\n\nTiny Code\nJump Search intuition:\n// Blocks of size sqrt(n)\n[1, 3, 5, 7, 9, 11, 13, 15]\nStep: 3 → 7 &gt; 6 → search previous block\nJumps reduce comparisons dramatically vs linear scan.\n\n\nWhy It Matters\nSearch optimization is about adapting structure to context. You don’t always need a fancy data structure , sometimes a tweak like fixed-step jumping or Fibonacci spacing yields massive gains.\nThese ideas influence:\n\nIndexing in databases- Compilers’ symbol resolution- Embedded systems with low-level constraints They embody the principle: search smarter, not harder.\n\n\n\nTry It Yourself\n\nImplement Jump Search and test vs Binary Search on 1M elements.\nWrite a Fibonacci Search , compare steps taken.\nUse Ternary Search to find min of a convex function.\nModify binary search to find element in rotated array.\nCombine Jump + Linear , how does it behave for small n?\n\nUnderstanding these variants arms you with flexibility , the heart of algorithmic mastery.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-3.-data-structures-in-actions",
    "href": "books/en-us/book.html#chapter-3.-data-structures-in-actions",
    "title": "The Book",
    "section": "Chapter 3. Data Structures in Actions",
    "text": "Chapter 3. Data Structures in Actions\n\n21. Arrays, Linked Lists, Stacks, Queues\nEvery data structure is built on top of a few core foundations , the ones that teach you how data is stored, accessed, and moved. In this section, we’ll revisit the essentials: arrays, linked lists, stacks, and queues.\nThey’re simple, but they show you the most important design trade-offs in algorithms:\n\nContiguity vs. flexibility- Speed vs. dynamic growth- Last-in-first-out vs. first-in-first-out access\n\n\n1. Arrays\nIdea: A contiguous block of memory storing elements of the same type. Access by index in O(1) time , that’s their superpower.\nOperations:\n\nAccess arr[i]: (O(1))- Update arr[i]: (O(1))- Insert at end: (O(1)) (amortized for dynamic arrays)- Insert in middle: (O(n))- Delete: (O(n)) Example:\n\nint arr[5] = {10, 20, 30, 40, 50};\nprintf(\"%d\", arr[2]); // 30\nStrengths:\n\nFast random access- Cache-friendly (contiguous memory)- Simple, predictable Weaknesses:\nFixed size (unless using dynamic array)- Costly inserts/deletes Dynamic Arrays: Languages provide resizable arrays (like vector in C++ or ArrayList in Java) using doubling strategy , when full, allocate new array twice as big and copy. This gives amortized (O(1)) insertion at end.\n\n\n\n2. Linked Lists\nIdea: A chain of nodes, where each node stores a value and a pointer to the next. No contiguous memory required.\nOperations:\n\nAccess: (O(n))- Insert/Delete at head: (O(1))- Search: (O(n)) Example:\n\ntypedef struct Node {\n    int data;\n    struct Node* next;\n} Node;\n\nNode* head = NULL;\nTypes:\n\nSingly Linked List: one pointer (next)- Doubly Linked List: two pointers (next, prev)- Circular Linked List: last node points back to first Strengths:\nDynamic size- Fast insert/delete (no shifting) Weaknesses:\nSlow access- Extra memory for pointers- Poor cache locality Linked lists shine when memory is fragmented or frequent insertions/deletions are needed.\n\n\n\n3. Stack\nIdea: A Last-In-First-Out (LIFO) structure , the most recently added element is the first to be removed.\nUsed in:\n\nFunction call stacks- Expression evaluation- Undo operations Operations:\npush(x): add element on top- pop(): remove top element- peek(): view top element Example (Array-based Stack):\n\n#define MAX 100\nint stack[MAX], top = -1;\n\nvoid push(int x) { stack[++top] = x; }\nint pop() { return stack[top--]; }\nint peek() { return stack[top]; }\nComplexity: All (O(1)): push, pop, peek\nVariants:\n\nLinked-list-based stack (no fixed size)- Min-stack (tracks minimums) Stacks also appear implicitly , in recursion and backtracking algorithms.\n\n\n\n4. Queue\nIdea: A First-In-First-Out (FIFO) structure , the first added element leaves first.\nUsed in:\n\nTask scheduling- BFS traversal- Producer-consumer pipelines Operations:\nenqueue(x): add to rear- dequeue(): remove from front- front(): view front Example (Array-based Queue):\n\n#define MAX 100\nint queue[MAX], front = 0, rear = 0;\n\nvoid enqueue(int x) { queue[rear++] = x; }\nint dequeue() { return queue[front++]; }\nThis simple implementation can waste space. A circular queue fixes that by wrapping indices modulo MAX:\nrear = (rear + 1) % MAX;\nComplexity: All (O(1)): enqueue, dequeue\nVariants:\n\nDeque (double-ended queue): push/pop from both ends- Priority Queue: dequeue highest priority (not strictly FIFO)\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nStructure\nAccess\nInsert\nDelete\nOrder\nMemory\nNotes\n\n\n\n\nArray\nO(1)\nO(n)\nO(n)\nIndexed\nContiguous\nFast access\n\n\nLinked List\nO(n)\nO(1)*\nO(1)*\nSequential\nPointers\nFlexible size\n\n\nStack\nO(1)\nO(1)\nO(1)\nLIFO\nMinimal\nCall stack, parsing\n\n\nQueue\nO(1)\nO(1)\nO(1)\nFIFO\nMinimal\nScheduling, BFS\n\n\n\n(* at head or tail with pointer)\n\n\nTiny Code\nSimple stack example:\npush(10);\npush(20);\nprintf(\"%d\", pop()); // 20\nSimple queue example:\nenqueue(5);\nenqueue(8);\nprintf(\"%d\", dequeue()); // 5\nThese short routines appear in almost every algorithm , from recursion stacks to graph traversals.\n\n\nWhy It Matters\nThese four structures form the spine of data structures:\n\nArrays teach indexing and memory- Linked lists teach pointers and dynamic allocation- Stacks teach recursion and reversal- Queues teach scheduling and order maintenance Every complex structure (trees, heaps, graphs) builds on these.\n\nMaster them, and every algorithm will feel more natural.\n\n\nTry It Yourself\n\nImplement a linked list with insert_front and delete_value.\nBuild a stack and use it to reverse an array.\nImplement a queue for a round-robin scheduler.\nConvert infix expression to postfix using a stack.\nCompare time taken to insert 1000 elements in array vs linked list.\n\nUnderstanding these foundations gives you the vocabulary of structure , the way algorithms organize their thoughts in memory.\n\n\n\n22. Hash Tables and Variants (Cuckoo, Robin Hood, Consistent)\nWhen you need lightning-fast lookups, insertions, and deletions, few data structures match the raw efficiency of a hash table. They’re everywhere , from symbol tables and caches to compilers and databases , powering average-case O(1) access.\nIn this section, we’ll unpack how hash tables work, their collision strategies, and explore modern variants like Cuckoo Hashing, Robin Hood Hashing, and Consistent Hashing, each designed to handle different real-world needs.\n\n1. The Core Idea\nA hash table maps keys to values using a hash function that transforms the key into an index in an array.\n\\[\n\\text{index} = h(\\text{key}) \\bmod \\text{table\\_size}\n\\]\nIf no two keys hash to the same index, all operations are (O(1)). But in practice, collisions happen , two keys may map to the same slot , and we must handle them smartly.\n\n\n2. Collision Resolution Strategies\nA. Separate Chaining Each table slot holds a linked list (or dynamic array) of entries with the same hash.\nPros: Simple, handles load factor &gt; 1 Cons: Extra pointers, memory overhead\nCode Sketch:\ntypedef struct Node {\n    int key, value;\n    struct Node* next;\n} Node;\n\nNode* table[SIZE];\n\nint hash(int key) { return key % SIZE; }\n\nvoid insert(int key, int value) {\n    int idx = hash(key);\n    Node* node = malloc(sizeof(Node));\n    node-&gt;key = key; node-&gt;value = value;\n    node-&gt;next = table[idx];\n    table[idx] = node;\n}\nB. Open Addressing All keys live directly in the table. On collision, find another slot.\nThree main strategies:\n\nLinear probing: try next slot (+1)- Quadratic probing: step size increases quadratically- Double hashing: second hash decides step size Example (Linear Probing):\n\nint hash(int key) { return key % SIZE; }\nint insert(int key, int value) {\n    int idx = hash(key);\n    while (table[idx].used)\n        idx = (idx + 1) % SIZE;\n    table[idx] = (Entry){key, value, 1};\n}\nLoad Factor \\(\\alpha = \\frac{n}{m}\\) affects performance , when too high, rehash to larger size.\n\n\n3. Modern Variants\nClassic hash tables can degrade under heavy collisions. Modern variants reduce probe chains and balance load more evenly.\n\n\nA. Cuckoo Hashing\nIdea: Each key has two possible locations , if both full, evict one (“kick out the cuckoo”) and reinsert. Ensures constant lookup , at most two probes.\nSteps:\n\nCompute two hashes (h_1(key)), (h_2(key))\nIf slot 1 empty → place\nElse evict occupant, reinsert it using alternate hash\nRepeat until placed or cycle detected (rehash if needed)\n\nCode Sketch (Conceptual):\nint h1(int key) { return key % SIZE; }\nint h2(int key) { return (key / SIZE) % SIZE; }\n\nvoid insert(int key) {\n    int pos1 = h1(key);\n    if (!table1[pos1]) { table1[pos1] = key; return; }\n    int displaced = table1[pos1]; table1[pos1] = key;\n\n    int pos2 = h2(displaced);\n    if (!table2[pos2]) { table2[pos2] = displaced; return; }\n    // continue evicting if needed\n}\nPros:\n\nWorst-case O(1) lookup (constant probes)- Predictable latency Cons:\nRehash needed on insertion failure- More complex logic Used in high-performance caches and real-time systems.\n\n\n\nB. Robin Hood Hashing\nIdea: Steal slots from richer (closer) keys to ensure fairness. When inserting, if you find someone with smaller probe distance, swap , “steal from the rich, give to the poor.”\nThis balances probe lengths and improves variance and average lookup time.\nKey Principle: \\[\n\\text{If new\\_probe\\_distance} &gt; \\text{existing\\_probe\\_distance} \\Rightarrow \\text{swap}\n\\]\nCode Sketch:\nint insert(int key) {\n    int idx = hash(key);\n    int dist = 0;\n    while (table[idx].used) {\n        if (table[idx].dist &lt; dist) {\n            // swap entries\n            Entry tmp = table[idx];\n            table[idx] = (Entry){key, dist, 1};\n            key = tmp.key;\n            dist = tmp.dist;\n        }\n        idx = (idx + 1) % SIZE;\n        dist++;\n    }\n    table[idx] = (Entry){key, dist, 1};\n}\nPros:\n\nReduced variance- Better performance under high load Cons:\nSlightly slower insertion Used in modern languages like Rust (hashbrown) and Swift.\n\n\n\nC. Consistent Hashing\nIdea: When distributing keys across multiple nodes, you want minimal movement when adding/removing a node. Consistent hashing maps both keys and nodes onto a circular hash ring.\nSteps:\n\nHash nodes into a ring\nHash keys into same ring\nEach key belongs to the next node clockwise\n\nWhen a node is added or removed, only nearby keys move.\nUsed in:\n\nDistributed caches (Memcached, DynamoDB)- Load balancing- Sharding in databases Code (Conceptual):\n\nRing: 0 -------------------------------- 2^32\nNodes: N1 at hash(\"A\"), N2 at hash(\"B\")\nKey: hash(\"User42\") → assign to next node clockwise\nPros:\n\nMinimal rebalancing- Scalable Cons:\nMore complex setup- Requires virtual nodes for even distribution\n\n\n\n4. Complexity Overview\n\n\n\n\n\n\n\n\n\n\n\nVariant\nInsert\nSearch\nDelete\nMemory\nNotes\n\n\n\n\nChaining\nO(1) avg\nO(1) avg\nO(1) avg\nHigh\nSimple, dynamic\n\n\nLinear Probing\nO(1) avg\nO(1) avg\nO(1) avg\nLow\nClustering risk\n\n\nCuckoo\nO(1)\nO(1)\nO(1)\nMedium\nTwo tables, predictable\n\n\nRobin Hood\nO(1)\nO(1)\nO(1)\nLow\nBalanced probes\n\n\nConsistent\nO(log n)\nO(log n)\nO(log n)\nDepends\nDistributed keys\n\n\n\n\n\nTiny Code\nSimple hash table with linear probing:\n#define SIZE 10\nint keys[SIZE], values[SIZE], used[SIZE];\n\nint hash(int key) { return key % SIZE; }\n\nvoid insert(int key, int value) {\n    int idx = hash(key);\n    while (used[idx]) idx = (idx + 1) % SIZE;\n    keys[idx] = key; values[idx] = value; used[idx] = 1;\n}\nLookup:\nint get(int key) {\n    int idx = hash(key);\n    while (used[idx]) {\n        if (keys[idx] == key) return values[idx];\n        idx = (idx + 1) % SIZE;\n    }\n    return -1;\n}\n\n\nWhy It Matters\nHash tables show how structure and randomness combine for speed. They embody the idea that a good hash function + smart collision handling = near-constant performance.\nVariants like Cuckoo and Robin Hood are examples of modern engineering trade-offs , balancing performance, memory, and predictability. Consistent hashing extends these ideas to distributed systems.\n\n\nTry It Yourself\n\nImplement a hash table with chaining and test collision handling.\nModify it to use linear probing , measure probe lengths.\nSimulate Cuckoo hashing with random inserts.\nImplement Robin Hood swapping logic , observe fairness.\nDraw a consistent hash ring with 3 nodes and 10 keys , track movement when adding one node.\n\nOnce you master these, you’ll see hashing everywhere , from dictionaries to distributed databases.\n\n\n\n23. Heaps (Binary, Fibonacci, Pairing)\nHeaps are priority-driven data structures , they always give you fast access to the most important element, typically the minimum or maximum. They’re essential for priority queues, scheduling, graph algorithms (like Dijkstra), and streaming analytics.\nIn this section, we’ll start from the basic binary heap and then explore more advanced ones like Fibonacci and pairing heaps, which trade off simplicity, speed, and amortized guarantees.\n\n1. The Heap Property\nA heap is a tree-based structure (often represented as an array) that satisfies:\n\nMin-Heap: Every node ≤ its children- Max-Heap: Every node ≥ its children This ensures the root always holds the smallest (or largest) element.\n\nComplete Binary Tree: All levels filled except possibly the last, which is filled left to right.\nExample (Min-Heap):\n        2\n      /   \\\n     4     5\n    / \\   /\n   9  10 15\nHere, the smallest element (2) is at the root.\n\n\n2. Binary Heap\nStorage: Stored compactly in an array. For index i (0-based):\n\nParent = (i - 1) / 2- Left child = 2i + 1- Right child = 2i + 2 Operations:\n\n\n\n\nOperation\nDescription\nTime\n\n\n\n\npush(x)\nInsert element\n(O\\(\\log n\\))\n\n\npop()\nRemove root\n(O\\(\\log n\\))\n\n\npeek()\nGet root\n(O(1))\n\n\nheapify()\nBuild heap\n(O(n))\n\n\n\n\n\nA. Insertion (Push)\nInsert at the end, then bubble up until heap property is restored.\nCode:\nvoid push(int heap[], int *n, int x) {\n    int i = (*n)++;\n    heap[i] = x;\n    while (i &gt; 0 && heap[(i - 1)/2] &gt; heap[i]) {\n        int tmp = heap[i];\n        heap[i] = heap[(i - 1)/2];\n        heap[(i - 1)/2] = tmp;\n        i = (i - 1) / 2;\n    }\n}\n\n\nB. Removal (Pop)\nReplace root with last element, then bubble down (heapify).\nCode:\nvoid heapify(int heap[], int n, int i) {\n    int smallest = i, l = 2*i + 1, r = 2*i + 2;\n    if (l &lt; n && heap[l] &lt; heap[smallest]) smallest = l;\n    if (r &lt; n && heap[r] &lt; heap[smallest]) smallest = r;\n    if (smallest != i) {\n        int tmp = heap[i]; heap[i] = heap[smallest]; heap[smallest] = tmp;\n        heapify(heap, n, smallest);\n    }\n}\nPop:\nint pop(int heap[], int *n) {\n    int root = heap[0];\n    heap[0] = heap[--(*n)];\n    heapify(heap, *n, 0);\n    return root;\n}\n\n\nC. Building a Heap\nHeapify bottom-up from last non-leaf: (O(n))\nfor (int i = n/2 - 1; i &gt;= 0; i--)\n    heapify(heap, n, i);\n\n\nD. Applications\n\nHeapsort: Repeatedly pop min (O(n log n))- Priority Queue: Fast access to smallest/largest- Graph Algorithms: Dijkstra, Prim- Streaming: Median finding using two heaps\n\n\n\n3. Fibonacci Heap\nIdea: A heap optimized for algorithms that do many decrease-key operations (like Dijkstra’s). It stores a collection of trees with lazy merging, giving amortized bounds:\n\n\n\nOperation\nAmortized Time\n\n\n\n\nInsert\n(O(1))\n\n\nFind-Min\n(O(1))\n\n\nExtract-Min\n(O\\(\\log n\\))\n\n\nDecrease-Key\n(O(1))\n\n\nMerge\n(O(1))\n\n\n\nIt achieves this by delaying structural fixes until absolutely necessary (using potential method in amortized analysis).\nStructure:\n\nA circular linked list of roots- Each node can have multiple children- Consolidation on extract-min ensures minimal degree duplication Used in theoretical optimizations where asymptotic complexity matters (e.g. Dijkstra in (O\\(E + V \\log V\\)) vs (O\\(E \\log V\\))).\n\n\n\n4. Pairing Heap\nIdea: A simpler, practical alternative to Fibonacci heaps. Self-adjusting structure using a tree with multiple children.\nOperations:\n\nInsert: (O(1))- Extract-Min: (O\\(\\log n\\)) amortized- Decrease-Key: (O\\(\\log n\\)) amortized Steps:\nmerge two heaps: attach one as child of the other- extract-min: remove root, merge children in pairs, then merge all results Why It’s Popular:\nEasier to implement- Great real-world performance- Used in functional programming and priority schedulers\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nHeap Type\nInsert\nExtract-Min\nDecrease-Key\nMerge\nSimplicity\nUse Case\n\n\n\n\nBinary Heap\nO(log n)\nO(log n)\nO(log n)\nO(n)\nEasy\nGeneral-purpose\n\n\nFibonacci Heap\nO(1)\nO(log n)\nO(1)\nO(1)\nComplex\nTheoretical optimality\n\n\nPairing Heap\nO(1)\nO(log n)\nO(log n)\nO(1)\nModerate\nPractical alternative\n\n\n\n\n\nTiny Code\nBinary Heap Demo:\nint heap[100], n = 0;\npush(heap, &n, 10);\npush(heap, &n, 4);\npush(heap, &n, 7);\nprintf(\"%d \", pop(heap, &n)); // 4\nOutput: 4\n\n\nWhy It Matters\nHeaps show how to prioritize elements dynamically. From sorting to scheduling, they’re the backbone of many “choose the best next” algorithms. Variants like Fibonacci and Pairing Heaps demonstrate how amortized analysis can unlock deeper efficiency , crucial in graph theory and large-scale optimization.\n\n\nTry It Yourself\n\nImplement a binary min-heap with push, pop, and peek.\nUse a heap to sort a list (Heapsort).\nBuild a priority queue for task scheduling.\nStudy how Dijkstra changes when replacing arrays with heaps.\nExplore Fibonacci heap pseudo-code , trace decrease-key.\n\nMastering heaps gives you a deep sense of priority-driven design , how to keep “the best” element always within reach.\n\n\n\n24. Balanced Trees (AVL, Red-Black, Splay, Treap)\nUnbalanced trees can degrade into linear lists, turning your beautiful (O\\(\\log n\\)) search into a sad (O(n)) crawl. Balanced trees solve this , they keep the height logarithmic, guaranteeing fast lookups, insertions, and deletions.\nIn this section, you’ll learn how different balancing philosophies work , AVL (strict balance), Red-Black (relaxed balance), Splay (self-adjusting), and Treap (randomized balance).\n\n1. The Idea of Balance\nFor a binary search tree (BST):\n\\[\n\\text{height} = O(\\log n)\n\\]\nonly if it’s balanced , meaning the number of nodes in left and right subtrees differ by a small factor.\nUnbalanced BST (bad):\n1\n \\\n  2\n   \\\n    3\nBalanced BST (good):\n  2\n / \\\n1   3\nBalance ensures efficient:\n\nsearch(x) → (O\\(\\log n\\))- insert(x) → (O\\(\\log n\\))- delete(x) → (O\\(\\log n\\))\n\n\n\n2. AVL Tree (Adelson-Velsky & Landis)\nInvented in 1962, AVL is the first self-balancing BST. It enforces strict balance: \\[\n| \\text{height(left)} - \\text{height(right)} | \\le 1\n\\]\nWhenever this condition breaks, rotations fix it.\nRotations:\n\nLL (Right Rotation): imbalance on left-left- RR (Left Rotation): imbalance on right-right- LR / RL: double rotation cases Code (Rotation Example):\n\nNode* rotateRight(Node* y) {\n    Node* x = y-&gt;left;\n    Node* T = x-&gt;right;\n    x-&gt;right = y;\n    y-&gt;left = T;\n    return x;\n}\nHeight & Balance Factor:\nint height(Node* n) { return n ? n-&gt;h : 0; }\nint balance(Node* n) { return height(n-&gt;left) - height(n-&gt;right); }\nProperties:\n\nStrict height bound: (O\\(\\log n\\))- More rotations (slower insertions)- Excellent lookup speed Used when lookups &gt; updates (databases, indexing).\n\n\n\n3. Red-Black Tree\nIdea: A slightly looser balance for faster insertions. Each node has a color (Red/Black) with these rules:\n\nRoot is black\nRed node’s children are black\nEvery path has same number of black nodes\nNull nodes are black\n\nBalance through color flips + rotations\nCompared to AVL:\n\nFewer rotations (faster insert/delete)- Slightly taller (slower lookup)- Simpler amortized balance Used in:\nC++ std::map, std::set- Java TreeMap, Linux scheduler Complexity: All major operations (O\\(\\log n\\))\n\n\n\n4. Splay Tree\nIdea: Bring recently accessed node to root via splaying (rotations). It adapts to access patterns , the more you access a key, the faster it becomes.\nSplaying Steps:\n\nZig: one rotation (root child)- Zig-Zig: two rotations (same side)- Zig-Zag: two rotations (different sides) Code (Conceptual):\n\nNode* splay(Node* root, int key) {\n    if (!root || root-&gt;key == key) return root;\n    if (key &lt; root-&gt;key) {\n        if (!root-&gt;left) return root;\n        // splay in left subtree\n        if (key &lt; root-&gt;left-&gt;key)\n            root-&gt;left-&gt;left = splay(root-&gt;left-&gt;left, key),\n            root = rotateRight(root);\n        else if (key &gt; root-&gt;left-&gt;key)\n            root-&gt;left-&gt;right = splay(root-&gt;left-&gt;right, key),\n            root-&gt;left = rotateLeft(root-&gt;left);\n        return rotateRight(root);\n    } else {\n        if (!root-&gt;right) return root;\n        // symmetric\n    }\n}\nWhy It’s Cool: No strict balance, but amortized (O\\(\\log n\\)). Frequently accessed elements stay near top.\nUsed in self-adjusting caches, rope data structures, memory allocators.\n\n\n5. Treap (Tree + Heap)\nIdea: Each node has two keys:\n\nBST key → order property- Priority → heap property Insertion = normal BST insert + heap fix via rotation.\n\nBalance comes from randomization , random priorities ensure expected (O\\(\\log n\\)) height.\nCode Sketch:\ntypedef struct Node {\n    int key, priority;\n    struct Node *left, *right;\n} Node;\n\nNode* insert(Node* root, int key) {\n    if (!root) return newNode(key, rand());\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else root-&gt;right = insert(root-&gt;right, key);\n\n    if (root-&gt;left && root-&gt;left-&gt;priority &gt; root-&gt;priority)\n        root = rotateRight(root);\n    if (root-&gt;right && root-&gt;right-&gt;priority &gt; root-&gt;priority)\n        root = rotateLeft(root);\n    return root;\n}\nAdvantages:\n\nSimple logic- Random balancing- Expected (O\\(\\log n\\)) Used in randomized algorithms and functional programming.\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nTree\nBalance Type\nRotations\nHeight\nInsert/Delete\nLookup\nNotes\n\n\n\n\nAVL\nStrict\nMore\n(O\\(\\log n\\))\nMedium\nFast\nLookup-heavy\n\n\nRed-Black\nRelaxed\nFewer\n(O\\(\\log n\\))\nFast\nMedium\nLibrary std\n\n\nSplay\nAdaptive\nVariable\nAmortized (O\\(\\log n\\))\nFast\nFast (amortized)\nAccess patterns\n\n\nTreap\nRandomized\nAvg few\n(O\\(\\log n\\)) expected\nSimple\nSimple\nProbabilistic\n\n\n\n\n\nTiny Code\nAVL Insert (Skeleton):\nNode* insert(Node* root, int key) {\n    if (!root) return newNode(key);\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else root-&gt;right = insert(root-&gt;right, key);\n    root-&gt;h = 1 + max(height(root-&gt;left), height(root-&gt;right));\n    int b = balance(root);\n    if (b &gt; 1 && key &lt; root-&gt;left-&gt;key) return rotateRight(root);\n    if (b &lt; -1 && key &gt; root-&gt;right-&gt;key) return rotateLeft(root);\n    // other cases...\n    return root;\n}\n\n\nWhy It Matters\nBalanced trees guarantee predictable performance under dynamic updates. Each variant represents a philosophy:\n\nAVL: precision- Red-Black: practicality- Splay: adaptability- Treap: randomness Together, they teach one core idea , keep height in check, no matter the operations.\n\n\n\nTry It Yourself\n\nImplement an AVL tree and visualize rotations.\nInsert keys [10, 20, 30, 40, 50] and trace Red-Black color changes.\nSplay after each access , see which keys stay near top.\nBuild a Treap with random priorities , measure average height.\nCompare performance of BST vs AVL on sorted input.\n\nBalanced trees are the architects of order , always keeping chaos one rotation away.\n\n\n\n25. Segment Trees and Fenwick Trees\nWhen you need to answer range queries quickly (like sum, min, max) and support updates to individual elements, simple prefix sums won’t cut it anymore.\nYou need something smarter , data structures that can divide and conquer over ranges, updating and combining results efficiently.\nThat’s exactly what Segment Trees and Fenwick Trees (Binary Indexed Trees) do:\n\nQuery over a range in (O\\(\\log n\\))- Update elements in (O\\(\\log n\\)) They’re the backbone of competitive programming, signal processing, and database analytics.\n\n\n1. The Problem\nGiven an array A[0..n-1], support:\n\nupdate(i, x) → change A[i] to x\nquery(L, R) → compute sum (or min, max) of A[L..R]\n\nNaive approach:\n\nUpdate: (O(1))- Query: (O(n)) Prefix sums fix one but not both. Segment and Fenwick trees fix both.\n\n\n\n2. Segment Tree\nIdea: Divide the array into segments (intervals) recursively. Each node stores an aggregate (sum, min, max) of its range. You can combine child nodes to get any range result.\nStructure (Sum Example):\n           [0,7] sum=36\n         /           \\\n   [0,3]=10         [4,7]=26\n   /     \\           /      \\\n[0,1]=3 [2,3]=7  [4,5]=11  [6,7]=15\nEach node represents a range [L,R]. Leaf nodes = single elements.\n\n\nA. Build\nRecursive Construction: Time: (O(n))\nvoid build(int node, int L, int R) {\n    if (L == R) tree[node] = arr[L];\n    else {\n        int mid = (L + R) / 2;\n        build(2*node, L, mid);\n        build(2*node+1, mid+1, R);\n        tree[node] = tree[2*node] + tree[2*node+1];\n    }\n}\n\n\nB. Query (Range Sum)\nQuery [l, r] recursively:\n\nIf current range [L, R] fully inside [l, r], return node value- If disjoint, return 0- Else combine children\n\nint query(int node, int L, int R, int l, int r) {\n    if (r &lt; L || R &lt; l) return 0;\n    if (l &lt;= L && R &lt;= r) return tree[node];\n    int mid = (L + R) / 2;\n    return query(2*node, L, mid, l, r)\n         + query(2*node+1, mid+1, R, l, r);\n}\n\n\nC. Update\nChange arr[i] = x and update tree nodes covering i.\nvoid update(int node, int L, int R, int i, int x) {\n    if (L == R) tree[node] = x;\n    else {\n        int mid = (L + R)/2;\n        if (i &lt;= mid) update(2*node, L, mid, i, x);\n        else update(2*node+1, mid+1, R, i, x);\n        tree[node] = tree[2*node] + tree[2*node+1];\n    }\n}\nComplexities:\n\nBuild: (O(n))- Query: (O\\(\\log n\\))- Update: (O\\(\\log n\\))- Space: (O(4n))\n\n\n\nD. Variants\nSegment trees are flexible:\n\nRange minimum/maximum- Range GCD- Lazy propagation → range updates- 2D segment tree for grids\n\n\n\n3. Fenwick Tree (Binary Indexed Tree)\nIdea: Stores cumulative frequencies using bit manipulation. Each node covers a range size = LSB(index).\nSimpler, smaller, but supports only associative ops (sum, xor, etc.)\nIndexing:\n\nParent: i + (i & -i)- Child: i - (i & -i) Build: Initialize with zero, then add elements one by one.\n\nAdd / Update:\nvoid add(int i, int x) {\n    for (; i &lt;= n; i += i & -i)\n        bit[i] += x;\n}\nPrefix Sum:\nint sum(int i) {\n    int res = 0;\n    for (; i &gt; 0; i -= i & -i)\n        res += bit[i];\n    return res;\n}\nRange Sum [L, R]: \\[\n\\text{sum}(R) - \\text{sum}(L-1)\n\\]\nComplexities:\n\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\log n\\))- Update: (O\\(\\log n\\))- Space: (O(n))\n\n\n\n4. Comparison\n\n\n\nFeature\nSegment Tree\nFenwick Tree\n\n\n\n\nSpace\nO(4n)\nO(n)\n\n\nBuild\nO(n)\nO(n log n)\n\n\nQuery\nO(log n)\nO(log n)\n\n\nUpdate\nO(log n)\nO(log n)\n\n\nRange Update\nWith Lazy\nTricky\n\n\nRange Query\nFlexible\nSum/XOR only\n\n\nImplementation\nModerate\nSimple\n\n\n\n\n\n5. Applications\n\nSum / Min / Max / XOR queries- Frequency counts- Inversions counting- Order statistics- Online problems where array updates over time Used in:\nCompetitive programming- Databases (analytics on changing data)- Time series queries- Games (damage/range updates)\n\n\n\nTiny Code\nFenwick Tree Example:\nint bit[1001], n;\n\nvoid update(int i, int val) {\n    for (; i &lt;= n; i += i & -i)\n        bit[i] += val;\n}\n\nint query(int i) {\n    int res = 0;\n    for (; i &gt; 0; i -= i & -i)\n        res += bit[i];\n    return res;\n}\n\n// range sum\nint range_sum(int L, int R) { return query(R) - query(L - 1); }\n\n\nWhy It Matters\nSegment and Fenwick trees embody divide-and-conquer over data , balancing dynamic updates with range queries. They’re how modern systems aggregate live data efficiently.\nThey teach a powerful mindset:\n\n“If you can split a problem, you can solve it fast.”\n\n\n\nTry It Yourself\n\nBuild a segment tree for sum queries.\nAdd range minimum queries (RMQ).\nImplement a Fenwick tree , test with prefix sums.\nSolve: number of inversions in array using Fenwick tree.\nAdd lazy propagation to segment tree for range updates.\n\nOnce you master these, range queries will never scare you again , you’ll slice through them in logarithmic time.\n\n\n\n26. Disjoint Set Union (Union-Find)\nMany problems involve grouping elements into sets and efficiently checking whether two elements belong to the same group , like connected components in a graph, network connectivity, Kruskal’s MST, or even social network clustering.\nFor these, the go-to structure is the Disjoint Set Union (DSU), also called Union-Find. It efficiently supports two operations:\n\nfind(x) → which set does x belong to?\nunion(x, y) → merge the sets containing x and y.\n\nWith path compression and union by rank, both operations run in near-constant time, specifically (O((n))), where \\(\\alpha\\) is the inverse Ackermann function (practically ≤ 4).\n\n1. The Problem\nSuppose you have (n) elements initially in separate sets. Over time, you want to:\n\nMerge two sets- Check if two elements share the same set Example:\n\nSets: {1}, {2}, {3}, {4}, {5}\nUnion(1,2) → {1,2}, {3}, {4}, {5}\nUnion(3,4) → {1,2}, {3,4}, {5}\nFind(2) == Find(1)? Yes\nFind(5) == Find(3)? No\n\n\n2. Basic Implementation\nEach element has a parent pointer. Initially, every node is its own parent.\nParent array representation:\nint parent[N];\n\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find(int v) {\n    if (v == parent[v]) return v;\n    return find(parent[v]);\n}\n\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b)\n        parent[b] = a;\n}\nThis works, but deep trees can form , making find slow. We fix that with path compression.\n\n\n3. Path Compression\nEvery time we call find(v), we make all nodes along the path point directly to the root. This flattens the tree dramatically.\nOptimized Find:\nint find(int v) {\n    if (v == parent[v]) return v;\n    return parent[v] = find(parent[v]);\n}\nSo next time, lookups will be (O(1)) for those nodes.\n\n\n4. Union by Rank / Size\nWhen merging, always attach the smaller tree to the larger to keep depth small.\nUnion by Rank:\nint parent[N], rank[N];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b])\n            swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\nUnion by Size (Alternative): Track size of each set and attach smaller to larger.\nint size[N];\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b) {\n        if (size[a] &lt; size[b]) swap(a, b);\n        parent[b] = a;\n        size[a] += size[b];\n    }\n}\n\n\n5. Complexity\nWith both path compression and union by rank, all operations are effectively constant time: \\[\nO(\\alpha(n)) \\approx O(1)\n\\]\nFor all practical (n), ((n) ).\n\n\n\nOperation\nTime\n\n\n\n\nMake set\nO(1)\n\n\nFind\nO(α(n))\n\n\nUnion\nO(α(n))\n\n\n\n\n\n6. Applications\n\nGraph Connectivity: determine connected components- Kruskal’s MST: add edges, avoid cycles- Dynamic connectivity- Image segmentation- Network clustering- Cycle detection in undirected graphs Example: Kruskal’s Algorithm\n\nsort(edges.begin(), edges.end());\nfor (edge e : edges)\n    if (find(e.u) != find(e.v)) {\n        union_sets(e.u, e.v);\n        mst_weight += e.w;\n    }\n\n\n7. Example\nint parent[6], rank[6];\n\nvoid init() {\n    for (int i = 1; i &lt;= 5; i++) {\n        parent[i] = i;\n        rank[i] = 0;\n    }\n}\n\nint main() {\n    init();\n    union_sets(1, 2);\n    union_sets(3, 4);\n    union_sets(2, 3);\n    printf(\"%d\\n\", find(4)); // prints representative of {1,2,3,4}\n}\nResult: {1,2,3,4}, {5}\n\n\n8. Visualization\nBefore compression:\n1\n \\\n  2\n   \\\n    3\n\nAfter compression:\n1\n├─2\n└─3\nEvery find call makes future queries faster.\n\n\n9. Comparison\n\n\n\nVariant\nFind\nUnion\nNotes\n\n\n\n\nBasic\nO(n)\nO(n)\nDeep trees\n\n\nPath Compression\nO(α(n))\nO(α(n))\nVery fast\n\n\n+ Rank / Size\nO(α(n))\nO(α(n))\nBalanced\n\n\nPersistent DSU\nO(log n)\nO(log n)\nUndo/rollback support\n\n\n\n\n\nTiny Code\nFull DSU with path compression + rank:\nint parent[1000], rank[1000];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nint find(int v) {\n    if (v != parent[v])\n        parent[v] = find(parent[v]);\n    return parent[v];\n}\n\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b]) swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\n\n\nWhy It Matters\nUnion-Find embodies structural sharing and lazy optimization , you don’t balance eagerly, but just enough. It’s one of the most elegant demonstrations of how constant-time algorithms are possible through clever organization.\nIt teaches a key algorithmic lesson:\n\n“Work only when necessary, and fix structure as you go.”\n\n\n\nTry It Yourself\n\nImplement DSU and test find/union.\nBuild a program that counts connected components.\nSolve Kruskal’s MST using DSU.\nAdd get_size(v) to return component size.\nTry rollback DSU (keep stack of changes).\n\nUnion-Find is the quiet powerhouse behind many graph and connectivity algorithms , simple, fast, and deeply elegant.\n\n\n\n27. Probabilistic Data Structures (Bloom, Count-Min, HyperLogLog)\nWhen you work with massive data streams , billions of elements, too big for memory , you can’t store everything. But what if you don’t need perfect answers, just fast and tiny approximate ones?\nThat’s where probabilistic data structures shine. They trade a bit of accuracy for huge space savings and constant-time operations.\nIn this section, we’ll explore three of the most famous:\n\nBloom Filters → membership queries- Count-Min Sketch → frequency estimation- HyperLogLog → cardinality estimation Each of them answers “How likely is X?” or “How many?” efficiently , perfect for modern analytics, caching, and streaming systems.\n\n\n1. Bloom Filter , “Is this element probably in the set?”\nA Bloom filter answers:\n\n“Is x in the set?” with either maybe yes or definitely no.\n\nNo false negatives, but some false positives.\n\n\nA. Idea\nUse an array of bits (size m), all initialized to 0. Use k different hash functions.\nTo insert an element:\n\nCompute k hashes: ( h_1(x), h_2(x), , h_k(x) )\nSet each bit position \\(b_i = 1\\)\n\nTo query an element:\n\nCompute same k hashes\nIf all bits are 1 → maybe yes\nIf any bit is 0 → definitely no\n\n\n\nB. Example\nInsert dog:\n\n(h_1(dog)=2, h_2(dog)=5, h_3(dog)=9) Set bits 2, 5, 9 → 1\n\nCheck cat:\n\nIf any hash bit = 0 → not present\n\n\n\nC. Complexity\n\n\n\nOperation\nTime\nSpace\nAccuracy\n\n\n\n\nInsert\nO(k)\nO(m)\nTunable\n\n\nQuery\nO(k)\nO(m)\nFalse positives\n\n\n\nFalse positive rate ≈ ( \\(1 - e^{-kn/m}\\)^k )\nChoose m and k based on expected n and acceptable error.\n\n\nD. Code\n#define M 1000\nint bitset[M];\n\nint hash1(int x) { return (x * 17) % M; }\nint hash2(int x) { return (x * 31 + 7) % M; }\n\nvoid add(int x) {\n    bitset[hash1(x)] = 1;\n    bitset[hash2(x)] = 1;\n}\n\nbool contains(int x) {\n    return bitset[hash1(x)] && bitset[hash2(x)];\n}\nUsed in:\n\nCaches (check before disk lookup)- Spam filters- Databases (join filtering)- Blockchain and peer-to-peer networks\n\n\n\n2. Count-Min Sketch , “How often has this appeared?”\nTracks frequency counts in a stream, using sub-linear memory.\nInstead of a full table, it uses a 2D array of counters, each row hashed with a different hash function.\n\n\nA. Insert\nFor each row i:\n\nCompute hash (h_i(x))- Increment count[i][h_i(x)]++ #### B. Query\n\nFor element x:\n\nCompute all (h_i(x))- Take min(count[i][h_i(x)]) across rows → gives an upper-bounded estimate of true frequency\n\n\n\nC. Code\n#define W 1000\n#define D 5\nint count[D][W];\n\nint hash(int i, int x) {\n    return (x * (17*i + 3)) % W;\n}\n\nvoid add(int x) {\n    for (int i = 0; i &lt; D; i++)\n        count[i][hash(i, x)]++;\n}\n\nint query(int x) {\n    int res = INT_MAX;\n    for (int i = 0; i &lt; D; i++)\n        res = min(res, count[i][hash(i, x)]);\n    return res;\n}\n\n\nD. Complexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(D)\nO(W×D)\n\n\nQuery\nO(D)\nO(W×D)\n\n\n\nError controlled by: \\[\n\\varepsilon = \\frac{1}{W}, \\quad \\delta = 1 - e^{-D}\n\\]\nUsed in:\n\nFrequency counting in streams- Hot-key detection- Network flow analysis- Trending topics\n\n\n\n3. HyperLogLog , “How many unique items?”\nEstimates cardinality (number of distinct elements) with very small memory (~1.5 KB for millions).\n\n\nA. Idea\nHash each element uniformly → 32-bit value. Split hash into:\n\nPrefix bits → bucket index- Suffix bits → count leading zeros Each bucket stores the max leading zero count seen. At the end, use harmonic mean of counts to estimate distinct values.\n\n\n\nB. Formula\n\\[\nE = \\alpha_m \\cdot m^2 \\cdot \\Big(\\sum_{i=1}^m 2^{-M[i]}\\Big)^{-1}\n\\]\nwhere M[i] is the zero count in bucket i, and \\(\\alpha_m\\) is a correction constant.\nAccuracy: ~1.04 / √m\n\n\nC. Complexity\n\n\n\nOperation\nTime\nSpace\nError\n\n\n\n\nAdd\nO(1)\nO(m)\n~1.04/√m\n\n\nMerge\nO(m)\nO(m)\n,\n\n\n\nUsed in:\n\nWeb analytics (unique visitors)- Databases (COUNT DISTINCT)- Distributed systems (mergeable estimates)\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nStructure\nPurpose\nQuery\nMemory\nError\nNotes\n\n\n\n\nBloom\nMembership\nO(k)\nTiny\nFalse positives\nNo deletions\n\n\nCount-Min\nFrequency\nO(D)\nSmall\nOverestimate\nStreaming counts\n\n\nHyperLogLog\nCardinality\nO(1)\nVery small\n~1%\nMergeable\n\n\n\n\n\nTiny Code\nBloom Filter Demo:\nadd(42);\nadd(17);\nprintf(\"%d\\n\", contains(42)); // 1 (maybe yes)\nprintf(\"%d\\n\", contains(99)); // 0 (definitely no)\n\n\nWhy It Matters\nProbabilistic data structures show how approximation beats impossibility when resources are tight. They make it feasible to process massive streams in real time, when storing everything is impossible.\nThey teach a deeper algorithmic truth:\n\n“A bit of uncertainty can buy you a world of scalability.”\n\n\n\nTry It Yourself\n\nImplement a Bloom filter with 3 hash functions.\nMeasure false positive rate for 10K elements.\nBuild a Count-Min Sketch and test frequency estimation.\nApproximate unique elements using HyperLogLog logic.\nExplore real-world systems: Redis (Bloom/CM Sketch), PostgreSQL (HyperLogLog).\n\nThese tiny probabilistic tools are how big data becomes tractable.\n\n\n\n28. Skip Lists and B-Trees\nWhen you want fast search, insert, and delete but need a structure that’s easier to code than trees or optimized for disk and memory blocks, two clever ideas step in:\n\nSkip Lists → randomized, layered linked lists that behave like balanced BSTs- B-Trees → multi-way trees that minimize disk I/O and organize large data blocks Both guarantee (O\\(\\log n\\)) operations, but they shine in very different environments , Skip Lists in-memory, B-Trees on disk.\n\n\n1. Skip Lists\nInvented by: William Pugh (1990) Goal: Simulate binary search using linked lists with probabilistic shortcuts.\n\n\nA. Idea\nA skip list is a stack of linked lists, each level skipping over more elements.\nExample:\nLevel 3:        ┌───────&gt; 50 ───────┐\nLevel 2:   ┌──&gt; 10 ─────&gt; 30 ─────&gt; 50 ───┐\nLevel 1:  5 ──&gt; 10 ──&gt; 20 ──&gt; 30 ──&gt; 40 ──&gt; 50\nHigher levels are sparser and let you “skip” large chunks of the list.\nYou search top-down:\n\nMove right while next ≤ target- Drop down when you can’t go further This mimics binary search , logarithmic layers, logarithmic hops.\n\n\n\nB. Construction\nEach inserted element is given a random height, with geometric distribution:\n\nLevel 1 (base) always exists- Level 2 with probability ½- Level 3 with ¼, etc. Expected total nodes = 2n, Expected height = (O\\(\\log n\\))\n\n\n\nC. Operations\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nSearch\n(O\\(\\log n\\))\nO(n)\nRandomized balance\n\n\nInsert\n(O\\(\\log n\\))\nO(n)\nRebuild towers\n\n\nDelete\n(O\\(\\log n\\))\nO(n)\nRewire pointers\n\n\n\nSearch Algorithm:\nNode* search(SkipList* sl, int key) {\n    Node* cur = sl-&gt;head;\n    for (int lvl = sl-&gt;level; lvl &gt;= 0; lvl--) {\n        while (cur-&gt;forward[lvl] && cur-&gt;forward[lvl]-&gt;key &lt; key)\n            cur = cur-&gt;forward[lvl];\n    }\n    cur = cur-&gt;forward[0];\n    if (cur && cur-&gt;key == key) return cur;\n    return NULL;\n}\nSkip Lists are simple, fast, and probabilistically balanced , no rotations, no rebalancing.\n\n\nD. Why Use Skip Lists?\n\nEasier to implement than balanced trees- Support concurrent access well- Randomized, not deterministic , but highly reliable Used in:\nRedis (sorted sets)- LevelDB / RocksDB internals- Concurrent maps\n\n\n\n2. B-Trees\nInvented by: Rudolf Bayer & Ed McCreight (1972) Goal: Reduce disk access by grouping data in blocks.\nA B-Tree is a generalization of a BST:\n\nEach node holds multiple keys and children- Keys are kept sorted- Child subtrees span ranges between keys\n\n\n\nA. Structure\nA B-Tree of order m:\n\nEach node has ≤ m children- Each internal node has k-1 keys if it has k children- All leaves at the same depth Example (order 3):\n\n        [17 | 35]\n       /    |     \\\n [5 10] [20 25 30] [40 45 50]\n\n\nB. Operations\n\nSearch\n\nTraverse from root - Binary search in each node’s key array - Follow appropriate child → (O\\(\\log_m n\\))\n\nInsert\n\nInsert in leaf - If overflow → split node - Promote median key to parent\n\nDelete\n\nBorrow or merge if node underflows Each split or merge keeps height minimal.\n\n\n\n\nC. Complexity\n\n\n\nOperation\nTime\nDisk Accesses\nNotes\n\n\n\n\nSearch\n(O\\(\\log_m n\\))\n(O\\(\\log_m n\\))\nm = branching factor\n\n\nInsert\n(O\\(\\log_m n\\))\n(O(1)) splits\nBalanced\n\n\nDelete\n(O\\(\\log_m n\\))\n(O(1)) merges\nBalanced\n\n\n\nHeight ≈ \\(\\log_m n\\) → very shallow when (m) large (e.g. 100).\n\n\nD. B+ Tree Variant\nIn B+ Trees:\n\nAll data in leaves (internal nodes = indexes)- Leaves linked → efficient range queries Used in:\nDatabases (MySQL, PostgreSQL)- File systems (NTFS, HFS+)- Key-value stores\n\n\n\nE. Example Flow\nInsert 25:\n[10 | 20 | 30] → overflow\nSplit → [10] [30]\nPromote 20\nRoot: [20]\n\n\n3. Comparison\n\n\n\nFeature\nSkip List\nB-Tree\n\n\n\n\nBalancing\nRandomized\nDeterministic\n\n\nFanout\n2 (linked)\nm-way\n\n\nEnvironment\nIn-memory\nDisk-based\n\n\nSearch\nO(log n)\nO\\(log_m n\\)\n\n\nInsert/Delete\nO(log n)\nO\\(log_m n\\)\n\n\nConcurrency\nEasy\nComplex\n\n\nRange Queries\nSequential scan\nLinked leaves (B+)\n\n\n\n\n\nTiny Code\nSkip List Search (Conceptual):\nNode* search(SkipList* list, int key) {\n    Node* cur = list-&gt;head;\n    for (int lvl = list-&gt;level; lvl &gt;= 0; lvl--) {\n        while (cur-&gt;next[lvl] && cur-&gt;next[lvl]-&gt;key &lt; key)\n            cur = cur-&gt;next[lvl];\n    }\n    cur = cur-&gt;next[0];\n    return (cur && cur-&gt;key == key) ? cur : NULL;\n}\nB-Tree Node (Skeleton):\n#define M 4\ntypedef struct {\n    int keys[M-1];\n    Node* child[M];\n    int n;\n} Node;\n\n\nWhy It Matters\nSkip Lists and B-Trees show two paths to balance:\n\nRandomized simplicity (Skip List)- Block-based order (B-Tree) Both offer logarithmic guarantees, but one optimizes pointer chasing, the other I/O.\n\nThey’re fundamental to:\n\nIn-memory caches (Skip List)- On-disk indexes (B-Tree, B+ Tree)- Sorted data structures across systems\n\n\n\nTry It Yourself\n\nBuild a basic skip list and insert random keys.\nTrace a search path across levels.\nImplement B-Tree insert and split logic.\nCompare height of BST vs B-Tree for 1,000 keys.\nExplore how Redis and MySQL use these internally.\n\nTogether, they form the bridge between linked lists and balanced trees, uniting speed, structure, and scalability.\n\n\n\n29. Persistent and Functional Data Structures\nMost data structures are ephemeral , when you update them, the old version disappears. But sometimes, you want to keep all past versions, so you can go back in time, undo operations, or run concurrent reads safely.\nThat’s the magic of persistent data structures: every update creates a new version while sharing most of the old structure.\nThis section introduces the idea of persistence, explores how to make classic structures like arrays and trees persistent, and explains why functional programming loves them.\n\n1. What Is Persistence?\nA persistent data structure preserves previous versions after updates. You can access any version , past or present , without side effects.\nThree levels:\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nPartial\nCan access past versions, but only modify the latest\nUndo stack\n\n\nFull\nCan access and modify any version\nImmutable map\n\n\nConfluent\nCan combine different versions\nGit-like merges\n\n\n\nThis is essential in functional programming, undo systems, version control, persistent segment trees, and immutable databases.\n\n\n2. Ephemeral vs Persistent\nEphemeral:\narr[2] = 7; // old value lost forever\nPersistent:\nnew_arr = update(arr, 2, 7); // old_arr still exists\nPersistent structures use structural sharing , unchanged parts are reused, not copied.\n\n\n3. Persistent Linked List\nEasiest example: each update creates a new head, reusing the tail.\nstruct Node { int val; Node* next; };\n\nNode* push(Node* head, int x) {\n    Node* newHead = malloc(sizeof(Node));\n    newHead-&gt;val = x;\n    newHead-&gt;next = head;\n    return newHead;\n}\nNow both old_head and new_head coexist. Each version is immutable , you never change existing nodes.\nAccess: old and new lists share most of their structure:\nv0: 1 → 2 → 3\nv1: 0 → 1 → 2 → 3\nOnly one new node was created.\n\n\n4. Persistent Binary Tree\nFor trees, updates create new paths from the root to the modified node, reusing the rest.\ntypedef struct Node {\n    int key;\n    struct Node *left, *right;\n} Node;\n\nNode* update(Node* root, int pos, int val) {\n    if (!root) return newNode(val);\n    Node* node = malloc(sizeof(Node));\n    *node = *root; // copy\n    if (pos &lt; root-&gt;key) node-&gt;left = update(root-&gt;left, pos, val);\n    else node-&gt;right = update(root-&gt;right, pos, val);\n    return node;\n}\nEach update creates a new version , only (O\\(\\log n\\)) new nodes per change.\nThis is the core of persistent segment trees used in competitive programming.\n\n\n5. Persistent Array (Functional Trick)\nArrays are trickier because of random access. Solutions:\n\nUse balanced binary trees as array replacements- Each update replaces one node- Persistent vector = tree of small arrays (used in Clojure, Scala) This gives:\nAccess: (O\\(\\log n\\))- Update: (O\\(\\log n\\))- Space: (O\\(\\log n\\)) per update\n\n\n\n6. Persistent Segment Tree\nUsed for versioned range queries:\n\nEach update = new root- Each version = snapshot of history Example: Track how array changes over time, query “sum in range [L,R] at version t”.\n\nBuild:\nNode* build(int L, int R) {\n    if (L == R) return newNode(arr[L]);\n    int mid = (L+R)/2;\n    return newNode(\n        build(L, mid),\n        build(mid+1, R),\n        sum\n    );\n}\nUpdate: only (O\\(\\log n\\)) new nodes\nNode* update(Node* prev, int L, int R, int pos, int val) {\n    if (L == R) return newNode(val);\n    int mid = (L+R)/2;\n    if (pos &lt;= mid)\n        return newNode(update(prev-&gt;left, L, mid, pos, val), prev-&gt;right);\n    else\n        return newNode(prev-&gt;left, update(prev-&gt;right, mid+1, R, pos, val));\n}\nEach version = new root; old ones still valid.\n\n\n7. Functional Perspective\nIn functional programming, data is immutable by default. Instead of mutating, you create a new version.\nThis allows:\n\nThread-safety (no races)- Time-travel debugging- Undo/redo systems- Concurrency without locks Languages like Haskell, Clojure, and Elm build everything this way.\n\nFor example, Clojure’s persistent vector uses path copying and branching factor 32 for (O\\(\\log_{32} n\\)) access.\n\n\n8. Applications\n\nUndo / Redo stacks (text editors, IDEs)- Version control (Git trees)- Immutable databases (Datomic)- Segment trees over time (competitive programming)- Snapshots in memory allocators or games\n\n\n\n9. Complexity\n\n\n\n\n\n\n\n\n\n\nStructure\nUpdate\nAccess\nSpace per Update\nNotes\n\n\n\n\nPersistent Linked List\nO(1)\nO(1)\nO(1)\nSimple sharing\n\n\nPersistent Tree\nO(log n)\nO(log n)\nO(log n)\nPath copying\n\n\nPersistent Array\nO(log n)\nO(log n)\nO(log n)\nTree-backed\n\n\nPersistent Segment Tree\nO(log n)\nO(log n)\nO(log n)\nVersioned queries\n\n\n\n\n\nTiny Code\nPersistent Linked List Example:\nNode* v0 = NULL;\nv0 = push(v0, 3);\nv0 = push(v0, 2);\nNode* v1 = push(v0, 1);\n// v0 = [2,3], v1 = [1,2,3]\n\n\nWhy It Matters\nPersistence is about time as a first-class citizen. It lets you:\n\nRoll back- Compare versions- Work immutably and safely It’s the algorithmic foundation behind functional programming, time-travel debugging, and immutable data systems.\n\nIt teaches this powerful idea:\n\n“Never destroy , always build upon what was.”\n\n\n\nTry It Yourself\n\nImplement a persistent stack using linked lists.\nWrite a persistent segment tree for range sums.\nTrack array versions after each update and query old states.\nCompare space/time with an ephemeral one.\nExplore persistent structures in Clojure (conj, assoc) or Rust (im crate).\n\nPersistence transforms data from fleeting state into a history you can navigate , a timeline of structure and meaning.\n\n\n\n30. Advanced Trees and Range Queries\nSo far, you’ve seen balanced trees (AVL, Red-Black, Treap) and segment-based structures (Segment Trees, Fenwick Trees). Now it’s time to combine those ideas and step into advanced trees , data structures that handle dynamic sets, order statistics, intervals, ranges, and geometry-like queries in logarithmic time.\nThis chapter is about trees that go beyond search , they store order, track ranges, and answer complex queries efficiently.\nWe’ll explore:\n\nOrder Statistic Trees (k-th element, rank queries)- Interval Trees (range overlaps)- Range Trees (multi-dimensional search)- KD-Trees (spatial partitioning)- Merge Sort Trees (offline range queries)\n\n\n1. Order Statistic Tree\nGoal: find the k-th smallest element, or the rank of an element, in (O\\(\\log n\\)).\nBuilt on top of a balanced BST (e.g. Red-Black) by storing subtree sizes.\n\n\nA. Augmented Tree Nodes\nEach node keeps:\n\nkey: element value- left, right: children- size: number of nodes in subtree\n\ntypedef struct Node {\n    int key, size;\n    struct Node *left, *right;\n} Node;\nWhenever you rotate or insert, update size:\nint get_size(Node* n) { return n ? n-&gt;size : 0; }\nvoid update_size(Node* n) {\n    if (n) n-&gt;size = get_size(n-&gt;left) + get_size(n-&gt;right) + 1;\n}\n\n\nB. Find k-th Element\nRecursively use subtree sizes:\nNode* kth(Node* root, int k) {\n    int left = get_size(root-&gt;left);\n    if (k == left + 1) return root;\n    else if (k &lt;= left) return kth(root-&gt;left, k);\n    else return kth(root-&gt;right, k - left - 1);\n}\nTime: (O\\(\\log n\\))\n\n\nC. Find Rank\nFind position of a key (number of smaller elements):\nint rank(Node* root, int key) {\n    if (!root) return 0;\n    if (key &lt; root-&gt;key) return rank(root-&gt;left, key);\n    if (key &gt; root-&gt;key) return get_size(root-&gt;left) + 1 + rank(root-&gt;right, key);\n    return get_size(root-&gt;left) + 1;\n}\nUsed in:\n\nDatabases (ORDER BY, pagination)- Quantile queries- Online median maintenance\n\n\n\n2. Interval Tree\nGoal: find all intervals overlapping with a given point or range.\nUsed in computational geometry, scheduling, and genomic data.\n\n\nA. Structure\nBST ordered by interval low endpoint. Each node stores:\n\nlow, high: interval bounds- max: maximum high in its subtree\n\ntypedef struct {\n    int low, high, max;\n    struct Node *left, *right;\n} Node;\n\n\nB. Query Overlap\nCheck if x overlaps node-&gt;interval: If not, go left or right based on max values.\nbool overlap(Interval a, Interval b) {\n    return a.low &lt;= b.high && b.low &lt;= a.high;\n}\n\nNode* overlap_search(Node* root, Interval q) {\n    if (!root) return NULL;\n    if (overlap(root-&gt;interval, q)) return root;\n    if (root-&gt;left && root-&gt;left-&gt;max &gt;= q.low)\n        return overlap_search(root-&gt;left, q);\n    return overlap_search(root-&gt;right, q);\n}\nTime: (O\\(\\log n\\)) average\n\n\nC. Use Cases\n\nCalendar/schedule conflict detection- Collision detection- Genome region lookup- Segment intersection\n\n\n\n3. Range Tree\nGoal: multi-dimensional queries like\n\n“How many points fall inside rectangle [x1, x2] × [y1, y2]?”\n\nStructure:\n\nPrimary BST on x- Each node stores secondary BST on y Query time: (O\\(\\log^2 n\\)) Space: (O\\(n \\log n\\))\n\nUsed in:\n\n2D search- Computational geometry- Databases (spatial joins)\n\n\n\n4. KD-Tree\nGoal: efficiently search points in k-dimensional space.\nAlternate splitting dimensions at each level:\n\nLevel 0 → split by x- Level 1 → split by y- Level 2 → split by z Each node stores:\nPoint (vector)- Split axis Used for:\nNearest neighbor search- Range queries- ML (k-NN classifiers) Time:\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\sqrt{n}\\)) average in 2D\n\n\n\n5. Merge Sort Tree\nGoal: query “number of elements ≤ k in range [L, R]”\nBuilt like a segment tree, but each node stores a sorted list of its range.\nBuild: merge children lists Query: binary search in node lists\nTime:\n\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\log^2 n\\)) Used in offline queries and order-statistics over ranges.\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nTree Type\nUse Case\nQuery\nUpdate\nNotes\n\n\n\n\nOrder Statistic\nk-th, rank\nO(log n)\nO(log n)\nAugmented BST\n\n\nInterval\nOverlaps\nO(log n + k)\nO(log n)\nStore intervals\n\n\nRange Tree\n2D range\nO(log² n + k)\nO(log² n)\nMulti-dim\n\n\nKD-Tree\nSpatial\nO(√n) avg\nO(log n)\nNearest neighbor\n\n\nMerge Sort Tree\nOffline rank\nO(log² n)\nStatic\nBuilt from sorted segments\n\n\n\n\n\nTiny Code\nOrder Statistic Example:\nNode* root = NULL;\nroot = insert(root, 10);\nroot = insert(root, 20);\nroot = insert(root, 30);\nprintf(\"%d\", kth(root, 2)-&gt;key); // 20\nInterval Query:\nInterval q = {15, 17};\nNode* res = overlap_search(root, q);\nif (res) printf(\"Overlap: [%d, %d]\\n\", res-&gt;low, res-&gt;high);\n\n\nWhy It Matters\nThese trees extend balance into dimensions and ranges. They let you query ordered data efficiently: “How many?”, “Which overlaps?”, “Where is k-th smallest?”.\nThey teach a deeper design principle:\n\n“Augment structure with knowledge , balance plus metadata equals power.”\n\n\n\nTry It Yourself\n\nImplement an order statistic tree , test rank/k-th queries.\nInsert intervals and test overlap detection.\nBuild a simple KD-tree for 2D points.\nSolve rectangle counting with a range tree.\nPrecompute a merge sort tree for offline queries.\n\nThese advanced trees form the final evolution of structured queries , blending geometry, order, and logarithmic precision.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-4.-graph-algorithms",
    "href": "books/en-us/book.html#chapter-4.-graph-algorithms",
    "title": "The Book",
    "section": "Chapter 4. Graph Algorithms",
    "text": "Chapter 4. Graph Algorithms\n\n31. Traversals (DFS, BFS, Iterative Deepening)\nGraphs are everywhere , maps, networks, dependencies, state spaces. Before you can analyze them, you need a way to visit their vertices , systematically, without getting lost or looping forever.\nThat’s where graph traversals come in. They’re the foundation for everything that follows: connected components, shortest paths, spanning trees, topological sorts, and more.\nThis section walks through the three pillars:\n\nDFS (Depth-First Search) , explore deeply before backtracking- BFS (Breadth-First Search) , explore level by level- Iterative Deepening , a memory-friendly hybrid\n\n\n1. Representing Graphs\nBefore traversal, you need a good structure.\nAdjacency List (most common):\n#define MAX 1000\nvector&lt;int&gt; adj[MAX];\nAdd edges:\nvoid add_edge(int u, int v) {\n    adj[u].push_back(v);\n    adj[v].push_back(u); // omit if directed\n}\nTrack visited vertices:\nbool visited[MAX];\n\n\n2. Depth-First Search (DFS)\nDFS dives deep, following one branch fully before exploring others. It’s recursive, like exploring a maze by always turning left until you hit a wall.\n\n\nA. Recursive Form\nvoid dfs(int u) {\n    visited[u] = true;\n    for (int v : adj[u]) {\n        if (!visited[v])\n            dfs(v);\n    }\n}\nStart it:\ndfs(start_node);\n\n\nB. Iterative Form (with Stack)\nvoid dfs_iter(int start) {\n    stack&lt;int&gt; s;\n    s.push(start);\n    while (!s.empty()) {\n        int u = s.top(); s.pop();\n        if (visited[u]) continue;\n        visited[u] = true;\n        for (int v : adj[u]) s.push(v);\n    }\n}\n\n\nC. Complexity\n\n\n\nGraph Type\nTime\nSpace\n\n\n\n\nAdjacency List\nO(V + E)\nO(V)\n\n\n\nDFS is used in:\n\nConnected components- Cycle detection- Topological sort- Backtracking & search- Articulation points / bridges\n\n\n\n3. Breadth-First Search (BFS)\nBFS explores neighbors first , it’s like expanding in waves. This guarantees shortest path in unweighted graphs.\n\n\nA. BFS with Queue\nvoid bfs(int start) {\n    queue&lt;int&gt; q;\n    q.push(start);\n    visited[start] = true;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (int v : adj[u]) {\n            if (!visited[v]) {\n                visited[v] = true;\n                q.push(v);\n            }\n        }\n    }\n}\n\n\nB. Track Distance\nint dist[MAX];\nvoid bfs_dist(int s) {\n    fill(dist, dist + MAX, -1);\n    dist[s] = 0;\n    queue&lt;int&gt; q; q.push(s);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (int v : adj[u]) {\n            if (dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n}\nNow dist[v] gives shortest distance from s.\n\n\nC. Complexity\nSame as DFS:\n\n\n\nTime\nSpace\n\n\n\n\nO(V + E)\nO(V)\n\n\n\nUsed in:\n\nShortest paths (unweighted)- Level-order traversal- Bipartite check- Connected components\n\n\n\n4. Iterative Deepening Search (IDS)\nDFS is memory-light but might go too deep. BFS is optimal but can use a lot of memory. Iterative Deepening Search (IDS) combines both.\nIt performs DFS with increasing depth limits:\nbool dls(int u, int target, int depth) {\n    if (u == target) return true;\n    if (depth == 0) return false;\n    for (int v : adj[u])\n        if (dls(v, target, depth - 1)) return true;\n    return false;\n}\n\nbool ids(int start, int target, int max_depth) {\n    for (int d = 0; d &lt;= max_depth; d++)\n        if (dls(start, target, d)) return true;\n    return false;\n}\nUsed in:\n\nAI search problems (state spaces)- Game trees (chess, puzzles)\n\n\n\n5. Traversal Order Examples\nFor a graph:\n1 - 2 - 3\n|   |\n4 - 5\nDFS (starting at 1): 1 → 2 → 3 → 5 → 4 BFS (starting at 1): 1 → 2 → 4 → 3 → 5\n\n\n6. Directed vs Undirected\n\nUndirected: mark both directions- Directed: follow edge direction only DFS on directed graphs is core to:\nSCC (Strongly Connected Components)- Topological Sorting- Reachability analysis\n\n\n\n7. Traversal Trees\nEach traversal implicitly builds a spanning tree:\n\nDFS Tree: based on recursion- BFS Tree: based on levels Use them to:\nDetect cross edges, back edges- Classify edges (important for algorithms like Tarjan’s)\n\n\n\n8. Comparison\n\n\n\n\n\n\n\n\nAspect\nDFS\nBFS\n\n\n\n\nStrategy\nDeep first\nLevel-wise\n\n\nSpace\nO(V) (stack)\nO(V) (queue)\n\n\nPath Optimality\nNot guaranteed\nYes (unweighted)\n\n\nApplications\nCycle detection, backtracking\nShortest path, level order\n\n\n\n\n\nTiny Code\nDFS + BFS Combo:\nvoid traverse(int n) {\n    for (int i = 1; i &lt;= n; i++) visited[i] = false;\n    dfs(1);\n    for (int i = 1; i &lt;= n; i++) visited[i] = false;\n    bfs(1);\n}\n\n\nWhy It Matters\nDFS and BFS are the roots of graph theory in practice. Every algorithm you’ll meet later , shortest paths, flows, SCCs , builds upon them.\nThey teach you how to navigate structure, how to systematically explore unknowns, and how search lies at the heart of computation.\n\n\nTry It Yourself\n\nBuild an adjacency list and run DFS/BFS from vertex 1.\nTrack discovery and finish times in DFS.\nUse BFS to compute shortest paths in an unweighted graph.\nModify DFS to count connected components.\nImplement IDS for a puzzle like the 8-puzzle.\n\nGraph traversal is the art of exploration , once you master it, the rest of graph theory falls into place.\n\n\n\n32. Strongly Connected Components (Tarjan, Kosaraju)\nIn directed graphs, edges have direction, so connectivity gets tricky. It’s not enough for vertices to be reachable , you need mutual reachability.\nThat’s the essence of a strongly connected component (SCC):\n\nA set of vertices where every vertex can reach every other vertex.\n\nThink of SCCs as islands of mutual connectivity , inside, you can go anywhere; outside, you can’t. They’re the building blocks for simplifying directed graphs into condensation DAGs (no cycles).\nWe’ll explore two classic algorithms:\n\nKosaraju’s Algorithm , clean, intuitive, two-pass- Tarjan’s Algorithm , one-pass, stack-based elegance\n\n\n1. Definition\nA Strongly Connected Component (SCC) in a directed graph ( G = (V, E) ) is a maximal subset of vertices \\(C \\subseteq V\\) such that for every pair ( (u, v) C ): \\(u \\to v\\) and \\(v \\to u\\).\nIn other words, every node in (C) is reachable from every other node in (C).\nExample:\n1 → 2 → 3 → 1   forms an SCC  \n4 → 5           separate SCCs\n\n\n2. Applications\n\nCondensation DAG: compress SCCs into single nodes , no cycles remain.- Component-based reasoning: topological sorting on DAG of SCCs.- Program analysis: detecting cycles, dependencies.- Web graphs: find clusters of mutually linked pages.- Control-flow: loops and strongly connected subroutines.\n\n\n\n3. Kosaraju’s Algorithm\nA simple two-pass algorithm using DFS and graph reversal.\nSteps:\n\nRun DFS and push nodes onto a stack in finish-time order.\nReverse the graph (edges flipped).\nPop nodes from stack; DFS on reversed graph; each DFS = one SCC.\n\n\n\nA. Implementation\nvector&lt;int&gt; adj[MAX], rev[MAX];\nbool visited[MAX];\nstack&lt;int&gt; st;\nvector&lt;vector&lt;int&gt;&gt; sccs;\n\nvoid dfs1(int u) {\n    visited[u] = true;\n    for (int v : adj[u])\n        if (!visited[v])\n            dfs1(v);\n    st.push(u);\n}\n\nvoid dfs2(int u, vector&lt;int&gt;& comp) {\n    visited[u] = true;\n    comp.push_back(u);\n    for (int v : rev[u])\n        if (!visited[v])\n            dfs2(v, comp);\n}\n\nvoid kosaraju(int n) {\n    // Pass 1: order by finish time\n    for (int i = 1; i &lt;= n; i++)\n        if (!visited[i]) dfs1(i);\n\n    // Reverse graph\n    for (int u = 1; u &lt;= n; u++)\n        for (int v : adj[u])\n            rev[v].push_back(u);\n\n    // Pass 2: collect SCCs\n    fill(visited, visited + n + 1, false);\n    while (!st.empty()) {\n        int u = st.top(); st.pop();\n        if (!visited[u]) {\n            vector&lt;int&gt; comp;\n            dfs2(u, comp);\n            sccs.push_back(comp);\n        }\n    }\n}\nTime Complexity: (O(V + E)) , two DFS passes.\nSpace Complexity: (O(V + E))\n\n\nB. Example\nGraph:\n1 → 2 → 3  \n↑   ↓   ↓  \n5 ← 4 ← 6\nSCCs:\n\n{1,2,4,5}- {3,6}\n\n\n\n4. Tarjan’s Algorithm\nMore elegant: one DFS pass, no reversal, stack-based. It uses discovery times and low-link values to detect SCC roots.\n\n\nA. Idea\n\ndisc[u]: discovery time of node u- low[u]: smallest discovery time reachable from u- A node is root of an SCC if disc[u] == low[u] Maintain a stack of active nodes (in current DFS path).\n\n\n\nB. Implementation\nvector&lt;int&gt; adj[MAX];\nint disc[MAX], low[MAX], timer;\nbool inStack[MAX];\nstack&lt;int&gt; st;\nvector&lt;vector&lt;int&gt;&gt; sccs;\n\nvoid dfs_tarjan(int u) {\n    disc[u] = low[u] = ++timer;\n    st.push(u);\n    inStack[u] = true;\n\n    for (int v : adj[u]) {\n        if (!disc[v]) {\n            dfs_tarjan(v);\n            low[u] = min(low[u], low[v]);\n        } else if (inStack[v]) {\n            low[u] = min(low[u], disc[v]);\n        }\n    }\n\n    if (disc[u] == low[u]) {\n        vector&lt;int&gt; comp;\n        while (true) {\n            int v = st.top(); st.pop();\n            inStack[v] = false;\n            comp.push_back(v);\n            if (v == u) break;\n        }\n        sccs.push_back(comp);\n    }\n}\n\nvoid tarjan(int n) {\n    for (int i = 1; i &lt;= n; i++)\n        if (!disc[i])\n            dfs_tarjan(i);\n}\nTime Complexity: (O(V + E))\nSpace Complexity: (O(V))\n\n\nC. Walkthrough\nGraph:\n1 → 2 → 3  \n↑   ↓   ↓  \n5 ← 4 ← 6\nDFS visits nodes in order; when it finds a node whose disc == low, it pops from the stack to form an SCC.\nResult:\nSCC1: 1 2 4 5\nSCC2: 3 6\n\n\n5. Comparison\n\n\n\nFeature\nKosaraju\nTarjan\n\n\n\n\nDFS Passes\n2\n1\n\n\nReversal Needed\nYes\nNo\n\n\nStack\nYes (finish order)\nYes (active path)\n\n\nImplementation\nSimple conceptually\nCompact, efficient\n\n\nTime\nO(V + E)\nO(V + E)\n\n\n\n\n\n6. Condensation Graph\nOnce SCCs are found, you can build a DAG: Each SCC becomes a node, edges represent cross-SCC connections. Topological sorting now applies.\nUsed in:\n\nDependency analysis- Strong component compression- DAG dynamic programming\n\n\n\nTiny Code\nPrint SCCs (Tarjan):\ntarjan(n);\nfor (auto &comp : sccs) {\n    for (int x : comp) printf(\"%d \", x);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\nSCC algorithms turn chaotic directed graphs into structured DAGs. They’re the key to reasoning about cycles, dependencies, and modularity.\nUnderstanding them reveals a powerful truth:\n\n“Every complex graph can be reduced to a simple hierarchy , once you find its strongly connected core.”\n\n\n\nTry It Yourself\n\nImplement both Kosaraju and Tarjan , verify they match.\nBuild SCC DAG and run topological sort on it.\nDetect cycles via SCC size &gt; 1.\nUse SCCs to solve 2-SAT (boolean satisfiability).\nVisualize condensation of a graph with 6 nodes.\n\nOnce you can find SCCs, you can tame directionality , transforming messy networks into ordered systems.\n\n\n\n33. Shortest Paths (Dijkstra, Bellman-Ford, A*, Johnson)\nOnce you can traverse a graph, the next natural question is:\n\n“What is the shortest path between two vertices?”\n\nShortest path algorithms are the heart of routing, navigation, planning, and optimization. They compute minimal cost paths , whether distance, time, or weight , and adapt to different edge conditions (non-negative, negative, heuristic).\nThis section covers the most essential algorithms:\n\nDijkstra’s Algorithm , efficient for non-negative weights- Bellman-Ford Algorithm , handles negative edges- A* , best-first with heuristics- Johnson’s Algorithm , all-pairs shortest paths in sparse graphs\n\n\n1. The Shortest Path Problem\nGiven a weighted graph ( G = (V, E) ) and a source ( s ), find \\(\\text{dist}[v]\\), the minimum total weight to reach every vertex ( v ).\nVariants:\n\nSingle-source shortest path (SSSP) , one source to all- Single-pair , one source to one target- All-pairs shortest path (APSP) , every pair- Dynamic shortest path , with updates\n\n\n\n2. Dijkstra’s Algorithm\nBest for non-negative weights. Idea: explore vertices in increasing distance order, like water spreading.\n\n\nA. Steps\n\nInitialize all distances to infinity.\nSet source distance = 0.\nUse a priority queue to always pick the node with smallest tentative distance.\nRelax all outgoing edges.\n\n\n\nB. Implementation (Adjacency List)\n#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n\nconst int INF = 1e9;\nvector&lt;pair&lt;int,int&gt;&gt; adj[1000]; // (neighbor, weight)\nint dist[1000];\n\nvoid dijkstra(int n, int s) {\n    fill(dist, dist + n + 1, INF);\n    dist[s] = 0;\n    priority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq;\n    pq.push({0, s});\n\n    while (!pq.empty()) {\n        auto [d, u] = pq.top(); pq.pop();\n        if (d != dist[u]) continue;\n        for (auto [v, w] : adj[u]) {\n            if (dist[v] &gt; dist[u] + w) {\n                dist[v] = dist[u] + w;\n                pq.push({dist[v], v});\n            }\n        }\n    }\n}\nComplexity:\n\nUsing priority queue (binary heap): \\(O((V + E)\\log V)\\)\nSpace: \\(O(V + E)\\)\n\n\n\nC. Example\nGraph:\n1 →(2) 2 →(3) 3\n↓(4)       ↑(1)\n4 →(2)─────┘\ndijkstra(1) gives shortest distances:\ndist[1] = 0  \ndist[2] = 2  \ndist[3] = 5  \ndist[4] = 4\n\n\nD. Properties\n\nWorks only if all edges \\(w \\ge 0\\)- Can reconstruct path via parent[v]- Used in:\n\nGPS and routing systems - Network optimization - Scheduling with positive costs\n\n\n\n\n3. Bellman-Ford Algorithm\nHandles negative edge weights, and detects negative cycles.\n\n\nA. Idea\nRelax all edges (V-1) times. If on (V)-th iteration you can still relax → negative cycle exists.\n\n\nB. Implementation\nstruct Edge { int u, v, w; };\nvector&lt;Edge&gt; edges;\nint dist[1000];\n\nbool bellman_ford(int n, int s) {\n    fill(dist, dist + n + 1, INF);\n    dist[s] = 0;\n    for (int i = 1; i &lt;= n - 1; i++) {\n        for (auto e : edges) {\n            if (dist[e.u] + e.w &lt; dist[e.v])\n                dist[e.v] = dist[e.u] + e.w;\n        }\n    }\n    // Check for negative cycle\n    for (auto e : edges)\n        if (dist[e.u] + e.w &lt; dist[e.v])\n            return false; // negative cycle\n    return true;\n}\nComplexity: (O(VE)) Works even when (w &lt; 0).\n\n\nC. Example\nGraph:\n1 →(2) 2 →(-5) 3 →(2) 4\nBellman-Ford finds path 1→2→3→4 with total cost (-1).\nIf a cycle reduces total weight indefinitely, algorithm detects it.\n\n\nD. Use Cases\n\nCurrency exchange arbitrage- Game graphs with penalties- Detecting impossible constraints\n\n\n\n4. A* Search Algorithm\nHeuristic-guided shortest path, perfect for pathfinding (AI, maps, games).\nIt combines actual cost and estimated cost: \\[\nf(v) = g(v) + h(v)\n\\] where\n\n(g(v)): known cost so far- (h(v)): heuristic estimate (must be admissible)\n\n\n\nA. Pseudocode\npriority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq;\ng[start] = 0;\npq.push({h[start], start});\n\nwhile (!pq.empty()) {\n    auto [f, u] = pq.top(); pq.pop();\n    if (u == goal) break;\n    for (auto [v, w] : adj[u]) {\n        int new_g = g[u] + w;\n        if (new_g &lt; g[v]) {\n            g[v] = new_g;\n            pq.push({g[v] + h[v], v});\n        }\n    }\n}\nHeuristic Example:\n\nEuclidean distance (for grids)- Manhattan distance (for 4-direction movement)\n\n\n\nB. Use Cases\n\nGame AI (pathfinding)- Robot motion planning- Map navigation Complexity: (O(E)) in best case, depends on heuristic quality.\n\n\n\n5. Johnson’s Algorithm\nGoal: All-Pairs Shortest Path in sparse graphs with negative edges (no negative cycles).\nIdea:\n\nAdd new vertex q connected to all others with edge weight 0\nRun Bellman-Ford from q to get potential h(v)\nReweight edges: (w’(u, v) = w(u, v) + h(u) - h(v)) (now all weights ≥ 0)\nRun Dijkstra from each vertex\n\nComplexity: (O\\(VE + V^2 \\log V\\))\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nHandles Negative Weights\nDetects Negative Cycle\nHeuristic\nComplexity\nUse Case\n\n\n\n\nDijkstra\nNo\nNo\nNo\nO((V+E) log V)\nNon-negative weights\n\n\nBellman-Ford\nYes\nYes\nNo\nO(VE)\nNegative edges\n\n\nA*\nNo (unless careful)\nNo\nYes\nDepends\nPathfinding\n\n\nJohnson\nYes (no neg. cycles)\nYes\nNo\nO(VE + V log V)\nAll-pairs, sparse\n\n\n\n\n\nTiny Code\nDijkstra Example:\ndijkstra(n, 1);\nfor (int i = 1; i &lt;= n; i++)\n    printf(\"dist[%d] = %d\\n\", i, dist[i]);\n\n\nWhy It Matters\nShortest paths are the essence of optimization , not just in graphs, but in reasoning: finding minimal cost, minimal distance, minimal risk.\nThese algorithms teach:\n\n“The path to a goal isn’t random , it’s guided by structure, weight, and knowledge.”\n\n\n\nTry It Yourself\n\nBuild a weighted graph and compare Dijkstra vs Bellman-Ford.\nIntroduce a negative edge and observe Bellman-Ford detecting it.\nImplement A* on a grid with obstacles.\nUse Dijkstra to plan routes in a city map dataset.\nTry Johnson’s algorithm for all-pairs shortest paths.\n\nMaster these, and you master direction + cost = intelligence in motion.\n\n\n\n34. Shortest Path Variants (0-1 BFS, Bidirectional, Heuristic A*)\nSometimes the classic shortest path algorithms aren’t enough. You might have special edge weights (only 0 or 1), a need for faster searches, or extra structure you can exploit.\nThat’s where shortest path variants come in , they’re optimized adaptations of the big three (BFS, Dijkstra, A*) for specific scenarios.\nIn this section, we’ll explore:\n\n0-1 BFS → when edge weights are only 0 or 1- Bidirectional Search → meet-in-the-middle for speed- Heuristic A* → smarter exploration guided by estimates Each shows how structure in your problem can yield speed-ups.\n\n\n1. 0-1 BFS\nIf all edge weights are either 0 or 1, you don’t need a priority queue. A deque (double-ended queue) is enough for (O(V + E)) time.\nWhy? Because edges with weight 0 should be processed immediately, while edges with weight 1 can wait one step longer.\n\n\nA. Algorithm\nUse a deque.\n\nWhen relaxing an edge with weight 0, push to front.- When relaxing an edge with weight 1, push to back.\n\nconst int INF = 1e9;\nvector&lt;pair&lt;int,int&gt;&gt; adj[1000]; // (v, w)\nint dist[1000];\n\nvoid zero_one_bfs(int n, int s) {\n    fill(dist, dist + n + 1, INF);\n    deque&lt;int&gt; dq;\n    dist[s] = 0;\n    dq.push_front(s);\n\n    while (!dq.empty()) {\n        int u = dq.front(); dq.pop_front();\n        for (auto [v, w] : adj[u]) {\n            if (dist[v] &gt; dist[u] + w) {\n                dist[v] = dist[u] + w;\n                if (w == 0) dq.push_front(v);\n                else dq.push_back(v);\n            }\n        }\n    }\n}\n\n\nB. Example\nGraph:\n1 -0-&gt; 2 -1-&gt; 3  \n|              ^  \n1              |  \n+--------------+\nShortest path from 1 to 3 = 1 (via edge 1-2-3). Deque ensures weight-0 edges don’t get delayed.\n\n\nC. Complexity\n\n\n\nTime\nSpace\nNotes\n\n\n\n\nO(V + E)\nO(V)\nOptimal for binary weights\n\n\n\nUsed in:\n\nLayered BFS- Grid problems with binary costs- BFS with teleportation (weight 0 edges)\n\n\n\n2. Bidirectional Search\nSometimes you just need one path , from source to target , in an unweighted graph. Instead of expanding from one side, expand from both ends and stop when they meet.\nThis reduces search depth from (O\\(b^d\\)) to (O\\(b^{d/2}\\)) (huge gain for large graphs).\n\n\nA. Idea\nRun BFS from both source and target simultaneously. When their frontiers intersect, you’ve found the shortest path.\n\n\nB. Implementation\nbool visited_from_s[MAX], visited_from_t[MAX];\nqueue&lt;int&gt; qs, qt;\n\nint bidirectional_bfs(int s, int t) {\n    qs.push(s); visited_from_s[s] = true;\n    qt.push(t); visited_from_t[t] = true;\n\n    while (!qs.empty() && !qt.empty()) {\n        if (step(qs, visited_from_s, visited_from_t)) return 1;\n        if (step(qt, visited_from_t, visited_from_s)) return 1;\n    }\n    return 0;\n}\n\nbool step(queue&lt;int&gt;& q, bool vis[], bool other[]) {\n    int size = q.size();\n    while (size--) {\n        int u = q.front(); q.pop();\n        if (other[u]) return true;\n        for (int v : adj[u]) {\n            if (!vis[v]) {\n                vis[v] = true;\n                q.push(v);\n            }\n        }\n    }\n    return false;\n}\n\n\nC. Complexity\n\n\n\nTime\nSpace\nNotes\n\n\n\n\nO\\(b^{d/2}\\)\nO\\(b^{d/2}\\)\nDoubly fast in practice\n\n\n\nUsed in:\n\nMaze solvers- Shortest paths in large sparse graphs- Social network “degrees of separation”\n\n\n\n3. Heuristic A* (Revisited)\nA* generalizes Dijkstra with goal-directed search using heuristics. We revisit it here to show how heuristics change exploration order.\n\n\nA. Cost Function\n\\[\nf(v) = g(v) + h(v)\n\\]\n\n(g(v)): cost so far- (h(v)): estimated cost to goal- (h(v)) must be admissible ((h(v) ))\n\n\n\nB. Implementation\nstruct Node {\n    int v; int f, g;\n    bool operator&gt;(const Node& o) const { return f &gt; o.f; }\n};\n\npriority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt;&gt; pq;\n\nvoid astar(int start, int goal) {\n    g[start] = 0;\n    h[start] = heuristic(start, goal);\n    pq.push({start, g[start] + h[start], g[start]});\n\n    while (!pq.empty()) {\n        auto [u, f_u, g_u] = pq.top(); pq.pop();\n        if (u == goal) break;\n        for (auto [v, w] : adj[u]) {\n            int new_g = g[u] + w;\n            if (new_g &lt; g[v]) {\n                g[v] = new_g;\n                int f_v = new_g + heuristic(v, goal);\n                pq.push({v, f_v, new_g});\n            }\n        }\n    }\n}\n\n\nC. Example Heuristics\n\nGrid map: Manhattan distance (h(x, y) = |x - x_g| + |y - y_g|)\nNavigation: straight-line (Euclidean)- Game tree: evaluation function\n\n\n\nD. Performance\n\n\n\nHeuristic\nEffect\n\n\n\n\nPerfect (h = true cost)\nOptimal, visits minimal nodes\n\n\nAdmissible but weak\nStill correct, more nodes\n\n\nOverestimate\nMay fail (non-admissible)\n\n\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWeight Type\nStrategy\nTime\nSpace\nNotes\n\n\n\n\n0-1 BFS\n0 or 1\nDeque-based\nO(V+E)\nO(V)\nNo heap\n\n\nBidirectional BFS\nUnweighted\nTwo-way search\nO\\(b^{d/2}\\)\nO\\(b^{d/2}\\)\nMeets in middle\n\n\nA*\nNon-negative\nHeuristic search\nDepends\nO(V)\nGuided\n\n\n\n\n\n5. Example Scenario\n\n\n\n\n\n\n\nProblem\nVariant\n\n\n\n\nGrid with teleport (cost 0)\n0-1 BFS\n\n\nHuge social graph (find shortest chain)\nBidirectional BFS\n\n\nGame AI pathfinding\nA* with Manhattan heuristic\n\n\n\n\n\nTiny Code\n0-1 BFS Quick Demo:\nadd_edge(1, 2, 0);\nadd_edge(2, 3, 1);\nzero_one_bfs(3, 1);\nprintf(\"%d\\n\", dist[3]); // shortest = 1\n\n\nWhy It Matters\nSpecial cases deserve special tools. These variants show that understanding structure (like edge weights or symmetry) can yield huge gains.\nThey embody a principle:\n\n“Don’t just run faster , run smarter, guided by what you know.”\n\n\n\nTry It Yourself\n\nImplement 0-1 BFS for a grid with cost 0 teleports.\nCompare BFS vs Bidirectional BFS on a large maze.\nWrite A* for an 8x8 chessboard knight’s move puzzle.\nTune heuristics , see how overestimating breaks A*.\nCombine A* and 0-1 BFS for hybrid search.\n\nWith these in hand, you can bend shortest path search to the shape of your problem , efficient, elegant, and exact.\n\n\n\n35. Minimum Spanning Trees (Kruskal, Prim, Borůvka)\nWhen a graph connects multiple points with weighted edges, sometimes you don’t want the shortest path, but the cheapest network that connects everything.\nThat’s the Minimum Spanning Tree (MST) problem:\n\nGiven a connected, weighted, undirected graph, find a subset of edges that connects all vertices with minimum total weight and no cycles.\n\nMSTs are everywhere , from building networks and designing circuits to clustering and approximation algorithms.\nThree cornerstone algorithms solve it beautifully:\n\nKruskal’s , edge-based, union-find- Prim’s , vertex-based, greedy expansion- Borůvka’s , component merging in parallel\n\n\n1. What Is a Spanning Tree?\nA spanning tree connects all vertices with exactly (V-1) edges. Among all spanning trees, the one with minimum total weight is the MST.\nProperties:\n\nContains no cycles- Connects all vertices- Edge count = (V - 1)- Unique if all weights distinct\n\n\n\n2. MST Applications\n\nNetwork design (roads, cables, pipelines)- Clustering (e.g., hierarchical clustering)- Image segmentation- Approximation (e.g., TSP ~ 2 × MST)- Graph simplification\n\n\n\n3. Kruskal’s Algorithm\nBuild the MST edge-by-edge, in order of increasing weight. Use Union-Find (Disjoint Set Union) to avoid cycles.\n\n\nA. Steps\n\nSort all edges by weight.\nInitialize each vertex as its own component.\nFor each edge (u, v):\n\nIf u and v are in different components → include edge - Union their sets Stop when (V-1) edges chosen.\n\n\n\n\nB. Implementation\nstruct Edge { int u, v, w; };\nvector&lt;Edge&gt; edges;\nint parent[MAX], rank_[MAX];\n\nint find(int x) {\n    return parent[x] == x ? x : parent[x] = find(parent[x]);\n}\nbool unite(int a, int b) {\n    a = find(a); b = find(b);\n    if (a == b) return false;\n    if (rank_[a] &lt; rank_[b]) swap(a, b);\n    parent[b] = a;\n    if (rank_[a] == rank_[b]) rank_[a]++;\n    return true;\n}\n\nint kruskal(int n) {\n    iota(parent, parent + n + 1, 0);\n    sort(edges.begin(), edges.end(), [](Edge a, Edge b){ return a.w &lt; b.w; });\n    int total = 0;\n    for (auto &e : edges)\n        if (unite(e.u, e.v))\n            total += e.w;\n    return total;\n}\nComplexity:\n\nSorting edges: (O\\(E \\log E\\))- Union-Find operations: (O((V))) (almost constant)- Total: (O\\(E \\log E\\))\n\n\n\nC. Example\nGraph:\n1 -4- 2  \n|     |  \n2     3  \n \\-1-/\nEdges sorted: (1-3,1), (1-2,4), (2-3,3)\nPick 1-3, 2-3 → MST weight = 1 + 3 = 4\n\n\n4. Prim’s Algorithm\nGrow MST from a starting vertex, adding the smallest outgoing edge each step.\nSimilar to Dijkstra , but pick edges, not distances.\n\n\nA. Steps\n\nStart with one vertex, mark as visited.\nUse priority queue for candidate edges.\nPick smallest edge that connects to an unvisited vertex.\nAdd vertex to MST, repeat until all visited.\n\n\n\nB. Implementation\nvector&lt;pair&lt;int,int&gt;&gt; adj[MAX]; // (v, w)\nbool used[MAX];\nint prim(int n, int start) {\n    priority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq;\n    pq.push({0, start});\n    int total = 0;\n\n    while (!pq.empty()) {\n        auto [w, u] = pq.top(); pq.pop();\n        if (used[u]) continue;\n        used[u] = true;\n        total += w;\n        for (auto [v, w2] : adj[u])\n            if (!used[v]) pq.push({w2, v});\n    }\n    return total;\n}\nComplexity:\n\n\\(O((V+E) \\log V)\\) with binary heap\n\nUsed when:\n\nGraph is dense\nEasier to grow tree than sort all edges\n\n\n\nC. Example\nGraph:\n1 -2- 2  \n|     |  \n4     1  \n \\-3-/\nStart at 1 → choose (1-2), (1-3) → MST weight = 2 + 3 = 5\n\n\n5. Borůvka’s Algorithm\nLess famous, but elegant , merges cheapest outgoing edge per component in parallel.\nEach component picks one cheapest outgoing edge, adds it, merges components. Repeat until one component left.\nComplexity: (O\\(E \\log V\\))\nUsed in parallel/distributed MST computations.\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nStrategy\nTime\nSpace\nBest For\n\n\n\n\nKruskal\nEdge-based, sort all edges\nO(E log E)\nO(E)\nSparse graphs\n\n\nPrim\nVertex-based, grow tree\nO(E log V)\nO(V+E)\nDense graphs\n\n\nBorůvka\nComponent merging\nO(E log V)\nO(E)\nParallel MST\n\n\n\n\n\n7. MST Properties\n\nCut Property: For any cut, smallest crossing edge ∈ MST.- Cycle Property: For any cycle, largest edge not ∈ MST.- MST may not be unique if equal weights.\n\n\n\n8. Building the Tree\nStore MST edges:\nvector&lt;Edge&gt; mst_edges;\nif (unite(e.u, e.v)) mst_edges.push_back(e);\nThen use MST for:\n\nPath queries- Clustering (remove largest edge)- Approximation TSP (preorder traversal)\n\n\n\nTiny Code\nKruskal Example:\nedges.push_back({1,2,4});\nedges.push_back({1,3,1});\nedges.push_back({2,3,3});\nprintf(\"MST = %d\\n\", kruskal(3)); // 4\n\n\nWhy It Matters\nMSTs model connection without redundancy. They’re about efficiency , connecting everything at minimal cost, a principle that appears in infrastructure, data, and even ideas.\nThey teach:\n\n“You can connect the whole with less , if you choose wisely.”\n\n\n\nTry It Yourself\n\nImplement Kruskal’s algorithm using union-find.\nRun Prim’s algorithm and compare output.\nBuild MST on random weighted graph , visualize tree.\nRemove heaviest edge from MST to form two clusters.\nExplore Borůvka for parallel execution.\n\nMSTs are how you span complexity with minimal effort , a tree of balance, economy, and order.\n\n\n\n36. Flows (Ford-Fulkerson, Edmonds-Karp, Dinic)\nSome graphs don’t just connect , they carry something. Imagine water flowing through pipes, traffic through roads, data through a network. Each edge has a capacity, and you want to know:\n\n“How much can I send from source to sink before the system clogs?”\n\nThat’s the Maximum Flow problem , a cornerstone of combinatorial optimization, powering algorithms for matching, cuts, scheduling, and more.\nThis section covers the big three:\n\nFord-Fulkerson , the primal idea- Edmonds-Karp , BFS-based implementation- Dinic’s Algorithm , layered speed\n\n\n1. Problem Definition\nGiven a directed graph ( G = (V, E) ), each edge ( (u, v) ) has a capacity ( c(u, v) ).\nWe have:\n\nSource ( s )- Sink ( t ) We want the maximum flow from ( s ) to ( t ): a function ( f(u, v) ) that satisfies:\n\n\nCapacity constraint: ( 0 f(u, v) c(u, v) )\nFlow conservation: For every vertex \\(v \\neq s, t\\): (f(u, v) = f(v, w))\n\nTotal flow = (f(s, v))\n\n\n2. The Big Picture\nMax Flow - Min Cut Theorem:\n\nThe value of the maximum flow equals the capacity of the minimum cut.\n\nSo finding a max flow is equivalent to finding the bottleneck.\n\n\n3. Ford-Fulkerson Method\nThe idea:\n\nWhile there exists a path from (s) to (t) with available capacity, push flow along it.\n\nEach step:\n\nFind augmenting path\nSend flow = min residual capacity along it\nUpdate residual capacities\n\nRepeat until no augmenting path.\n\n\nA. Residual Graph\nResidual capacity: \\[\nr(u, v) = c(u, v) - f(u, v)\n\\] If ( f(u, v) &gt; 0 ), then add reverse edge ( (v, u) ) with capacity ( f(u, v) ).\nThis allows undoing flow if needed.\n\n\nB. Implementation (DFS-style)\nconst int INF = 1e9;\nvector&lt;pair&lt;int,int&gt;&gt; adj[MAX];\nint cap[MAX][MAX];\n\nint dfs(int u, int t, int flow, vector&lt;int&gt;& vis) {\n    if (u == t) return flow;\n    vis[u] = 1;\n    for (auto [v, _] : adj[u]) {\n        if (!vis[v] && cap[u][v] &gt; 0) {\n            int pushed = dfs(v, t, min(flow, cap[u][v]), vis);\n            if (pushed &gt; 0) {\n                cap[u][v] -= pushed;\n                cap[v][u] += pushed;\n                return pushed;\n            }\n        }\n    }\n    return 0;\n}\n\nint ford_fulkerson(int s, int t, int n) {\n    int flow = 0;\n    while (true) {\n        vector&lt;int&gt; vis(n + 1, 0);\n        int pushed = dfs(s, t, INF, vis);\n        if (pushed == 0) break;\n        flow += pushed;\n    }\n    return flow;\n}\nComplexity: (O\\(E \\cdot \\text{max flow}\\)) , depends on flow magnitude.\n\n\n4. Edmonds-Karp Algorithm\nA refinement:\n\nAlways choose shortest augmenting path (by edges) using BFS.\n\nGuarantees polynomial time.\n\n\nA. Implementation (BFS + parent tracking)\nint bfs(int s, int t, vector&lt;int&gt;& parent, int n) {\n    fill(parent.begin(), parent.end(), -1);\n    queue&lt;pair&lt;int,int&gt;&gt; q;\n    q.push({s, INF});\n    parent[s] = -2;\n    while (!q.empty()) {\n        auto [u, flow] = q.front(); q.pop();\n        for (auto [v, _] : adj[u]) {\n            if (parent[v] == -1 && cap[u][v] &gt; 0) {\n                int new_flow = min(flow, cap[u][v]);\n                parent[v] = u;\n                if (v == t) return new_flow;\n                q.push({v, new_flow});\n            }\n        }\n    }\n    return 0;\n}\n\nint edmonds_karp(int s, int t, int n) {\n    int flow = 0;\n    vector&lt;int&gt; parent(n + 1);\n    int new_flow;\n    while ((new_flow = bfs(s, t, parent, n))) {\n        flow += new_flow;\n        int v = t;\n        while (v != s) {\n            int u = parent[v];\n            cap[u][v] -= new_flow;\n            cap[v][u] += new_flow;\n            v = u;\n        }\n    }\n    return flow;\n}\nComplexity: (O\\(VE^2\\)) Always terminates (no dependence on flow values).\n\n\n5. Dinic’s Algorithm\nA modern classic , uses BFS to build level graph, and DFS to send blocking flow.\nIt works layer-by-layer, avoiding useless exploration.\n\n\nA. Steps\n\nBuild level graph via BFS (assign levels to reachable nodes).\nDFS sends flow along level-respecting paths.\nRepeat until no path remains.\n\n\n\nB. Implementation\nvector&lt;int&gt; level, ptr;\n\nbool bfs_level(int s, int t, int n) {\n    fill(level.begin(), level.end(), -1);\n    queue&lt;int&gt; q;\n    q.push(s);\n    level[s] = 0;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (auto [v, _] : adj[u])\n            if (level[v] == -1 && cap[u][v] &gt; 0) {\n                level[v] = level[u] + 1;\n                q.push(v);\n            }\n    }\n    return level[t] != -1;\n}\n\nint dfs_flow(int u, int t, int pushed) {\n    if (u == t || pushed == 0) return pushed;\n    for (int &cid = ptr[u]; cid &lt; (int)adj[u].size(); cid++) {\n        int v = adj[u][cid].first;\n        if (level[v] == level[u] + 1 && cap[u][v] &gt; 0) {\n            int tr = dfs_flow(v, t, min(pushed, cap[u][v]));\n            if (tr &gt; 0) {\n                cap[u][v] -= tr;\n                cap[v][u] += tr;\n                return tr;\n            }\n        }\n    }\n    return 0;\n}\n\nint dinic(int s, int t, int n) {\n    int flow = 0;\n    level.resize(n + 1);\n    ptr.resize(n + 1);\n    while (bfs_level(s, t, n)) {\n        fill(ptr.begin(), ptr.end(), 0);\n        while (int pushed = dfs_flow(s, t, INF))\n            flow += pushed;\n    }\n    return flow;\n}\nComplexity: (O\\(EV^2\\)) worst case, (O\\(E \\sqrt{V}\\)) in practice.\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nStrategy\nHandles\nTime\nNotes\n\n\n\n\nFord-Fulkerson\nDFS augmenting paths\nIntegral capacities\nO\\(E × max_flow\\)\nSimple, may loop on reals\n\n\nEdmonds-Karp\nBFS augmenting paths\nAll capacities\nO(VE²)\nAlways terminates\n\n\nDinic\nLevel graph + DFS\nAll capacities\nO(V²E)\nFast in practice\n\n\n\n\n\n7. Applications\n\nNetwork routing- Bipartite matching- Task assignment (flows = people → jobs)- Image segmentation (min-cut)- Circulation with demands- Data pipelines, max throughput systems\n\n\n\nTiny Code\nFord-Fulkerson Example:\nadd_edge(1, 2, 3);\nadd_edge(1, 3, 2);\nadd_edge(2, 3, 5);\nadd_edge(2, 4, 2);\nadd_edge(3, 4, 3);\nprintf(\"Max flow = %d\\n\", ford_fulkerson(1, 4, 4)); // 5\n\n\nWhy It Matters\nFlow algorithms transform capacity constraints into solvable systems. They reveal the deep unity between optimization and structure: every maximum flow defines a minimum bottleneck cut.\nThey embody a timeless truth:\n\n“To understand limits, follow the flow.”\n\n\n\nTry It Yourself\n\nImplement Ford-Fulkerson using DFS.\nSwitch to Edmonds-Karp and observe performance gain.\nBuild Dinic’s level graph and visualize layers.\nModel job assignment as bipartite flow.\nVerify Max Flow = Min Cut on small examples.\n\nOnce you master flows, you’ll see them hidden in everything that moves , from data to decisions.\n\n\n\n37. Cuts (Stoer-Wagner, Karger, Gomory-Hu)\nWhere flow problems ask “How much can we send?”, cut problems ask “Where does it break?”\nA cut splits a graph into two disjoint sets. The minimum cut is the smallest set of edges whose removal disconnects the graph , the tightest “bottleneck” holding it together.\nThis chapter explores three major algorithms:\n\nStoer-Wagner , deterministic min-cut for undirected graphs- Karger’s Randomized Algorithm , fast, probabilistic- Gomory-Hu Tree , compress all-pairs min-cuts into one tree Cuts reveal hidden structure , clusters, vulnerabilities, boundaries , and form the dual to flows via the Max-Flow Min-Cut Theorem.\n\n\n1. The Min-Cut Problem\nGiven a weighted undirected graph ( G = (V, E) ): Find the minimum total weight of edges whose removal disconnects the graph.\nEquivalent to:\n\nThe smallest sum of edge weights crossing any partition ( \\(S, V \\setminus S\\) ).\n\nFor directed graphs, you use max-flow methods; For undirected graphs, specialized algorithms exist.\n\n\n2. Applications\n\nNetwork reliability , weakest link detection- Clustering , partition graph by minimal interconnection- Circuit design , splitting components- Image segmentation , separating regions- Community detection , sparse connections between groups\n\n\n\n3. Stoer-Wagner Algorithm (Deterministic)\nA clean, deterministic method for global minimum cut in undirected graphs.\n\n\nA. Idea\n\nStart with the full vertex set ( V ).\nRepeatedly run Maximum Adjacency Search:\n\nStart from a vertex - Grow a set by adding the most tightly connected vertex - The last added vertex defines a cut3. Contract the last two added vertices into one.\n\nKeep track of smallest cut seen.\n\nRepeat until one vertex remains.\n\n\nB. Implementation (Adjacency Matrix)\nconst int INF = 1e9;\nint g[MAX][MAX], w[MAX];\nbool added[MAX], exist[MAX];\n\nint stoer_wagner(int n) {\n    int best = INF;\n    vector&lt;int&gt; v(n);\n    iota(v.begin(), v.end(), 0);\n\n    while (n &gt; 1) {\n        fill(w, w + n, 0);\n        fill(added, added + n, false);\n        int prev = 0;\n        for (int i = 0; i &lt; n; i++) {\n            int sel = -1;\n            for (int j = 0; j &lt; n; j++)\n                if (!added[j] && (sel == -1 || w[j] &gt; w[sel])) sel = j;\n            if (i == n - 1) {\n                best = min(best, w[sel]);\n                for (int j = 0; j &lt; n; j++)\n                    g[prev][j] = g[j][prev] += g[sel][j];\n                v.erase(v.begin() + sel);\n                n--;\n                break;\n            }\n            added[sel] = true;\n            for (int j = 0; j &lt; n; j++) w[j] += g[sel][j];\n            prev = sel;\n        }\n    }\n    return best;\n}\nComplexity: (O\\(V^3\\)), or (O\\(VE + V^2 \\log V\\)) with heaps Input: weighted undirected graph Output: global min cut value\n\n\nC. Example\nGraph:\n1 -3- 2  \n|     |  \n4     2  \n \\-5-/\nCuts:\n\n{1,2}|{3} → 7- {1,3}|{2} → 5 Min cut = 5\n\n\n\n4. Karger’s Algorithm (Randomized)\nA simple, elegant probabilistic method. Repeatedly contract random edges until two vertices remain; the remaining crossing edges form a cut.\nRun multiple times → high probability of finding min cut.\n\n\nA. Algorithm\n\nWhile ( |V| &gt; 2 ):\n\nChoose random edge ((u, v)) - Contract (u, v) into one node - Remove self-loops2. Return number of edges between remaining nodes\n\n\nRepeat (O\\(n^2 \\log n\\)) times for high confidence.\n\n\nB. Implementation Sketch\nstruct Edge { int u, v; };\nvector&lt;Edge&gt; edges;\nint parent[MAX];\n\nint find(int x) { return parent[x] == x ? x : parent[x] = find(parent[x]); }\nvoid unite(int a, int b) { parent[find(b)] = find(a); }\n\nint karger(int n) {\n    int m = edges.size();\n    iota(parent, parent + n, 0);\n    int vertices = n;\n    while (vertices &gt; 2) {\n        int i = rand() % m;\n        int u = find(edges[i].u), v = find(edges[i].v);\n        if (u == v) continue;\n        unite(u, v);\n        vertices--;\n    }\n    int cuts = 0;\n    for (auto e : edges)\n        if (find(e.u) != find(e.v)) cuts++;\n    return cuts;\n}\nExpected Time: (O\\(n^2\\)) per run Probability of success: (2 / (n(n-1))) per run Run multiple trials and take minimum.\n\n\nC. Use Case\nGreat for large sparse graphs, or when approximate solutions are acceptable. Intuitive: the min cut survives random contractions if chosen carefully enough.\n\n\n5. Gomory-Hu Tree\nA compact way to store all-pairs min-cuts. It compresses (O\\(V^2\\)) flow computations into V-1 cuts.\n\n\nA. Idea\n\nBuild a tree where the min cut between any two vertices = the minimum edge weight on their path in the tree.\n\n\n\nB. Algorithm\n\nPick vertex (s).\nFor each vertex \\(t \\neq s\\),\n\nRun max flow to find min cut between (s, t). - Partition vertices accordingly.3. Connect partitions to form a tree.\n\n\nResult: Gomory-Hu tree (V-1 edges).\nNow any pair’s min cut = smallest edge on path between them.\nComplexity: (O(V)) max flow runs.\n\n\nC. Uses\n\nQuickly answer all-pairs cut queries- Network reliability- Hierarchical clustering\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nType\nRandomized\nGraph\nComplexity\nOutput\n\n\n\n\nStoer-Wagner\nDeterministic\nNo\nUndirected\nO(V³)\nGlobal min cut\n\n\nKarger\nRandomized\nYes\nUndirected\nO(n² log n) (multi-run)\nProbabilistic min cut\n\n\nGomory-Hu\nDeterministic\nNo\nUndirected\nO(V × MaxFlow)\nAll-pairs min cuts\n\n\n\n\n\n7. Relationship to Flows\nBy Max-Flow Min-Cut, min-cut capacity = max-flow value.\nSo you can find:\n\ns-t min cut = via max flow- global min cut = min over all (s, t) pairs Specialized algorithms just make it faster.\n\n\n\nTiny Code\nStoer-Wagner Example:\nprintf(\"Global Min Cut = %d\\n\", stoer_wagner(n));\nKarger Multi-Run:\nint ans = INF;\nfor (int i = 0; i &lt; 100; i++)\n    ans = min(ans, karger(n));\nprintf(\"Approx Min Cut = %d\\n\", ans);\n\n\nWhy It Matters\nCuts show you fragility , the weak seams of connection. While flows tell you how much can pass, cuts reveal where it breaks first.\nThey teach:\n\n“To understand strength, study what happens when you pull things apart.”\n\n\n\nTry It Yourself\n\nImplement Stoer-Wagner and test on small graphs.\nRun Karger 100 times and track success rate.\nBuild a Gomory-Hu tree and answer random pair queries.\nVerify Max-Flow = Min-Cut equivalence on examples.\nUse cuts for community detection in social graphs.\n\nMastering cuts gives you both grip and insight , where systems hold, and where they give way.\n\n\n\n38. Matchings (Hopcroft-Karp, Hungarian, Blossom)\nIn many problems, we need to pair up elements efficiently: students to schools, jobs to workers, tasks to machines.\nThese are matching problems , find sets of edges with no shared endpoints that maximize cardinality or weight.\nDepending on graph type, different algorithms apply:\n\nHopcroft-Karp , fast matching in bipartite graphs- Hungarian Algorithm , optimal weighted assignment- Edmonds’ Blossom Algorithm , general graphs (non-bipartite) Matching is a fundamental combinatorial structure, appearing in scheduling, flow networks, and resource allocation.\n\n\n1. Terminology\n\nMatching: set of edges with no shared vertices- Maximum Matching: matching with largest number of edges- Perfect Matching: covers all vertices (each vertex matched once)- Maximum Weight Matching: matching with largest total edge weight Graph Types:\nBipartite: vertices split into two sets (L, R); edges only between sets- General: arbitrary connections (may contain odd cycles)\n\n\n\n2. Applications\n\nJob assignment- Network flows- Resource allocation- Student-project pairing- Stable marriages (with preferences)- Computer vision (feature correspondence)\n\n\n\n3. Hopcroft-Karp Algorithm (Bipartite Matching)\nA highly efficient algorithm for maximum cardinality matching in bipartite graphs.\nIt uses layered BFS + DFS to find multiple augmenting paths simultaneously.\n\n\nA. Idea\n\nInitialize matching empty.\nWhile augmenting paths exist:\n\nBFS builds layer graph (shortest augmenting paths). - DFS finds all augmenting paths along those layers. Each phase increases matching size significantly.\n\n\n\n\nB. Complexity\n\\[\nO(E \\sqrt{V})\n\\]\nMuch faster than augmenting one path at a time (like Ford-Fulkerson).\n\n\nC. Implementation\nLet pairU[u] = matched vertex in R, or 0 if unmatched pairV[v] = matched vertex in L, or 0 if unmatched\nvector&lt;int&gt; adjL[MAX];\nint pairU[MAX], pairV[MAX], dist[MAX];\nint nL, nR;\n\nbool bfs() {\n    queue&lt;int&gt; q;\n    for (int u = 1; u &lt;= nL; u++) {\n        if (!pairU[u]) dist[u] = 0, q.push(u);\n        else dist[u] = INF;\n    }\n    int found = INF;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        if (dist[u] &lt; found) {\n            for (int v : adjL[u]) {\n                if (!pairV[v]) found = dist[u] + 1;\n                else if (dist[pairV[v]] == INF) {\n                    dist[pairV[v]] = dist[u] + 1;\n                    q.push(pairV[v]);\n                }\n            }\n        }\n    }\n    return found != INF;\n}\n\nbool dfs(int u) {\n    for (int v : adjL[u]) {\n        if (!pairV[v] || (dist[pairV[v]] == dist[u] + 1 && dfs(pairV[v]))) {\n            pairU[u] = v;\n            pairV[v] = u;\n            return true;\n        }\n    }\n    dist[u] = INF;\n    return false;\n}\n\nint hopcroft_karp() {\n    int matching = 0;\n    while (bfs()) {\n        for (int u = 1; u &lt;= nL; u++)\n            if (!pairU[u] && dfs(u)) matching++;\n    }\n    return matching;\n}\n\n\nD. Example\nGraph:\nU = {1,2,3}, V = {a,b}\nEdges: 1–a, 2–a, 3–b\nMatching: {1-a, 3-b} (size 2)\n\n\n4. Hungarian Algorithm (Weighted Bipartite Matching)\nSolves assignment problem , given cost matrix \\(c_{ij}\\), assign each (i) to one (j) minimizing total cost (or maximizing profit).\n\n\nA. Idea\nSubtract minimums row- and column-wise → expose zeros → find minimal zero-cover → adjust matrix → repeat.\nEquivalent to solving min-cost perfect matching on a bipartite graph.\n\n\nB. Complexity\n\\[\nO(V^3)\n\\]\nWorks for dense graphs, moderate sizes.\n\n\nC. Implementation Sketch (Matrix Form)\nint hungarian(const vector&lt;vector&lt;int&gt;&gt;& cost) {\n    int n = cost.size();\n    vector&lt;int&gt; u(n+1), v(n+1), p(n+1), way(n+1);\n    for (int i = 1; i &lt;= n; i++) {\n        p[0] = i; int j0 = 0;\n        vector&lt;int&gt; minv(n+1, INF);\n        vector&lt;char&gt; used(n+1, false);\n        do {\n            used[j0] = true;\n            int i0 = p[j0], delta = INF, j1;\n            for (int j = 1; j &lt;= n; j++) if (!used[j]) {\n                int cur = cost[i0-1][j-1] - u[i0] - v[j];\n                if (cur &lt; minv[j]) minv[j] = cur, way[j] = j0;\n                if (minv[j] &lt; delta) delta = minv[j], j1 = j;\n            }\n            for (int j = 0; j &lt;= n; j++)\n                if (used[j]) u[p[j]] += delta, v[j] -= delta;\n                else minv[j] -= delta;\n            j0 = j1;\n        } while (p[j0]);\n        do { int j1 = way[j0]; p[j0] = p[j1]; j0 = j1; } while (j0);\n    }\n    return -v[0]; // minimal cost\n}\n\n\nD. Example\nCost matrix:\n  a  b  c\n1 3  2  1\n2 2  3  2\n3 3  2  3\nOptimal assignment = 1-c, 2-a, 3-b Cost = 1 + 2 + 2 = 5\n\n\n5. Edmonds’ Blossom Algorithm (General Graphs)\nFor non-bipartite graphs, simple augmenting path logic breaks down (odd cycles). Blossom algorithm handles this via contraction of blossoms (odd cycles).\n\n\nA. Idea\n\nFind augmenting paths- When odd cycle encountered (blossom), shrink it into one vertex- Continue search- Expand blossoms at end\n\n\n\nB. Complexity\n\\[\nO(V^3)\n\\]\nThough complex to implement, it’s the general-purpose solution for matchings.\n\n\nC. Use Cases\n\nNon-bipartite job/task assignments- General pairing problems- Network design\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nGraph Type\nWeighted\nComplexity\nOutput\n\n\n\n\nHopcroft-Karp\nBipartite\nNo\nO(E√V)\nMax cardinality\n\n\nHungarian\nBipartite\nYes\nO(V³)\nMin/Max cost matching\n\n\nBlossom\nGeneral\nYes\nO(V³)\nMax cardinality or weight\n\n\n\n\n\n7. Relation to Flows\nBipartite matching = max flow on network:\n\nLeft → Source edges (capacity 1)- Right → Sink edges (capacity 1)- Between sets → edges (capacity 1) Matching size = flow value\n\n\n\nTiny Code\nHopcroft-Karp Demo:\nnL = 3; nR = 2;\nadjL[1] = {1};\nadjL[2] = {1};\nadjL[3] = {2};\nprintf(\"Max Matching = %d\\n\", hopcroft_karp()); // 2\n\n\nWhy It Matters\nMatchings are the language of pairing and assignment. They express cooperation without overlap , a structure of balance.\nThey reveal a deep duality:\n\n“Every match is a flow, every assignment an optimization.”\n\n\n\nTry It Yourself\n\nBuild a bipartite graph and run Hopcroft-Karp.\nSolve an assignment problem with Hungarian algorithm.\nExplore Blossom’s contraction idea conceptually.\nCompare max-flow vs matching approach.\nUse matching to model scheduling (people ↔︎ tasks).\n\nMatching teaches how to pair without conflict, a lesson both mathematical and universal.\n\n\n\n39. Tree Algorithms (LCA, HLD, Centroid Decomposition)\nTrees are the backbone of many algorithms , they are connected, acyclic, and wonderfully structured.\nBecause of their simplicity, they allow elegant divide-and-conquer, dynamic programming, and query techniques. This section covers three fundamental patterns:\n\nLowest Common Ancestor (LCA) , answer ancestor queries fast- Heavy-Light Decomposition (HLD) , break trees into chains for segment trees / path queries- Centroid Decomposition , recursively split tree by balance for divide-and-conquer Each reveals a different way to reason about trees , by depth, by chains, or by balance.\n\n\n1. Lowest Common Ancestor (LCA)\nGiven a tree, two nodes (u, v). The LCA is the lowest node (farthest from root) that is an ancestor of both.\nApplications:\n\nDistance queries- Path decomposition- RMQ / binary lifting- Tree DP and rerooting\n\n\n\nA. Naive Approach\nClimb ancestors until they meet. But this is (O(n)) per query , too slow for many queries.\n\n\nB. Binary Lifting\nPrecompute ancestors at powers of 2. Then jump up by powers to align depths.\nPreprocessing:\n\nDFS to record depth\nup[v][k] = 2^k-th ancestor of v\n\nAnswering query:\n\nLift deeper node up to same depth\nLift both together while up[u][k] != up[v][k]\nReturn parent\n\nCode:\nconst int LOG = 20;\nvector&lt;int&gt; adj[MAX];\nint up[MAX][LOG], depth[MAX];\n\nvoid dfs(int u, int p) {\n    up[u][0] = p;\n    for (int k = 1; k &lt; LOG; k++)\n        up[u][k] = up[up[u][k-1]][k-1];\n    for (int v : adj[u]) if (v != p) {\n        depth[v] = depth[u] + 1;\n        dfs(v, u);\n    }\n}\n\nint lca(int u, int v) {\n    if (depth[u] &lt; depth[v]) swap(u, v);\n    int diff = depth[u] - depth[v];\n    for (int k = 0; k &lt; LOG; k++)\n        if (diff & (1 &lt;&lt; k)) u = up[u][k];\n    if (u == v) return u;\n    for (int k = LOG-1; k &gt;= 0; k--)\n        if (up[u][k] != up[v][k])\n            u = up[u][k], v = up[v][k];\n    return up[u][0];\n}\nComplexity:\n\nPreprocess: (O\\(n \\log n\\))- Query: (O\\(\\log n\\))\n\n\n\nC. Example\nTree:\n    1\n   / \\\n  2   3\n / \\\n4   5\n\nLCA(4,5) = 2- LCA(4,3) = 1\n\n\n\n2. Heavy-Light Decomposition (HLD)\nWhen you need to query paths (sum, max, min, etc.) on trees efficiently, you can use Heavy-Light Decomposition.\n\n\nA. Idea\nDecompose the tree into chains:\n\nHeavy edge = edge to child with largest subtree- Light edges = others Result: Every path from root to leaf crosses at most (O\\(\\log n\\)) light edges.\n\nSo, a path query can be broken into (O\\(\\log^2 n\\)) segment tree queries.\n\n\nB. Steps\n\nDFS to compute subtree sizes and identify heavy child\nDecompose into chains\nAssign IDs for segment tree\nUse Segment Tree / BIT on linearized array\n\nKey functions:\n\ndfs_sz(u) → compute subtree sizes- decompose(u, head) → assign chain heads Code (core):\n\nint parent[MAX], depth[MAX], heavy[MAX], head[MAX], pos[MAX];\nint cur_pos = 0;\n\nint dfs_sz(int u) {\n    int size = 1, max_sz = 0;\n    for (int v : adj[u]) if (v != parent[u]) {\n        parent[v] = u;\n        depth[v] = depth[u] + 1;\n        int sz = dfs_sz(v);\n        if (sz &gt; max_sz) max_sz = sz, heavy[u] = v;\n        size += sz;\n    }\n    return size;\n}\n\nvoid decompose(int u, int h) {\n    head[u] = h;\n    pos[u] = cur_pos++;\n    if (heavy[u] != -1) decompose(heavy[u], h);\n    for (int v : adj[u])\n        if (v != parent[u] && v != heavy[u])\n            decompose(v, v);\n}\nQuery path(u, v):\n\nWhile heads differ, move up chain by chain- Query segment tree in [pos[head[u]], pos[u]]- When in same chain, query segment [pos[v], pos[u]] Complexity:\nBuild: (O(n))- Query/Update: (O\\(\\log^2 n\\))\n\n\n\nC. Use Cases\n\nPath sums- Path maximums- Edge updates- Subtree queries\n\n\n\n3. Centroid Decomposition\nCentroid = node that splits tree into subtrees ≤ n/2 each. By removing centroids recursively, we form a centroid tree.\nUsed for divide-and-conquer on trees.\n\n\nA. Steps\n\nFind centroid\n\nDFS to compute subtree sizes - Choose node where largest subtree ≤ n/22. Decompose:\nRemove centroid - Recurse on subtrees Code (core):\n\n\nint subtree[MAX];\nbool removed[MAX];\nvector&lt;int&gt; adj[MAX];\n\nint dfs_size(int u, int p) {\n    subtree[u] = 1;\n    for (int v : adj[u])\n        if (v != p && !removed[v])\n            subtree[u] += dfs_size(v, u);\n    return subtree[u];\n}\n\nint find_centroid(int u, int p, int n) {\n    for (int v : adj[u])\n        if (v != p && !removed[v])\n            if (subtree[v] &gt; n / 2)\n                return find_centroid(v, u, n);\n    return u;\n}\n\nvoid decompose(int u, int p) {\n    int n = dfs_size(u, -1);\n    int c = find_centroid(u, -1, n);\n    removed[c] = true;\n    // process centroid here\n    for (int v : adj[c])\n        if (!removed[v])\n            decompose(v, c);\n}\nComplexity: (O\\(n \\log n\\))\n\n\nB. Applications\n\nDistance queries (decompose + store distance to centroid)- Tree problems solvable by divide-and-conquer- Dynamic queries (add/remove nodes)\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nQuery\nPreprocess\nComplexity\nNotes\n\n\n\n\nLCA\nAncestor query\n(O\\(\\log n\\))\n(O\\(n \\log n\\))\nFast ancestor lookup\n\n\n\nHLD\nPath queries\n(O\\(\\log^2 n\\))\n(O(n))\nSegment tree-friendly\n\n\n\nCentroid Decomposition\nDivide tree\n-\n(O\\(n \\log n\\))\nBalanced splits\n\n\n\n\n\n\n5. Interconnections\n\nHLD often uses LCA internally.- Centroid decomposition may use distance to ancestor (via LCA).- All exploit tree structure to achieve sublinear queries.\n\n\n\nTiny Code\nLCA(4,5):\ndfs(1,1);\nprintf(\"%d\\n\", lca(4,5)); // 2\nHLD Path Sum: Build segment tree on pos[u] order, query along chains.\nCentroid: decompose(1, -1);\n\n\nWhy It Matters\nTree algorithms show how structure unlocks efficiency. They transform naive traversals into fast, layered, or recursive solutions.\nTo master data structures, you must learn to “climb” and “cut” trees intelligently.\n\n“Every rooted path hides a logarithm.”\n\n\n\nTry It Yourself\n\nImplement binary lifting LCA and test queries.\nAdd segment tree over HLD and run path sums.\nDecompose tree by centroid and count nodes at distance k.\nCombine LCA + HLD for path min/max.\nDraw centroid tree of a simple graph.\n\nMaster these, and trees will stop being “just graphs” , they’ll become tools.\n\n\n\n40. Advanced Graph Algorithms and Tricks\nBy now you’ve seen the big families , traversals, shortest paths, flows, matchings, cuts, and trees. But real-world graphs often bring extra constraints: dynamic updates, multiple sources, layered structures, or special properties (planar, DAG, sparse).\nThis section gathers powerful advanced graph techniques , tricks and patterns that appear across problems once you’ve mastered the basics.\nWe’ll explore:\n\nTopological Sorting & DAG DP- Strongly Connected Components (Condensation Graphs)- Articulation Points & Bridges (2-Edge/Vertex Connectivity)- Eulerian & Hamiltonian Paths- Graph Coloring & Bipartiteness Tests- Cycle Detection & Directed Acyclic Reasoning- Small-to-Large Merging, DSU on Tree, Mo’s Algorithm on Trees- Bitmask DP on Graphs- Dynamic Graphs (Incremental/Decremental BFS/DFS)- Special Graphs (Planar, Sparse, Dense) These aren’t just algorithms , they’re patterns that let you attack harder graph problems with insight.\n\n\n1. Topological Sorting & DAG DP\nIn a DAG (Directed Acyclic Graph), edges always point forward. This makes it possible to order vertices linearly so all edges go from left to right , a topological order.\nUse cases:\n\nTask scheduling- Dependency resolution- DP on DAG (longest/shortest path, counting paths) Algorithm (Kahn’s):\n\nvector&lt;int&gt; topo_sort(int n) {\n    vector&lt;int&gt; indeg(n+1), res;\n    queue&lt;int&gt; q;\n    for (int u = 1; u &lt;= n; u++)\n        for (int v : adj[u]) indeg[v]++;\n    for (int u = 1; u &lt;= n; u++)\n        if (!indeg[u]) q.push(u);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        res.push_back(u);\n        for (int v : adj[u])\n            if (--indeg[v] == 0) q.push(v);\n    }\n    return res;\n}\nDAG DP:\nvector&lt;int&gt; dp(n+1, 0);\nfor (int u : topo_order)\n    for (int v : adj[u])\n        dp[v] = max(dp[v], dp[u] + weight(u,v));\nComplexity: O(V + E)\n\n\n2. Strongly Connected Components (Condensation)\nIn directed graphs, vertices may form SCCs (mutually reachable components). Condensing SCCs yields a DAG, often easier to reason about.\nUse:\n\nComponent compression- Meta-graph reasoning- Cycle condensation Tarjan’s Algorithm: DFS with low-link values, single pass.\n\nKosaraju’s Algorithm: Two passes , DFS on graph and reversed graph.\nComplexity: O(V + E)\nOnce SCCs are built, you can run DP or topological sort on the condensed DAG.\n\n\n3. Articulation Points & Bridges\nFind critical vertices/edges whose removal disconnects the graph.\n\nArticulation point: vertex whose removal increases component count- Bridge: edge whose removal increases component count Algorithm: Tarjan’s DFS Track discovery time tin[u] and lowest reachable ancestor low[u].\n\nvoid dfs(int u, int p) {\n    tin[u] = low[u] = ++timer;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        if (!tin[v]) {\n            dfs(v, u);\n            low[u] = min(low[u], low[v]);\n            if (low[v] &gt; tin[u]) bridge(u, v);\n            if (low[v] &gt;= tin[u] && p != -1) cut_vertex(u);\n        } else low[u] = min(low[u], tin[v]);\n    }\n}\nApplications:\n\nNetwork reliability- Biconnected components- 2-edge/vertex connectivity tests\n\n\n\n4. Eulerian & Hamiltonian Paths\n\nEulerian Path: visits every edge exactly once\n\nExists if graph is connected and 0 or 2 vertices have odd degree- Hamiltonian Path: visits every vertex exactly once (NP-hard) Euler Tour Construction: Hierholzer’s algorithm (O(E))\n\n\nApplications:\n\nRoute reconstruction (e.g., word chains)- Postman problems\n\n\n\n5. Graph Coloring & Bipartiteness\nBipartite Check: DFS/ BFS alternating colors Fails if odd cycle found.\nbool bipartite(int n) {\n    vector&lt;int&gt; color(n+1, -1);\n    for (int i = 1; i &lt;= n; i++) if (color[i] == -1) {\n        queue&lt;int&gt; q; q.push(i); color[i] = 0;\n        while (!q.empty()) {\n            int u = q.front(); q.pop();\n            for (int v : adj[u]) {\n                if (color[v] == -1)\n                    color[v] = color[u] ^ 1, q.push(v);\n                else if (color[v] == color[u])\n                    return false;\n            }\n        }\n    }\n    return true;\n}\nApplications:\n\n2-SAT reduction- Planar graph coloring- Conflict-free assignment\n\n\n\n6. Cycle Detection\n\nDFS + recursion stack for directed graphs- Union-Find for undirected graphs Used to test acyclicity, detect back edges, or find cycles for rollback or consistency checks.\n\n\n\n7. DSU on Tree (Small-to-Large Merging)\nFor queries like “count distinct colors in subtree,” merge results from smaller to larger subtrees to maintain O(n log n).\nPattern:\n\nDFS through children\nKeep large child’s data structure\nMerge small child’s data in\n\nApplications:\n\nOffline subtree queries- Heavy subproblem caching\n\n\n\n8. Mo’s Algorithm on Trees\nOffline algorithm to answer path queries efficiently:\n\nConvert path queries to ranges via Euler Tour- Use Mo’s ordering to process in O((N + Q)√N) Useful when online updates aren’t required.\n\n\n\n9. Bitmask DP on Graphs\nFor small graphs (n ≤ 20): State = subset of vertices e.g., Traveling Salesman Problem (TSP)\ndp[mask][u] = min cost to visit mask, end at u\nTransition:\ndp[mask | (1&lt;&lt;v)][v] = min(dp[mask][u] + cost[u][v])\nComplexity: O(n² 2ⁿ)\n\n\n10. Dynamic Graphs\nGraphs that change:\n\nIncremental BFS: maintain distances as edges added- Decremental connectivity: union-find rollback or dynamic trees Used in online queries, evolving networks, or real-time systems.\n\n\n\n11. Special Graph Classes\n\nPlanar graphs: ≤ 3V-6E; use face counting- Sparse graphs: adjacency lists best- Dense graphs: adjacency matrix / bitset Optimizations often hinge on density.\n\n\n\nTiny Code\nTopological Order:\nauto order = topo_sort(n);\nfor (int u : order) printf(\"%d \", u);\nBridge Check: if (low[v] &gt; tin[u]) edge is a bridge.\nEuler Path Check: Count odd-degree nodes == 0 or 2.\n\n\nWhy It Matters\nThese advanced techniques complete your toolkit. They’re not isolated , they combine to solve real-world puzzles: dependency graphs, robust networks, optimized paths, compressed states.\nThey teach a mindset:\n\n“Graphs are not obstacles , they’re shapes of possibility.”\n\n\n\nTry It Yourself\n\nImplement topological sort and DAG DP.\nFind SCCs and build condensation graph.\nDetect articulation points and bridges.\nCheck Euler path conditions on random graphs.\nTry DSU on tree for subtree statistics.\nSolve TSP via bitmask DP for n ≤ 15.\n\nOnce you can mix and match these tools, you’re no longer just navigating graphs , you’re shaping them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-5.-dynamic-programming",
    "href": "books/en-us/book.html#chapter-5.-dynamic-programming",
    "title": "The Book",
    "section": "Chapter 5. Dynamic Programming",
    "text": "Chapter 5. Dynamic Programming\n\n41. DP Basics and State Transitions\nDynamic Programming (DP) is one of the most powerful ideas in algorithm design. It’s about breaking a big problem into smaller overlapping subproblems, solving each once, and reusing their answers.\nWhen brute force explodes exponentially, DP brings it back under control. This section introduces the mindset, the mechanics, and the math behind DP.\n\n1. The Core Idea\nMany problems have two key properties:\n\nOverlapping subproblems: The same smaller computations repeat many times.\nOptimal substructure: The optimal solution to a problem can be built from optimal solutions to its subproblems.\n\nDP solves each subproblem once, stores the result, and reuses it. This saves exponential time , often reducing ( O\\(2^n\\) ) to ( O\\(n^2\\) ) or ( O(n) ).\n\n\n2. The Recipe\nWhen approaching a DP problem, follow this pattern:\n\nDefine the state. Decide what subproblems you’ll solve. Example: dp[i] = best answer for first i elements.\nWrite the recurrence. Express each state in terms of smaller ones. Example: dp[i] = dp[i-1] + cost(i)\nSet the base cases. Where does the recursion start? Example: dp[0] = 0\nDecide the order. Bottom-up (iterative) or top-down (recursive with memoization).\nReturn the final answer. Often dp[n] or max(dp[i]).\n\n\n\n3. Example: Fibonacci Numbers\nLet’s begin with a classic , the nth Fibonacci number ( F(n) = F(n-1) + F(n-2) ).\nRecursive (slow):\nint fib(int n) {\n    if (n &lt;= 1) return n;\n    return fib(n - 1) + fib(n - 2);\n}\nThis recomputes the same values over and over , exponential time.\nTop-Down DP (Memoization):\nint dp[MAX];\nint fib(int n) {\n    if (n &lt;= 1) return n;\n    if (dp[n] != -1) return dp[n];\n    return dp[n] = fib(n-1) + fib(n-2);\n}\nBottom-Up DP (Tabulation):\nint fib(int n) {\n    int dp[n+1];\n    dp[0] = 0; dp[1] = 1;\n    for (int i = 2; i &lt;= n; i++)\n        dp[i] = dp[i-1] + dp[i-2];\n    return dp[n];\n}\nSpace Optimized:\nint fib(int n) {\n    int a = 0, b = 1, c;\n    for (int i = 2; i &lt;= n; i++) {\n        c = a + b;\n        a = b;\n        b = c;\n    }\n    return b;\n}\n\n\n4. States, Transitions, and Dependencies\nA DP table is a map from states to answers. Each state depends on others via a transition function.\nThink of it like a graph , each edge represents a recurrence relation.\nExample:\n\nState: dp[i] = number of ways to reach step i- Transition: dp[i] = dp[i-1] + dp[i-2] (like stairs)- Base: dp[0] = 1\n\n\n\n5. Common DP Patterns\n\n1D Linear DP\n\nProblems like Fibonacci, climbing stairs, LIS.\n\n2D DP\n\nGrids, sequences, or combinations (LCS, knapsack).\n\nBitmask DP\n\nSubsets, TSP, combinatorial optimization.\n\nDP on Trees\n\nSubtree computations (sum, diameter).\n\nDigit DP\n\nCounting numbers with properties in a range.\n\nSegment DP\n\nMatrix chain multiplication, interval merges.\n\n\n\n\n6. Top-Down vs Bottom-Up\n\n\n\n\n\n\n\n\n\nApproach\nMethod\nPros\nCons\n\n\n\n\nTop-Down\nRecursion + Memoization\nEasy to write, intuitive\nStack overhead, needs memo\n\n\nBottom-Up\nIteration\nFast, space-optimizable\nHarder to derive order\n\n\n\nWhen dependencies are simple and acyclic, bottom-up shines. When they’re complex, top-down is easier.\n\n\n7. Example 2: Climbing Stairs\nYou can climb 1 or 2 steps at a time. How many distinct ways to reach step ( n )?\nState: dp[i] = ways to reach step i Transition: dp[i] = dp[i-1] + dp[i-2] Base: dp[0] = 1, dp[1] = 1\nCode:\nint climb(int n) {\n    int dp[n+1];\n    dp[0] = dp[1] = 1;\n    for (int i = 2; i &lt;= n; i++)\n        dp[i] = dp[i-1] + dp[i-2];\n    return dp[n];\n}\n\n\n8. Debugging DP\nTo debug DP:\n\nPrint intermediate states.- Visualize table (especially 2D).- Check base cases.- Trace one small example by hand.\n\n\n\n9. Complexity\nMost DP algorithms are linear or quadratic in number of states:\n\nTime = (#states) × (work per transition)- Space = (#states) Example: Fibonacci: ( O(n) ) time, ( O(1) ) space Knapsack: ( O\\(n \\times W\\) ) LCS: ( O\\(n \\times m\\) )\n\n\n\nTiny Code\nFibonacci (tabulated):\nint dp[100];\ndp[0] = 0; dp[1] = 1;\nfor (int i = 2; i &lt;= n; i++)\n    dp[i] = dp[i-1] + dp[i-2];\nprintf(\"%d\", dp[n]);\n\n\nWhy It Matters\nDP is the art of remembering. It transforms recursion into iteration, chaos into order.\nFrom optimization to counting, from paths to sequences , once you see substructure, DP becomes your hammer.\n\n“Every repetition hides a recurrence.”\n\n\n\nTry It Yourself\n\nWrite top-down and bottom-up Fibonacci.\nCount ways to climb stairs with steps {1,2,3}.\nCompute number of paths in an n×m grid.\nTry to spot state, recurrence, base in each problem.\nDraw dependency graphs to visualize transitions.\n\nDP isn’t a formula , it’s a mindset: break problems into parts, remember the past, and build from it.\n\n\n\n42. Classic Problems (Knapsack, Subset Sum, Coin Change)\nNow that you know what dynamic programming is, let’s dive into the classic trio , problems that every programmer meets early on:\n\nKnapsack (maximize value under weight constraint)- Subset Sum (can we form a given sum?)- Coin Change (how many ways or fewest coins to reach a total) These are the training grounds of DP: each shows how to define states, transitions, and base cases clearly.\n\n\n1. 0/1 Knapsack Problem\nProblem: You have n items, each with weight w[i] and value v[i]. A knapsack with capacity W. Pick items (each at most once) to maximize total value, without exceeding weight.\n\n\nA. State\ndp[i][w] = max value using first i items with capacity w\n\n\nB. Recurrence\nFor item i:\n\nIf we don’t take it: dp[i-1][w]- If we take it (if w[i] ≤ w): dp[i-1][w - w[i]] + v[i] So, \\[\ndp[i][w] = \\max(dp[i-1][w], dp[i-1][w - w[i]] + v[i])\n\\]\n\n\n\nC. Base Case\ndp[0][w] = 0 for all w (no items = no value)\n\n\nD. Implementation\nint knapsack(int n, int W, int w[], int v[]) {\n    int dp[n+1][W+1];\n    for (int i = 0; i &lt;= n; i++) {\n        for (int j = 0; j &lt;= W; j++) {\n            if (i == 0 || j == 0) dp[i][j] = 0;\n            else if (w[i-1] &lt;= j)\n                dp[i][j] = max(dp[i-1][j], dp[i-1][j - w[i-1]] + v[i-1]);\n            else\n                dp[i][j] = dp[i-1][j];\n        }\n    }\n    return dp[n][W];\n}\nComplexity: Time: (O(nW)) Space: (O(nW)) (can be optimized to 1D (O(W)))\n\n\nE. Space Optimization (1D DP)\nint dp[W+1] = {0};\nfor (int i = 0; i &lt; n; i++)\n    for (int w = W; w &gt;= weight[i]; w--)\n        dp[w] = max(dp[w], dp[w - weight[i]] + value[i]);\n\n\nF. Example\nItems:\nw = [2, 3, 4, 5]\nv = [3, 4, 5, 6]\nW = 5\nBest: take items 1 + 2 → value 7\n\n\n2. Subset Sum\nProblem: Given a set S of integers, can we pick some to sum to target?\n\n\nA. State\ndp[i][sum] = true if we can form sum sum using first i elements.\n\n\nB. Recurrence\n\nDon’t take: dp[i-1][sum]- Take (if a[i] ≤ sum): dp[i-1][sum - a[i]] So, \\[\ndp[i][sum] = dp[i-1][sum] ; || ; dp[i-1][sum - a[i]]\n\\]\n\n\n\nC. Base Case\ndp[0][0] = true (sum 0 possible with no elements) dp[0][sum] = false for sum &gt; 0\n\n\nD. Implementation\nbool subset_sum(int a[], int n, int target) {\n    bool dp[n+1][target+1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = true;\n    for (int j = 1; j &lt;= target; j++) dp[0][j] = false;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= target; j++) {\n            if (a[i-1] &gt; j) dp[i][j] = dp[i-1][j];\n            else dp[i][j] = dp[i-1][j] || dp[i-1][j - a[i-1]];\n        }\n    }\n    return dp[n][target];\n}\nComplexity: Time: (O\\(n \\cdot target\\))\n\n\nE. Example\nS = [3, 34, 4, 12, 5, 2], target = 9 Yes → 4 + 5\n\n\n3. Coin Change\nTwo variants:\n\n\n(a) Count Ways (Unbounded Coins)\n“How many ways to make total T with coins c[]?”\nOrder doesn’t matter.\nState: dp[i][t] = ways using first i coins for total t\nRecurrence:\n\nSkip coin: dp[i-1][t]- Take coin (unlimited): dp[i][t - c[i]] \\[\ndp[i][t] = dp[i-1][t] + dp[i][t - c[i]]\n\\]\n\nBase: dp[0][0] = 1\n1D Simplified:\nint dp[T+1] = {0};\ndp[0] = 1;\nfor (int coin : coins)\n    for (int t = coin; t &lt;= T; t++)\n        dp[t] += dp[t - coin];\n\n\n(b) Min Coins (Fewest Coins to Reach Total)\nState: dp[t] = min coins to reach t\nRecurrence: \\[\ndp[t] = \\min_{c_i \\le t}(dp[t - c_i] + 1)\n\\]\nBase: dp[0] = 0, rest = INF\nint dp[T+1];\nfill(dp, dp+T+1, INF);\ndp[0] = 0;\nfor (int t = 1; t &lt;= T; t++)\n    for (int c : coins)\n        if (t &gt;= c) dp[t] = min(dp[t], dp[t - c] + 1);\n\n\nExample\nCoins = [1,2,5], Total = 5\n\nWays: 4 (5; 2+2+1; 2+1+1+1; 1+1+1+1+1)- Min Coins: 1 (5)\n\n\n\n4. Summary\n\n\n\n\n\n\n\n\n\n\nProblem\nType\nState\nTransition\nComplexity\n\n\n\n\n0/1 Knapsack\nMax value\ndp[i][w]\nmax(take, skip)\nO(nW)\n\n\nSubset Sum\nFeasibility\ndp[i][sum]\nOR of include/exclude\nO(n * sum)\n\n\nCoin Change (ways)\nCounting\ndp[t]\ndp[t] + dp[t - coin]\nO(nT)\n\n\nCoin Change (min)\nOptimization\ndp[t]\nmin(dp[t - coin] + 1)\nO(nT)\n\n\n\n\n\nTiny Code\nMin Coin Change (1D):\nint dp[T+1];\nfill(dp, dp+T+1, INF);\ndp[0] = 0;\nfor (int c : coins)\n    for (int t = c; t &lt;= T; t++)\n        dp[t] = min(dp[t], dp[t - c] + 1);\nprintf(\"%d\\n\", dp[T]);\n\n\nWhy It Matters\nThese three are archetypes:\n\nKnapsack: optimize under constraint- Subset Sum: choose feasibility- Coin Change: count or minimize Once you master them, you can spot their patterns in harder problems , from resource allocation to pathfinding.\n\n\n“Every constraint hides a choice; every choice hides a state.”\n\n\n\nTry It Yourself\n\nImplement 0/1 Knapsack (2D and 1D).\nSolve Subset Sum for target 30 with random list.\nCount coin combinations for amount 10.\nCompare “min coins” vs “ways to form.”\nWrite down state-transition diagram for each.\n\nThese three form your DP foundation , the grammar for building more complex algorithms.\n\n\n\n43. Sequence Problems (LIS, LCS, Edit Distance)\nSequence problems form the heart of dynamic programming. They appear in strings, arrays, genomes, text comparison, and version control. Their power comes from comparing prefixes , building large answers from aligned smaller ones.\nThis section explores three cornerstones:\n\nLIS (Longest Increasing Subsequence)- LCS (Longest Common Subsequence)- Edit Distance (Levenshtein Distance) Each teaches a new way to think about subproblems, transitions, and structure.\n\n\n1. Longest Increasing Subsequence (LIS)\nProblem: Given an array, find the length of the longest subsequence that is strictly increasing.\nA subsequence isn’t necessarily contiguous , you can skip elements.\nExample: [10, 9, 2, 5, 3, 7, 101, 18] → LIS is [2, 3, 7, 18] → length 4\n\n\nA. State\ndp[i] = length of LIS ending at index i\n\n\nB. Recurrence\n\\[\ndp[i] = 1 + \\max_{j &lt; i \\land a[j] &lt; a[i]} dp[j]\n\\]\nIf no smaller a[j], then dp[i] = 1.\n\n\nC. Base\ndp[i] = 1 for all i (each element alone is an LIS)\n\n\nD. Implementation\nint lis(int a[], int n) {\n    int dp[n], best = 0;\n    for (int i = 0; i &lt; n; i++) {\n        dp[i] = 1;\n        for (int j = 0; j &lt; i; j++)\n            if (a[j] &lt; a[i])\n                dp[i] = max(dp[i], dp[j] + 1);\n        best = max(best, dp[i]);\n    }\n    return best;\n}\nComplexity: (O\\(n^2\\))\n\n\nE. Binary Search Optimization\nUse a tail array:\n\ntail[len] = min possible ending value of LIS of length len For each x:\nReplace tail[idx] via lower_bound\n\nint lis_fast(vector&lt;int&gt;& a) {\n    vector&lt;int&gt; tail;\n    for (int x : a) {\n        auto it = lower_bound(tail.begin(), tail.end(), x);\n        if (it == tail.end()) tail.push_back(x);\n        else *it = x;\n    }\n    return tail.size();\n}\nComplexity: (O\\(n \\log n\\))\n\n\n2. Longest Common Subsequence (LCS)\nProblem: Given two strings, find the longest subsequence present in both.\nExample: s1 = \"ABCBDAB\", s2 = \"BDCABA\" LCS = “BCBA” → length 4\n\n\nA. State\ndp[i][j] = LCS length between s1[0..i-1] and s2[0..j-1]\n\n\nB. Recurrence\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1] + 1, & \\text{if } s_1[i-1] = s_2[j-1], \\\\\n\\max(dp[i-1][j],\\, dp[i][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\nC. Base\ndp[0][*] = dp[*][0] = 0 (empty string)\n\n\nD. Implementation\nint lcs(string a, string b) {\n    int n = a.size(), m = b.size();\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++)\n        for (int j = 0; j &lt;= m; j++)\n            if (i == 0 || j == 0) dp[i][j] = 0;\n            else if (a[i-1] == b[j-1])\n                dp[i][j] = dp[i-1][j-1] + 1;\n            else\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1]);\n    return dp[n][m];\n}\nComplexity: (O(nm))\n\n\nE. Reconstruct LCS\nTrace back from dp[n][m]:\n\nIf chars equal → take it and move diagonally- Else move toward larger neighbor\n\n\n\nF. Example\na = “AGGTAB”, b = “GXTXAYB” LCS = “GTAB” → 4\n\n\n3. Edit Distance (Levenshtein Distance)\nProblem: Minimum operations (insert, delete, replace) to convert string a → b.\nExample: kitten → sitting = 3 (replace k→s, insert i, insert g)\n\n\nA. State\ndp[i][j] = min edits to convert a[0..i-1] → b[0..j-1]\n\n\nB. Recurrence\nIf a[i-1] == b[j-1]: \\[\ndp[i][j] = dp[i-1][j-1]\n\\]\nElse: \\[\ndp[i][j] = 1 + \\min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n\\] (Delete, Insert, Replace)\n\n\nC. Base\n\ndp[0][j] = j (insert all)- dp[i][0] = i (delete all)\n\n\n\nD. Implementation\nint edit_distance(string a, string b) {\n    int n = a.size(), m = b.size();\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++)\n        for (int j = 0; j &lt;= m; j++) {\n            if (i == 0) dp[i][j] = j;\n            else if (j == 0) dp[i][j] = i;\n            else if (a[i-1] == b[j-1])\n                dp[i][j] = dp[i-1][j-1];\n            else\n                dp[i][j] = 1 + min({dp[i-1][j], dp[i][j-1], dp[i-1][j-1]});\n        }\n    return dp[n][m];\n}\nComplexity: (O(nm))\n\n\nE. Example\na = “horse”, b = “ros”\n\nreplace h→r, delete r, delete e → 3\n\n\n\n4. Summary\n\n\n\n\n\n\n\n\n\n\nProblem\nType\nState\nTransition\nComplexity\n\n\n\n\nLIS\nSingle seq\ndp[i]\n1 + max(dp[j])\nO(n²) / O(n log n)\n\n\nLCS\nTwo seqs\ndp[i][j]\nif match +1 else max\nO(nm)\n\n\nEdit Distance\nTwo seqs\ndp[i][j]\nif match 0 else 1 + min\nO(nm)\n\n\n\n\n\n5. Common Insights\n\nLIS builds upward , from smaller sequences.- LCS aligns two sequences , compare prefixes.- Edit Distance quantifies difference , minimal edits. They’re templates for bioinformatics, text diffing, version control, and more.\n\n\n\nTiny Code\nLCS:\nif (a[i-1] == b[j-1])\n    dp[i][j] = dp[i-1][j-1] + 1;\nelse\n    dp[i][j] = max(dp[i-1][j], dp[i][j-1]);\n\n\nWhy It Matters\nSequence DPs teach you how to compare progressions , how structure and similarity evolve over time.\nThey transform vague “compare these” tasks into crisp recurrence relations.\n\n“To align is to understand.”\n\n\n\nTry It Yourself\n\nImplement LIS (O(n²) and O(n log n))\nFind LCS of two given strings\nCompute edit distance between “intention” and “execution”\nModify LCS to print one valid subsequence\nTry to unify LCS and Edit Distance in a single table\n\nMaster these, and you can handle any DP on sequences , the DNA of algorithmic thinking.\n\n\n\n44. Matrix and Chain Problems\nDynamic programming shines when a problem involves choices over intervals , which order, which split, which parenthesis. This chapter explores a class of problems built on chains and matrices, where order matters and substructure is defined by intervals.\nWe’ll study:\n\nMatrix Chain Multiplication (MCM) - optimal parenthesization- Polygon Triangulation - divide shape into minimal-cost triangles- Optimal BST / Merge Patterns - weighted merging decisions These problems teach interval DP, where each state represents a segment ([i, j]).\n\n\n1. Matrix Chain Multiplication (MCM)\nProblem: Given matrices \\(A_1, A_2, ..., A_n\\), find the parenthesization that minimizes total scalar multiplications.\nMatrix \\(A_i\\) has dimensions \\(p[i-1] \\times p[i]\\). We can multiply \\(A_i \\cdot A_{i+1}\\) only if inner dimensions match.\nGoal: Minimize operations: \\[\n\\text{cost}(i, j) = \\min_k \\big(\\text{cost}(i, k) + \\text{cost}(k+1, j) + p[i-1] \\cdot p[k] \\cdot p[j]\\big)\n\\]\n\n\nA. State\ndp[i][j] = min multiplications to compute \\(A_i...A_j\\)\n\n\nB. Base\ndp[i][i] = 0 (single matrix needs no multiplication)\n\n\nC. Recurrence\n\\[\ndp[i][j] = \\min_{i \\le k &lt; j} { dp[i][k] + dp[k+1][j] + p[i-1] \\times p[k] \\times p[j] }\n\\]\n\n\nD. Implementation\nint matrix_chain(int p[], int n) {\n    int dp[n][n];\n    for (int i = 1; i &lt; n; i++) dp[i][i] = 0;\n\n    for (int len = 2; len &lt; n; len++) {\n        for (int i = 1; i + len - 1 &lt; n; i++) {\n            int j = i + len - 1;\n            dp[i][j] = INT_MAX;\n            for (int k = i; k &lt; j; k++)\n                dp[i][j] = min(dp[i][j],\n                    dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]);\n        }\n    }\n    return dp[1][n-1];\n}\nComplexity: (O\\(n^3\\)) time, (O\\(n^2\\)) space\n\n\nE. Example\np = [10, 20, 30, 40, 30] Optimal order: ((A1A2)A3)A4 → cost 30000\n\n\n2. Polygon Triangulation\nGiven a convex polygon with n vertices, connect non-intersecting diagonals to minimize total cost. Cost of a triangle = perimeter or product of side weights.\nThis is the same structure as MCM , divide polygon by diagonals.\n\n\nA. State\ndp[i][j] = min triangulation cost for polygon vertices from i to j.\n\n\nB. Recurrence\n\\[\ndp[i][j] = \\min_{i &lt; k &lt; j} (dp[i][k] + dp[k][j] + cost(i, j, k))\n\\]\nBase: dp[i][i+1] = 0 (fewer than 3 points)\n\n\nC. Implementation\ndouble polygon_triangulation(vector&lt;Point&gt; &p) {\n    int n = p.size();\n    double dp[n][n];\n    for (int i = 0; i &lt; n; i++) for (int j = 0; j &lt; n; j++) dp[i][j] = 0;\n    for (int len = 2; len &lt; n; len++) {\n        for (int i = 0; i + len &lt; n; i++) {\n            int j = i + len;\n            dp[i][j] = 1e18;\n            for (int k = i+1; k &lt; j; k++)\n                dp[i][j] = min(dp[i][j],\n                    dp[i][k] + dp[k][j] + dist(p[i],p[k])+dist(p[k],p[j])+dist(p[j],p[i]));\n        }\n    }\n    return dp[0][n-1];\n}\nComplexity: (O\\(n^3\\))\n\n\n3. Optimal Binary Search Tree (OBST)\nGiven sorted keys \\(k_1 &lt; k_2 &lt; \\dots &lt; k_n\\) with search frequencies ( f[i] ), construct a BST with minimal expected search cost.\nThe more frequently accessed nodes should be nearer the root.\n\n\nA. State\ndp[i][j] = min cost to build BST from keys i..j sum[i][j] = sum of frequencies from i to j (precomputed)\n\n\nB. Recurrence\n\\[\ndp[i][j] = \\min_{k=i}^{j} (dp[i][k-1] + dp[k+1][j] + sum[i][j])\n\\]\nEach root adds one to depth of its subtrees → extra cost = sum[i][j]\n\n\nC. Implementation\nint optimal_bst(int freq[], int n) {\n    int dp[n][n], sum[n][n];\n    for (int i = 0; i &lt; n; i++) {\n        dp[i][i] = freq[i];\n        sum[i][i] = freq[i];\n        for (int j = i+1; j &lt; n; j++)\n            sum[i][j] = sum[i][j-1] + freq[j];\n    }\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 0; i+len-1 &lt; n; i++) {\n            int j = i + len - 1;\n            dp[i][j] = INT_MAX;\n            for (int r = i; r &lt;= j; r++) {\n                int left = (r &gt; i) ? dp[i][r-1] : 0;\n                int right = (r &lt; j) ? dp[r+1][j] : 0;\n                dp[i][j] = min(dp[i][j], left + right + sum[i][j]);\n            }\n        }\n    }\n    return dp[0][n-1];\n}\nComplexity: (O\\(n^3\\))\n\n\n4. Merge Pattern Problems\nMany problems , merging files, joining ropes, Huffman coding , involve repeatedly combining elements with minimal total cost.\nAll follow this template: \\[\ndp[i][j] = \\min_{k} (dp[i][k] + dp[k+1][j] + \\text{merge cost})\n\\]\nSame structure as MCM.\n\n\n5. Key Pattern: Interval DP\nState: dp[i][j] = best answer for subarray [i..j] Transition: Try all splits k between i and j\nTemplate:\nfor (len = 2; len &lt;= n; len++)\n for (i = 0; i + len - 1 &lt; n; i++) {\n    j = i + len - 1;\n    dp[i][j] = INF;\n    for (k = i; k &lt; j; k++)\n       dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j,k));\n }\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\nProblem\nState\nRecurrence\nComplexity\n\n\n\n\nMCM\ndp[i][j]\nmin(dp[i][k]+dp[k+1][j]+p[i-1]p[k]p[j])\nO(n³)\n\n\nPolygon Triangulation\ndp[i][j]\nmin(dp[i][k]+dp[k][j]+cost)\nO(n³)\n\n\nOBST\ndp[i][j]\nmin(dp[i][k-1]+dp[k+1][j]+sum[i][j])\nO(n³)\n\n\nMerge Problems\ndp[i][j]\nmin(dp[i][k]+dp[k+1][j]+merge cost)\nO(n³)\n\n\n\n\n\nTiny Code\nMatrix Chain (Compact):\nfor (len = 2; len &lt; n; len++)\n  for (i = 1; i + len - 1 &lt; n; i++) {\n    j = i + len - 1; dp[i][j] = INF;\n    for (k = i; k &lt; j; k++)\n      dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]);\n  }\n\n\nWhy It Matters\nThese problems are DP in 2D , reasoning over intervals and splits. They train your ability to “cut the problem” at every possible point.\n\n“Between every start and end lies a choice of where to divide.”\n\n\n\nTry It Yourself\n\nImplement MCM and print parenthesization.\nSolve polygon triangulation with edge weights.\nBuild OBST for frequencies [34, 8, 50].\nVisualize DP table diagonally.\nGeneralize to merging k segments at a time.\n\nMaster these, and you’ll see interval DP patterns hiding in parsing, merging, and even AI planning.\n\n\n\n45. Bitmask DP and Traveling Salesman\nSome dynamic programming problems require you to track which items have been used, or which subset of elements is active at a given point. This is where Bitmask DP shines. It encodes subsets as binary masks, allowing you to represent state space efficiently.\nThis technique is a must-know for:\n\nTraveling Salesman Problem (TSP)- Subset covering / visiting problems- Permutations and combinations of sets- Game states and toggles\n\n\n1. The Idea of Bitmask DP\nA bitmask is an integer whose binary representation encodes a subset.\nFor ( n ) elements:\n\nThere are \\(2^n\\) subsets.- A subset is represented by a mask from 0 to (1 &lt;&lt; n) - 1. Example for n = 4:\n\n\n\n\nSubset\nMask (binary)\nMask (decimal)\n\n\n\n\n∅\n0000\n0\n\n\n{0}\n0001\n1\n\n\n{1}\n0010\n2\n\n\n{0,1,3}\n1011\n11\n\n\n\nWe can check membership:\n\nmask & (1 &lt;&lt; i) → whether element i is in subset We can add elements:\nmask | (1 &lt;&lt; i) → add element i We can remove elements:\nmask & ~(1 &lt;&lt; i) → remove element i\n\n\n\n2. Example: Traveling Salesman Problem (TSP)\nProblem: Given n cities and cost matrix cost[i][j], find the minimum cost Hamiltonian cycle visiting all cities exactly once and returning to start.\n\n\nA. State\ndp[mask][i] = minimum cost to reach city i having visited subset mask\n\nmask → set of visited cities- i → current city\n\n\n\nB. Base Case\ndp[1&lt;&lt;0][0] = 0 (start at city 0, only 0 visited)\n\n\nC. Transition\nFor each subset mask and city i in mask, try moving from i to j not in mask:\n\\[\ndp[mask \\cup (1 &lt;&lt; j)][j] = \\min \\big(dp[mask \\cup (1 &lt;&lt; j)][j], dp[mask][i] + cost[i][j]\\big)\n\\]\n\n\nD. Implementation\nint tsp(int n, int cost[20][20]) {\n    int N = 1 &lt;&lt; n;\n    const int INF = 1e9;\n    int dp[N][n];\n    for (int m = 0; m &lt; N; m++)\n        for (int i = 0; i &lt; n; i++)\n            dp[m][i] = INF;\n\n    dp[1][0] = 0; // start at city 0\n\n    for (int mask = 1; mask &lt; N; mask++) {\n        for (int i = 0; i &lt; n; i++) {\n            if (!(mask & (1 &lt;&lt; i))) continue;\n            for (int j = 0; j &lt; n; j++) {\n                if (mask & (1 &lt;&lt; j)) continue;\n                int next = mask | (1 &lt;&lt; j);\n                dp[next][j] = min(dp[next][j], dp[mask][i] + cost[i][j]);\n            }\n        }\n    }\n\n    int ans = INF;\n    for (int i = 1; i &lt; n; i++)\n        ans = min(ans, dp[N-1][i] + cost[i][0]);\n    return ans;\n}\nComplexity:\n\nStates: ( O\\(n \\cdot 2^n\\) )- Transitions: ( O(n) )- Total: ( O\\(n^2 \\cdot 2^n\\) )\n\n\n\nE. Example\nn = 4\ncost = {\n {0, 10, 15, 20},\n {10, 0, 35, 25},\n {15, 35, 0, 30},\n {20, 25, 30, 0}\n}\nOptimal path: 0 → 1 → 3 → 2 → 0 Cost = 80\n\n\n3. Other Common Bitmask DP Patterns\n\nSubset Sum / Partition dp[mask] = true if subset represented by mask satisfies property\nCounting Set Bits __builtin_popcount(mask) gives number of elements in subset.\nIterating Over Submasks\n\nfor (int sub = mask; sub; sub = (sub-1) & mask)\n    // handle subset sub\n\nAssigning Tasks (Assignment Problem)\n\n\nEach mask represents set of workers assigned.- State: dp[mask] = min cost for assigned tasks.\n\nfor (mask) for (task)\n if (!(mask & (1 &lt;&lt; task)))\n   dp[mask | (1 &lt;&lt; task)] = min(dp[mask | (1 &lt;&lt; task)],\n        dp[mask] + cost[__builtin_popcount(mask)][task]);\n\n\n4. Memory Tricks\n\nIf only previous masks needed, use rolling arrays:\n\ndp[next][j] = ...\nswap(dp, next_dp)\n\nCompress dimensions: (O\\(2^n\\)) memory for small n\n\n\n\n5. Summary\n\n\n\n\n\n\n\n\n\nProblem\nState\nTransition\nComplexity\n\n\n\n\nTSP\ndp[mask][i]\nmin(dp[mask][i] + cost[i][j])\nO(n²·2ⁿ)\n\n\nAssignment\ndp[mask]\nadd one new element\nO(n²·2ⁿ)\n\n\nSubset Sum\ndp[mask]\nunion of valid subsets\nO(2ⁿ·n)\n\n\n\n\n\nTiny Code\nCore Transition:\nfor (mask)\n  for (i)\n    if (mask & (1&lt;&lt;i))\n      for (j)\n        if (!(mask & (1&lt;&lt;j)))\n          dp[mask|(1&lt;&lt;j)][j] = min(dp[mask|(1&lt;&lt;j)][j], dp[mask][i] + cost[i][j]);\n\n\nWhy It Matters\nBitmask DP is how you enumerate subsets efficiently. It bridges combinatorics and optimization, solving exponential problems with manageable constants.\n\n“Every subset is a story, and bits are its alphabet.”\n\n\n\nTry It Yourself\n\nSolve TSP with 4 cities (hand-trace the table).\nImplement Assignment Problem using bitmask DP.\nCount subsets with even sum.\nUse bitmask DP to find maximum compatible set of tasks.\nExplore how to optimize memory with bit tricks.\n\nBitmask DP unlocks the world of subset-based reasoning , the foundation of combinatorial optimization.\n\n\n\n46. Digit DP and SOS DP\nIn some problems, you don’t iterate over indices or subsets , you iterate over digits or masks to count or optimize over structured states. Two major flavors stand out:\n\nDigit DP - counting numbers with certain properties (e.g. digit sum, constraints)- SOS DP (Sum Over Subsets) - efficiently computing functions over all subsets These are essential techniques when brute force would require enumerating every number or subset, which quickly becomes impossible.\n\n\n1. Digit DP (Counting with Constraints)\nDigit DP is used to count or sum over all numbers ≤ N that satisfy a condition, such as:\n\nThe sum of digits equals a target.- The number doesn’t contain a forbidden digit.- The number has certain parity or divisibility. Instead of iterating over all numbers (up to 10¹⁸!), we iterate digit-by-digit.\n\n\n\nA. State Design\nTypical DP state:\ndp[pos][sum][tight][leading_zero]\n\npos: current digit index (from most significant to least)- sum: property tracker (e.g. sum of digits, remainder)- tight: whether we’re still restricted by N’s prefix- leading_zero: whether we’ve started placing nonzero digits\n\n\n\nB. Transition\nAt each digit position, we choose a digit d:\nlimit = tight ? (digit at pos in N) : 9\nfor (d = 0; d &lt;= limit; d++) {\n    new_tight = tight && (d == limit)\n    new_sum = sum + d\n    // or new_mod = (mod * 10 + d) % M\n}\nTransition accumulates results across valid choices.\n\n\nC. Base Case\nWhen pos == len(N) (end of digits):\n\nReturn 1 if condition holds (e.g. sum == target), else 0\n\n\n\nD. Example: Count numbers ≤ N with digit sum = S\nlong long dp[20][200][2];\n\nlong long solve(string s, int pos, int sum, bool tight) {\n    if (pos == s.size()) return sum == 0;\n    if (sum &lt; 0) return 0;\n    if (dp[pos][sum][tight] != -1) return dp[pos][sum][tight];\n\n    int limit = tight ? (s[pos] - '0') : 9;\n    long long res = 0;\n    for (int d = 0; d &lt;= limit; d++)\n        res += solve(s, pos+1, sum-d, tight && (d==limit));\n\n    return dp[pos][sum][tight] = res;\n}\nUsage:\nstring N = \"12345\";\nint S = 9;\nmemset(dp, -1, sizeof dp);\ncout &lt;&lt; solve(N, 0, S, 1);\nComplexity: O(number of digits × sum × 2) → typically O(20 × 200 × 2)\n\n\nE. Example Variants\n\nCount numbers divisible by 3 → track remainder: new_rem = (rem*10 + d) % 3\nCount numbers without consecutive equal digits → add last_digit to state.\nCount beautiful numbers (like palindromes, no repeated digits) → track bitmask of used digits.\n\n\n\nF. Summary\n\n\n\n\n\n\n\n\n\nProblem\nState\nTransition\nComplexity\n\n\n\n\nSum of digits = S\ndp[pos][sum][tight]\nsum-d\nO(len·S)\n\n\nDivisible by k\ndp[pos][rem][tight]\n(rem*10+d)%k\nO(len·k)\n\n\nNo repeated digits\ndp[pos][mask][tight]\nmask\nO(len·2¹⁰)\n\n\n\n\n\nTiny Code\nfor (int d = 0; d &lt;= limit; d++)\n    res += solve(pos+1, sum-d, tight && (d==limit));\n\n\n2. SOS DP (Sum Over Subsets)\nWhen dealing with functions on subsets, we sometimes need to compute:\n\\[\nf(S) = \\sum_{T \\subseteq S} g(T)\n\\]\nNaively O(3ⁿ). SOS DP reduces it to O(n·2ⁿ).\n\n\nA. Setup\nLet f[mask] = g[mask] initially. For each bit i:\nfor (mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n    if (mask & (1&lt;&lt;i))\n        f[mask] += f[mask^(1&lt;&lt;i)];\nAfter this, f[mask] = sum of g[sub] for all sub ⊆ mask.\n\n\nB. Example\nGiven array a[mask], compute sum[mask] = sum_{sub ⊆ mask} a[sub]\nint n = 3;\nint N = 1 &lt;&lt; n;\nint f[N], a[N];\n// initialize a[]\nfor (int mask = 0; mask &lt; N; mask++) f[mask] = a[mask];\nfor (int i = 0; i &lt; n; i++)\n  for (int mask = 0; mask &lt; N; mask++)\n    if (mask & (1 &lt;&lt; i))\n        f[mask] += f[mask ^ (1 &lt;&lt; i)];\n\n\nC. Why It Works\nEach iteration adds contributions from subsets differing by one bit. By processing all bits, every subset’s contribution propagates upward.\n\n\nD. Variants\n\nSum over supersets: reverse direction.- Max instead of sum: replace += with max=.- XOR convolution: combine values under XOR subset relation.\n\n\n\nE. Applications\n\nInclusion-exclusion acceleration- Precomputing subset statistics- DP over masks with subset transitions\n\n\n\nF. Complexity\n\n\n\nProblem\nNaive\nSOS DP\n\n\n\n\nSubset sum\nO(3ⁿ)\nO(n·2ⁿ)\n\n\nSuperset sum\nO(3ⁿ)\nO(n·2ⁿ)\n\n\n\n\n\nWhy It Matters\nDigit DP teaches counting under constraints , thinking digit by digit. SOS DP teaches subset propagation , spreading information efficiently.\nTogether, they show how to tame exponential state spaces with structure.\n\n“When the search space explodes, symmetry and structure are your compass.”\n\n\n\nTry It Yourself\n\nCount numbers ≤ 10⁹ whose digit sum = 10.\nCount numbers ≤ 10⁶ without repeated digits.\nCompute f[mask] = sum_{sub⊆mask} a[sub] for n=4.\nUse SOS DP to find how many subsets of bits have even sum.\nModify Digit DP to handle leading zeros explicitly.\n\nMaster these, and you can handle structured exponential problems with elegance and speed.\n\n\n\n47. DP Optimizations (Divide & Conquer, Convex Hull Trick, Knuth)\nDynamic Programming often starts with a simple recurrence, but naïve implementations can be too slow (e.g., ( O\\(n^2\\) ) or worse). When the recurrence has special structure , such as monotonicity or convexity , we can exploit it to reduce time complexity drastically.\nThis chapter introduces three powerful optimization families:\n\nDivide and Conquer DP\nConvex Hull Trick (CHT)\nKnuth Optimization\n\nEach one is based on discovering order or geometry hidden inside transitions.\n\n1. Divide and Conquer Optimization\nIf you have a recurrence like: \\[\ndp[i] = \\min_{k &lt; i} { dp[k] + C(k, i) }\n\\]\nand the optimal k for dp[i] ≤ optimal k for dp[i+1], you can use divide & conquer to compute dp in ( O\\(n \\log n\\) ) or ( O\\(n \\log^2 n\\) ).\nThis property is called monotonicity of argmin.\n\n\nA. Conditions\nLet ( C(k, i) ) be the cost to transition from ( k ) to ( i ). Divide and conquer optimization applies if:\n\\[\nopt(i) \\le opt(i+1)\n\\]\nand ( C ) satisfies quadrangle inequality (or similar convex structure).\n\n\nB. Template\nvoid compute(int l, int r, int optL, int optR) {\n    if (l &gt; r) return;\n    int mid = (l + r) / 2;\n    pair&lt;long long,int&gt; best = {INF, -1};\n    for (int k = optL; k &lt;= min(mid, optR); k++) {\n        long long val = dp_prev[k] + cost(k, mid);\n        if (val &lt; best.first) best = {val, k};\n    }\n    dp[mid] = best.first;\n    int opt = best.second;\n    compute(l, mid-1, optL, opt);\n    compute(mid+1, r, opt, optR);\n}\nYou call it as:\ncompute(1, n, 0, n-1);\n\n\nC. Example: Divide Array into K Segments\nGiven array a[1..n], divide into k parts to minimize \\[\ndp[i][k] = \\min_{j &lt; i} dp[j][k-1] + cost(j+1, i)\n\\] If cost satisfies quadrangle inequality, you can optimize each layer in ( O\\(n \\log n\\) ).\n\n\nD. Complexity\nNaive: ( O\\(n^2\\) ) → Optimized: ( O\\(n \\log n\\) )\n\n\n2. Convex Hull Trick (CHT)\nApplies when DP recurrence is linear in i and k: \\[\ndp[i] = \\min_{k &lt; i} (m_k \\cdot x_i + b_k)\n\\]\nwhere:\n\n\\(m_k\\) is slope (depends on k)- ( b_k = dp[k] + c(k) )- \\(x_i\\) is known (monotonic) You can maintain lines \\(y = m_k x + b_k\\) in a convex hull, and query min efficiently.\n\n\n\nA. Conditions\n\nSlopes \\(m_k\\) are monotonic (either increasing or decreasing)- Query points \\(x_i\\) are sorted If both monotonic, we can use pointer walk in O(1) amortized per query. Otherwise, use Li Chao Tree (O(log n)).\n\n\n\nB. Implementation (Monotonic Slopes)\nstruct Line { long long m, b; };\ndeque&lt;Line&gt; hull;\n\nbool bad(Line l1, Line l2, Line l3) {\n    return (l3.b - l1.b)*(l1.m - l2.m) &lt;= (l2.b - l1.b)*(l1.m - l3.m);\n}\n\nvoid add(long long m, long long b) {\n    Line l = {m, b};\n    while (hull.size() &gt;= 2 && bad(hull[hull.size()-2], hull.back(), l))\n        hull.pop_back();\n    hull.push_back(l);\n}\n\nlong long query(long long x) {\n    while (hull.size() &gt;= 2 && \n          hull[0].m*x + hull[0].b &gt;= hull[1].m*x + hull[1].b)\n        hull.pop_front();\n    return hull.front().m*x + hull.front().b;\n}\n\n\nC. Example: DP for Line-Based Recurrence\n\\[\ndp[i] = a_i^2 + \\min_{j &lt; i} (dp[j] + b_j \\cdot a_i)\n\\] Here \\(m_j = b_j\\), \\(x_i = a_i\\), \\(b_j = dp[j]\\)\n\n\nD. Complexity\n\nNaive: ( O\\(n^2\\) )- CHT: ( O(n) ) or ( O\\(n \\log n\\) )\n\n\n\n3. Knuth Optimization\nUsed in interval DP problems (like Matrix Chain, Merging Stones).\nIf:\n\n\\(dp[i][j] = \\min_{k=i}^{j-1} (dp[i][k] + dp[k+1][j] + w(i,j))\\)\nThe cost \\(w(i,j)\\) satisfies the quadrangle inequality: \\[\nw(a,c) + w(b,d) \\le w(a,d) + w(b,c)\n\\]\nAnd the monotonicity condition: \\[\nopt[i][j-1] \\le opt[i][j] \\le opt[i+1][j]\n\\]\n\nThen you can reduce the search space from ( O(n) ) to ( O(1) ) per cell, making total complexity ( O\\(n^2\\) ) instead of ( O\\(n^3\\) ).\n\n\nA. Implementation\nfor (int len = 2; len &lt;= n; len++) {\n    for (int i = 1; i + len - 1 &lt;= n; i++) {\n        int j = i + len - 1;\n        dp[i][j] = INF;\n        for (int k = opt[i][j-1]; k &lt;= opt[i+1][j]; k++) {\n            long long val = dp[i][k] + dp[k+1][j] + cost(i,j);\n            if (val &lt; dp[i][j]) {\n                dp[i][j] = val;\n                opt[i][j] = k;\n            }\n        }\n    }\n}\n\n\nB. Example\nOptimal Binary Search Tree or Merging Stones (with additive cost). Typical improvement: ( O\\(n^3\\) O\\(n^2\\) )\n\n\n4. Summary\n\n\n\n\n\n\n\n\n\nTechnique\nApplies To\nKey Property\nComplexity\n\n\n\n\nDivide & Conquer DP\n1D transitions\nMonotonic argmin\nO(n log n)\n\n\nConvex Hull Trick\nLinear transitions\nMonotonic slopes\nO(n) / O(n log n)\n\n\nKnuth Optimization\nInterval DP\nQuadrangle + Monotonicity\nO(n²)\n\n\n\n\n\nTiny Code\nDivide & Conquer Template\nvoid compute(int l, int r, int optL, int optR);\nCHT Query\nwhile (size&gt;=2 && f[1](x) &lt; f[0](x)) pop_front();\n\n\nWhy It Matters\nThese optimizations show that DP isn’t just brute force with memory , it’s mathematical reasoning on structure.\nOnce you spot monotonicity or linearity, you can shrink a quadratic solution into near-linear time.\n\n“Optimization is the art of seeing simplicity hiding in structure.”\n\n\n\nTry It Yourself\n\nOptimize Matrix Chain DP using Knuth.\nApply Divide & Conquer on dp[i] = min_{k&lt;i}(dp[k]+(i-k)^2).\nSolve Slope DP with CHT for convex cost functions.\nCompare runtime vs naive DP on random data.\nDerive conditions for opt monotonicity in your custom recurrence.\n\nMaster these techniques, and you’ll turn your DPs from slow prototypes into lightning-fast solutions.\n\n\n\n48. Tree DP and Rerooting\nDynamic Programming on trees is one of the most elegant and powerful patterns in algorithm design. Unlike linear arrays or grids, trees form hierarchical structures, where each node depends on its children or parent. Tree DP teaches you how to aggregate results up and down the tree, handling problems where subtrees interact.\nIn this section, we’ll cover:\n\nBasic Tree DP (rooted trees)\nDP over children (bottom-up aggregation)\nRerooting technique (top-down propagation)\nCommon applications and examples\n\n\n1. Basic Tree DP: The Idea\nWe define dp[u] to represent some property of the subtree rooted at u. Then we combine children’s results to compute dp[u].\nThis bottom-up approach is like postorder traversal.\nExample structure:\nfunction dfs(u, parent):\n    dp[u] = base_value\n    for v in adj[u]:\n        if v == parent: continue\n        dfs(v, u)\n        dp[u] = combine(dp[u], dp[v])\n\n\nExample 1: Size of Subtree\nLet dp[u] = number of nodes in subtree rooted at u\nvoid dfs(int u, int p) {\n    dp[u] = 1;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u] += dp[v];\n    }\n}\nKey idea: Combine children’s sizes to get parent size. Complexity: ( O(n) )\n\n\nExample 2: Height of Tree\nLet dp[u] = height of subtree rooted at u\nvoid dfs(int u, int p) {\n    dp[u] = 0;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u] = max(dp[u], 1 + dp[v]);\n    }\n}\nThis gives you the height when rooted at any node.\n\n\n2. DP Over Children (Bottom-Up Aggregation)\nTree DP is all about merging children.\nFor example, if you want the number of ways to color or number of independent sets, you compute children’s dp and merge results at parent.\n\n\nExample 3: Counting Independent Sets\nIn a tree, an independent set is a set of nodes with no two adjacent.\nState:\n\ndp[u][0] = ways if u is not selected- dp[u][1] = ways if u is selected Recurrence: \\[\ndp[u][0] = \\prod_{v \\in children(u)} (dp[v][0] + dp[v][1])\n\\]\n\n\\[\ndp[u][1] = \\prod_{v \\in children(u)} dp[v][0]\n\\]\nImplementation:\nvoid dfs(int u, int p) {\n    dp[u][0] = dp[u][1] = 1;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u][0] *= (dp[v][0] + dp[v][1]);\n        dp[u][1] *= dp[v][0];\n    }\n}\nFinal answer = dp[root][0] + dp[root][1]\n\n\nExample 4: Maximum Path Sum in Tree\nLet dp[u] = max path sum starting at u and going down To find best path anywhere, store a global max over child pairs.\nint ans = 0;\nint dfs(int u, int p) {\n    int best1 = 0, best2 = 0;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        int val = dfs(v, u) + weight(u, v);\n        if (val &gt; best1) swap(best1, val);\n        if (val &gt; best2) best2 = val;\n    }\n    ans = max(ans, best1 + best2);\n    return best1;\n}\nThis gives tree diameter or max path sum.\n\n\n3. Rerooting Technique\nRerooting DP allows you to compute answers for every node as root, without recomputing from scratch ( O\\(n^2\\) ). It’s also known as DP on trees with re-rooting.\n\n\nIdea\n\nFirst, compute dp_down[u] = answer for subtree when rooted at u.\nThen, propagate info from parent to child (dp_up[u]), so each node gets info from outside its subtree.\nCombine dp_down and dp_up to get dp_all[u].\n\n\n\nExample 5: Sum of Distances from Each Node\nLet’s find ans[u] = sum of distances from u to all nodes.\n\nRoot the tree at 0.\nCompute subtree sizes and total distance from root.\nReroot to adjust distances using parent’s info.\n\nStep 1: Bottom-up:\nvoid dfs1(int u, int p) {\n    sz[u] = 1;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs1(v, u);\n        sz[u] += sz[v];\n        dp[u] += dp[v] + sz[v];\n    }\n}\nStep 2: Top-down:\nvoid dfs2(int u, int p) {\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dp[v] = dp[u] - sz[v] + (n - sz[v]);\n        dfs2(v, u);\n    }\n}\nAfter dfs2, dp[u] = sum of distances from node u.\nComplexity: ( O(n) )\n\n\n4. General Rerooting Template\n// 1. Postorder: compute dp_down[u] from children\nvoid dfs_down(u, p):\n    dp_down[u] = base\n    for v in adj[u]:\n        if v != p:\n            dfs_down(v, u)\n            dp_down[u] = merge(dp_down[u], dp_down[v])\n\n// 2. Preorder: use parent's dp_up to compute dp_all[u]\nvoid dfs_up(u, p, dp_up_parent):\n    ans[u] = merge(dp_down[u], dp_up_parent)\n    prefix, suffix = prefix products of children\n    for each child v:\n        dp_up_v = merge(prefix[v-1], suffix[v+1], dp_up_parent)\n        dfs_up(v, u, dp_up_v)\nThis template generalizes rerooting to many problems:\n\nMaximum distance from each node- Number of ways to select subtrees- Sum of subtree sizes seen from each root\n\n\n\n5. Summary\n\n\n\nPattern\nDescription\nComplexity\n\n\n\n\nBasic Tree DP\nCombine child subresults\nO(n)\n\n\nDP Over Children\nEach node depends on children\nO(n)\n\n\nRerooting DP\nCompute result for every root\nO(n)\n\n\nMultiple States\nTrack choices (e.g. include/exclude)\nO(n·state)\n\n\n\n\n\nTiny Code\nSubtree Size\nvoid dfs(int u, int p) {\n    dp[u] = 1;\n    for (int v: adj[u]) if (v != p) {\n        dfs(v,u);\n        dp[u] += dp[v];\n    }\n}\nReroot Sum Distances\ndp[v] = dp[u] - sz[v] + (n - sz[v]);\n\n\nWhy It Matters\nTree DP is how we think recursively over structure , each node’s truth emerges from its children. Rerooting expands this idea globally, giving every node its own perspective.\n\n“In the forest of states, each root sees a different world , yet all follow the same law.”\n\n\n\nTry It Yourself\n\nCount number of nodes in each subtree.\nCompute sum of depths from each node.\nFind tree diameter using DP.\nCount number of independent sets modulo 1e9+7.\nImplement rerooting to find max distance from each node.\n\nTree DP turns recursive patterns into universal strategies for hierarchical data.\n\n\n\n49. DP Reconstruction and Traceback\nSo far, we’ve focused on computing optimal values (min cost, max score, count of ways). But in most real problems, we don’t just want the number , we want to know how we got it.\nThat’s where reconstruction comes in: once you’ve filled your DP table, you can trace back the decisions that led to the optimal answer.\nThis chapter shows you how to:\n\nRecord transitions during DP computation\nReconstruct paths, subsets, or sequences\nHandle multiple reconstructions (paths, sets, parent links)\nUnderstand traceback in 1D, 2D, and graph-based DPs\n\n\n1. The Core Idea\nEach DP state comes from a choice. If you store which choice was best, you can walk backward from the final state to rebuild the solution.\nThink of it as:\ndp[i] = best over options\nchoice[i] = argmin or argmax option\nThen:\nreconstruction_path = []\ni = n\nwhile i &gt; 0:\n    reconstruction_path.push(choice[i])\n    i = choice[i].prev\nYou’re not just solving , you’re remembering the path.\n\n\n2. Reconstruction in 1D DP\n\n\nExample: Coin Change (Minimum Coins)\nProblem: Find minimum number of coins to make value n.\nRecurrence: \\[\ndp[x] = 1 + \\min_{c \\in coins, c \\le x} dp[x-c]\n\\]\nTo reconstruct which coins were used:\nint dp[MAXN], prev_coin[MAXN];\ndp[0] = 0;\nfor (int x = 1; x &lt;= n; x++) {\n    dp[x] = INF;\n    for (int c : coins) {\n        if (x &gt;= c && dp[x-c] + 1 &lt; dp[x]) {\n            dp[x] = dp[x-c] + 1;\n            prev_coin[x] = c;\n        }\n    }\n}\nReconstruction:\nvector&lt;int&gt; used;\nint cur = n;\nwhile (cur &gt; 0) {\n    used.push_back(prev_coin[cur]);\n    cur -= prev_coin[cur];\n}\nOutput: coins used in one optimal solution.\n\n\nExample: LIS Reconstruction\nYou know how to find LIS length. Now reconstruct the sequence.\nint dp[n], prev[n];\nint best_end = 0;\nfor (int i = 0; i &lt; n; i++) {\n    dp[i] = 1; prev[i] = -1;\n    for (int j = 0; j &lt; i; j++)\n        if (a[j] &lt; a[i] && dp[j] + 1 &gt; dp[i]) {\n            dp[i] = dp[j] + 1;\n            prev[i] = j;\n        }\n    if (dp[i] &gt; dp[best_end]) best_end = i;\n}\nRebuild LIS:\nvector&lt;int&gt; lis;\nfor (int i = best_end; i != -1; i = prev[i])\n    lis.push_back(a[i]);\nreverse(lis.begin(), lis.end());\n\n\n3. Reconstruction in 2D DP\n\n\nExample: LCS (Longest Common Subsequence)\nWe have dp[i][j] filled using:\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1] + 1, & \\text{if } a[i-1] = b[j-1], \\\\\n\\max(dp[i-1][j], dp[i][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\nTo reconstruct LCS:\nint i = n, j = m;\nstring lcs = \"\";\nwhile (i &gt; 0 && j &gt; 0) {\n    if (a[i-1] == b[j-1]) {\n        lcs.push_back(a[i-1]);\n        i--; j--;\n    }\n    else if (dp[i-1][j] &gt; dp[i][j-1]) i--;\n    else j--;\n}\nreverse(lcs.begin(), lcs.end());\nOutput: one valid LCS string.\n\n\nExample: Edit Distance\nOperations: insert, delete, replace.\nYou can store the operation:\nif (a[i-1] == b[j-1]) op[i][j] = \"match\";\nelse if (dp[i][j] == dp[i-1][j-1] + 1) op[i][j] = \"replace\";\nelse if (dp[i][j] == dp[i-1][j] + 1) op[i][j] = \"delete\";\nelse op[i][j] = \"insert\";\nThen backtrack to list operations:\nwhile (i &gt; 0 || j &gt; 0) {\n    if (op[i][j] == \"match\") i--, j--;\n    else if (op[i][j] == \"replace\") { print(\"Replace\"); i--; j--; }\n    else if (op[i][j] == \"delete\") { print(\"Delete\"); i--; }\n    else { print(\"Insert\"); j--; }\n}\n\n\n4. Reconstruction in Path Problems\nWhen DP tracks shortest paths, you can keep parent pointers.\n\n\nExample: Bellman-Ford Path Reconstruction\nint dist[n], parent[n];\ndist[src] = 0;\nfor (int k = 0; k &lt; n-1; k++)\n  for (auto [u,v,w] : edges)\n    if (dist[u] + w &lt; dist[v]) {\n        dist[v] = dist[u] + w;\n        parent[v] = u;\n    }\n\nvector&lt;int&gt; path;\nfor (int v = dest; v != src; v = parent[v])\n    path.push_back(v);\npath.push_back(src);\nreverse(path.begin(), path.end());\nYou now have the actual shortest path.\n\n\n5. Handling Multiple Solutions\nSometimes multiple optimal paths exist. You can:\n\nStore all predecessors instead of one- Backtrack recursively to enumerate all solutions- Tie-break deterministically (e.g., lexicographically smallest) Example:\n\nif (new_val == dp[i]) parents[i].push_back(j);\nThen recursively generate all possible paths.\n\n\n6. Visualization\nDP reconstruction often looks like following arrows in a grid or graph:\n\nLCS: diagonal (↖), up (↑), left (←)- Shortest path: parent edges- LIS: predecessor chain You’re walking through decisions, not just numbers.\n\n\n\n7. Summary\n\n\n\nType\nState\nReconstruction\n\n\n\n\n1D DP\nprev[i]\nTrace chain\n\n\n2D DP\nop[i][j]\nFollow choices\n\n\nGraph DP\nparent[v]\nFollow edges\n\n\nCounting DP\noptional\nRecover counts / paths\n\n\n\n\n\nTiny Code\nGeneral pattern:\nfor (state)\n  for (choice)\n    if (better) {\n        dp[state] = value;\n        parent[state] = choice;\n    }\nThen:\nwhile (state != base) {\n    path.push_back(parent[state]);\n    state = parent[state];\n}\n\n\nWhy It Matters\nSolving DP gets you the score , reconstructing shows you the story. It’s the difference between knowing the answer and understanding the reasoning.\n\n“Numbers tell you the outcome; pointers tell you the path.”\n\n\n\nTry It Yourself\n\nReconstruct one LIS path.\nPrint all LCSs for small strings.\nShow edit operations to transform “cat” → “cut”.\nTrack subset used in Knapsack to reach exact weight.\nRecover optimal merge order in Matrix Chain DP.\n\nReconstruction turns DP from a static table into a narrative of decisions , a map back through the maze of optimal choices.\n\n\n\n50. Meta-DP and Optimization Templates\nWe’ve now explored many flavors of dynamic programming , on sequences, grids, trees, graphs, subsets, and digits. This final chapter in the DP arc zooms out to the meta-level: how to see DP patterns, generalize them, and turn them into reusable templates.\nIf classical DP is about solving one problem, meta-DP is about recognizing families of problems that share structure. You’ll learn how to build your own DP frameworks, use common templates, and reason from first principles.\n\n1. What Is Meta-DP?\nA Meta-DP is a high-level abstraction of a dynamic programming pattern. It encodes:\n\nState definition pattern- Transition pattern- Optimization structure- Dimensional dependencies By learning these patterns, you can design DPs faster, reuse logic across problems, and spot optimizations early.\n\nThink of Meta-DP as:\n\n“Instead of memorizing 100 DPs, master 10 DP blueprints.”\n\n\n\n2. The Four Building Blocks\nEvery DP has the same core ingredients:\n\nState: what subproblem you’re solving\n\nOften dp[i], dp[i][j], or dp[mask] - Represents smallest unit of progress2. Transition: how to build larger subproblems from smaller ones\nE.g. dp[i] = min(dp[j] + cost(j, i))3. Base Case: known trivial answers\nE.g. dp[0] = 04. Order: how to fill the states\nE.g. increasing i, decreasing i, or topological order Once you can describe a problem in these four, it is a DP.\n\n\n\n\n3. Meta-Templates for Common Structures\nBelow are generalized templates to use and adapt.\n\n\nA. Line DP (1D Sequential)\nShape: linear progression Examples:\n\nFibonacci- Knapsack (1D capacity)- LIS (sequential dependency)\n\nfor (int i = 1; i &lt;= n; i++) {\n    dp[i] = base;\n    for (int j : transitions(i))\n        dp[i] = min(dp[i], dp[j] + cost(j, i));\n}\nVisualization: → → → Each state depends on previous positions.\n\n\nB. Grid DP (2D Spatial)\nShape: grid or matrix Examples:\n\nPaths in a grid- Edit Distance- Counting paths with obstacles\n\nfor (i = 0; i &lt; n; i++)\n  for (j = 0; j &lt; m; j++)\n    dp[i][j] = combine(dp[i-1][j], dp[i][j-1]);\nVisualization: ⬇️ ⬇️ ➡️ Moves from top-left to bottom-right.\n\n\nC. Interval DP\nShape: segments or subarrays Examples:\n\nMatrix Chain Multiplication- Optimal BST- Merging Stones\n\nfor (len = 2; len &lt;= n; len++)\n  for (i = 0; i + len - 1 &lt; n; i++) {\n      j = i + len - 1;\n      dp[i][j] = INF;\n      for (k = i; k &lt; j; k++)\n          dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j));\n  }\nKey Insight: overlapping intervals, split points.\n\n\nD. Subset DP\nShape: subsets of a set Examples:\n\nTraveling Salesman (TSP)- Assignment problem- SOS DP\n\nfor (mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n  for (sub = mask; sub; sub = (sub-1)&mask)\n      dp[mask] = combine(dp[mask], dp[sub]);\nKey Insight: use bitmasks to represent subsets.\n\n\nE. Tree DP\nShape: hierarchical dependencies Examples:\n\nSubtree sizes- Independent sets- Rerooting\n\nvoid dfs(u, p):\n  dp[u] = base\n  for (v in children)\n    if (v != p)\n      dfs(v, u)\n      dp[u] = merge(dp[u], dp[v])\n\n\nF. Graph DP (Topological Order)\nShape: DAG structure Examples:\n\nLongest path in DAG- Counting paths- DAG shortest path\n\nfor (u in topo_order)\n  for (v in adj[u])\n    dp[v] = combine(dp[v], dp[u] + weight(u,v));\nKey: process nodes in topological order.\n\n\nG. Digit DP\nShape: positional digits, constrained transitions Examples:\n\nCount numbers satisfying digit conditions- Divisibility / digit sum problems\n\ndp[pos][sum][tight] = ∑ dp[next_pos][new_sum][new_tight];\n\n\nH. Knuth / Divide & Conquer / Convex Hull Trick\nShape: optimization over monotone or convex transitions Examples:\n\nCost-based splits- Line-based transitions\n\ndp[i] = min_k (dp[k] + cost(k, i))\nKey: structure in opt[i] or slope.\n\n\n4. Recognizing DP Type\nAsk these diagnostic questions:\n\n\n\n\n\n\n\nQuestion\nClue\n\n\n\n\n“Does each step depend on smaller subproblems?”\nDP\n\n\n“Do I split a segment?”\nInterval DP\n\n\n“Do I choose subsets?”\nSubset / Bitmask DP\n\n\n“Do I move along positions?”\nLine DP\n\n\n“Do I merge children?”\nTree DP\n\n\n“Do I process in a DAG?”\nGraph DP\n\n\n“Do I track digits or constraints?”\nDigit DP\n\n\n\n\n\n5. Optimization Layer\nOnce you have a working DP, ask:\n\nCan transitions be reduced (monotonicity)?- Can overlapping cost be cached (prefix sums)?- Can dimensions be compressed (rolling arrays)?- Can you reuse solutions for each segment (Divide & Conquer / Knuth)? This transforms your DP from conceptual to efficient.\n\n\n\n6. Meta-DP: Transformations\n\nCompress dimensions: if only dp[i-1] needed, use 1D array.- Invert loops: bottom-up ↔︎ top-down.- Change base: prefix-sums for range queries.- State lifting: add dimension for new property (like remainder, parity, bitmask). &gt; “When stuck, add a dimension. When slow, remove one.”\n\n\n\n7. Common Template Snippets\nRolling 1D Knapsack:\nfor (c = C; c &gt;= w[i]; c--)\n  dp[c] = max(dp[c], dp[c-w[i]] + val[i]);\nTop-Down Memoization:\nint solve(state):\n  if (visited[state]) return dp[state];\n  dp[state] = combine(solve(next_states));\nIterative DP:\nfor (state in order)\n  dp[state] = merge(prev_states);\n\n\n8. Building Your Own DP Framework\nYou can design a generic DP(state, transition) class:\nstruct DP {\n    vector&lt;long long&gt; dp;\n    function&lt;long long(int,int)&gt; cost;\n    DP(int n, auto cost): dp(n, INF), cost(cost) {}\n    void solve() { for (int i=1; i&lt;n; i++) for (int j=0; j&lt;i; j++) \n         dp[i] = min(dp[i], dp[j] + cost(j, i)); }\n};\nReusable, readable, flexible.\n\n\n9. Summary\n\n\n\nDP Type\nCore State\nShape\nTypical Complexity\n\n\n\n\nLine DP\ndp[i]\nLinear\nO(n²) → O(n)\n\n\nGrid DP\ndp[i][j]\nMatrix\nO(n·m)\n\n\nInterval DP\ndp[i][j]\nTriangular\nO(n³)\n\n\nSubset DP\ndp[mask]\nExponential\nO(n·2ⁿ)\n\n\nTree DP\ndp[u]\nTree\nO(n)\n\n\nDigit DP\ndp[pos][sum]\nRecursive\nO(len·sum)\n\n\nGraph DP\ndp[v]\nDAG\nO(V+E)\n\n\n\n\n\nTiny Code\nfor (state in order)\n  dp[state] = combine(all_prev(state));\n\n\nWhy It Matters\nMeta-DP turns your thinking from case-by-case to pattern-by-pattern. You stop memorizing formulas and start seeing shapes: lines, grids, intervals, trees, masks.\nOnce you can name the shape, you can write the DP.\n\n“DP is not about filling tables. It’s about recognizing structure.”\n\n\n\nTry It Yourself\n\nClassify each classic DP problem into a type.\nWrite one template per pattern (Line, Grid, Tree, etc.).\nCreate a dp_solve(state, transitions) function to generalize logic.\nFor each pattern, practice a small example.\nBuild your own “Little Book of DP Patterns” with code snippets.\n\nThis is your bridge from concrete solutions to algorithmic fluency , the foundation for mastering the next 950 algorithms ahead.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-6.-strings-and-text-algorithms",
    "href": "books/en-us/book.html#chapter-6.-strings-and-text-algorithms",
    "title": "The Book",
    "section": "Chapter 6. Strings and Text Algorithms",
    "text": "Chapter 6. Strings and Text Algorithms\n\n51. Number Theory (GCD, Modular Arithmetic, CRT)\nNumber theory forms the mathematical backbone of many algorithms , from hashing and cryptography to modular combinatorics and primality testing. In algorithmic problem-solving, it’s all about working with integers, divisibility, and modular systems efficiently.\nThis section covers the essential toolkit:\n\nGCD and Extended Euclidean Algorithm- Modular Arithmetic (addition, subtraction, multiplication, inverse)- Modular Exponentiation- Chinese Remainder Theorem (CRT)\n\n\n1. The Greatest Common Divisor (GCD)\nThe GCD of two integers \\(a\\) and \\(b\\), denoted \\(\\gcd(a, b)\\), is the largest integer that divides both. It’s the cornerstone for fraction simplification, Diophantine equations, and modular inverses.\n\n\nA. Euclidean Algorithm\nBased on: \\[\n\\gcd(a, b) = \\gcd(b, a \\bmod b)\n\\]\nint gcd(int a, int b) {\n    return b == 0 ? a : gcd(b, a % b);\n}\nTime complexity: \\(O(\\log \\min(a,b))\\)\n\n\nB. Extended Euclidean Algorithm\nFinds integers ( x, y ) such that: \\[\nax + by = \\gcd(a, b)\n\\]\nThis is critical for finding modular inverses.\nint ext_gcd(int a, int b, int &x, int &y) {\n    if (b == 0) {\n        x = 1; y = 0;\n        return a;\n    }\n    int x1, y1;\n    int g = ext_gcd(b, a % b, x1, y1);\n    x = y1;\n    y = x1 - (a / b) * y1;\n    return g;\n}\n\n\nC. Bezout’s Identity\nIf \\(g = \\gcd(a,b)\\), then \\(ax + by = g\\) has integer solutions. If \\(g = 1\\), \\(x\\) is the modular inverse of \\(a modulo b\\).\n\n\n2. Modular Arithmetic\nA modular system “wraps around” after a certain value ( m ).\nWe write: \\[\na \\equiv b \\pmod{m} \\quad \\text{if } m \\mid (a - b)\n\\]\nIt behaves like ordinary arithmetic, with the rule:\n\n\\((a + b) \\bmod m = ((a \\bmod m) + (b \\bmod m)) \\bmod m\\)\n\\((a \\cdot b) \\bmod m = ((a \\bmod m) \\cdot (b \\bmod m)) \\bmod m\\)\n\\((a - b) \\bmod m = ((a \\bmod m) - (b \\bmod m) + m) \\bmod m\\)\n\n\n\nA. Modular Exponentiation\nCompute \\(a^b \\bmod m\\) efficiently using binary exponentiation.\nlong long modpow(long long a, long long b, long long m) {\n    long long res = 1;\n    a %= m;\n    while (b &gt; 0) {\n        if (b & 1) res = (res * a) % m;\n        a = (a * a) % m;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\nComplexity: ( O\\(\\log b\\) )\n\n\nB. Modular Inverse\nGiven ( a ), find \\(a^{-1}\\) such that: \\[\na \\cdot a^{-1} \\equiv 1 \\pmod{m}\n\\]\nCase 1: If ( m ) is prime, use Fermat’s Little Theorem: \\[\na^{-1} \\equiv a^{m-2} \\pmod{m}\n\\]\nint modinv(int a, int m) {\n    return modpow(a, m-2, m);\n}\nCase 2: If ( a ) and ( m ) are coprime, use Extended GCD:\nint inv(int a, int m) {\n    int x, y;\n    int g = ext_gcd(a, m, x, y);\n    if (g != 1) return -1; // no inverse\n    return (x % m + m) % m;\n}\n\n\nC. Modular Division\nTo divide \\(a / b \\bmod m\\): \\[\na / b \\equiv a \\cdot b^{-1} \\pmod{m}\n\\]\nSo compute the inverse and multiply.\n\n\n3. Chinese Remainder Theorem (CRT)\nThe CRT solves systems of congruences: \\[\nx \\equiv a_1 \\pmod{m_1}\n\\]\n\\[\nx \\equiv a_2 \\pmod{m_2}\n\\] If moduli \\(m_1, m_2, \\dots, m_k\\) are pairwise coprime, there exists a unique solution modulo \\(M = m_1 m_2 \\dots m_k\\).\n\n\nA. 2-Equation Example\nSolve: \\[\nx \\equiv a_1 \\pmod{m_1}, \\quad x \\equiv a_2 \\pmod{m_2}\n\\]\nLet:\n\n\\(M = m_1 m_2\\)- \\(M_1 = M / m_1\\)- \\(M_2 = M / m_2\\) Find inverses \\(inv_1 = M_1^{-1} \\bmod m_1\\), \\(inv_2 = M_2^{-1} \\bmod m_2\\)\n\nThen: \\[\nx = (a_1 \\cdot M_1 \\cdot inv_1 + a_2 \\cdot M_2 \\cdot inv_2) \\bmod M\n\\]\n\n\nB. Implementation\nlong long crt(vector&lt;int&gt; a, vector&lt;int&gt; m) {\n    long long M = 1;\n    for (int mod : m) M *= mod;\n    long long res = 0;\n    for (int i = 0; i &lt; a.size(); i++) {\n        long long Mi = M / m[i];\n        long long inv = modinv(Mi % m[i], m[i]);\n        res = (res + 1LL * a[i] * Mi % M * inv % M) % M;\n    }\n    return (res % M + M) % M;\n}\n\n\nC. Example\nSolve:\nx ≡ 2 (mod 3)\nx ≡ 3 (mod 5)\nx ≡ 2 (mod 7)\nSolution: ( x = 23 ) (mod 105)\nCheck:\n\n( 23 % 3 = 2 )- ( 23 % 5 = 3 )- ( 23 % 7 = 2 )\n\n\n\n4. Tiny Code\nGCD\nint gcd(int a, int b) { return b ? gcd(b, a % b) : a; }\nModular Power\nmodpow(a, b, m)\nModular Inverse\nmodinv(a, m)\nCRT\ncrt(a[], m[])\n\n\n5. Summary\n\n\n\n\n\n\n\n\nConcept\nFormula\nPurpose\n\n\n\n\nGCD\n\\(\\gcd(a,b) = \\gcd(b, a \\bmod b)\\)\nSimplify ratios\n\n\nExtended GCD\n\\(ax + by = \\gcd(a,b)\\)\nFind modular inverse\n\n\nModular Inverse\n\\(a^{-1} \\equiv a^{m-2} \\pmod{m}\\)\nSolve modular equations\n\n\nModular Exp\n\\(a^b \\bmod m\\)\nFast exponentiation\n\n\nCRT\nCombine congruences\nMulti-mod system\n\n\n\n\n\nWhy It Matters\nNumber theory lets algorithms speak the language of integers , turning huge computations into modular games. From hashing to RSA, from combinatorics to cryptography, it’s everywhere.\n\n“When numbers wrap around, math becomes modular , and algorithms become magical.”\n\n\n\nTry It Yourself\n\nCompute gcd(48, 180).\nFind inverse of 7 mod 13.\nSolve \\(x ≡ 1 \\pmod{2}, x ≡ 2 \\pmod{3}, x ≡ 3 \\pmod{5}\\).\nImplement modular division \\(a / b \\bmod m\\).\nUse modpow to compute \\(3^{200} \\bmod 13\\).\n\nThese basics unlock higher algorithms in cryptography, combinatorics, and beyond.\n\n\n\n52. Primality and Factorization (Miller-Rabin, Pollard Rho)\nPrimality and factorization are core to number theory, cryptography, and competitive programming. Many modern systems (RSA, ECC) rely on the hardness of factoring large numbers. Here, we learn how to test if a number is prime and break it into factors efficiently.\nWe’ll cover:\n\nTrial Division\nSieve of Eratosthenes (for precomputation)\nProbabilistic Primality Test (Miller-Rabin)\nInteger Factorization (Pollard Rho)\n\n\n1. Trial Division\nThe simplest way to test primality is by dividing by all integers up to √n.\nbool is_prime(long long n) {\n    if (n &lt; 2) return false;\n    if (n % 2 == 0) return n == 2;\n    for (long long i = 3; i * i &lt;= n; i += 2)\n        if (n % i == 0) return false;\n    return true;\n}\nTime Complexity: ( O\\(\\sqrt{n}\\) ) Good for \\(n \\le 10^6\\), impractical for large ( n ).\n\n\n2. Sieve of Eratosthenes\nFor checking many numbers at once, use a sieve.\nIdea: Mark all multiples of each prime starting from 2.\nvector&lt;bool&gt; sieve(int n) {\n    vector&lt;bool&gt; is_prime(n+1, true);\n    is_prime[0] = is_prime[1] = false;\n    for (int i = 2; i * i &lt;= n; i++)\n        if (is_prime[i])\n            for (int j = i * i; j &lt;= n; j += i)\n                is_prime[j] = false;\n    return is_prime;\n}\nTime Complexity: ( O\\(n \\log \\log n\\) )\nUseful for generating primes up to \\(10^7\\).\n\n\n3. Modular Multiplication\nBefore we do probabilistic tests or factorization, we need safe modular multiplication for large numbers.\nlong long modmul(long long a, long long b, long long m) {\n    __int128 res = (__int128)a * b % m;\n    return (long long)res;\n}\nAvoid overflow for \\(n \\approx 10^{18}\\).\n\n\n4. Miller-Rabin Primality Test\nA probabilistic test that can check if ( n ) is prime or composite in ( O\\(k \\log^3 n\\) ).\nIdea: For a prime ( n ): \\[\na^{n-1} \\equiv 1 \\pmod{n}\n\\] But for composites, most ( a ) fail this.\nWe write \\(n - 1 = 2^s \\cdot d\\), ( d ) odd.\nFor each base ( a ):\n\nCompute \\(x = a^d \\bmod n\\)- If ( x = 1 ) or ( x = n - 1 ), pass- Else, square ( s-1 ) times- If none equal ( n - 1 ), composite\n\nbool miller_rabin(long long n) {\n    if (n &lt; 2) return false;\n    for (long long p : {2,3,5,7,11,13,17,19,23,29,31,37})\n        if (n % p == 0) return n == p;\n    long long d = n - 1, s = 0;\n    while ((d & 1) == 0) d &gt;&gt;= 1, s++;\n    auto modpow = [&](long long a, long long b) {\n        long long r = 1;\n        while (b) {\n            if (b & 1) r = modmul(r, a, n);\n            a = modmul(a, a, n);\n            b &gt;&gt;= 1;\n        }\n        return r;\n    };\n    for (long long a : {2, 325, 9375, 28178, 450775, 9780504, 1795265022}) {\n        if (a % n == 0) continue;\n        long long x = modpow(a, d);\n        if (x == 1 || x == n - 1) continue;\n        bool composite = true;\n        for (int r = 1; r &lt; s; r++) {\n            x = modmul(x, x, n);\n            if (x == n - 1) {\n                composite = false;\n                break;\n            }\n        }\n        if (composite) return false;\n    }\n    return true;\n}\nDeterministic for:\n\n\\(n &lt; 2^{64}\\) with bases above. Complexity: ( O\\(k \\log^3 n\\) )\n\n\n\n5. Pollard Rho Factorization\nEfficient for finding nontrivial factors of large composites. Based on Floyd’s cycle detection (Tortoise and Hare).\nIdea: Define a pseudo-random function: \\[\nf(x) = (x^2 + c) \\bmod n\n\\] Then find \\(\\gcd(|x - y|, n)\\) where \\(x, y\\) move at different speeds.\nlong long pollard_rho(long long n) {\n    if (n % 2 == 0) return 2;\n    auto f = [&](long long x, long long c) {\n        return (modmul(x, x, n) + c) % n;\n    };\n    while (true) {\n        long long x = rand() % (n - 2) + 2;\n        long long y = x;\n        long long c = rand() % (n - 1) + 1;\n        long long d = 1;\n        while (d == 1) {\n            x = f(x, c);\n            y = f(f(y, c), c);\n            d = gcd(abs(x - y), n);\n        }\n        if (d != n) return d;\n    }\n}\nUse:\n\nCheck if ( n ) is prime (Miller-Rabin)\nIf not, find a factor using Pollard Rho\nRecurse on factors\n\nComplexity: ~ ( O\\(n^{1/4}\\) ) average\n\n\n6. Example\nFactorize ( n = 8051 ):\n\nMiller-Rabin → composite\nPollard Rho → factor 83\n( 8051 / 83 = 97 )\nBoth primes ⇒ ( 8051 = 83 × 97 )\n\n\n\n7. Tiny Code\nvoid factor(long long n, vector&lt;long long&gt; &f) {\n    if (n == 1) return;\n    if (miller_rabin(n)) {\n        f.push_back(n);\n        return;\n    }\n    long long d = pollard_rho(n);\n    factor(d, f);\n    factor(n / d, f);\n}\nCall factor(n, f) to get prime factors.\n\n\n8. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nComplexity\nType\n\n\n\n\nTrial Division\nSmall primes\n( O\\(\\sqrt{n}\\) )\nDeterministic\n\n\nSieve\nPrecompute primes\n( O\\(n \\log \\log n\\) )\nDeterministic\n\n\nMiller-Rabin\nPrimality test\n( O\\(k \\log^3 n\\) )\nProbabilistic\n\n\nPollard Rho\nFactorization\n( O\\(n^{1/4}\\) )\nProbabilistic\n\n\n\n\n\nWhy It Matters\nModern security, number theory problems, and many algorithmic puzzles depend on knowing when a number is prime and factoring it quickly when it isn’t. These tools are the entry point to RSA, modular combinatorics, and advanced cryptography.\n\n\nTry It Yourself\n\nCheck if 97 is prime using trial division and Miller-Rabin.\nFactorize 5959 (should yield 59 × 101).\nGenerate all primes ≤ 100 using a sieve.\nWrite a recursive factorizer using Pollard Rho + Miller-Rabin.\nMeasure performance difference between \\(\\sqrt{n}\\) trial and Pollard Rho for \\(n \\approx 10^{12}\\).\n\nThese techniques make huge numbers approachable , one factor at a time.\n\n\n\n53. Combinatorics (Permutations, Combinations, Subsets)\nCombinatorics is the art of counting structures , how many ways can we arrange, select, or group things? In algorithms, it’s everywhere: DP transitions, counting paths, bitmask enumeration, and probabilistic reasoning. Here we’ll build a toolkit for computing factorials, nCr, nPr, and subset counts, both exactly and under a modulus.\n\n1. Factorials and Precomputation\nMost combinatorial formulas rely on factorials: \\[\nn! = 1 \\times 2 \\times 3 \\times \\dots \\times n\n\\]\nWe can precompute them modulo ( m ) (often \\(10^9+7\\)) for efficiency.\nconst int MOD = 1e9 + 7;\nconst int MAXN = 1e6;\nlong long fact[MAXN + 1], invfact[MAXN + 1];\n\nlong long modpow(long long a, long long b) {\n    long long res = 1;\n    while (b &gt; 0) {\n        if (b & 1) res = res * a % MOD;\n        a = a * a % MOD;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\n\nvoid init_factorials() {\n    fact[0] = 1;\n    for (int i = 1; i &lt;= MAXN; i++)\n        fact[i] = fact[i - 1] * i % MOD;\n    invfact[MAXN] = modpow(fact[MAXN], MOD - 2);\n    for (int i = MAXN - 1; i &gt;= 0; i--)\n        invfact[i] = invfact[i + 1] * (i + 1) % MOD;\n}\nNow you can compute ( nCr ) and ( nPr ) in ( O(1) ) time.\n\n\n2. Combinations ( nCr )\nThe number of ways to choose r items from ( n ) items: \\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\nlong long nCr(int n, int r) {\n    if (r &lt; 0 || r &gt; n) return 0;\n    return fact[n] * invfact[r] % MOD * invfact[n - r] % MOD;\n}\nProperties:\n\n\\((C(n, 0) = 1),\\ (C(n, n) = 1)\\)\n\\(C(n, r) = C(n, n - r)\\)\nPascal’s Rule: \\(C(n, r) = C(n - 1, r - 1) + C(n - 1, r)\\)\n\n\n\nExample\n( C(5, 2) = 10 ) There are 10 ways to pick 2 elements from a 5-element set.\n\n\n3. Permutations ( nPr )\nNumber of ways to arrange r elements chosen from ( n ): \\[\nP(n, r) = \\frac{n!}{(n-r)!}\n\\]\nlong long nPr(int n, int r) {\n    if (r &lt; 0 || r &gt; n) return 0;\n    return fact[n] * invfact[n - r] % MOD;\n}\n\n\nExample\n( P(5, 2) = 20 ) Choosing 2 out of 5 elements and arranging them yields 20 orders.\n\n\n4. Subsets and Power Set\nEach element has 2 choices: include or exclude. Hence, number of subsets: \\[\n2^n\n\\]\nlong long subsets_count(int n) {\n    return modpow(2, n);\n}\nEnumerating subsets using bitmasks:\nfor (int mask = 0; mask &lt; (1 &lt;&lt; n); mask++) {\n    for (int i = 0; i &lt; n; i++)\n        if (mask & (1 &lt;&lt; i))\n            ; // include element i\n}\nTotal: \\(2^n\\) subsets, ( O\\(n2^n\\) ) time to enumerate.\n\n\n5. Multisets and Repetition\nNumber of ways to choose ( r ) items from ( n ) with repetition: \\[\nC(n + r - 1, r)\n\\]\nFor example, number of ways to give 5 candies to 3 kids (each can get 0): ( C(3+5-1, 5) = C(7,5) = 21 )\n\n\n6. Modular Combinatorics\nWhen working modulo ( p ): - Use modular inverse for division. - \\(C(n, r) \\bmod p = fact[n] \\cdot invfact[r] \\cdot invfact[n - r] \\bmod p\\)\nWhen \\(n \\ge p\\), use Lucas’ Theorem: \\[\nC(n, r) \\bmod p = C(n/p, r/p) \\cdot C(n%p, r%p) \\bmod p\n\\]\n\n\n7. Stirling and Bell Numbers (Advanced)\n\nStirling Numbers of 2nd Kind: ways to partition ( n ) items into ( k ) non-empty subsets \\[\nS(n,k) = k \\cdot S(n-1,k) + S(n-1,k-1)\n\\]\nBell Numbers: total number of partitions \\[\nB(n) = \\sum_{k=0}^{n} S(n,k)\n\\]\n\nUsed in set partition and grouping problems.\n\n\n8. Tiny Code\ninit_factorials();\nprintf(\"%lld\\n\", nCr(10, 3));  // 120\nprintf(\"%lld\\n\", nPr(10, 3));  // 720\nprintf(\"%lld\\n\", subsets_count(5)); // 32\n\n\n9. Summary\n\n\n\n\n\n\n\n\n\nConcept\nFormula\nMeaning\nExample\n\n\n\n\nFactorial\n\\(n!\\)\nAll arrangements\n\\(5! = 120\\)\n\n\nCombination\n\\(C(n, r) = \\frac{n!}{r!(n - r)!}\\)\nChoose\n\\(C(5, 2) = 10\\)\n\n\nPermutation\n\\(P(n, r) = \\frac{n!}{(n - r)!}\\)\nArrange\n\\(P(5, 2) = 20\\)\n\n\nSubsets\n\\(2^n\\)\nAll combinations\n\\(2^3 = 8\\)\n\n\nMultisets\n\\(C(n + r - 1, r)\\)\nRepetition allowed\n\\(C(4, 2) = 6\\)\n\n\n\n\n\nWhy It Matters\nCombinatorics underlies probability, DP counting, and modular problems. You can’t master competitive programming or algorithm design without counting possibilities correctly. It teaches how structure emerges from choice , and how to count it efficiently.\n\n\nTry It Yourself\n\nCompute \\(C(1000, 500) \\bmod (10^9 + 7)\\).\nCount the number of 5-bit subsets with exactly 3 bits on, i.e. \\(C(5, 3)\\).\nWrite a loop to print all subsets of {a, b, c, d}.\nUse Lucas’ theorem for \\(C(10^6, 1000) \\bmod 13\\).\nImplement Stirling recursion and print \\(S(5, 2)\\).\n\nEvery algorithmic counting trick , from Pascal’s triangle to binomial theorem , starts right here.\n\n\n\n54. Probability and Randomized Algorithms\nProbability introduces controlled randomness into computation. Instead of deterministic steps, randomized algorithms use random choices to achieve speed, simplicity, or robustness. This section bridges probability theory and algorithm design , teaching how to model, analyze, and exploit randomness.\nWe’ll cover:\n\nProbability Basics\nExpected Value\nMonte Carlo and Las Vegas Algorithms\nRandomized Data Structures and Algorithms\n\n\n1. Probability Basics\nEvery event has a probability between 0 and 1.\nIf a sample space has \\(n\\) equally likely outcomes and \\(k\\) of them satisfy a condition, then\n\\[\nP(E) = \\frac{k}{n}\n\\]\nExamples\n\nRolling a fair die: \\(P(\\text{even}) = \\frac{3}{6} = \\frac{1}{2}\\)\nDrawing an ace from a deck: \\(P(\\text{ace}) = \\frac{4}{52} = \\frac{1}{13}\\)\n\nKey Rules\n\nComplement: \\(P(\\bar{E}) = 1 - P(E)\\)\n\nAddition: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\nMultiplication: \\(P(A \\cap B) = P(A) \\cdot P(B \\mid A)\\)\n\n\n\n2. Expected Value\nThe expected value is the weighted average of outcomes.\n\\[\nE[X] = \\sum_{i} P(x_i) \\cdot x_i\n\\]\nExample: Expected value of a die: \\[\nE[X] = \\frac{1+2+3+4+5+6}{6} = 3.5\n\\]\nProperties:\n\nLinearity: \\(E[aX + bY] = aE[X] + bE[Y]\\)\nUseful for average-case analysis\n\nIn algorithms:\n\nExpected number of comparisons in QuickSort is \\(O(n \\log n)\\)\nExpected time for hash table lookup is \\(O(1)\\)\n\n\n\n3. Monte Carlo vs Las Vegas\nRandomized algorithms are broadly two types:\n\n\n\n\n\n\n\n\n\nType\nOutput\nRuntime\nExample\n\n\n\n\nMonte Carlo\nMay be wrong (probabilistically)\nFixed\nMiller-Rabin Primality\n\n\nLas Vegas\nAlways correct\nRandom runtime\nRandomized QuickSort\n\n\n\nMonte Carlo:\n\nFaster, approximate\nYou can control error probability\nE.g. primality test returns “probably prime”\n\nLas Vegas:\n\nOutput guaranteed correct\nRuntime varies by luck\nE.g. QuickSort with random pivot\n\n\n\n4. Randomization in Algorithms\nRandomization helps break worst-case patterns.\n\n\nA. Randomized QuickSort\nPick a random pivot instead of first element. Expected time becomes ( O\\(n \\log n\\) ) regardless of input order.\nint partition(int a[], int l, int r) {\n    int pivot = a[l + rand() % (r - l + 1)];\n    // move pivot to end, then normal partition\n}\nThis avoids adversarial inputs like sorted arrays.\n\n\nB. Randomized Hashing\nHash collisions can be exploited by adversaries. Using random coefficients in hash functions makes attacks infeasible.\nlong long hash(long long x, long long a, long long b, long long p) {\n    return (a * x + b) % p;\n}\nPick random ( a, b ) for robustness.\n\n\nC. Randomized Data Structures\n\nSkip List: uses random levels for nodes Expected ( O\\(\\log n\\) ) search/insert/delete\nTreap: randomized heap priority + BST order Maintains balance in expectation\n\nstruct Node {\n    int key, priority;\n    Node *l, *r;\n};\nRandomized balancing gives good average performance without rotation logic.\n\n\nD. Random Sampling\nPick random elements efficiently:\n\nReservoir Sampling: sample ( k ) items from a stream of unknown size- Shuffle: Fisher-Yates Algorithm\n\nfor (int i = n - 1; i &gt; 0; i--) {\n    int j = rand() % (i + 1);\n    swap(a[i], a[j]);\n}\n\n\n5. Probabilistic Guarantees\nRandomized algorithms often use Chernoff bounds and Markov’s inequality to bound errors:\n\nMarkov: \\(P(X \\ge kE[X]) \\le \\frac{1}{k}\\)\nChebyshev: \\(P(|X - E[X]| \\ge c\\sigma) \\le \\frac{1}{c^2}\\)\nChernoff: Exponentially small tail bounds\n\nThese ensure “with high probability” (\\(1 - \\frac{1}{n^c}\\)) guarantees.\n\n\n6. Tiny Code\nRandomized QuickSort:\nint partition(int arr[], int low, int high) {\n    int pivotIdx = low + rand() % (high - low + 1);\n    swap(arr[pivotIdx], arr[high]);\n    int pivot = arr[high], i = low;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) swap(arr[i++], arr[j]);\n    }\n    swap(arr[i], arr[high]);\n    return i;\n}\n\nvoid quicksort(int arr[], int low, int high) {\n    if (low &lt; high) {\n        int pi = partition(arr, low, high);\n        quicksort(arr, low, pi - 1);\n        quicksort(arr, pi + 1, high);\n    }\n}\n\n\n7. Summary\n\n\n\n\n\n\n\n\nConcept\nKey Idea\nUse Case\n\n\n\n\nExpected Value\nWeighted average outcome\nAnalyze average case\n\n\nMonte Carlo\nProbabilistic correctness\nPrimality test\n\n\nLas Vegas\nProbabilistic runtime\nQuickSort\n\n\nRandom Pivot\nBreak worst-case\nSorting\n\n\nSkip List / Treap\nRandom balancing\nData Structures\n\n\nReservoir Sampling\nStream selection\nLarge data\n\n\n\n\n\nWhy It Matters\nRandomization is not “luck” , it’s a design principle. It transforms rigid algorithms into adaptive, robust systems. In complexity theory, randomness helps achieve bounds impossible deterministically.\n\n“A bit of randomness turns worst cases into best friends.”\n\n\n\nTry It Yourself\n\nSimulate rolling two dice and compute expected sum.\nImplement randomized QuickSort and measure average runtime.\nWrite a Monte Carlo primality checker.\nCreate a random hash function for integers.\nImplement reservoir sampling for a large input stream.\n\nThese experiments show how uncertainty can become a powerful ally in algorithm design.\n\n\n\n55. Sieve Methods and Modular Math\nSieve methods are essential tools in number theory for generating prime numbers, prime factors, and function values (φ, μ) efficiently. Combined with modular arithmetic, they form the backbone of algorithms in cryptography, combinatorics, and competitive programming.\nThis section introduces:\n\nSieve of Eratosthenes- Optimized Linear Sieve- Sieve for Smallest Prime Factor (SPF)- Euler’s Totient Function (φ)- Modular Applications\n\n\n1. The Sieve of Eratosthenes\nThe classic algorithm to find all primes ≤ ( n ).\nIdea: Start from 2, mark all multiples as composite. Continue to √n.\nvector&lt;int&gt; sieve(int n) {\n    vector&lt;int&gt; primes;\n    vector&lt;bool&gt; is_prime(n + 1, true);\n    is_prime[0] = is_prime[1] = false;\n    for (int i = 2; i * i &lt;= n; i++)\n        if (is_prime[i])\n            for (int j = i * i; j &lt;= n; j += i)\n                is_prime[j] = false;\n    for (int i = 2; i &lt;= n; i++)\n        if (is_prime[i]) primes.push_back(i);\n    return primes;\n}\nTime Complexity: ( O\\(n \\log \\log n\\) )\nSpace: ( O(n) )\nExample: Primes up to 20 → 2, 3, 5, 7, 11, 13, 17, 19\n\n\n2. Linear Sieve (O(n))\nUnlike the basic sieve, each number is marked exactly once by its smallest prime factor (SPF).\nIdea:\n\nFor each prime ( p ), mark \\(p \\times i\\) only once.- Use spf[i] to store smallest prime factor.\n\nconst int N = 1e6;\nint spf[N + 1];\nvector&lt;int&gt; primes;\n\nvoid linear_sieve() {\n    for (int i = 2; i &lt;= N; i++) {\n        if (!spf[i]) {\n            spf[i] = i;\n            primes.push_back(i);\n        }\n        for (int p : primes) {\n            if (p &gt; spf[i] || 1LL * i * p &gt; N) break;\n            spf[i * p] = p;\n        }\n    }\n}\nBenefits:\n\nGet primes, SPF, and factorizations in ( O(n) ).- Ideal for problems needing many factorizations.\n\n\n\n3. Smallest Prime Factor (SPF) Table\nWith spf[], factorization becomes ( O\\(\\log n\\) ).\nvector&lt;int&gt; factorize(int x) {\n    vector&lt;int&gt; f;\n    while (x != 1) {\n        f.push_back(spf[x]);\n        x /= spf[x];\n    }\n    return f;\n}\nExample: spf[12] = 2 → factors = [2, 2, 3]\n\n\n4. Euler’s Totient Function ( (n) )\nThe number of integers ≤ ( n ) that are coprime with ( n ).\nFormula: \\[\n\\varphi(n) = n \\prod_{p|n} \\left(1 - \\frac{1}{p}\\right)\n\\]\nProperties:\n\n\\(\\varphi(p) = p - 1\\) if \\(p\\) is prime\nMultiplicative: if \\(\\gcd(a, b) = 1\\), then \\(\\varphi(ab) = \\varphi(a)\\varphi(b)\\)\n\nImplementation (Linear Sieve):\nconst int N = 1e6;\nint phi[N + 1];\nbool is_comp[N + 1];\nvector&lt;int&gt; primes;\n\nvoid phi_sieve() {\n    phi[1] = 1;\n    for (int i = 2; i &lt;= N; i++) {\n        if (!is_comp[i]) {\n            primes.push_back(i);\n            phi[i] = i - 1;\n        }\n        for (int p : primes) {\n            if (1LL * i * p &gt; N) break;\n            is_comp[i * p] = true;\n            if (i % p == 0) {\n                phi[i * p] = phi[i] * p;\n                break;\n            } else {\n                phi[i * p] = phi[i] * (p - 1);\n            }\n        }\n    }\n}\nExample:\n\n\\(\\varphi(6) = 6(1 - \\frac{1}{2})(1 - \\frac{1}{3}) = 2\\)\nNumbers coprime with 6: 1, 5\n\n\n\n5. Modular Math Applications\nOnce we have primes and totients, we can do many modular computations.\n\n\nA. Fermat’s Little Theorem\nIf ( p ) is prime, \\[\na^{p-1} \\equiv 1 \\pmod{p}\n\\] Hence, \\[\na^{-1} \\equiv a^{p-2} \\pmod{p}\n\\]\nUsed in: modular inverses, combinatorics.\n\n\nB. Euler’s Theorem\nIf \\(\\gcd(a, n) = 1\\), then\n\\[\na^{\\varphi(n)} \\equiv 1 \\pmod{n}\n\\]\nGeneralizes Fermat’s theorem to composite moduli.\n\n\nC. Modular Exponentiation with Totient Reduction\nFor very large powers:\n\\[\na^b \\bmod n = a^{b \\bmod \\varphi(n)} \\bmod n\n\\]\n(when \\(a\\) and \\(n\\) are coprime)\n\n\n6. Tiny Code\nPrimes up to n:\nauto primes = sieve(100);\nTotients up to n:\nphi_sieve();\ncout &lt;&lt; phi[10]; // 4\nFactorization:\nauto f = factorize(60); // [2, 2, 3, 5]\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nTime\nUse\n\n\n\n\nEratosthenes\nMark multiples\n(O\\(n \\log \\log n\\))\nSimple prime gen\n\n\nLinear Sieve\nMark once\n(O(n))\nPrime + SPF\n\n\nSPF Table\nSmallest prime factor\n(O(1)) query\nFast factorization\n\n\nφ(n)\nCoprime count\n(O(n))\nModular exponent\n\n\nFermat / Euler\nInverses, reduction\n(O\\(\\log n\\))\nModular arithmetic\n\n\n\n\n\nWhy It Matters\nSieve methods are the fastest way to preprocess arithmetic information. They unlock efficient solutions to problems involving primes, divisors, modular equations, and cryptography.\n\n“Before you can reason about numbers, you must first sieve them clean.”\n\n\n\nTry It Yourself\n\nGenerate all primes \\(\\le 10^6\\) using a linear sieve.\nFactorize \\(840\\) using the SPF array.\nCompute \\(\\varphi(n)\\) for \\(n = 1..20\\).\nVerify \\(a^{\\varphi(n)} \\equiv 1 \\pmod{n}\\) for \\(a = 3\\), \\(n = 10\\).\nSolve \\(a^b \\bmod n\\) with \\(b\\) very large using \\(\\varphi(n)\\).\n\nSieve once, and modular math becomes effortless forever after.\n\n\n\n56. Linear Algebra (Gaussian Elimination, LU, SVD)\nLinear algebra gives algorithms their mathematical backbone. From solving equations to powering ML models, it’s the hidden engine behind optimization, geometry, and numerical computation.\nIn this section, we’ll focus on the algorithmic toolkit:\n\nGaussian Elimination (solve systems, invert matrices)\nLU Decomposition (efficient repeated solving)\nSVD (Singular Value Decomposition) overview\n\nYou’ll see how algebra becomes code , step by step.\n\n1. Systems of Linear Equations\nWe want to solve: \\[\nA \\cdot x = b\n\\] where ( A ) is an \\(n \\times n\\) matrix, and ( x, b ) are vectors.\nFor example: \\[\\begin{cases}\n2x + 3y = 8 \\\nx + 2y = 5\n\\end{cases}\\]\nThe solution is the intersection of two lines. In general, \\(A^{-1}b\\) gives ( x ), but we usually solve it more directly using Gaussian elimination.\n\n\n2. Gaussian Elimination (Row Reduction)\nIdea: Transform ( [A|b] ) (augmented matrix) into upper-triangular form, then back-substitute.\nSteps:\n\nFor each row, select a pivot (non-zero leading element).\nEliminate below it using row operations.\nAfter all pivots, back-substitute to get the solution.\n\n\n\nA. Implementation (C)\nconst double EPS = 1e-9;\n\nvector&lt;double&gt; gauss(vector&lt;vector&lt;double&gt;&gt; A, vector&lt;double&gt; b) {\n    int n = A.size();\n    for (int i = 0; i &lt; n; i++) {\n        // 1. Find pivot\n        int pivot = i;\n        for (int j = i + 1; j &lt; n; j++)\n            if (fabs(A[j][i]) &gt; fabs(A[pivot][i]))\n                pivot = j;\n        swap(A[i], A[pivot]);\n        swap(b[i], b[pivot]);\n\n        // 2. Normalize pivot row\n        double div = A[i][i];\n        if (fabs(div) &lt; EPS) continue;\n        for (int k = i; k &lt; n; k++) A[i][k] /= div;\n        b[i] /= div;\n\n        // 3. Eliminate below\n        for (int j = i + 1; j &lt; n; j++) {\n            double factor = A[j][i];\n            for (int k = i; k &lt; n; k++) A[j][k] -= factor * A[i][k];\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // 4. Back substitution\n    vector&lt;double&gt; x(n);\n    for (int i = n - 1; i &gt;= 0; i--) {\n        x[i] = b[i];\n        for (int j = i + 1; j &lt; n; j++)\n            x[i] -= A[i][j] * x[j];\n    }\n    return x;\n}\nTime complexity: ( O\\(n^3\\) )\n\n\nB. Example\nSolve: \\[\\begin{cases}\n2x + 3y = 8 \\\nx + 2y = 5\n\\end{cases}\\]\nAugmented matrix: \\[\\begin{bmatrix}\n2 & 3 & | & 8 \\\n1 & 2 & | & 5\n\\end{bmatrix}\\]\nReduce:\n\nRow2 ← Row2 − ½ Row1 → \\([1, 2 | 5] \\to [0, 0.5 | 1]\\)- Back substitute → ( y = 2, x = 1 )\n\n\n\n3. LU Decomposition\nLU factorization expresses: \\[\nA = L \\cdot U\n\\] where ( L ) is lower-triangular (1s on diagonal), ( U ) is upper-triangular.\nThis allows solving ( A x = b ) in two triangular solves:\n\nSolve ( L y = b )\nSolve ( U x = y )\n\nEfficient when solving for multiple b’s (same A).\n\n\nA. Decomposition Algorithm\nvoid lu_decompose(vector&lt;vector&lt;double&gt;&gt;& A, vector&lt;vector&lt;double&gt;&gt;& L, vector&lt;vector&lt;double&gt;&gt;& U) {\n    int n = A.size();\n    L.assign(n, vector&lt;double&gt;(n, 0));\n    U.assign(n, vector&lt;double&gt;(n, 0));\n\n    for (int i = 0; i &lt; n; i++) {\n        // Upper\n        for (int k = i; k &lt; n; k++) {\n            double sum = 0;\n            for (int j = 0; j &lt; i; j++)\n                sum += L[i][j] * U[j][k];\n            U[i][k] = A[i][k] - sum;\n        }\n        // Lower\n        for (int k = i; k &lt; n; k++) {\n            if (i == k) L[i][i] = 1;\n            else {\n                double sum = 0;\n                for (int j = 0; j &lt; i; j++)\n                    sum += L[k][j] * U[j][i];\n                L[k][i] = (A[k][i] - sum) / U[i][i];\n            }\n        }\n    }\n}\nSolve with forward + backward substitution.\n\n\n4. Singular Value Decomposition (SVD)\nSVD generalizes diagonalization for non-square matrices: \\[\nA = U \\Sigma V^T\n\\]\nWhere:\n\n( U ): left singular vectors (orthogonal)- \\(\\Sigma\\): diagonal of singular values- \\(V^T\\): right singular vectors Applications:\nData compression (PCA)- Noise reduction- Rank estimation- Pseudoinverse \\(A^+ = V \\Sigma^{-1} U^T\\) In practice, use libraries (e.g. LAPACK, Eigen).\n\n\n\n5. Numerical Stability and Pivoting\nIn floating-point math:\n\nAlways pick largest pivot (partial pivoting)- Avoid dividing by small numbers- Use EPS = 1e-9 threshold Small numerical errors can amplify quickly , stability is key.\n\n\n\n6. Tiny Code\nvector&lt;vector&lt;double&gt;&gt; A = {{2, 3}, {1, 2}};\nvector&lt;double&gt; b = {8, 5};\nauto x = gauss(A, b);\n// Output: x = [1, 2]\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nComplexity\nNotes\n\n\n\n\nGaussian Elimination\nSolve Ax=b\n(O\\(n^3\\))\nDirect method\n\n\nLU Decomposition\nRepeated solves\n(O\\(n^3\\))\nTriangular factorization\n\n\nSVD\nGeneral decomposition\n(O\\(n^3\\))\nRobust, versatile\n\n\n\n\n\nWhy It Matters\nLinear algebra is the language of algorithms , it solves equations, optimizes functions, and projects data. Whether building solvers or neural networks, these methods are your foundation.\n\n“Every algorithm lives in a vector space , it just needs a basis to express itself.”\n\n\n\nTry It Yourself\n\nSolve a 3×3 linear system with Gaussian elimination.\nImplement LU decomposition and test \\(L \\cdot U = A\\).\nUse LU to solve multiple ( b ) vectors.\nExplore SVD using a math library; compute singular values of a 2×2 matrix.\nCompare results between naive and pivoted elimination for unstable systems.\n\nStart with row operations , and you’ll see how geometry and algebra merge into code.\n\n\n\n57. FFT and NTT (Fast Transforms)\nThe Fast Fourier Transform (FFT) is one of the most beautiful and practical algorithms ever invented. It converts data between time (or coefficient) domain and frequency (or point) domain efficiently. The Number Theoretic Transform (NTT) is its modular counterpart for integer arithmetic , ideal for polynomial multiplication under a modulus.\nThis section covers:\n\nWhy we need transforms- Discrete Fourier Transform (DFT)- Cooley-Tukey FFT (complex numbers)- NTT (modular version)- Applications (polynomial multiplication, convolution)\n\n\n1. Motivation\nSuppose you want to multiply two polynomials: \\[\nA(x) = a_0 + a_1x + a_2x^2\n\\]\n\\[\nB(x) = b_0 + b_1x + b_2x^2\n\\]\nTheir product has coefficients: \\[\nc_k = \\sum_{i+j=k} a_i \\cdot b_j\n\\]\nThis is convolution: \\[\nC = A * B\n\\] Naively, this takes ( O\\(n^2\\) ). FFT reduces it to ( O\\(n \\log n\\) ).\n\n\n2. Discrete Fourier Transform (DFT)\nThe DFT maps coefficients \\(a_0, a_1, \\ldots, a_{n-1}\\) to evaluations at ( n )-th roots of unity:\n\\[\nA_k = \\sum_{j=0}^{n-1} a_j \\cdot e^{-2\\pi i \\cdot jk / n}\n\\]\nand the inverse transform recovers \\(a_j\\) from \\(A_k\\).\n\n\n3. Cooley-Tukey FFT\nKey idea: recursively split the sum into even and odd parts:\n\\[\nA_k = A_{even}(w_n^2) + w_n^k \\cdot A_{odd}(w_n^2)\n\\]\nWhere \\(w_n = e^{-2\\pi i / n}\\) is an ( n )-th root of unity.\n\n\nImplementation (C++)\n#include &lt;complex&gt;\n#include &lt;vector&gt;\n#include &lt;cmath&gt;\nusing namespace std;\n\nusing cd = complex&lt;double&gt;;\nconst double PI = acos(-1);\n\nvoid fft(vector&lt;cd&gt; &a, bool invert) {\n    int n = a.size();\n    for (int i = 1, j = 0; i &lt; n; i++) {\n        int bit = n &gt;&gt; 1;\n        for (; j & bit; bit &gt;&gt;= 1) j ^= bit;\n        j ^= bit;\n        if (i &lt; j) swap(a[i], a[j]);\n    }\n\n    for (int len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        double ang = 2 * PI / len * (invert ? -1 : 1);\n        cd wlen(cos(ang), sin(ang));\n        for (int i = 0; i &lt; n; i += len) {\n            cd w(1);\n            for (int j = 0; j &lt; len / 2; j++) {\n                cd u = a[i + j], v = a[i + j + len / 2] * w;\n                a[i + j] = u + v;\n                a[i + j + len / 2] = u - v;\n                w *= wlen;\n            }\n        }\n    }\n\n    if (invert) {\n        for (cd &x : a) x /= n;\n    }\n}\n\n\nPolynomial Multiplication with FFT\nvector&lt;long long&gt; multiply(vector&lt;int&gt; const& a, vector&lt;int&gt; const& b) {\n    vector&lt;cd&gt; fa(a.begin(), a.end()), fb(b.begin(), b.end());\n    int n = 1;\n    while (n &lt; (int)a.size() + (int)b.size()) n &lt;&lt;= 1;\n    fa.resize(n);\n    fb.resize(n);\n\n    fft(fa, false);\n    fft(fb, false);\n    for (int i = 0; i &lt; n; i++) fa[i] *= fb[i];\n    fft(fa, true);\n\n    vector&lt;long long&gt; result(n);\n    for (int i = 0; i &lt; n; i++)\n        result[i] = llround(fa[i].real());\n    return result;\n}\nComplexity: ( O\\(n \\log n\\) )\n\n\n4. Number Theoretic Transform (NTT)\nFFT uses complex numbers , NTT uses modular arithmetic with roots of unity mod p. We need a prime ( p ) such that: \\[\np = c \\cdot 2^k + 1\n\\] so a primitive root ( g ) exists.\nPopular choices:\n\n( p = 998244353, g = 3 )- ( p = 7340033, g = 3 )\n\n\n\nImplementation (NTT)\nconst int MOD = 998244353;\nconst int G = 3;\n\nint modpow(int a, int b) {\n    long long res = 1;\n    while (b) {\n        if (b & 1) res = res * a % MOD;\n        a = 1LL * a * a % MOD;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\n\nvoid ntt(vector&lt;int&gt; &a, bool invert) {\n    int n = a.size();\n    for (int i = 1, j = 0; i &lt; n; i++) {\n        int bit = n &gt;&gt; 1;\n        for (; j & bit; bit &gt;&gt;= 1) j ^= bit;\n        j ^= bit;\n        if (i &lt; j) swap(a[i], a[j]);\n    }\n    for (int len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        int wlen = modpow(G, (MOD - 1) / len);\n        if (invert) wlen = modpow(wlen, MOD - 2);\n        for (int i = 0; i &lt; n; i += len) {\n            long long w = 1;\n            for (int j = 0; j &lt; len / 2; j++) {\n                int u = a[i + j];\n                int v = (int)(a[i + j + len / 2] * w % MOD);\n                a[i + j] = u + v &lt; MOD ? u + v : u + v - MOD;\n                a[i + j + len / 2] = u - v &gt;= 0 ? u - v : u - v + MOD;\n                w = w * wlen % MOD;\n            }\n        }\n    }\n    if (invert) {\n        int inv_n = modpow(n, MOD - 2);\n        for (int &x : a) x = 1LL * x * inv_n % MOD;\n    }\n}\n\n\n5. Applications\n\nPolynomial Multiplication: ( O\\(n \\log n\\) )\nConvolution: digital signal processing\nBig Integer Multiplication (Karatsuba, FFT)\nSubset Convolution and combinatorial transforms\nNumber-theoretic sums (NTT-friendly modulus)\n\n\n\n6. Tiny Code\nvector&lt;int&gt; A = {1, 2, 3};\nvector&lt;int&gt; B = {4, 5, 6};\n// Result = {4, 13, 28, 27, 18}\nauto C = multiply(A, B);\n\n\n7. Summary\n\n\n\nAlgorithm\nDomain\nComplexity\nType\n\n\n\n\nDFT\nComplex\n( O\\(n^2\\) )\nNaive\n\n\nFFT\nComplex\n( O\\(n \\log n\\) )\nRecursive\n\n\nNTT\nModular\n( O\\(n \\log n\\) )\nInteger arithmetic\n\n\n\n\n\nWhy It Matters\nFFT and NTT bring polynomial algebra to life. They turn slow convolutions into lightning-fast transforms. From multiplying huge integers to compressing signals, they’re the ultimate divide-and-conquer on structure.\n\n“To multiply polynomials fast, you first turn them into music , then back again.”\n\n\n\nTry It Yourself\n\nMultiply (\\(1 + 2x + 3x^2\\)) and (\\(4 + 5x + 6x^2\\)) using FFT.\nImplement NTT over 998244353 and verify results mod p.\nCompare ( O\\(n^2\\) ) vs FFT performance for n = 1024.\nExperiment with inverse FFT (invert = true).\nExplore circular convolution for signal data.\n\nOnce you master FFT/NTT, you hold the power of speed in algebraic computation.\n\n\n\n58. Numerical Methods (Newton, Simpson, Runge-Kutta)\nNumerical methods let us approximate solutions when exact algebraic answers are hard or impossible. They are the foundation of scientific computing, simulation, and optimization , bridging the gap between continuous math and discrete computation.\nIn this section, we’ll explore three classics:\n\nNewton-Raphson: root finding- Simpson’s Rule: numerical integration- Runge-Kutta (RK4): solving differential equations These algorithms showcase how iteration, approximation, and convergence build powerful tools.\n\n\n1. Newton-Raphson Method\nUsed to find a root of ( f(x) = 0 ). Starting from a guess \\(x_0\\), iteratively refine:\n\\[\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\\]\nConvergence is quadratic if ( f ) is smooth and \\(x_0\\) is close enough.\n\n\nA. Example\nSolve ( f(x) = x^2 - 2 = 0 ) We know root = \\(\\sqrt{2}\\)\nStart \\(x_0 = 1\\)\n\n\n\nIter\n\\(x_n\\)\n(f\\(x_n\\))\n(f’\\(x_n\\))\n\\(x_{n+1}\\)\n\n\n\n\n0\n1.000\n-1.000\n2.000\n1.500\n\n\n1\n1.500\n0.250\n3.000\n1.417\n\n\n2\n1.417\n0.006\n2.834\n1.414\n\n\n\nConverged: \\(1.414 \\approx \\sqrt{2}\\)\n\n\nB. Implementation\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\ndouble f(double x) { return x * x - 2; }\ndouble df(double x) { return 2 * x; }\n\ndouble newton(double x0) {\n    for (int i = 0; i &lt; 20; i++) {\n        double fx = f(x0);\n        double dfx = df(x0);\n        if (fabs(fx) &lt; 1e-9) break;\n        x0 = x0 - fx / dfx;\n    }\n    return x0;\n}\n\nint main() {\n    printf(\"Root: %.6f\\n\", newton(1.0)); // 1.414214\n}\nTime Complexity: ( O(k) ) iterations, each ( O(1) )\n\n\n2. Simpson’s Rule (Numerical Integration)\nWhen you can’t integrate ( f(x) ) analytically, approximate the area under the curve.\nDivide interval ([a, b]) into even ( n ) subintervals (width ( h )).\n\\[\nI \\approx \\frac{h}{3} \\Big( f(a) + 4 \\sum f(x_{odd}) + 2 \\sum f(x_{even}) + f(b) \\Big)\n\\]\n\n\nA. Implementation\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\ndouble f(double x) { return x * x; } // integrate x^2\n\ndouble simpson(double a, double b, int n) {\n    double h = (b - a) / n;\n    double s = f(a) + f(b);\n    for (int i = 1; i &lt; n; i++) {\n        double x = a + i * h;\n        s += f(x) * (i % 2 == 0 ? 2 : 4);\n    }\n    return s * h / 3;\n}\n\nint main() {\n    printf(\"∫₀¹ x² dx ≈ %.6f\\n\", simpson(0, 1, 100)); // ~0.333333\n}\nAccuracy: ( O\\(h^4\\) ) Note: ( n ) must be even.\n\n\nB. Example\n\\[\n\\int_0^1 x^2 dx = \\frac{1}{3}\n\\] With ( n = 100 ), Simpson gives ( 0.333333 ).\n\n\n3. Runge-Kutta (RK4)\nUsed to solve first-order ODEs: \\[\ny' = f(x, y), \\quad y(x_0) = y_0\n\\]\nRK4 Formula:\n\\[\\begin{aligned}\nk_1 &= f(x_n, y_n) \\\nk_2 &= f(x_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1) \\\nk_3 &= f(x_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2) \\\nk_4 &= f(x_n + h, y_n + hk_3) \\\ny_{n+1} &= y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n\\end{aligned}\\]\nAccuracy: ( O\\(h^4\\) )\n\n\nA. Example\nSolve ( y’ = x + y ), ( y(0) = 1 ), step ( h = 0.1 ).\nEach iteration refines ( y ) with weighted slope average.\n\n\nB. Implementation\n#include &lt;stdio.h&gt;\n\ndouble f(double x, double y) {\n    return x + y;\n}\n\ndouble runge_kutta(double x0, double y0, double h, double xn) {\n    double x = x0, y = y0;\n    while (x &lt; xn) {\n        double k1 = f(x, y);\n        double k2 = f(x + h / 2, y + h * k1 / 2);\n        double k3 = f(x + h / 2, y + h * k2 / 2);\n        double k4 = f(x + h, y + h * k3);\n        y += h * (k1 + 2*k2 + 2*k3 + k4) / 6;\n        x += h;\n    }\n    return y;\n}\n\nint main() {\n    printf(\"y(0.1) ≈ %.6f\\n\", runge_kutta(0, 1, 0.1, 0.1));\n}\n\n\n4. Tiny Code Summary\n\n\n\n\n\n\n\n\n\n\nMethod\nPurpose\nFormula\nAccuracy\nType\n\n\n\n\nNewton-Raphson\nRoot finding\n\\(x_{n+1}=x_n-\\frac{f}{f'}\\)\nQuadratic\nIterative\n\n\nSimpson’s Rule\nIntegration\n(h/3(…))\n(O\\(h^4\\))\nDeterministic\n\n\nRunge-Kutta (RK4)\nODEs\nWeighted slope avg\n(O\\(h^4\\))\nIterative\n\n\n\n\n\n5. Numerical Stability\n\nSmall step ( h ): better accuracy, more cost- Large ( h ): faster, less stable- Always check convergence (\\(|x_{n+1} - x_n| &lt; \\varepsilon\\))- Avoid division by small derivatives in Newton’s method\n\n\n\nWhy It Matters\nNumerical methods let computers simulate the continuous world. From physics to AI training, they solve what calculus cannot symbolically.\n\n“When equations won’t talk, iterate , and they’ll whisper their answers.”\n\n\n\nTry It Yourself\n\nUse Newton’s method for \\(\\cos x - x = 0\\).\nApproximate \\(\\displaystyle \\int_0^{\\pi/2} \\sin x\\,dx\\) with Simpson’s rule.\nSolve \\(y' = y - x^2 + 1,\\ y(0) = 0.5\\) using RK4.\nCompare RK4 with Euler’s method for the same ODE.\nExperiment with step sizes \\(h \\in \\{0.1, 0.01, 0.001\\}\\) and observe convergence.\n\nNumerical thinking turns continuous problems into iterative algorithms , precise enough to power every simulation and solver you’ll ever write.\n\n\n\n59. Mathematical Optimization (Simplex, Gradient, Convex)\nMathematical optimization is about finding the best solution , smallest cost, largest profit, shortest path , under given constraints. It’s the heart of machine learning, operations research, and engineering design.\nIn this section, we’ll explore three pillars:\n\nSimplex Method , for linear programs\nGradient Descent , for continuous optimization\nConvex Optimization , the theory ensuring global optima\n\n\n1. What Is Optimization?\nA general optimization problem looks like:\n\\[\n\\min_x \\ f(x)\n\\] subject to constraints: \\[\ng_i(x) \\le 0, \\quad h_j(x) = 0\n\\]\nWhen ( f ) and \\(g_i, h_j\\) are linear, it’s a Linear Program (LP). When ( f ) is differentiable, we can use gradients. When ( f ) is convex, every local minimum is global , the ideal world.\n\n\n2. The Simplex Method (Linear Programming)\nA linear program has the form:\n\\[\n\\max \\ c^T x\n\\] subject to \\[\nA x \\le b, \\quad x \\ge 0\n\\]\nGeometrically, each constraint forms a half-space. The feasible region is a convex polytope, and the optimum lies at a vertex.\n\n\nA. Example\nMaximize ( z = 3x + 2y ) subject to \\[\\begin{cases}\n2x + y \\le 18 \\\n2x + 3y \\le 42 \\\nx, y \\ge 0\n\\end{cases}\\]\nSolution: ( x=9, y=8 ), ( z=43 )\n\n\nB. Algorithm (Sketch)\n\nConvert inequalities to equalities by adding slack variables.\nInitialize at a vertex (basic feasible solution).\nAt each step:\n\nChoose entering variable (most negative coefficient in objective). - Choose leaving variable (min ratio test). - Pivot to new vertex.4. Repeat until optimal.\n\n\n\n\nC. Implementation (Simplified Pseudocode)\n// Basic simplex-like outline\nwhile (exists negative coefficient in objective row) {\n    choose entering column j;\n    choose leaving row i (min b[i]/a[i][j]);\n    pivot(i, j);\n}\nLibraries (like GLPK or Eigen) handle full implementations.\nTime Complexity: worst ( O\\(2^n\\) ), but fast in practice.\n\n\n3. Gradient Descent\nFor differentiable ( f(x) ), we move opposite the gradient:\n\\[\nx_{k+1} = x_k - \\eta \\nabla f(x_k)\n\\]\nwhere \\(\\eta\\) is the learning rate.\nIntuition: ( f(x) ) points uphill, so step opposite it.\n\n\nA. Example\nMinimize ( f(x) = (x-3)^2 )\n\\[\nf'(x) = 2(x-3)\n\\]\nStart \\(x_0 = 0\\), \\(\\eta = 0.1\\)\n\n\n\nIter\n\\(x_k\\)\n(f\\(x_k\\))\nGradient\nNew (x)\n\n\n\n\n0\n0\n9\n-6\n0.6\n\n\n1\n0.6\n5.76\n-4.8\n1.08\n\n\n2\n1.08\n3.69\n-3.84\n1.46\n\n\n…\n→3\n→0\n→0\n→3\n\n\n\nConverges to ( x = 3 )\n\n\nB. Implementation\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\ndouble f(double x) { return (x - 3) * (x - 3); }\ndouble df(double x) { return 2 * (x - 3); }\n\ndouble gradient_descent(double x0, double lr) {\n    for (int i = 0; i &lt; 100; i++) {\n        double g = df(x0);\n        if (fabs(g) &lt; 1e-6) break;\n        x0 -= lr * g;\n    }\n    return x0;\n}\n\nint main() {\n    printf(\"Min at x = %.6f\\n\", gradient_descent(0, 0.1));\n}\n\n\nC. Variants\n\nMomentum: ( v = v + \\(1-\\beta\\)f(x) )- Adam: adaptive learning rates- Stochastic Gradient Descent (SGD): random subset of data All used heavily in machine learning.\n\n\n\n4. Convex Optimization\nA function ( f ) is convex if: \\[\nf(\\lambda x + (1-\\lambda)y) \\le \\lambda f(x) + (1-\\lambda)f(y)\n\\]\nThis means any local minimum is global.\nExamples:\n\n( f(x) = x^2 ) (convex)- ( f(x) = x^3 ) (not convex) For convex functions with linear constraints, gradient-based methods always converge to the global optimum.\n\n\n\nA. Checking Convexity\n\n1D: ( f’’(x) )- Multivariate: Hessian ( ^2 f(x) ) is positive semidefinite\n\n\n\n5. Applications\n\nLinear Programming (Simplex): logistics, scheduling- Quadratic Programming: portfolio optimization- Gradient Methods: ML, curve fitting- Convex Programs: control systems, regularization\n\n\n\n6. Tiny Code\nSimple gradient descent to minimize ( f(x,y)=x2+y2 ):\ndouble f(double x, double y) { return x*x + y*y; }\nvoid grad(double x, double y, double *gx, double *gy) {\n    *gx = 2*x; *gy = 2*y;\n}\n\nvoid optimize() {\n    double x=5, y=3, lr=0.1;\n    for(int i=0; i&lt;100; i++){\n        double gx, gy;\n        grad(x, y, &gx, &gy);\n        x -= lr * gx;\n        y -= lr * gy;\n    }\n    printf(\"Min at (%.3f, %.3f)\\n\", x, y);\n}\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nDomain\nComplexity\nNotes\n\n\n\n\nSimplex\nLinear\nPolynomial (average case)\nLP solver\n\n\nGradient Descent\nContinuous\n\\(O(k)\\)\nNeeds step size\n\n\nConvex Methods\nConvex\n\\(O(k \\log \\frac{1}{\\varepsilon})\\)\nGlobal optima guaranteed\n\n\n\n\n\nWhy It Matters\nOptimization turns math into decisions. From fitting curves to planning resources, it formalizes trade-offs and efficiency. It’s where computation meets purpose , finding the best in all possible worlds.\n\n“Every algorithm is, at heart, an optimizer , searching for something better.”\n\n\n\nTry It Yourself\n\nSolve a linear program with 2 constraints manually via Simplex.\nImplement gradient descent for \\(f(x) = (x - 5)^2 + 2\\).\nAdd momentum to your gradient descent loop.\nCheck convexity by plotting \\(f(x) = x^4 - 3x^2\\).\nExperiment with learning rates: too small leads to slow convergence; too large can diverge.\n\nMastering optimization means mastering how algorithms improve themselves , step by step, iteration by iteration.\n\n\n\n60. Algebraic Tricks and Transform Techniques\nIn algorithm design, algebra isn’t just theory , it’s a toolbox for transforming problems. By expressing computations algebraically, we can simplify, accelerate, or generalize solutions. This section surveys common algebraic techniques that turn hard problems into manageable ones.\nWe’ll explore:\n\nAlgebraic identities and factorizations\nGenerating functions and transforms\nConvolution tricks\nPolynomial methods and FFT applications\nMatrix and linear transforms for acceleration\n\n\n1. Algebraic Identities\nThese let you rewrite or decompose expressions to reveal structure or reduce complexity.\nClassic Forms:\n\nDifference of squares: \\[\na^2 - b^2 = (a-b)(a+b)\n\\]\nSum of cubes: \\[\na^3 + b^3 = (a+b)(a^2 - ab + b^2)\n\\]\nSquare of sum: \\[\n(a+b)^2 = a^2 + 2ab + b^2\n\\]\n\nUsed in dynamic programming, geometry, and optimization when simplifying recurrence terms or constraints.\nExample: Transforming \\((x+y)^2\\) lets you compute both \\(x^2 + y^2\\) and cross terms efficiently.\n\n\n2. Generating Functions\nA generating function encodes a sequence \\(a_0, a_1, a_2, \\ldots\\) into a formal power series:\n\\[\nG(x) = a_0 + a_1x + a_2x^2 + \\ldots\n\\]\nThey turn recurrence relations and counting problems into algebraic equations.\nExample: Fibonacci sequence \\[\nF(x) = F_0 + F_1x + F_2x^2 + \\ldots\n\\] with recurrence \\(F_n = F_{n-1} + F_{n-2}\\)\nSolve algebraically: \\[\nF(x) = \\frac{x}{1 - x - x^2}\n\\]\nApplications: combinatorics, probability, counting partitions.\n\n\n3. Convolution Tricks\nConvolution arises in combining sequences: \\[\n(c_n) = (a * b)*n = \\sum*{i=0}^{n} a_i b_{n-i}\n\\]\nNaive computation: ( O\\(n^2\\) ) Using Fast Fourier Transform (FFT): ( O\\(n \\log n\\) )\nExample: Polynomial multiplication Let \\[\nA(x) = a_0 + a_1x + a_2x^2, \\quad B(x) = b_0 + b_1x + b_2x^2\n\\] Then ( C(x) = A(x)B(x) ) gives coefficients by convolution.\nThis trick is used in:\n\nLarge integer multiplication- Pattern matching (cross-correlation)- Subset sum acceleration\n\n\n\n4. Polynomial Methods\nMany algorithmic problems can be represented as polynomials, where coefficients encode combinatorial structure.\n\n\nA. Polynomial Interpolation\nGiven ( n+1 ) points, there’s a unique degree-( n ) polynomial passing through them.\nUsed in error correction, FFT-based reconstruction, and number-theoretic transforms.\nLagrange Interpolation: \\[\nP(x) = \\sum_i y_i \\prod_{j \\ne i} \\frac{x - x_j}{x_i - x_j}\n\\]\n\n\nB. Root Representation\nSolve equations or check identities by working modulo a polynomial. Used in finite fields and coding theory (e.g., Reed-Solomon).\n\n\n5. Transform Techniques\nTransforms convert problems to simpler domains where operations become efficient.\n\n\n\n\n\n\n\n\n\nTransform\nConverts\nKey Property\nUsed In\n\n\n\n\nFFT / NTT\nTime ↔︎ Frequency\nConvolution → Multiplication\nSignal, polynomial mult\n\n\nZ-Transform\nSequence ↔︎ Function\nRecurrence solving\nDSP, control\n\n\nLaplace Transform\nFunction ↔︎ Algebraic\nDiff. eq. → Algebraic eq.\nContinuous systems\n\n\nWalsh-Hadamard Transform\nBoolean vectors\nXOR convolution\nSubset sum, SOS DP\n\n\n\nExample: Subset Convolution via FWT\nFor all subsets ( S ): \\[\nf'(S) = \\sum_{T \\subseteq S} f(T)\n\\]\nUse Fast Walsh-Hadamard Transform (FWHT) to compute in ( O\\(n2^n\\) ) instead of ( O\\(3^n\\) ).\n\n\n6. Matrix Tricks\nMatrix algebra enables transformations and compact formulations.\n\nMatrix exponentiation: solve recurrences in \\(O(\\log n)\\)\nDiagonalization: \\(A = P D P^{-1}\\), then \\(A^k = P D^k P^{-1}\\)\nFast power: speeds up Fibonacci, linear recurrences, Markov chains\n\nExample: Fibonacci\n\\[\n\\begin{bmatrix}\nF_{n+1} \\\\\nF_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0\n\\end{bmatrix}^n\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\n\n\n7. Tiny Code\nPolynomial Multiplication via FFT (Pseudo-C):\n// Outline using complex FFT library\nfft(A, false);\nfft(B, false);\nfor (int i = 0; i &lt; n; i++)\n    C[i] = A[i] * B[i];\nfft(C, true); // inverse\nMatrix Power (Fibonacci):\nvoid matmul(long long A[2][2], long long B[2][2]) {\n    long long C[2][2] = {{0}};\n    for (int i=0;i&lt;2;i++)\n        for (int j=0;j&lt;2;j++)\n            for (int k=0;k&lt;2;k++)\n                C[i][j] += A[i][k]*B[k][j];\n    memcpy(A, C, sizeof(C));\n}\n\nvoid matpow(long long A[2][2], int n) {\n    long long R[2][2] = {{1,0},{0,1}};\n    while(n){\n        if(n&1) matmul(R,A);\n        matmul(A,A);\n        n&gt;&gt;=1;\n    }\n    memcpy(A, R, sizeof(R));\n}\n\n\n8. Summary\n\n\n\n\n\n\n\n\nTechnique\nPurpose\nSpeedup\n\n\n\n\nAlgebraic Identities\nSimplify expressions\nConstant factor\n\n\nGenerating Functions\nSolve recurrences\nConceptual\n\n\nFFT / Convolution\nCombine sequences fast\n(O\\(n^2\\) O\\(n \\log n\\))\n\n\nPolynomial Interpolation\nReconstruction\n(O\\(n^2\\) O\\(n \\log^2 n\\))\n\n\nMatrix Tricks\nAccelerate recurrences\n(O(n) O\\(\\log n\\))\n\n\n\n\n\nWhy It Matters\nAlgebra turns computation into structure. By rewriting problems in algebraic form, you reveal hidden symmetries, exploit fast transforms, and find elegant solutions. It’s not magic , it’s the math beneath performance.\n\n“The smartest code is often the one that solves itself on paper first.”\n\n\n\nTry It Yourself\n\nMultiply two polynomials using FFT.\nRepresent Fibonacci as a matrix and compute \\(F_{100}\\).\nUse generating functions to count coin change ways.\nImplement subset sum via Walsh-Hadamard transform.\nDerive a recurrence and solve it algebraically.\n\nUnderstanding algebraic tricks makes you not just a coder, but a mathematical engineer , bending structure to will.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-7.-strings-and-text-algorithms",
    "href": "books/en-us/book.html#chapter-7.-strings-and-text-algorithms",
    "title": "The Book",
    "section": "Chapter 7. Strings and Text Algorithms",
    "text": "Chapter 7. Strings and Text Algorithms\n\n61. String Matching (KMP, Z, Rabin-Karp, Boyer-Moore)\nString matching is one of the oldest and most fundamental problems in computer science: given a text ( T ) of length ( n ) and a pattern ( P ) of length ( m ), find all positions where ( P ) appears in ( T ).\nThis section walks you through both naive and efficient algorithms , from the straightforward brute-force method to elegant linear-time solutions like KMP and Z-algorithm, and clever heuristics like Boyer-Moore and Rabin-Karp.\n\n1. Problem Setup\nWe’re given:\n\nText: \\(T = t_1 t_2 \\ldots t_n\\)- Pattern: \\(P = p_1 p_2 \\ldots p_m\\) Goal: find all ( i ) such that \\[\nT[i \\ldots i+m-1] = P[1 \\ldots m]\n\\]\n\nNaive solution: compare ( P ) with every substring of ( T ) Time complexity: ( O(nm) )\nWe’ll now see how to reduce it to ( O(n + m) ) or close.\n\n\n2. Knuth-Morris-Pratt (KMP)\nKMP avoids rechecking characters by precomputing overlaps within the pattern.\nIt builds a prefix-function (also called failure function), which tells how much to shift when a mismatch happens.\n\n\nA. Prefix Function\nFor each position ( i ), compute \\(\\pi[i]\\) = length of longest prefix that’s also a suffix of ( P[1..i] ).\nExample: Pattern ababc\n\n\n\ni\nP[i]\nπ[i]\n\n\n\n\n1\na\n0\n\n\n2\nb\n0\n\n\n3\na\n1\n\n\n4\nb\n2\n\n\n5\nc\n0\n\n\n\n\n\nB. Search Phase\nUse \\(\\pi[]\\) to skip mismatched prefixes in the text.\nTime Complexity: ( O(n + m) ) Space: ( O(m) )\nTiny Code (C)\nvoid compute_pi(char *p, int m, int pi[]) {\n    pi[0] = 0;\n    for (int i = 1, k = 0; i &lt; m; i++) {\n        while (k &gt; 0 && p[k] != p[i]) k = pi[k-1];\n        if (p[k] == p[i]) k++;\n        pi[i] = k;\n    }\n}\n\nvoid kmp_search(char *t, char *p) {\n    int n = strlen(t), m = strlen(p);\n    int pi[m]; compute_pi(p, m, pi);\n    for (int i = 0, k = 0; i &lt; n; i++) {\n        while (k &gt; 0 && p[k] != t[i]) k = pi[k-1];\n        if (p[k] == t[i]) k++;\n        if (k == m) {\n            printf(\"Found at %d\\n\", i - m + 1);\n            k = pi[k-1];\n        }\n    }\n}\n\n\n3. Z-Algorithm\nZ-algorithm computes the Z-array,\nwhere \\(Z[i]\\) = length of the longest substring starting at \\(i\\) that matches the prefix of \\(P\\).\nTo match \\(P\\) in \\(T\\), build the string:\n\\[\nS = P + \\# + T\n\\]\nThen every \\(i\\) where \\(Z[i] = |P|\\) corresponds to a match.\nTime: \\(O(n + m)\\)\nSimple and elegant.\nExample:\nP = \"aba\", T = \"ababa\"\nS = \"aba#ababa\"\nZ = [0,0,1,0,3,0,1,0]\nMatch at index 0, 2\n\n\n4. Rabin-Karp (Rolling Hash)\nInstead of comparing strings character-by-character, compute a hash for each window in ( T ), and compare hashes.\n\\[\nh(s_1s_2\\ldots s_m) = (s_1b^{m-1} + s_2b^{m-2} + \\ldots + s_m) \\bmod M\n\\]\nUse a rolling hash to update in ( O(1) ) per shift.\nTime: average ( O(n + m) ), worst ( O(nm) ) Good for multiple pattern search.\nTiny Code (Rolling Hash)\n#define B 256\n#define M 101\n\nvoid rabin_karp(char *t, char *p) {\n    int n = strlen(t), m = strlen(p);\n    int h = 1, pHash = 0, tHash = 0;\n    for (int i = 0; i &lt; m-1; i++) h = (h*B) % M;\n    for (int i = 0; i &lt; m; i++) {\n        pHash = (B*pHash + p[i]) % M;\n        tHash = (B*tHash + t[i]) % M;\n    }\n    for (int i = 0; i &lt;= n-m; i++) {\n        if (pHash == tHash && strncmp(&t[i], p, m) == 0)\n            printf(\"Found at %d\\n\", i);\n        if (i &lt; n-m)\n            tHash = (B*(tHash - t[i]*h) + t[i+m]) % M;\n        if (tHash &lt; 0) tHash += M;\n    }\n}\n\n\n5. Boyer-Moore (Heuristic Skipping)\nBoyer-Moore compares from right to left and uses two heuristics:\n\nBad Character Rule When mismatch at ( j ), shift pattern so next occurrence of ( T[i] ) in ( P ) aligns.\nGood Suffix Rule Shift pattern so a suffix of matched portion aligns with another occurrence.\n\nTime: ( O(n/m) ) on average Practical and fast, especially for English text.\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nIdea\nBest For\n\n\n\n\nNaive\n(O(nm))\n(O(1))\nDirect compare\nSimple cases\n\n\nKMP\n(O(n+m))\n(O(m))\nPrefix overlap\nGeneral use\n\n\nZ\n(O(n+m))\n(O(n+m))\nPrefix matching\nPattern prep\n\n\nRabin-Karp\n(O(n+m)) avg\n(O(1))\nHashing\nMulti-pattern\n\n\nBoyer-Moore\n(O(n/m)) avg\n(O\\(m+\\sigma\\))\nRight-to-left skip\nLong texts\n\n\n\n\n\nWhy It Matters\nString matching powers text editors, DNA search, spam filters, and search engines. These algorithms show how structure and clever preprocessing turn brute force into elegance.\n\n“To find is human, to match efficiently is divine.”\n\n\n\nTry It Yourself\n\nImplement KMP and print all matches in a sentence.\nUse Rabin-Karp to find multiple keywords.\nCompare running times on large text files.\nModify KMP for case-insensitive matching.\nVisualize prefix function computation step-by-step.\n\nBy mastering these, you’ll wield the foundation of pattern discovery , the art of finding order in streams of symbols.\n\n\n\n62. Multi-Pattern Search (Aho-Corasick)\nSo far, we’ve matched one pattern against a text. But what if we have many patterns , say, a dictionary of keywords , and we want to find all occurrences of all patterns in a single pass?\nThat’s where the Aho-Corasick algorithm shines. It builds a trie with failure links, turning multiple patterns into one efficient automaton. Think of it as “KMP for many words at once.”\n\n1. Problem Setup\nGiven:\n\nA text ( T ) of length ( n )- A set of patterns \\({ P_1, P_2, \\ldots, P_k }\\) with total length \\(m = \\sum |P_i|\\)\n\nGoal: find all occurrences of every \\(P_i\\) in ( T ).\nNaive solution: Run KMP for each pattern , ( O(kn) )\nBetter idea: Merge all patterns into a trie, and use failure links to transition on mismatches.\nAho-Corasick achieves O(n + m + z), where ( z ) = number of matches reported.\n\n\n2. Trie Construction\nEach pattern is inserted into a trie node-by-node.\nExample Patterns:\nhe, she, his, hers\nTrie:\n(root)\n ├─ h ─ e*\n │   └─ r ─ s*\n ├─ s ─ h ─ e*\n └─ h ─ i ─ s*\nEach node may mark an output (end of pattern).\n\n\n3. Failure Links\nFailure link of a node points to the longest proper suffix that’s also a prefix in the trie.\nThese links let us “fall back” like KMP.\nWhen mismatch happens, follow failure link to find next possible match.\n\n\nBuilding Failure Links (BFS)\n\nRoot’s failure = null\nChildren of root → failure = root\nBFS over nodes:\n\nFor each edge ( (u, c) → v ): follow failure links from ( u ) until you find ( f ) with edge ( c ) then \\(v.\\text{fail} = f.c\\)\n\n\n\n\nExample\nFor “he”, “she”, “his”, “hers”:\n\nfail(\"he\") = root- fail(\"hers\") = \"rs\" path invalid → fallback to \"s\" if exists So failure links connect partial suffixes.\n\n\n\n4. Matching Phase\nNow we can process the text in one pass:\nstate = root\nfor each character c in text:\n    while state has no child c and state != root:\n        state = state.fail\n    if state has child c:\n        state = state.child[c]\n    else:\n        state = root\n    if state.output:\n        report matches at this position\nEach transition costs O(1) amortized. No backtracking , fully linear time.\n\n\n5. Example Walkthrough\nPatterns: he, she, his, hers Text: ahishers\nAt each character:\na → root (no match)\nh → go to h\ni → go to hi\ns → go to his → output \"his\"\nh → fallback → h\ne → he → output \"he\"\nr → her → continue\ns → hers → output \"hers\"\nOutputs: \"his\", \"he\", \"hers\"\n\n\n6. Tiny Code (C Implementation Sketch)\n#define ALPHA 26\n\ntypedef struct Node {\n    struct Node *next[ALPHA];\n    struct Node *fail;\n    int out;\n} Node;\n\nNode* newNode() {\n    Node *n = calloc(1, sizeof(Node));\n    return n;\n}\n\nvoid insert(Node *root, char *p) {\n    for (int i = 0; p[i]; i++) {\n        int c = p[i] - 'a';\n        if (!root-&gt;next[c]) root-&gt;next[c] = newNode();\n        root = root-&gt;next[c];\n    }\n    root-&gt;out = 1;\n}\n\nvoid build_failures(Node *root) {\n    Node *q[10000];\n    int front=0, back=0;\n    root-&gt;fail = root;\n    q[back++] = root;\n    while (front &lt; back) {\n        Node *u = q[front++];\n        for (int c=0; c&lt;ALPHA; c++) {\n            Node *v = u-&gt;next[c];\n            if (!v) continue;\n            Node *f = u-&gt;fail;\n            while (f != root && !f-&gt;next[c]) f = f-&gt;fail;\n            if (f-&gt;next[c] && f-&gt;next[c] != v) v-&gt;fail = f-&gt;next[c];\n            else v-&gt;fail = root;\n            if (v-&gt;fail-&gt;out) v-&gt;out = 1;\n            q[back++] = v;\n        }\n    }\n}\n\n\n7. Complexity\n\n\n\nPhase\nTime\nSpace\n\n\n\n\nTrie Build\n( O(m) )\n( O(m) )\n\n\nFailure Links\n( O(m) )\n( O(m) )\n\n\nSearch\n( O(n + z) )\n( O(1) )\n\n\n\nTotal: O(n + m + z)\n\n\n8. Summary\n\n\n\nStep\nPurpose\n\n\n\n\nTrie\nMerge patterns\n\n\nFail Links\nHandle mismatches\n\n\nOutputs\nCollect matches\n\n\nBFS\nBuild efficiently\n\n\nOne Pass\nMatch all patterns\n\n\n\n\n\nWhy It Matters\nAho-Corasick is the core of:\n\nSpam filters- Intrusion detection (e.g., Snort IDS)- Keyword search in compilers- DNA sequence scanners It’s a masterclass in blending automata theory with practical efficiency.\n\n\n“Why search one word at a time when your algorithm can read the whole dictionary?”\n\n\n\nTry It Yourself\n\nBuild an automaton for words {“he”, “she”, “hers”} and trace it manually.\nModify code for uppercase letters.\nExtend to report overlapping matches.\nMeasure runtime vs. naive multi-search.\nVisualize the failure links in a graph.\n\nOnce you grasp Aho-Corasick, you’ll see pattern search not as a loop , but as a machine that reads and recognizes.\n\n\n\n63. Suffix Structures (Suffix Array, Suffix Tree, LCP)\nSuffix-based data structures are among the most powerful tools in string algorithms. They enable fast searching, substring queries, pattern matching, and lexicographic operations , all from one fundamental idea:\n\nRepresent all suffixes of a string in a structured form.\n\nIn this section, we explore three key constructs:\n\nSuffix Array (SA) - lexicographically sorted suffix indices- Longest Common Prefix (LCP) array - shared prefix lengths between neighbors- Suffix Tree - compressed trie of all suffixes Together, they power many advanced algorithms in text processing, bioinformatics, and compression.\n\n\n1. Suffix Array (SA)\nA suffix array stores all suffixes of a string in lexicographic order, represented by their starting indices.\nExample: String banana$ All suffixes:\n\n\n\nIndex\nSuffix\n\n\n\n\n0\nbanana$\n\n\n1\nanana$\n\n\n2\nnana$\n\n\n3\nana$\n\n\n4\nna$\n\n\n5\na$\n\n\n6\n$\n\n\n\nSort them:\n\n\n\nSorted Order\nSuffix\nIndex\n\n\n\n\n0\n$\n6\n\n\n1\na$\n5\n\n\n2\nana$\n3\n\n\n3\nanana$\n1\n\n\n4\nbanana$\n0\n\n\n5\nna$\n4\n\n\n6\nnana$\n2\n\n\n\nSuffix Array: [6, 5, 3, 1, 0, 4, 2]\n\n\nConstruction (Prefix Doubling)\nWe iteratively sort suffixes by first 2ⁱ characters, using radix sort on pairs of ranks.\nSteps:\n\nAssign initial rank by character.\nSort by (rank[i], rank[i+k]).\nRepeat doubling \\(k \\leftarrow 2k\\) until all ranks distinct.\n\nTime Complexity: ( O\\(n \\log n\\) ) Space: ( O(n) )\nTiny Code (C, Sketch)\ntypedef struct { int idx, rank[2]; } Suffix;\nint cmp(Suffix a, Suffix b) {\n    return (a.rank[0]==b.rank[0]) ? (a.rank[1]-b.rank[1]) : (a.rank[0]-b.rank[0]);\n}\n\nvoid buildSA(char *s, int n, int sa[]) {\n    Suffix suf[n];\n    for (int i = 0; i &lt; n; i++) {\n        suf[i].idx = i;\n        suf[i].rank[0] = s[i];\n        suf[i].rank[1] = (i+1&lt;n) ? s[i+1] : -1;\n    }\n    for (int k = 2; k &lt; 2*n; k *= 2) {\n        qsort(suf, n, sizeof(Suffix), cmp);\n        int r = 0, rank[n]; rank[suf[0].idx]=0;\n        for (int i=1;i&lt;n;i++) {\n            if (suf[i].rank[0]!=suf[i-1].rank[0] || suf[i].rank[1]!=suf[i-1].rank[1]) r++;\n            rank[suf[i].idx]=r;\n        }\n        for (int i=0;i&lt;n;i++){\n            suf[i].rank[0] = rank[suf[i].idx];\n            suf[i].rank[1] = (suf[i].idx+k/2&lt;n)?rank[suf[i].idx+k/2]:-1;\n        }\n    }\n    for (int i=0;i&lt;n;i++) sa[i]=suf[i].idx;\n}\n\n\n2. Longest Common Prefix (LCP)\nThe LCP array stores the length of the longest common prefix between consecutive suffixes in SA order.\nExample: banana$\n\n\n\nSA\nSuffix\nLCP\n\n\n\n\n6\n$\n0\n\n\n5\na$\n0\n\n\n3\nana$\n1\n\n\n1\nanana$\n3\n\n\n0\nbanana$\n0\n\n\n4\nna$\n0\n\n\n2\nnana$\n2\n\n\n\nSo LCP = [0,0,1,3,0,0,2]\n\n\nKasai’s Algorithm (Build in O(n))\nWe compute LCP in one pass using inverse SA:\nvoid buildLCP(char *s, int n, int sa[], int lcp[]) {\n    int rank[n];\n    for (int i=0;i&lt;n;i++) rank[sa[i]]=i;\n    int k=0;\n    for (int i=0;i&lt;n;i++) {\n        if (rank[i]==n-1) { k=0; continue; }\n        int j = sa[rank[i]+1];\n        while (i+k&lt;n && j+k&lt;n && s[i+k]==s[j+k]) k++;\n        lcp[rank[i]]=k;\n        if (k&gt;0) k--;\n    }\n}\nTime Complexity: ( O(n) )\n\n\n3. Suffix Tree\nA suffix tree is a compressed trie of all suffixes.\nEach edge holds a substring interval, not individual characters. This gives:\n\nConstruction in ( O(n) ) (Ukkonen’s algorithm)- Pattern search in ( O(m) )- Many advanced uses (e.g., longest repeated substring) Example: String: banana$ Suffix tree edges:\n\n(root)\n ├─ b[0:0] → ...\n ├─ a[1:1] → ...\n ├─ n[2:2] → ...\nEdges compress consecutive letters into intervals like [start:end].\n\n\nComparison\n\n\n\nStructure\nSpace\nBuild Time\nSearch\n\n\n\n\nSuffix Array\n( O(n) )\n( O\\(n \\log n\\) )\n( O\\(m \\log n\\) )\n\n\nLCP Array\n( O(n) )\n( O(n) )\nRange queries\n\n\nSuffix Tree\n( O(n) )\n( O(n) )\n( O(m) )\n\n\n\nSuffix Array + LCP ≈ compact Suffix Tree.\n\n\n4. Applications\n\nSubstring search - binary search in SA\nLongest repeated substring - max(LCP)\nLexicographic order - direct from SA\nDistinct substrings count = ( n(n+1)/2 - LCP[i] )\nPattern frequency - range query in SA using LCP\n\n\n\n5. Tiny Code (Search via SA)\nint searchSA(char *t, int n, char *p, int sa[]) {\n    int l=0, r=n-1, m=strlen(p);\n    while (l &lt;= r) {\n        int mid = (l+r)/2;\n        int cmp = strncmp(t+sa[mid], p, m);\n        if (cmp==0) return sa[mid];\n        else if (cmp&lt;0) l=mid+1;\n        else r=mid-1;\n    }\n    return -1;\n}\n\n\n6. Summary\n\n\n\nConcept\nPurpose\nComplexity\n\n\n\n\nSuffix Array\nSorted suffix indices\n( O\\(n \\log n\\) )\n\n\nLCP Array\nAdjacent suffix overlap\n( O(n) )\n\n\nSuffix Tree\nCompressed trie of suffixes\n( O(n) )\n\n\n\nTogether they form the core of advanced string algorithms.\n\n\nWhy It Matters\nSuffix structures reveal hidden order in strings. They turn raw text into searchable, analyzable data , ideal for compression, search engines, and DNA analysis.\n\n“All suffixes, perfectly sorted , the DNA of text.”\n\n\n\nTry It Yourself\n\nBuild suffix array for banana$ by hand.\nWrite code to compute LCP and longest repeated substring.\nSearch multiple patterns using binary search on SA.\nCount distinct substrings from SA + LCP.\nCompare SA-based vs. tree-based search performance.\n\nMastering suffix structures equips you to tackle problems that were once “too big” for brute force , now solvable with elegance and order.\n\n\n\n64. Palindromes and Periodicity (Manacher)\nPalindromes are symmetric strings that read the same forwards and backwards , like “level”, “racecar”, or “madam”. They arise naturally in text analysis, bioinformatics, and even in data compression.\nThis section introduces efficient algorithms to detect and analyze palindromic structure and periodicity in strings, including the legendary Manacher’s Algorithm, which finds all palindromic substrings in linear time.\n\n1. What Is a Palindrome?\nA string ( S ) is a palindrome if: \\[\nS[i] = S[n - i + 1] \\quad \\text{for all } i\n\\]\nExamples:\n\n\"abba\" is even-length palindrome- \"aba\" is odd-length palindrome A string may contain many palindromic substrings , our goal is to find all centers efficiently.\n\n\n\n2. Naive Approach\nFor each center (between characters or at characters), expand outward while characters match.\nfor each center c:\n    expand left, right while S[l] == S[r]\nComplexity: ( O\\(n^2\\) ) , too slow for large strings.\nWe need something faster , that’s where Manacher’s Algorithm steps in.\n\n\n3. Manacher’s Algorithm (O(n))\nManacher’s Algorithm finds the radius of the longest palindrome centered at each position in linear time.\nIt cleverly reuses previous computations using mirror symmetry and a current right boundary.\n\n\nStep-by-Step\n\nPreprocess string to handle even-length palindromes: Insert # between characters.\nExample:\nS = \"abba\"\nT = \"^#a#b#b#a#$\"\n(^ and $ are sentinels)\nMaintain:\n\nC: center of rightmost palindrome - R: right boundary - P[i]: palindrome radius at i\n\nFor each position i:\n\nmirror position mirror = 2*C - i - initialize P[i] = min(R - i, P[mirror]) - expand around i while characters match - if new palindrome extends past R, update C and R\n\nThe maximum value of P[i] gives the longest palindrome.\n\n\n\nExample\nS = \"abba\"\nT = \"^#a#b#b#a#$\"\nP = [0,0,1,0,3,0,3,0,1,0,0]\nLongest radius = 3 → \"abba\"\nTiny Code (C Implementation)\nint manacher(char *s) {\n    int n = strlen(s);\n    char t[2*n + 3];\n    int p[2*n + 3];\n    int m = 0;\n    t[m++] = '^';\n    for (int i=0;i&lt;n;i++) {\n        t[m++] = '#';\n        t[m++] = s[i];\n    }\n    t[m++] = '#'; t[m++] = '$';\n    t[m] = '\\0';\n    \n    int c = 0, r = 0, maxLen = 0;\n    for (int i=1; i&lt;m-1; i++) {\n        int mirror = 2*c - i;\n        if (i &lt; r)\n            p[i] = (r - i &lt; p[mirror]) ? (r - i) : p[mirror];\n        else p[i] = 0;\n        while (t[i + 1 + p[i]] == t[i - 1 - p[i]])\n            p[i]++;\n        if (i + p[i] &gt; r) {\n            c = i;\n            r = i + p[i];\n        }\n        if (p[i] &gt; maxLen) maxLen = p[i];\n    }\n    return maxLen;\n}\nTime Complexity: ( O(n) ) Space: ( O(n) )\n\n\n4. Periodicity and Repetition\nA string ( S ) has a period ( p ) if: \\[\nS[i] = S[i + p] \\text{ for all valid } i\n\\]\nExample: abcabcabc has period 3 (abc).\nChecking Periodicity:\n\nBuild prefix function (as in KMP).\nLet ( n = |S| ), \\(p = n - \\pi[n-1]\\).\nIf \\(n \\mod p = 0\\), period = ( p ).\n\nExample:\nS = \"ababab\"\nπ = [0,0,1,2,3,4]\np = 6 - 4 = 2\n6 mod 2 = 0 → periodic\nTiny Code (Check Periodicity)\nint period(char *s) {\n    int n = strlen(s), pi[n];\n    pi[0]=0;\n    for(int i=1,k=0;i&lt;n;i++){\n        while(k&gt;0 && s[k]!=s[i]) k=pi[k-1];\n        if(s[k]==s[i]) k++;\n        pi[i]=k;\n    }\n    int p = n - pi[n-1];\n    return (n % p == 0) ? p : n;\n}\n\n\n5. Applications\n\nPalindrome Queries: is substring ( S[l:r] ) palindrome? → precompute radii- Longest Palindromic Substring- DNA Symmetry Analysis- Pattern Compression / Period Detection- String Regularity Tests\n\n\n\n6. Summary\n\n\n\nConcept\nPurpose\nTime\n\n\n\n\nNaive Expand\nSimple palindrome check\n( O\\(n^2\\) )\n\n\nManacher\nLongest palindromic substring\n( O(n) )\n\n\nKMP Prefix\nPeriod detection\n( O(n) )\n\n\n\n\n\nWhy It Matters\nPalindromes reveal hidden symmetries. Manacher’s algorithm is a gem , a linear-time mirror-based solution to a quadratic problem.\n\n“In every word, there may hide a reflection.”\n\n\n\nTry It Yourself\n\nRun Manacher’s algorithm on \"abacdfgdcaba\".\nModify code to print all palindromic substrings.\nUse prefix function to find smallest period.\nCombine both to find palindromic periodic substrings.\nCompare runtime vs. naive expand method.\n\nUnderstanding palindromes and periodicity teaches how structure emerges from repetition , a central theme in all of algorithmic design.\n\n\n\n65. Edit Distance and Alignment\nEdit distance measures how different two strings are , the minimal number of operations needed to turn one into the other. It’s a cornerstone of spell checking, DNA sequence alignment, plagiarism detection, and fuzzy search.\nThe most common form is the Levenshtein distance, using:\n\nInsertion (add a character)- Deletion (remove a character)- Substitution (replace a character) We’ll also touch on alignment, which generalizes this idea with custom scoring and penalties.\n\n\n1. Problem Definition\nGiven two strings ( A ) and ( B ), find the minimum number of edits to convert \\(A \\to B\\).\nIf ( A = “kitten” ) ( B = “sitting” )\nOne optimal sequence:\nkitten → sitten (substitute 'k'→'s')\nsitten → sittin (substitute 'e'→'i')\nsittin → sitting (insert 'g')\nSo edit distance = 3.\n\n\n2. Dynamic Programming Solution\nLet \\(dp[i][j]\\) be the minimum edits to convert \\(A[0..i-1] \\to B[0..j-1]\\).\nRecurrence: \\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1], & \\text{if } A[i-1] = B[j-1], \\\\\n1 + \\min\\big(dp[i-1][j],\\, dp[i][j-1],\\, dp[i-1][j-1]\\big), & \\text{otherwise}\n\\end{cases}\n\\]\nWhere: - \\(dp[i-1][j]\\): delete from \\(A\\) - \\(dp[i][j-1]\\): insert into \\(A\\) - \\(dp[i-1][j-1]\\): substitute\nBase cases: \\[\ndp[0][j] = j,\\quad dp[i][0] = i\n\\]\nTime complexity: \\(O(|A||B|)\\)\n\n\nExample\nA = kitten, B = sitting\n\n\n\n\n“”\ns\ni\nt\nt\ni\nn\ng\n\n\n\n\n“”\n0\n1\n2\n3\n4\n5\n6\n7\n\n\nk\n1\n1\n2\n3\n4\n5\n6\n7\n\n\ni\n2\n2\n1\n2\n3\n4\n5\n6\n\n\nt\n3\n3\n2\n1\n2\n3\n4\n5\n\n\nt\n4\n4\n3\n2\n1\n2\n3\n4\n\n\ne\n5\n5\n4\n3\n2\n2\n3\n4\n\n\nn\n6\n6\n5\n4\n3\n3\n2\n3\n\n\n\nEdit distance = 3\nTiny Code (C)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#define MIN3(a,b,c) ((a&lt;b)?((a&lt;c)?a:c):((b&lt;c)?b:c))\n\nint edit_distance(char *A, char *B) {\n    int n = strlen(A), m = strlen(B);\n    int dp[n+1][m+1];\n    for (int i=0;i&lt;=n;i++) dp[i][0]=i;\n    for (int j=0;j&lt;=m;j++) dp[0][j]=j;\n    for (int i=1;i&lt;=n;i++)\n        for (int j=1;j&lt;=m;j++)\n            if (A[i-1]==B[j-1])\n                dp[i][j]=dp[i-1][j-1];\n            else\n                dp[i][j]=1+MIN3(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]);\n    return dp[n][m];\n}\n\nint main() {\n    printf(\"%d\\n\", edit_distance(\"kitten\",\"sitting\")); // 3\n}\n\n\n3. Space Optimization\nWe only need the previous row to compute the current row.\nSo,\nSpace complexity: \\(O(\\min(|A|, |B|))\\)\nint edit_distance_opt(char *A, char *B) {\n    int n=strlen(A), m=strlen(B);\n    int prev[m+1], curr[m+1];\n    for(int j=0;j&lt;=m;j++) prev[j]=j;\n    for(int i=1;i&lt;=n;i++){\n        curr[0]=i;\n        for(int j=1;j&lt;=m;j++){\n            if(A[i-1]==B[j-1]) curr[j]=prev[j-1];\n            else curr[j]=1+MIN3(prev[j], curr[j-1], prev[j-1]);\n        }\n        memcpy(prev,curr,sizeof(curr));\n    }\n    return prev[m];\n}\n\n\n4. Alignment\nAlignment shows which characters correspond between two strings. Used in bioinformatics (e.g., DNA sequence alignment).\nEach operation has a cost:\n\nMatch: 0\nMismatch: 1\nGap (insert/delete): 1 We fill the DP table similarly, but track choices to trace back alignment.\n\n\n\nExample Alignment\nA: kitten-\nB: sitt-ing\nWe can visualize the transformation path by backtracking dp table.\n\n\nScoring Alignment (General Form)\nWe can generalize: \\[\ndp[i][j] = \\min \\begin{cases}\ndp[i-1][j-1] + cost(A_i,B_j) \\\ndp[i-1][j] + gap \\\ndp[i][j-1] + gap\n\\end{cases}\n\\]\nUsed in Needleman-Wunsch (global alignment) and Smith-Waterman (local alignment).\n\n\n5. Variants\n\nDamerau-Levenshtein: adds transposition (swap adjacent chars)- Hamming Distance: only substitutions, equal-length strings- Weighted Distance: different operation costs- Local Alignment: only best matching substrings\n\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\nMethod\nOperations\nTime\nUse\n\n\n\n\nLevenshtein\ninsert, delete, replace\n(O(nm))\nSpell check, fuzzy search\n\n\nHamming\nsubstitution only\n(O(n))\nDNA, binary strings\n\n\nAlignment (Needleman-Wunsch)\nwith scoring\n(O(nm))\nBioinformatics\n\n\nLocal Alignment (Smith-Waterman)\nbest substring\n(O(nm))\nDNA regions\n\n\n\n\n\nWhy It Matters\nEdit distance transforms “difference” into data. It quantifies how far apart two strings are, enabling flexible, robust comparisons.\n\n“Similarity isn’t perfection , it’s the cost of becoming alike.”\n\n\n\nTry It Yourself\n\nCompute edit distance between “intention” and “execution”.\nTrace back operations to show alignment.\nModify costs (insertion=2, deletion=1, substitution=2) and compare results.\nImplement Hamming distance for equal-length strings.\nExplore Smith-Waterman for longest common substring.\n\nOnce you master edit distance, you can build tools that understand typos, align genomes, and search imperfectly , perfectly.\n\n\n\n66. Compression (Huffman, Arithmetic, LZ77, BWT)\nCompression algorithms let us encode information efficiently, reducing storage or transmission cost without losing meaning. They turn patterns and redundancy into shorter representations , the essence of data compression.\nThis section introduces the key families of lossless compression algorithms that form the backbone of formats like ZIP, PNG, and GZIP.\nWe’ll explore:\n\nHuffman Coding (prefix-free variable-length codes)\nArithmetic Coding (fractional interval encoding)\nLZ77 / LZ78 (dictionary-based methods)\nBurrows-Wheeler Transform (BWT) (reversible sorting transform)\n\n\n1. Huffman Coding\nHuffman coding assigns shorter codes to frequent symbols, and longer codes to rare ones , achieving optimal compression among prefix-free codes.\n\n\nA. Algorithm\n\nCount frequencies of all symbols.\nBuild a min-heap of nodes (symbol, freq).\nWhile heap size &gt; 1:\n\nExtract two smallest nodes a, b. - Create new node with freq = a.freq + b.freq. - Push back into heap.4. Assign 0 to left, 1 to right.\n\nTraverse tree , collect codes.\n\nEach symbol gets a unique prefix code (no code is prefix of another).\n\n\nB. Example\nText: ABRACADABRA\nFrequencies:\n\n\n\nSymbol\nCount\n\n\n\n\nA\n5\n\n\nB\n2\n\n\nR\n2\n\n\nC\n1\n\n\nD\n1\n\n\n\nBuilding tree gives codes like:\nA: 0  \nB: 101  \nR: 100  \nC: 1110  \nD: 1111\nEncoded text: 0 101 100 0 1110 0 1111 0 101 100 0 Compression achieved!\nTiny Code (C, Sketch)\ntypedef struct Node {\n    char ch;\n    int freq;\n    struct Node *left, *right;\n} Node;\nUse a min-heap (priority queue) to build the tree. Traverse recursively to print codewords.\nComplexity: (O\\(n \\log n\\))\n\n\n2. Arithmetic Coding\nInstead of mapping symbols to bit strings, arithmetic coding maps the entire message to a single number in [0,1).\nWe start with interval ([0,1)), and iteratively narrow it based on symbol probabilities.\n\n\nExample\nSymbols: {A: 0.5, B: 0.3, C: 0.2} Message: ABC\nIntervals:\nStart: [0, 1)\nA → [0, 0.5)\nB → [0.25, 0.4)\nC → [0.34, 0.37)\nFinal code = any number in [0.34, 0.37) (e.g. 0.35)\nDecoding reverses this process.\nAdvantage: achieves near-optimal entropy compression. Used in: JPEG2000, H.264\nTime Complexity: ( O(n) )\n\n\n3. LZ77 (Sliding Window Compression)\nLZ77 replaces repeated substrings with back-references (offset, length, next_char) pointing into a sliding window.\n\n\nExample\nText: abcabcabcx\nWindow slides; when abc repeats:\n(0,0,'a'), (0,0,'b'), (0,0,'c'),\n(3,3,'x')  // \"abc\" repeats from 3 chars back\nSo sequence is compressed as references to earlier substrings.\nUsed in: DEFLATE (ZIP, GZIP), PNG\nTime: ( O(n) ), Space: proportional to window size.\n\n\nTiny Code (Conceptual)\nstruct Token { int offset, length; char next; };\nSearch previous window for longest match before emitting token.\n\n\n4. LZ78 (Dictionary-Based)\nInstead of sliding window, LZ78 builds an explicit dictionary of substrings.\nAlgorithm:\n\nStart with empty dictionary.- Read input, find longest prefix in dictionary.- Output (index, next_char) and insert new entry. Example:\n\nInput: ABAABABAABAB\nOutput: (0,A), (0,B), (1,B), (2,A), (4,A), (3,B)\nUsed in: LZW (GIF, TIFF)\n\n\n5. Burrows-Wheeler Transform (BWT)\nBWT is not compression itself , it permutes text to cluster similar characters, making it more compressible by run-length or Huffman coding.\n\n\nSteps\n\nGenerate all rotations of string.\nSort them lexicographically.\nTake last column as output.\n\nExample: banana$\n\n\n\nRotations\nSorted\n\n\n\n\nbanana$\n\\(banana |\n| anana\\)b\n\n\na\\(banan   | na\\)bana\n\n\n\n\\(banana   | nana\\)ba\n\n\n\n\nLast column: annb$aa BWT(“banana\\(\") = \"annb\\)aa”\nReversible with index of original row.\nUsed in: bzip2, FM-index (bioinformatics)\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nIdea\nComplexity\nUse\n\n\n\n\nHuffman\nVariable-length prefix codes\n(O\\(n \\log n\\))\nGeneral compression\n\n\nArithmetic\nInterval encoding\n(O(n))\nNear-optimal entropy\n\n\nLZ77\nSliding window matches\n(O(n))\nZIP, PNG\n\n\nLZ78\nDictionary building\n(O(n))\nGIF, TIFF\n\n\nBWT\nPermute for clustering\n(O\\(n \\log n\\))\nbzip2\n\n\n\n\n\nWhy It Matters\nCompression algorithms reveal structure in data , they exploit patterns that humans can’t see. They’re also a window into information theory, showing how close we can get to the entropy limit.\n\n“To compress is to understand , every bit saved is a pattern found.”\n\n\n\nTry It Yourself\n\nBuild a Huffman tree for MISSISSIPPI.\nImplement a simple LZ77 encoder for repeating patterns.\nApply BWT and observe clustering of symbols.\nCompare Huffman and Arithmetic outputs on same input.\nExplore DEFLATE format combining LZ77 + Huffman.\n\nUnderstanding compression means learning to see redundancy , the key to efficient storage, transmission, and understanding itself.\n\n\n\n67. Cryptographic Hashes and Checksums\nIn algorithms, hashing helps us map data to fixed-size values. But when used for security and verification, hashing becomes a cryptographic tool. This section explores cryptographic hashes and checksums , algorithms that verify integrity, detect corruption, and secure data.\nWe’ll look at:\n\nSimple checksums (parity, CRC)- Cryptographic hash functions (MD5, SHA family, BLAKE3)- Properties like collision resistance and preimage resistance- Practical uses in verification, signing, and storage\n\n\n1. Checksums\nChecksums are lightweight methods to detect accidental errors in data (not secure against attackers). They’re used in filesystems, networking, and storage to verify integrity.\n\n\nA. Parity Bit\nAdds one bit to make total 1s even or odd. Used in memory or communication to detect single-bit errors.\nExample: Data = 1011 → has three 1s. Add parity bit 1 to make total 4 (even parity).\nLimitation: Only detects odd number of bit errors.\n\n\nB. Modular Sum (Simple Checksum)\nSum all bytes (mod 256 or 65536).\nTiny Code (C)\nuint8_t checksum(uint8_t *data, int n) {\n    uint32_t sum = 0;\n    for (int i = 0; i &lt; n; i++) sum += data[i];\n    return (uint8_t)(sum % 256);\n}\nUse: Simple file or packet validation.\n\n\nC. CRC (Cyclic Redundancy Check)\nCRCs treat bits as coefficients of a polynomial. Divide by a generator polynomial, remainder = CRC code.\nUsed in Ethernet, ZIP, and PNG.\nExample: CRC-32, CRC-16.\nFast hardware and table-driven implementations available.\nKey Property:\n\nDetects most burst errors- Not cryptographically secure\n\n\n\n2. Cryptographic Hash Functions\nA cryptographic hash function ( h(x) ) maps any input to a fixed-size output such that:\n\nDeterministic: same input → same output\nFast computation\nPreimage resistance: hard to find ( x ) given ( h(x) )\nSecond-preimage resistance: hard to find \\(x' \\neq x\\) with ( h(x’) = h(x) )\nCollision resistance: hard to find any two distinct inputs with same hash\n\n\n\n\nAlgorithm\nOutput (bits)\nNotes\n\n\n\n\nMD5\n128\nBroken (collisions found)\n\n\nSHA-1\n160\nDeprecated\n\n\nSHA-256\n256\nStandard (SHA-2 family)\n\n\nSHA-3\n256\nKeccak-based sponge\n\n\nBLAKE3\n256\nFast, parallel, modern\n\n\n\n\n\nExample\nh(\"hello\") = 2cf24dba5fb0a... (SHA-256)\nChange one letter, hash changes completely (avalanche effect):\nh(\"Hello\") = 185f8db32271f... \nEven small changes → big differences.\n\n\nTiny Code (C, using pseudo-interface)\n#include &lt;openssl/sha.h&gt;\n\nunsigned char hash[SHA256_DIGEST_LENGTH];\nSHA256((unsigned char*)\"hello\", 5, hash);\nPrint hash as hex string to verify.\n\n\n3. Applications\n\nData integrity: verify files (e.g., SHA256SUM)- Digital signatures: sign hashes, not raw data- Password storage: store hashes, not plaintext- Deduplication: detect identical files via hashes- Blockchain: link blocks with hash pointers- Git: stores objects via SHA-1 identifiers\n\n\n\n4. Hash Collisions\nA collision occurs when ( h(x) = h(y) ) for \\(x \\neq y\\). Good cryptographic hashes make this computationally infeasible.\nBy the birthday paradox, collisions appear after \\(2^{n/2}\\) operations for an ( n )-bit hash.\nHence, SHA-256 → ~\\(2^{128}\\) effort to collide.\n\n\n5. Checksums vs Hashes\n\n\n\nFeature\nChecksum\nCryptographic Hash\n\n\n\n\nGoal\nDetect errors\nEnsure integrity and authenticity\n\n\nResistance\nLow\nHigh\n\n\nOutput Size\nSmall\n128-512 bits\n\n\nPerformance\nVery fast\nFast but secure\n\n\nExample\nCRC32\nSHA-256, BLAKE3\n\n\n\n\n\nWhy It Matters\nChecksums catch accidental corruption, hashes protect against malicious tampering. Together, they guard the trustworthiness of data , the foundation of secure systems.\n\n“Integrity is invisible , until it’s lost.”\n\n\n\nTry It Yourself\n\nCompute CRC32 of a text file, flip one bit, and recompute.\nUse sha256sum to verify file integrity.\nExperiment: change one character in input, observe avalanche.\nCompare performance of SHA-256 and BLAKE3.\nResearch how Git uses SHA-1 to track versions.\n\nBy learning hashes, you master one of the pillars of security , proof that something hasn’t changed, even when everything else does.\n\n\n\n68. Approximate and Streaming Matching\nExact string matching (like KMP or Boyer-Moore) demands perfect alignment between pattern and text. But what if errors, noise, or incomplete data exist?\nThat’s where approximate matching and streaming matching come in. These algorithms let you search efficiently even when:\n\nThe pattern might contain typos or mutations- The text arrives in a stream (too large to store entirely)- You want to match “close enough,” not “exactly” They’re crucial in search engines, spell checkers, bioinformatics, and real-time monitoring systems.\n\n\n1. Approximate String Matching\nApproximate string matching finds occurrences of a pattern in a text allowing mismatches, insertions, or deletions , often measured by edit distance.\n\n\nA. Dynamic Programming (Levenshtein Distance)\nGiven two strings \\(A\\) and \\(B\\), the edit distance is the minimum number of insertions, deletions, or substitutions to turn \\(A\\) into \\(B\\).\nWe can build a DP table \\(dp[i][j]\\):\n\n\\(dp[i][0] = i\\) (delete all characters)\n\n\\(dp[0][j] = j\\) (insert all characters)\n\nIf \\(A[i] = B[j]\\), then \\(dp[i][j] = dp[i-1][j-1]\\)\n\nElse \\(dp[i][j] = 1 + \\min(dp[i-1][j],\\, dp[i][j-1],\\, dp[i-1][j-1])\\)\n\nTiny Code (C)\nint edit_distance(char *a, char *b) {\n    int n = strlen(a), m = strlen(b);\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= m; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= n; i++)\n        for (int j = 1; j &lt;= m; j++)\n            if (a[i-1] == b[j-1]) dp[i][j] = dp[i-1][j-1];\n            else dp[i][j] = 1 + fmin(fmin(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]);\n    return dp[n][m];\n}\nThis computes Levenshtein distance in ( O(nm) ) time.\n\n\nB. Bitap Algorithm (Shift-Or)\nWhen pattern length is small, Bitap uses bitmasks to track mismatches. It efficiently supports up to k errors and runs in near linear time for small patterns.\nUsed in grep -E, ag, and fuzzy searching systems.\nIdea: Maintain a bitmask where 1 = mismatch, 0 = match. Shift and OR masks as we scan text.\n\n\nC. k-Approximate Matching\nFind all positions where edit distance ≤ k. Efficient for small ( k ) (e.g., spell correction: edit distance ≤ 2).\nApplications:\n\nTypo-tolerant search- DNA sequence matching- Autocomplete systems\n\n\n\n2. Streaming Matching\nIn streaming, the text is too large or unbounded, so we must process input online. We can’t store everything , only summaries or sketches.\n\n\nA. Rolling Hash (Rabin-Karp style)\nMaintains a moving hash of recent characters. When new character arrives, update hash in ( O(1) ). Compare with pattern’s hash for possible match.\nGood for sliding window matching.\nExample:\nhash = (base * (hash - old_char * base^(m-1)) + new_char) % mod;\n\n\nB. Fingerprinting (Karp-Rabin Fingerprint)\nA compact representation of a substring. If fingerprints match, do full verification (avoid false positives). Used in streaming algorithms and chunking.\n\n\nC. Sketch-Based Matching\nAlgorithms like Count-Min Sketch or SimHash build summaries of large data. They help approximate similarity between streams.\nApplications:\n\nNear-duplicate detection (SimHash in Google)- Network anomaly detection- Real-time log matching\n\n\n\n3. Approximate Matching in Practice\n\n\n\n\n\n\n\n\nDomain\nUse Case\nAlgorithm\n\n\n\n\nSpell Checking\n“recieve” → “receive”\nEdit Distance\n\n\nDNA Alignment\nFind similar sequences\nSmith-Waterman\n\n\nAutocomplete\nSuggest close matches\nFuzzy Search\n\n\nLogs & Streams\nOnline pattern alerts\nStreaming Bitap, Karp-Rabin\n\n\nNear-Duplicate\nDetect similar text\nSimHash, MinHash\n\n\n\n\n\n4. Complexity\n\n\n\nAlgorithm\nTime\nSpace\nNotes\n\n\n\n\nLevenshtein DP\n(O(nm))\n(O(nm))\nExact distance\n\n\nBitap\n(O(n))\n(O(1))\nFor small patterns\n\n\nRolling Hash\n(O(n))\n(O(1))\nProbabilistic match\n\n\nSimHash\n(O(n))\n(O(1))\nApproximate similarity\n\n\n\n\n\nWhy It Matters\nReal-world data is messy , typos, noise, loss, corruption. Approximate matching lets you build algorithms that forgive errors and adapt to streams. It powers everything from search engines to genomics, ensuring your algorithms stay practical in an imperfect world.\n\n\nTry It Yourself\n\nCompute edit distance between “kitten” and “sitting.”\nImplement fuzzy search that returns words with ≤1 typo.\nUse rolling hash to detect repeated substrings in a stream.\nExperiment with SimHash to compare document similarity.\nObserve how small typos affect fuzzy vs exact search.\n\n\n\n\n69. Bioinformatics Alignment (Needleman-Wunsch, Smith-Waterman)\nIn bioinformatics, comparing DNA, RNA, or protein sequences is like comparing strings , but with biological meaning. Each sequence is made of letters (A, C, G, T for DNA; amino acids for proteins). To analyze similarity, scientists use sequence alignment algorithms that handle insertions, deletions, and substitutions.\nTwo fundamental methods dominate:\n\nNeedleman-Wunsch for global alignment- Smith-Waterman for local alignment\n\n\n1. Sequence Alignment\nAlignment means placing two sequences side by side to maximize matches and minimize gaps or mismatches.\nFor example:\nA C G T G A\n| | |   | |\nA C G A G A\nHere, mismatches and gaps may occur, but the alignment finds the best possible match under a scoring system.\n\n\nScoring System\nAlignment uses scores instead of just counts. Typical scheme:\n\nMatch: +1- Mismatch: -1- Gap (insertion or deletion): -2 You can adjust weights depending on the biological context.\n\n\n\n2. Needleman-Wunsch (Global Alignment)\nUsed when you want to align entire sequences , from start to end.\nIt uses dynamic programming to build a score table ( dp[i][j] ), where each cell represents the best score for aligning prefixes ( A[1..i] ) and ( B[1..j] ).\nRecurrence:\n\\[dp[i][j] = \\max\n\\begin{cases}\ndp[i-1][j-1] + \\text{score}(A_i, B_j) \\\ndp[i-1][j] + \\text{gap penalty} \\\ndp[i][j-1] + \\text{gap penalty}\n\\end{cases}\\]\nBase cases: \\[\ndp[0][j] = j \\times \\text{gap penalty}, \\quad dp[i][0] = i \\times \\text{gap penalty}\n\\]\nTiny Code (C)\nint max3(int a, int b, int c) {\n    return a &gt; b ? (a &gt; c ? a : c) : (b &gt; c ? b : c);\n}\n\nint needleman_wunsch(char *A, char *B, int match, int mismatch, int gap) {\n    int n = strlen(A), m = strlen(B);\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = i * gap;\n    for (int j = 0; j &lt;= m; j++) dp[0][j] = j * gap;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= m; j++) {\n            int s = (A[i-1] == B[j-1]) ? match : mismatch;\n            dp[i][j] = max3(dp[i-1][j-1] + s, dp[i-1][j] + gap, dp[i][j-1] + gap);\n        }\n    }\n    return dp[n][m];\n}\nExample:\nA = \"ACGT\"\nB = \"AGT\"\nmatch = +1, mismatch = -1, gap = -2\nProduces optimal alignment:\nA C G T\nA - G T\n\n\n3. Smith-Waterman (Local Alignment)\nUsed when sequences may have similar segments, not full-length similarity. Perfect for finding motifs or conserved regions.\nRecurrence is similar, but with local reset to zero:\n\\[dp[i][j] = \\max\n\\begin{cases}\n0, \\\ndp[i-1][j-1] + \\text{score}(A_i, B_j), \\\ndp[i-1][j] + \\text{gap penalty}, \\\ndp[i][j-1] + \\text{gap penalty}\n\\end{cases}\\]\nFinal answer = maximum value in the table (not necessarily at the end).\nIt finds the best substring alignment.\n\n\nExample\nA = \"ACGTTG\"\nB = \"CGT\"\nSmith-Waterman finds best local match:\nA C G T\n  | | |\n  C G T\nUnlike global alignment, extra prefixes or suffixes are ignored.\n\n\n4. Variants and Extensions\n\n\n\n\n\n\n\n\nAlgorithm\nType\nNotes\n\n\n\n\nNeedleman-Wunsch\nGlobal\nAligns full sequences\n\n\nSmith-Waterman\nLocal\nFinds similar subsequences\n\n\nGotoh Algorithm\nGlobal\nUses affine gap penalty (opening + extension)\n\n\nBLAST\nHeuristic\nSpeeds up search for large databases\n\n\n\nBLAST (Basic Local Alignment Search Tool) uses word seeds and extension, trading exactness for speed , essential for large genome databases.\n\n\n5. Complexity\nBoth Needleman-Wunsch and Smith-Waterman run in:\n\nTime: ( O(nm) )- Space: ( O(nm) ) But optimized versions use banded DP or Hirschberg’s algorithm to cut memory to ( O(n + m) ).\n\n\n\nWhy It Matters\nSequence alignment bridges computer science and biology. It’s how we:\n\nCompare species- Identify genes- Detect mutations- Trace ancestry- Build phylogenetic trees The idea of “minimum edit cost” echoes everywhere , from spell checkers to DNA analysis.\n\n\n“In biology, similarity is a story. Alignment is how we read it.”\n\n\n\nTry It Yourself\n\nImplement Needleman-Wunsch for short DNA sequences.\nChange gap penalties , see how alignment shifts.\nCompare outputs from global and local alignment.\nUse real sequences from GenBank to test.\nExplore BLAST online and compare to exact alignment results.\n\n\n\n\n70. Text Indexing and Search Structures\nWhen text becomes large , think books, databases, or the entire web , searching naively for patterns (O(nm)) is far too slow. We need indexing structures that let us search fast, often in O(m) or O(log n) time.\nThis section covers the backbone of search engines and string processing:\n\nSuffix Arrays- Suffix Trees- Inverted Indexes- Tries and Prefix Trees- Compressed Indexes like FM-Index (Burrows-Wheeler)\n\n\n1. Why Index?\nA text index is like a table of contents , it doesn’t store the book, but lets you jump straight to what you want.\nIf you have a text of length ( n ), and you’ll run many queries, it’s worth building an index (even if it costs ( O\\(n \\log n\\) ) to build).\nWithout indexing: each query takes ( O(nm) ). With indexing: each query can take ( O(m) ) or less.\n\n\n2. Suffix Array\nA suffix array is a sorted array of all suffixes of a string.\nFor text \"banana\", suffixes are:\n0: banana  \n1: anana  \n2: nana  \n3: ana  \n4: na  \n5: a\nSorted lexicographically:\n5: a  \n3: ana  \n1: anana  \n0: banana  \n4: na  \n2: nana\nSuffix Array = [5, 3, 1, 0, 4, 2]\nTo search, binary search over the suffix array using your pattern , ( O\\(m \\log n\\) ).\nTiny Code (C) (naive construction)\nint cmp(const void *a, const void *b, void *txt) {\n    int i = *(int*)a, j = *(int*)b;\n    return strcmp((char*)txt + i, (char*)txt + j);\n}\n\nvoid build_suffix_array(char *txt, int n, int sa[]) {\n    for (int i = 0; i &lt; n; i++) sa[i] = i;\n    qsort_r(sa, n, sizeof(int), cmp, txt);\n}\nModern methods like prefix doubling or radix sort build it in ( O\\(n \\log n\\) ).\nApplications:\n\nFast substring search- Longest common prefix (LCP) array- Pattern matching in DNA sequences- Plagiarism detection\n\n\n\n3. Suffix Tree\nA suffix tree is a compressed trie of all suffixes , each edge stores multiple characters.\nFor \"banana\", you’d build a tree where each leaf corresponds to a suffix index.\nAdvantages:\n\nPattern search in ( O(m) )- Space ( O(n) ) (with compression) Built using Ukkonen’s algorithm in ( O(n) ).\n\nUse Suffix Array + LCP as a space-efficient alternative.\n\n\n4. FM-Index (Burrows-Wheeler Transform)\nUsed in compressed full-text search (e.g., Bowtie, BWA). Combines:\n\nBurrows-Wheeler Transform (BWT)- Rank/select bitvectors Supports pattern search in O(m) time with very low memory.\n\nIdea: transform text so similar substrings cluster together, enabling compression and backward search.\nApplications:\n\nDNA alignment- Large text archives- Memory-constrained search\n\n\n\n5. Inverted Index\nUsed in search engines. Instead of suffixes, it indexes words.\nFor example, text corpus:\ndoc1: quick brown fox  \ndoc2: quick red fox\nInverted index:\n\"quick\" → [doc1, doc2]\n\"brown\" → [doc1]\n\"red\"   → [doc2]\n\"fox\"   → [doc1, doc2]\nNow searching “quick fox” becomes set intersection of lists.\nUsed with ranking functions (TF-IDF, BM25).\n\n\n6. Tries and Prefix Trees\nA trie stores strings character by character. Each node = prefix.\ntypedef struct Node {\n    struct Node *child[26];\n    int end;\n} Node;\nPerfect for:\n\nAutocomplete- Prefix search- Spell checkers Search: O(m), where m = pattern length.\n\nCompressed tries (Patricia trees) reduce space.\n\n\n7. Comparing Structures\n\n\n\nStructure\nSearch Time\nBuild Time\nSpace\nNotes\n\n\n\n\nTrie\nO(m)\nO(n)\nHigh\nPrefix queries\n\n\nSuffix Array\nO(m log n)\nO(n log n)\nMedium\nSorted suffixes\n\n\nSuffix Tree\nO(m)\nO(n)\nHigh\nRich structure\n\n\nFM-Index\nO(m)\nO(n)\nLow\nCompressed\n\n\nInverted Index\nO(k)\nO(N)\nMedium\nWord-based\n\n\n\n\n\nWhy It Matters\nText indexing is the backbone of search engines, DNA alignment, and autocomplete systems. Without it, Google searches, code lookups, or genome scans would take minutes, not milliseconds.\n\n“Indexing turns oceans of text into navigable maps.”\n\n\n\nTry It Yourself\n\nBuild a suffix array for “banana” and perform binary search for “ana.”\nConstruct a trie for a dictionary and query prefixes.\nWrite a simple inverted index for a few documents.\nCompare memory usage of suffix tree vs suffix array.\nExperiment with FM-index using an online demo (like BWT explorer).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-8.-geometry-graphics-and-spatial-algorithms",
    "href": "books/en-us/book.html#chapter-8.-geometry-graphics-and-spatial-algorithms",
    "title": "The Book",
    "section": "Chapter 8. Geometry, Graphics, and Spatial Algorithms",
    "text": "Chapter 8. Geometry, Graphics, and Spatial Algorithms\n\n71. Convex Hull (Graham, Andrew, Chan)\nIn computational geometry, the convex hull of a set of points is the smallest convex polygon that contains all the points. Intuitively, imagine stretching a rubber band around a set of nails on a board , the shape the band takes is the convex hull.\nConvex hulls are foundational for many geometric algorithms, like closest pair, Voronoi diagrams, and collision detection.\nIn this section, we’ll explore three classical algorithms:\n\nGraham Scan - elegant and simple (O(n log n))- Andrew’s Monotone Chain - robust and practical (O(n log n))- Chan’s Algorithm - advanced and optimal (O(n log h), where h = number of hull points)\n\n\n1. Definition\nGiven a set of points \\(P = {p_1, p_2, ..., p_n}\\), the convex hull, ( (P) ), is the smallest convex polygon enclosing all points.\nFormally: \\[\n\\text{CH}(P) = \\bigcap {C \\subseteq \\mathbb{R}^2 \\mid C \\text{ is convex and } P \\subseteq C }\n\\]\nA polygon is convex if every line segment between two points of the polygon lies entirely inside it.\n\n\n2. Orientation Test\nAll convex hull algorithms rely on an orientation test using cross product: Given three points ( a, b, c ):\n\\[\n\\text{cross}(a,b,c) = (b_x - a_x)(c_y - a_y) - (b_y - a_y)(c_x - a_x)\n\\]\n\n&gt; 0 → counter-clockwise turn- &lt; 0 → clockwise turn- = 0 → collinear\n\n\n\n3. Graham Scan\nOne of the earliest convex hull algorithms.\nIdea:\n\nPick the lowest point (and leftmost if tie).\nSort all other points by polar angle with respect to it.\nTraverse points and maintain a stack:\n\nAdd point - While last three points make a right turn, pop middle one4. Remaining points form convex hull in CCW order.\n\n\nTiny Code (C)\ntypedef struct { double x, y; } Point;\n\ndouble cross(Point a, Point b, Point c) {\n    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);\n}\n\nint cmp(const void *p1, const void *p2) {\n    Point *a = (Point*)p1, *b = (Point*)p2;\n    // Compare by polar angle or distance\n    return (a-&gt;y != b-&gt;y) ? (a-&gt;y - b-&gt;y) : (a-&gt;x - b-&gt;x);\n}\n\nint graham_scan(Point pts[], int n, Point hull[]) {\n    qsort(pts, n, sizeof(Point), cmp);\n    int top = 0;\n    for (int i = 0; i &lt; n; i++) {\n        while (top &gt;= 2 && cross(hull[top-2], hull[top-1], pts[i]) &lt;= 0)\n            top--;\n        hull[top++] = pts[i];\n    }\n    return top; // number of hull points\n}\nComplexity:\n\nSorting: ( O\\(n \\log n\\) )- Scanning: ( O(n) ) → Total: O(n log n)\n\n\n\nExample\nInput:\n(0, 0), (1, 1), (2, 2), (2, 0), (0, 2)\nHull (CCW):\n(0,0) → (2,0) → (2,2) → (0,2)\n\n\n4. Andrew’s Monotone Chain\nSimpler and more robust for floating-point coordinates. Builds lower and upper hulls separately.\nSteps:\n\nSort points lexicographically (x, then y).\nBuild lower hull (left-to-right)\nBuild upper hull (right-to-left)\nConcatenate (excluding duplicates)\n\nTiny Code (C)\nint monotone_chain(Point pts[], int n, Point hull[]) {\n    qsort(pts, n, sizeof(Point), cmp);\n    int k = 0;\n    // Lower hull\n    for (int i = 0; i &lt; n; i++) {\n        while (k &gt;= 2 && cross(hull[k-2], hull[k-1], pts[i]) &lt;= 0) k--;\n        hull[k++] = pts[i];\n    }\n    // Upper hull\n    for (int i = n-2, t = k+1; i &gt;= 0; i--) {\n        while (k &gt;= t && cross(hull[k-2], hull[k-1], pts[i]) &lt;= 0) k--;\n        hull[k++] = pts[i];\n    }\n    return k-1; // last point == first point\n}\nTime Complexity: ( O\\(n \\log n\\) )\n\n\n5. Chan’s Algorithm\nWhen \\(h \\ll n\\), Chan’s method achieves ( O\\(n \\log h\\) ):\n\nPartition points into groups of size ( m ).\nCompute hulls for each group (Graham).\nMerge hulls with Jarvis March (gift wrapping).\nChoose ( m ) cleverly (\\(m = 2^k\\)) to ensure ( O\\(n \\log h\\) ).\n\nUsed in: large-scale geometric processing.\n\n\n6. Applications\n\n\n\nDomain\nUse\n\n\n\n\nComputer Graphics\nShape boundary, hitboxes\n\n\nGIS / Mapping\nRegion boundaries\n\n\nRobotics\nObstacle envelopes\n\n\nClustering\nOutlier detection\n\n\nData Analysis\nMinimal bounding shape\n\n\n\n\n\n7. Complexity Summary\n\n\n\nAlgorithm\nTime\nSpace\nNotes\n\n\n\n\nGraham Scan\n( O\\(n \\log n\\) )\n( O(n) )\nSimple, classic\n\n\nMonotone Chain\n( O\\(n \\log n\\) )\n( O(n) )\nStable, robust\n\n\nChan’s Algorithm\n( O\\(n \\log h\\) )\n( O(n) )\nBest asymptotic\n\n\n\n\n\nWhy It Matters\nConvex hulls are one of the cornerstones of computational geometry. They teach sorting, cross products, and geometric reasoning , and form the basis for many spatial algorithms.\n\n“Every scattered set hides a simple shape. The convex hull is that hidden simplicity.”\n\n\n\nTry It Yourself\n\nImplement Graham Scan for 10 random points.\nPlot the points and verify the hull.\nCompare results with Andrew’s Monotone Chain.\nTest with collinear and duplicate points.\nExplore 3D convex hulls (QuickHull, Gift Wrapping) next.\n\n\n\n\n72. Closest Pair and Segment Intersection\nGeometric problems often ask: what’s the shortest distance between two points? or do these segments cross? These are classic building blocks in computational geometry , essential for collision detection, graphics, clustering, and path planning.\nThis section covers two foundational problems:\n\nClosest Pair of Points - find two points with minimum Euclidean distance- Segment Intersection - determine if (and where) two line segments intersect\n\n\n1. Closest Pair of Points\nGiven ( n ) points in 2D, find a pair with the smallest distance. The brute force solution is ( O\\(n^2\\) ), but using Divide and Conquer, we can solve it in O(n log n).\n\n\nA. Divide and Conquer Algorithm\nIdea:\n\nSort points by x-coordinate.\nSplit into left and right halves.\nRecursively find closest pairs in each half (distance = ( d )).\nMerge step: check pairs across the split line within ( d ).\n\nIn merge step, we only need to check at most 6 neighbors per point (by geometric packing).\nTiny Code (C, Sketch)\n#include &lt;math.h&gt;\ntypedef struct { double x, y; } Point;\n\ndouble dist(Point a, Point b) {\n    double dx = a.x - b.x, dy = a.y - b.y;\n    return sqrt(dx*dx + dy*dy);\n}\n\ndouble brute_force(Point pts[], int n) {\n    double d = 1e9;\n    for (int i = 0; i &lt; n; i++)\n        for (int j = i + 1; j &lt; n; j++)\n            d = fmin(d, dist(pts[i], pts[j]));\n    return d;\n}\nRecursive divide and merge:\ndouble closest_pair(Point pts[], int n) {\n    if (n &lt;= 3) return brute_force(pts, n);\n    int mid = n / 2;\n    double d = fmin(closest_pair(pts, mid),\n                    closest_pair(pts + mid, n - mid));\n    // merge step: check strip points within distance d\n    // sort by y, check neighbors\n    return d;\n}\nTime Complexity: ( O\\(n \\log n\\) )\nExample:\nPoints:\n(2,3), (12,30), (40,50), (5,1), (12,10), (3,4)\nClosest pair: (2,3) and (3,4), distance = √2\n\n\nB. Sweep Line Variant\nAnother method uses a line sweep and a balanced tree to keep active points. As you move from left to right, maintain a window of recent points within ( d ).\nUsed in large-scale spatial systems.\n\n\nApplications\n\n\n\nDomain\nUse\n\n\n\n\nClustering\nFind nearest neighbors\n\n\nRobotics\nAvoid collisions\n\n\nGIS\nNearest city search\n\n\nNetworking\nSensor proximity\n\n\n\n\n\n2. Segment Intersection\nGiven two segments ( AB ) and ( CD ), determine whether they intersect. It’s the core of geometry engines and vector graphics systems.\n\n\nA. Orientation Test\nWe use the cross product (orientation) test again. Two segments ( AB ) and ( CD ) intersect if and only if:\n\nThe segments straddle each other: \\[\n\\text{orient}(A, B, C) \\neq \\text{orient}(A, B, D)\n\\]\n\n\\[\n\\text{orient}(C, D, A) \\neq \\text{orient}(C, D, B)\n\\] 2. Special cases for collinear points (check bounding boxes).\nTiny Code (C)\ndouble cross(Point a, Point b, Point c) {\n    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);\n}\n\nint on_segment(Point a, Point b, Point c) {\n    return fmin(a.x, b.x) &lt;= c.x && c.x &lt;= fmax(a.x, b.x) &&\n           fmin(a.y, b.y) &lt;= c.y && c.y &lt;= fmax(a.y, b.y);\n}\n\nint intersect(Point a, Point b, Point c, Point d) {\n    double o1 = cross(a, b, c);\n    double o2 = cross(a, b, d);\n    double o3 = cross(c, d, a);\n    double o4 = cross(c, d, b);\n    if (o1*o2 &lt; 0 && o3*o4 &lt; 0) return 1; // general case\n    if (o1 == 0 && on_segment(a,b,c)) return 1;\n    if (o2 == 0 && on_segment(a,b,d)) return 1;\n    if (o3 == 0 && on_segment(c,d,a)) return 1;\n    if (o4 == 0 && on_segment(c,d,b)) return 1;\n    return 0;\n}\n\n\nB. Line Sweep Algorithm (Bentley-Ottmann)\nFor multiple segments, check all intersections efficiently. Algorithm:\n\nSort all endpoints by x-coordinate.\nSweep from left to right.\nMaintain active set (balanced BST).\nCheck neighboring segments for intersections.\n\nTime complexity: \\(O((n + k) \\log n)\\), where \\(k\\) is the number of intersections.\nUsed in CAD, map rendering, and collision systems.\n\n\n3. Complexity Summary\n\n\n\n\n\n\n\n\n\nProblem\nNaive\nOptimal\nTechnique\n\n\n\n\nClosest Pair\n\\(O(n^2)\\)\n\\(O(n \\log n)\\)\nDivide & Conquer\n\n\nSegment Intersection\n\\(O(n^2)\\)\n\\(O((n + k) \\log n)\\)\nSweep Line\n\n\n\n\n\nWhy It Matters\nGeometric algorithms like these teach how to reason spatially , blending math, sorting, and logic. They power real-world systems where precision matters: from self-driving cars to game engines.\n\n“Every point has a neighbor; every path may cross another , geometry finds the truth in space.”\n\n\n\nTry It Yourself\n\nImplement the closest pair algorithm using divide and conquer.\nVisualize all pairwise distances , see which pairs are minimal.\nTest segment intersection on random pairs.\nModify for 3D line segments using vector cross products.\nTry building a line sweep visualizer to catch intersections step-by-step.\n\n\n\n\n73. Line Sweep and Plane Sweep Algorithms\nThe sweep line (or plane sweep) technique is one of the most powerful paradigms in computational geometry. It transforms complex spatial problems into manageable one-dimensional events , by sweeping a line (or plane) across the input and maintaining a dynamic set of active elements.\nThis method underlies many geometric algorithms:\n\nEvent sorting → handle things in order- Active set maintenance → track current structure- Updates and queries → respond as the sweep progresses Used for intersection detection, closest pair, rectangle union, computational geometry in graphics and GIS.\n\n\n1. The Core Idea\nImagine a vertical line sweeping from left to right across the plane. At each “event” (like a point or segment endpoint), we update the set of objects the line currently touches , the active set.\nEach event may trigger queries, insertions, or removals.\nThis approach works because geometry problems often depend only on local relationships between nearby elements as the sweep advances.\n\n\nA. Sweep Line Template\nA general structure looks like this:\nstruct Event { double x; int type; Object *obj; };\nsort(events.begin(), events.end());\n\nActiveSet S;\n\nfor (Event e : events) {\n    if (e.type == START) S.insert(e.obj);\n    else if (e.type == END) S.erase(e.obj);\n    else if (e.type == QUERY) handle_query(S, e.obj);\n}\nSorting ensures events are processed in order of increasing x (or another dimension).\n\n\n2. Classic Applications\nLet’s explore three foundational problems solvable by sweep techniques.\n\n\nA. Segment Intersection (Bentley-Ottmann)\nGoal: detect all intersections among ( n ) line segments.\nSteps:\n\nSort endpoints by x-coordinate.\nSweep from left to right.\nMaintain an ordered set of active segments (sorted by y).\nWhen a new segment starts, check intersection with neighbors above and below.\nWhen segments intersect, record intersection and insert a new event at the x-coordinate of intersection.\n\nComplexity: \\(O((n + k)\\log n)\\), where \\(k\\) is the number of intersections.\n\n\nB. Closest Pair of Points\nSweep line version sorts by x, then slides a vertical line while maintaining active points within a strip of width ( d ) (current minimum). Only need to check at most 6-8 nearby points in strip.\nComplexity: ( O\\(n \\log n\\) )\n\n\nC. Rectangle Union Area\nGiven axis-aligned rectangles, compute total area covered.\nIdea:\n\nTreat vertical edges as events (entering/exiting rectangles).- Sweep line moves along x-axis.- Maintain y-intervals in active set (using a segment tree or interval tree).- At each step, multiply current width × height of union of active intervals. Complexity: ( O\\(n \\log n\\) )\n\nTiny Code Sketch (C)\ntypedef struct { double x, y1, y2; int type; } Event;\nEvent events[MAX];\nint n_events;\n\nqsort(events, n_events, sizeof(Event), cmp_by_x);\n\ndouble prev_x = events[0].x, area = 0;\nSegmentTree T;\n\nfor (int i = 0; i &lt; n_events; i++) {\n    double dx = events[i].x - prev_x;\n    area += dx * T.total_length(); // current union height\n    if (events[i].type == START)\n        T.insert(events[i].y1, events[i].y2);\n    else\n        T.remove(events[i].y1, events[i].y2);\n    prev_x = events[i].x;\n}\n\n\n3. Other Applications\n\n\n\n\n\n\n\n\nProblem\nDescription\nTime\n\n\n\n\nK-closest points\nMaintain top \\(k\\) in active set\n\\(O(n \\log n)\\)\n\n\nUnion of rectangles\nCompute covered area\n\\(O(n \\log n)\\)\n\n\nPoint location\nLocate point in planar subdivision\n\\(O(\\log n)\\)\n\n\nVisibility graph\nTrack visible edges\n\\(O(n \\log n)\\)\n\n\n\n\n\n4. Plane Sweep Extensions\nWhile line sweep moves in one dimension (x), plane sweep handles 2D or higher-dimensional spaces, where:\n\nEvents are 2D cells or regions.- Sweep front is a plane instead of a line. Used in 3D collision detection, computational topology, and CAD systems.\n\n\n\nConceptual Visualization\n\nSort events by one axis (say, x).\nMaintain structure (set, tree, or heap) of intersecting or active elements.\nUpdate at each event and record desired output (intersection, union, coverage).\n\nThe key is the locality principle: only neighbors in the sweep structure can change outcomes.\n\n\n5. Complexity\n\n\n\nPhase\nComplexity\n\n\n\n\nSorting events\n\\(O(n \\log n)\\)\n\n\nProcessing events\n\\(O(n \\log n)\\)\n\n\nTotal\n\\(O(n \\log n)\\) (typical)\n\n\n\n\n\nWhy It Matters\nThe sweep line method transforms geometric chaos into order , turning spatial relationships into sorted sequences. It’s the bridge between geometry and algorithms, blending structure with motion.\n\n“A sweep line sees everything , not all at once, but just in time.”\n\n\n\nTry It Yourself\n\nImplement a sweep-line segment intersection finder.\nCompute the union area of 3 rectangles with overlaps.\nAnimate the sweep line to visualize event processing.\nModify for circular or polygonal objects.\nExplore how sweep-line logic applies to time-based events in scheduling.\n\n\n\n\n74. Delaunay and Voronoi Diagrams\nIn geometry and spatial computing, Delaunay triangulations and Voronoi diagrams are duals , elegant structures that capture proximity, territory, and connectivity among points.\nThey’re used everywhere: from mesh generation, pathfinding, geospatial analysis, to computational biology. This section introduces both, their relationship, and algorithms to construct them efficiently.\n\n1. Voronoi Diagram\nGiven a set of sites (points) \\(P = {p_1, p_2, \\ldots, p_n}\\), the Voronoi diagram partitions the plane into regions , one per point , so that every location in a region is closer to its site than to any other.\nFormally, the Voronoi cell for \\(p_i\\) is: \\[\nV(p_i) = {x \\in \\mathbb{R}^2 \\mid d(x, p_i) \\le d(x, p_j), \\forall j \\neq i }\n\\]\nEach region is convex, and boundaries are formed by perpendicular bisectors.\n\n\nExample\nFor points ( A, B, C ):\n\nDraw bisectors between each pair.- Intersection points define Voronoi vertices.- Resulting polygons cover the plane, one per site. Used to model nearest neighbor regions , “which tower serves which area?”\n\n\n\nProperties\n\nEvery cell is convex.- Neighboring cells share edges.- The diagram’s vertices are centers of circumcircles through three sites.- Dual graph = Delaunay triangulation.\n\n\n\n2. Delaunay Triangulation\nThe Delaunay triangulation (DT) connects points so that no point lies inside the circumcircle of any triangle.\nEquivalently, it’s the dual graph of the Voronoi diagram.\nIt tends to avoid skinny triangles , maximizing minimum angles, creating well-shaped meshes.\n\n\nFormal Definition\nA triangulation ( T ) of ( P ) is Delaunay if for every triangle \\(\\triangle abc \\in T\\), no point \\(p \\in P \\setminus {a,b,c}\\) lies inside the circumcircle of \\(\\triangle abc\\).\nWhy It Matters:\n\nAvoids sliver triangles.- Used in finite element meshes, terrain modeling, and path planning.- Leads to natural neighbor interpolation and smooth surfaces.\n\n\n\n3. Relationship\nVoronoi and Delaunay are geometric duals:\n\n\n\nVoronoi\nDelaunay\n\n\n\n\nRegions = proximity zones\nTriangles = neighbor connections\n\n\nEdges = bisectors\nEdges = neighbor pairs\n\n\nVertices = circumcenters\nFaces = circumcircles\n\n\n\nConnecting neighboring Voronoi cells gives Delaunay edges.\n\n\n4. Algorithms\nSeveral algorithms can build these diagrams efficiently.\n\n\nA. Incremental Insertion\n\nStart with a super-triangle enclosing all points.\nInsert points one by one.\nRemove triangles whose circumcircle contains the point.\nRe-triangulate the resulting polygonal hole.\n\nTime Complexity: ( O\\(n^2\\) ), improved to ( O\\(n \\log n\\) ) with randomization.\n\n\nB. Divide and Conquer\n\nSort points by x.\nRecursively build DT for left and right halves.\nMerge by finding common tangents.\n\nTime Complexity: ( O\\(n \\log n\\) ) Elegant, structured, and deterministic.\n\n\nC. Fortune’s Sweep Line Algorithm\nFor Voronoi diagrams, Fortune’s algorithm sweeps a line from top to bottom. Maintains a beach line of parabolic arcs and event queue.\nEach event (site or circle) updates the structure , building Voronoi edges incrementally.\nTime Complexity: ( O\\(n \\log n\\) )\n\n\nD. Bowyer-Watson (Delaunay via Circumcircle Test)\nA practical incremental version widely used in graphics and simulation.\nSteps:\n\nStart with supertriangle- Insert point- Remove all triangles whose circumcircle contains point- Reconnect the resulting cavity\n\nTiny Code (Conceptual)\ntypedef struct { double x, y; } Point;\n\ntypedef struct { Point a, b, c; } Triangle;\n\nbool in_circle(Point a, Point b, Point c, Point p) {\n    double A[3][3] = {\n        {a.x - p.x, a.y - p.y, (a.x*a.x + a.y*a.y) - (p.x*p.x + p.y*p.y)},\n        {b.x - p.x, b.y - p.y, (b.x*b.x + b.y*b.y) - (p.x*p.x + p.y*p.y)},\n        {c.x - p.x, c.y - p.y, (c.x*c.x + c.y*c.y) - (p.x*p.x + p.y*p.y)}\n    };\n    return determinant(A) &gt; 0;\n}\nThis test ensures Delaunay property.\n\n\n5. Applications\n\n\n\nDomain\nApplication\n\n\n\n\nGIS\nNearest facility, region partition\n\n\nMesh Generation\nFinite element methods\n\n\nRobotics\nVisibility graphs, navigation\n\n\nComputer Graphics\nTerrain triangulation\n\n\nClustering\nSpatial neighbor structure\n\n\n\n\n\n6. Complexity Summary\n\n\n\nAlgorithm\nType\nTime\nNotes\n\n\n\n\nFortune\nVoronoi\n( O\\(n \\log n\\) )\nSweep line\n\n\nBowyer-Watson\nDelaunay\n( O\\(n \\log n\\) )\nIncremental\n\n\nDivide & Conquer\nDelaunay\n( O\\(n \\log n\\) )\nRecursive\n\n\n\n\n\nWhy It Matters\nVoronoi and Delaunay diagrams reveal natural structure in point sets. They convert distance into geometry, showing how space is divided and connected. If geometry is the shape of space, these diagrams are its skeleton.\n\n“Every point claims its territory; every territory shapes its network.”\n\n\n\nTry It Yourself\n\nDraw Voronoi regions for 5 random points by hand.\nBuild Delaunay triangles (connect neighboring sites).\nVerify the empty circumcircle property.\nUse a library (CGAL / SciPy) to visualize both structures.\nExplore how adding new points reshapes the diagrams.\n\n\n\n\n75. Point in Polygon and Polygon Triangulation\nGeometry often asks two fundamental questions:\n\nIs a point inside or outside a polygon?\nHow can a complex polygon be broken into triangles for computation?\n\nThese are the building blocks of spatial analysis, computer graphics, and computational geometry.\n\n1. Point in Polygon (PIP)\nGiven a polygon defined by vertices ( \\(x_1, y_1\\), \\(x_2, y_2\\), , \\(x_n, y_n\\) ) and a test point ( (x, y) ), we want to determine if the point lies inside, on the boundary, or outside the polygon.\n\n\nMethods\n\n\nA. Ray Casting Algorithm\nShoot a ray horizontally to the right of the point. Count how many times it intersects polygon edges.\n\nOdd count → Inside- Even count → Outside This is based on the even-odd rule.\n\nTiny Code (Ray Casting in C)\nbool point_in_polygon(Point p, Point poly[], int n) {\n    bool inside = false;\n    for (int i = 0, j = n - 1; i &lt; n; j = i++) {\n        if (((poly[i].y &gt; p.y) != (poly[j].y &gt; p.y)) &&\n            (p.x &lt; (poly[j].x - poly[i].x) * \n                   (p.y - poly[i].y) / \n                   (poly[j].y - poly[i].y) + poly[i].x))\n            inside = !inside;\n    }\n    return inside;\n}\nThis toggles inside every time a crossing is found.\n\n\nB. Winding Number Algorithm\nCounts how many times the polygon winds around the point.\n\nNonzero winding number → Inside- Zero → Outside More robust for complex polygons with holes or self-intersections.\n\n\n\n\nMethod\nTime Complexity\nRobustness\n\n\n\n\nRay Casting\n(O(n))\nSimple, may fail on edge cases\n\n\nWinding Number\n(O(n))\nMore accurate for complex shapes\n\n\n\n\n\nEdge Cases\nHandle:\n\nPoints on edges or vertices- Horizontal edges (special treatment to avoid double counting) Numerical precision is key.\n\n\n\nApplications\n\nHit testing in computer graphics- GIS spatial queries- Collision detection\n\n\n\n2. Polygon Triangulation\nA polygon triangulation divides a polygon into non-overlapping triangles whose union equals the polygon.\nWhy triangulate?\n\nTriangles are simple, stable, and efficient for rendering and computation.- Used in graphics pipelines, area computation, physics, and mesh generation.\n\n\n\nA. Triangulation Basics\nFor a simple polygon with ( n ) vertices,\n\nAlways possible- Always yields ( n - 2 ) triangles Goal: Find a triangulation efficiently and stably.\n\n\n\nB. Ear Clipping Algorithm\nAn intuitive and widely used method for triangulation.\n\n\nIdea\n\nFind an ear: a triangle formed by three consecutive vertices ( \\(v_{i-1}, v_i, v_{i+1}\\) ) such that:\n\nIt is convex - Contains no other vertex inside\n\nClip the ear (remove vertex \\(v_i\\))\nRepeat until only one triangle remains\n\nTime Complexity: ( O\\(n^2\\) )\nTiny Code (Ear Clipping Sketch)\nwhile (n &gt; 3) {\n    for (i = 0; i &lt; n; i++) {\n        if (is_ear(i)) {\n            add_triangle(i-1, i, i+1);\n            remove_vertex(i);\n            break;\n        }\n    }\n}\nHelper is_ear() checks convexity and emptiness.\n\n\nC. Dynamic Programming for Convex Polygons\nIf the polygon is convex, use DP triangulation:\n\\[\ndp[i][j] = \\min_{k \\in (i,j)} dp[i][k] + dp[k][j] + cost(i, j, k)\n\\]\nCost: perimeter or area (for minimum-weight triangulation)\nTime Complexity: ( O\\(n^3\\) ) Space: ( O\\(n^2\\) )\n\n\nD. Divide and Conquer\nRecursively split polygon and triangulate sub-polygons. Useful for convex or near-convex shapes.\n\n\n\nAlgorithm\nTime\nNotes\n\n\n\n\nEar Clipping\n(O\\(n^2\\))\nSimple polygons\n\n\nDP Triangulation\n(O\\(n^3\\))\nWeighted cost\n\n\nConvex Polygon\n(O(n))\nStraightforward\n\n\n\n\n\n3. Applications\n\n\n\nDomain\nUsage\n\n\n\n\nComputer Graphics\nRendering, rasterization\n\n\nComputational Geometry\nArea computation, integration\n\n\nFinite Element Analysis\nMesh subdivision\n\n\nRobotics\nPath planning, map decomposition\n\n\n\n\n\nWhy It Matters\nPoint-in-polygon answers where you are. Triangulation tells you how space is built. Together, they form the foundation of geometric reasoning.\n\n“From a single point to a thousand triangles, geometry turns space into structure.”\n\n\n\nTry It Yourself\n\nDraw a non-convex polygon and test random points using the ray casting rule.\nImplement the ear clipping algorithm for a simple polygon.\nVisualize how each step removes an ear and simplifies the shape.\nCompare triangulation results for convex vs concave shapes.\n\n\n\n\n76. Spatial Data Structures (KD, R-tree)\nWhen working with geometric data, points, rectangles, or polygons, efficient lookup and organization are crucial. Spatial data structures are designed to answer queries like:\n\nWhich objects are near a given point?- Which shapes intersect a region?- What’s the nearest neighbor? They form the backbone of computational geometry, computer graphics, GIS, and search systems.\n\n\n1. Motivation\nBrute force approaches that check every object have ( O(n) ) or worse performance. Spatial indexing structures, like KD-Trees and R-Trees, enable efficient range queries, nearest neighbor searches, and spatial joins.\n\n\n2. KD-Tree (k-dimensional tree)\nA KD-tree is a binary tree that recursively partitions space using axis-aligned hyperplanes.\nEach node splits the data by one coordinate axis (x, y, z, …).\n\n\nStructure\n\nEach node represents a point.- Each level splits by a different axis (x, y, x, y, …).- Left child contains points with smaller coordinate.- Right child contains larger coordinate.\n\nTiny Code (KD-tree Construction in 2D)\ntypedef struct {\n    double x, y;\n} Point;\n\nint axis; // 0 for x, 1 for y\n\nKDNode* build(Point points[], int n, int depth) {\n    if (n == 0) return NULL;\n    axis = depth % 2;\n    int mid = n / 2;\n    nth_element(points, points + mid, points + n, compare_by_axis);\n    KDNode* node = new_node(points[mid]);\n    node-&gt;left  = build(points, mid, depth + 1);\n    node-&gt;right = build(points + mid + 1, n - mid - 1, depth + 1);\n    return node;\n}\nSearch Complexity:\n\nAverage: ( O\\(\\log n\\) )- Worst-case: ( O(n) )\n\n\n\nQueries\n\nRange query: Find points in a region.- Nearest neighbor: Search branches that might contain closer points.- K-nearest neighbors: Use priority queues.\n\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nEfficient for static data\nCostly updates\n\n\nGood for low dimensions\nDegrades with high dimensions\n\n\n\n\n\nApplications\n\nNearest neighbor in ML- Collision detection- Clustering (e.g., k-means acceleration)\n\n\n\n3. R-Tree (Rectangle Tree)\nAn R-tree is a height-balanced tree for rectangular bounding boxes. It’s the spatial analog of a B-tree.\n\n\nIdea\n\nStore objects or bounding boxes in leaf nodes.- Internal nodes store MBRs (Minimum Bounding Rectangles) that cover child boxes.- Query by traversing overlapping MBRs.\n\nTiny Code (R-Tree Node Sketch)\ntypedef struct {\n    Rectangle mbr;\n    Node* children[MAX_CHILDREN];\n    int count;\n} Node;\nInsertion chooses the child whose MBR expands least to accommodate the new entry.\n\n\nOperations\n\nInsert: Choose subtree → Insert → Adjust MBRs- Search: Descend into nodes whose MBR intersects query- Split: When full, use heuristics (linear, quadratic, R*-Tree) Complexity:\nQuery: ( O\\(\\log n\\) )- Insert/Delete: ( O\\(\\log n\\) ) average\n\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nSupports dynamic data\nOverlaps can degrade performance\n\n\nIdeal for rectangles\nComplex split rules\n\n\n\n\n\nVariants\n\nR*-Tree: Optimized reinsertion, better packing- R+ Tree: Non-overlapping partitions- Hilbert R-Tree: Uses space-filling curves\n\n\n\n4. Comparison\n\n\n\nFeature\nKD-Tree\nR-Tree\n\n\n\n\nData Type\nPoints\nRectangles / Regions\n\n\nDimensionality\nLow (2-10)\nMedium\n\n\nUse Case\nNN, range queries\nSpatial joins, overlap queries\n\n\nUpdates\nExpensive\nDynamic-friendly\n\n\nBalance\nRecursive median\nB-tree-like\n\n\n\n\n\n5. Other Spatial Structures\n\n\n\nStructure\nDescription\n\n\n\n\nQuadtree\nRecursive 2D subdivision into 4 quadrants\n\n\nOctree\n3D analog of quadtree\n\n\nBSP Tree\nBinary partition using arbitrary planes\n\n\nGrid Index\nDivide space into uniform grid cells\n\n\n\n\n\n6. Applications\n\n\n\nDomain\nUsage\n\n\n\n\nGIS\nRegion queries, map intersections\n\n\nGraphics\nRay tracing acceleration\n\n\nRobotics\nCollision and path planning\n\n\nML\nNearest neighbor search\n\n\nDatabases\nSpatial indexing\n\n\n\n\n\nWhy It Matters\nSpatial structures turn geometry into searchable data. They enable efficient algorithms for where and what’s near, vital for real-time systems.\n\n“Divide space wisely, and queries become whispers instead of shouts.”\n\n\n\nTry It Yourself\n\nBuild a KD-tree for 10 random 2D points.\nImplement nearest neighbor search.\nInsert rectangles into a simple R-tree and query intersection with a bounding box.\nCompare query time vs brute force.\n\n\n\n\n77. Rasterization and Scanline Techniques\nWhen you draw shapes on a screen, triangles, polygons, circles, they must be converted into pixels. This conversion is called rasterization. It’s the bridge between geometric math and visible images.\nRasterization and scanline algorithms are foundational to computer graphics, game engines, and rendering pipelines.\n\n1. What Is Rasterization?\nRasterization transforms vector shapes (continuous lines and surfaces) into discrete pixels on a grid.\nFor example, a triangle defined by vertices (x1, y1), (x2, y2), (x3, y3) must be filled pixel by pixel.\n\n\n2. Core Idea\nEach shape (line, polygon, circle) is sampled over a grid. The algorithm decides which pixels are inside, on, or outside the shape.\nA rasterizer answers:\n\nWhich pixels should be lit?- What color or depth should each pixel have?\n\n\n\n3. Line Rasterization (Bresenham’s Algorithm)\nA classic method for drawing straight lines with integer arithmetic.\nKey Idea: Move from one pixel to the next, choosing the pixel closest to the true line path.\nvoid draw_line(int x0, int y0, int x1, int y1) {\n    int dx = abs(x1 - x0), dy = abs(y1 - y0);\n    int sx = (x0 &lt; x1) ? 1 : -1;\n    int sy = (y0 &lt; y1) ? 1 : -1;\n    int err = dx - dy;\n    while (true) {\n        plot(x0, y0); // draw pixel\n        if (x0 == x1 && y0 == y1) break;\n        int e2 = 2 * err;\n        if (e2 &gt; -dy) { err -= dy; x0 += sx; }\n        if (e2 &lt; dx) { err += dx; y0 += sy; }\n    }\n}\nWhy it works: Bresenham avoids floating-point math and keeps the line visually continuous.\n\n\n4. Polygon Rasterization\nTo fill shapes, we need scanline algorithms, they sweep a horizontal line (y-axis) across the shape and fill pixels in between edges.\n\n\nScanline Fill Steps\n\nSort edges by their y-coordinates.\nScan each line (y).\nFind intersections with polygon edges.\nFill between intersection pairs.\n\nThis guarantees correct filling for convex and concave polygons.\n\n\nExample (Simple Triangle Rasterization)\nfor (int y = y_min; y &lt;= y_max; y++) {\n    find all x-intersections with polygon edges;\n    sort x-intersections;\n    for (int i = 0; i &lt; count; i += 2)\n        draw_line(x[i], y, x[i+1], y);\n}\n\n\n5. Circle Rasterization (Midpoint Algorithm)\nUse symmetry, a circle is symmetric in 8 octants.\nEach step calculates the error term to decide whether to move horizontally or diagonally.\nvoid draw_circle(int xc, int yc, int r) {\n    int x = 0, y = r, d = 3 - 2 * r;\n    while (y &gt;= x) {\n        plot_circle_points(xc, yc, x, y);\n        x++;\n        if (d &gt; 0) { y--; d += 4 * (x - y) + 10; }\n        else d += 4 * x + 6;\n    }\n}\n\n\n6. Depth and Shading\nIn 3D graphics, rasterization includes depth testing (Z-buffer) and color interpolation. Each pixel stores its depth; new pixels overwrite only if closer.\nInterpolated shading (Gouraud, Phong) computes smooth color transitions across polygons.\n\n\n7. Hardware Rasterization\nModern GPUs perform rasterization in parallel:\n\nVertex Shader → Projection- Rasterizer → Pixel Grid- Fragment Shader → Color & Depth Each pixel is processed in fragment shaders for lighting, texture, and effects.\n\n\n\n8. Optimizations\n\n\n\nTechnique\nPurpose\n\n\n\n\nBounding Box Clipping\nSkip off-screen regions\n\n\nEarly Z-Culling\nDiscard hidden pixels early\n\n\nEdge Functions\nFast inside-test for triangles\n\n\nBarycentric Coordinates\nInterpolate depth/color smoothly\n\n\n\n\n\n9. Why It Matters\nRasterization turns math into imagery. It’s the foundation of all visual computing, renderers, CAD, games, and GUIs. Even with ray tracing rising, rasterization remains dominant for real-time rendering.\n\n“Every pixel you see began as math, it’s just geometry painted by light.”\n\n\n\n10. Try It Yourself\n\nImplement Bresenham’s algorithm for lines.\nWrite a scanline polygon fill for triangles.\nExtend it with color interpolation using barycentric coordinates.\nCompare performance vs brute force (looping over all pixels).\n\n\n\n\n78. Computer Vision (Canny, Hough, SIFT)\nComputer vision is where algorithms learn to see, to extract structure, shape, and meaning from images. Behind every object detector, edge map, and keypoint matcher lies a handful of powerful geometric algorithms.\nIn this section, we explore four pillars of classical vision: Canny edge detection, Hough transform, and SIFT (Scale-Invariant Feature Transform).\n\n1. The Vision Pipeline\nMost vision algorithms follow a simple pattern:\n\nInput: Raw pixels (grayscale or color)\nPreprocess: Smoothing or filtering\nFeature extraction: Edges, corners, blobs\nDetection or matching: Shapes, keypoints\nInterpretation: Object recognition, tracking\n\nCanny, Hough, and SIFT live in the feature extraction and detection stages.\n\n\n2. Canny Edge Detector\nEdges mark places where intensity changes sharply, the outlines of objects. The Canny algorithm (1986) is one of the most robust and widely used edge detectors.\n\n\nSteps\n\nSmoothing: Apply Gaussian blur to reduce noise.\nGradient computation:\n\nCompute \\(G_x\\) and \\(G_y\\) via Sobel filters\n\nGradient magnitude: \\(G = \\sqrt{G_x^2 + G_y^2}\\)\n\nGradient direction: \\(\\theta = \\tan^{-1}\\frac{G_y}{G_x}\\)\n\nNon-maximum suppression:\n\nKeep only local maxima along the gradient direction\n\nDouble thresholding:\n\nStrong edges (high gradient)\n\nWeak edges (connected to strong ones)\n\nEdge tracking by hysteresis:\n\nConnect weak edges linked to strong edges\n\n\n\n\nTiny Code (Pseudocode)\nImage canny(Image input) {\n    Image smoothed = gaussian_blur(input);\n    Gradient grad = sobel(smoothed);\n    Image suppressed = non_max_suppression(grad);\n    Image edges = hysteresis_threshold(suppressed, low, high);\n    return edges;\n}\n\n\nWhy Canny Works\nCanny maximizes three criteria:\n\nGood detection (low false negatives)\nGood localization (edges close to true edges)\nSingle response (no duplicates)\n\nIt’s a careful balance between sensitivity and stability.\n\n\n3. Hough Transform\nCanny finds edge points, Hough connects them into shapes.\nThe Hough transform detects lines, circles, and other parametric shapes using voting in parameter space.\n\n\nLine Detection\nEquation of a line: \\[\n\\rho = x\\cos\\theta + y\\sin\\theta\n\\]\nEach edge point votes for all (\\(\\rho, \\theta\\)) combinations it could belong to. Peaks in the accumulator array correspond to strong lines.\nTiny Code (Hough Transform)\nfor each edge point (x, y):\n  for theta in [0, 180):\n    rho = x*cos(theta) + y*sin(theta);\n    accumulator[rho, theta]++;\nThen pick (\\(\\rho, \\theta\\)) with highest votes.\n\n\nCircle Detection\nUse 3D accumulator \\(center_x, center_y, radius\\). Each edge pixel votes for possible circle centers.\n\n\nApplications\n\nLane detection in self-driving- Shape recognition (circles, ellipses)- Document analysis (lines, grids)\n\n\n\n4. SIFT (Scale-Invariant Feature Transform)\nSIFT finds keypoints that remain stable under scale, rotation, and illumination changes.\nIt’s widely used for image matching, panoramas, 3D reconstruction, and object recognition.\n\n\nSteps\n\nScale-space extrema detection\n\nUse Difference of Gaussians (DoG) across scales. - Detect maxima/minima in space-scale neighborhood.2. Keypoint localization\nRefine keypoint position and discard unstable ones.3. Orientation assignment\nAssign dominant gradient direction.4. Descriptor generation\nBuild a 128D histogram of gradient orientations in a local patch.\n\n\nTiny Code (Outline)\nfor each octave:\n  build scale-space pyramid\n  find DoG extrema\n  localize keypoints\n  assign orientations\n  compute 128D descriptor\n\n\nProperties\n\n\n\nProperty\nDescription\n\n\n\n\nScale Invariant\nDetects features at multiple scales\n\n\nRotation Invariant\nUses local orientation\n\n\nRobust\nHandles lighting, noise, affine transforms\n\n\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nOutput\nRobustness\n\n\n\n\nCanny\nEdge detection\nBinary edge map\nSensitive to thresholds\n\n\nHough\nShape detection\nLines, circles\nNeeds clean edges\n\n\nSIFT\nFeature detection\nKeypoints, descriptors\nVery robust\n\n\n\n\n\n6. Applications\n\n\n\nDomain\nUse Case\n\n\n\n\nRobotics\nVisual SLAM, localization\n\n\nAR / VR\nMarker tracking\n\n\nSearch\nImage matching\n\n\nMedical\nEdge segmentation\n\n\nIndustry\nQuality inspection\n\n\n\n\n\n7. Modern Successors\n\nORB (FAST + BRIEF): Efficient for real-time- SURF: Faster SIFT alternative- Harris / FAST: Corner detectors- Deep features: CNN-based descriptors\n\n\n\nWhy It Matters\nThese algorithms gave machines their first eyes, before deep learning, they were how computers recognized structure. Even today, they’re used in preprocessing, embedded systems, and hybrid pipelines.\n\n“Before neural nets could dream, vision began with gradients, geometry, and votes.”\n\n\n\nTry It Yourself\n\nImplement Canny using Sobel and hysteresis.\nUse Hough transform to detect lines in a synthetic image.\nTry OpenCV SIFT to match keypoints between two rotated images.\nCompare edge maps before and after Gaussian blur.\n\n\n\n\n79. Pathfinding in Space (A*, RRT, PRM)\nWhen navigating a maze, driving an autonomous car, or moving a robot arm, the question is the same: How do we find a path from start to goal efficiently and safely?\nPathfinding algorithms answer this question, balancing optimality, speed, and adaptability. In this section, we explore three foundational families:\n\nA*: Heuristic search in grids and graphs- RRT (Rapidly-Exploring Random Tree): Sampling-based exploration- PRM (Probabilistic Roadmap): Precomputed navigation networks\n\n\n1. The Pathfinding Problem\nGiven:\n\nA space (grid, graph, or continuous)- A start node and goal node- A cost function (distance, time, energy)- Optional obstacles Find a collision-free, low-cost path.\n\n\n\n2. A* (A-star) Search\nA* combines Dijkstra’s algorithm with a heuristic that estimates remaining cost. It’s the most popular graph-based pathfinding algorithm.\n\n\nKey Idea\nEach node ( n ) has: \\[\nf(n) = g(n) + h(n)\n\\]\n\n( g(n) ): cost so far- ( h(n) ): estimated cost to goal- ( f(n) ): total estimated cost\n\nAlgorithm\n\nInitialize priority queue with start node\nWhile queue not empty:\n\nPop node with smallest ( f(n) ) - If goal reached → return path - For each neighbor:\n\nCompute new ( g ), ( f ) - Update queue if better\n\n\n\nTiny Code (Grid A*)\ntypedef struct { int x, y; double g, f; } Node;\n\ndouble heuristic(Node a, Node b) {\n    return fabs(a.x - b.x) + fabs(a.y - b.y); // Manhattan\n}\n\nvoid a_star(Node start, Node goal) {\n    PriorityQueue open;\n    push(open, start);\n    while (!empty(open)) {\n        Node cur = pop_min(open);\n        if (cur == goal) return reconstruct_path();\n        for (Node n : neighbors(cur)) {\n            double tentative_g = cur.g + dist(cur, n);\n            if (tentative_g &lt; n.g) {\n                n.g = tentative_g;\n                n.f = n.g + heuristic(n, goal);\n                push(open, n);\n            }\n        }\n    }\n}\n\n\nComplexity\n\nTime: ( O\\(E \\log V\\) )- Space: ( O(V) )- Optimal if ( h(n) ) is admissible (never overestimates)\n\n\n\nVariants\n\n\n\nVariant\nDescription\n\n\n\n\nDijkstra\nA* with ( h(n) = 0 )\n\n\nGreedy Best-First\nUses ( h(n) ) only\n\n\nWeighted A*\nSpeeds up with tradeoff on optimality\n\n\nJump Point Search\nOptimized for uniform grids\n\n\n\n\n\n3. RRT (Rapidly-Exploring Random Tree)\nA* struggles in continuous or high-dimensional spaces (e.g. robot arms). RRT tackles this with randomized exploration.\n\n\nCore Idea\n\nGrow a tree from the start by randomly sampling points.- Extend tree toward each sample (step size \\(\\epsilon\\)).- Stop when near the goal.\n\nTiny Code (RRT Sketch)\nTree T = {start};\nfor (int i = 0; i &lt; MAX_ITERS; i++) {\n    Point q_rand = random_point();\n    Point q_near = nearest(T, q_rand);\n    Point q_new = steer(q_near, q_rand, step_size);\n    if (collision_free(q_near, q_new))\n        add_edge(T, q_near, q_new);\n    if (distance(q_new, goal) &lt; eps)\n        return path;\n}\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nWorks in continuous space\nPaths are suboptimal\n\n\nHandles high dimensions\nRandomness may miss narrow passages\n\n\nSimple and fast\nNeeds post-processing (smoothing)\n\n\n\n\n\nVariants\n\n\n\nVariant\nDescription\n\n\n\n\nRRT*\nAsymptotically optimal\n\n\nBi-RRT\nGrow from both start and goal\n\n\nInformed RRT*\nFocus on promising regions\n\n\n\n\n\n4. PRM (Probabilistic Roadmap)\nPRM builds a graph of feasible configurations, a roadmap, then searches it.\n\n\nSteps\n\nSample random points in free space\nConnect nearby points with collision-free edges\nSearch roadmap (e.g., with A*)\n\nTiny Code (PRM Sketch)\nGraph G = {};\nfor (int i = 0; i &lt; N; i++) {\n    Point p = random_free_point();\n    G.add_vertex(p);\n}\nfor each p in G:\n    for each q near p:\n        if (collision_free(p, q))\n            G.add_edge(p, q);\npath = a_star(G, start, goal);\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nPrecomputes reusable roadmap\nNeeds many samples for coverage\n\n\nGood for multiple queries\nPoor for single-query planning\n\n\nWorks in high-dim spaces\nMay need post-smoothing\n\n\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nSpace\nNature\nOptimal\nUse Case\n\n\n\n\nA*\nDiscrete\nDeterministic\nYes\nGrids, graphs\n\n\nRRT\nContinuous\nRandomized\nNo (RRT* = Yes)\nRobotics, motion planning\n\n\nPRM\nContinuous\nRandomized\nApprox.\nMulti-query planning\n\n\n\n\n\n6. Applications\n\n\n\nDomain\nUse Case\n\n\n\n\nRobotics\nArm motion, mobile navigation\n\n\nGames\nNPC pathfinding, AI navigation mesh\n\n\nAutonomous vehicles\nRoute planning\n\n\nAerospace\nDrone and spacecraft trajectory\n\n\nLogistics\nWarehouse robot movement\n\n\n\n\n\nWhy It Matters\nPathfinding is decision-making in space, it gives agents the ability to move, explore, and act purposefully. From Pac-Man to Mars rovers, every journey starts with an algorithm.\n\n“To move with purpose, one must first see the paths that are possible.”\n\n\n\nTry It Yourself\n\nImplement A* on a 2D grid with walls.\nGenerate an RRT in a 2D obstacle field.\nBuild a PRM for a continuous space and run A* on the roadmap.\nCompare speed and path smoothness across methods.\n\n\n\n\n80. Computational Geometry Variants and Applications\nComputational geometry is the study of algorithms on geometric data, points, lines, polygons, circles, and higher-dimensional shapes. By now, you’ve seen core building blocks: convex hulls, intersections, nearest neighbors, triangulations, and spatial indexing.\nThis final section brings them together through variants, generalizations, and real-world applications, showing how geometry quietly powers modern computing.\n\n1. Beyond the Plane\nMost examples so far assumed 2D geometry. But real systems often live in 3D or N-D spaces.\n\n\n\n\n\n\n\n\nDimension\nExample Problems\nTypical Uses\n\n\n\n\n2D\nConvex hull, polygon area, line sweep\nGIS, CAD, mapping\n\n\n3D\nConvex polyhedra, mesh intersection, visibility\nGraphics, simulation\n\n\nN-D\nVoronoi in high-D, KD-trees, optimization\nML, robotics, data science\n\n\n\nHigher dimensions add complexity (and sometimes impossibility):\n\nExact geometry often replaced by approximations.- Volume, distance, and intersection tests become more expensive.\n\n\n\n2. Approximate and Robust Geometry\nReal-world geometry faces numerical errors (floating point) and degenerate cases (collinear, overlapping). To handle this, algorithms adopt robustness and approximation strategies.\n\nEpsilon comparisons: treat values within tolerance as equal- Orientation tests: robustly compute turn direction via cross product- Exact arithmetic: rational or symbolic computation- Grid snapping: quantize space for stability Approximate geometry accepts small error for large speed-up, essential in graphics and machine learning.\n\n\n\n3. Geometric Duality\nA powerful tool for reasoning about problems: map points to lines, lines to points. For example:\n\nA point ( (a, b) ) maps to line ( y = ax - b ).- A line ( y = mx + c ) maps to point ( (m, -c) ). Applications:\nTransforming line intersection problems into point location problems- Simplifying half-plane intersections- Enabling arrangement algorithms in computational geometry Duality is a common trick: turn geometry upside-down to make it simpler.\n\n\n\n4. Geometric Data Structures\nRecap of core spatial structures and what they’re best at:\n\n\n\n\n\n\n\n\n\nStructure\nStores\nQueries\nUse Case\n\n\n\n\nKD-Tree\nPoints\nNN, range\nLow-D search\n\n\nR-Tree\nRectangles\nOverlaps\nSpatial DB\n\n\nQuad/Octree\nSpace partitions\nPoint lookup\nGraphics, GIS\n\n\nBSP Tree\nPolygons\nVisibility\nRendering\n\n\nDelaunay Triangulation\nPoints\nNeighbors\nMesh generation\n\n\nSegment Tree\nIntervals\nRange\nSweep-line events\n\n\n\n\n\n5. Randomized Geometry\nRandomness simplifies deterministic geometry:\n\nRandomized incremental construction (Convex Hulls, Delaunay)- Random sampling for approximation (ε-nets, VC dimension)- Monte Carlo geometry for probabilistic intersection and coverage Example: randomized incremental convex hull builds expected ( O\\(n \\log n\\) ) structures with elegant proofs.\n\n\n\n6. Computational Topology\nBeyond geometry lies shape connectivity, studied by topology. Algorithms compute connected components, holes, homology, and Betti numbers.\nApplications include:\n\n3D printing (watertightness)- Data analysis (persistent homology)- Robotics (free space topology) Geometry meets topology in alpha-shapes, simplicial complexes, and manifold reconstruction.\n\n\n\n7. Geometry Meets Machine Learning\nMany ML methods are geometric at heart:\n\nNearest neighbor → Voronoi diagram- SVM → hyperplane separation- K-means → Voronoi partitioning- Manifold learning → low-dim geometry- Convex optimization → geometric feasibility Visualization tools (t-SNE, UMAP) rely on spatial embedding and distance geometry.\n\n\n\n8. Applications Across Fields\n\n\n\n\n\n\n\n\nField\nApplication\nGeometric Core\n\n\n\n\nGraphics\nRendering, collision\nTriangulation, ray tracing\n\n\nGIS\nMaps, roads\nPolygons, point-in-region\n\n\nRobotics\nPath planning\nObstacles, configuration space\n\n\nArchitecture\nModeling\nMesh operations\n\n\nVision\nObject boundaries\nContours, convexity\n\n\nAI\nClustering, similarity\nDistance metrics\n\n\nPhysics\nSimulation\nParticle collision\n\n\nDatabases\nSpatial joins\nR-Trees, indexing\n\n\n\nGeometry underpins structure, position, and relationship, the backbone of spatial reasoning.\n\n\n9. Complexity and Open Problems\nSome problems still challenge efficient solutions:\n\nPoint location in dynamic settings- Visibility graphs in complex polygons- Motion planning in high dimensions- Geometric median / center problems- Approximation guarantees in robust settings These remain active areas in computational geometry research.\n\n\n\nTiny Code (Point-in-Polygon via Ray Casting)\nbool inside(Point p, Polygon poly) {\n    int cnt = 0;\n    for (int i = 0; i &lt; poly.n; i++) {\n        Point a = poly[i], b = poly[(i + 1) % poly.n];\n        if (intersect_ray(p, a, b)) cnt++;\n    }\n    return cnt % 2 == 1; // odd crossings = inside\n}\nThis small routine appears everywhere, maps, games, GUIs, and physics engines.\n\n\n10. Why It Matters\nComputational geometry is more than shape, it’s the mathematics of space, powering visual computing, spatial data, and intelligent systems. Everywhere something moves, collides, maps, or recognizes form, geometry is the invisible hand guiding it.\n\n“All computation lives somewhere, and geometry is how we understand the where.”\n\n\n\nTry It Yourself\n\nImplement point-in-polygon and test on convex vs concave shapes.\nVisualize a Delaunay triangulation and its Voronoi dual.\nExperiment with KD-trees for nearest neighbor queries.\nWrite a small convex hull in 3D using incremental insertion.\nSketch an RRT path over a geometric map.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-9.-systems-databases-and-distributed-algorithms",
    "href": "books/en-us/book.html#chapter-9.-systems-databases-and-distributed-algorithms",
    "title": "The Book",
    "section": "Chapter 9. Systems, Databases, and Distributed Algorithms",
    "text": "Chapter 9. Systems, Databases, and Distributed Algorithms\n\n81. Concurrency Control (2PL, MVCC, OCC)\nIn multi-user or multi-threaded systems, many operations want to read or write shared data at the same time. Without discipline, this leads to chaos, lost updates, dirty reads, or even inconsistent states.\nConcurrency control ensures correctness under parallelism, so that the result is as if each transaction ran alone (a property called serializability).\nThis section introduces three foundational techniques:\n\n2PL - Two-Phase Locking- MVCC - Multi-Version Concurrency Control- OCC - Optimistic Concurrency Control\n\n\n1. The Goal: Serializability\nWe want transactions to behave as if executed in some serial order, even though they’re interleaved.\nA schedule is serializable if it yields the same result as some serial order of transactions.\nConcurrency control prevents problems like:\n\nLost Update: Two writes overwrite each other.- Dirty Read: Read uncommitted data.- Non-repeatable Read: Data changes mid-transaction.- Phantom Read: New rows appear after a query.\n\n\n\n2. Two-Phase Locking (2PL)\nIdea: Use locks to coordinate access. Each transaction has two phases:\n\nGrowing phase: acquire locks (shared or exclusive)\nShrinking phase: release locks (no new locks allowed after release)\n\nThis ensures conflict-serializability.\n\n\nLock Types\n\n\n\nType\nOperation\nShared?\nExclusive?\n\n\n\n\nShared (S)\nRead\nYes\nNo\n\n\nExclusive (X)\nWrite\nNo\nNo\n\n\n\nIf a transaction needs to read: request S-lock. If it needs to write: request X-lock.\nTiny Code (Lock Manager Sketch)\nvoid acquire_lock(Transaction *T, Item *X, LockType type) {\n    while (conflict_exists(X, type))\n        wait();\n    add_lock(X, T, type);\n}\n\nvoid release_all(Transaction *T) {\n    for (Lock *l in T-&gt;locks)\n        unlock(l);\n}\n\n\nExample\nT1: read(A); write(A)\nT2: read(A); write(A)\nWithout locks → race condition. With 2PL → one must wait → consistent.\n\n\nVariants\n\n\n\n\n\n\n\nVariant\nDescription\n\n\n\n\nStrict 2PL\nHolds all locks until commit → avoids cascading aborts\n\n\nRigorous 2PL\nSame as Strict, all locks released at end\n\n\nConservative 2PL\nAcquires all locks before execution\n\n\n\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nGuarantees serializability\nCan cause deadlocks\n\n\nSimple concept\nBlocking, contention under load\n\n\n\n\n\n3. Multi-Version Concurrency Control (MVCC)\nIdea: Readers don’t block writers, and writers don’t block readers. Each write creates a new version of data with a timestamp.\nTransactions read from a consistent snapshot based on their start time.\n\n\nSnapshot Isolation\n\nReaders see the latest committed version at transaction start.- Writers produce new versions; conflicts detected at commit time. Each record stores:\nvalue- created_at- deleted_at (if applicable)\n\nTiny Code (Version Chain)\nstruct Version {\n    int value;\n    Timestamp created;\n    Timestamp deleted;\n    Version *next;\n};\nRead finds version with created &lt;= tx.start && deleted &gt; tx.start.\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nNo read locks\nHigher memory (multiple versions)\n\n\nReaders never block\nWrite conflicts at commit\n\n\nGreat for OLTP systems\nGC of old versions needed\n\n\n\n\n\nUsed In\n\nPostgreSQL- Oracle- MySQL (InnoDB)- Spanner\n\n\n\n4. Optimistic Concurrency Control (OCC)\nIdea: Assume conflicts are rare. Let transactions run without locks. At commit time, validate, if conflicts exist, rollback.\n\n\nPhases\n\nRead phase - execute, read data, buffer writes.\nValidation phase - check if conflicts occurred.\nWrite phase - apply changes if valid, else abort.\n\nTiny Code (OCC Validation)\nbool validate(Transaction *T) {\n    for (Transaction *U in committed_since(T.start))\n        if (conflict(T, U))\n            return false;\n    return true;\n}\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nNo locks → no deadlocks\nHigh abort rate under contention\n\n\nGreat for low-conflict workloads\nWasted work on abort\n\n\n\n\n\nUsed In\n\nIn-memory DBs- Distributed systems- STM (Software Transactional Memory)\n\n\n\n5. Choosing a Strategy\n\n\n\nSystem Type\nPreferred Control\n\n\n\n\nOLTP (many reads/writes)\nMVCC\n\n\nOLAP (read-heavy)\nMVCC or OCC\n\n\nReal-time systems\n2PL (predictable)\n\n\nLow contention\nOCC\n\n\nHigh contention\n2PL / MVCC\n\n\n\n\n\n6. Why It Matters\nConcurrency control is the backbone of consistency in databases, distributed systems, and even multi-threaded programs. It enforces correctness amid chaos, ensuring your data isn’t silently corrupted.\n\n“Without order, parallelism is noise. Concurrency control is its conductor.”\n\n\n\nTry It Yourself\n\nSimulate 2PL with two transactions updating shared data.\nImplement a toy MVCC table with version chains.\nWrite an OCC validator for three concurrent transactions.\nExperiment: under high conflict, which model performs best?\n\n\n\n\n82. Logging, Recovery, and Commit Protocols\nNo matter how elegant your algorithms or how fast your storage, failures happen. Power cuts, crashes, and network splits are inevitable. What matters is recovery, restoring the system to a consistent state without losing committed work.\nLogging, recovery, and commit protocols form the backbone of reliable transactional systems, ensuring durability and correctness in the face of crashes.\n\n1. The Problem\nWe need to guarantee the ACID properties:\n\nAtomicity - all or nothing- Consistency - valid before and after- Isolation - no interference- Durability - once committed, always safe If a crash occurs mid-transaction, how do we roll back or redo correctly?\n\nThe answer: Log everything, then replay or undo after failure.\n\n\n2. Write-Ahead Logging (WAL)\nThe golden rule:\n\n“Write log entries before modifying the database.”\n\nEvery action is recorded in a sequential log on disk, ensuring the system can reconstruct the state.\n\n\nLog Record Format\nEach log entry typically includes:\n\nLSN (Log Sequence Number)- Transaction ID- Operation (update, insert, delete)- Before image (old value)- After image (new value)\n\nstruct LogEntry {\n    int lsn;\n    int tx_id;\n    char op[10];\n    Value before, after;\n};\nWhen a transaction commits, the system first flushes logs to disk (fsync). Only then is the commit acknowledged.\n\n\n3. Recovery Actions\nWhen the system restarts, it reads logs and applies a recovery algorithm.\n\n\nThree Phases (ARIES Model)\n\nAnalysis - determine state at crash (active vs committed)\nRedo - repeat all actions from last checkpoint\nUndo - rollback incomplete transactions\n\nARIES (Algorithm for Recovery and Isolation Exploiting Semantics) is the most widely used approach (IBM DB2, PostgreSQL, SQL Server).\n\n\nRedo Rule\nIf the system committed before crash → redo all updates so data is preserved.\n\n\nUndo Rule\nIf the system didn’t commit → undo to maintain atomicity.\nTiny Code (Simplified Recovery Sketch)\nvoid recover(Log log) {\n    for (Entry e : log) {\n        if (e.committed)\n            apply(e.after);\n        else\n            apply(e.before);\n    }\n}\n\n\n4. Checkpointing\nInstead of replaying the entire log, systems take checkpoints, periodic snapshots marking a safe state.\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSharp checkpoint\nStop all transactions briefly, flush data + log\n\n\nFuzzy checkpoint\nMark consistent LSN; continue running\n\n\n\nCheckpoints reduce recovery time: only replay after the last checkpoint.\n\n\n5. Commit Protocols\nIn distributed systems, multiple nodes must agree to commit or abort together. This is handled by atomic commit protocols.\n\n\nTwo-Phase Commit (2PC)\nGoal: All participants either commit or abort in unison.\nSteps:\n\nPrepare phase (voting):\n\nCoordinator asks all participants to “prepare” - Each replies yes/no2. Commit phase (decision):\nIf all say yes → commit - Else → abort\n\nCoordinator: PREPARE  \nParticipants: VOTE YES / NO  \nCoordinator: COMMIT / ABORT\n\nIf the coordinator crashes after prepare, participants must wait → blocking protocol.\nTiny Code (2PC Pseudocode)\nbool two_phase_commit(Participants P) {\n    for each p in P:\n        if (!p.prepare()) return abort_all();\n    for each p in P:\n        p.commit();\n    return true;\n}\n\n\nThree-Phase Commit (3PC)\nImproves on 2PC by adding an intermediate phase to avoid indefinite blocking. More complex, used in systems with reliable failure detection.\n\n\n6. Logging in Distributed Systems\nEach participant maintains its own WAL. To recover globally:\n\nUse coordinated checkpoints- Maintain global commit logs- Consensus-based protocols (Paxos Commit, Raft) can replace 2PC for high availability\n\n\n\n7. Example Timeline\n\n\n\nStep\nAction\n\n\n\n\nT1 updates record A\nWAL entry written\n\n\nT1 updates record B\nWAL entry written\n\n\nT1 commits\nWAL flush, commit record\n\n\nCrash!\nDisk may be inconsistent\n\n\nRestart\nRecovery scans log, redoes T1\n\n\n\n\n\n8. Pros and Cons\n\n\n\nApproach\nStrength\nWeakness\n\n\n\n\nWAL\nSimple, durable\nWrite overhead\n\n\nCheckpointing\nFaster recovery\nI/O spikes\n\n\n2PC\nGlobal atomicity\nBlocking\n\n\n3PC / Consensus\nNon-blocking\nComplex, slower\n\n\n\n\n\n9. Real Systems\n\n\n\nSystem\nStrategy\n\n\n\n\nPostgreSQL\nWAL + ARIES + Checkpoint\n\n\nMySQL (InnoDB)\nWAL + Fuzzy checkpoint\n\n\nSpanner\nWAL + 2PC + TrueTime\n\n\nKafka\nWAL for durability\n\n\nRocksDB\nWAL + LSM checkpoints\n\n\n\n\n\n10. Why It Matters\nLogging and commit protocols make data survive crashes and stay consistent across machines. Without them, every failure risks corruption.\n\n“Persistence is not about never failing, it’s about remembering how to stand back up.”\n\n\n\nTry It Yourself\n\nWrite a toy WAL system that logs before writes.\nSimulate a crash mid-transaction and replay the log.\nImplement a simple 2PC coordinator with two participants.\nCompare recovery time with vs without checkpoints.\n\n\n\n\n83. Scheduling (Round Robin, EDF, Rate-Monotonic)\nIn operating systems and real-time systems, scheduling determines the order in which tasks or processes run. Since resources like CPU time are limited, a good scheduler aims to balance fairness, efficiency, and responsiveness.\n\n1. The Goal of Scheduling\nEvery system has tasks competing for the CPU. Scheduling decides:\n\nWhich task runs next- How long it runs- When it yields or preempts Different goals apply in different domains:\n\n\n\n\nDomain\nObjective\n\n\n\n\nGeneral-purpose OS\nFairness, responsiveness\n\n\nReal-time systems\nMeeting deadlines\n\n\nEmbedded systems\nPredictability\n\n\nHigh-performance servers\nThroughput, latency balance\n\n\n\nA scheduler’s policy can be preemptive (interrupts tasks) or non-preemptive (waits for voluntary yield).\n\n\n2. Round Robin Scheduling\nRound Robin (RR) is one of the simplest preemptive schedulers. Each process gets a fixed time slice (quantum) and runs in a circular queue.\nIf a process doesn’t finish, it’s put back at the end of the queue.\n\n\nTiny Code: Round Robin (Pseudocode)\nqueue processes;\nwhile (!empty(processes)) {\n    process = dequeue(processes);\n    run_for_quantum(process);\n    if (!process.finished)\n        enqueue(processes, process);\n}\n\n\nCharacteristics\n\nFair: Every process gets CPU time.- Responsive: Short tasks don’t starve.- Downside: Context switching overhead if quantum is too small. #### Example\n\n\n\n\nProcess\nBurst Time\n\n\n\n\n\nP1\n4\n\n\n\nP2\n3\n\n\n\nP3\n2\n\n\n\n\nQuantum = 1 Order: P1, P2, P3, P1, P2, P3, P1, P2 → all finish fairly.\n\n\n3. Priority Scheduling\nEach task has a priority. The scheduler always picks the highest-priority ready task.\n\nPreemptive: A higher-priority task can interrupt a lower one.- Non-preemptive: The CPU is released voluntarily. #### Problems\nStarvation: Low-priority tasks may never run.- Solution: Aging - gradually increase waiting task priority.\n\n\n\n4. Earliest Deadline First (EDF)\nEDF is a dynamic priority scheduler for real-time systems. Each task has a deadline, and the task with the earliest deadline runs first.\n\n\nRule\nAt any time, run the ready task with the closest deadline.\n\n\nExample\n\n\n\nTask\nExecution Time\nDeadline\n\n\n\n\nT1\n1\n3\n\n\nT2\n2\n5\n\n\nT3\n1\n2\n\n\n\nOrder: T3 → T1 → T2\nEDF is optimal for preemptive scheduling of independent tasks on a single processor.\n\n\n5. Rate-Monotonic Scheduling (RMS)\nIn periodic real-time systems, tasks repeat at fixed intervals. RMS assigns higher priority to tasks with shorter periods.\n\n\n\nTask\nPeriod\nPriority\n\n\n\n\nT1\n2 ms\nHigh\n\n\nT2\n5 ms\nMedium\n\n\nT3\n10 ms\nLow\n\n\n\nIt’s static (priorities don’t change) and optimal among fixed-priority schedulers.\n\n\nUtilization Bound\nFor n tasks, RMS is guaranteed schedulable if:\n\\[\nU = \\sum_{i=1}^{n} \\frac{C_i}{T_i} \\le n(2^{1/n} - 1)\n\\]\nFor example, for 3 tasks, \\(U \\le 0.78\\).\n\n\n6. Shortest Job First (SJF)\nRun the task with the shortest burst time first.\n\nNon-preemptive SJF: Once started, runs to completion.- Preemptive SJF (Shortest Remaining Time First): Preempts if a shorter job arrives. Advantage: Minimizes average waiting time. Disadvantage: Needs knowledge of future job lengths.\n\n\n\n7. Multilevel Queue Scheduling\nDivide processes into classes (interactive, batch, system). Each class has its own queue with own policy, e.g.:\n\nQueue 1: System → RR (quantum = 10ms)- Queue 2: Interactive → RR (quantum = 50ms)- Queue 3: Batch → FCFS (First-Come-First-Serve) CPU is assigned based on queue priority.\n\n\n\n8. Multilevel Feedback Queue (MLFQ)\nProcesses move between queues based on behavior.\n\nCPU-bound → move down (lower priority)- I/O-bound → move up (higher priority) Goal: Adaptive scheduling that rewards interactive tasks.\n\nUsed in modern OS kernels (Linux, Windows).\n\n\n9. Scheduling Metrics\n\n\n\nMetric\nMeaning\n\n\n\n\nTurnaround Time\nCompletion − Arrival\n\n\nWaiting Time\nTime spent in ready queue\n\n\nResponse Time\nTime from arrival to first execution\n\n\nThroughput\nCompleted tasks per unit time\n\n\nCPU Utilization\n% of time CPU is busy\n\n\n\nSchedulers balance these based on design goals.\n\n\n10. Why It Matters\nSchedulers shape how responsive, efficient, and fair a system feels. In operating systems, they govern multitasking. In real-time systems, they ensure deadlines are met. In servers, they keep latency low and throughput high.\n\n“Scheduling is not just about time. It’s about fairness, foresight, and flow.”\n\n\n\nTry It Yourself\n\nSimulate Round Robin with quantum = 2, compare average waiting time.\nImplement EDF for a set of periodic tasks with deadlines.\nCheck schedulability under RMS for 3 periodic tasks.\nExplore Linux CFS (Completely Fair Scheduler) source code.\nCompare SJF and RR for CPU-bound vs I/O-bound workloads.\n\n\n\n\n84. Caching and Replacement (LRU, LFU, CLOCK)\nCaching is the art of remembering the past to speed up the future. In computing, caches store recently used or frequently accessed data to reduce latency and load on slower storage (like disks or networks). The challenge: caches have limited capacity, so when full, we must decide what to evict. That’s where replacement policies come in.\n\n1. The Need for Caching\nCaches appear everywhere:\n\nCPU: L1, L2, L3 caches speed up memory access- Databases: query results or index pages- Web browsers / CDNs: recently fetched pages- Operating systems: page cache for disk blocks The principle guiding all caches is locality:\nTemporal locality: recently used items are likely used again soon- Spatial locality: nearby items are likely needed next\n\n\n\n2. Cache Replacement Problem\nWhen the cache is full, which item should we remove?\nWe want to minimize cache misses (requests not found in cache).\nFormally:\n\nGiven a sequence of accesses, find a replacement policy that minimizes misses.\n\nTheoretical optimal policy (OPT): always evict the item used farthest in the future. But OPT requires future knowledge, so we rely on heuristics like LRU, LFU, CLOCK.\n\n\n3. Least Recently Used (LRU)\nLRU evicts the least recently accessed item. It assumes recently used = likely to be used again.\n\n\nImplementation Approaches\n\nStack (list): move item to top on access- Hash map + doubly linked list: O(1) insert, delete, lookup #### Tiny Code: LRU (Simplified)\n\ntypedef struct Node {\n    int key;\n    struct Node *prev, *next;\n} Node;\n\nHashMap cache;\nList lru_list;\n\nvoid access(int key) {\n    if (in_cache(key)) move_to_front(key);\n    else {\n        if (cache_full()) remove_lru();\n        insert_front(key);\n    }\n}\n\n\nPros\n\nGood for workloads with strong temporal locality #### Cons\nCostly in hardware or massive caches (metadata overhead)\n\n\n\n4. Least Frequently Used (LFU)\nLFU evicts the least frequently accessed item.\nTracks usage count for each item:\n\nIncrement on each access- Evict lowest-count item #### Example\n\n\n\n\nItem\nAccesses\nFrequency\n\n\n\n\nA\n3\n3\n\n\nB\n1\n1\n\n\nC\n2\n2\n\n\n\nEvict B.\n\n\nVariants\n\nLFU with aging: gradually reduce counts to adapt to new trends- Approximate LFU: counters in ranges (for memory efficiency) #### Pros\nGreat for stable, repetitive workloads #### Cons\nPoor for workloads with shifting popularity (slow adaptation)\n\n\n\n5. FIFO (First In First Out)\nSimple but naive:\n\nEvict the oldest item, ignoring usage Used in simple hardware caches. Good when access pattern is cyclic, bad otherwise.\n\n\n\n6. Random Replacement (RR)\nEvict a random entry.\nSurprisingly competitive in some high-concurrency systems, and trivial to implement. Used in memcached (as an option).\n\n\n7. CLOCK Algorithm\nA practical approximation of LRU, widely used in OS page replacement.\nEach page has a reference bit (R). Pages form a circular list.\nAlgorithm:\n\nClock hand sweeps over pages.\nIf R = 0, evict page.\nIf R = 1, set R = 0 and skip.\n\nThis mimics LRU with O(1) cost and low overhead.\n\n\n8. Second-Chance and Enhanced CLOCK\nSecond-Chance: give recently used pages a “second chance” before eviction. Enhanced CLOCK: also uses modify bit (M) to prefer clean pages.\nUsed in Linux’s page replacement (with Active/Inactive lists).\n\n\n9. Adaptive Algorithms\nModern systems use hybrid or adaptive policies:\n\nARC (Adaptive Replacement Cache) - balances recency and frequency- CAR (Clock with Adaptive Replacement) - CLOCK-style adaptation- TinyLFU - frequency sketch + admission policy- Hyperbolic caching - popularity decay for large-scale systems These adapt dynamically to changing workloads.\n\n\n\n10. Why It Matters\nCaching is the backbone of system speed:\n\nOS uses it for paging- Databases for buffer pools- CPUs for memory hierarchies- CDNs for global acceleration Choosing the right eviction policy can mean orders of magnitude improvement in latency and throughput.\n\n\n“A good cache remembers what matters, and forgets what no longer does.”\n\n\n\nTry It Yourself\n\nSimulate a cache of size 3 with sequence: A B C A B D A B C D Compare LRU, LFU, and FIFO miss counts.\nImplement LRU with a doubly-linked list and hash map in C.\nTry CLOCK with reference bits, simulate a sweep.\nExperiment with ARC and TinyLFU for dynamic workloads.\nMeasure hit ratios for different access patterns (sequential, random, looping).\n\n\n\n\n85. Networking (Routing, Congestion Control)\nNetworking algorithms make sure data finds its way through vast, connected systems, efficiently, reliably, and fairly. Two core pillars of network algorithms are routing (deciding where packets go) and congestion control (deciding how fast to send them).\nTogether, they ensure the internet functions under heavy load, dynamic topology, and unpredictable demand.\n\n1. The Goals of Networking Algorithms\n\nCorrectness: all destinations are reachable if paths exist- Efficiency: use minimal resources (bandwidth, latency, hops)- Scalability: support large, dynamic networks- Robustness: recover from failures- Fairness: avoid starving flows\n\n\n\n2. Types of Routing\nRouting decides paths packets should follow through a graph-like network.\n\n\nStatic vs Dynamic Routing\n\nStatic: fixed routes, manual configuration (good for small networks)- Dynamic: routes adjust automatically as topology changes (internet-scale) #### Unicast, Multicast, Broadcast\nUnicast: one-to-one (most traffic)- Multicast: one-to-many (video streaming, gaming)- Broadcast: one-to-all (local networks)\n\n\n\n3. Shortest Path Routing\nMost routing relies on shortest path algorithms:\n\n\nDijkstra’s Algorithm\n\nBuilds shortest paths from one source- Complexity: O(E log V) with priority queue Used in:\nOSPF (Open Shortest Path First)- IS-IS (Intermediate System to Intermediate System) #### Bellman-Ford Algorithm\nHandles negative edges- Basis for Distance-Vector routing (RIP) #### Tiny Code: Dijkstra for Routing\n\n#define INF 1e9\nint dist[MAX], visited[MAX];\nvector&lt;pair&lt;int,int&gt;&gt; adj[MAX];\n\nvoid dijkstra(int s, int n) {\n    for (int i = 0; i &lt; n; i++) dist[i] = INF;\n    dist[s] = 0;\n    priority_queue&lt;pair&lt;int,int&gt;&gt; pq;\n    pq.push({0, s});\n    while (!pq.empty()) {\n        int u = pq.top().second; pq.pop();\n        if (visited[u]) continue;\n        visited[u] = 1;\n        for (auto [v, w]: adj[u]) {\n            if (dist[v] &gt; dist[u] + w) {\n                dist[v] = dist[u] + w;\n                pq.push({-dist[v], v});\n            }\n        }\n    }\n}\n\n\n4. Distance-Vector vs Link-State\n\n\n\nFeature\nDistance-Vector (RIP)\nLink-State (OSPF)\n\n\n\n\nInfo Shared\nDistance to neighbors\nFull topology map\n\n\nConvergence\nSlower (loops possible)\nFast (SPF computation)\n\n\nComplexity\nLower\nHigher\n\n\nExamples\nRIP, BGP (conceptually)\nOSPF, IS-IS\n\n\n\nRIP uses Bellman-Ford. OSPF floods link-state updates, runs Dijkstra at each node.\n\n\n5. Hierarchical Routing\nLarge-scale networks (like the Internet) use hierarchical routing:\n\nRouters grouped into Autonomous Systems (AS)- Intra-AS routing: OSPF, IS-IS- Inter-AS routing: BGP (Border Gateway Protocol) BGP exchanges reachability info, not shortest paths, and prefers policy-based routing (e.g., cost, contracts, peering).\n\n\n\n6. Congestion Control\nEven with good routes, we can’t flood links. Congestion control ensures fair and efficient use of bandwidth.\nImplemented primarily at the transport layer (TCP).\n\n\nTCP Congestion Control\nKey components:\n\nAdditive Increase, Multiplicative Decrease (AIMD)- Slow Start: probe capacity- Congestion Avoidance: grow cautiously- Fast Retransmit / Recovery Modern variants:\nTCP Reno: classic AIMD- TCP Cubic: non-linear growth for high-speed networks- BBR (Bottleneck Bandwidth + RTT): model-based control #### Algorithm Sketch (AIMD)\n\nOn ACK: cwnd += 1/cwnd  // increase slowly\nOn loss: cwnd /= 2      // halve window\n\n\n7. Queue Management\nRouters maintain queues. Too full? =&gt; Packet loss, latency spikes, tail drop.\nSolutions:\n\nRED (Random Early Detection) - drop packets early- CoDel (Controlled Delay) - monitor queue delay, drop adaptively These prevent bufferbloat, improving latency for real-time traffic.\n\n\n\n8. Flow Control vs Congestion Control\n\nFlow Control: prevent sender from overwhelming receiver- Congestion Control: prevent sender from overwhelming network TCP uses both: receive window (rwnd) and congestion window (cwnd). Actual sending rate = min(rwnd, cwnd).\n\n\n\n9. Data Plane vs Control Plane\n\nControl Plane: decides routes (OSPF, BGP)- Data Plane: forwards packets (fast path) Modern networking (e.g. SDN, Software Defined Networking) separates these:\nController computes routes- Switches act on flow rules\n\n\n\n10. Why It Matters\nRouting and congestion control shape the performance of:\n\nThe Internet backbone- Data center networks (with load balancing)- Cloud services and microservice meshes- Content delivery networks (CDNs) Every packet’s journey, from your laptop to a global data center, relies on these ideas.\n\n\n“Networking is not magic. It’s algorithms moving data through time and space.”\n\n\n\nTry It Yourself\n\nImplement Dijkstra’s algorithm for a small network graph.\nSimulate RIP (Distance Vector): each node updates from neighbors.\nModel TCP AIMD window growth; visualize with Python.\nTry RED: drop packets when queue length &gt; threshold.\nCompare TCP Reno, Cubic, BBR throughput in simulation.\n\n\n\n\n86. Distributed Consensus (Paxos, Raft, PBFT)\nIn a distributed system, multiple nodes must agree on a single value, for example, the state of a log, a database entry, or a blockchain block. This agreement process is called consensus.\nConsensus algorithms let distributed systems act as one reliable system, even when some nodes fail, crash, or lie (Byzantine faults).\n\n1. Why Consensus?\nImagine a cluster managing a shared log (like in databases or Raft). Each node might:\n\nSee different requests,- Fail and recover,- Communicate over unreliable links. We need all non-faulty nodes to agree on the same order of operations.\n\nA valid consensus algorithm must satisfy:\n\nAgreement: all correct nodes choose the same value- Validity: the chosen value was proposed by a node- Termination: every correct node eventually decides- Fault Tolerance: works despite failures\n\n\n\n2. The FLP Impossibility\nThe FLP theorem (Fischer, Lynch, Paterson, 1985) says:\n\nIn an asynchronous system with even one faulty process, no deterministic algorithm can guarantee consensus.\n\nSo practical algorithms use:\n\nRandomization, or- Partial synchrony (timeouts, retries)\n\n\n\n3. Paxos: The Classical Algorithm\nPaxos, by Leslie Lamport, is the theoretical foundation for distributed consensus.\nIt revolves around three roles:\n\nProposers: suggest values- Acceptors: vote on proposals- Learners: learn the final decision Consensus proceeds in two phases.\n\n\n\nPhase 1 (Prepare)\n\nProposer picks a proposal number n and sends (Prepare, n) to acceptors.\nAcceptors respond with their highest accepted proposal (if any).\n\n\n\nPhase 2 (Accept)\n\nIf proposer receives a majority of responses, it sends (Accept, n, v) with value v (highest seen or new).\nAcceptors accept if they haven’t promised higher n.\n\nWhen a majority accept, value v is chosen.\n\n\nGuarantees\n\nSafety: no two different values chosen- Liveness: possible under stable leadership #### Drawbacks\nComplex to implement correctly- High messaging overhead &gt; “Paxos is for theorists; Raft is for engineers.”\n\n\n\n4. Raft: Understandable Consensus\nRaft was designed to be simpler and more practical than Paxos, focusing on replicated logs.\n\n\nRoles\n\nLeader: coordinates all changes- Followers: replicate leader’s log- Candidates: during elections #### Workflow\n\n\nLeader Election\n\nTimeout triggers candidate election. - Each follower votes; majority wins.2. Log Replication\nLeader appends entries, sends AppendEntries RPCs. - Followers acknowledge; leader commits when majority ack.3. Safety\nLogs are consistent across majority. - Followers accept only valid prefixes. Raft ensures:\n\n\n\nAt most one leader per term- Committed entries never lost- Logs stay consistent #### Pseudocode Sketch\n\non timeout -&gt; become_candidate()\nsend RequestVote(term, id)\nif majority_votes -&gt; become_leader()\n\non AppendEntries(term, entries):\n    if term &gt;= current_term:\n        append(entries)\n        reply success\n\n\n5. PBFT: Byzantine Fault Tolerance\nPaxos and Raft assume crash faults (nodes stop, not lie). For Byzantine faults (arbitrary behavior), we use PBFT (Practical Byzantine Fault Tolerance).\nTolerates up to f faulty nodes out of 3f + 1 total.\n\n\nPhases\n\nPre-Prepare: Leader proposes value\nPrepare: Nodes broadcast proposal hashes\nCommit: Nodes confirm receipt by 2f+1 votes\n\nUsed in blockchains and critical systems (space, finance).\n\n\n6. Quorum Concept\nConsensus often relies on quorums (majorities):\n\nTwo quorums always intersect, ensuring consistency.- Write quorum + read quorum ≥ total nodes. In Raft/Paxos:\nMajority = N/2 + 1- Guarantees overlap even if some nodes fail.\n\n\n\n7. Log Replication and State Machines\nConsensus underlies Replicated State Machines (RSM):\n\nEvery node applies the same commands in the same order.- Guarantees deterministic, identical states. This model powers:\nDatabases (etcd, Spanner, TiKV)- Coordination systems (ZooKeeper, Consul)- Kubernetes control planes\n\n\n\n8. Leader Election\nAll practical consensus systems need leaders:\n\nSimplifies coordination- Reduces conflicts- Heartbeats detect failures- New elections restore progress Algorithms:\nRaft Election (random timeouts)- Bully Algorithm- Chang-Roberts Ring Election\n\n\n\n9. Performance and Optimization\n\nBatching: amortize RPC overhead- Pipeline: parallelize appends- Read-only optimizations: serve from followers (stale reads)- Witness nodes: participate in quorum without full data Advanced:\nMulti-Paxos: reuse leader, fewer rounds- Fast Paxos: shortcut some phases- Viewstamped Replication: Paxos-like log replication\n\n\n\n10. Why It Matters\nConsensus is the backbone of reliability in modern distributed systems. Every consistent database, service registry, or blockchain depends on it.\nSystems using consensus:\n\netcd, Consul, ZooKeeper - cluster coordination- Raft in Kubernetes - leader election- PBFT in blockchains - fault-tolerant ledgers- Spanner, TiDB - consistent databases &gt; “Consensus is how machines learn to agree, and trust.”\n\n\n\nTry It Yourself\n\nImplement Raft leader election in C or Python.\nSimulate Paxos on 5 nodes with message drops.\nExplore PBFT: try failing nodes and Byzantine behavior.\nCompare performance of Raft vs Paxos under load.\nBuild a replicated key-value store with Raft.\n\n\n\n\n87. Load Balancing and Rate Limiting\nWhen systems scale, no single server can handle all requests alone. Load balancing distributes incoming traffic across multiple servers to improve throughput, reduce latency, and prevent overload. Meanwhile, rate limiting protects systems by controlling how often requests are allowed, ensuring fairness, stability, and security.\nThese two ideas, spreading the load and controlling the flow, are cornerstones of modern distributed systems and APIs.\n\n1. Why Load Balancing Matters\nImagine a web service receiving thousands of requests per second. If every request went to one machine, it would crash. A load balancer (LB) acts as a traffic director, spreading requests across many backends.\nGoals:\n\nEfficiency - fully utilize servers- Reliability - no single point of failure- Scalability - handle growing workloads- Flexibility - add/remove servers dynamically\n\n\n\n2. Types of Load Balancers\n\n\n1. Layer 4 (Transport Layer)\nBalances based on IP and port. Fast and protocol-agnostic (works for TCP/UDP).\nExample: Linux IPVS, Envoy, HAProxy\n\n\n2. Layer 7 (Application Layer)\nUnderstands protocols like HTTP. Can route by URL path, headers, cookies.\nExample: Nginx, Envoy, AWS ALB\n\n\n3. Load Balancing Algorithms\n\n\nRound Robin\nCycles through backends in order.\nReq1 → ServerA  \nReq2 → ServerB  \nReq3 → ServerC\nSimple, fair (if all nodes equal).\n\n\nWeighted Round Robin\nAssigns weights to reflect capacity. Example: ServerA(2x), ServerB(1x)\n\n\nLeast Connections\nSend request to server with fewest active connections.\n\n\nLeast Response Time\nSelect backend with lowest latency (monitored dynamically).\n\n\nHash-Based (Consistent Hashing)\nDeterministically route based on request key (like user ID).\n\nKeeps cache locality- Used in CDNs, distributed caches (e.g. memcached) #### Random\n\nPick a random backend, surprisingly effective under uniform load.\n\n\n4. Consistent Hashing (In Depth)\nUsed for sharding and sticky sessions.\nKey idea:\n\nMap servers to a hash ring- A request’s key is hashed onto the ring- Assigned to next clockwise server When servers join/leave, only small fraction of keys move.\n\nUsed in:\n\nCDNs- Distributed caches (Redis Cluster, DynamoDB)- Load-aware systems\n\n\n\n5. Health Checks and Failover\nA smart LB monitors health of each server:\n\nHeartbeat pings (HTTP/TCP)- Auto-remove unhealthy servers- Rebalance traffic instantly Example: If ServerB fails, remove from rotation:\n\nHealthy: [ServerA, ServerC]\nAlso supports active-passive failover: hot standby servers take over when active fails.\n\n\n6. Global Load Balancing\nAcross regions or data centers:\n\nGeoDNS: route to nearest region- Anycast: advertise same IP globally; routing picks nearest- Latency-based routing: measure and pick lowest RTT Used by CDNs, cloud services, multi-region apps\n\n\n\n7. Rate Limiting: The Other Side\nIf load balancing spreads the work, rate limiting keeps total work reasonable.\nIt prevents:\n\nAbuse (bots, DDoS)- Overload (too many requests)- Fairness issues (no user dominates resources) Policies:\nPer-user, per-IP, per-API-key- Global or per-endpoint\n\n\n\n8. Rate Limiting Algorithms\n\n\nToken Bucket\n\nBucket holds tokens (capacity = burst limit)- Each request consumes 1 token- Tokens refill at constant rate (rate limit)- If empty → reject or delay Good for bursty traffic.\n\nif (tokens &gt; 0) {\n    tokens--;\n    allow();\n} else reject();\n\n\nLeaky Bucket\n\nRequests flow into a bucket, drain at fixed rate- Excess = overflow = dropped Smooths bursts; used for shaping.\n\n\n\nFixed Window Counter\n\nCount requests in fixed interval (e.g. 1s)- Reset every window- Simple but unfair around boundaries #### Sliding Window Log / Sliding Window Counter\nMaintain timestamps of requests- Remove old ones beyond time window- More accurate and fair\n\n\n\n9. Combining Both\nA full system might:\n\nUse rate limiting per user or service- Use load balancing across nodes- Apply circuit breakers when overload persists Together, they form resilient architectures that stay online even under spikes.\n\n\n\n10. Why It Matters\nThese techniques make large-scale systems:\n\nScalable - handle millions of users- Stable - prevent cascading failures- Fair - each client gets a fair share- Resilient - recover gracefully from spikes or node loss Used in:\nAPI Gateways (Kong, Envoy, Nginx)- Cloud Load Balancers (AWS ALB, GCP LB)- Kubernetes Ingress and Service Meshes- Distributed Caches and Databases &gt; “Balance keeps systems alive. Limits keep them sane.”\n\n\n\nTry It Yourself\n\nSimulate Round Robin and Least Connections balancing across 3 servers.\nImplement a Token Bucket rate limiter in C or Python.\nTest burst traffic, observe drops or delays.\nCombine Consistent Hashing with Token Bucket for user-level control.\nVisualize how load balancing + rate limiting keep system latency low.\n\n\n\n\n88. Search and Indexing (Inverted, BM25, WAND)\nSearch engines, whether web-scale like Google or local like SQLite’s FTS, rely on efficient indexing and ranking to answer queries fast. Instead of scanning all documents, they use indexes (structured lookup tables) to quickly find relevant matches.\nThis section explores inverted indexes, ranking algorithms (TF-IDF, BM25), and efficient retrieval techniques like WAND.\n\n1. The Search Problem\nGiven:\n\nA corpus of documents- A query (e.g., “machine learning algorithms”) We want to return:\nRelevant documents- Ranked by importance and similarity Naive search → O(N × M) comparisons Inverted indexes → O(K log N), where K = terms in query\n\n\n\n2. Inverted Index: The Heart of Search\nAn inverted index maps terms → documents containing them.\n\n\nExample\n\n\n\nTerm\nPostings List\n\n\n\n\n“data”\n[1, 4, 5]\n\n\n“algorithm”\n[2, 3, 5]\n\n\n“machine”\n[1, 2]\n\n\n\nEach posting may include:\n\ndocID- term frequency (tf)- positions (for phrase search) #### Construction Steps\n\n\nTokenize documents → words\nNormalize (lowercase, stemming, stopword removal)\nBuild postings: term → [docIDs, tf, positions]\nSort & compress for storage efficiency\n\nUsed by:\n\nElasticsearch, Lucene, Whoosh, Solr\n\n\n\n3. Boolean Retrieval\nSimplest model:\n\nQuery = Boolean expression e.g. (machine AND learning) OR AI\n\nUse set operations on postings:\n\nAND → intersection- OR → union- NOT → difference Fast intersection uses merge algorithm on sorted lists.\n\nvoid intersect(int A[], int B[], int n, int m) {\n    int i = 0, j = 0;\n    while (i &lt; n && j &lt; m) {\n        if (A[i] == B[j]) { print(A[i]); i++; j++; }\n        else if (A[i] &lt; B[j]) i++;\n        else j++;\n    }\n}\nBut Boolean search doesn’t rank results, so we need scoring models.\n\n\n4. Vector Space Model\nRepresent documents and queries as term vectors. Each dimension = term weight (tf-idf).\n\ntf: term frequency in document- idf: inverse document frequency \\(idf = \\log\\frac{N}{df_t}\\)\n\nCosine similarity measures relevance: \\[\n\\text{score}(q, d) = \\frac{q \\cdot d}{|q| |d|}\n\\]\nSimple, interpretable, forms basis of BM25 and modern embeddings.\n\n\n5. BM25: The Classic Ranking Function\nBM25 (Best Match 25) is the de facto standard in information retrieval.\n\\[\n\\text{score}(q, d) = \\sum_{t \\in q} IDF(t) \\cdot \\frac{f(t, d) \\cdot (k_1 + 1)}{f(t, d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{avgdl})}\n\\]\nWhere:\n\n( f(t, d) ): term frequency- ( |d| ): doc length- ( avgdl ): average doc length- \\(k_1, b\\): tunable params (typ. 1.2-2.0, 0.75) #### Advantages\nBalances term frequency, document length, and rarity- Fast and effective baseline- Still used in Elasticsearch, Lucene, OpenSearch\n\n\n\n6. Efficiency Tricks: WAND, Block-Max WAND\nRanking involves merging multiple postings. We can skip irrelevant documents early with WAND (Weak AND).\n\n\nWAND Principle\n\nEach term has upper-bound score- Maintain pointers in each posting- Compute potential max score- If max &lt; current threshold, skip doc Improves latency for top-k retrieval.\n\nVariants:\n\nBMW (Block-Max WAND) - uses block-level score bounds- MaxScore - simpler thresholding- Dynamic pruning - skip unpromising candidates\n\n\n\n7. Index Compression\nPostings lists are long, compression is crucial.\nCommon schemes:\n\nDelta encoding: store gaps between docIDs- Variable-byte (VB) or Gamma coding- Frame of Reference (FOR) and SIMD-BP128 for vectorized decoding Goal: smaller storage + faster decompression\n\n\n\n8. Advanced Retrieval\n\n\nProximity Search\nRequire words appear near each other. Use positional indexes.\n\n\nPhrase Search\nMatch exact sequences using positions: “machine learning” ≠ “learning machine”\n\n\nFuzzy / Approximate Search\nAllow typos: Use Levenshtein automata, n-grams, or k-approximate matching\n\n\nFielded Search\nScore per field (title, body, tags) Weighted combination\n\n\n9. Learning-to-Rank and Semantic Search\nModern search adds ML-based re-ranking:\n\nLearning to Rank (LTR): use features (tf, idf, BM25, clicks)- Neural re-ranking: BERT-style embeddings for semantic similarity- Hybrid retrieval: combine BM25 + dense vectors (e.g. ColBERT, RRF) Also: ANN (Approximate Nearest Neighbor) for vector-based search.\n\n\n\n10. Why It Matters\nEfficient search powers:\n\nWeb search engines- IDE symbol lookup- Log search, code search- Database full-text search- AI retrieval pipelines (RAG) It’s where algorithms meet language and scale.\n\n\n“Search is how we connect meaning to memory.”\n\n\n\nTry It Yourself\n\nBuild a tiny inverted index in C or Python.\nImplement Boolean AND and OR queries.\nCompute TF-IDF and BM25 scores for a toy dataset.\nAdd WAND pruning for top-k retrieval.\nCompare BM25 vs semantic embeddings for relevance.\n\n\n\n\n89. Compression and Encoding in Systems\nCompression and encoding algorithms are the quiet workhorses of computing, shrinking data to save space, bandwidth, and time. They allow systems to store more, transmit faster, and process efficiently. From files and databases to networks and logs, compression shapes nearly every layer of system design.\n\n1. Why Compression Matters\nCompression is everywhere:\n\nDatabases - column stores, indexes, logs- Networks - HTTP, TCP, QUIC payloads- File systems - ZFS, NTFS, btrfs compression- Streaming - video/audio codecs- Logs & telemetry - reduce I/O and storage cost Benefits:\nSmaller data = faster I/O- Less storage = lower cost- Less transfer = higher throughput Trade-offs:\nCPU overhead (compression/decompression)- Latency (especially for small data)- Suitability (depends on entropy and structure)\n\n\n\n2. Key Concepts\n\n\nEntropy\nMinimum bits needed to represent data (Shannon). High entropy → less compressible.\n\n\nRedundancy\nCompression exploits repetition and patterns.\n\n\nLossless vs Lossy\n\nLossless: reversible (ZIP, PNG, LZ4)- Lossy: approximate (JPEG, MP3, H.264) In system contexts, lossless dominates.\n\n\n\n3. Common Lossless Compression Families\n\n\nHuffman Coding\n\nPrefix-free variable-length codes- Frequent symbols = short codes- Optimal under symbol-level model Used in: DEFLATE, JPEG, MP3\n\n\n\nArithmetic Coding\n\nEncodes sequence as fractional interval- More efficient than Huffman for skewed distributions- Used in: H.264, bzip2, AV1 #### Dictionary-Based (LZ77, LZ78)\nReplace repeated substrings with references- Core of ZIP, gzip, zlib, LZMA, Snappy #### LZ77 Sketch\n\nwhile (not EOF) {\n    find longest match in sliding window;\n    output (offset, length, next_char);\n}\nVariants:\n\nLZ4 - fast, lower ratio- Snappy - optimized for speed- Zstandard (Zstd) - tunable speed/ratio, dictionary support #### Burrows-Wheeler Transform (BWT)\nReorders data to group similar symbols- Followed by Move-To-Front + Huffman- Used in bzip2, BWT-based compressors #### Run-Length Encoding (RLE)\nReplace consecutive repeats with (symbol, count)- Great for structured or sparse data Example: AAAAABBBCC → (A,5)(B,3)(C,2)\n\n\n\n4. Specialized Compression in Systems\n\n\nColumnar Databases\nCompress per column:\n\nDictionary encoding - map strings → ints- Run-length encoding - good for sorted columns- Delta encoding - store differences (time series)- Bit-packing - fixed-width integers in minimal bits Combine multiple for optimal ratio.\n\nExample (time deltas):\n[100, 102, 103, 107] → [100, +2, +1, +4]\n\n\nLog and Telemetry Compression\n\nStructured formats → fieldwise encoding- Often Snappy or LZ4 for fast decode- Aggregators (Fluentd, Loki, Kafka) rely heavily on them #### Data Lakes and Files\nParquet, ORC, Arrow → columnar + compressed- Choose codec per column: LZ4 for speed, Zstd for ratio\n\n\n\n5. Streaming and Chunked Compression\nLarge data often processed in chunks:\n\nEnables random access and parallelism- Needed for network streams, distributed files Example: zlib block, Zstd frame, gzip chunk\n\nUsed in:\n\nHTTP chunked encoding- Kafka log segments- MapReduce shuffle\n\n\n\n6. Encoding Schemes\nCompression ≠ encoding. Encoding ensures safe transport.\n\n\nBase64\n\nMaps 3 bytes → 4 chars- 33% overhead- Used for binary → text (emails, JSON APIs) #### URL Encoding\nEscape unsafe chars with %xx #### Delta Encoding\nStore differences, not full values #### Varint / Zigzag Encoding\nCompact integers (e.g. protobufs)- Smaller numbers → fewer bytes Example:\n\nwhile (x &gt;= 0x80) { emit((x & 0x7F) | 0x80); x &gt;&gt;= 7; }\nemit(x);\n\n\n7. Adaptive and Context Models\nModern compressors adapt to local patterns:\n\nPPM (Prediction by Partial Matching)- Context mixing (PAQ)- Zstd uses FSE (Finite State Entropy) coding Balance between speed, memory, and compression ratio.\n\n\n\n8. Hardware Acceleration\nCompression can be offloaded to:\n\nCPUs with SIMD (AVX2, SSE4.2)- GPUs (parallel encode/decode)- NICs / SmartNICs- ASICs (e.g., Intel QAT) Critical for high-throughput databases, network appliances, storage systems.\n\n\n\n9. Design Trade-offs\n\n\n\nGoal\nBest Choice\n\n\n\n\nMax speed\nLZ4, Snappy\n\n\nMax ratio\nZstd, LZMA\n\n\nBalance\nZstd (tunable)\n\n\nColumn store\nRLE, Delta, Dict\n\n\nLogs / telemetry\nSnappy, LZ4\n\n\nArchival\nbzip2, xz\n\n\nReal-time\nLZ4, Brotli (fast mode)\n\n\n\nChoose based on CPU budget, I/O cost, latency tolerance.\n\n\n10. Why It Matters\nCompression is a first-class optimization:\n\nSaves petabytes in data centers- Boosts throughput across networks- Powers cloud storage (S3, BigQuery, Snowflake)- Enables efficient analytics and ML pipelines &gt; “Every byte saved is time earned.”\n\n\n\nTry It Yourself\n\nCompress text using Huffman coding (build frequency table).\nCompare gzip, Snappy, and Zstd on a 1GB dataset.\nImplement delta encoding and RLE for numeric data.\nTry dictionary encoding on repetitive strings.\nMeasure compression ratio, speed, and CPU usage trade-offs.\n\n\n\n\n90. Fault Tolerance and Replication\nModern systems must survive hardware crashes, network partitions, or data loss without stopping. Fault tolerance ensures that a system continues to function, even when parts fail. Replication underpins this resilience, duplicating data or computation across multiple nodes for redundancy, performance, and consistency.\nTogether, they form the backbone of reliability in distributed systems.\n\n1. Why Fault Tolerance?\nNo system is perfect:\n\nServers crash- Disks fail- Networks partition- Power goes out The question isn’t if failure happens, but when. Fault-tolerant systems detect, contain, and recover from failure automatically.\n\nGoals:\n\nAvailability - keep serving requests- Durability - never lose data- Consistency - stay correct across replicas\n\n\n\n2. Failure Models\n\n\nCrash Faults\nNode stops responding but doesn’t misbehave. Handled by restarts or replication (Raft, Paxos).\n\n\nOmission Faults\nLost messages or dropped updates. Handled with retries and acknowledgments.\n\n\nByzantine Faults\nArbitrary/malicious behavior. Handled by Byzantine Fault Tolerance (PBFT), expensive but robust.\n\n\n3. Redundancy: The Core Strategy\nFault tolerance = redundancy + detection + recovery\nRedundancy types:\n\nHardware: multiple power supplies, disks (RAID)- Software: replicated services, retries- Data: multiple copies, erasure codes- Temporal: retry or checkpoint and replay\n\n\n\n4. Replication Models\n\n\n1. Active Replication\nAll replicas process requests in parallel (lockstep). Results must match. Used in real-time and Byzantine systems.\n\n\n2. Passive (Primary-Backup)\nOne leader (primary) handles requests. Backups replicate log, take over on failure. Used in Raft, ZooKeeper, PostgreSQL streaming.\n\n\n3. Quorum Replication\nWrites and reads contact majority of replicas. Ensures overlap → consistency. Used in Cassandra, DynamoDB, Etcd.\n\n\n5. Consistency Models\nReplication introduces a trade-off between consistency and availability (CAP theorem).\n\n\nStrong Consistency\nAll clients see the same value immediately. Example: Raft, Etcd, Spanner.\n\n\nEventual Consistency\nReplicas converge over time. Example: DynamoDB, Cassandra.\n\n\nCausal Consistency\nPreserves causal order of events. Example: Vector clocks, CRDTs.\nChoice depends on workload:\n\nBanking → strong- Social feeds → eventual- Collaborative editing → causal\n\n\n\n6. Checkpointing and Recovery\nTo recover after crash:\n\nPeriodically checkpoint state- On restart, replay log of missed events Example: Databases → Write-Ahead Log (WAL) Stream systems → Kafka checkpoints\n\n1. Save state to disk\n2. Record latest log position\n3. On restart → reload + replay\n\n\n7. Erasure Coding\nInstead of full copies, store encoded fragments. With ( k ) data blocks, ( m ) parity blocks → tolerate ( m ) failures.\nExample: Reed-Solomon (used in HDFS, Ceph)\n\n\n\nk\nm\nTotal\nFault Tolerance\n\n\n\n\n4\n2\n6\n2 failures\n\n\n\nBetter storage efficiency than 3× replication.\n\n\n8. Failure Detection\nDetecting failure is tricky in distributed systems (because of latency). Common techniques:\n\nHeartbeats - periodic “I’m alive” messages- Timeouts - suspect node if no heartbeat- Gossip protocols - share failure info among peers Used in Consul, Cassandra, Kubernetes health checks.\n\n\n\n9. Self-Healing Systems\nAfter failure:\n\nDetect it\nIsolate faulty component\nReplace or restart\nRebalance load or re-replicate data\n\nPatterns:\n\nSupervisor trees (Erlang/Elixir)- Self-healing clusters (Kubernetes)- Rebalancing (Cassandra ring repair) “Never trust a single machine, trust the system.”\n\n\n\n10. Why It Matters\nFault tolerance turns fragile infrastructure into reliable services.\nUsed in:\n\nDatabases (replication + WAL)- Distributed storage (HDFS, Ceph, S3)- Orchestration (Kubernetes controllers)- Streaming systems (Kafka, Flink) Without replication and fault tolerance, large-scale systems would collapse under failure.\n\n\n“Resilience is built, not assumed.”\n\n\n\nTry It Yourself\n\nBuild a primary-backup key-value store: leader writes, follower replicates.\nAdd heartbeat + timeout detection to trigger failover.\nSimulate partition: explore behavior under strong vs eventual consistency.\nImplement checkpoint + replay recovery for a small app.\nCompare 3× replication vs Reed-Solomon (4+2) for space and reliability.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-10.-ai-ml-and-optimization",
    "href": "books/en-us/book.html#chapter-10.-ai-ml-and-optimization",
    "title": "The Book",
    "section": "Chapter 10. AI, ML and Optimization",
    "text": "Chapter 10. AI, ML and Optimization\n\n91. Classical ML (k-means, Naive Bayes, SVM, Decision Trees)\nClassical machine learning is built on interpretable mathematics and solid optimization foundations. Long before deep learning, these algorithms powered search engines, spam filters, and recommendation systems. They’re still used today, fast, explainable, and easy to deploy.\nThis section covers the four pillars of classical ML:\n\nk-means - unsupervised clustering- Naive Bayes - probabilistic classification- SVM - margin-based classification- Decision Trees - rule-based learning\n\n\n1. The Essence of Classical ML\nClassical ML is about learning from data using statistical principles, often without huge compute. Given dataset ( D = {\\(x_i, y_i\\)} ), the task is to:\n\nPredict ( y ) from ( x )- Generalize beyond seen data- Balance bias and variance\n\n\n\n2. k-means Clustering\nGoal: partition data into ( k ) groups (clusters) such that intra-cluster distance is minimized.\n\n\nObjective\n\\[\n\\min_{C} \\sum_{i=1}^k \\sum_{x \\in C_i} |x - \\mu_i|^2\n\\] Where \\(\\mu_i\\) = centroid of cluster ( i ).\n\n\nAlgorithm\n\nChoose ( k ) random centroids\nAssign each point to nearest centroid\nRecompute centroids\nRepeat until stable\n\n\n\nTiny Code (C-style)\nfor (iter = 0; iter &lt; max_iter; iter++) {\n    assign_points_to_clusters();\n    recompute_centroids();\n}\n\n\nPros\n\nSimple, fast (( O(nkd) ))- Works well for spherical clusters #### Cons\nRequires ( k )- Sensitive to initialization, outliers Variants:\nk-means++ (better initialization)- Mini-batch k-means (scalable)\n\n\n\n3. Naive Bayes Classifier\nA probabilistic model using Bayes’ theorem under independence assumptions.\n\\[\nP(y|x) \\propto P(y) \\prod_{i=1}^n P(x_i | y)\n\\]\n\n\nAlgorithm\n\nCompute prior ( P(y) )\nCompute likelihood ( P\\(x_i | y\\) )\nPredict class with max posterior\n\n\n\nTypes\n\nMultinomial NB - text (bag of words)- Gaussian NB - continuous features- Bernoulli NB - binary features #### Example (Spam Detection)\n\nP(spam | \"win money\") ∝ P(spam) * P(\"win\"|spam) * P(\"money\"|spam)\n\n\nPros\n\nFast, works well for text- Needs little data- Probabilistic interpretation #### Cons\nAssumes feature independence- Poor for correlated features\n\n\n\n4. Support Vector Machines (SVM)\nSVM finds the max-margin hyperplane separating classes.\n\n\nObjective\nMaximize margin = distance between boundary and nearest points.\n\\[\n\\min_{w, b} \\frac{1}{2} |w|^2 \\quad \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\ge 1\n\\]\nCan be solved via Quadratic Programming.\n\n\nIntuition\n\nEach data point → vector- Hyperplane: \\(w \\cdot x + b = 0\\)- Support vectors = boundary points #### Kernel Trick\n\nTransform input via kernel ( K\\(x_i, x_j\\) = \\(x_i\\) \\(x_j\\) ):\n\nLinear: dot product- Polynomial: ( \\(x_i \\cdot x_j + c\\)^d )- RBF: \\(e^{-\\gamma |x_i - x_j|^2}\\) #### Pros\nEffective in high dimensions- Can model nonlinear boundaries- Few hyperparameters #### Cons\nSlow on large data- Harder to tune kernel parameters\n\n\n\n5. Decision Trees\nIf-else structure for classification/regression.\nAt each node:\n\nPick feature ( f ) and threshold ( t )- Split to maximize information gain #### Metrics\nEntropy: \\(H = -\\sum p_i \\log p_i\\)- Gini: \\(G = 1 - \\sum p_i^2\\) #### Pseudocode\n\nif (feature &lt; threshold)\n    go left;\nelse\n    go right;\nBuild recursively until:\n\nMax depth- Min samples per leaf- Pure nodes #### Pros\nInterpretable- Handles mixed data- No scaling needed #### Cons\nProne to overfitting- Unstable (small data changes) Fixes:\nPruning (reduce depth)- Ensembles: Random Forests, Gradient Boosting\n\n\n\n6. Bias-Variance Tradeoff\n\n\n\nAlgorithm\nBias\nVariance\n\n\n\n\nk-means\nHigh\nLow\n\n\nNaive Bayes\nHigh\nLow\n\n\nSVM\nLow\nMedium\n\n\nDecision Tree\nLow\nHigh\n\n\n\nBalancing both = good generalization.\n\n\n7. Evaluation Metrics\nFor classification:\n\nAccuracy, Precision, Recall, F1-score- ROC-AUC, Confusion Matrix For clustering:\nInertia, Silhouette Score Always use train/test split or cross-validation.\n\n\n\n8. Scaling to Large Data\nTechniques:\n\nMini-batch training- Online updates (SGD)- Dimensionality reduction (PCA)- Approximation (Random Projections) Libraries:\nscikit-learn (Python)- liblinear, libsvm (C/C++)- MLlib (Spark)\n\n\n\n9. When to Use What\n\n\n\nTask\nRecommended Algorithm\n\n\n\n\nText classification\nNaive Bayes\n\n\nClustering\nk-means\n\n\nNonlinear classification\nSVM (RBF)\n\n\nTabular data\nDecision Tree\n\n\nQuick baseline\nLogistic Regression / NB\n\n\n\n\n\n10. Why It Matters\nThese algorithms are fast, interpretable, and theoretical foundations of modern ML. They remain the go-to choice for:\n\nSmall to medium datasets- Real-time classification- Explainable AI &gt; “Classical ML is the art of solving problems with math you can still write on a whiteboard.”\n\n\n\nTry It Yourself\n\nCluster 2D points with k-means, plot centroids.\nTrain Naive Bayes on a spam/ham dataset.\nClassify linearly separable data with SVM.\nBuild a Decision Tree from scratch (entropy, Gini).\nCompare models’ accuracy and interpretability.\n\n\n\n\n92. Ensemble Methods (Bagging, Boosting, Random Forests)\nEnsemble methods combine multiple weak learners to build a strong predictor. Instead of relying on one model, ensembles vote, average, or boost multiple models, improving stability and accuracy.\nThey are the bridge between classical and modern ML , simple models, combined smartly, become powerful.\n\n1. The Core Idea\n\n“Many weak learners, when combined, can outperform a single strong one.”\n\nMathematically, if \\(f_1, f_2, \\ldots, f_k\\) are weak learners, an ensemble predictor is:\n\\[\nF(x) = \\frac{1}{k}\\sum_{i=1}^k f_i(x)\n\\]\nFor classification, combine via majority vote. For regression, combine via average.\n\n\n2. Bagging (Bootstrap Aggregating)\nBagging reduces variance by training models on different samples.\n\n\nSteps\n\nDraw ( B ) bootstrap samples from dataset ( D ).\nTrain one model per sample.\nAggregate predictions by averaging or voting.\n\n\\[\n\\hat{f}*{bag}(x) = \\frac{1}{B} \\sum*{b=1}^B f_b(x)\n\\]\nEach \\(f_b\\) is trained on a random subset (with replacement).\n\n\nExample\n\nBase learner: Decision Tree- Ensemble: Bagged Trees- Famous instance: Random Forest #### Tiny Code (C-style Pseudocode)\n\nfor (int b = 0; b &lt; B; b++) {\n    D_b = bootstrap_sample(D);\n    model[b] = train_tree(D_b);\n}\nprediction = average_predictions(model, x);\n\n\nPros\n\nReduces variance- Works well with high-variance learners- Parallelizable #### Cons\nIncreases computation- Doesn’t reduce bias\n\n\n\n3. Random Forest\nA bagging-based ensemble of decision trees with feature randomness.\n\n\nKey Ideas\n\nEach tree trained on bootstrap sample.- At each split, consider random subset of features.- Final prediction = majority vote or average. This decorrelates trees, improving generalization.\n\n\\[\nF(x) = \\frac{1}{B} \\sum_{b=1}^{B} T_b(x)\n\\]\n\n\nPros\n\nHandles large feature sets- Low overfitting- Good default for tabular data #### Cons\nLess interpretable- Slower on huge datasets OOB (Out-of-Bag) error = internal validation from unused samples.\n\n\n\n4. Boosting\nBoosting focuses on reducing bias by sequentially training models , each one corrects errors from the previous.\n\n\nSteps\n\nStart with weak learner ( f_1(x) )\nTrain next learner ( f_2(x) ) on residuals/errors\nCombine with weighted sum\n\n\\[\nF_m(x) = F_{m-1}(x) + \\alpha_m f_m(x)\n\\]\nWeights \\(\\alpha_m\\) focus on difficult examples.\n\n\nTiny Code (Conceptual)\nF = 0;\nfor (int m = 0; m &lt; M; m++) {\n    residual = y - predict(F, x);\n    f_m = train_weak_learner(x, residual);\n    F += alpha[m] * f_m;\n}\n\n\n5. AdaBoost (Adaptive Boosting)\nAdaBoost adapts weights on samples after each iteration.\n\n\nAlgorithm\n\nInitialize weights: \\(w_i = \\frac{1}{n}\\)\nTrain weak classifier \\(f_t\\)\nCompute error: \\(\\epsilon_t\\)\nUpdate weights: \\[\nw_i \\leftarrow w_i \\cdot e^{\\alpha_t \\cdot I(y_i \\ne f_t(x_i))}\n\\] where \\(\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\\)\nNormalize weights\n\nFinal classifier: \\[\nF(x) = \\text{sign}\\left( \\sum_t \\alpha_t f_t(x) \\right)\n\\]\n\n\nPros\n\nHigh accuracy on clean data- Simple and interpretable weights #### Cons\nSensitive to outliers- Sequential → not easily parallelizable\n\n\n\n6. Gradient Boosting\nA modern version of boosting using gradient descent on loss.\nAt each step, fit new model to negative gradient of loss function.\n\n\nObjective\n\\[\nF_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n\\]\nwhere \\(h_m(x) \\approx -\\frac{\\partial L(y, F(x))}{\\partial F(x)}\\)\n\n\nCommon Libraries\n\nXGBoost\nLightGBM\nCatBoost #### Pros\nHigh performance on tabular data- Flexible (custom loss)- Handles mixed feature types #### Cons\nSlower to train- Sensitive to hyperparameters\n\n\n\n7. Stacking (Stacked Generalization)\nCombine multiple models (base learners) via a meta-model.\n\n\nSteps\n\nTrain base models (SVM, Tree, NB, etc.)\nCollect their predictions\nTrain meta-model (e.g. Logistic Regression) on outputs\n\n\\[\n\\hat{y} = f_{meta}(f_1(x), f_2(x), \\ldots, f_k(x))\n\\]\n\n\n8. Bagging vs Boosting\n\n\n\nFeature\nBagging\nBoosting\n\n\n\n\nStrategy\nParallel\nSequential\n\n\nGoal\nReduce variance\nReduce bias\n\n\nWeighting\nUniform\nAdaptive\n\n\nExample\nRandom Forest\nAdaBoost, XGBoost\n\n\n\n\n\n9. Bias-Variance Behavior\n\nBagging: ↓ variance- Boosting: ↓ bias- Random Forest: balanced- Stacking: flexible but complex\n\n\n\n10. Why It Matters\nEnsemble methods are the workhorses of classical ML competitions and real-world tabular problems. They blend interpretability, flexibility, and predictive power.\n\n“One tree may fall, but a forest stands strong.”\n\n\n\nTry It Yourself\n\nTrain a Random Forest on the Iris dataset.\nImplement AdaBoost from scratch using decision stumps.\nCompare Bagging vs Boosting accuracy.\nTry XGBoost with different learning rates.\nVisualize feature importance across models.\n\n\n\n\n93. Gradient Methods (SGD, Adam, RMSProp)\nGradient-based optimization is the heartbeat of machine learning. These methods update parameters iteratively by following the negative gradient of the loss function. They power everything from linear regression to deep neural networks.\n\n1. The Core Idea\nWe want to minimize a loss function ( L\\(\\theta\\) ). Starting from some initial parameters \\(\\theta_0\\), we move in the opposite direction of the gradient:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta L(\\theta_t)\n\\]\nwhere \\(\\eta\\) is the learning rate (step size).\nThe gradient tells us which way the function increases fastest , we move the other way.\n\n\n2. Batch Gradient Descent\nUses the entire dataset to compute the gradient.\n\\[\n\\nabla_\\theta L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\ell_i(\\theta)\n\\]\n\nAccurate but slow for large ( N )- Each update is expensive Tiny Code\n\nfor (int t = 0; t &lt; T; t++) {\n    grad = compute_full_gradient(data, theta);\n    theta = theta - eta * grad;\n}\nGood for: small datasets or convex problems\n\n\n3. Stochastic Gradient Descent (SGD)\nInstead of full data, use one random sample per step.\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\ell_i(\\theta_t)\n\\]\n\nNoisy but faster updates- Can escape local minima- Great for online learning Tiny Code\n\nfor each sample (x_i, y_i):\n    grad = grad_loss(theta, x_i, y_i);\n    theta -= eta * grad;\nPros\n\nFast convergence- Works on large datasets Cons\nNoisy updates- Requires learning rate tuning\n\n\n\n4. Mini-Batch Gradient Descent\nCompromise between batch and stochastic.\nUse small subset (mini-batch) of samples:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{1}{m} \\sum_{i=1}^m \\nabla_\\theta \\ell_i(\\theta_t)\n\\]\nUsually batch size = 32 or 64. Faster, more stable updates.\n\n\n5. Momentum\nAdds velocity to smooth oscillations.\n\\[\nv_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_\\theta L(\\theta_t)\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\eta v_t\n\\]\nThis accumulates past gradients to speed movement in consistent directions.\nThink of it like a heavy ball rolling down a hill.\n\n\n6. Nesterov Accelerated Gradient (NAG)\nImproves momentum by looking ahead:\n\\[\nv_t = \\beta v_{t-1} + \\eta \\nabla_\\theta L(\\theta_t - \\beta v_{t-1})\n\\]\nIt anticipates the future position before computing the gradient.\nFaster convergence in convex settings.\n\n\n7. RMSProp\nAdjusts learning rate per parameter using exponential average of squared gradients:\n\\[\nE[g^2]*t = \\rho E[g^2]*{t-1} + (1 - \\rho) g_t^2\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n\\]\nThis helps when gradients vary in magnitude.\nGood for: non-stationary objectives, deep networks\n\n\n8. Adam (Adaptive Moment Estimation)\nCombines momentum + RMSProp:\n\\[\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n\\]\n\\[\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n\\]\nBias-corrected estimates:\n\\[\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n\\]\nUpdate rule:\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\]\nTiny Code (Conceptual)\nm = 0; v = 0;\nfor (int t = 1; t &lt;= T; t++) {\n    g = grad(theta);\n    m = beta1 * m + (1 - beta1) * g;\n    v = beta2 * v + (1 - beta2) * g * g;\n    m_hat = m / (1 - pow(beta1, t));\n    v_hat = v / (1 - pow(beta2, t));\n    theta -= eta * m_hat / (sqrt(v_hat) + eps);\n}\nPros\n\nWorks well out of the box- Adapts learning rate- Great for deep learning Cons\nMay not converge exactly- Needs decay schedule for stability\n\n\n\n9. Learning Rate Schedules\nControl \\(\\eta\\) over time:\n\nStep decay: \\(\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}\\)- Exponential decay: \\(\\eta_t = \\eta_0 e^{-\\lambda t}\\)- Cosine annealing: smooth periodic decay- Warm restarts: reset learning rate periodically\n\n\n\n10. Why It Matters\nAll modern deep learning is built on gradients. Choosing the right optimizer can mean faster training and better accuracy.\n\n\n\nOptimizer\nAdaptive\nMomentum\nCommon Use\n\n\n\n\nSGD\nNo\nOptional\nSimple tasks\n\n\nSGD + Momentum\nNo\nYes\nConvNets\n\n\nRMSProp\nYes\nNo\nRNNs\n\n\nAdam\nYes\nYes\nTransformers, DNNs\n\n\n\n\n“Optimization is the art of taking small steps in the right direction , many times over.”\n\n\n\nTry It Yourself\n\nImplement SGD and Adam on a linear regression task.\nCompare training curves for SGD, Momentum, RMSProp, and Adam.\nExperiment with learning rate schedules.\nVisualize optimization paths on a 2D contour plot.\n\n\n\n\n94. Deep Learning (Backpropagation, Dropout, Normalization)\nDeep learning is about stacking layers of computation so that the network can learn hierarchical representations. From raw pixels to abstract features, deep nets build meaning through composition of functions.\nAt the core of this process are three ideas: backpropagation, regularization (dropout), and normalization.\n\n1. The Essence of Deep Learning\nA neural network is a chain of functions:\n\\[\nf(x; \\theta) = f_L(f_{L-1}(\\cdots f_1(x)))\n\\]\nEach layer transforms its input and passes it on.\nTraining involves finding parameters \\(\\theta\\) that minimize a loss ( L(f\\(x; \\theta\\), y) ).\n\n\n2. Backpropagation\nBackpropagation is the algorithm that teaches neural networks.\nIt uses the chain rule of calculus to efficiently compute gradients layer by layer.\nFor each layer ( i ):\n\\[\n\\frac{\\partial L}{\\partial \\theta_i} = \\frac{\\partial L}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial \\theta_i}\n\\]\nand propagate backward:\n\\[\n\\frac{\\partial L}{\\partial a_{i-1}} = \\frac{\\partial L}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial a_{i-1}}\n\\]\nSo every neuron learns how much it contributed to the error.\nTiny Code\n// Pseudocode for 2-layer network\nforward:\n    z1 = W1*x + b1;\n    a1 = relu(z1);\n    z2 = W2*a1 + b2;\n    y_hat = softmax(z2);\n    loss = cross_entropy(y_hat, y);\n\nbackward:\n    dz2 = y_hat - y;\n    dW2 = dz2 * a1.T;\n    db2 = sum(dz2);\n    da1 = W2.T * dz2;\n    dz1 = da1 * relu_grad(z1);\n    dW1 = dz1 * x.T;\n    db1 = sum(dz1);\nEach gradient is computed by local differentiation and multiplied back.\n\n\n3. Activation Functions\nNonlinear activations let networks approximate nonlinear functions.\n\n\n\nFunction\nFormula\nUse\n\n\n\n\nReLU\n\\(\\max(0, x)\\)\nDefault, fast\n\n\nSigmoid\n\\(\\frac{1}{1 + e^{-x}}\\)\nProbabilities\n\n\nTanh\n\\(\\tanh(x)\\)\nCentered activations\n\n\nGELU\n\\(x \\, \\Phi(x)\\)\nModern transformers\n\n\n\nWithout nonlinearity, stacking layers is just one big linear transformation.\n\n\n4. Dropout\nDropout is a regularization technique that prevents overfitting. During training, randomly turn off neurons:\n\\[\n\\tilde{a}_i = a_i \\cdot m_i, \\quad m_i \\sim \\text{Bernoulli}(p)\n\\]\nAt inference, scale activations by ( p ) (keep probability).\nIt forces the network to not rely on any single path.\nTiny Code\nfor (int i = 0; i &lt; n; i++) {\n    if (rand_uniform() &lt; p) a[i] = 0;\n    else a[i] /= p; // scaling\n}\n\n\n5. Normalization\nNormalization stabilizes and speeds up training by reducing internal covariate shift.\n\n\nBatch Normalization\nNormalize activations per batch:\n\\[\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\]\n\\[\ny = \\gamma \\hat{x} + \\beta\n\\]\nLearnable parameters \\(\\gamma, \\beta\\) restore flexibility.\nBenefits:\n\nSmooth gradients- Allows higher learning rates- Acts as regularizer #### Layer Normalization\n\nUsed in transformers (normalizes across features, not batch).\n\n\n6. Initialization\nProper initialization helps gradients flow.\n\n\n\nScheme\nFormula\nUse\n\n\n\n\nXavier\n\\(\\text{Var}(W) = \\frac{1}{n_{in}}\\)\nTanh\n\n\nHe\n\\(\\text{Var}(W) = \\frac{2}{n_{in}}\\)\nReLU\n\n\n\nPoor initialization can lead to vanishing or exploding gradients.\n\n\n7. Training Pipeline\n\nInitialize weights\nForward pass\nCompute loss\nBackward pass (backprop)\nUpdate weights (e.g. with Adam)\n\nRepeat until convergence.\n\n\n8. Deep Architectures\n\n\n\nModel\nKey Idea\nTypical Use\n\n\n\n\nMLP\nFully connected\nTabular data\n\n\nCNN\nConvolutions\nImages\n\n\nRNN\nSequential recurrence\nTime series, text\n\n\nTransformer\nSelf-attention\nLanguage, vision\n\n\n\nEach architecture stacks linear operations and nonlinearities in different ways.\n\n\n9. Overfitting and Regularization\nCommon fixes:\n\nDropout- Weight decay (\\(L_2\\) regularization)- Data augmentation- Early stopping The key is to improve generalization, not just minimize training loss.\n\n\n\n10. Why It Matters\nBackpropagation turned neural networks from theory to practice. Normalization made them train faster. Dropout made them generalize better.\nTogether, they unlocked the deep learning revolution.\n\n“Depth gives power, but gradients give life.”\n\n\n\nTry It Yourself\n\nImplement a 2-layer network with ReLU and softmax.\nAdd dropout and batch normalization.\nVisualize training with and without dropout.\nCompare performance on MNIST with and without normalization.\n\n\n\n\n95. Sequence Models (Viterbi, Beam Search, CTC)\nSequence models process data where order matters, text, speech, DNA, time series. They capture dependencies across positions, predicting the next step from context.\nThis section explores three fundamental tools: Viterbi, Beam Search, and CTC (Connectionist Temporal Classification).\n\n1. The Nature of Sequential Data\nSequential data has temporal or structural order. Each element \\(x_t\\) depends on past inputs \\(x_{1:t-1}\\).\nCommon sequence tasks:\n\nTagging (POS tagging, named entity recognition)- Transcription (speech → text)- Decoding (translation, path reconstruction) To handle such problems, we need models that remember.\n\n\n\n2. Hidden Markov Models (HMMs)\nA Hidden Markov Model assumes:\n\nA sequence of hidden states \\(z_1, z_2, \\ldots, z_T\\)- Each state emits an observation \\(x_t\\)- Transition and emission probabilities govern the process \\[\nP(z_t | z_{t-1}) = A_{z_{t-1}, z_t}, \\quad P(x_t | z_t) = B_{z_t}(x_t)\n\\]\n\nGoal: find the most likely sequence of hidden states given observations.\n\n\n3. The Viterbi Algorithm\nViterbi is a dynamic programming algorithm to decode the most probable path:\n\\[\n\\delta_t(i) = \\max_{z_{1:t-1}} P(z_{1:t-1}, z_t = i, x_{1:t})\n\\]\nRecurrence:\n\\[\n\\delta_t(i) = \\max_j \\big( \\delta_{t-1}(j) \\cdot A_{j,i} \\big) \\cdot B_i(x_t)\n\\]\nTrack backpointers to reconstruct the best sequence.\nTime complexity: \\(O(T \\cdot N^2)\\),\nwhere \\(N\\) = number of states, \\(T\\) = sequence length.\nTiny Code\nfor (t = 1; t &lt; T; t++) {\n    for (i = 0; i &lt; N; i++) {\n        double best = -INF;\n        int argmax = -1;\n        for (j = 0; j &lt; N; j++) {\n            double score = delta[t-1][j] * A[j][i];\n            if (score &gt; best) { best = score; argmax = j; }\n        }\n        delta[t][i] = best * B[i][x[t]];\n        backptr[t][i] = argmax;\n    }\n}\nUse backptr to trace back the optimal path.\n\n\n4. Beam Search\nFor many sequence models (e.g. neural machine translation), exhaustive search is impossible. Beam search keeps only the top-k best hypotheses at each step.\nAlgorithm:\n\nStart with an empty sequence and score 0\nAt each step, expand each candidate with all possible next tokens\nKeep only k best sequences (beam size)\nStop when all sequences end or reach max length\n\nBeam size controls trade-off:\n\nLarger beam → better accuracy, slower- Smaller beam → faster, riskier\n\nTiny Code\nfor (step = 0; step &lt; max_len; step++) {\n    vector&lt;Candidate&gt; new_beam;\n    for (c in beam) {\n        probs = model_next(c.seq);\n        for (token, p in probs)\n            new_beam.push({c.seq + token, c.score + log(p)});\n    }\n    beam = top_k(new_beam, k);\n}\nUse log probabilities to avoid underflow.\n\n\n5. Connectionist Temporal Classification (CTC)\nUsed in speech recognition and handwriting recognition where input and output lengths differ.\nCTC learns to align input frames with output symbols without explicit alignment.\nAdd a special blank symbol (∅) to allow flexible alignment.\nExample (CTC decoding):\n\n\n\nFrame\nOutput\nAfter Collapse\n\n\n\n\nA ∅ A A\nA ∅ A\nA A\n\n\nH ∅ ∅ H\nH H\nH\n\n\n\nLoss: \\[\nP(y | x) = \\sum_{\\pi \\in \\text{Align}(x, y)} P(\\pi | x)\n\\] where \\(\\pi\\) are all alignments that reduce to ( y ).\nCTC uses dynamic programming to compute forward-backward probabilities.\n\n\n6. Comparing Methods\n\n\n\n\n\n\n\n\n\nMethod\nUsed In\nKey Idea\nHandles Alignment?\n\n\n\n\nViterbi\nHMMs\nMost probable state path\nYes\n\n\nBeam Search\nNeural decoders\nApproximate search\nImplicit\n\n\nCTC\nSpeech / seq2seq\nSum over alignments\nYes\n\n\n\n\n\n7. Use Cases\n\nViterbi: POS tagging, speech decoding- Beam Search: translation, text generation- CTC: ASR, OCR, gesture recognition\n\n\n\n8. Implementation Tips\n\nUse log-space for probabilities- In beam search, apply length normalization- In CTC, use dynamic programming tables- Combine CTC + beam search for speech decoding\n\n\n\n9. Common Pitfalls\n\nViterbi assumes Markov property (limited memory)- Beam Search can miss global optimum- CTC can confuse repeated characters without blanks\n\n\n\n10. Why It Matters\nSequence models are the bridge between structure and time. They show how to decode hidden meaning in ordered data.\nFrom decoding Morse code to transcribing speech, these algorithms give machines the gift of sequence understanding.\n\n\nTry It Yourself\n\nImplement Viterbi for a 3-state HMM.\nCompare greedy decoding vs beam search on a toy language model.\nBuild a CTC loss table for a short sequence (like “HELLO”).\n\n\n\n\n96. Metaheuristics (GA, SA, PSO, ACO)\nMetaheuristics are general-purpose optimization strategies that search through vast, complex spaces when exact methods are too slow or infeasible. They don’t guarantee the perfect answer but often find good-enough solutions fast.\nThis section covers four classics:\n\nGA (Genetic Algorithm)- SA (Simulated Annealing)- PSO (Particle Swarm Optimization)- ACO (Ant Colony Optimization)\n\n\n1. The Metaheuristic Philosophy\nMetaheuristics draw inspiration from nature and physics. They combine exploration (searching widely) and exploitation (refining promising spots).\nThey’re ideal for:\n\nNP-hard problems (TSP, scheduling)- Continuous optimization (parameter tuning)- Black-box functions (no gradients) They trade mathematical guarantees for practical power.\n\n\n\n2. Genetic Algorithm (GA)\nInspired by natural selection, GAs evolve a population of solutions.\n\n\nCore Steps\n\nInitialize population randomly\nEvaluate fitness of each\nSelect parents\nCrossover to produce offspring\nMutate to add variation\nReplace worst with new candidates\n\nRepeat until convergence.\nTiny Code\nfor (gen = 0; gen &lt; max_gen; gen++) {\n    evaluate(pop);\n    parents = select_best(pop);\n    offspring = crossover(parents);\n    mutate(offspring);\n    pop = select_survivors(pop, offspring);\n}\nOperators\n\nSelection: tournament, roulette-wheel- Crossover: one-point, uniform- Mutation: bit-flip, Gaussian Strengths: global search, diverse exploration Weakness: may converge slowly\n\n\n\n3. Simulated Annealing (SA)\nMimics cooling of metals, start hot (high randomness), slowly cool.\nAt each step:\n\nPropose random neighbor\nAccept if better\nIf worse, accept with probability \\[\nP = e^{-\\frac{\\Delta E}{T}}\n\\]\nGradually lower ( T )\n\nTiny Code\nT = T_init;\nstate = random_state();\nwhile (T &gt; T_min) {\n    next = neighbor(state);\n    dE = cost(next) - cost(state);\n    if (dE &lt; 0 || exp(-dE/T) &gt; rand_uniform())\n        state = next;\n    T *= alpha; // cooling rate\n}\nStrengths: escapes local minima Weakness: sensitive to cooling schedule\n\n\n4. Particle Swarm Optimization (PSO)\nInspired by bird flocking. Each particle adjusts velocity based on:\n\nIts own best position- The global best found \\[\nv_i \\leftarrow w v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i)\n\\]\n\n\\[\nx_i \\leftarrow x_i + v_i\n\\]\nTiny Code\nfor each particle i:\n    v[i] = w*v[i] + c1*r1*(pbest[i]-x[i]) + c2*r2*(gbest-x[i]);\n    x[i] += v[i];\n    update_best(i);\nStrengths: continuous domains, easy Weakness: premature convergence\n\n\n5. Ant Colony Optimization (ACO)\nInspired by ant foraging, ants deposit pheromones on paths. The stronger the trail, the more likely others follow.\nSteps:\n\nInitialize pheromone on all edges\nEach ant builds a solution (prob. ∝ pheromone)\nEvaluate paths\nEvaporate pheromone\nReinforce good paths\n\n\\[\n\\tau_{ij} \\leftarrow (1 - \\rho)\\tau_{ij} + \\sum_k \\Delta\\tau_{ij}^k\n\\]\nTiny Code\nfor each iteration:\n    for each ant:\n        path = build_solution(pheromone);\n        score = evaluate(path);\n    evaporate(pheromone);\n    deposit(pheromone, best_paths);\nStrengths: combinatorial problems (TSP) Weakness: parameter tuning, slower convergence\n\n\n6. Comparing the Four\n\n\n\n\n\n\n\n\n\nMethod\nInspiration\nBest For\nKey Idea\n\n\n\n\nGA\nEvolution\nDiscrete search\nSelection, crossover, mutation\n\n\nSA\nThermodynamics\nLocal optima escape\nCooling + randomness\n\n\nPSO\nSwarm behavior\nContinuous search\nLocal + global attraction\n\n\nACO\nAnt foraging\nGraph paths\nPheromone reinforcement\n\n\n\n\n\n7. Design Patterns\nCommon metaheuristic pattern:\n\nRepresent solution- Define fitness / cost function- Define neighbor / mutation operators- Balance randomness and greediness Tuning parameters often matters more than equations.\n\n\n\n8. Hybrid Metaheuristics\nCombine strengths:\n\nGA + SA: evolve population, fine-tune locally- PSO + DE: use swarm + differential evolution- ACO + Local Search: reinforce with hill-climbing These hybrids often outperform single methods.\n\n\n\n9. Common Pitfalls\n\nPoor representation → weak search- Over-exploitation → stuck in local optima- Bad parameters → chaotic or stagnant behavior Always visualize progress (fitness over time).\n\n\n\n10. Why It Matters\nMetaheuristics give us adaptive intelligence, searching without gradients, equations, or complete knowledge. They reflect nature’s way of solving complex puzzles: iterate, adapt, survive.\n\n“Optimization is not about perfection. It’s about progress guided by curiosity.”\n\n\n\nTry It Yourself\n\nImplement Simulated Annealing for the Traveling Salesman Problem.\nCreate a Genetic Algorithm for knapsack optimization.\nTune PSO parameters to fit a function \\(f(x) = x^2 + 10\\sin x\\).\nCompare ACO paths for TSP at different evaporation rates.\n\n\n\n\n97. Reinforcement Learning (Q-learning, Policy Gradients)\nReinforcement Learning (RL) is about learning through interaction , an agent explores an environment, takes actions, and learns from rewards. Unlike supervised learning (where correct labels are given), RL learns what to do by trial and error.\nThis section introduces two core approaches:\n\nQ-learning (value-based)- Policy Gradient (policy-based)\n\n\n1. The Reinforcement Learning Setting\nAn RL problem is modeled as a Markov Decision Process (MDP):\n\nStates \\(S\\)\nActions \\(A\\)\nTransition \\(P(s' \\mid s, a)\\)\nReward \\(R(s, a)\\)\nDiscount factor \\(\\gamma\\)\n\nThe agent’s goal is to find a policy \\(\\pi(a \\mid s)\\) that maximizes expected return:\n\\[\nG_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n\\]\n\n\n2. Value Functions\nThe value function measures how good a state (or state-action pair) is.\n\nState-value: \\[\nV^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]\n\\]\nAction-value (Q-function): \\[\nQ^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]\n\\]\n\n\n\n3. Bellman Equation\nThe Bellman equation relates a state’s value to its neighbors:\n\\[\nQ^*(s,a) = R(s,a) + \\gamma \\max_{a'} Q^*(s',a')\n\\]\nThis recursive definition drives value iteration and Q-learning.\n\n\n4. Q-Learning\nQ-learning learns the optimal action-value function off-policy (independent of behavior policy):\nUpdate Rule: \\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\big]\n\\]\nTiny Code\nQ[s][a] += alpha * (r + gamma * max(Q[s_next]) - Q[s][a]);\ns = s_next;\nRepeat while exploring (e.g., \\(\\varepsilon\\)-greedy):\n\nWith probability \\(\\varepsilon\\), choose a random action\nWith probability \\(1 - \\varepsilon\\), choose the best action\n\nOver time, \\(Q\\) converges to \\(Q^*\\).\n\n\n5. Exploration vs Exploitation\nRL is a balancing act:\n\nExploration: try new actions to gather knowledge- Exploitation: use current best knowledge to maximize reward Strategies:\nε-greedy- Softmax action selection- Upper Confidence Bound (UCB)\n\n\n\n6. Policy Gradient Methods\nInstead of learning Q-values, learn the policy directly. Represent policy with parameters \\(\\theta\\):\n\\[\n\\pi_\\theta(a|s) = P(a | s; \\theta)\n\\]\nGoal: maximize expected return \\[\nJ(\\theta) = \\mathbb{E}*{\\pi*\\theta}[G_t]\n\\]\nGradient ascent update: \\[\n\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n\\]\nREINFORCE Algorithm: \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}\\big[ G_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\big]\n\\]\nTiny Code\ntheta += alpha * G_t * grad_logpi(a_t, s_t);\n\n\n7. Actor-Critic Architecture\nCombines policy gradient (actor) + value estimation (critic).\n\nActor: updates policy- Critic: estimates value (baseline) Update: \\[\n\\theta \\leftarrow \\theta + \\alpha_\\theta \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n\\]\n\n\\[\nw \\leftarrow w + \\alpha_w \\delta_t \\nabla_w V_w(s_t)\n\\]\nwith TD error: \\[\n\\delta_t = r + \\gamma V(s') - V(s)\n\\]\n\n\n8. Comparing Methods\n\n\n\n\n\n\n\n\n\n\n\nMethod\nType\nLearns\nOn/Off Policy\nContinuous?\n\n\n\n\n\nQ-learning\nValue-based\nQ(s, a)\nOff-policy\nNo\n\n\n\nPolicy Gradient\nPolicy-based\nπ(a\ns)\nOn-policy\nYes\n\n\nActor-Critic\nHybrid\nBoth\nOn-policy\nYes\n\n\n\n\n\n\n9. Extensions\n\nDeep Q-Networks (DQN): use neural nets for Q(s, a)- PPO / A3C: advanced actor-critic methods- TD(λ): tradeoff between MC and TD learning- Double Q-learning: reduce overestimation- Entropy regularization: encourage exploration\n\n\n\n10. Why It Matters\nReinforcement learning powers autonomous agents, game AIs, and control systems. It’s the foundation of AlphaGo, robotics control, and adaptive decision systems.\n\n“An agent learns not from instruction but from experience.”\n\n\n\nTry It Yourself\n\nImplement Q-learning for a grid-world maze.\nAdd ε-greedy exploration.\nVisualize the learned policy.\nTry REINFORCE with a simple policy (e.g. softmax over actions).\nCompare convergence of Q-learning vs Policy Gradient.\n\n\n\n\n98. Approximation and Online Algorithms\nIn the real world, we often can’t wait for a perfect solution , data arrives on the fly, or the problem is too hard to solve exactly. That’s where approximation and online algorithms shine. They aim for good-enough results, fast and adaptively, under uncertainty.\n\n1. The Big Picture\n\nApproximation algorithms: Solve NP-hard problems with provable bounds.- Online algorithms: Make immediate decisions without knowing the future. Both trade optimality for efficiency or adaptability.\n\n\n\n2. Approximation Algorithms\nAn approximation algorithm finds a solution within a factor \\(\\rho\\) of the optimal.\nIf ( C ) is cost of the algorithm, and \\(C^*\\) is optimal cost:\n\\[\n\\rho = \\max\\left(\\frac{C}{C^*}, \\frac{C^*}{C}\\right)\n\\]\nExample: \\(\\rho = 2\\) → solution at most twice worse than optimal.\n\n\n3. Example: Vertex Cover\nProblem: Given graph ( G(V,E) ), choose smallest set of vertices covering all edges.\nAlgorithm (2-approximation):\n\nInitialize cover = ∅\nWhile edges remain:\n\nPick any edge (u, v) - Add both u, v to cover - Remove all edges incident on u or v Guarantee: At most 2× optimal size.\n\n\nTiny Code\ncover = {};\nwhile (!edges.empty()) {\n    (u, v) = edges.pop();\n    cover.add(u);\n    cover.add(v);\n    remove_incident_edges(u, v);\n}\n\n\n4. Example: Metric TSP (Triangle Inequality)\nAlgorithm (Christofides):\n\nFind MST\nFind odd-degree vertices\nFind min perfect matching\nCombine + shortcut to get tour\n\nGuarantee: ≤ 1.5 × optimal.\n\n\n5. Greedy Approximation: Set Cover\nGoal: Cover universe ( U ) with minimum sets \\(S_i\\).\nGreedy Algorithm: Pick set covering most uncovered elements each time. Guarantee: \\(H_n \\approx \\ln n\\) factor approximation.\n\n\n6. Online Algorithms\nOnline algorithms must decide now, before future input is known.\nGoal: Minimize competitive ratio:\n\\[\n\\text{CR} = \\max_{\\text{input}} \\frac{\\text{Cost}*{\\text{online}}}{\\text{Cost}*{\\text{optimal offline}}}\n\\]\nLower CR → better adaptability.\n\n\n7. Classic Example: Online Paging\nYou have k pages in cache, sequence of page requests.\n\nIf page in cache → hit- Else → miss, must evict one page Strategies:\nLRU (Least Recently Used): evict oldest- FIFO: evict first loaded- Random: pick randomly Competitive Ratio:\nLRU: ≤ ( k )- Random: ≤ ( 2k-1 )\n\nTiny Code\ncache = LRUCache(k);\nfor (page in requests) {\n    if (!cache.contains(page))\n        cache.evict_oldest();\n    cache.add(page);\n}\n\n\n8. Online Bipartite Matching (Karp-Vazirani-Vazirani)\nGiven offline set U and online set V (arrives one by one), match greedily. Competitive ratio: \\(1 - \\frac{1}{e}\\)\nUsed in ad allocation and resource assignment.\n\n\n9. Approximation + Online Together\nModern algorithms blend both:\n\nStreaming algorithms: One pass, small memory (Count-Min, reservoir sampling)- Online learning: Update models incrementally (SGD, perceptron)- Approximate dynamic programming: RL and heuristic search These are approximate online solvers , both quick and adaptive.\n\n\n\n10. Why It Matters\nApproximation algorithms give us provable near-optimal answers. Online algorithms give us real-time adaptivity. Together, they model intelligence under limits , when time and information are scarce.\n\n“Sometimes, good and on time beats perfect and late.”\n\n\n\nTry It Yourself\n\nImplement 2-approx vertex cover on a small graph.\nSimulate online paging with LRU vs Random.\nBuild a greedy set cover solver.\nMeasure competitive ratio on test sequences.\nCombine ideas: streaming + approximation for big data filtering.\n\n\n\n\n99. Fairness, Causal Inference, and Robust Optimization\nAs algorithms increasingly shape decisions , from hiring to lending to healthcare , we must ensure they’re fair, causally sound, and robust to uncertainty. This section blends ideas from ethics, statistics, and optimization to make algorithms not just efficient, but responsible and reliable.\n\n1. Why Fairness Matters\nMachine learning systems often inherit biases from data. Without intervention, they can amplify inequality or discrimination.\nFairness-aware algorithms explicitly measure and correct these effects.\nCommon sources of bias:\n\nHistorical bias (biased data)- Measurement bias (imprecise features)- Selection bias (skewed samples) The goal: equitable treatment across sensitive groups (gender, race, region, etc.)\n\n\n\n2. Formal Fairness Criteria\nSeveral fairness notions exist, often conflicting:\n\n\n\n\n\n\n\n\n\n\nCriterion\nDescription\nExample\n\n\n\n\n\n\nDemographic Parity\n( P\\(\\hat{Y}=1                      | A=a\\) = P\\(\\hat{Y}=1                               | A=b\\) )\nEqual positive rate\n\n\n\n\nEqual Opportunity\nEqual true positive rates\nSame recall for all groups\n\n\n\n\nEqualized Odds\nEqual TPR & FPR\nBalanced errors\n\n\n\n\nCalibration\nSame predicted probability meaning\nIf model says 70%, all groups should achieve 70%\n\n\n\n\n\nNo single measure fits all , fairness depends on context and trade-offs.\n\n\n3. Algorithmic Fairness Techniques\n\nPre-processing Rebalance or reweight data before training. Example: reweighing, sampling.\nIn-processing Add fairness constraints to loss function. Example: adversarial debiasing.\nPost-processing Adjust predictions after training. Example: threshold shifting.\n\nTiny Code (Adversarial Debiasing Skeleton)\nfor x, a, y in data:\n    y_pred = model(x)\n    loss_main = loss_fn(y_pred, y)\n    loss_adv = adv_fn(y_pred, a)\n    loss_total = loss_main - λ * loss_adv\n    update(loss_total)\nHere, the adversary tries to predict sensitive attribute, encouraging invariance.\n\n\n4. Causal Inference Basics\nCorrelation ≠ causation. To reason about fairness and robustness, we need causal understanding , what would happen if we changed something.\nCausal inference models relationships via Directed Acyclic Graphs (DAGs):\n\nNodes: variables- Edges: causal influence\n\n\n\n5. Counterfactual Reasoning\nA counterfactual asks:\n\n“What would the outcome be if we intervened differently?”\n\nFormally: \\[\nP(Y_{do(X=x)})\n\\]\nUsed in:\n\nFairness (counterfactual fairness)- Policy evaluation- Robust decision making\n\n\n\n6. Counterfactual Fairness\nAn algorithm is counterfactually fair if prediction stays the same under hypothetical changes to sensitive attributes.\n\\[\n\\hat{Y}*{A \\leftarrow a}(U) = \\hat{Y}*{A \\leftarrow a'}(U)\n\\]\nThis requires causal models , not just data.\n\n\n7. Robust Optimization\nIn uncertain environments, we want solutions that hold up under worst-case conditions.\nFormulation: \\[\n\\min_x \\max_{\\xi \\in \\Xi} f(x, \\xi)\n\\]\nwhere \\(\\Xi\\) is the uncertainty set.\nExample: Design a portfolio that performs well under varying market conditions.\nTiny Code\ndouble robust_objective(double x[], Scenario Xi[], int N) {\n    double worst = -INF;\n    for (i=0; i&lt;N; i++)\n        worst = max(worst, f(x, Xi[i]));\n    return worst;\n}\nThis searches for a solution minimizing worst-case loss.\n\n\n8. Distributional Robustness\nInstead of worst-case instances, protect against worst-case distributions:\n\\[\n\\min_\\theta \\sup_{Q \\in \\mathcal{B}(P)} \\mathbb{E}_{x \\sim Q}[L(\\theta, x)]\n\\]\nUsed in adversarial training and domain adaptation.\nExample: Add noise or perturbations to improve resilience:\nx_adv = x + ε * sign(grad(loss, x))\n\n\n9. Balancing Fairness, Causality, and Robustness\n\n\n\n\n\n\n\n\nGoal\nMethod\nChallenge\n\n\n\n\nFairness\nParity, Adversarial, Counterfactual\nCompeting definitions\n\n\nCausality\nDAGs, do-calculus, SCMs\nIdentifying true structure\n\n\nRobustness\nMin-max, DRO, Adversarial Training\nTrade-off with accuracy\n\n\n\nReal-world design involves balancing trade-offs.\nSometimes improving fairness reduces accuracy, or robustness increases conservatism.\n\n\n10. Why It Matters\nAlgorithms don’t exist in isolation , they affect people. Embedding fairness, causality, and robustness ensures systems are trustworthy, interpretable, and just.\n\n“The goal is not just intelligent algorithms , but responsible ones.”\n\n\n\nTry It Yourself\n\nTrain a simple classifier on biased data.\nApply reweighing or adversarial debiasing.\nDraw a causal DAG of your data features.\nCompute counterfactual fairness for a sample.\nImplement a robust loss using adversarial perturbations.\n\n\n\n\n100. AI Planning, Search, and Learning Systems\nAI systems are not just pattern recognizers , they are decision makers. They plan, search, and learn in structured environments, choosing actions that lead to long-term goals. This section explores how modern AI combines planning, search, and learning to solve complex tasks.\n\n1. What Is AI Planning?\nAI planning is about finding a sequence of actions that transforms an initial state into a goal state.\nFormally, a planning problem consists of:\n\nStates ( S )- Actions ( A )- Transition function ( T(s, a) s’ )- Goal condition \\(G \\subseteq S\\)- Cost function ( c(a) ) The objective: Find a plan \\(\\pi = [a_1, a_2, \\ldots, a_n]\\) minimizing total cost or maximizing reward.\n\n\n\n2. Search-Based Planning\nAt the heart of planning lies search. Search explores possible action sequences, guided by heuristics.\n\n\n\nAlgorithm\nType\nDescription\n\n\n\n\nDFS\nUninformed\nDeep exploration, no guarantee\n\n\nBFS\nUninformed\nFinds shortest path\n\n\nDijkstra\nWeighted\nOptimal if costs ≥ 0\n\n\nA*\nHeuristic\nCombines cost + heuristic\n\n\n\nA* Search Formula: \\[\nf(n) = g(n) + h(n)\n\\] where:\n\n( g(n) ): cost so far- ( h(n) ): heuristic estimate to goal If ( h ) is admissible, A* is optimal.\n\nTiny Code (A* Skeleton)\npriority_queue&lt;Node&gt; open;\ng[start] = 0;\nopen.push({start, h(start)});\n\nwhile (!open.empty()) {\n    n = open.pop_min();\n    if (goal(n)) break;\n    for (a in actions(n)) {\n        s = step(n, a);\n        cost = g[n] + c(n, a);\n        if (cost &lt; g[s]) {\n            g[s] = cost;\n            f[s] = g[s] + h(s);\n            open.push({s, f[s]});\n        }\n    }\n}\n\n\n3. Heuristics and Admissibility\nA heuristic ( h(s) ) estimates distance to the goal.\n\nAdmissible: never overestimates- Consistent: satisfies triangle inequality Examples:\nManhattan distance (grids)- Euclidean distance (geometry)- Pattern databases (puzzles) Good heuristics = faster convergence.\n\n\n\n4. Classical Planning (STRIPS)\nIn symbolic AI, states are represented by facts (predicates), and actions have preconditions and effects.\nExample:\nAction: Move(x, y)\nPrecondition: At(x), Clear(y)\nEffect: ¬At(x), At(y)\nSearch happens in logical state space.\nPlanners:\n\nForward search (progression)- Backward search (regression)- Heuristic planners (FF, HSP)\n\n\n\n5. Hierarchical Planning\nBreak complex goals into subgoals.\n\nHTN (Hierarchical Task Networks): Define high-level tasks broken into subtasks.\n\nExample: “Make dinner” → [Cook rice, Stir-fry vegetables, Set table]\nHierarchy makes planning modular and interpretable.\n\n\n6. Probabilistic Planning\nWhen actions are uncertain:\n\nMDPs: full observability, stochastic transitions- POMDPs: partial observability Use value iteration, policy iteration, or Monte Carlo planning.\n\n\n\n7. Learning to Plan\nCombine learning with search:\n\nLearned heuristics: neural networks approximate ( h(s) )- AlphaZero-style planning: learn value + policy, guide tree search- Imitation learning: mimic expert demonstrations This bridges classical AI and modern ML.\n\nTiny Code (Learning-Guided A*)\nf = g + alpha * learned_heuristic(s)\nNeural net learns ( h_(s) ) from solved examples.\n\n\n8. Integrated Systems\nModern AI stacks combine:\n\nSearch (planning backbone)- Learning (policy, heuristic, model)- Simulation (data generation) Examples:\nAlphaZero: self-play + MCTS + neural nets- MuZero: learns model + value + policy jointly- Large Language Agents: use reasoning + memory + search\n\n\n\n9. Real-World Applications\n\nRobotics: motion planning, pathfinding- Games: Go, Chess, strategy games- Logistics: route optimization- Autonomy: drones, vehicles, AI assistants- Synthesis: program and query generation Each blends symbolic reasoning and statistical learning.\n\n\n\n10. Why It Matters\nPlanning, search, and learning form the triad of intelligence:\n\nSearch explores possibilities- Planning sequences actions toward goals- Learning adapts heuristics from experience Together, they power systems that think, adapt, and act.\n\n\n“Intelligence is not just knowing , it is choosing wisely under constraints.”\n\n\n\nTry It Yourself\n\nImplement A* search on a grid maze.\nAdd a Manhattan heuristic.\nExtend to probabilistic transitions (simulate noise).\nBuild a simple planner with preconditions and effects.\nTrain a neural heuristic to guide search on puzzles.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/plan.html",
    "href": "books/en-us/plan.html",
    "title": "The Plan",
    "section": "",
    "text": "Chapter 1. Foundations of Algorithms\n\n1. What Is an Algorithm?\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n1\nEuclid’s GCD\nOldest known algorithm for greatest common divisor\n\n\n2\nSieve of Eratosthenes\nGenerate primes efficiently\n\n\n3\nBinary Search\nDivide and conquer search\n\n\n4\nExponentiation by Squaring\nFast power computation\n\n\n5\nLong Division\nClassic step-by-step arithmetic\n\n\n6\nModular Addition Algorithm\nWrap-around arithmetic\n\n\n7\nBase Conversion Algorithm\nConvert between number systems\n\n\n8\nFactorial Computation\nRecursive and iterative approaches\n\n\n9\nFibonacci Sequence\nRecursive vs. dynamic computation\n\n\n10\nTower of Hanoi\nRecursive problem-solving pattern\n\n\n\n\n\n2. Measuring Time and Space\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n11\nCounting Operations\nManual step-counting for complexity\n\n\n12\nLoop Analysis\nEvaluate time cost of loops\n\n\n13\nRecurrence Expansion\nAnalyze recursive costs\n\n\n14\nAmortized Analysis\nAverage per-operation cost\n\n\n15\nSpace Counting\nStack and heap tracking\n\n\n16\nMemory Footprint Estimator\nTrack per-variable usage\n\n\n17\nTime Complexity Table\nMap O(1)…O(n²)…O(2ⁿ)\n\n\n18\nSpace-Time Tradeoff\nCache vs. recomputation\n\n\n19\nProfiling Algorithm\nEmpirical time measurement\n\n\n20\nBenchmarking Framework\nCompare algorithm performance\n\n\n\n\n\n3. Big-O, Big-Theta, Big-Omega\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n21\nGrowth Rate Comparator\nCompare asymptotic behaviors\n\n\n22\nDominant Term Extractor\nSimplify runtime expressions\n\n\n23\nLimit-Based Complexity Test\nUsing limits for asymptotics\n\n\n24\nSummation Simplifier\nSum of arithmetic/geometric sequences\n\n\n25\nRecurrence Tree Method\nVisualize recursive costs\n\n\n26\nMaster Theorem Evaluator\nSolve T(n) recurrences\n\n\n27\nBig-Theta Proof Builder\nBounding upper and lower limits\n\n\n28\nBig-Omega Case Finder\nBest-case scenario analysis\n\n\n29\nEmpirical Complexity Estimator\nMeasure via doubling experiments\n\n\n30\nComplexity Class Identifier\nMatch runtime to known class\n\n\n\n\n\n4. Algorithmic Paradigms (Greedy, Divide and Conquer, DP)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n31\nGreedy Coin Change\nLocal optimal step-by-step\n\n\n32\nHuffman Coding\nGreedy compression tree\n\n\n33\nMerge Sort\nDivide and conquer sort\n\n\n34\nBinary Search\nDivide and conquer search\n\n\n35\nKaratsuba Multiplication\nRecursive divide & conquer\n\n\n36\nMatrix Chain Multiplication\nDP with substructure\n\n\n37\nLongest Common Subsequence\nClassic DP problem\n\n\n38\nRod Cutting\nDP optimization\n\n\n39\nActivity Selection\nGreedy scheduling\n\n\n40\nOptimal Merge Patterns\nGreedy file merging\n\n\n\n\n\n5. Recurrence Relations\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n41\nLinear Recurrence Solver\nClosed-form for linear recurrences\n\n\n42\nMaster Theorem\nDivide-and-conquer complexity\n\n\n43\nSubstitution Method\nInductive proof approach\n\n\n44\nIteration Method\nExpand recurrence step-by-step\n\n\n45\nGenerating Functions\nTransform recurrences\n\n\n46\nMatrix Exponentiation\nSolve linear recurrences fast\n\n\n47\nRecurrence to DP Table\nTabulation approach\n\n\n48\nDivide & Combine Template\nConvert recurrence into algorithm\n\n\n49\nMemoized Recursive Solver\nStore overlapping results\n\n\n50\nCharacteristic Polynomial\nSolve homogeneous recurrence\n\n\n\n\n\n6. Searching Basics\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n51\nLinear Search\nSequential element scan\n\n\n52\nBinary Search\nMidpoint halving\n\n\n53\nJump Search\nBlock skip linear\n\n\n54\nExponential Search\nDoubling step size\n\n\n55\nInterpolation Search\nEstimate position by value\n\n\n56\nTernary Search\nDivide into thirds\n\n\n57\nFibonacci Search\nGolden ratio search\n\n\n58\nSentinel Search\nEarly termination optimization\n\n\n59\nBidirectional Search\nMeet-in-the-middle\n\n\n60\nSearch in Rotated Array\nAdapted binary search\n\n\n\n\n\n7. Sorting Basics\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n61\nBubble Sort\nAdjacent swap sort\n\n\n62\nSelection Sort\nFind minimum each pass\n\n\n63\nInsertion Sort\nIncremental build sort\n\n\n64\nShell Sort\nGap-based insertion\n\n\n65\nMerge Sort\nDivide-and-conquer\n\n\n66\nQuick Sort\nPartition-based\n\n\n67\nHeap Sort\nBinary heap order\n\n\n68\nCounting Sort\nInteger key distribution\n\n\n69\nRadix Sort\nDigit-by-digit\n\n\n70\nBucket Sort\nGroup into ranges\n\n\n\n\n\n8. Data Structures Overview\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n71\nStack Push/Pop\nLIFO operations\n\n\n72\nQueue Enqueue/Dequeue\nFIFO operations\n\n\n73\nSingly Linked List\nLinear node chain\n\n\n74\nDoubly Linked List\nBidirectional traversal\n\n\n75\nHash Table Insertion\nKey-value indexing\n\n\n76\nBinary Search Tree Insert\nOrdered node placement\n\n\n77\nHeapify\nBuild heap in-place\n\n\n78\nUnion-Find Operations\nDisjoint-set management\n\n\n79\nGraph Adjacency List Build\nSparse representation\n\n\n80\nTrie Insertion/Search\nPrefix tree for strings\n\n\n\n\n\n9. Graphs and Trees Overview\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n81\nDFS Traversal\nDepth-first exploration\n\n\n82\nBFS Traversal\nLevel-order exploration\n\n\n83\nTopological Sort\nDAG ordering\n\n\n84\nMinimum Spanning Tree\nKruskal/Prim overview\n\n\n85\nDijkstra’s Shortest Path\nWeighted graph shortest route\n\n\n86\nBellman-Ford\nHandle negative edges\n\n\n87\nFloyd-Warshall\nAll-pairs shortest path\n\n\n88\nUnion-Find for MST\nEdge grouping\n\n\n89\nTree Traversals\nInorder, Preorder, Postorder\n\n\n90\nLCA (Lowest Common Ancestor)\nCommon node in tree\n\n\n\n\n\n10. Algorithm Design Patterns\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n91\nBrute Force\nTry all possibilities\n\n\n92\nGreedy Choice\nLocal optimum per step\n\n\n93\nDivide and Conquer\nBreak and merge\n\n\n94\nDynamic Programming\nReuse subproblems\n\n\n95\nBacktracking\nExplore with undo\n\n\n96\nBranch and Bound\nPrune search space\n\n\n97\nRandomized Algorithm\nInject randomness\n\n\n98\nApproximation Algorithm\nNear-optimal solution\n\n\n99\nOnline Algorithm\nStep-by-step decision\n\n\n100\nHybrid Strategy\nCombine paradigms\n\n\n\n\n\n\nChapter 2. Sorting and Searching\n\n11. Elementary Sorting (Bubble, Insertion, Selection)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n101\nBubble Sort\nSwap adjacent out-of-order elements\n\n\n102\nImproved Bubble Sort\nStop early if already sorted\n\n\n103\nCocktail Shaker Sort\nBidirectional bubble pass\n\n\n104\nSelection Sort\nSelect smallest element each pass\n\n\n105\nDouble Selection Sort\nFind both min and max each pass\n\n\n106\nInsertion Sort\nInsert each element into correct spot\n\n\n107\nBinary Insertion Sort\nUse binary search for position\n\n\n108\nGnome Sort\nSimple insertion-like with swaps\n\n\n109\nOdd-Even Sort\nParallel-friendly comparison sort\n\n\n110\nStooge Sort\nRecursive quirky educational sort\n\n\n\n\n\n12. Divide-and-Conquer Sorting (Merge, Quick, Heap)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n111\nMerge Sort\nRecursive divide and merge\n\n\n112\nIterative Merge Sort\nBottom-up non-recursive version\n\n\n113\nQuick Sort\nPartition-based recursive sort\n\n\n114\nHoare Partition Scheme\nClassic quicksort partition\n\n\n115\nLomuto Partition Scheme\nSimpler but less efficient\n\n\n116\nRandomized Quick Sort\nAvoid worst-case pivot\n\n\n117\nHeap Sort\nHeapify + extract max repeatedly\n\n\n118\n3-Way Quick Sort\nHandle duplicates efficiently\n\n\n119\nExternal Merge Sort\nDisk-based merge for large data\n\n\n120\nParallel Merge Sort\nDivide work among threads\n\n\n\n\n\n13. Counting and Distribution Sorts (Counting, Radix, Bucket)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n121\nCounting Sort\nCount key occurrences\n\n\n122\nStable Counting Sort\nPreserve order of equals\n\n\n123\nRadix Sort (LSD)\nLeast significant digit first\n\n\n124\nRadix Sort (MSD)\nMost significant digit first\n\n\n125\nBucket Sort\nDistribute into buckets\n\n\n126\nPigeonhole Sort\nSimple bucket variant\n\n\n127\nFlash Sort\nDistribution with in-place correction\n\n\n128\nPostman Sort\nStable multi-key sort\n\n\n129\nAddress Calculation Sort\nHash-like distribution\n\n\n130\nSpread Sort\nHybrid radix/quick strategy\n\n\n\n\n\n14. Hybrid Sorts (IntroSort, Timsort)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n131\nIntroSort\nQuick + Heap fallback\n\n\n132\nTimSort\nMerge + Insertion + Runs\n\n\n133\nDual-Pivot QuickSort\nModern quicksort optimization\n\n\n134\nSmoothSort\nHeap-like adaptive sort\n\n\n135\nBlock Merge Sort\nCache-efficient merge variant\n\n\n136\nAdaptive Merge Sort\nAdjusts to partially sorted data\n\n\n137\nPDQSort\nPattern-defeating quicksort\n\n\n138\nWikiSort\nStable in-place merge\n\n\n139\nGrailSort\nIn-place stable mergesort\n\n\n140\nAdaptive Hybrid Sort\nDynamically selects strategy\n\n\n\n\n\n15. Special Sorts (Cycle, Gnome, Comb, Pancake)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n141\nCycle Sort\nMinimal writes\n\n\n142\nComb Sort\nShrinking gap bubble\n\n\n143\nGnome Sort\nInsertion-like with swaps\n\n\n144\nCocktail Sort\nTwo-way bubble\n\n\n145\nPancake Sort\nFlip-based sorting\n\n\n146\nBitonic Sort\nParallel network sorting\n\n\n147\nOdd-Even Merge Sort\nSorting network design\n\n\n148\nSleep Sort\nUses timing as order key\n\n\n149\nBead Sort\nSimulates gravity\n\n\n150\nBogo Sort\nRandomly permute until sorted\n\n\n\n\n\n16. Linear and Binary Search\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n151\nLinear Search\nScan sequentially\n\n\n152\nLinear Search (Sentinel)\nGuard element at end\n\n\n153\nBinary Search (Iterative)\nHalve interval each loop\n\n\n154\nBinary Search (Recursive)\nHalve interval via recursion\n\n\n155\nBinary Search (Lower Bound)\nFirst &gt;= target\n\n\n156\nBinary Search (Upper Bound)\nFirst &gt; target\n\n\n157\nExponential Search\nDouble step size\n\n\n158\nJump Search\nJump fixed steps then linear\n\n\n159\nFibonacci Search\nGolden-ratio style jumps\n\n\n160\nUniform Binary Search\nAvoid recomputing midpoints\n\n\n\n\n\n17. Interpolation and Exponential Search\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n161\nInterpolation Search\nEstimate index by value\n\n\n162\nRecursive Interpolation Search\nDivide by estimated midpoint\n\n\n163\nExponential Search\nDouble and binary refine\n\n\n164\nDoubling Search\nGeneric exponential pattern\n\n\n165\nGalloping Search\nUsed in TimSort merges\n\n\n166\nUnbounded Binary Search\nFind bounds dynamically\n\n\n167\nRoot-Finding Bisection\nSearch zero-crossing\n\n\n168\nGolden Section Search\nOptimize unimodal function\n\n\n169\nFibonacci Search (Optimum)\nSimilar to golden search\n\n\n170\nJump + Binary Hybrid\nCombined probing strategy\n\n\n\n\n\n18. Selection Algorithms (Quickselect, Median of Medians)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n171\nQuickselect\nPartition-based selection\n\n\n172\nMedian of Medians\nDeterministic pivot\n\n\n173\nRandomized Select\nRandom pivot version\n\n\n174\nBinary Search on Answer\nRange-based selection\n\n\n175\nOrder Statistics Tree\nBST with rank queries\n\n\n176\nTournament Tree Selection\nHierarchical comparison\n\n\n177\nHeap Select (Min-Heap)\nMaintain top-k elements\n\n\n178\nPartial QuickSort\nSort partial prefix\n\n\n179\nBFPRT Algorithm\nLinear-time selection\n\n\n180\nKth Largest Stream\nStreaming selection\n\n\n\n\n\n19. Range Searching and Nearest Neighbor\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n181\nBinary Search Range\nFind lower and upper bounds\n\n\n182\nSegment Tree Query\nSum/min/max over interval\n\n\n183\nFenwick Tree Query\nEfficient prefix sums\n\n\n184\nInterval Tree Search\nOverlap queries\n\n\n185\nKD-Tree Search\nSpatial nearest neighbor\n\n\n186\nR-Tree Query\nRange search in geometry\n\n\n187\nRange Minimum Query (RMQ)\nSparse table approach\n\n\n188\nMo’s Algorithm\nOffline query reordering\n\n\n189\nSweep Line Range Search\nSort + scan technique\n\n\n190\nBall Tree Nearest Neighbor\nMetric-space search\n\n\n\n\n\n20. Search Optimizations and Variants\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n191\nBinary Search with Tolerance\nFor floating values\n\n\n192\nTernary Search\nUnimodal optimization\n\n\n193\nHash-Based Search\nO(1) expected lookup\n\n\n194\nBloom Filter Lookup\nProbabilistic membership\n\n\n195\nCuckoo Hash Search\nDual-hash relocation\n\n\n196\nRobin Hood Hashing\nEqualize probe lengths\n\n\n197\nJump Consistent Hashing\nStable hash assignment\n\n\n198\nPrefix Search in Trie\nAuto-completion lookup\n\n\n199\nPattern Search in Suffix Array\nFast substring lookup\n\n\n200\nSearch in Infinite Array\nDynamic bound finding\n\n\n\n\n\n\nChapter 3. Data Structures in Action\n\n21. Arrays, Linked Lists, Stacks, Queues\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n201\nDynamic Array Resize\nDoubling strategy for capacity\n\n\n202\nCircular Array Implementation\nWrap-around indexing\n\n\n203\nSingly Linked List Insert/Delete\nBasic node manipulation\n\n\n204\nDoubly Linked List Insert/Delete\nTwo-way linkage\n\n\n205\nStack Push/Pop\nLIFO structure\n\n\n206\nQueue Enqueue/Dequeue\nFIFO structure\n\n\n207\nDeque Implementation\nDouble-ended queue\n\n\n208\nCircular Queue\nFixed-size queue with wrap-around\n\n\n209\nStack via Queue\nImplement stack using two queues\n\n\n210\nQueue via Stack\nImplement queue using two stacks\n\n\n\n\n\n22. Hash Tables and Variants (Cuckoo, Robin Hood, Consistent)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n211\nHash Table Insertion\nKey-value pair with modulo\n\n\n212\nLinear Probing\nResolve collisions sequentially\n\n\n213\nQuadratic Probing\nNonlinear probing sequence\n\n\n214\nDouble Hashing\nAlternate hash on collision\n\n\n215\nCuckoo Hashing\nTwo-table relocation strategy\n\n\n216\nRobin Hood Hashing\nEqualize probe length fairness\n\n\n217\nChained Hash Table\nLinked list buckets\n\n\n218\nPerfect Hashing\nNo-collision mapping\n\n\n219\nConsistent Hashing\nStable distribution across nodes\n\n\n220\nDynamic Rehashing\nResize on load factor threshold\n\n\n\n\n\n23. Heaps (Binary, Fibonacci, Pairing)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n221\nBinary Heap Insert\nBubble-up maintenance\n\n\n222\nBinary Heap Delete\nHeapify-down maintenance\n\n\n223\nBuild Heap (Heapify)\nBottom-up O(n) build\n\n\n224\nHeap Sort\nExtract max repeatedly\n\n\n225\nMin Heap Implementation\nFor smallest element access\n\n\n226\nMax Heap Implementation\nFor largest element access\n\n\n227\nFibonacci Heap Insert/Delete\nAmortized efficient operations\n\n\n228\nPairing Heap Merge\nLightweight mergeable heap\n\n\n229\nBinomial Heap Merge\nMerge trees of equal order\n\n\n230\nLeftist Heap Merge\nMaintain rank-skewed heap\n\n\n\n\n\n24. Balanced Trees (AVL, Red-Black, Splay, Treap)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n231\nAVL Tree Insert\nRotate to maintain balance\n\n\n232\nAVL Tree Delete\nBalance after deletion\n\n\n233\nRed-Black Tree Insert\nColor fix and rotations\n\n\n234\nRed-Black Tree Delete\nMaintain invariants\n\n\n235\nSplay Tree Access\nMove accessed node to root\n\n\n236\nTreap Insert\nPriority-based rotation\n\n\n237\nTreap Delete\nRandomized balance\n\n\n238\nWeight Balanced Tree\nMaintain subtree weights\n\n\n239\nScapegoat Tree Rebuild\nRebalance on size threshold\n\n\n240\nAA Tree\nSimplified red-black variant\n\n\n\n\n\n25. Segment Trees and Fenwick Trees\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n241\nBuild Segment Tree\nRecursive construction\n\n\n242\nRange Sum Query\nRecursive or iterative query\n\n\n243\nRange Update\nLazy propagation technique\n\n\n244\nPoint Update\nModify single element\n\n\n245\nFenwick Tree Build\nIncremental binary index\n\n\n246\nFenwick Update\nUpdate cumulative sums\n\n\n247\nFenwick Query\nPrefix sum retrieval\n\n\n248\nSegment Tree Merge\nCombine child results\n\n\n249\nPersistent Segment Tree\nMaintain history of versions\n\n\n250\n2D Segment Tree\nFor matrix range queries\n\n\n\n\n\n26. Disjoint Set Union (Union-Find)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n251\nMake-Set\nInitialize each element\n\n\n252\nFind\nLocate representative\n\n\n253\nUnion\nMerge two sets\n\n\n254\nUnion by Rank\nAttach smaller tree to larger\n\n\n255\nPath Compression\nFlatten tree structure\n\n\n256\nDSU with Rollback\nSupport undo operations\n\n\n257\nDSU on Tree\nTrack subtree connectivity\n\n\n258\nKruskal’s MST\nEdge selection with DSU\n\n\n259\nConnected Components\nGroup graph nodes\n\n\n260\nOffline Query DSU\nHandle dynamic unions\n\n\n\n\n\n27. Probabilistic Data Structures (Bloom, Count-Min, HyperLogLog)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n261\nBloom Filter Insert\nHash to bit array\n\n\n262\nBloom Filter Query\nProbabilistic membership check\n\n\n263\nCounting Bloom Filter\nSupport deletions via counters\n\n\n264\nCuckoo Filter\nSpace-efficient alternative\n\n\n265\nCount-Min Sketch\nApproximate frequency table\n\n\n266\nHyperLogLog\nCardinality estimation\n\n\n267\nFlajolet-Martin\nEarly probabilistic counting\n\n\n268\nMinHash\nEstimate Jaccard similarity\n\n\n269\nReservoir Sampling\nRandom k-sample stream\n\n\n270\nSkip Bloom Filter\nRange queries on Bloom\n\n\n\n\n\n28. Skip Lists and B-Trees\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n271\nSkip List Insert\nProbabilistic layered list\n\n\n272\nSkip List Delete\nAdjust pointers\n\n\n273\nSkip List Search\nJump via tower levels\n\n\n274\nB-Tree Insert\nSplit on overflow\n\n\n275\nB-Tree Delete\nMerge on underflow\n\n\n276\nB+ Tree Search\nLeaf-based sequential scan\n\n\n277\nB+ Tree Range Query\nEfficient ordered access\n\n\n278\nB* Tree\nMore space-efficient variant\n\n\n279\nAdaptive Radix Tree\nByte-wise branching\n\n\n280\nTrie Compression\nPath compression optimization\n\n\n\n\n\n29. Persistent and Functional Data Structures\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n281\nPersistent Stack\nKeep all versions\n\n\n282\nPersistent Array\nCopy-on-write segments\n\n\n283\nPersistent Segment Tree\nVersioned updates\n\n\n284\nPersistent Linked List\nImmutable nodes\n\n\n285\nFunctional Queue\nAmortized reverse lists\n\n\n286\nFinger Tree\nFast concat and split\n\n\n287\nZipper Structure\nLocalized mutation\n\n\n288\nRed-Black Persistent Tree\nImmutable balanced tree\n\n\n289\nTrie with Versioning\nHistorical string lookup\n\n\n290\nPersistent Union-Find\nTime-travel connectivity\n\n\n\n\n\n30. Advanced Trees and Range Queries\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n291\nSparse Table Build\nStatic range min/max\n\n\n292\nCartesian Tree\nRMQ to LCA transformation\n\n\n293\nSegment Tree Beats\nHandle complex queries\n\n\n294\nMerge Sort Tree\nRange count queries\n\n\n295\nWavelet Tree\nRank/select by value\n\n\n296\nKD-Tree\nMultidimensional queries\n\n\n297\nRange Tree\nOrthogonal range queries\n\n\n298\nFenwick 2D Tree\nMatrix prefix sums\n\n\n299\nTreap Split/Merge\nRange-based treap ops\n\n\n300\nMo’s Algorithm on Tree\nOffline subtree queries\n\n\n\n\n\n\nChapter 4. Graph Algorithms\n\n31. Traversals (DFS, BFS, Iterative Deepening)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n301\nDepth-First Search (Recursive)\nExplore deeply before backtracking\n\n\n302\nDepth-First Search (Iterative)\nStack-based exploration\n\n\n303\nBreadth-First Search (Queue)\nLevel-order traversal\n\n\n304\nIterative Deepening DFS\nCombine depth-limit + completeness\n\n\n305\nBidirectional BFS\nSearch from both ends\n\n\n306\nDFS on Grid\nMaze solving / connected components\n\n\n307\nBFS on Grid\nShortest path in unweighted graph\n\n\n308\nMulti-Source BFS\nParallel layer expansion\n\n\n309\nTopological Sort (DFS-based)\nDAG ordering\n\n\n310\nTopological Sort (Kahn’s Algorithm)\nIn-degree tracking\n\n\n\n\n\n32. Strongly Connected Components (Tarjan, Kosaraju)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n311\nKosaraju’s Algorithm\nTwo-pass DFS\n\n\n312\nTarjan’s Algorithm\nLow-link discovery\n\n\n313\nGabow’s Algorithm\nStack pair tracking\n\n\n314\nSCC DAG Construction\nCondensed component graph\n\n\n315\nSCC Online Merge\nIncremental condensation\n\n\n316\nComponent Label Propagation\nIterative labeling\n\n\n317\nPath-Based SCC\nDFS with path stack\n\n\n318\nKosaraju Parallel Version\nSCC via parallel DFS\n\n\n319\nDynamic SCC Maintenance\nAdd/remove edges\n\n\n320\nSCC for Weighted Graph\nCombine with edge weights\n\n\n\n\n\n33. Shortest Paths (Dijkstra, Bellman-Ford, A*, Johnson)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n321\nDijkstra (Binary Heap)\nGreedy edge relaxation\n\n\n322\nDijkstra (Fibonacci Heap)\nImproved priority queue\n\n\n323\nBellman-Ford\nNegative weights support\n\n\n324\nSPFA (Queue Optimization)\nFaster average Bellman-Ford\n\n\n325\nA* Search\nHeuristic-guided path\n\n\n326\nFloyd–Warshall\nAll-pairs shortest path\n\n\n327\nJohnson’s Algorithm\nAll-pairs using reweighting\n\n\n328\n0-1 BFS\nDeque-based shortest path\n\n\n329\nDial’s Algorithm\nInteger weight buckets\n\n\n330\nMulti-Source Dijkstra\nMultiple starting points\n\n\n\n\n\n34. Shortest Path Variants (0–1 BFS, Bidirectional, Heuristic A*)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n331\n0–1 BFS\nFor edges with weight 0 or 1\n\n\n332\nBidirectional Dijkstra\nMeet in the middle\n\n\n333\nA* with Euclidean Heuristic\nSpatial shortest path\n\n\n334\nALT Algorithm\nA* landmarks + triangle inequality\n\n\n335\nContraction Hierarchies\nPreprocessing for road networks\n\n\n336\nCH Query Algorithm\nShortcut-based routing\n\n\n337\nBellman-Ford Queue Variant\nEarly termination\n\n\n338\nDijkstra with Early Stop\nHalt on target\n\n\n339\nGoal-Directed Search\nRestrict expansion direction\n\n\n340\nYen’s K Shortest Paths\nEnumerate multiple best paths\n\n\n\n\n\n35. Minimum Spanning Trees (Kruskal, Prim, Borůvka)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n341\nKruskal’s Algorithm\nSort edges + union-find\n\n\n342\nPrim’s Algorithm (Heap)\nGrow MST from seed\n\n\n343\nPrim’s Algorithm (Adj Matrix)\nDense graph variant\n\n\n344\nBorůvka’s Algorithm\nComponent merging\n\n\n345\nReverse-Delete MST\nRemove heavy edges\n\n\n346\nMST via Dijkstra Trick\nFor positive weights\n\n\n347\nDynamic MST Maintenance\nHandle edge updates\n\n\n348\nMinimum Bottleneck Spanning Tree\nMax edge minimization\n\n\n349\nManhattan MST\nGrid graph optimization\n\n\n350\nEuclidean MST (Kruskal + Geometry)\nUse Delaunay graph\n\n\n\n\n\n36. Flows (Ford–Fulkerson, Edmonds–Karp, Dinic)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n351\nFord–Fulkerson\nAugmenting path method\n\n\n352\nEdmonds–Karp\nBFS-based Ford–Fulkerson\n\n\n353\nDinic’s Algorithm\nLevel graph + blocking flow\n\n\n354\nPush–Relabel\nLocal preflow push\n\n\n355\nCapacity Scaling\nSpeed-up with capacity tiers\n\n\n356\nCost Scaling\nMin-cost optimization\n\n\n357\nMin-Cost Max-Flow (Bellman-Ford)\nCosted augmenting paths\n\n\n358\nMin-Cost Max-Flow (SPFA)\nFaster average\n\n\n359\nCirculation with Demands\nGeneralized flow formulation\n\n\n360\nSuccessive Shortest Path\nIncremental min-cost updates\n\n\n\n\n\n37. Cuts (Stoer–Wagner, Karger, Gomory–Hu)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n361\nStoer–Wagner Minimum Cut\nGlobal min cut\n\n\n362\nKarger’s Randomized Cut\nContract edges randomly\n\n\n363\nKarger–Stein\nRecursive randomized cut\n\n\n364\nGomory–Hu Tree\nAll-pairs min-cut\n\n\n365\nMax-Flow Min-Cut\nDuality theorem application\n\n\n366\nStoer–Wagner Repeated Phase\nMultiple passes\n\n\n367\nDynamic Min Cut\nMaintain on edge update\n\n\n368\nMinimum s–t Cut (Edmonds–Karp)\nBased on flow\n\n\n369\nApproximate Min Cut\nRandom sampling\n\n\n370\nMin k-Cut\nPartition graph into k parts\n\n\n\n\n\n38. Matchings (Hopcroft–Karp, Hungarian, Blossom)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n371\nBipartite Matching (DFS)\nSimple augmenting path\n\n\n372\nHopcroft–Karp\nO(E√V) bipartite matching\n\n\n373\nHungarian Algorithm\nWeighted assignment\n\n\n374\nKuhn–Munkres\nMax-weight matching\n\n\n375\nBlossom Algorithm\nGeneral graph matching\n\n\n376\nEdmonds’ Blossom Shrinking\nOdd cycle contraction\n\n\n377\nGreedy Matching\nFast approximate\n\n\n378\nStable Marriage (Gale–Shapley)\nStable pairing\n\n\n379\nWeighted b-Matching\nCapacity-constrained\n\n\n380\nMaximal Matching\nLocal greedy maximal set\n\n\n\n\n\n39. Tree Algorithms (LCA, HLD, Centroid Decomposition)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n381\nEuler Tour LCA\nFlatten tree to array\n\n\n382\nBinary Lifting LCA\nJump powers of two\n\n\n383\nTarjan’s LCA (Offline DSU)\nQuery via union-find\n\n\n384\nHeavy-Light Decomposition\nDecompose paths\n\n\n385\nCentroid Decomposition\nRecursive split on centroid\n\n\n386\nTree Diameter (DFS Twice)\nFarthest pair\n\n\n387\nTree DP\nSubtree-based optimization\n\n\n388\nRerooting DP\nCompute all roots’ answers\n\n\n389\nBinary Search on Tree\nEdge weight constraints\n\n\n390\nVirtual Tree\nBuild on query subset\n\n\n\n\n\n40. Advanced Graph Algorithms and Tricks\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n391\nTopological DP\nDP on DAG order\n\n\n392\nSCC Condensed Graph DP\nMeta-graph processing\n\n\n393\nEulerian Path\nTrail covering all edges\n\n\n394\nHamiltonian Path\nNP-complete exploration\n\n\n395\nChinese Postman\nEulerian circuit with repeats\n\n\n396\nHierholzer’s Algorithm\nConstruct Eulerian circuit\n\n\n397\nJohnson’s Cycle Finding\nEnumerate all cycles\n\n\n398\nTransitive Closure (Floyd–Warshall)\nReachability matrix\n\n\n399\nGraph Coloring (Backtracking)\nConstraint satisfaction\n\n\n400\nArticulation Points & Bridges\nCritical structure detection\n\n\n\n\n\n\nChapter 5. Dynamic Programming\n\n41. DP Basics and State Transitions\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n401\nFibonacci DP\nClassic top-down vs bottom-up\n\n\n402\nClimbing Stairs\nCount paths with small steps\n\n\n403\nGrid Paths\nDP over 2D lattice\n\n\n404\nMin Cost Path\nAccumulate minimal sums\n\n\n405\nCoin Change (Count Ways)\nCombinatorial sums\n\n\n406\nCoin Change (Min Coins)\nMinimize step count\n\n\n407\nKnapsack 0/1\nSelect items under weight limit\n\n\n408\nKnapsack Unbounded\nRepeatable items\n\n\n409\nLongest Increasing Subsequence (DP)\nSubsequence optimization\n\n\n410\nEdit Distance (Levenshtein)\nMeasure similarity step-by-step\n\n\n\n\n\n42. Classic Problems (Knapsack, Subset Sum, Coin Change)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n411\n0/1 Knapsack\nValue maximization under capacity\n\n\n412\nSubset Sum\nBoolean feasibility DP\n\n\n413\nEqual Partition\nDivide set into equal halves\n\n\n414\nCount of Subsets with Sum\nCounting variant\n\n\n415\nTarget Sum\nDP with +/- transitions\n\n\n416\nUnbounded Knapsack\nReuse items\n\n\n417\nFractional Knapsack\nGreedy + DP comparison\n\n\n418\nCoin Change (Min Coins)\nDP shortest path\n\n\n419\nCoin Change (Count Ways)\nCombinatorial counting\n\n\n420\nMulti-Dimensional Knapsack\nCapacity in multiple dimensions\n\n\n\n\n\n43. Sequence Problems (LIS, LCS, Edit Distance)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n421\nLongest Increasing Subsequence\nO(n²) DP\n\n\n422\nLIS (Patience Sorting)\nO(n log n) optimized\n\n\n423\nLongest Common Subsequence\nTwo-sequence DP\n\n\n424\nEdit Distance (Levenshtein)\nTransform operations\n\n\n425\nLongest Palindromic Subsequence\nSymmetric DP\n\n\n426\nShortest Common Supersequence\nMerge sequences\n\n\n427\nLongest Repeated Subsequence\nDP with overlap\n\n\n428\nString Interleaving\nMerge with order preservation\n\n\n429\nSequence Alignment (Bioinformatics)\nGap penalties\n\n\n430\nDiff Algorithm (Myers/DP)\nMinimal edit path\n\n\n\n\n\n44. Matrix and Chain Problems\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n431\nMatrix Chain Multiplication\nParenthesization cost\n\n\n432\nBoolean Parenthesization\nCount true outcomes\n\n\n433\nBurst Balloons\nInterval DP\n\n\n434\nOptimal BST\nWeighted search cost\n\n\n435\nPolygon Triangulation\nDP over partitions\n\n\n436\nMatrix Path Sum\nDP on 2D grid\n\n\n437\nLargest Square Submatrix\nDynamic growth check\n\n\n438\nMax Rectangle in Binary Matrix\nHistogram + DP\n\n\n439\nSubmatrix Sum Queries\nPrefix sum DP\n\n\n440\nPalindrome Partitioning\nDP with cuts\n\n\n\n\n\n45. Bitmask DP and Traveling Salesman\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n441\nTraveling Salesman (TSP)\nVisit all cities\n\n\n442\nSubset DP\nOver subsets of states\n\n\n443\nHamiltonian Path DP\nState compression\n\n\n444\nAssignment Problem DP\nMask over tasks\n\n\n445\nPartition into Two Sets\nBalanced load\n\n\n446\nCount Hamiltonian Cycles\nBitmask enumeration\n\n\n447\nSteiner Tree DP\nMinimal connection of terminals\n\n\n448\nSOS DP (Sum Over Subsets)\nPrecompute sums\n\n\n449\nBitmask Knapsack\nState compression\n\n\n450\nBitmask Independent Set\nGraph subset optimization\n\n\n\n\n\n46. Digit DP and SOS DP\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n451\nCount Numbers with Property\nDigit-state transitions\n\n\n452\nCount Without Adjacent Duplicates\nAdjacent constraints\n\n\n453\nSum of Digits in Range\nCarry-dependent states\n\n\n454\nCount with Mod Condition\nDP over digit sum mod M\n\n\n455\nCount of Increasing Digits\nOrdered constraint\n\n\n456\nCount with Forbidden Digits\nExclusion transitions\n\n\n457\nSOS DP Subset Sum\nSum over bitmask subsets\n\n\n458\nSOS DP Superset Sum\nSum over supersets\n\n\n459\nXOR Basis DP\nCombine digit and bit DP\n\n\n460\nDigit DP for Palindromes\nSymmetric digit state\n\n\n\n\n\n47. DP Optimizations (Divide & Conquer, Convex Hull Trick, Knuth)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n461\nDivide & Conquer DP\nMonotone decision property\n\n\n462\nKnuth Optimization\nDP with quadrangle inequality\n\n\n463\nConvex Hull Trick\nLinear recurrence min queries\n\n\n464\nLi Chao Tree\nSegment-based hull maintenance\n\n\n465\nSlope Trick\nPiecewise-linear optimization\n\n\n466\nMonotonic Queue Optimization\nSliding DP state\n\n\n467\nBitset DP\nSpeed via bit-parallel\n\n\n468\nOffline DP Queries\nPreprocessing state\n\n\n469\nDP + Segment Tree\nRange-based optimization\n\n\n470\nDivide & Conquer Knapsack\nSplit-space DP\n\n\n\n\n\n48. Tree DP and Rerooting\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n471\nSubtree Sum DP\nAggregate values\n\n\n472\nDiameter DP\nMax path via child states\n\n\n473\nIndependent Set DP\nChoose or skip nodes\n\n\n474\nVertex Cover DP\nTree constraint problem\n\n\n475\nPath Counting DP\nCount root-leaf paths\n\n\n476\nDP on Rooted Tree\nBottom-up aggregation\n\n\n477\nRerooting Technique\nCompute for all roots\n\n\n478\nDistance Sum Rerooting\nEfficient recomputation\n\n\n479\nTree Coloring DP\nCombinatorial counting\n\n\n480\nBinary Search on Tree DP\nMonotonic transitions\n\n\n\n\n\n49. DP Reconstruction and Traceback\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n481\nReconstruct LCS\nBacktrack table\n\n\n482\nReconstruct LIS\nTrack predecessors\n\n\n483\nReconstruct Knapsack\nRecover selected items\n\n\n484\nEdit Distance Alignment\nTrace insert/delete/substitute\n\n\n485\nMatrix Chain Parentheses\nRebuild parenthesization\n\n\n486\nCoin Change Reconstruction\nBacktrack last used coin\n\n\n487\nPath Reconstruction DP\nTrace minimal route\n\n\n488\nSequence Reconstruction\nRebuild from states\n\n\n489\nMulti-Choice Reconstruction\nCombine best subpaths\n\n\n490\nTraceback Visualization\nVisual DP backtrack tool\n\n\n\n\n\n50. Meta-DP and Optimization Templates\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n491\nState Compression Template\nRepresent subsets compactly\n\n\n492\nTransition Optimization Template\nPrecompute transitions\n\n\n493\nSpace Optimization Template\nRolling arrays\n\n\n494\nMulti-Dimensional DP Template\nNested loops version\n\n\n495\nDecision Monotonicity\nOptimization hint\n\n\n496\nMonge Array Optimization\nMatrix property leverage\n\n\n497\nDivide & Conquer Template\nHalf-split recursion\n\n\n498\nRerooting Template\nGeneralized tree DP\n\n\n499\nIterative DP Pattern\nBottom-up unrolling\n\n\n500\nMemoization Template\nRecursive caching skeleton\n\n\n\n\n\n\nChapter 6. Mathematics for Algorithms\n\n51. Number Theory (GCD, Modular Arithmetic, CRT)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n501\nEuclidean Algorithm\nCompute gcd(a, b)\n\n\n502\nExtended Euclidean Algorithm\nSolve ax + by = gcd(a, b)\n\n\n503\nModular Addition\nAdd under modulo M\n\n\n504\nModular Multiplication\nMultiply under modulo M\n\n\n505\nModular Exponentiation\nFast power mod M\n\n\n506\nModular Inverse\nCompute a⁻¹ mod M\n\n\n507\nChinese Remainder Theorem\nCombine modular systems\n\n\n508\nBinary GCD (Stein’s Algorithm)\nBitwise gcd\n\n\n509\nModular Reduction\nNormalize residues\n\n\n510\nModular Linear Equation Solver\nSolve ax ≡ b (mod m)\n\n\n\n\n\n52. Primality and Factorization (Miller–Rabin, Pollard Rho)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n511\nTrial Division\nSimple prime test\n\n\n512\nSieve of Eratosthenes\nGenerate primes up to n\n\n\n513\nSieve of Atkin\nFaster sieve variant\n\n\n514\nMiller–Rabin Primality Test\nProbabilistic primality\n\n\n515\nFermat Primality Test\nModular power check\n\n\n516\nPollard’s Rho\nRandomized factorization\n\n\n517\nPollard’s p−1 Method\nFactor using smoothness\n\n\n518\nWheel Factorization\nSkip known composites\n\n\n519\nAKS Primality Test\nDeterministic polynomial test\n\n\n520\nSegmented Sieve\nPrime generation for large n\n\n\n\n\n\n53. Combinatorics (Permutations, Combinations, Subsets)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n521\nFactorial Precomputation\nBuild n! table\n\n\n522\nnCr Computation\nUse Pascal’s or factorials\n\n\n523\nPascal’s Triangle\nBinomial coefficients\n\n\n524\nMultiset Combination\nRepetition allowed\n\n\n525\nPermutation Generation\nLexicographic order\n\n\n526\nNext Permutation\nSTL-style increment\n\n\n527\nSubset Generation\nBitmask or recursion\n\n\n528\nGray Code Generation\nSingle-bit flips\n\n\n529\nCatalan Number DP\nCount valid parentheses\n\n\n530\nStirling Numbers\nPartition counting\n\n\n\n\n\n54. Probability and Randomized Algorithms\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n531\nMonte Carlo Simulation\nApproximate via randomness\n\n\n532\nLas Vegas Algorithm\nAlways correct, variable time\n\n\n533\nReservoir Sampling\nUniform sampling from stream\n\n\n534\nRandomized QuickSort\nExpected O(n log n)\n\n\n535\nRandomized QuickSelect\nRandom pivot\n\n\n536\nBirthday Paradox Simulation\nProbability collision\n\n\n537\nRandom Hashing\nReduce collision chance\n\n\n538\nRandom Walk Simulation\nState transitions\n\n\n539\nCoupon Collector Estimation\nExpected trials\n\n\n540\nMarkov Chain Simulation\nTransition matrix sampling\n\n\n\n\n\n55. Sieve Methods and Modular Math\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n541\nSieve of Eratosthenes\nBase prime sieve\n\n\n542\nLinear Sieve\nO(n) sieve variant\n\n\n543\nSegmented Sieve\nRange prime generation\n\n\n544\nSPF (Smallest Prime Factor) Table\nFactorization via sieve\n\n\n545\nMöbius Function Sieve\nMultiplicative function calc\n\n\n546\nEuler’s Totient Sieve\nCompute φ(n) for all n\n\n\n547\nDivisor Count Sieve\nCount divisors efficiently\n\n\n548\nModular Precomputation\nStore inverses, factorials\n\n\n549\nFermat Little Theorem\na^(p−1) ≡ 1 mod p\n\n\n550\nWilson’s Theorem\nPrime test via factorial mod p\n\n\n\n\n\n56. Linear Algebra (Gaussian Elimination, LU, SVD)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n551\nGaussian Elimination\nSolve Ax = b\n\n\n552\nGauss-Jordan Elimination\nReduced row echelon\n\n\n553\nLU Decomposition\nFactor A into L·U\n\n\n554\nCholesky Decomposition\nA = L·Lᵀ for SPD\n\n\n555\nQR Decomposition\nOrthogonal factorization\n\n\n556\nMatrix Inversion (Gauss-Jordan)\nFind A⁻¹\n\n\n557\nDeterminant by Elimination\nProduct of pivots\n\n\n558\nRank of Matrix\nCount non-zero rows\n\n\n559\nEigenvalue Power Method\nApproximate dominant eigenvalue\n\n\n560\nSingular Value Decomposition\nA = UΣVᵀ\n\n\n\n\n\n57. FFT and NTT (Fast Transforms)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n561\nDiscrete Fourier Transform (DFT)\nO(n²) baseline\n\n\n562\nFast Fourier Transform (FFT)\nO(n log n) convolution\n\n\n563\nCooley–Tukey FFT\nRecursive divide and conquer\n\n\n564\nIterative FFT\nIn-place bit reversal\n\n\n565\nInverse FFT\nRecover time-domain\n\n\n566\nConvolution via FFT\nPolynomial multiplication\n\n\n567\nNumber Theoretic Transform (NTT)\nModulo prime FFT\n\n\n568\nInverse NTT\nModular inverse transform\n\n\n569\nBluestein’s Algorithm\nFFT of arbitrary size\n\n\n570\nFFT-Based Multiplication\nBig integer product\n\n\n\n\n\n58. Numerical Methods (Newton, Simpson, Runge–Kutta)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n571\nNewton–Raphson\nRoot finding via tangent\n\n\n572\nBisection Method\nInterval halving\n\n\n573\nSecant Method\nApproximate derivative\n\n\n574\nFixed-Point Iteration\nx = f(x) convergence\n\n\n575\nGaussian Quadrature\nWeighted integration\n\n\n576\nSimpson’s Rule\nPiecewise quadratic integral\n\n\n577\nTrapezoidal Rule\nLinear interpolation integral\n\n\n578\nRunge–Kutta (RK4)\nODE solver\n\n\n579\nEuler’s Method\nStep-by-step ODE\n\n\n580\nGradient Descent (1D)\nNumerical optimization\n\n\n\n\n\n59. Mathematical Optimization (Simplex, Gradient, Convex)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n581\nSimplex Method\nLinear programming solver\n\n\n582\nDual Simplex Method\nSolve dual constraints\n\n\n583\nInterior-Point Method\nConvex optimization\n\n\n584\nGradient Descent\nUnconstrained optimization\n\n\n585\nStochastic Gradient Descent\nSample-based updates\n\n\n586\nNewton’s Method (Multivariate)\nQuadratic convergence\n\n\n587\nConjugate Gradient\nSolve SPD systems\n\n\n588\nLagrange Multipliers\nConstrained optimization\n\n\n589\nKKT Conditions Solver\nConvex constraint handling\n\n\n590\nCoordinate Descent\nSequential variable updates\n\n\n\n\n\n60. Algebraic Tricks and Transform Techniques\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n591\nPolynomial Multiplication (FFT)\nFast convolution\n\n\n592\nPolynomial Inversion\nNewton iteration\n\n\n593\nPolynomial Derivative\nTerm-wise multiply by index\n\n\n594\nPolynomial Integration\nDivide by index+1\n\n\n595\nFormal Power Series Composition\nSubstitute series\n\n\n596\nExponentiation by Squaring\nFast powering\n\n\n597\nModular Exponentiation\nFast power mod M\n\n\n598\nFast Walsh–Hadamard Transform\nXOR convolution\n\n\n599\nZeta Transform\nSubset summation\n\n\n600\nMöbius Inversion\nRecover original from sums\n\n\n\n\n\n\nChapter 7. Strings and Text Algorithms\n\n61. String Matching (KMP, Z, Rabin–Karp, Boyer–Moore)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n601\nNaive String Matching\nCompare every position\n\n\n602\nKnuth–Morris–Pratt (KMP)\nPrefix function skipping\n\n\n603\nZ-Algorithm\nMatch using Z-values\n\n\n604\nRabin–Karp\nRolling hash comparison\n\n\n605\nBoyer–Moore\nBackward skip based on mismatch\n\n\n606\nBoyer–Moore–Horspool\nSimplified shift table\n\n\n607\nSunday Algorithm\nLast-character shift\n\n\n608\nFinite Automaton Matching\nDFA-based matching\n\n\n609\nBitap Algorithm\nBitmask approximate matching\n\n\n610\nTwo-Way Algorithm\nOptimal linear matching\n\n\n\n\n\n62. Multi-Pattern Search (Aho–Corasick)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n611\nAho–Corasick Automaton\nTrie + failure links\n\n\n612\nTrie Construction\nPrefix tree build\n\n\n613\nFailure Link Computation\nBFS for transitions\n\n\n614\nOutput Link Management\nHandle overlapping patterns\n\n\n615\nMulti-Pattern Search\nFind all keywords\n\n\n616\nDictionary Matching\nFind multiple substrings\n\n\n617\nDynamic Aho–Corasick\nAdd/remove patterns\n\n\n618\nParallel AC Search\nMulti-threaded traversal\n\n\n619\nCompressed AC Automaton\nMemory-optimized\n\n\n620\nExtended AC with Wildcards\nFlexible matching\n\n\n\n\n\n63. Suffix Structures (Suffix Array, Suffix Tree, LCP)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n621\nSuffix Array (Naive)\nSort all suffixes\n\n\n622\nSuffix Array (Doubling)\nO(n log n) rank-based\n\n\n623\nKasai’s LCP Algorithm\nLongest common prefix\n\n\n624\nSuffix Tree (Ukkonen)\nLinear-time online\n\n\n625\nSuffix Automaton\nMinimal DFA of substrings\n\n\n626\nSA-IS Algorithm\nO(n) suffix array\n\n\n627\nLCP RMQ Query\nRange minimum for substring\n\n\n628\nGeneralized Suffix Array\nMultiple strings\n\n\n629\nEnhanced Suffix Array\nCombine SA + LCP\n\n\n630\nSparse Suffix Tree\nSpace-efficient variant\n\n\n\n\n\n64. Palindromes and Periodicity (Manacher)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n631\nNaive Palindrome Check\nExpand around center\n\n\n632\nManacher’s Algorithm\nO(n) longest palindrome\n\n\n633\nLongest Palindromic Substring\nCenter expansion\n\n\n634\nPalindrome DP Table\nSubstring boolean matrix\n\n\n635\nPalindromic Tree (Eertree)\nTrack distinct palindromes\n\n\n636\nPrefix Function Periodicity\nDetect repetition patterns\n\n\n637\nZ-Function Periodicity\nIdentify periodic suffix\n\n\n638\nKMP Prefix Period Check\nShortest repeating unit\n\n\n639\nLyndon Factorization\nDecompose string into Lyndon words\n\n\n640\nMinimal Rotation (Booth’s Algorithm)\nLexicographically minimal shift\n\n\n\n\n\n65. Edit Distance and Alignment\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n641\nLevenshtein Distance\nInsert/delete/replace cost\n\n\n642\nDamerau–Levenshtein\nSwap included\n\n\n643\nHamming Distance\nCount differing bits\n\n\n644\nNeedleman–Wunsch\nGlobal alignment\n\n\n645\nSmith–Waterman\nLocal alignment\n\n\n646\nHirschberg’s Algorithm\nMemory-optimized alignment\n\n\n647\nEdit Script Reconstruction\nBacktrack operations\n\n\n648\nAffine Gap Penalty DP\nVarying gap cost\n\n\n649\nMyers Bit-Vector Algorithm\nFast edit distance\n\n\n650\nLongest Common Subsequence\nAlignment by inclusion\n\n\n\n\n\n66. Compression (Huffman, Arithmetic, LZ77, BWT)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n651\nHuffman Coding\nOptimal prefix tree\n\n\n652\nCanonical Huffman\nDeterministic ordering\n\n\n653\nArithmetic Coding\nInterval probability coding\n\n\n654\nShannon–Fano Coding\nEarly prefix method\n\n\n655\nRun-Length Encoding (RLE)\nRepeat compression\n\n\n656\nLZ77\nSliding-window match\n\n\n657\nLZ78\nDictionary building\n\n\n658\nLZW\nVariant used in GIF\n\n\n659\nBurrows–Wheeler Transform\nBlock reordering\n\n\n660\nMove-to-Front Encoding\nLocality boosting transform\n\n\n\n\n\n67. Cryptographic Hashes and Checksums\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n661\nRolling Hash\nPolynomial mod-based\n\n\n662\nCRC32\nCyclic redundancy check\n\n\n663\nAdler-32\nLightweight checksum\n\n\n664\nMD5\nLegacy cryptographic hash\n\n\n665\nSHA-1\nDeprecated hash function\n\n\n666\nSHA-256\nSecure hash standard\n\n\n667\nSHA-3 (Keccak)\nSponge construction\n\n\n668\nHMAC\nKeyed message authentication\n\n\n669\nMerkle Tree\nHierarchical hashing\n\n\n670\nHash Collision Detection\nBirthday bound simulation\n\n\n\n\n\n68. Approximate and Streaming Matching\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n671\nK-Approximate Matching\nAllow k mismatches\n\n\n672\nBitap Algorithm\nBitwise dynamic programming\n\n\n673\nLandau–Vishkin Algorithm\nEdit distance ≤ k\n\n\n674\nFiltering Algorithm\nFast approximate search\n\n\n675\nWu–Manber\nMulti-pattern approximate search\n\n\n676\nStreaming KMP\nOnline prefix updates\n\n\n677\nRolling Hash Sketch\nSliding window hashing\n\n\n678\nSketch-based Similarity\nMinHash / LSH variants\n\n\n679\nWeighted Edit Distance\nWeighted operations\n\n\n680\nOnline Levenshtein\nDynamic stream update\n\n\n\n\n\n69. Bioinformatics Alignment (Needleman–Wunsch, Smith–Waterman)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n681\nNeedleman–Wunsch\nGlobal sequence alignment\n\n\n682\nSmith–Waterman\nLocal alignment\n\n\n683\nGotoh Algorithm\nAffine gap penalties\n\n\n684\nHirschberg Alignment\nLinear-space alignment\n\n\n685\nMultiple Sequence Alignment (MSA)\nProgressive methods\n\n\n686\nProfile Alignment\nAlign sequence to profile\n\n\n687\nHidden Markov Model Alignment\nProbabilistic alignment\n\n\n688\nBLAST\nHeuristic local search\n\n\n689\nFASTA\nWord-based alignment\n\n\n690\nPairwise DP Alignment\nGeneral DP framework\n\n\n\n\n\n70. Text Indexing and Search Structures\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n691\nInverted Index Build\nWord-to-document mapping\n\n\n692\nPositional Index\nStore word positions\n\n\n693\nTF-IDF Weighting\nImportance scoring\n\n\n694\nBM25 Ranking\nModern ranking formula\n\n\n695\nTrie Index\nPrefix search structure\n\n\n696\nSuffix Array Index\nSubstring search\n\n\n697\nCompressed Suffix Array\nSpace-optimized\n\n\n698\nFM-Index\nBWT-based compressed index\n\n\n699\nDAWG (Directed Acyclic Word Graph)\nShared suffix graph\n\n\n700\nWavelet Tree for Text\nRank/select on sequences\n\n\n\n\n\n\nChapter 8. Geometry, Graphics, and Spatial Algorithms\n\n71. Convex Hull (Graham, Andrew, Chan)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n701\nGift Wrapping (Jarvis March)\nWrap hull one point at a time\n\n\n702\nGraham Scan\nSort by angle, maintain stack\n\n\n703\nAndrew’s Monotone Chain\nSort by x, upper + lower hull\n\n\n704\nChan’s Algorithm\nOutput-sensitive O(n log h)\n\n\n705\nQuickHull\nDivide-and-conquer hull\n\n\n706\nIncremental Convex Hull\nAdd points one by one\n\n\n707\nDivide & Conquer Hull\nMerge two partial hulls\n\n\n708\n3D Convex Hull\nExtend to 3D geometry\n\n\n709\nDynamic Convex Hull\nMaintain hull with inserts\n\n\n710\nRotating Calipers\nCompute diameter, width, antipodal pairs\n\n\n\n\n\n72. Closest Pair and Segment Intersection\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n711\nClosest Pair (Divide & Conquer)\nSplit, merge minimal distance\n\n\n712\nClosest Pair (Sweep Line)\nMaintain active window\n\n\n713\nBrute Force Closest Pair\nCheck all O(n²) pairs\n\n\n714\nBentley–Ottmann\nFind all line intersections\n\n\n715\nSegment Intersection Test\nCross product orientation\n\n\n716\nLine Sweep for Segments\nEvent-based intersection\n\n\n717\nIntersection via Orientation\nCCW test\n\n\n718\nCircle Intersection\nGeometry of two circles\n\n\n719\nPolygon Intersection\nClip overlapping polygons\n\n\n720\nNearest Neighbor Pair\nCombine KD-tree + search\n\n\n\n\n\n73. Line Sweep and Plane Sweep Algorithms\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n721\nSweep Line for Events\nProcess sorted events\n\n\n722\nInterval Scheduling\nSelect non-overlapping intervals\n\n\n723\nRectangle Union Area\nSweep edges to count area\n\n\n724\nSegment Intersection (Bentley–Ottmann)\nDetect all crossings\n\n\n725\nSkyline Problem\nMerge height profiles\n\n\n726\nClosest Pair Sweep\nMaintain active set\n\n\n727\nCircle Arrangement\nSweep and count regions\n\n\n728\nSweep for Overlapping Rectangles\nDetect collisions\n\n\n729\nRange Counting\nCount points in rectangle\n\n\n730\nPlane Sweep for Triangles\nPolygon overlay computation\n\n\n\n\n\n74. Delaunay and Voronoi Diagrams\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n731\nDelaunay Triangulation (Incremental)\nAdd points, maintain Delaunay\n\n\n732\nDelaunay (Divide & Conquer)\nMerge triangulations\n\n\n733\nDelaunay (Fortune’s Sweep)\nO(n log n) construction\n\n\n734\nVoronoi Diagram (Fortune’s)\nSweep line beachline\n\n\n735\nIncremental Voronoi\nUpdate on insertion\n\n\n736\nBowyer–Watson\nEmpty circle criterion\n\n\n737\nDuality Transform\nConvert between Voronoi/Delaunay\n\n\n738\nPower Diagram\nWeighted Voronoi\n\n\n739\nLloyd’s Relaxation\nSmooth Voronoi cells\n\n\n740\nVoronoi Nearest Neighbor\nRegion-based lookup\n\n\n\n\n\n75. Point in Polygon and Polygon Triangulation\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n741\nRay Casting\nCount edge crossings\n\n\n742\nWinding Number\nAngle sum method\n\n\n743\nConvex Polygon Point Test\nOrientation checks\n\n\n744\nEar Clipping Triangulation\nRemove ears iteratively\n\n\n745\nMonotone Polygon Triangulation\nSweep line triangulation\n\n\n746\nDelaunay Triangulation\nOptimal triangle quality\n\n\n747\nConvex Decomposition\nSplit into convex parts\n\n\n748\nPolygon Area (Shoelace Formula)\nSigned area computation\n\n\n749\nMinkowski Sum\nAdd shapes geometrically\n\n\n750\nPolygon Intersection (Weiler–Atherton)\nClip overlapping shapes\n\n\n\n\n\n76. Spatial Data Structures (KD, R-tree)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n751\nKD-Tree Build\nRecursive median split\n\n\n752\nKD-Tree Search\nAxis-aligned query\n\n\n753\nRange Search KD-Tree\nOrthogonal query\n\n\n754\nNearest Neighbor KD-Tree\nClosest point search\n\n\n755\nR-Tree Build\nBounding box hierarchy\n\n\n756\nR*-Tree\nOptimized split strategy\n\n\n757\nQuad Tree\nSpatial decomposition\n\n\n758\nOctree\n3D spatial decomposition\n\n\n759\nBSP Tree (Binary Space Partition)\nSplit by planes\n\n\n760\nMorton Order (Z-Curve)\nSpatial locality index\n\n\n\n\n\n77. Rasterization and Scanline Techniques\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n761\nBresenham’s Line Algorithm\nEfficient integer drawing\n\n\n762\nMidpoint Circle Algorithm\nCircle rasterization\n\n\n763\nScanline Fill\nPolygon interior fill\n\n\n764\nEdge Table Fill\nSort edges by y\n\n\n765\nZ-Buffer Algorithm\nHidden surface removal\n\n\n766\nPainter’s Algorithm\nSort by depth\n\n\n767\nGouraud Shading\nVertex interpolation shading\n\n\n768\nPhong Shading\nNormal interpolation\n\n\n769\nAnti-Aliasing (Supersampling)\nSmooth jagged edges\n\n\n770\nScanline Polygon Clipping\nEfficient clipping\n\n\n\n\n\n78. Computer Vision (Canny, Hough, SIFT)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n771\nCanny Edge Detector\nGradient + hysteresis\n\n\n772\nSobel Operator\nGradient magnitude filter\n\n\n773\nHough Transform (Lines)\nAccumulator for line detection\n\n\n774\nHough Transform (Circles)\nRadius-based accumulator\n\n\n775\nHarris Corner Detector\nEigenvalue-based corners\n\n\n776\nFAST Corner Detector\nIntensity circle test\n\n\n777\nSIFT (Scale-Invariant Feature Transform)\nKeypoint detection\n\n\n778\nSURF (Speeded-Up Robust Features)\nFaster descriptor\n\n\n779\nORB (Oriented FAST + BRIEF)\nBinary robust feature\n\n\n780\nRANSAC\nRobust model fitting\n\n\n\n\n\n79. Pathfinding in Space (A*, RRT, PRM)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n781\nA* Search\nHeuristic pathfinding\n\n\n782\nDijkstra for Grid\nWeighted shortest path\n\n\n783\nTheta*\nAny-angle pathfinding\n\n\n784\nJump Point Search\nGrid acceleration\n\n\n785\nRRT (Rapidly-Exploring Random Tree)\nRandom sampling tree\n\n\n786\nRRT*\nOptimal variant with rewiring\n\n\n787\nPRM (Probabilistic Roadmap)\nGraph sampling planner\n\n\n788\nVisibility Graph\nConnect visible vertices\n\n\n789\nPotential Field Pathfinding\nGradient-based navigation\n\n\n790\nBug Algorithms\nSimple obstacle avoidance\n\n\n\n\n\n80. Computational Geometry Variants and Applications\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n791\nConvex Polygon Intersection\nClip convex sets\n\n\n792\nMinkowski Sum\nShape convolution\n\n\n793\nRotating Calipers\nClosest/farthest pair\n\n\n794\nHalf-Plane Intersection\nFeasible region\n\n\n795\nLine Arrangement\nCount regions\n\n\n796\nPoint Location (Trapezoidal Map)\nQuery region lookup\n\n\n797\nVoronoi Nearest Facility\nRegion query\n\n\n798\nDelaunay Mesh Generation\nTriangulation refinement\n\n\n799\nSmallest Enclosing Circle\nWelzl’s algorithm\n\n\n800\nCollision Detection (SAT)\nSeparating axis theorem\n\n\n\n\n\n\nChapter 9. Systems, Databases, and Distributed Algorithms\n\n81. Concurrency Control (2PL, MVCC, OCC)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n801\nTwo-Phase Locking (2PL)\nAcquire-then-release locks\n\n\n802\nStrict 2PL\nHold locks until commit\n\n\n803\nConservative 2PL\nPrevent deadlocks via prelock\n\n\n804\nTimestamp Ordering\nSchedule by timestamps\n\n\n805\nMultiversion Concurrency Control (MVCC)\nSnapshot isolation\n\n\n806\nOptimistic Concurrency Control (OCC)\nValidate at commit\n\n\n807\nSerializable Snapshot Isolation\nMerge read/write sets\n\n\n808\nLock-Free Algorithm\nAtomic CAS updates\n\n\n809\nWait-Die / Wound-Wait\nDeadlock prevention policies\n\n\n810\nDeadlock Detection (Wait-for Graph)\nCycle detection in waits\n\n\n\n\n\n82. Logging, Recovery, and Commit Protocols\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n811\nWrite-Ahead Logging (WAL)\nLog before commit\n\n\n812\nARIES Recovery\nRe-do/undo with LSNs\n\n\n813\nShadow Paging\nCopy-on-write persistence\n\n\n814\nTwo-Phase Commit (2PC)\nCoordinator-driven commit\n\n\n815\nThree-Phase Commit (3PC)\nNon-blocking variant\n\n\n816\nCheckpointing\nSave state for recovery\n\n\n817\nUndo Logging\nRollback uncommitted\n\n\n818\nRedo Logging\nReapply committed\n\n\n819\nQuorum Commit\nMajority agreement\n\n\n820\nConsensus Commit\nCombine 2PC + Paxos\n\n\n\n\n\n83. Scheduling (Round Robin, EDF, Rate-Monotonic)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n821\nFirst-Come First-Served (FCFS)\nSequential job order\n\n\n822\nShortest Job First (SJF)\nOptimal average wait\n\n\n823\nRound Robin (RR)\nTime-slice fairness\n\n\n824\nPriority Scheduling\nWeighted selection\n\n\n825\nMultilevel Queue\nTiered priority queues\n\n\n826\nEarliest Deadline First (EDF)\nReal-time optimal\n\n\n827\nRate Monotonic Scheduling (RMS)\nFixed periodic priority\n\n\n828\nLottery Scheduling\nProbabilistic fairness\n\n\n829\nMultilevel Feedback Queue\nAdaptive behavior\n\n\n830\nFair Queuing (FQ)\nFlow-based proportional sharing\n\n\n\n\n\n84. Caching and Replacement (LRU, LFU, CLOCK)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n831\nLRU (Least Recently Used)\nEvict oldest used\n\n\n832\nLFU (Least Frequently Used)\nEvict lowest frequency\n\n\n833\nFIFO Cache\nSimple queue eviction\n\n\n834\nCLOCK Algorithm\nApproximate LRU\n\n\n835\nARC (Adaptive Replacement Cache)\nMix of recency + frequency\n\n\n836\nTwo-Queue (2Q)\nSeparate recent/frequent\n\n\n837\nLIRS (Low Inter-reference Recency Set)\nPredict reuse distance\n\n\n838\nTinyLFU\nFrequency sketch admission\n\n\n839\nRandom Replacement\nSimple stochastic policy\n\n\n840\nBelady’s Optimal\nEvict farthest future use\n\n\n\n\n\n85. Networking (Routing, Congestion Control)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n841\nDijkstra’s Routing\nShortest path routing\n\n\n842\nBellman–Ford Routing\nDistance-vector routing\n\n\n843\nLink-State Routing (OSPF)\nGlobal view routing\n\n\n844\nDistance-Vector Routing (RIP)\nLocal neighbor updates\n\n\n845\nPath Vector (BGP)\nRoute advertisement\n\n\n846\nFlooding\nBroadcast to all nodes\n\n\n847\nSpanning Tree Protocol\nLoop-free topology\n\n\n848\nCongestion Control (AIMD)\nTCP window control\n\n\n849\nRandom Early Detection (RED)\nQueue preemptive drop\n\n\n850\nECN (Explicit Congestion Notification)\nMark packets early\n\n\n\n\n\n86. Distributed Consensus (Paxos, Raft, PBFT)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n851\nBasic Paxos\nMajority consensus\n\n\n852\nMulti-Paxos\nSequence of agreements\n\n\n853\nRaft\nLog replication + leader election\n\n\n854\nViewstamped Replication\nAlternative consensus design\n\n\n855\nPBFT (Practical Byzantine Fault Tolerance)\nByzantine safety\n\n\n856\nZab (Zookeeper Atomic Broadcast)\nBroadcast + ordering\n\n\n857\nEPaxos\nLeaderless fast path\n\n\n858\nVRR (Virtual Ring Replication)\nLog around ring\n\n\n859\nTwo-Phase Commit with Consensus\nTransactional commit\n\n\n860\nChain Replication\nOrdered state replication\n\n\n\n\n\n87. Load Balancing and Rate Limiting\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n861\nRound Robin Load Balancing\nSequential distribution\n\n\n862\nWeighted Round Robin\nProportional to weight\n\n\n863\nLeast Connections\nPick least loaded node\n\n\n864\nConsistent Hashing\nMap requests stably\n\n\n865\nPower of Two Choices\nSample and choose lesser load\n\n\n866\nRandom Load Balancing\nSimple uniform random\n\n\n867\nToken Bucket\nRate-based limiter\n\n\n868\nLeaky Bucket\nSteady flow shaping\n\n\n869\nSliding Window Counter\nRolling time window\n\n\n870\nFixed Window Counter\nResettable counter limiter\n\n\n\n\n\n88. Search and Indexing (Inverted, BM25, WAND)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n871\nInverted Index Construction\nWord → document list\n\n\n872\nPositional Index Build\nStore term positions\n\n\n873\nTF-IDF Scoring\nTerm frequency weighting\n\n\n874\nBM25 Ranking\nModern scoring model\n\n\n875\nBoolean Retrieval\nLogical AND/OR/NOT\n\n\n876\nWAND Algorithm\nEfficient top-k retrieval\n\n\n877\nBlock-Max WAND (BMW)\nEarly skipping optimization\n\n\n878\nImpact-Ordered Indexing\nSort by contribution\n\n\n879\nTiered Indexing\nPrioritize high-score docs\n\n\n880\nDAAT vs SAAT Evaluation\nDocument vs score-at-a-time\n\n\n\n\n\n89. Compression and Encoding in Systems\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n881\nRun-Length Encoding (RLE)\nSimple repetition encoding\n\n\n882\nHuffman Coding\nOptimal variable-length code\n\n\n883\nArithmetic Coding\nFractional interval coding\n\n\n884\nDelta Encoding\nStore differences\n\n\n885\nVariable Byte Encoding\nCompact integers\n\n\n886\nElias Gamma Coding\nPrefix integer encoding\n\n\n887\nRice Coding\nUnary + remainder scheme\n\n\n888\nSnappy\nFast block compression\n\n\n889\nZstandard (Zstd)\nModern adaptive codec\n\n\n890\nLZ4\nHigh-speed dictionary compressor\n\n\n\n\n\n90. Fault Tolerance and Replication\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n891\nPrimary–Backup Replication\nOne leader, one standby\n\n\n892\nQuorum Replication\nMajority write/read rule\n\n\n893\nChain Replication\nOrdered consistency\n\n\n894\nGossip Protocol\nEpidemic state exchange\n\n\n895\nAnti-Entropy Repair\nPeriodic reconciliation\n\n\n896\nErasure Coding\nRedundant data blocks\n\n\n897\nChecksum Verification\nDetect corruption\n\n\n898\nHeartbeat Monitoring\nLiveness detection\n\n\n899\nLeader Election (Bully)\nHighest ID wins\n\n\n900\nLeader Election (Ring)\nToken-based rotation\n\n\n\n\n\n\nChapter 10. AI, ML, and Optimization\n\n91. Classical ML (k-means, Naive Bayes, SVM, Decision Trees)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n901\nk-Means Clustering\nPartition by centroid iteration\n\n\n902\nk-Medoids (PAM)\nCluster by exemplars\n\n\n903\nGaussian Mixture Model (EM)\nSoft probabilistic clustering\n\n\n904\nNaive Bayes Classifier\nProbabilistic feature independence\n\n\n905\nLogistic Regression\nSigmoid linear classifier\n\n\n906\nPerceptron\nOnline linear separator\n\n\n907\nDecision Tree (CART)\nRecursive partition by impurity\n\n\n908\nID3 Algorithm\nInformation gain splitting\n\n\n909\nk-Nearest Neighbors (kNN)\nDistance-based classification\n\n\n910\nLinear Discriminant Analysis (LDA)\nProjection for separation\n\n\n\n\n\n92. Ensemble Methods (Bagging, Boosting, Random Forests)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n911\nBagging\nBootstrap aggregation\n\n\n912\nRandom Forest\nEnsemble of decision trees\n\n\n913\nAdaBoost\nWeighted error correction\n\n\n914\nGradient Boosting\nSequential residual fitting\n\n\n915\nXGBoost\nOptimized gradient boosting\n\n\n916\nLightGBM\nHistogram-based leaf growth\n\n\n917\nCatBoost\nOrdered boosting for categoricals\n\n\n918\nStacking\nMeta-model ensemble\n\n\n919\nVoting Classifier\nMajority aggregation\n\n\n920\nSnapshot Ensemble\nAveraged checkpoints\n\n\n\n\n\n93. Gradient Methods (SGD, Adam, RMSProp)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n921\nGradient Descent\nBatch full-gradient step\n\n\n922\nStochastic Gradient Descent (SGD)\nSample-wise updates\n\n\n923\nMini-Batch SGD\nTradeoff speed and variance\n\n\n924\nMomentum\nAdd velocity to descent\n\n\n925\nNesterov Accelerated Gradient\nLookahead correction\n\n\n926\nAdaGrad\nAdaptive per-parameter rate\n\n\n927\nRMSProp\nExponential moving average\n\n\n928\nAdam\nMomentum + adaptive rate\n\n\n929\nAdamW\nDecoupled weight decay\n\n\n930\nL-BFGS\nLimited-memory quasi-Newton\n\n\n\n\n\n94. Deep Learning (Backpropagation, Dropout, Normalization)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n931\nBackpropagation\nGradient chain rule\n\n\n932\nXavier/He Initialization\nScaled variance init\n\n\n933\nDropout\nRandom neuron deactivation\n\n\n934\nBatch Normalization\nNormalize per batch\n\n\n935\nLayer Normalization\nNormalize per feature\n\n\n936\nGradient Clipping\nPrevent explosion\n\n\n937\nEarly Stopping\nPrevent overfitting\n\n\n938\nWeight Decay\nRegularization via penalty\n\n\n939\nLearning Rate Scheduling\nDynamic LR adjustment\n\n\n940\nResidual Connections\nSkip layer improvement\n\n\n\n\n\n95. Sequence Models (Viterbi, Beam Search, CTC)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n941\nHidden Markov Model (Forward–Backward)\nProbabilistic sequence model\n\n\n942\nViterbi Algorithm\nMost probable path\n\n\n943\nBaum–Welch\nEM training for HMMs\n\n\n944\nBeam Search\nTop-k path exploration\n\n\n945\nGreedy Decoding\nFast approximate decoding\n\n\n946\nConnectionist Temporal Classification (CTC)\nUnaligned sequence training\n\n\n947\nAttention Mechanism\nWeighted context aggregation\n\n\n948\nTransformer Decoder\nSelf-attention stack\n\n\n949\nSeq2Seq with Attention\nEncoder-decoder framework\n\n\n950\nPointer Network\nOutput index selection\n\n\n\n\n\n96. Metaheuristics (GA, SA, PSO, ACO)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n951\nGenetic Algorithm (GA)\nEvolutionary optimization\n\n\n952\nSimulated Annealing (SA)\nTemperature-controlled search\n\n\n953\nTabu Search\nMemory of forbidden moves\n\n\n954\nParticle Swarm Optimization (PSO)\nVelocity-based search\n\n\n955\nAnt Colony Optimization (ACO)\nPheromone-guided path\n\n\n956\nDifferential Evolution (DE)\nVector-based mutation\n\n\n957\nHarmony Search\nMusic-inspired improvisation\n\n\n958\nFirefly Algorithm\nBrightness-attraction movement\n\n\n959\nBee Colony Optimization\nExplore-exploit via scouts\n\n\n960\nHill Climbing\nLocal incremental improvement\n\n\n\n\n\n97. Reinforcement Learning (Q-learning, Policy Gradients)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n961\nMonte Carlo Control\nAverage returns\n\n\n962\nTemporal Difference (TD) Learning\nBootstrap updates\n\n\n963\nSARSA\nOn-policy TD learning\n\n\n964\nQ-Learning\nOff-policy TD learning\n\n\n965\nDouble Q-Learning\nReduce overestimation\n\n\n966\nDeep Q-Network (DQN)\nNeural Q approximator\n\n\n967\nREINFORCE\nPolicy gradient by sampling\n\n\n968\nActor–Critic\nValue-guided policy update\n\n\n969\nPPO (Proximal Policy Optimization)\nClipped surrogate objective\n\n\n970\nDDPG / SAC\nContinuous action RL\n\n\n\n\n\n98. Approximation and Online Algorithms\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n971\nGreedy Set Cover\nln(n)-approximation\n\n\n972\nVertex Cover Approximation\nDouble-matching heuristic\n\n\n973\nTraveling Salesman Approximation\nMST-based 2-approx\n\n\n974\nk-Center Approximation\nFarthest-point heuristic\n\n\n975\nOnline Paging (LRU)\nCompetitive analysis\n\n\n976\nOnline Matching (Ranking)\nAdversarial input resilience\n\n\n977\nOnline Knapsack\nRatio-based acceptance\n\n\n978\nCompetitive Ratio Evaluation\nBound worst-case performance\n\n\n979\nPTAS / FPTAS Schemes\nPolynomial approximation\n\n\n980\nPrimal–Dual Method\nApproximate combinatorial optimization\n\n\n\n\n\n99. Fairness, Causal Inference, and Robust Optimization\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n981\nReweighting for Fairness\nAdjust sample weights\n\n\n982\nDemographic Parity Constraint\nEqualize positive rates\n\n\n983\nEqualized Odds\nAlign error rates\n\n\n984\nAdversarial Debiasing\nLearn fair representations\n\n\n985\nCausal DAG Discovery\nGraphical causal inference\n\n\n986\nPropensity Score Matching\nEstimate treatment effect\n\n\n987\nInstrumental Variable Estimation\nHandle confounders\n\n\n988\nRobust Optimization\nWorst-case aware optimization\n\n\n989\nDistributionally Robust Optimization\nMinimax over uncertainty sets\n\n\n990\nCounterfactual Fairness\nSimulate do-interventions\n\n\n\n\n\n100. AI Planning, Search, and Learning Systems\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n991\nBreadth-First Search (BFS)\nUninformed search\n\n\n992\nDepth-First Search (DFS)\nBacktracking search\n\n\n993\nA* Search\nHeuristic guided\n\n\n994\nIterative Deepening A* (IDA*)\nMemory-bounded heuristic\n\n\n995\nUniform Cost Search\nExpand by path cost\n\n\n996\nMonte Carlo Tree Search (MCTS)\nExploration vs exploitation\n\n\n997\nMinimax\nGame tree evaluation\n\n\n998\nAlpha–Beta Pruning\nPrune unneeded branches\n\n\n999\nSTRIPS Planning\nAction-based state transition\n\n\n1000\nHierarchical Task Network (HTN)\nStructured AI planning",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Plan</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html",
    "href": "books/en-us/list-1.html",
    "title": "Chapter 1. Foundatmemtions of Algorithms",
    "section": "",
    "text": "Section 1. What is an algorithms?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundatmemtions of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-1.-what-is-an-algorithms",
    "href": "books/en-us/list-1.html#section-1.-what-is-an-algorithms",
    "title": "Chapter 1. Foundatmemtions of Algorithms",
    "section": "",
    "text": "1 Euclid’s GCD\nEuclid’s algorithm is one of the oldest and most elegant procedures in mathematics. It computes the greatest common divisor (GCD) of two integers by repeatedly applying a simple rule: replace the larger number with its remainder when divided by the smaller. When the remainder becomes zero, the smaller number at that step is the GCD.\n\nWhat Problem Are We Solving?\nWe want the greatest common divisor of two integers \\(a\\) and \\(b\\): the largest number that divides both without a remainder.\nA naive way would be to check all numbers from \\(\\min(a,b)\\) down to 1. That’s \\(O(\\min(a,b))\\) steps, which is too slow for large inputs. Euclid’s insight gives a much faster recursive method using division:\n\\[\n\\gcd(a, b) =\n\\begin{cases}\na, & \\text{if } b = 0, \\\\\n\\gcd(b, a \\bmod b), & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\nHow It Works (Plain Language)\nImagine two sticks of lengths \\(a\\) and \\(b\\). You can keep cutting the longer stick by the shorter one until one divides evenly. The length of the last nonzero remainder is the GCD.\nSteps:\n\nTake \\(a, b\\) with \\(a \\ge b\\).\nReplace \\(a\\) by \\(b\\), and \\(b\\) by \\(a \\bmod b\\).\nRepeat until \\(b = 0\\).\nReturn \\(a\\).\n\nThis process always terminates, since \\(b\\) strictly decreases each step.\n\n\nExample Step by Step\nFind \\(\\gcd(48, 18)\\):\n\n\n\nStep\n\\(a\\)\n\\(b\\)\n\\(a \\bmod b\\)\n\n\n\n\n1\n48\n18\n12\n\n\n2\n18\n12\n6\n\n\n3\n12\n6\n0\n\n\n\nWhen \\(b = 0\\), \\(a = 6\\). So \\(\\gcd(48, 18) = 6\\).\n\n\nTiny Code (Python)\ndef gcd(a, b):\n    while b != 0:\n        a, b = b, a % b\n    return a\n\nprint(gcd(48, 18))  # Output: 6\n\n\nWhy It Matters\n\nFoundational example of algorithmic thinking\nCore building block in modular arithmetic, number theory, and cryptography\nEfficient: runs in \\(O(\\log \\min(a,b))\\) steps\nEasy to implement iteratively or recursively\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(a = bq + r\\), any common divisor of \\(a\\) and \\(b\\) also divides \\(r\\), since \\(r = a - bq\\). Thus, the set of common divisors of \\((a, b)\\) and \\((b, r)\\) is the same, and their greatest element (the GCD) is unchanged.\nRepeatedly applying this property leads to \\(b = 0\\), where \\(\\gcd(a, 0) = a\\).\n\n\nTry It Yourself\n\nCompute \\(\\gcd(270, 192)\\) step by step.\nImplement the recursive version:\n\n\\[\n\\gcd(a, b) = \\gcd(b,, a \\bmod b)\n\\]\n\nExtend to find \\(\\gcd(a, b, c)\\) using \\(\\gcd(\\gcd(a, b), c)\\).\n\n\n\nTest Cases\n\n\n\nInput \\((a, b)\\)\nExpected Output\n\n\n\n\n(48, 18)\n6\n\n\n(270, 192)\n6\n\n\n(7, 3)\n1\n\n\n(10, 0)\n10\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nGCD\n\\(O(\\log \\min(a,b))\\)\n\\(O(1)\\)\n\n\n\nEuclid’s GCD algorithm is where algorithmic elegance begins, a timeless loop of division that turns mathematics into motion.\n\n\n\n2 Sieve of Eratosthenes\nThe Sieve of Eratosthenes is a classic ancient algorithm for finding all prime numbers up to a given limit. It works by iteratively marking the multiples of each prime, starting from 2. The numbers that remain unmarked at the end are primes.\n\nWhat Problem Are We Solving?\nWe want to find all prime numbers less than or equal to \\(n\\). A naive method checks each number \\(k\\) by testing divisibility from \\(2\\) to \\(\\sqrt{k}\\), which is too slow for large \\(n\\). The sieve improves this by using elimination instead of repeated checking.\nWe aim for an algorithm with time complexity close to \\(O(n \\log \\log n)\\).\n\n\nHow It Works (Plain Language)\n\nCreate a list is_prime[0..n] and mark all as true.\nMark 0 and 1 as non-prime.\nStarting from \\(p = 2\\), if \\(p\\) is still marked prime:\n\nMark all multiples of \\(p\\) (from \\(p^2\\) to \\(n\\)) as non-prime.\n\nIncrement \\(p\\) and repeat until \\(p^2 &gt; n\\).\nAll indices still marked true are primes.\n\nThis process “filters out” composite numbers step by step, just like passing sand through finer and finer sieves.\n\n\nExample Step by Step\nFind all primes up to \\(30\\):\nStart: \\([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\\)\n\n\\(p = 2\\): cross out multiples of 2\n\\(p = 3\\): cross out multiples of 3\n\\(p = 5\\): cross out multiples of 5\n\nRemaining numbers: \\(2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\)\n\n\nTiny Code (Python)\ndef sieve(n):\n    is_prime = [True] * (n + 1)\n    is_prime[0] = is_prime[1] = False\n\n    p = 2\n    while p * p &lt;= n:\n        if is_prime[p]:\n            for i in range(p * p, n + 1, p):\n                is_prime[i] = False\n        p += 1\n\n    return [i for i in range(2, n + 1) if is_prime[i]]\n\nprint(sieve(30))  # [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n\n\nWhy It Matters\n\nOne of the earliest and most efficient ways to generate primes\nForms the basis for number-theoretic algorithms and cryptographic systems\nConceptually simple yet mathematically deep\nDemonstrates elimination instead of brute force\n\n\n\nA Gentle Proof (Why It Works)\nEvery composite number \\(n\\) has a smallest prime divisor \\(p \\le \\sqrt{n}\\). Thus, when we mark multiples of primes up to \\(\\sqrt{n}\\), every composite number is crossed out by its smallest prime factor. Numbers that remain unmarked are prime by definition.\n\n\nTry It Yourself\n\nRun the sieve for \\(n = 50\\) and list primes.\nModify to count primes instead of listing them.\nCompare runtime with naive primality tests for large \\(n\\).\nExtend to a segmented sieve for \\(n &gt; 10^7\\).\n\n\n\nTest Cases\n\n\n\nInput \\(n\\)\nExpected Primes\n\n\n\n\n10\n[2, 3, 5, 7]\n\n\n20\n[2, 3, 5, 7, 11, 13, 17, 19]\n\n\n30\n[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSieve\n\\(O(n \\log \\log n)\\)\n\\(O(n)\\)\n\n\n\nThe Sieve of Eratosthenes turns the search for primes into a graceful pattern of elimination, simple loops revealing the hidden order of numbers.\n\n\n\n3 Linear Step Trace\nA Linear Step Trace is a simple yet powerful visualization tool for understanding how an algorithm progresses line by line. It records each step of execution, showing how variables change over time, helping beginners see the flow of computation.\n\nWhat Problem Are We Solving?\nWhen learning algorithms, it’s easy to lose track of what’s happening after each instruction. A Linear Step Trace helps us see execution in motion, one step, one update at a time.\nInstead of abstract reasoning alone, we follow the exact state changes that occur during the run, making debugging and reasoning far easier.\n\n\nHow It Works (Plain Language)\n\nWrite down your pseudocode or code.\nCreate a table with columns for step number, current line, and variable values.\nEach time a line executes, record the line number and updated variables.\nContinue until the program finishes.\n\nThis method is algorithm-agnostic, it works for loops, recursion, conditionals, and all flow patterns.\n\n\nExample Step by Step\nLet’s trace a simple loop:\nsum = 0\nfor i in 1..4:\n    sum = sum + i\n\n\n\nStep\nLine\ni\nsum\nNote\n\n\n\n\n1\n1\n-\n0\nInitialize sum\n\n\n2\n2\n1\n0\nLoop start\n\n\n3\n3\n1\n1\nsum = 0 + 1\n\n\n4\n2\n2\n1\nNext iteration\n\n\n5\n3\n2\n3\nsum = 1 + 2\n\n\n6\n2\n3\n3\nNext iteration\n\n\n7\n3\n3\n6\nsum = 3 + 3\n\n\n8\n2\n4\n6\nNext iteration\n\n\n9\n3\n4\n10\nsum = 6 + 4\n\n\n10\n-\n-\n10\nEnd\n\n\n\nFinal result: \\(sum = 10\\).\n\n\nTiny Code (Python)\nsum = 0\ntrace = []\n\nfor i in range(1, 5):\n    trace.append((i, sum))\n    sum += i\n\ntrace.append((\"final\", sum))\nprint(trace)\n# [(1, 0), (2, 1), (3, 3), (4, 6), ('final', 10)]\n\n\nWhy It Matters\n\nBuilds step-by-step literacy in algorithm reading\nGreat for teaching loops, conditions, and recursion\nReveals hidden assumptions and logic errors\nIdeal for debugging and analysis\n\n\n\nA Gentle Proof (Why It Works)\nEvery algorithm can be expressed as a sequence of state transitions. If each transition is recorded, we obtain a complete trace of computation. Thus, correctness can be verified by comparing expected vs. actual state sequences. This is equivalent to an inductive proof: each step matches the specification.\n\n\nTry It Yourself\n\nTrace a recursive factorial function step by step.\nAdd a “call stack” column to visualize recursion depth.\nTrace an array-sorting loop and mark swaps.\nCompare traces before and after optimization.\n\n\n\nTest Cases\n\n\n\nProgram\nExpected Final State\n\n\n\n\nsum of 1..4\nsum = 10\n\n\nsum of 1..10\nsum = 55\n\n\nfactorial(5)\nresult = 120\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nTrace Recording\n\\(O(n)\\)\n\\(O(n)\\)\n\n\n\nA Linear Step Trace transforms invisible logic into a visible path, a story of each line’s journey, one state at a time.\n\n\n\n4 Algorithm Flow Diagram Builder\nAn Algorithm Flow Diagram Builder turns abstract pseudocode into a visual map, a diagram of control flow that shows where decisions branch, where loops repeat, and where computations end. It’s the bridge between code and comprehension.\n\nWhat Problem Are We Solving?\nWhen an algorithm becomes complex, it’s easy to lose track of its structure. We may know what each line does, but not how control moves through the program.\nA flow diagram lays out that control structure explicitly, revealing loops, branches, merges, and exits at a glance.\n\n\nHow It Works (Plain Language)\n\nIdentify actions and decisions\n\nActions: assignments, computations\nDecisions: if, while, for, switch\n\nRepresent them with symbols\n\nRectangle → action\nDiamond → decision\nArrow → flow of control\n\nConnect nodes based on what happens next\nLoop back arrows for iterations, and mark exit points\n\nThis yields a graph of control, a shape you can follow from start to finish.\n\n\nExample Step by Step\nLet’s draw the flow for finding the sum of numbers \\(1\\) to \\(n\\):\nPseudocode:\nsum = 0\ni = 1\nwhile i ≤ n:\n    sum = sum + i\n    i = i + 1\nprint(sum)\nFlow Outline:\n\nStart\nInitialize sum = 0, i = 1\nDecision: i ≤ n?\n\nYes → Update sum, Increment i → Loop back\nNo → Print sum → End\n\n\nTextual Diagram:\n  [Start]\n     |\n[sum=0, i=1]\n     |\n  (i ≤ n?) ----No----&gt; [Print sum] -&gt; [End]\n     |\n    Yes\n     |\n [sum = sum + i]\n     |\n [i = i + 1]\n     |\n   (Back to i ≤ n?)\n\n\nTiny Code (Python)\ndef sum_to_n(n):\n    sum = 0\n    i = 1\n    while i &lt;= n:\n        sum += i\n        i += 1\n    return sum\nUse this code to generate flow diagrams automatically with libraries like graphviz or pyflowchart.\n\n\nWhy It Matters\n\nReveals structure at a glance\nMakes debugging easier by visualizing possible paths\nHelps design before coding\nUniversal representation (language-agnostic)\n\n\n\nA Gentle Proof (Why It Works)\nEach algorithm’s execution path can be modeled as a directed graph:\n\nVertices = program states or actions\nEdges = transitions (next step)\n\nA flow diagram is simply this control graph rendered visually. It preserves correctness since each edge corresponds to a valid jump in control flow.\n\n\nTry It Yourself\n\nDraw a flowchart for binary search.\nMark all possible comparison outcomes.\nAdd loopbacks for mid-point updates.\nCompare with recursive version, note structural difference.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nKey Decision Node\nExpected Paths\n\n\n\n\nSum loop\n\\(i \\le n\\)\n2 (continue, exit)\n\n\nBinary search\n\\(key == mid?\\)\n3 (left, right, found)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDiagram Construction\n\\(O(n)\\) nodes\n\\(O(n)\\) edges\n\n\n\nAn Algorithm Flow Diagram is a lens, it turns invisible execution paths into a map you can walk, from “Start” to “End.”\n\n\n\n5 Long Division\nLong Division is a step-by-step algorithm for dividing one integer by another. It’s one of the earliest examples of a systematic computational procedure, showing how large problems can be solved through a sequence of local, repeatable steps.\n\nWhat Problem Are We Solving?\nWe want to compute the quotient and remainder when dividing two integers \\(a\\) (dividend) and \\(b\\) (divisor).\nNaively, repeated subtraction would take \\(O(a/b)\\) steps, far too many for large numbers. Long Division improves this by grouping subtractions by powers of 10, performing digit-wise computation efficiently.\n\n\nHow It Works (Plain Language)\n\nAlign digits of \\(a\\) (the dividend).\nCompare current portion of \\(a\\) to \\(b\\).\nFind the largest multiple of \\(b\\) that fits in the current portion.\nSubtract, write the quotient digit, and bring down the next digit.\nRepeat until all digits have been processed.\nThe digits written form the quotient; what’s left is the remainder.\n\nThis method extends naturally to decimals, just continue bringing down zeros.\n\n\nExample Step by Step\nCompute \\(153 \\div 7\\):\n\n\n\n\n\n\n\n\n\n\nStep\nPortion\nQuotient Digit\nRemainder\nAction\n\n\n\n\n1\n15\n2\n1\n\\(7 \\times 2 = 14\\), subtract \\(15 - 14 = 1\\)\n\n\n2\nBring down 3 → 13\n1\n6\n\\(7 \\times 1 = 7\\), subtract \\(13 - 7 = 6\\)\n\n\n3\nNo more digits\n,\n6\nDone\n\n\n\nResult: Quotient \\(= 21\\), Remainder \\(= 6\\) Check: \\(7 \\times 21 + 6 = 153\\)\n\n\nTiny Code (Python)\ndef long_division(a, b):\n    quotient = 0\n    remainder = 0\n    for digit in str(a):\n        remainder = remainder * 10 + int(digit)\n        q = remainder // b\n        remainder = remainder % b\n        quotient = quotient * 10 + q\n    return quotient, remainder\n\nprint(long_division(153, 7))  # (21, 6)\n\n\nWhy It Matters\n\nIntroduces loop invariants and digit-by-digit reasoning\nFoundation for division in arbitrary-precision arithmetic\nCore to implementing division in CPUs and big integer libraries\nDemonstrates decomposing a large task into simple, local operations\n\n\n\nA Gentle Proof (Why It Works)\nAt each step:\n\nThe current remainder \\(r_i\\) satisfies \\(0 \\le r_i &lt; b\\).\nThe algorithm maintains the invariant: \\[\na = b \\times Q_i + r_i\n\\] where \\(Q_i\\) is the partial quotient so far.\nEach step reduces the unprocessed part of \\(a\\), ensuring termination with correct \\(Q\\) and \\(r\\).\n\n\n\nTry It Yourself\n\nPerform \\(2345 \\div 13\\) by hand.\nVerify with Python’s divmod(2345, 13).\nExtend your code to produce decimal expansions.\nCompare digit-wise trace with manual process.\n\n\n\nTest Cases\n\n\n\nDividend \\(a\\)\nDivisor \\(b\\)\nExpected Output \\((Q, R)\\)\n\n\n\n\n153\n7\n(21, 6)\n\n\n100\n8\n(12, 4)\n\n\n99\n9\n(11, 0)\n\n\n23\n5\n(4, 3)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLong Division\n\\(O(d)\\)\n\\(O(1)\\)\n\n\n\nwhere \\(d\\) is the number of digits in \\(a\\).\nLong Division is more than arithmetic, it’s the first encounter with algorithmic thinking: state, iteration, and correctness unfolding one digit at a time.\n\n\n\n6 Modular Addition\nModular addition is arithmetic on a clock, we add numbers, then wrap around when reaching a fixed limit. It’s the simplest example of modular arithmetic, a system that underlies cryptography, hashing, and cyclic data structures.\n\nWhat Problem Are We Solving?\nWe want to add two integers \\(a\\) and \\(b\\), but keep the result within a fixed modulus \\(m\\). That means we compute the remainder after dividing the sum by \\(m\\).\nFormally, we want:\n\\[\n(a + b) \\bmod m\n\\]\nThis ensures results always lie in the range \\([0, m - 1]\\), regardless of how large \\(a\\) or \\(b\\) become.\n\n\nHow It Works (Plain Language)\n\nCompute the sum \\(s = a + b\\).\nDivide \\(s\\) by \\(m\\) to find the remainder.\nThe remainder is the modular sum.\n\nIf \\(s \\ge m\\), we “wrap around” by subtracting \\(m\\) until it fits in the modular range.\nThis idea is like hours on a clock: \\(10 + 5\\) hours on a \\(12\\)-hour clock → \\(3\\).\n\n\nExample Step by Step\nLet \\(a = 10\\), \\(b = 7\\), \\(m = 12\\).\n\nCompute \\(s = 10 + 7 = 17\\).\n\\(17 \\bmod 12 = 5\\).\nSo \\((10 + 7) \\bmod 12 = 5\\).\n\nCheck: \\(17 - 12 = 5\\), fits in \\([0, 11]\\).\n\n\nTiny Code (Python)\ndef mod_add(a, b, m):\n    return (a + b) % m\n\nprint(mod_add(10, 7, 12))  # 5\n\n\nWhy It Matters\n\nFoundation of modular arithmetic\nUsed in hashing, cyclic buffers, and number theory\nCrucial for secure encryption (RSA, ECC)\nDemonstrates wrap-around logic in bounded systems\n\n\n\nA Gentle Proof (Why It Works)\nBy definition of modulus:\n\\[\nx \\bmod m = r \\quad \\text{such that } x = q \\times m + r,\\ 0 \\le r &lt; m\n\\]\nThus, for \\(a + b = q \\times m + r\\), we have \\((a + b) \\bmod m = r\\). All equivalent sums differ by a multiple of \\(m\\), so modular addition preserves congruence:\n\\[\n(a + b) \\bmod m \\equiv (a \\bmod m + b \\bmod m) \\bmod m\n\\]\n\n\nTry It Yourself\n\nCompute \\((15 + 8) \\bmod 10\\).\nVerify \\((a + b) \\bmod m = ((a \\bmod m) + (b \\bmod m)) \\bmod m\\).\nTest with negative values: \\((−3 + 5) \\bmod 7\\).\nApply to time arithmetic: what is \\(11 + 5\\) on a \\(12\\)-hour clock?\n\n\n\nTest Cases\n\n\n\n\\(a\\)\n\\(b\\)\n\\(m\\)\nResult\n\n\n\n\n10\n7\n12\n5\n\n\n5\n5\n10\n0\n\n\n8\n15\n10\n3\n\n\n11\n5\n12\n4\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nModular Addition\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nModular addition teaches the rhythm of modular arithmetic, every sum wraps back into harmony, always staying within its finite world.\n\n\n\n7 Base Conversion\nBase conversion is the algorithmic process of expressing a number in a different numeral system. It’s how we translate between decimal, binary, octal, hexadecimal, or any base, the language of computers and mathematics alike.\n\nWhat Problem Are We Solving?\nWe want to represent an integer \\(n\\) in base \\(b\\). In base 10, digits go from 0 to 9. In base 2, only 0 and 1. In base 16, digits are \\(0 \\ldots 9\\) and \\(A \\ldots F\\).\nThe goal is to find a sequence of digits \\(d_k d_{k-1} \\ldots d_0\\) such that:\n\\[\nn = \\sum_{i=0}^{k} d_i \\cdot b^i\n\\]\nwhere \\(0 \\le d_i &lt; b\\).\n\n\nHow It Works (Plain Language)\n\nStart with the integer \\(n\\).\nRepeatedly divide \\(n\\) by \\(b\\).\nRecord the remainder each time (these are the digits).\nStop when \\(n = 0\\).\nThe base-\\(b\\) representation is the remainders read in reverse order.\n\nThis works because division extracts digits starting from the least significant position.\n\n\nExample Step by Step\nConvert \\(45\\) to binary (\\(b = 2\\)):\n\n\n\nStep\n\\(n\\)\n\\(n \\div 2\\)\nRemainder\n\n\n\n\n1\n45\n22\n1\n\n\n2\n22\n11\n0\n\n\n3\n11\n5\n1\n\n\n4\n5\n2\n1\n\n\n5\n2\n1\n0\n\n\n6\n1\n0\n1\n\n\n\nRead remainders upward: 101101\nSo \\(45_{10} = 101101_2\\).\nCheck: \\(1 \\cdot 2^5 + 0 \\cdot 2^4 + 1 \\cdot 2^3 + 1 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0 = 32 + 0 + 8 + 4 + 0 + 1 = 45\\) ✅\n\n\nTiny Code (Python)\ndef to_base(n, b):\n    digits = []\n    while n &gt; 0:\n        digits.append(n % b)\n        n //= b\n    return digits[::-1] or [0]\n\nprint(to_base(45, 2))  # [1, 0, 1, 1, 0, 1]\n\n\nWhy It Matters\n\nConverts numbers between human and machine representations\nCore in encoding, compression, and cryptography\nBuilds intuition for positional number systems\nUsed in parsing, serialization, and digital circuits\n\n\n\nA Gentle Proof (Why It Works)\nEach division step produces one digit \\(r_i = n_i \\bmod b\\). We have:\n\\[\nn_i = b \\cdot n_{i+1} + r_i\n\\]\nUnfolding the recurrence gives:\n\\[\nn = \\sum_{i=0}^{k} r_i b^i\n\\]\nSo collecting remainders in reverse order reconstructs \\(n\\) exactly.\n\n\nTry It Yourself\n\nConvert \\(100_{10}\\) to base 8.\nConvert \\(255_{10}\\) to base 16.\nVerify by recombining digits via \\(\\sum d_i b^i\\).\nWrite a reverse converter: base-\\(b\\) to decimal.\n\n\n\nTest Cases\n\n\n\nDecimal \\(n\\)\nBase \\(b\\)\nRepresentation\n\n\n\n\n45\n2\n101101\n\n\n100\n8\n144\n\n\n255\n16\nFF\n\n\n31\n5\n111\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBase Conversion\n\\(O(\\log_b n)\\)\n\\(O(\\log_b n)\\)\n\n\n\nBase conversion is arithmetic storytelling, peeling away remainders until only digits remain, revealing the same number through a different lens.\n\n\n\n8 Factorial Computation\nFactorial computation is the algorithmic act of multiplying a sequence of consecutive integers, a simple rule that grows explosively. It lies at the foundation of combinatorics, probability, and mathematical analysis.\n\nWhat Problem Are We Solving?\nWe want to compute the factorial of a non-negative integer \\(n\\), written \\(n!\\), defined as:\n\\[\nn! = n \\times (n - 1) \\times (n - 2) \\times \\cdots \\times 1\n\\]\nwith the base case:\n\\[\n0! = 1\n\\]\nFactorial counts the number of ways to arrange \\(n\\) distinct objects, the building block of permutations and combinations.\n\n\nHow It Works (Plain Language)\nThere are two main ways:\nIterative:\n\nStart with result = 1\nMultiply by each \\(i\\) from 1 to \\(n\\)\nReturn result\n\nRecursive:\n\n\\(n! = n \\times (n - 1)!\\)\nStop when \\(n = 0\\)\n\nBoth methods produce the same result; recursion mirrors the mathematical definition, iteration avoids call overhead.\n\n\nExample Step by Step\nCompute \\(5!\\):\n\n\n\nStep\n\\(n\\)\nProduct\n\n\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n3\n6\n\n\n4\n4\n24\n\n\n5\n5\n120\n\n\n\nSo \\(5! = 120\\) ✅\n\n\nTiny Code (Python)\nIterative Version\ndef factorial_iter(n):\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nprint(factorial_iter(5))  # 120\nRecursive Version\ndef factorial_rec(n):\n    if n == 0:\n        return 1\n    return n * factorial_rec(n - 1)\n\nprint(factorial_rec(5))  # 120\n\n\nWhy It Matters\n\nCore operation in combinatorics, calculus, and probability\nDemonstrates recursion, iteration, and induction\nGrows rapidly, useful for testing overflow and asymptotics\nAppears in binomial coefficients, Taylor series, and permutations\n\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\(n! = n \\times (n - 1)!\\). Assume \\((n - 1)!\\) is correctly computed. Then multiplying by \\(n\\) yields \\(n!\\).\nBy induction:\n\nBase case: \\(0! = 1\\)\nStep: if \\((n - 1)!\\) is correct, so is \\(n!\\)\n\nThus, the recursive and iterative definitions are equivalent and correct.\n\n\nTry It Yourself\n\nCompute \\(6!\\) both iteratively and recursively.\nPrint intermediate products to trace the growth.\nCompare runtime for \\(n = 1000\\) using both methods.\nExplore factorial in floating point (math.gamma) for non-integers.\n\n\n\nTest Cases\n\n\n\nInput \\(n\\)\nExpected Output \\(n!\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n3\n6\n\n\n5\n120\n\n\n6\n720\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nIterative\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nRecursive\n\\(O(n)\\)\n\\(O(n)\\) (stack)\n\n\n\nFactorial computation is where simplicity meets infinity, a single rule that scales from 1 to astronomical numbers with graceful inevitability.\n\n\n\n9 Iterative Process Tracer\nAn Iterative Process Tracer is a diagnostic algorithm that follows each iteration of a loop, recording variable states, conditions, and updates. It helps visualize the evolution of a program’s internal state, turning looping logic into a clear timeline.\n\nWhat Problem Are We Solving?\nWhen writing iterative algorithms, it’s easy to lose sight of what happens at each step. Are variables updating correctly? Are loop conditions behaving as expected? A tracer captures this process, step by step, so we can verify correctness, find bugs, and teach iteration with clarity.\n\n\nHow It Works (Plain Language)\n\nIdentify the loop (for or while).\nBefore or after each iteration, record:\n\nThe iteration number\nKey variable values\nCondition evaluations\n\nStore these snapshots in a trace table.\nAfter execution, review how values evolve over time.\n\nThink of it as an “execution diary”, every iteration gets a journal entry.\n\n\nExample Step by Step\nLet’s trace a simple accumulation:\nsum = 0\nfor i in 1..5:\n    sum = sum + i\n\n\n\nStep\n\\(i\\)\n\\(sum\\)\nDescription\n\n\n\n\n1\n1\n1\nAdd first number\n\n\n2\n2\n3\nAdd second number\n\n\n3\n3\n6\nAdd third number\n\n\n4\n4\n10\nAdd fourth number\n\n\n5\n5\n15\nAdd fifth number\n\n\n\nFinal result: \\(sum = 15\\)\n\n\nTiny Code (Python)\ndef trace_sum(n):\n    sum = 0\n    trace = []\n    for i in range(1, n + 1):\n        sum += i\n        trace.append((i, sum))\n    return trace\n\nprint(trace_sum(5))\n# [(1, 1), (2, 3), (3, 6), (4, 10), (5, 15)]\n\n\nWhy It Matters\n\nTurns hidden state changes into visible data\nIdeal for debugging loops and verifying invariants\nSupports algorithm teaching and step-by-step reasoning\nUseful in profiling, logging, and unit testing\n\n\n\nA Gentle Proof (Why It Works)\nAn iterative algorithm is a sequence of deterministic transitions:\n\\[\nS_{i+1} = f(S_i)\n\\]\nRecording \\(S_i\\) at each iteration yields the complete trajectory of execution. The trace table captures all intermediate states, ensuring reproducibility and clarity, a form of operational proof.\n\n\nTry It Yourself\n\nTrace variable updates in a multiplication loop.\nAdd condition checks (e.g. early exits).\nRecord both pre- and post-update states.\nCompare traces of iterative vs recursive versions.\n\n\n\nTest Cases\n\n\n\nInput \\(n\\)\nExpected Trace\n\n\n\n\n3\n[(1, 1), (2, 3), (3, 6)]\n\n\n4\n[(1, 1), (2, 3), (3, 6), (4, 10)]\n\n\n5\n[(1, 1), (2, 3), (3, 6), (4, 10), (5, 15)]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nTracing\n\\(O(n)\\)\n\\(O(n)\\)\n\n\n\nAn Iterative Process Tracer makes thinking visible, a loop’s internal rhythm laid out, step by step, until the final note resolves.\n\n\n\n10 Tower of Hanoi\nThe Tower of Hanoi is a legendary recursive puzzle that beautifully illustrates how complex problems can be solved through simple repeated structure. It’s a timeless example of divide and conquer thinking in its purest form.\n\nWhat Problem Are We Solving?\nWe want to move \\(n\\) disks from a source peg to a target peg, using one auxiliary peg. Rules:\n\nMove only one disk at a time.\nNever place a larger disk on top of a smaller one.\n\nThe challenge is to find the minimal sequence of moves that achieves this.\n\n\nHow It Works (Plain Language)\nThe key insight: To move \\(n\\) disks, first move \\(n-1\\) disks aside, move the largest one, then bring the smaller ones back.\nSteps:\n\nMove \\(n-1\\) disks from source → auxiliary\nMove the largest disk from source → target\nMove \\(n-1\\) disks from auxiliary → target\n\nThis recursive structure repeats until the smallest disk moves directly.\n\n\nExample Step by Step\nFor \\(n = 3\\), pegs: A (source), B (auxiliary), C (target)\n\n\n\nStep\nMove\n\n\n\n\n1\nA → C\n\n\n2\nA → B\n\n\n3\nC → B\n\n\n4\nA → C\n\n\n5\nB → A\n\n\n6\nB → C\n\n\n7\nA → C\n\n\n\nTotal moves: \\(2^3 - 1 = 7\\)\n\n\nTiny Code (Python)\ndef hanoi(n, source, target, aux):\n    if n == 1:\n        print(f\"{source} → {target}\")\n        return\n    hanoi(n - 1, source, aux, target)\n    print(f\"{source} → {target}\")\n    hanoi(n - 1, aux, target, source)\n\nhanoi(3, 'A', 'C', 'B')\n\n\nWhy It Matters\n\nClassic recursive pattern: break → solve → combine\nDemonstrates exponential growth (\\(2^n - 1\\) moves)\nTrains recursive reasoning and stack visualization\nAppears in algorithm analysis, recursion trees, and combinatorics\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be the number of moves for \\(n\\) disks. We must move \\(n-1\\) disks twice and one largest disk once:\n\\[\nT(n) = 2T(n-1) + 1, \\quad T(1) = 1\n\\]\nSolving the recurrence:\n\\[\nT(n) = 2^n - 1\n\\]\nEach recursive step preserves rules and reduces the problem size, ensuring correctness by structural induction.\n\n\nTry It Yourself\n\nTrace \\(n = 2\\) and \\(n = 3\\) by hand.\nCount recursive calls.\nModify code to record moves in a list.\nExtend to display peg states after each move.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected Moves\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n7\n\n\n4\n15\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nMoves\n\\(O(2^n)\\)\n\\(O(n)\\) (recursion stack)\n\n\n\nThe Tower of Hanoi turns recursion into art, every move guided by symmetry, every step revealing how simplicity builds complexity one disk at a time.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundatmemtions of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-2.-measuring-time-and-space",
    "href": "books/en-us/list-1.html#section-2.-measuring-time-and-space",
    "title": "Chapter 1. Foundatmemtions of Algorithms",
    "section": "Section 2. Measuring time and space",
    "text": "Section 2. Measuring time and space\n\n11 Counting Operations\nCounting operations is the first step toward understanding time complexity. It’s the art of translating code into math by measuring how many basic steps an algorithm performs, helping us predict performance before running it.\n\nWhat Problem Are We Solving?\nWe want to estimate how long an algorithm takes, not by clock time, but by how many fundamental operations it executes. Instead of relying on hardware speed, we count abstract steps, comparisons, assignments, additions, each treated as one unit of work.\nThis turns algorithms into analyzable formulas.\n\n\nHow It Works (Plain Language)\n\nIdentify the unit step (like one comparison or addition).\nBreak the algorithm into lines or loops.\nCount repetitions for each operation.\nSum all counts to get a total step function \\(T(n)\\).\nSimplify to dominant terms for asymptotic analysis.\n\nWe’re not measuring seconds, we’re measuring structure.\n\n\nExample Step by Step\nCount operations for:\nsum = 0\nfor i in range(1, n + 1):\n    sum += i\nBreakdown:\n\n\n\nLine\nOperation\nCount\n\n\n\n\n1\nInitialization\n1\n\n\n2\nLoop comparison\n\\(n + 1\\)\n\n\n3\nAddition + assignment\n\\(n\\)\n\n\n\nTotal: \\[\nT(n) = 1 + (n + 1) + n = 2n + 2\n\\]\nAsymptotically: \\[\nT(n) = O(n)\n\\]\n\n\nTiny Code (Python)\ndef count_sum_ops(n):\n    ops = 0\n    ops += 1  # init sum\n    for i in range(1, n + 1):\n        ops += 1  # loop check\n        ops += 1  # sum += i\n    ops += 1  # final loop check\n    return ops\nTest: count_sum_ops(5) → 13\n\n\nWhy It Matters\n\nBuilds intuition for algorithm growth\nReveals hidden costs (nested loops, recursion)\nFoundation for Big-O and runtime proofs\nLanguage-agnostic: works for any pseudocode\n\n\n\nA Gentle Proof (Why It Works)\nEvery program can be modeled as a finite sequence of operations parameterized by input size \\(n\\). If \\(f(n)\\) counts these operations exactly, then for large \\(n\\), growth rate \\(\\Theta(f(n))\\) matches actual performance up to constant factors. Counting operations therefore predicts asymptotic runtime behavior.\n\n\nTry It Yourself\n\nCount operations in a nested loop:\nfor i in range(n):\n    for j in range(n):\n        x += 1\nDerive \\(T(n) = n^2 + 2n + 1\\).\nSimplify to \\(O(n^2)\\).\nCompare iterative vs recursive counting.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nStep Function\nBig-O\n\n\n\n\nLinear Loop\n\\(2n + 2\\)\n\\(O(n)\\)\n\n\nNested Loop\n\\(n^2 + 2n + 1\\)\n\\(O(n^2)\\)\n\n\nConstant Work\n\\(c\\)\n\\(O(1)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nCounting Steps\n\\(O(1)\\) (analysis)\n\\(O(1)\\)\n\n\n\nCounting operations transforms code into mathematics, a microscope for understanding how loops, branches, and recursion scale with input size.\n\n\n\n12 Loop Analysis\nLoop analysis is the key to unlocking how algorithms grow, it tells us how many times a loop runs and, therefore, how many operations are performed. Every time you see a loop, you’re looking at a formula in disguise.\n\nWhat Problem Are We Solving?\nWe want to determine how many iterations a loop executes as a function of input size \\(n\\). This helps us estimate total runtime before measuring it empirically.\nWhether a loop is linear, nested, logarithmic, or mixed, understanding its iteration count reveals the algorithm’s true complexity.\n\n\nHow It Works (Plain Language)\n\nIdentify the loop variable (like i in for i in range(...)).\nFind its update rule, additive (i += 1) or multiplicative (i *= 2).\nSolve for how many times the condition holds true.\nMultiply by inner loop work if nested.\nSum all contributions from independent loops.\n\nThis transforms loops into algebraic expressions you can reason about.\n\n\nExample Step by Step\nExample 1: Linear Loop\nfor i in range(1, n + 1):\n    work()\n\\(i\\) runs from \\(1\\) to \\(n\\), incrementing by \\(1\\). Iterations: \\(n\\) Work: \\(O(n)\\)\nExample 2: Logarithmic Loop\ni = 1\nwhile i &lt;= n:\n    work()\n    i *= 2\n\\(i\\) doubles each step: \\(1, 2, 4, 8, \\dots, n\\) Iterations: \\(\\log_2 n + 1\\) Work: \\(O(\\log n)\\)\nExample 3: Nested Loop\nfor i in range(n):\n    for j in range(n):\n        work()\nOuter loop: \\(n\\) Inner loop: \\(n\\) Total work: \\(n \\times n = n^2\\)\n\n\nTiny Code (Python)\ndef linear_loop(n):\n    count = 0\n    for i in range(n):\n        count += 1\n    return count  # n\n\ndef log_loop(n):\n    count = 0\n    i = 1\n    while i &lt;= n:\n        count += 1\n        i *= 2\n    return count  # ≈ log2(n)\n\n\nWhy It Matters\n\nReveals complexity hidden inside loops\nCore tool for deriving \\(O(n)\\), \\(O(\\log n)\\), and \\(O(n^2)\\)\nMakes asymptotic behavior predictable and measurable\nWorks for for-loops, while-loops, and nested structures\n\n\n\nA Gentle Proof (Why It Works)\nEach loop iteration corresponds to a true condition in its guard. If the loop variable \\(i\\) evolves monotonically (by addition or multiplication), the total number of iterations is the smallest \\(k\\) satisfying the exit condition.\nFor additive updates: \\[\ni_0 + k \\cdot \\Delta \\ge n \\implies k = \\frac{n - i_0}{\\Delta}\n\\]\nFor multiplicative updates: \\[\ni_0 \\cdot r^k \\ge n \\implies k = \\log_r \\frac{n}{i_0}\n\\]\n\n\nTry It Yourself\n\nAnalyze loop:\ni = n\nwhile i &gt; 0:\n    i //= 2\n→ \\(O(\\log n)\\)\nAnalyze double loop:\nfor i in range(n):\n    for j in range(i):\n        work()\n→ \\(\\frac{n(n-1)}{2} = O(n^2)\\)\nCombine additive + multiplicative loops.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nCode Pattern\nIterations\nComplexity\n\n\n\n\nfor i in range(n)\n\\(n\\)\n\\(O(n)\\)\n\n\nwhile i &lt; n: i *= 2\n\\(\\log_2 n\\)\n\\(O(\\log n)\\)\n\n\nfor i in range(n): for j in range(n)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\nfor i in range(n): for j in range(i)\n\\(\\frac{n(n-1)}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLoop Analysis\n\\(O(1)\\) (per loop)\n\\(O(1)\\)\n\n\n\nLoop analysis turns repetition into arithmetic, every iteration becomes a term, every loop a story in the language of growth.\n\n\n\n13 Recurrence Expansion\nRecurrence expansion is how we unfold recursive definitions to see their true cost. Many recursive algorithms (like Merge Sort or Quick Sort) define runtime in terms of smaller copies of themselves. By expanding the recurrence, we reveal the total work step by step.\n\nWhat Problem Are We Solving?\nRecursive algorithms often express their runtime as:\n\\[\nT(n) = a \\cdot T!\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nHere:\n\n\\(a\\) = number of recursive calls\n\\(b\\) = factor by which input size is reduced\n\\(f(n)\\) = work done outside recursion (splitting, merging, etc.)\n\nWe want to estimate \\(T(n)\\) by expanding this relation until the base case.\n\n\nHow It Works (Plain Language)\nThink of recurrence expansion as peeling an onion. Each recursive layer contributes some cost, and we add all layers until the base.\nSteps:\n\nWrite the recurrence.\nExpand one level: replace \\(T(\\cdot)\\) with its formula.\nRepeat until the argument becomes the base case.\nSum the work done at each level.\nSimplify the sum to get asymptotic form.\n\n\n\nExample Step by Step\nTake Merge Sort:\n\\[\nT(n) = 2T!\\left(\\frac{n}{2}\\right) + n\n\\]\nExpand:\n\nLevel 0: \\(T(n) = 2T(n/2) + n\\)\nLevel 1: \\(T(n/2) = 2T(n/4) + n/2\\) → Substitute \\(T(n) = 4T(n/4) + 2n\\)\nLevel 2: \\(T(n) = 8T(n/8) + 3n\\)\n…\nLevel \\(\\log_2 n\\): \\(T(1) = c\\)\n\nSum work across levels:\n\\[\nT(n) = n \\log_2 n + n = O(n \\log n)\n\\]\n\n\nTiny Code (Python)\ndef recurrence_expand(a, b, f, n, base=1):\n    level = 0\n    total = 0\n    size = n\n    while size &gt;= base:\n        cost = (a  level) * f(size)\n        total += cost\n        size //= b\n        level += 1\n    return total\nUse f = lambda x: x for Merge Sort.\n\n\nWhy It Matters\n\nCore tool for analyzing recursive algorithms\nBuilds intuition before applying the Master Theorem\nTurns abstract recurrence into tangible pattern\nHelps visualize total work per recursion level\n\n\n\nA Gentle Proof (Why It Works)\nAt level \\(i\\):\n\nThere are \\(a^i\\) subproblems.\nEach subproblem has size \\(\\frac{n}{b^i}\\).\nWork per level: \\(a^i \\cdot f!\\left(\\frac{n}{b^i}\\right)\\)\n\nTotal cost:\n\\[\nT(n) = \\sum_{i=0}^{\\log_b n} a^i f!\\left(\\frac{n}{b^i}\\right)\n\\]\nDepending on how \\(f(n)\\) compares to \\(n^{\\log_b a}\\), either top, bottom, or middle levels dominate.\n\n\nTry It Yourself\n\nExpand \\(T(n) = 3T(n/2) + n^2\\).\nExpand \\(T(n) = T(n/2) + 1\\).\nVisualize total work per level.\nCheck your result with Master Theorem.\n\n\n\nTest Cases\n\n\n\nRecurrence\nExpansion Result\nComplexity\n\n\n\n\n\\(T(n) = 2T(n/2) + n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(T(n) = T(n/2) + 1\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n) = 4T(n/2) + n\\)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nExpansion\n\\(O(\\log n)\\) levels\n\\(O(\\log n)\\) tree depth\n\n\n\nRecurrence expansion turns recursion into rhythm, each level adding its verse, the sum revealing the melody of the algorithm’s growth.\n\n\n\n14 Amortized Analysis\nAmortized analysis looks beyond the worst case of individual operations to capture the average cost per operation over a long sequence. It tells us when “expensive” actions even out, revealing algorithms that are faster than they first appear.\n\nWhat Problem Are We Solving?\nSome operations occasionally take a long time (like resizing an array), but most are cheap. A naive worst-case analysis exaggerates total cost. Amortized analysis finds the true average cost across a sequence.\nWe’re not averaging across inputs, but across operations in one run.\n\n\nHow It Works (Plain Language)\nSuppose an operation is usually \\(O(1)\\), but sometimes \\(O(n)\\). If that expensive case happens rarely enough, the average per operation is still small.\nThree main methods:\n\nAggregate method, total cost ÷ number of operations\nAccounting method, charge extra for cheap ops, save credit for costly ones\nPotential method, define potential energy (stored work) and track change\n\n\n\nExample Step by Step\nDynamic Array Resizing\nWhen an array is full, double its size and copy elements.\n\n\n\nOperation\nCost\nComment\n\n\n\n\nInsert #1–#1\n1\ninsert directly\n\n\nInsert #2\n2\nresize to 2, copy 1\n\n\nInsert #3\n3\nresize to 4, copy 2\n\n\nInsert #5\n5\nresize to 8, copy 4\n\n\n…\n…\n…\n\n\n\nTotal cost after \\(n\\) inserts ≈ \\(2n\\) Average cost = \\(2n / n = O(1)\\) So each insert is amortized \\(O(1)\\), not \\(O(n)\\).\n\n\nTiny Code (Python)\ndef dynamic_array(n):\n    arr = []\n    capacity = 1\n    cost = 0\n    for i in range(n):\n        if len(arr) == capacity:\n            capacity *= 2\n            cost += len(arr)  # copying cost\n        arr.append(i)\n        cost += 1  # insert cost\n    return cost, cost / n  # total, amortized average\nTry dynamic_array(10) → roughly total cost ≈ 20, average ≈ 2.\n\n\nWhy It Matters\n\nShows average efficiency over sequences\nKey to analyzing stacks, queues, hash tables, and dynamic arrays\nExplains why “occasionally expensive” operations are still efficient overall\nSeparates perception (worst-case) from reality (aggregate behavior)\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(C_i\\) = cost of \\(i\\)th operation, and \\(n\\) = total operations.\nAggregate Method:\n\\[\n\\text{Amortized cost} = \\frac{\\sum_{i=1}^n C_i}{n}\n\\]\nIf \\(\\sum C_i = O(n)\\), each operation’s average = \\(O(1)\\).\nPotential Method:\nDefine potential \\(\\Phi_i\\) representing saved work. Amortized cost = \\(C_i + \\Phi_i - \\Phi_{i-1}\\) Summing over all operations telescopes potential away, leaving total cost bounded by initial + final potential.\n\n\nTry It Yourself\n\nAnalyze amortized cost for stack with occasional full pop.\nUse accounting method to assign “credits” to inserts.\nShow \\(O(1)\\) amortized insert in hash table with resizing.\nCompare amortized vs worst-case time.\n\n\n\nTest Cases\n\n\n\nOperation Type\nWorst Case\nAmortized\n\n\n\n\nArray Insert (Doubling)\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nStack Push\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nQueue Dequeue (2-stack)\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nUnion-Find (Path Compression)\n\\(O(\\log n)\\)\n\\(O(\\alpha(n))\\)\n\n\n\n\n\nComplexity\n\n\n\nAnalysis Type\nFormula\nGoal\n\n\n\n\nAggregate\n\\(\\frac{\\text{Total Cost}}{n}\\)\nSimplicity\n\n\nAccounting\nAssign credits\nIntuition\n\n\nPotential\n\\(\\Delta \\Phi\\)\nFormal rigor\n\n\n\nAmortized analysis reveals the calm beneath chaos — a few storms don’t define the weather, and one \\(O(n)\\) moment doesn’t ruin \\(O(1)\\) harmony.\n\n\n\n15 Space Counting\nSpace counting is the spatial twin of operation counting, instead of measuring time, we measure how much memory an algorithm consumes. Every variable, array, stack frame, or temporary buffer adds to the footprint. Understanding it helps us write programs that fit in memory and scale gracefully.\n\nWhat Problem Are We Solving?\nWe want to estimate the space complexity of an algorithm — how much memory it needs as input size \\(n\\) grows.\nThis includes:\n\nStatic space (fixed variables)\nDynamic space (arrays, recursion, data structures)\nAuxiliary space (extra working memory beyond input)\n\nOur goal: express total memory as a function \\(S(n)\\).\n\n\nHow It Works (Plain Language)\n\nCount primitive variables (constants, counters, pointers). → constant space \\(O(1)\\)\nAdd data structure sizes (arrays, lists, matrices). → often proportional to \\(n\\), \\(n^2\\), etc.\nAdd recursion stack depth, if applicable.\nIgnore constants for asymptotic space, focus on growth.\n\nIn the end, \\[\nS(n) = S_{\\text{static}} + S_{\\text{dynamic}} + S_{\\text{recursive}}\n\\]\n\n\nExample Step by Step\nExample 1: Linear Array\narr = [0] * n\n\n\\(n\\) integers → \\(O(n)\\) space\n\nExample 2: 2D Matrix\nmatrix = [[0] * n for _ in range(n)]\n\n\\(n \\times n\\) elements → \\(O(n^2)\\) space\n\nExample 3: Recursive Factorial\ndef fact(n):\n    if n == 0:\n        return 1\n    return n * fact(n - 1)\n\nDepth = \\(n\\) → Stack = \\(O(n)\\)\nNo extra data structures → \\(S(n) = O(n)\\)\n\n\n\nTiny Code (Python)\ndef space_counter(n):\n    const = 1             # O(1)\n    arr = [0] * n         # O(n)\n    matrix = [[0]*n for _ in range(n)]  # O(n^2)\n    return const + len(arr) + len(matrix)\nThis simple example illustrates additive contributions.\n\n\nWhy It Matters\n\nMemory is a first-class constraint in large systems\nCritical for embedded, streaming, and real-time algorithms\nReveals tradeoffs between time and space\nGuides design of in-place vs out-of-place solutions\n\n\n\nA Gentle Proof (Why It Works)\nEach algorithm manipulates a finite set of data elements. If \\(s_i\\) is the space allocated for structure \\(i\\), total space is:\n\\[\nS(n) = \\sum_i s_i(n)\n\\]\nAsymptotic space is dominated by the largest term, so \\(S(n) = \\Theta(\\max_i s_i(n))\\).\nThis ensures our analysis scales with data growth.\n\n\nTry It Yourself\n\nCount space for Merge Sort (temporary arrays).\nCompare with Quick Sort (in-place).\nAdd recursion cost explicitly.\nAnalyze time–space tradeoff for dynamic programming.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nSpace\nReason\n\n\n\n\nLinear Search\n\\(O(1)\\)\nConstant extra memory\n\n\nMerge Sort\n\\(O(n)\\)\nExtra array for merging\n\n\nQuick Sort\n\\(O(\\log n)\\)\nStack depth\n\n\nDP Table (2D)\n\\(O(n^2)\\)\nFull grid of states\n\n\n\n\n\nComplexity\n\n\n\nComponent\nExample\nCost\n\n\n\n\nVariables\n\\(a, b, c\\)\n\\(O(1)\\)\n\n\nArrays\narr[n]\n\\(O(n)\\)\n\n\nMatrices\nmatrix[n][n]\n\\(O(n^2)\\)\n\n\nRecursion Stack\nDepth \\(n\\)\n\\(O(n)\\)\n\n\n\nSpace counting turns memory into a measurable quantity, every variable a footprint, every structure a surface, every stack frame a layer in the architecture of an algorithm.\n\n\n\n16 Memory Footprint Estimator\nA Memory Footprint Estimator calculates how much memory an algorithm or data structure truly consumes, not just asymptotically, but in real bytes. It bridges the gap between theoretical space complexity and practical implementation.\n\nWhat Problem Are We Solving?\nKnowing an algorithm is \\(O(n)\\) in space isn’t enough when working close to memory limits. We need actual estimates: how many bytes per element, how much total allocation, and what overheads exist.\nA footprint estimator converts theoretical counts into quantitative estimates for real-world scaling.\n\n\nHow It Works (Plain Language)\n\nIdentify data types used: int, float, pointer, struct, etc.\nEstimate size per element (language dependent, e.g. int = 4 bytes).\nMultiply by count to find total memory usage.\nInclude overheads from:\n\nObject headers or metadata\nPadding or alignment\nPointers or references\n\n\nFinal footprint: \\[\n\\text{Memory} = \\sum_i (\\text{count}_i \\times \\text{size}_i) + \\text{overhead}\n\\]\n\n\nExample Step by Step\nSuppose we have a list of \\(n = 1{,}000{,}000\\) integers in Python.\n\n\n\nComponent\nSize (Bytes)\nCount\nTotal\n\n\n\n\nList object\n64\n1\n64\n\n\nPointers\n8\n1,000,000\n8,000,000\n\n\nInteger objects\n28\n1,000,000\n28,000,000\n\n\n\nTotal ≈ 36 MB (plus interpreter overhead).\nIf using a fixed array('i') (C-style ints): \\(4 \\text{ bytes} \\times 10^6 = 4\\) MB, far more memory-efficient.\n\n\nTiny Code (Python)\nimport sys\n\nn = 1_000_000\narr_list = list(range(n))\narr_array = bytearray(n * 4)\n\nprint(sys.getsizeof(arr_list))   # list object\nprint(sys.getsizeof(arr_array))  # raw byte array\nCompare memory cost using sys.getsizeof().\n\n\nWhy It Matters\n\nReveals true memory requirements\nCritical for large datasets, embedded systems, and databases\nExplains performance tradeoffs in languages with object overhead\nSupports system design and capacity planning\n\n\n\nA Gentle Proof (Why It Works)\nEach variable or element consumes a fixed number of bytes depending on type. If \\(n_i\\) elements of type \\(t_i\\) are allocated, total memory is:\n\\[\nM(n) = \\sum_i n_i \\cdot s(t_i)\n\\]\nSince \\(s(t_i)\\) is constant, growth rate follows counts: \\(M(n) = O(\\max_i n_i)\\), matching asymptotic analysis while giving concrete magnitudes.\n\n\nTry It Yourself\n\nEstimate memory for a matrix of \\(1000 \\times 1000\\) floats (8 bytes each).\nCompare Python list of lists vs NumPy array.\nAdd overheads for pointers and headers.\nRepeat for custom struct or class with multiple fields.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nStructure\nFormula\nApprox Memory\n\n\n\n\nList of \\(n\\) ints\n\\(n \\times 28\\) B\n28 MB (1M items)\n\n\nArray of \\(n\\) ints\n\\(n \\times 4\\) B\n4 MB\n\n\nMatrix \\(n \\times n\\) floats\n\\(8n^2\\) B\n8 MB for \\(n=1000\\)\n\n\nHash Table \\(n\\) entries\n\\(O(n)\\)\nDepends on load factor\n\n\n\n\n\nComplexity\n\n\n\nMetric\nGrowth\nUnit\n\n\n\n\nSpace\n\\(O(n)\\)\nBytes\n\n\nOverhead\n\\(O(1)\\)\nMetadata\n\n\n\nA Memory Footprint Estimator turns abstract “\\(O(n)\\) space” into tangible bytes, letting you see how close you are to the edge before your program runs out of room.\n\n\n\n17 Time Complexity Table\nA Time Complexity Table summarizes how different algorithms grow as input size increases, it’s a map from formula to feeling, showing which complexities are fast, which are dangerous, and how they compare in scale.\n\nWhat Problem Are We Solving?\nWe want a quick reference that links mathematical growth rates to practical performance. Knowing that an algorithm is \\(O(n \\log n)\\) is good; understanding what that means for \\(n = 10^6\\) is better.\nThe table helps estimate feasibility: Can this algorithm handle a million inputs? A billion?\n\n\nHow It Works (Plain Language)\n\nList common complexity classes: constant, logarithmic, linear, etc.\nWrite their formulas and interpretations.\nEstimate operations for various \\(n\\).\nHighlight tipping points, where performance becomes infeasible.\n\nThis creates an intuition grid for algorithmic growth.\n\n\nExample Step by Step\nLet \\(n = 10^6\\) (1 million). Estimate operations per complexity class (approximate scale):\n\n\n\n\n\n\n\n\n\nComplexity\nFormula\nOperations (n=10⁶)\nIntuition\n\n\n\n\n\\(O(1)\\)\nconstant\n1\ninstant\n\n\n\\(O(\\log n)\\)\n\\(\\log_2 10^6 \\approx 20\\)\n20\nlightning fast\n\n\n\\(O(n)\\)\n\\(10^6\\)\n1,000,000\nmanageable\n\n\n\\(O(n \\log n)\\)\n\\(10^6 \\cdot 20\\)\n20M\nstill OK\n\n\n\\(O(n^2)\\)\n\\((10^6)^2\\)\n\\(10^{12}\\)\ntoo slow\n\n\n\\(O(2^n)\\)\n\\(2^{20} \\approx 10^6\\)\nimpossible beyond \\(n=30\\)\n\n\n\n\\(O(n!)\\)\nfactorial\n\\(10^6!\\)\nabsurdly huge\n\n\n\nThe table makes complexity feel real.\n\n\nTiny Code (Python)\nimport math\n\ndef ops_estimate(n):\n    return {\n        \"O(1)\": 1,\n        \"O(log n)\": math.log2(n),\n        \"O(n)\": n,\n        \"O(n log n)\": n * math.log2(n),\n        \"O(n^2)\": n2\n    }\n\nprint(ops_estimate(106))\n\n\nWhy It Matters\n\nBuilds numerical intuition for asymptotics\nHelps choose the right algorithm for large \\(n\\)\nExplains why \\(O(n^2)\\) might work for \\(n=1000\\) but not \\(n=10^6\\)\nConnects abstract math to real-world feasibility\n\n\n\nA Gentle Proof (Why It Works)\nEach complexity class describes a function \\(f(n)\\) bounding operations. Comparing \\(f(n)\\) for common \\(n\\) values illustrates relative growth rates. Because asymptotic notation suppresses constants, differences in growth dominate as \\(n\\) grows.\nThus, numerical examples are faithful approximations of asymptotic behavior.\n\n\nTry It Yourself\n\nFill the table for \\(n = 10^3, 10^4, 10^6\\).\nPlot growth curves for each \\(f(n)\\).\nCompare runtime if each operation = 1 microsecond.\nIdentify feasible vs infeasible complexities for your hardware.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(O(1)\\)\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\\(O(n^2)\\)\n\n\n\n\n\\(10^3\\)\n1\n10\n1,000\n1,000,000\n\n\n\\(10^6\\)\n1\n20\n1,000,000\n\\(10^{12}\\)\n\n\n\\(10^9\\)\n1\n30\n1,000,000,000\n\\(10^{18}\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nType\nInsight\n\n\n\n\nTable Generation\n\\(O(1)\\)\nStatic reference\n\n\nEvaluation\n\\(O(1)\\)\nAnalytical\n\n\n\nA Time Complexity Table turns abstract Big-O notation into a living chart, where \\(O(\\log n)\\) feels tiny, \\(O(n^2)\\) feels heavy, and \\(O(2^n)\\) feels impossible.\n\n\n\n18 Space–Time Tradeoff Explorer\nA Space–Time Tradeoff Explorer helps us understand one of the most fundamental balances in algorithm design: using more memory to gain speed, or saving memory at the cost of time. It’s the art of finding equilibrium between storage and computation.\n\nWhat Problem Are We Solving?\nWe often face a choice:\n\nPrecompute and store results for instant access (more space, less time)\nCompute on demand to save memory (less space, more time)\n\nThe goal is to analyze both sides and choose the best fit for the problem’s constraints.\n\n\nHow It Works (Plain Language)\n\nIdentify repeated computations that can be stored.\nEstimate memory cost of storing precomputed data.\nEstimate time saved per query or reuse.\nCompare tradeoffs using total cost models: \\[\n\\text{Total Cost} = \\text{Time Cost} + \\lambda \\cdot \\text{Space Cost}\n\\] where \\(\\lambda\\) reflects system priorities.\nDecide whether caching, tabulation, or recomputation is preferable.\n\nYou’re tuning performance with two dials, one for memory, one for time.\n\n\nExample Step by Step\nExample 1: Fibonacci Numbers\n\nRecursive (no memory): \\(O(2^n)\\) time, \\(O(1)\\) space\nMemoized: \\(O(n)\\) time, \\(O(n)\\) space\nIterative (tabulated): \\(O(n)\\) time, \\(O(1)\\) space (store only last two)\n\nDifferent tradeoffs for the same problem.\nExample 2: Lookup Tables\nSuppose you need \\(\\sin(x)\\) for many \\(x\\) values:\n\nCompute each time → \\(O(n)\\) per query\nStore all results → \\(O(n)\\) memory, \\(O(1)\\) lookup\nHybrid: store sampled points, interpolate → balance\n\n\n\nTiny Code (Python)\ndef fib_naive(n):\n    if n &lt;= 1: return n\n    return fib_naive(n-1) + fib_naive(n-2)\n\ndef fib_memo(n, memo={}):\n    if n in memo: return memo[n]\n    if n &lt;= 1: return n\n    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)\n    return memo[n]\nCompare time vs memory for each version.\n\n\nWhy It Matters\n\nHelps design algorithms under memory limits or real-time constraints\nEssential in databases, graphics, compilers, and AI caching\nConnects theory (asymptotics) to engineering (resources)\nPromotes thinking in trade curves, not absolutes\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) = time, \\(S(n)\\) = space. If we precompute \\(k\\) results, \\[\nT'(n) = T(n) - \\Delta T, \\quad S'(n) = S(n) + \\Delta S\n\\]\nSince \\(\\Delta T\\) and \\(\\Delta S\\) are usually monotonic, minimizing one increases the other. Thus, the optimal configuration lies where \\[\n\\frac{dT}{dS} = -\\lambda\n\\] reflecting the system’s valuation of time vs memory.\n\n\nTry It Yourself\n\nCompare naive vs memoized vs iterative Fibonacci.\nBuild a lookup table for factorials modulo \\(M\\).\nExplore DP tabulation (space-heavy) vs rolling array (space-light).\nEvaluate caching in a recursive tree traversal.\n\n\n\nTest Cases\n\n\n\nProblem\nSpace\nTime\nStrategy\n\n\n\n\nFibonacci\n\\(O(1)\\)\n\\(O(2^n)\\)\nNaive recursion\n\n\nFibonacci\n\\(O(n)\\)\n\\(O(n)\\)\nMemoization\n\n\nFibonacci\n\\(O(1)\\)\n\\(O(n)\\)\nIterative\n\n\nLookup Table\n\\(O(n)\\)\n\\(O(1)\\)\nPrecompute\n\n\nRecompute\n\\(O(1)\\)\n\\(O(n)\\)\nOn-demand\n\n\n\n\n\nComplexity\n\n\n\nOperation\nDimension\nNote\n\n\n\n\nSpace–Time Analysis\n\\(O(1)\\)\nConceptual\n\n\nOptimization\n\\(O(1)\\)\nTradeoff curve\n\n\n\nA Space–Time Tradeoff Explorer turns resource limits into creative levers, helping you choose when to remember, when to recompute, and when to balance both in harmony.\n\n\n\n19 Profiling Algorithm\nProfiling an algorithm means measuring how it actually behaves, how long it runs, how much memory it uses, how often loops iterate, and where time is really spent. It turns theoretical complexity into real performance data.\n\nWhat Problem Are We Solving?\nBig-O tells us how an algorithm scales, but not how it performs in practice. Constant factors, system load, compiler optimizations, and cache effects all matter.\nProfiling answers:\n\nWhere is the time going?\nWhich function dominates?\nAre we bound by CPU, memory, or I/O?\n\nIt’s the microscope for runtime behavior.\n\n\nHow It Works (Plain Language)\n\nInstrument your code, insert timers, counters, or use built-in profilers.\nRun with representative inputs.\nRecord runtime, call counts, and memory allocations.\nAnalyze hotspots, the 10% of code causing 90% of cost.\nOptimize only where it matters.\n\nProfiling doesn’t guess, it measures.\n\n\nExample Step by Step\n\n\nExample 1: Timing a Function\nimport time\n\nstart = time.perf_counter()\nresult = algorithm(n)\nend = time.perf_counter()\n\nprint(\"Elapsed:\", end - start)\nMeasure total runtime for a given input size.\n\n\nExample 2: Line-Level Profiling\nimport cProfile, pstats\n\ncProfile.run('algorithm(1000)', 'stats')\np = pstats.Stats('stats')\np.sort_stats('cumtime').print_stats(10)\nShows the 10 most time-consuming functions.\n\n\nTiny Code (Python)\ndef slow_sum(n):\n    s = 0\n    for i in range(n):\n        for j in range(i):\n            s += j\n    return s\n\nimport cProfile\ncProfile.run('slow_sum(500)')\nOutput lists functions, calls, total time, and cumulative time.\n\n\nWhy It Matters\n\nBridges theory (Big-O) and practice (runtime)\nIdentifies bottlenecks for optimization\nValidates expected scaling across inputs\nPrevents premature optimization, measure first, fix later\n\n\n\nA Gentle Proof (Why It Works)\nEvery algorithm execution is a trace of operations. Profiling samples or counts these operations in real time.\nIf \\(t_i\\) is time spent in component \\(i\\), then total runtime \\(T = \\sum_i t_i\\). Ranking \\(t_i\\) reveals the dominant terms empirically, confirming or refuting theoretical assumptions.\n\n\nTry It Yourself\n\nProfile a recursive function (like Fibonacci).\nCompare iterative vs recursive runtimes.\nPlot \\(n\\) vs runtime to visualize empirical complexity.\nUse memory_profiler to capture space usage.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nAlgorithm\nExpected\nObserved (example)\nNotes\n\n\n\n\nLinear Search\n\\(O(n)\\)\nruntime ∝ \\(n\\)\nscales linearly\n\n\nMerge Sort\n\\(O(n \\log n)\\)\nruntime grows slightly faster than \\(n\\)\nmerge overhead\n\n\nNaive Fibonacci\n\\(O(2^n)\\)\nexplodes at \\(n&gt;30\\)\nconfirms exponential cost\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nProfiling Run\n\\(O(n)\\) (per trial)\n\\(O(1)\\)\n\n\nReport Generation\n\\(O(f)\\) (per function)\n\\(O(f)\\)\n\n\n\nProfiling is where math meets the stopwatch, transforming asymptotic guesses into concrete numbers and revealing the true heartbeat of your algorithm.\n\n\n\n20 Benchmarking Framework\nA Benchmarking Framework provides a structured way to compare algorithms under identical conditions. It measures performance across input sizes, multiple trials, and varying hardware, revealing which implementation truly performs best in practice.\n\nWhat Problem Are We Solving?\nYou’ve got several algorithms solving the same problem — which one is actually faster? Which scales better? Which uses less memory?\nBenchmarking answers these questions with fair, repeatable experiments instead of intuition or isolated timing tests.\n\n\nHow It Works (Plain Language)\n\nDefine test cases (input sizes, data patterns).\nRun all candidate algorithms under the same conditions.\nRepeat trials to reduce noise.\nRecord metrics:\n\nRuntime\nMemory usage\nThroughput or latency\n\nAggregate results and visualize trends.\n\nThink of it as a “tournament” where each algorithm plays by the same rules.\n\n\nExample Step by Step\nSuppose we want to benchmark sorting methods:\n\nInputs: random arrays of sizes \\(10^3\\), \\(10^4\\), \\(10^5\\)\nAlgorithms: bubble_sort, merge_sort, timsort\nMetric: average runtime over 5 runs\nResult: table or plot\n\n\n\n\nSize\nBubble Sort\nMerge Sort\nTimsort\n\n\n\n\n\\(10^3\\)\n0.05s\n0.001s\n0.0008s\n\n\n\\(10^4\\)\n5.4s\n0.02s\n0.012s\n\n\n\\(10^5\\)\n–\n0.25s\n0.15s\n\n\n\nTimsort wins across all sizes, data confirms theory.\n\n\nTiny Code (Python)\nimport timeit\nimport random\n\ndef bench(func, n, trials=5):\n    data = [random.randint(0, n) for _ in range(n)]\n    return min(timeit.repeat(lambda: func(data.copy()), number=1, repeat=trials))\n\ndef bubble_sort(arr):\n    for i in range(len(arr)):\n        for j in range(len(arr)-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n\ndef merge_sort(arr):\n    if len(arr) &lt;= 1: return arr\n    mid = len(arr)//2\n    return merge(merge_sort(arr[:mid]), merge_sort(arr[mid:]))\n\ndef merge(left, right):\n    result = []\n    while left and right:\n        result.append(left.pop(0) if left[0] &lt; right[0] else right.pop(0))\n    return result + left + right\n\nprint(\"Bubble:\", bench(bubble_sort, 1000))\nprint(\"Merge:\", bench(merge_sort, 1000))\n\n\nWhy It Matters\n\nConverts abstract complexity into empirical performance\nSupports evidence-based optimization\nDetects constant factor effects Big-O hides\nEnsures fair comparisons across algorithms\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(t_{i,j}\\) be time of algorithm \\(i\\) on trial \\(j\\). Benchmarking reports \\(\\min\\), \\(\\max\\), or \\(\\text{mean}(t_{i,*})\\).\nBy controlling conditions (hardware, input distribution), we treat \\(t_{i,j}\\) as samples of the same distribution, allowing valid comparisons of \\(E[t_i]\\) (expected runtime). Hence, results reflect true relative performance.\n\n\nTry It Yourself\n\nBenchmark linear vs binary search on sorted arrays.\nTest dynamic array insertion vs linked list insertion.\nRun across input sizes \\(10^3\\), \\(10^4\\), \\(10^5\\).\nPlot results: \\(n\\) (x-axis) vs time (y-axis).\n\n\n\nTest Cases\n\n\n\nComparison\nExpectation\n\n\n\n\nBubble vs Merge\nMerge faster after small \\(n\\)\n\n\nLinear vs Binary Search\nBinary faster for \\(n &gt; 100\\)\n\n\nList vs Dict lookup\nDict \\(O(1)\\) outperforms List \\(O(n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nRun Each Trial\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nAggregate Results\n\\(O(k)\\)\n\\(O(k)\\)\n\n\nTotal Benchmark\n\\(O(nk)\\)\n\\(O(1)\\)\n\n\n\n(\\(k\\) = number of trials)\nA Benchmarking Framework transforms comparison into science, fair tests, real data, and performance truths grounded in experiment, not hunch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundatmemtions of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-3.-big-o-big-theta-big-omega",
    "href": "books/en-us/list-1.html#section-3.-big-o-big-theta-big-omega",
    "title": "Chapter 1. Foundatmemtions of Algorithms",
    "section": "Section 3. Big-O, Big-Theta, Big-Omega",
    "text": "Section 3. Big-O, Big-Theta, Big-Omega\n\n21 Growth Rate Comparator\nA Growth Rate Comparator helps us see how functions grow relative to each other, the backbone of asymptotic reasoning. It lets us answer questions like: does \\(n^2\\) outgrow \\(n \\log n\\)? How fast is \\(2^n\\) compared to \\(n!\\)?\n\nWhat Problem Are We Solving?\nWe need a clear way to compare how fast two functions increase as \\(n\\) becomes large. When analyzing algorithms, runtime functions like \\(n\\), \\(n \\log n\\), and \\(n^2\\) all seem similar at small scales, but their growth rates diverge quickly.\nA comparator gives us a mathematical and visual way to rank them.\n\n\nHow It Works (Plain Language)\n\nWrite the two functions \\(f(n)\\) and \\(g(n)\\).\nCompute the ratio \\(\\dfrac{f(n)}{g(n)}\\) as \\(n \\to \\infty\\).\nInterpret the result:\n\nIf \\(\\dfrac{f(n)}{g(n)} \\to 0\\) → \\(f(n) = o(g(n))\\) (grows slower)\nIf \\(\\dfrac{f(n)}{g(n)} \\to c &gt; 0\\) → \\(f(n) = \\Theta(g(n))\\) (same growth)\nIf \\(\\dfrac{f(n)}{g(n)} \\to \\infty\\) → \\(f(n) = \\omega(g(n))\\) (grows faster)\n\n\nThis ratio test tells us which function dominates for large \\(n\\).\n\n\nExample Step by Step\nExample 1: Compare \\(n \\log n\\) vs \\(n^2\\)\n\\[\n\\frac{n \\log n}{n^2} = \\frac{\\log n}{n}\n\\]\nAs \\(n \\to \\infty\\), \\(\\frac{\\log n}{n} \\to 0\\) → \\(n \\log n = o(n^2)\\)\nExample 2: Compare \\(2^n\\) vs \\(n!\\)\n\\[\n\\frac{2^n}{n!} \\to 0\n\\]\nsince \\(n!\\) grows faster than \\(2^n\\). → \\(2^n = o(n!)\\)\n\n\nTiny Code (Python)\nimport math\n\ndef compare_growth(f, g, ns):\n    for n in ns:\n        ratio = f(n)/g(n)\n        print(f\"n={n:6}, ratio={ratio:.6e}\")\n\ncompare_growth(lambda n: n*math.log2(n),\n               lambda n: n2,\n               [10, 100, 1000, 10000])\nOutput shows ratio shrinking → confirms slower growth.\n\n\nWhy It Matters\n\nBuilds intuition for asymptotic dominance\nEssential for Big-O, Big-Theta, Big-Omega proofs\nClarifies why some algorithms scale better\nTranslates math into visual and numerical comparisons\n\n\n\nA Gentle Proof (Why It Works)\nBy definition of asymptotic notation:\nIf \\(\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\\), then for any \\(\\varepsilon &gt; 0\\), \\(f(n) &lt; \\varepsilon g(n)\\) for large \\(n\\).\nThus, \\(f(n)\\) grows slower than \\(g(n)\\).\nThis formal limit test underlies Big-O reasoning.\n\n\nTry It Yourself\n\nCompare \\(n^3\\) vs \\(2^n\\)\nCompare \\(\\sqrt{n}\\) vs \\(\\log n\\)\nCompare \\(n!\\) vs \\(n^n\\)\nPlot both functions and see where one overtakes the other\n\n\n\nTest Cases\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nResult\nRelation\n\n\n\n\n\\(\\log n\\)\n\\(\\sqrt{n}\\)\n\\(0\\)\n\\(o(\\sqrt{n})\\)\n\n\n\\(n\\)\n\\(n \\log n\\)\n\\(0\\)\n\\(o(n \\log n)\\)\n\n\n\\(n^2\\)\n\\(2^n\\)\n\\(0\\)\n\\(o(2^n)\\)\n\n\n\\(2^n\\)\n\\(n!\\)\n\\(0\\)\n\\(o(n!)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nComparison\n\\(O(1)\\) per pair\n\\(O(1)\\)\n\n\n\nA Growth Rate Comparator turns asymptotic theory into a conversation, showing, with numbers and limits, who really grows faster as \\(n\\) climbs toward infinity.\n\n\n\n22 Dominant Term Extractor\nA Dominant Term Extractor simplifies complexity expressions by identifying which term matters most as \\(n\\) grows large. It’s how we turn messy runtime formulas into clean Big-O notation, by keeping only what truly drives growth.\n\nWhat Problem Are We Solving?\nAlgorithms often produce composite cost formulas like \\[\nT(n) = 3n^2 + 10n + 25\n\\] Not all terms grow equally. The dominant term determines long-run behavior, so we want to isolate it and discard the rest.\nThis step bridges detailed operation counting and asymptotic notation.\n\n\nHow It Works (Plain Language)\n\nWrite the runtime function \\(T(n)\\) (from counting steps).\nList all terms by their growth type (\\(n^3\\), \\(n^2\\), \\(n\\), \\(\\log n\\), constants).\nFind the fastest-growing term as \\(n \\to \\infty\\).\nDrop coefficients and lower-order terms.\nThe result is the Big-O class.\n\nThink of it as zooming out on a curve, smaller waves vanish at infinity.\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = 5n^3 + 2n^2 + 7n + 12\n\\]\nFor large \\(n\\), \\(n^3\\) dominates.\nSo: \\[\nT(n) = O(n^3)\n\\]\nExample 2: \\[\nT(n) = n^2 + n\\log n + 10n\n\\]\nCompare term by term: \\[\nn^2 &gt; n \\log n &gt; n\n\\]\nSo dominant term is \\(n^2\\). \\(\\Rightarrow T(n) = O(n^2)\\)\n\n\nTiny Code (Python)\ndef dominant_term(terms):\n    growth_order = {'1': 0, 'logn': 1, 'n': 2, 'nlogn': 3, 'n^2': 4, 'n^3': 5, '2^n': 6}\n    return max(terms, key=lambda t: growth_order[t])\n\nprint(dominant_term(['n^2', 'nlogn', 'n']))  # n^2\nYou can extend this with symbolic simplification using SymPy.\n\n\nWhy It Matters\n\nSimplifies detailed formulas into clean asymptotics\nFocuses attention on scaling behavior, not constants\nMakes performance comparison straightforward\nCore step in deriving Big-O from raw step counts\n\n\n\nA Gentle Proof (Why It Works)\nLet \\[\nT(n) = a_k n^k + a_{k-1} n^{k-1} + \\dots + a_0\n\\]\nAs \\(n \\to \\infty\\), \\[\n\\frac{a_{k-1} n^{k-1}}{a_k n^k} = \\frac{a_{k-1}}{a_k n} \\to 0\n\\]\nAll lower-order terms vanish relative to the largest exponent. So \\(T(n) = \\Theta(n^k)\\).\nThis generalizes beyond polynomials to any family of functions with strict growth ordering.\n\n\nTry It Yourself\n\nSimplify \\(T(n) = 4n \\log n + 10n + 100\\).\nSimplify \\(T(n) = 2n^3 + 50n^2 + 1000\\).\nSimplify \\(T(n) = 5n + 10\\log n + 100\\).\nVerify using ratio test: \\(\\frac{\\text{lower term}}{\\text{dominant term}} \\to 0\\).\n\n\n\nTest Cases\n\n\n\nExpression\nDominant Term\nBig-O\n\n\n\n\n\\(3n^2 + 4n + 10\\)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\n\\(5n + 8\\log n + 7\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(n \\log n + 100n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(4n^3 + n^2 + 2n\\)\n\\(n^3\\)\n\\(O(n^3)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nExtraction\n\\(O(k)\\)\n\\(O(1)\\)\n\n\n\n(\\(k\\) = number of terms)\nA Dominant Term Extractor is like a spotlight, it shines on the one term that decides the pace, letting you see the true asymptotic character of your algorithm.\n\n\n\n23 Limit-Based Complexity Test\nThe Limit-Based Complexity Test is a precise way to compare how fast two functions grow by using limits. It’s a mathematical tool that turns intuition (“this one feels faster”) into proof (“this one is faster”).\n\nWhat Problem Are We Solving?\nWhen analyzing algorithms, we often ask: Does \\(f(n)\\) grow slower, equal, or faster than \\(g(n)\\)? Instead of guessing, we use limits to determine the exact relationship and classify them using Big-O, \\(\\Theta\\), or \\(\\Omega\\).\nThis method gives a formal and reliable comparison of growth rates.\n\n\nHow It Works (Plain Language)\n\nStart with two positive functions \\(f(n)\\) and \\(g(n)\\).\nCompute the ratio: \\[\nL = \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)}\n\\]\nInterpret the limit:\n\nIf \\(L = 0\\), then \\(f(n) = o(g(n))\\) → \\(f\\) grows slower.\nIf \\(0 &lt; L &lt; \\infty\\), then \\(f(n) = \\Theta(g(n))\\) → same growth rate.\nIf \\(L = \\infty\\), then \\(f(n) = \\omega(g(n))\\) → \\(f\\) grows faster.\n\n\nThe ratio tells us how one function “scales” relative to another.\n\n\nExample Step by Step\nExample 1:\nCompare \\(f(n) = n \\log n\\) and \\(g(n) = n^2\\).\n\\[\n\\frac{f(n)}{g(n)} = \\frac{n \\log n}{n^2} = \\frac{\\log n}{n}\n\\]\nAs \\(n \\to \\infty\\), \\(\\frac{\\log n}{n} \\to 0\\). So \\(n \\log n = o(n^2)\\) → grows slower.\nExample 2:\nCompare \\(f(n) = 3n^2 + 4n\\) and \\(g(n) = n^2\\).\n\\[\n\\frac{f(n)}{g(n)} = \\frac{3n^2 + 4n}{n^2} = 3 + \\frac{4}{n}\n\\]\nAs \\(n \\to \\infty\\), \\(\\frac{4}{n} \\to 0\\). So \\(\\lim = 3\\), constant and positive. Therefore, \\(f(n) = \\Theta(g(n))\\).\n\n\nTiny Code (Python)\nimport sympy as sp\n\nn = sp.symbols('n', positive=True)\nf = n * sp.log(n)\ng = n2\nL = sp.limit(f/g, n, sp.oo)\nprint(\"Limit:\", L)\nOutputs 0, confirming \\(n \\log n = o(n^2)\\).\n\n\nWhy It Matters\n\nProvides formal proof of asymptotic relationships\nEliminates guesswork in comparing growth rates\nCore step in Big-O proofs and recurrence analysis\nHelps verify if approximations are valid\n\n\n\nA Gentle Proof (Why It Works)\nThe definition of asymptotic comparison uses limits:\nIf \\(\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\\), then for any \\(\\varepsilon &gt; 0\\), \\(\\exists N\\) such that \\(\\forall n &gt; N\\), \\(f(n) \\le \\varepsilon g(n)\\).\nThis satisfies the formal condition for \\(f(n) = o(g(n))\\). Similarly, constant or infinite limits define \\(\\Theta\\) and \\(\\omega\\).\n\n\nTry It Yourself\n\nCompare \\(n^3\\) vs \\(2^n\\).\nCompare \\(\\sqrt{n}\\) vs \\(\\log n\\).\nCompare \\(n!\\) vs \\(n^n\\).\nCheck ratio for \\(n^2 + n\\) vs \\(n^2\\).\n\n\n\nTest Cases\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nLimit\nRelationship\n\n\n\n\n\\(n\\)\n\\(n \\log n\\)\n0\n\\(o(g(n))\\)\n\n\n\\(n^2 + n\\)\n\\(n^2\\)\n1\n\\(\\Theta(g(n))\\)\n\n\n\\(2^n\\)\n\\(n^3\\)\n\\(\\infty\\)\n\\(\\omega(g(n))\\)\n\n\n\\(\\log n\\)\n\\(\\sqrt{n}\\)\n0\n\\(o(g(n))\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLimit Evaluation\n\\(O(1)\\) symbolic\n\\(O(1)\\)\n\n\n\nThe Limit-Based Complexity Test is your mathematical magnifying glass, a clean, rigorous way to compare algorithmic growth and turn asymptotic intuition into certainty.\n\n\n\n24 Summation Simplifier\nA Summation Simplifier converts loops and recursive cost expressions into closed-form formulas using algebra and known summation rules. It bridges the gap between raw iteration counts and Big-O notation.\n\nWhat Problem Are We Solving?\nWhen analyzing loops, we often get total work expressed as a sum:\n\\[\nT(n) = \\sum_{i=1}^{n} i \\quad \\text{or} \\quad T(n) = \\sum_{i=1}^{n} \\log i\n\\]\nBut Big-O requires us to simplify these sums into familiar functions of \\(n\\). Summation simplification transforms iteration patterns into asymptotic form.\n\n\nHow It Works (Plain Language)\n\nWrite down the summation from your loop or recurrence.\nApply standard formulas or approximations:\n\n\\(\\sum_{i=1}^{n} 1 = n\\)\n\\(\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}\\)\n\\(\\sum_{i=1}^{n} i^2 = \\frac{n(n+1)(2n+1)}{6}\\)\n\\(\\sum_{i=1}^{n} \\log i = O(n \\log n)\\)\n\nDrop constants and lower-order terms.\nReturn simplified function \\(f(n)\\) → then apply Big-O.\n\nIt’s like algebraic compression for iteration counts.\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = \\sum_{i=1}^{n} i\n\\] Use formula: \\[\nT(n) = \\frac{n(n+1)}{2}\n\\] Simplify: \\[\nT(n) = O(n^2)\n\\]\nExample 2: \\[\nT(n) = \\sum_{i=1}^{n} \\log i\n\\] Approximate by integral: \\[\n\\int_1^n \\log x , dx = n \\log n - n + 1\n\\] So \\(T(n) = O(n \\log n)\\)\nExample 3: \\[\nT(n) = \\sum_{i=1}^{n} \\frac{1}{i}\n\\] ≈ \\(\\log n\\) (Harmonic series)\n\n\nTiny Code (Python)\nimport sympy as sp\n\ni, n = sp.symbols('i n', positive=True)\nexpr = sp.summation(i, (i, 1, n))\nprint(sp.simplify(expr))  # n*(n+1)/2\nOr use sp.summation(sp.log(i), (i,1,n)) for logarithmic sums.\n\n\nWhy It Matters\n\nConverts nested loops into analyzable formulas\nCore tool in time complexity derivation\nHelps visualize how cumulative work builds up\nConnects discrete steps with continuous approximations\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(f(i)\\) is positive and increasing, then by the integral test:\n\\[\n\\int_1^n f(x),dx \\le \\sum_{i=1}^n f(i) \\le f(n) + \\int_1^n f(x),dx\n\\]\nSo for asymptotic purposes, \\(\\sum f(i)\\) and \\(\\int f(x)\\) grow at the same rate.\nThis equivalence justifies approximations like \\(\\sum \\log i = O(n \\log n)\\).\n\n\nTry It Yourself\n\nSimplify \\(\\sum_{i=1}^n i^3\\).\nSimplify \\(\\sum_{i=1}^n \\sqrt{i}\\).\nSimplify \\(\\sum_{i=1}^n \\frac{1}{i^2}\\).\nApproximate \\(\\sum_{i=1}^{n/2} i\\) using integrals.\n\n\n\nTest Cases\n\n\n\nSummation\nFormula\nBig-O\n\n\n\n\n\\(\\sum 1\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(\\sum i\\)\n\\(\\frac{n(n+1)}{2}\\)\n\\(O(n^2)\\)\n\n\n\\(\\sum i^2\\)\n\\(\\frac{n(n+1)(2n+1)}{6}\\)\n\\(O(n^3)\\)\n\n\n\\(\\sum \\log i\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(\\sum \\frac{1}{i}\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSimplification\n\\(O(1)\\) (formula lookup)\n\\(O(1)\\)\n\n\n\nA Summation Simplifier turns looping arithmetic into elegant formulas, the difference between counting steps and seeing the shape of growth.\n\n\n\n25 Recurrence Tree Method\nThe Recurrence Tree Method is a visual technique for solving divide-and-conquer recurrences. It expands the recursive formula into a tree of subproblems, sums the work done at each level, and reveals the total cost.\n\nWhat Problem Are We Solving?\nMany recursive algorithms (like Merge Sort or Quick Sort) define their running time as \\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\] where:\n\n\\(a\\) = number of subproblems,\n\\(b\\) = size reduction factor,\n\\(f(n)\\) = non-recursive work per call.\n\nThe recurrence tree lets us see the full cost by summing over levels instead of applying a closed-form theorem immediately.\n\n\nHow It Works (Plain Language)\n\nDraw the recursion tree\n\nRoot: problem of size \\(n\\), cost \\(f(n)\\).\nEach node: subproblem of size \\(\\frac{n}{b}\\) with cost \\(f(\\frac{n}{b})\\).\n\nExpand levels until base case (\\(n=1\\)).\nSum work per level:\n\nLevel \\(i\\) has \\(a^i\\) nodes, each size \\(\\frac{n}{b^i}\\).\nTotal work at level \\(i\\): \\[\nW_i = a^i \\cdot f!\\left(\\frac{n}{b^i}\\right)\n\\]\n\nAdd all levels: \\[\nT(n) = \\sum_{i=0}^{\\log_b n} W_i\n\\]\nIdentify the dominant level (top, middle, or bottom).\nSimplify to Big-O form.\n\n\n\nExample Step by Step\nTake Merge Sort:\n\\[\nT(n) = 2T!\\left(\\frac{n}{2}\\right) + n\n\\]\nLevel 0: \\(1 \\times n = n\\) Level 1: \\(2 \\times \\frac{n}{2} = n\\) Level 2: \\(4 \\times \\frac{n}{4} = n\\) ⋯ Depth: \\(\\log_2 n\\) levels\nTotal work: \\[\nT(n) = n \\log_2 n + n = O(n \\log n)\n\\]\nEvery level costs \\(n\\), total = \\(n \\times \\log n\\).\n\n\nTiny Code (Python)\nimport math\n\ndef recurrence_tree(a, b, f, n):\n    total = 0\n    level = 0\n    while n &gt;= 1:\n        work = (alevel) * f(n/(blevel))\n        total += work\n        level += 1\n        n /= b\n    return total\nUse f = lambda x: x for \\(f(n) = n\\).\n\n\nWhy It Matters\n\nMakes recurrence structure visible and intuitive\nExplains why Master Theorem results hold\nHighlights dominant levels (top-heavy vs bottom-heavy)\nGreat teaching and reasoning tool for recursive cost breakdown\n\n\n\nA Gentle Proof (Why It Works)\nEach recursive call contributes \\(f(n)\\) work plus child subcalls. Because each level’s subproblems have equal size, total cost is additive:\n\\[\nT(n) = \\sum_{i=0}^{\\log_b n} a^i f!\\left(\\frac{n}{b^i}\\right)\n\\]\nDominant level dictates asymptotic order:\n\nTop-heavy: \\(f(n)\\) dominates → \\(O(f(n))\\)\nBalanced: all levels equal → \\(O(f(n) \\log n)\\)\nBottom-heavy: leaves dominate → \\(O(n^{\\log_b a})\\)\n\nThis reasoning leads directly to the Master Theorem.\n\n\nTry It Yourself\n\nBuild tree for \\(T(n) = 3T(n/2) + n\\).\nSum each level’s work.\nCompare with Master Theorem result.\nTry \\(T(n) = T(n/2) + 1\\) (logarithmic tree).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nRecurrence\nLevel Work\nLevels\nTotal\nBig-O\n\n\n\n\n\\(2T(n/2)+n\\)\n\\(n\\)\n\\(\\log n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(T(n/2)+1\\)\n\\(1\\)\n\\(\\log n\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\\(4T(n/2)+n\\)\n\\(a^i = 4^i\\), work = \\(n \\cdot 2^i\\)\nbottom dominates\n\\(O(n^2)\\)\n\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nTree Construction\n\\(O(\\log n)\\) levels\n\\(O(\\log n)\\)\n\n\n\nThe Recurrence Tree Method turns abstract formulas into living diagrams, showing each layer’s effort, revealing which level truly drives the algorithm’s cost.\n\n\n\n26 Master Theorem Evaluator\nThe Master Theorem Evaluator gives a quick, formula-based way to solve divide-and-conquer recurrences of the form \\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\] It tells you the asymptotic behavior of \\(T(n)\\) without full expansion or summation, a shortcut born from the recurrence tree.\n\nWhat Problem Are We Solving?\nWe want to find the Big-O complexity of divide-and-conquer algorithms quickly. Manually expanding recursions (via recurrence trees) works, but is tedious. The Master Theorem classifies solutions by comparing the recursive work (\\(a , T(n/b)\\)) and non-recursive work (\\(f(n)\\)).\n\n\nHow It Works (Plain Language)\nGiven \\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\]\n\n\\(a\\) = number of subproblems\n\\(b\\) = shrink factor\n\\(f(n)\\) = work done outside recursion\n\nCompute critical exponent: \\[\nn^{\\log_b a}\n\\]\nCompare \\(f(n)\\) to \\(n^{\\log_b a}\\):\n\nCase 1 (Top-heavy): If \\(f(n) = O(n^{\\log_b a - \\varepsilon})\\), \\[T(n) = \\Theta(n^{\\log_b a})\\] Recursive part dominates.\nCase 2 (Balanced): If \\(f(n) = \\Theta(n^{\\log_b a} \\log^k n)\\), \\[T(n) = \\Theta(n^{\\log_b a} \\log^{k+1} n)\\] Both contribute equally.\nCase 3 (Bottom-heavy): If \\(f(n) = \\Omega(n^{\\log_b a + \\varepsilon})\\) and regularity condition holds: \\[a f(n/b) \\le c f(n)\\] for some \\(c&lt;1\\), then \\[T(n) = \\Theta(f(n))\\] Non-recursive part dominates.\n\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = 2T(n/2) + n\n\\]\n\n\\(a = 2\\), \\(b = 2\\), \\(f(n) = n\\)\n\\(n^{\\log_2 2} = n\\) So \\(f(n) = \\Theta(n^{\\log_2 2})\\) → Case 2\n\n\\[\nT(n) = \\Theta(n \\log n)\n\\]\nExample 2: \\[\nT(n) = 4T(n/2) + n\n\\]\n\n\\(a = 4\\), \\(b = 2\\) → \\(n^{\\log_2 4} = n^2\\)\n\\(f(n) = n = O(n^{2 - \\varepsilon})\\) → Case 1\n\n\\[\nT(n) = \\Theta(n^2)\n\\]\nExample 3: \\[\nT(n) = T(n/2) + n\n\\]\n\n\\(a=1\\), \\(b=2\\), \\(n^{\\log_2 1}=1\\)\n\\(f(n)=n = \\Omega(n^{0+\\varepsilon})\\) → Case 3\n\n\\[\nT(n) = \\Theta(n)\n\\]\n\n\nTiny Code (Python)\nimport math\n\ndef master_theorem(a, b, f_exp):\n    critical = math.log(a, b)\n    if f_exp &lt; critical:\n        return f\"O(n^{critical:.2f})\"\n    elif f_exp == critical:\n        return f\"O(n^{critical:.2f} log n)\"\n    else:\n        return f\"O(n^{f_exp})\"\nFor \\(T(n) = 2T(n/2) + n\\), call master_theorem(2,2,1) → O(n log n)\n\n\nWhy It Matters\n\nSolves recurrences in seconds\nFoundation for analyzing divide-and-conquer algorithms\nValidates intuition from recurrence trees\nUsed widely in sorting, searching, matrix multiplication, FFT\n\n\n\nA Gentle Proof (Why It Works)\nEach recursion level costs: \\[a^i , f!\\left(\\frac{n}{b^i}\\right)\\]\nTotal cost: \\[T(n) = \\sum_{i=0}^{\\log_b n} a^i f!\\left(\\frac{n}{b^i}\\right)\\]\nThe relative growth of \\(f(n)\\) to \\(n^{\\log_b a}\\) determines which level dominates, top, middle, or bottom, yielding the three canonical cases.\n\n\nTry It Yourself\n\n\\(T(n) = 3T(n/2) + n\\)\n\\(T(n) = 2T(n/2) + n^2\\)\n\\(T(n) = 8T(n/2) + n^3\\)\nIdentify \\(a, b, f(n)\\) and apply theorem.\n\n\n\nTest Cases\n\n\n\nRecurrence\nCase\nResult\n\n\n\n\n\\(2T(n/2)+n\\)\n2\n\\(O(n \\log n)\\)\n\n\n\\(4T(n/2)+n\\)\n1\n\\(O(n^2)\\)\n\n\n\\(T(n/2)+n\\)\n3\n\\(O(n)\\)\n\n\n\\(3T(n/3)+n\\log n\\)\n2\n\\(O(n\\log^2 n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nEvaluation\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Master Theorem Evaluator is your formulaic compass, it points instantly to the asymptotic truth hidden in recursive equations, no tree-drawing required.\n\n\n\n27 Big-Theta Proof Builder\nA Big-Theta Proof Builder helps you formally prove that a function grows at the same rate as another. It’s the precise way to show that \\(f(n)\\) and \\(g(n)\\) are asymptotically equivalent, growing neither faster nor slower beyond constant factors.\n\nWhat Problem Are We Solving?\nWe often say an algorithm is \\(T(n) = \\Theta(n \\log n)\\), but how do we prove it? A Big-Theta proof uses inequalities to pin \\(T(n)\\) between two scaled versions of a simpler function \\(g(n)\\), confirming tight asymptotic bounds.\nThis transforms intuition into rigorous evidence.\n\n\nHow It Works (Plain Language)\nWe say \\[\nf(n) = \\Theta(g(n))\n\\] if there exist constants \\(c_1, c_2 &gt; 0\\) and \\(n_0\\) such that for all \\(n \\ge n_0\\): \\[\nc_1 g(n) \\le f(n) \\le c_2 g(n)\n\\]\nSo \\(f(n)\\) is sandwiched between two constant multiples of \\(g(n)\\).\nSteps:\n\nIdentify \\(f(n)\\) and candidate \\(g(n)\\).\nFind constants \\(c_1\\), \\(c_2\\), and threshold \\(n_0\\).\nVerify inequality for all \\(n \\ge n_0\\).\nConclude \\(f(n) = \\Theta(g(n))\\).\n\n\n\nExample Step by Step\nExample 1: \\[\nf(n) = 3n^2 + 10n + 5\n\\] Candidate: \\(g(n) = n^2\\)\nFor large \\(n\\), \\(10n + 5\\) is small compared to \\(3n^2\\).\nWe can show: \\[\n3n^2 \\le 3n^2 + 10n + 5 \\le 4n^2, \\quad \\text{for } n \\ge 10\n\\]\nThus, \\(f(n) = \\Theta(n^2)\\) with \\(c_1 = 3\\), \\(c_2 = 4\\), \\(n_0 = 10\\).\nExample 2: \\[\nf(n) = n \\log n + 100n\n\\] Candidate: \\(g(n) = n \\log n\\)\nFor \\(n \\ge 2\\), \\(\\log n \\ge 1\\), so \\(100n \\le 100n \\log n\\). Hence, \\[\nn \\log n \\le f(n) \\le 101n \\log n\n\\] → \\(f(n) = \\Theta(n \\log n)\\)\n\n\nTiny Code (Python)\ndef big_theta_proof(f, g, n0, c1, c2):\n    for n in range(n0, n0 + 5):\n        if not (c1*g(n) &lt;= f(n) &lt;= c2*g(n)):\n            return False\n    return True\n\nf = lambda n: 3*n2 + 10*n + 5\ng = lambda n: n2\nprint(big_theta_proof(f, g, 10, 3, 4))  # True\n\n\nWhy It Matters\n\nConverts informal claims (“it’s \\(n^2\\)-ish”) into formal proofs\nBuilds rigor in asymptotic reasoning\nEssential for algorithm analysis, recurrence proofs, and coursework\nReinforces understanding of constants and thresholds\n\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\[\nf(n) = \\Theta(g(n)) \\iff \\exists c_1, c_2, n_0 : c_1 g(n) \\le f(n) \\le c_2 g(n)\n\\] This mirrors how Big-O and Big-Omega combine:\n\n\\(f(n) = O(g(n))\\) gives upper bound,\n\\(f(n) = \\Omega(g(n))\\) gives lower bound. Together, they form a tight bound, hence \\(\\Theta\\).\n\n\n\nTry It Yourself\n\nProve \\(5n^3 + n^2 + 100 = \\Theta(n^3)\\).\nProve \\(4n + 10 = \\Theta(n)\\).\nShow \\(n \\log n + 100n = \\Theta(n \\log n)\\).\nFail a proof: \\(n^2 + 3n = \\Theta(n)\\) (not true).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\n\\(c_1, c_2, n_0\\)\nResult\n\n\n\n\n\\(3n^2 + 10n + 5\\)\n\\(n^2\\)\n\\(3,4,10\\)\n\\(\\Theta(n^2)\\)\n\n\n\\(n \\log n + 100n\\)\n\\(n \\log n\\)\n\\(1,101,2\\)\n\\(\\Theta(n \\log n)\\)\n\n\n\\(10n + 50\\)\n\\(n\\)\n\\(10,11,5\\)\n\\(\\Theta(n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nVerification\n\\(O(1)\\) (symbolic)\n\\(O(1)\\)\n\n\n\nThe Big-Theta Proof Builder is your asymptotic courtroom, you bring evidence, constants, and inequalities, and the proof delivers a verdict: \\(\\Theta(g(n))\\), beyond reasonable doubt.\n\n\n\n28 Big-Omega Case Finder\nA Big-Omega Case Finder helps you identify lower bounds on an algorithm’s growth, the guaranteed minimum cost, even in the best-case scenario. It’s the mirror image of Big-O, showing what an algorithm must at least do.\n\nWhat Problem Are We Solving?\nBig-O gives us an upper bound (“it won’t be slower than this”), but sometimes we need to know the floor, a complexity it can never beat.\nBig-Omega helps us state:\n\nThe fastest possible asymptotic behavior, or\nThe minimal cost inherent to the problem itself.\n\nThis is key when analyzing best-case performance or complexity limits (like comparison sorting’s \\(\\Omega(n \\log n)\\) lower bound).\n\n\nHow It Works (Plain Language)\nWe say \\[\nf(n) = \\Omega(g(n))\n\\] if \\(\\exists c &gt; 0, n_0\\) such that \\[\nf(n) \\ge c \\cdot g(n) \\quad \\text{for all } n \\ge n_0\n\\]\nSteps:\n\nIdentify candidate lower-bound function \\(g(n)\\).\nShow \\(f(n)\\) eventually stays above a constant multiple of \\(g(n)\\).\nFind constants \\(c\\) and \\(n_0\\).\nConclude \\(f(n) = \\Omega(g(n))\\).\n\n\n\nExample Step by Step\nExample 1: \\[\nf(n) = 3n^2 + 5n + 10\n\\] Candidate: \\(g(n) = n^2\\)\nFor \\(n \\ge 1\\), \\[\nf(n) \\ge 3n^2 \\ge 3 \\cdot n^2\n\\]\nSo \\(f(n) = \\Omega(n^2)\\) with \\(c = 3\\), \\(n_0 = 1\\).\nExample 2: \\[\nf(n) = n \\log n + 100n\n\\] Candidate: \\(g(n) = n\\)\nSince \\(\\log n \\ge 1\\) for \\(n \\ge 2\\), \\[\nf(n) = n \\log n + 100n \\ge n + 100n = 101n\n\\] → \\(f(n) = \\Omega(n)\\) with \\(c = 101\\), \\(n_0 = 2\\)\n\n\nTiny Code (Python)\ndef big_omega_proof(f, g, n0, c):\n    for n in range(n0, n0 + 5):\n        if f(n) &lt; c * g(n):\n            return False\n    return True\n\nf = lambda n: 3*n2 + 5*n + 10\ng = lambda n: n2\nprint(big_omega_proof(f, g, 1, 3))  # True\n\n\nWhy It Matters\n\nDefines best-case performance\nProvides theoretical lower limits (impossible to beat)\nComplements Big-O (upper bound) and Theta (tight bound)\nKey in proving problem hardness or optimality\n\n\n\nA Gentle Proof (Why It Works)\nIf \\[\n\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = L &gt; 0,\n\\] then for any \\(c \\le L\\), \\(f(n) \\ge c \\cdot g(n)\\) for large \\(n\\). Thus \\(f(n) = \\Omega(g(n))\\). This mirrors the formal definition of \\(\\Omega\\) and follows directly from asymptotic ratio reasoning.\n\n\nTry It Yourself\n\nShow \\(4n^3 + n^2 = \\Omega(n^3)\\)\nShow \\(n \\log n + n = \\Omega(n)\\)\nShow \\(2^n + n^5 = \\Omega(2^n)\\)\nCompare with their Big-O forms for contrast.\n\n\n\nTest Cases\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nConstants\nResult\n\n\n\n\n\\(3n^2 + 10n\\)\n\\(n^2\\)\n\\(c=3\\), \\(n_0=1\\)\n\\(\\Omega(n^2)\\)\n\n\n\\(n \\log n + 100n\\)\n\\(n\\)\n\\(c=101\\), \\(n_0=2\\)\n\\(\\Omega(n)\\)\n\n\n\\(n^3 + n^2\\)\n\\(n^3\\)\n\\(c=1\\), \\(n_0=1\\)\n\\(\\Omega(n^3)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nVerification\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Big-Omega Case Finder shows the floor beneath the curve, ensuring every algorithm stands on a solid lower bound, no matter how fast it tries to run.\n\n\n\n29 Empirical Complexity Estimator\nAn Empirical Complexity Estimator bridges theory and experiment, it measures actual runtimes for various input sizes and fits them to known growth models like \\(O(n)\\), \\(O(n \\log n)\\), or \\(O(n^2)\\). It’s how we discover complexity when the math is unclear or the code is complex.\n\nWhat Problem Are We Solving?\nSometimes the exact formula for \\(T(n)\\) is too messy, or the implementation details are opaque. We can still estimate complexity empirically by observing how runtime changes as \\(n\\) grows.\nThis approach is especially useful for:\n\nBlack-box code (unknown implementation)\nExperimental validation of asymptotic claims\nComparing real-world scaling with theoretical predictions\n\n\n\nHow It Works (Plain Language)\n\nChoose representative input sizes \\(n_1, n_2, \\dots, n_k\\).\nMeasure runtime \\(T(n_i)\\) for each size.\nNormalize or compare ratios:\n\n\\(T(2n)/T(n) \\approx 2\\) → \\(O(n)\\)\n\\(T(2n)/T(n) \\approx 4\\) → \\(O(n^2)\\)\n\\(T(2n)/T(n) \\approx \\log 2\\) → \\(O(\\log n)\\)\n\nFit data to candidate models using regression or ratio tests.\nVisualize trends (e.g., log–log plot) to identify slope = exponent.\n\n\n\nExample Step by Step\nSuppose we test input sizes: \\(n = 1000, 2000, 4000, 8000\\)\n\n\n\n\\(n\\)\n\\(T(n)\\) (ms)\nRatio \\(T(2n)/T(n)\\)\n\n\n\n\n1000\n5\n–\n\n\n2000\n10\n2.0\n\n\n4000\n20\n2.0\n\n\n8000\n40\n2.0\n\n\n\nRatio \\(\\approx 2\\) → linear growth → \\(T(n) = O(n)\\)\nNow suppose:\n\n\n\n\\(n\\)\n\\(T(n)\\)\nRatio\n\n\n\n\n1000\n5\n–\n\n\n2000\n20\n4\n\n\n4000\n80\n4\n\n\n8000\n320\n4\n\n\n\nRatio \\(\\approx 4\\) → quadratic growth → \\(O(n^2)\\)\n\n\nTiny Code (Python)\nimport time, math\n\ndef empirical_estimate(f, ns):\n    times = []\n    for n in ns:\n        start = time.perf_counter()\n        f(n)\n        end = time.perf_counter()\n        times.append(end - start)\n    for i in range(1, len(ns)):\n        ratio = times[i] / times[i-1]\n        print(f\"n={ns[i]:6}, ratio={ratio:.2f}\")\nTest with different algorithms to see scaling.\n\n\nWhy It Matters\n\nConverts runtime data into Big-O form\nDetects bottlenecks or unexpected scaling\nUseful when theoretical analysis is hard\nHelps validate optimizations or refactors\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(T(n) \\approx c \\cdot f(n)\\), then the ratio test \\[\n\\frac{T(kn)}{T(n)} \\approx \\frac{f(kn)}{f(n)}\n\\] reveals the exponent \\(p\\) if \\(f(n) = n^p\\): \\[\n\\frac{f(kn)}{f(n)} = k^p \\implies p = \\log_k \\frac{T(kn)}{T(n)}\n\\]\nRepeated over multiple \\(n\\), this converges to the true growth exponent.\n\n\nTry It Yourself\n\nMeasure runtime of sorting for increasing \\(n\\).\nEstimate \\(p\\) using ratio test.\nPlot \\(\\log n\\) vs \\(\\log T(n)\\), slope ≈ exponent.\nCompare \\(p\\) to theoretical value.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nObserved Ratio\nEstimated Complexity\n\n\n\n\nBubble Sort\n4\n\\(O(n^2)\\)\n\n\nMerge Sort\n2.2\n\\(O(n \\log n)\\)\n\n\nLinear Search\n2\n\\(O(n)\\)\n\n\nBinary Search\n1.1\n\\(O(\\log n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nMeasurement\n\\(O(k \\cdot T(n))\\)\n\\(O(k)\\)\n\n\nEstimation\n\\(O(k)\\)\n\\(O(1)\\)\n\n\n\n(\\(k\\) = number of sample points)\nAn Empirical Complexity Estimator transforms stopwatches into science, turning performance data into curves, curves into equations, and equations into Big-O intuition.\n\n\n\n30 Complexity Class Identifier\nA Complexity Class Identifier helps you categorize problems and algorithms into broad complexity classes like constant, logarithmic, linear, quadratic, exponential, or polynomial time. It’s a way to understand where your algorithm lives in the vast map of computational growth.\n\nWhat Problem Are We Solving?\nWhen analyzing an algorithm, we often want to know how big its time cost gets as input grows. Instead of exact formulas, we classify algorithms into families based on their asymptotic growth.\nThis tells us what is feasible (polynomial) and what is explosive (exponential), guiding both design choices and theoretical limits.\n\n\nHow It Works (Plain Language)\nWe map the growth rate of \\(T(n)\\) to a known complexity class:\n\n\n\n\n\n\n\n\nClass\nExample\nDescription\n\n\n\n\n\\(O(1)\\)\nHash lookup\nConstant time, no scaling\n\n\n\\(O(\\log n)\\)\nBinary search\nSublinear, halves each step\n\n\n\\(O(n)\\)\nLinear scan\nWork grows with input size\n\n\n\\(O(n \\log n)\\)\nMerge sort\nNear-linear with log factor\n\n\n\\(O(n^2)\\)\nNested loops\nQuadratic growth\n\n\n\\(O(n^3)\\)\nMatrix multiplication\nCubic growth\n\n\n\\(O(2^n)\\)\nBacktracking\nExponential explosion\n\n\n\\(O(n!)\\)\nBrute-force permutations\nFactorial blowup\n\n\n\nSteps to Identify:\n\nAnalyze loops and recursion structure.\nCount dominant operations.\nMatch pattern to table above.\nVerify with recurrence or ratio test.\nAssign class: constant → logarithmic → polynomial → exponential.\n\n\n\nExample Step by Step\nExample 1: Single loop:\nfor i in range(n):\n    work()\n→ \\(O(n)\\) → Linear\nExample 2: Nested loops:\nfor i in range(n):\n    for j in range(n):\n        work()\n→ \\(O(n^2)\\) → Quadratic\nExample 3: Divide and conquer: \\[\nT(n) = 2T(n/2) + n\n\\] → \\(O(n \\log n)\\) → Log-linear\nExample 4: Brute force subsets: \\[\n2^n \\text{ possibilities}\n\\] → \\(O(2^n)\\) → Exponential\n\n\nTiny Code (Python)\ndef classify_complexity(code_structure):\n    if \"nested n\" in code_structure:\n        return \"O(n^2)\"\n    if \"divide and conquer\" in code_structure:\n        return \"O(n log n)\"\n    if \"constant\" in code_structure:\n        return \"O(1)\"\n    return \"O(n)\"\nYou can extend this to pattern-match pseudocode shapes.\n\n\nWhy It Matters\n\nGives instant intuition about scalability\nGuides design trade-offs (speed vs. simplicity)\nConnects practical code to theoretical limits\nHelps compare algorithms solving the same problem\n\n\n\nA Gentle Proof (Why It Works)\nIf an algorithm performs \\(f(n)\\) fundamental operations for input size \\(n\\), and \\(f(n)\\) is asymptotically similar to a known class \\(g(n)\\): \\[\nf(n) = \\Theta(g(n))\n\\] then it belongs to the same class. Classes form equivalence groups under \\(\\Theta\\) notation, simplifying infinite functions into a finite taxonomy.\n\n\nTry It Yourself\nClassify each:\n\n\\(T(n) = 5n + 10\\)\n\\(T(n) = n \\log n + 100\\)\n\\(T(n) = n^3 + 4n^2\\)\n\\(T(n) = 2^n\\)\n\nIdentify their Big-O class and interpret feasibility.\n\n\nTest Cases\n\n\n\n\\(T(n)\\)\nClass\nDescription\n\n\n\n\n\\(7n + 3\\)\n\\(O(n)\\)\nLinear\n\n\n\\(3n^2 + 10n\\)\n\\(O(n^2)\\)\nQuadratic\n\n\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\nLog-linear\n\n\n\\(2^n\\)\n\\(O(2^n)\\)\nExponential\n\n\n\\(100\\)\n\\(O(1)\\)\nConstant\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nClassification\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Complexity Class Identifier is your map of the algorithmic universe, helping you locate where your code stands, from calm constant time to the roaring infinity of factorial growth.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundatmemtions of Algorithms</span>"
    ]
  }
]