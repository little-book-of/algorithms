[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Algorithms",
    "section": "",
    "text": "Content\nA Friendly Guide from Numbers to Neural Networks\n\nDownload PDF - print-ready\nDownload EPUB - e-reader friendly\nView LaTex - .tex source\nSource code (Github) - Markdown source\nRead on GitHub Pages - view online\n\nLicensed under CC BY-NC-SA 4.0.\n\nChapter 1. Foundations of Algorithms\n\n\nWhat Is an Algorithm?\n\n\n\nMeasuring Time and Space\n\n\n\nBig-O, Big-Theta, Big-Omega\n\n\n\nAlgorithmic Paradigms (Greedy, Divide and Conquer, DP)\n\n\n\nRecurrence Relations\n\n\n\nSearching Basics\n\n\n\nSorting Basics\n\n\n\nData Structures Overview\n\n\n\nGraphs and Trees Overview\n\n\n\nAlgorithm Design Patterns\n\n\n\n\nChapter 2. Sorting and Searching\n\n\nElementary Sorting (Bubble, Insertion, Selection)\n\n\n\nDivide-and-Conquer Sorting (Merge, Quick, Heap)\n\n\n\nCounting and Distribution Sorts (Counting, Radix, Bucket)\n\n\n\nHybrid Sorts (IntroSort, Timsort)\n\n\n\nSpecial Sorts (Cycle, Gnome, Comb, Pancake)\n\n\n\nLinear and Binary Search\n\n\n\nInterpolation and Exponential Search\n\n\n\nSelection Algorithms (Quickselect, Median of Medians)\n\n\n\nRange Searching and Nearest Neighbor\n\n\n\nSearch Optimizations and Variants\n\n\n\n\nChapter 3. Data Structures in Action\n\n\nArrays, Linked Lists, Stacks, Queues\n\n\n\nHash Tables and Variants (Cuckoo, Robin Hood, Consistent)\n\n\n\nHeaps (Binary, Fibonacci, Pairing)\n\n\n\nBalanced Trees (AVL, Red-Black, Splay, Treap)\n\n\n\nSegment Trees and Fenwick Trees\n\n\n\nDisjoint Set Union (Union-Find)\n\n\n\nProbabilistic Data Structures (Bloom, Count-Min, HyperLogLog)\n\n\n\nSkip Lists and B-Trees\n\n\n\nPersistent and Functional Data Structures\n\n\n\nAdvanced Trees and Range Queries\n\n\n\n\nChapter 4. Graph Algorithms\n\n\nTraversals (DFS, BFS, Iterative Deepening)\n\n\n\nStrongly Connected Components (Tarjan, Kosaraju)\n\n\n\nShortest Paths (Dijkstra, Bellman-Ford, A*, Johnson)\n\n\n\nShortest Path Variants (0–1 BFS, Bidirectional, Heuristic A*)\n\n\n\nMinimum Spanning Trees (Kruskal, Prim, Borůvka)\n\n\n\nFlows (Ford–Fulkerson, Edmonds–Karp, Dinic)\n\n\n\nCuts (Stoer–Wagner, Karger, Gomory–Hu)\n\n\n\nMatchings (Hopcroft–Karp, Hungarian, Blossom)\n\n\n\nTree Algorithms (LCA, HLD, Centroid Decomposition)\n\n\n\nAdvanced Graph Algorithms and Tricks\n\n\n\n\nChapter 5. Dynamic Programming\n\n\nDP Basics and State Transitions\n\n\n\nClassic Problems (Knapsack, Subset Sum, Coin Change)\n\n\n\nSequence Problems (LIS, LCS, Edit Distance)\n\n\n\nMatrix and Chain Problems\n\n\n\nBitmask DP and Traveling Salesman\n\n\n\nDigit DP and SOS DP\n\n\n\nDP Optimizations (Divide & Conquer, Convex Hull Trick, Knuth)\n\n\n\nTree DP and Rerooting\n\n\n\nDP Reconstruction and Traceback\n\n\n\nMeta-DP and Optimization Templates\n\n\n\n\nChapter 6. Mathematics for Algorithms\n\n\nNumber Theory (GCD, Modular Arithmetic, CRT)\n\n\n\nPrimality and Factorization (Miller–Rabin, Pollard Rho)\n\n\n\nCombinatorics (Permutations, Combinations, Subsets)\n\n\n\nProbability and Randomized Algorithms\n\n\n\nSieve Methods and Modular Math\n\n\n\nLinear Algebra (Gaussian Elimination, LU, SVD)\n\n\n\nFFT and NTT (Fast Transforms)\n\n\n\nNumerical Methods (Newton, Simpson, Runge–Kutta)\n\n\n\nMathematical Optimization (Simplex, Gradient, Convex)\n\n\n\nAlgebraic Tricks and Transform Techniques\n\n\n\n\nChapter 7. Strings and Text Algorithms\n\n\nString Matching (KMP, Z, Rabin–Karp, Boyer–Moore)\n\n\n\nMulti-Pattern Search (Aho–Corasick)\n\n\n\nSuffix Structures (Suffix Array, Suffix Tree, LCP)\n\n\n\nPalindromes and Periodicity (Manacher)\n\n\n\nEdit Distance and Alignment\n\n\n\nCompression (Huffman, Arithmetic, LZ77, BWT)\n\n\n\nCryptographic Hashes and Checksums\n\n\n\nApproximate and Streaming Matching\n\n\n\nBioinformatics Alignment (Needleman–Wunsch, Smith–Waterman)\n\n\n\nText Indexing and Search Structures\n\n\n\n\nChapter 8. Geometry, Graphics, and Spatial Algorithms\n\n\nConvex Hull (Graham, Andrew, Chan)\n\n\n\nClosest Pair and Segment Intersection\n\n\n\nLine Sweep and Plane Sweep Algorithms\n\n\n\nDelaunay and Voronoi Diagrams\n\n\n\nPoint in Polygon and Polygon Triangulation\n\n\n\nSpatial Data Structures (KD, R-tree)\n\n\n\nRasterization and Scanline Techniques\n\n\n\nComputer Vision (Canny, Hough, SIFT)\n\n\n\nPathfinding in Space (A*, RRT, PRM)\n\n\n\nComputational Geometry Variants and Applications\n\n\n\n\nChapter 9. Systems, Databases, and Distributed Algorithms\n\n\nConcurrency Control (2PL, MVCC, OCC)\n\n\n\nLogging, Recovery, and Commit Protocols\n\n\n\nScheduling (Round Robin, EDF, Rate-Monotonic)\n\n\n\nCaching and Replacement (LRU, LFU, CLOCK)\n\n\n\nNetworking (Routing, Congestion Control)\n\n\n\nDistributed Consensus (Paxos, Raft, PBFT)\n\n\n\nLoad Balancing and Rate Limiting\n\n\n\nSearch and Indexing (Inverted, BM25, WAND)\n\n\n\nCompression and Encoding in Systems\n\n\n\nFault Tolerance and Replication\n\n\n\n\nChapter 10. AI, ML, and Optimization\n\n\nClassical ML (k-means, Naive Bayes, SVM, Decision Trees)\n\n\n\nEnsemble Methods (Bagging, Boosting, Random Forests)\n\n\n\nGradient Methods (SGD, Adam, RMSProp)\n\n\n\nDeep Learning (Backpropagation, Dropout, Normalization)\n\n\n\nSequence Models (Viterbi, Beam Search, CTC)\n\n\n\nMetaheuristics (GA, SA, PSO, ACO)\n\n\n\nReinforcement Learning (Q-learning, Policy Gradients)\n\n\n\nApproximation and Online Algorithms\n\n\n\nFairness, Causal Inference, and Robust Optimization\n\n\n\nAI Planning, Search, and Learning Systems",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Content</span>"
    ]
  },
  {
    "objectID": "books/en-us/cheatsheet.html",
    "href": "books/en-us/cheatsheet.html",
    "title": "The Cheatsheet",
    "section": "",
    "text": "Page 1. Big Picture and Complexity\nA quick reference for understanding algorithms, efficiency, and growth rates. Keep this sheet beside you as you read or code.\n\nWhat Is an Algorithm?\nAn algorithm is a clear, step-by-step process that solves a problem.\n\n\n\nProperty\nDescription\n\n\n\n\nPrecise\nEach step is unambiguous\n\n\nFinite\nMust stop after a certain number of steps\n\n\nEffective\nEvery step is doable by machine or human\n\n\nDeterministic\nSame input, same output (usually)\n\n\n\nThink of it like a recipe:\n\nInput: ingredients\nSteps: instructions\nOutput: final dish\n\n\n\nCore Qualities\n\n\n\nConcept\nQuestion to Ask\n\n\n\n\nCorrectness\nDoes it always solve the problem\n\n\nTermination\nDoes it eventually stop\n\n\nComplexity\nHow much time and space it needs\n\n\nClarity\nIs it easy to understand and implement\n\n\n\n\n\nWhy Complexity Matters\nDifferent algorithms grow differently as input size \\(n\\) increases.\n\n\n\nGrowth Rate\nExample Algorithm\nEffect When \\(n\\) Doubles\n\n\n\n\n\\(O(1)\\)\nHash lookup\nNo change\n\n\n\\(O(\\log n)\\)\nBinary search\nSlight increase\n\n\n\\(O(n)\\)\nLinear scan\nDoubled\n\n\n\\(O(n\\log n)\\)\nMerge sort\nSlightly more than 2×\n\n\n\\(O(n^2)\\)\nBubble sort\nQuadrupled\n\n\n\\(O(2^n)\\)\nSubset generation\nExplodes\n\n\n\\(O(n!)\\)\nBrute-force permutations\nUnusable beyond \\(n=10\\)\n\n\n\n\n\nMeasuring Time and Space\n\n\n\n\n\n\n\n\nMeasure\nMeaning\nExample\n\n\n\n\nTime Complexity\nNumber of operations\nLoop from 1 to \\(n\\): \\(O(n)\\)\n\n\nSpace Complexity\nMemory usage (stack, heap, data structures)\nRecursive call depth: \\(O(n)\\)\n\n\n\nSimple rules:\n\nSequential steps: sum of costs\nNested loops: product of sizes\nRecursion: use recurrence relations\n\n\n\nCommon Patterns\n\n\n\nPattern\nCost Formula\nComplexity\n\n\n\n\nSingle Loop (1 to \\(n\\))\n\\(T(n) = n\\)\n\\(O(n)\\)\n\n\nNested Loops (\\(n \\times n\\))\n\\(T(n) = n^2\\)\n\\(O(n^2)\\)\n\n\nHalving Each Step\n\\(T(n) = \\log_2 n\\)\n\\(O(\\log n)\\)\n\n\nDivide and Conquer (2 halves)\n\\(T(n) = 2T(n/2) + n\\)\n\\(O(n\\log n)\\)\n\n\n\n\n\nDoubling Rule\nRun algorithm for \\(n\\) and \\(2n\\):\n\n\n\nObservation\nLikely Complexity\n\n\n\n\nConstant time\n\\(O(1)\\)\n\n\nTime doubles\n\\(O(n)\\)\n\n\nTime quadruples\n\\(O(n^2)\\)\n\n\nTime × log factor\n\\(O(n\\log n)\\)\n\n\n\n\n\nTiny Code: Binary Search\ndef binary_search(arr, x):\n    lo, hi = 0, len(arr) - 1\n    while lo &lt;= hi:\n        mid = (lo + hi) // 2\n        if arr[mid] == x:\n            return mid\n        elif arr[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid - 1\n    return -1\nComplexity: \\[T(n) = T(n/2) + 1 \\Rightarrow O(\\log n)\\]\n\n\nCommon Pitfalls\n\n\n\n\n\n\n\nIssue\nTip\n\n\n\n\nOff-by-one error\nCheck loop bounds carefully\n\n\nInfinite loop\nEnsure termination condition is reachable\n\n\nMidpoint overflow (C/C++)\nUse mid = lo + (hi - lo) / 2\n\n\nUnsorted data in search\nBinary search only works on sorted input\n\n\n\n\n\nQuick Growth Summary\n\n\n\nType\nFormula Example\nDescription\n\n\n\n\nConstant\n\\(1\\)\nFixed time\n\n\nLogarithmic\n\\(\\log n\\)\nDivide each time\n\n\nLinear\n\\(n\\)\nStep through all items\n\n\nLinearithmic\n\\(n \\log n\\)\nSort-like complexity\n\n\nQuadratic\n\\(n^2\\)\nDouble loop\n\n\nCubic\n\\(n^3\\)\nTriple nested loops\n\n\nExponential\n\\(2^n\\)\nAll subsets\n\n\nFactorial\n\\(n!\\)\nAll permutations\n\n\n\n\n\nSimple Rule of Thumb\nTrace small examples by hand. Count steps, memory, and recursion depth. You’ll see how growth behaves before running code.\n\n\n\nPage 2. Recurrences and Master Theorem\nThis page helps you break down recursive algorithms and estimate their runtime using recurrences.\n\nWhat Is a Recurrence?\nA recurrence relation expresses a problem’s cost \\(T(n)\\) in terms of smaller subproblems.\nTypical structure:\n\\[\nT(n) = a , T\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nwhere:\n\n\\(a\\) = number of subproblems\n\\(b\\) = factor by which input shrinks\n\\(f(n)\\) = extra work per call (merge, combine, etc.)\n\n\n\nCommon Recurrences\n\n\n\nAlgorithm\nRecurrence Form\nSolution\n\n\n\n\nBinary Search\n\\(T(n)=T(n/2)+1\\)\n\\(O(\\log n)\\)\n\n\nMerge Sort\n\\(T(n)=2T(n/2)+n\\)\n\\(O(n\\log n)\\)\n\n\nQuick Sort (avg)\n\\(T(n)=2T(n/2)+O(n)\\)\n\\(O(n\\log n)\\)\n\n\nQuick Sort (worst)\n\\(T(n)=T(n-1)+O(n)\\)\n\\(O(n^2)\\)\n\n\nMatrix Multiply\n\\(T(n)=8T(n/2)+O(n^2)\\)\n\\(O(n^3)\\)\n\n\nKaratsuba\n\\(T(n)=3T(n/2)+O(n)\\)\n\\(O(n^{\\log_2 3})\\)\n\n\n\n\n\nSolving Recurrences\nThere are several methods to solve them:\n\n\n\n\n\n\n\n\nMethod\nDescription\nBest For\n\n\n\n\nIteration\nExpand step by step\nSimple recurrences\n\n\nSubstitution\nGuess and prove with induction\nVerification\n\n\nRecursion Tree\nVisualize total work per level\nDivide and conquer\n\n\nMaster Theorem\nShortcut for \\(T(n)=aT(n/b)+f(n)\\)\nStandard forms\n\n\n\n\n\nThe Master Theorem\nGiven \\[T(n) = aT(n/b) + f(n)\\]\nLet \\[n^{\\log_b a}\\] be the “critical term”\n\n\n\n\n\n\n\n\nCase\nCondition\nResult\n\n\n\n\n1\nIf \\(f(n) = O(n^{\\log_b a - \\varepsilon})\\)\n\\(T(n) = \\Theta(n^{\\log_b a})\\)\n\n\n2\nIf \\(f(n) = \\Theta(n^{\\log_b a}\\log^k n)\\)\n\\(T(n) = \\Theta(n^{\\log_b a}\\log^{k+1} n)\\)\n\n\n3\nIf \\(f(n) = \\Omega(n^{\\log_b a + \\varepsilon})\\) and regularity holds\n\\(T(n) = \\Theta(f(n))\\)\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\n\\(a\\)\n\\(b\\)\n\\(f(n)\\)\nCase\n\\(T(n)\\)\n\n\n\n\nMerge Sort\n2\n2\n\\(n\\)\n2\n\\(\\Theta(n\\log n)\\)\n\n\nBinary Search\n1\n2\n\\(1\\)\n1\n\\(\\Theta(\\log n)\\)\n\n\nStrassen Multiply\n7\n2\n\\(n^2\\)\n2\n\\(\\Theta(n^{\\log_2 7})\\)\n\n\nQuick Sort (avg)\n2\n2\n\\(n\\)\n2\n\\(\\Theta(n\\log n)\\)\n\n\n\n\n\nRecursion Tree Visualization\nBreak cost into levels:\nExample: \\(T(n)=2T(n/2)+n\\)\n\n\n\nLevel\n#Nodes\nWork per Node\nTotal Work\n\n\n\n\n0\n1\n\\(n\\)\n\\(n\\)\n\n\n1\n2\n\\(n/2\\)\n\\(n\\)\n\n\n2\n4\n\\(n/4\\)\n\\(n\\)\n\n\n…\n…\n…\n…\n\n\n\nSum across \\(\\log_2 n\\) levels:\n\\[T(n) = n \\log_2 n\\]\n\n\nTiny Code: Fast Exponentiation\nCompute \\(a^n\\) efficiently.\ndef power(a, n):\n    res = 1\n    while n &gt; 0:\n        if n % 2 == 1:\n            res *= a\n        a *= a\n        n //= 2\n    return res\nRecurrence:\n\\[T(n) = T(n/2) + O(1) \\Rightarrow O(\\log n)\\]\n\n\nIteration Method Example\nSolve \\(T(n)=T(n/2)+n\\)\nExpand:\n\\[\n\\begin{aligned}\nT(n) &= T(n/2) + n \\\n&= T(n/4) + n/2 + n \\\n&= T(n/8) + n/4 + n/2 + n \\\n&= \\ldots + n(1 + 1/2 + 1/4 + \\ldots) \\\n&= O(n)\n\\end{aligned}\n\\]\n\n\nCommon Forms\n\n\n\nForm\nResult\n\n\n\n\n\\(T(n)=T(n-1)+O(1)\\)\n\\(O(n)\\)\n\n\n\\(T(n)=T(n/2)+O(1)\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n)=2T(n/2)+O(1)\\)\n\\(O(n)\\)\n\n\n\\(T(n)=2T(n/2)+O(n)\\)\n\\(O(n\\log n)\\)\n\n\n\\(T(n)=T(n/2)+O(n)\\)\n\\(O(n)\\)\n\n\n\n\n\nQuick Checklist\n\nIdentify \\(a\\), \\(b\\), and \\(f(n)\\)\nCompare \\(f(n)\\) to \\(n^{\\log_b a}\\)\nApply correct case\nConfirm assumptions (regularity)\nState final complexity\n\nUnderstanding recurrences helps you estimate performance before coding. Always look for subproblem count, size, and merge cost.\n\n\n\nPage 3. Sorting at a Glance\nSorting is one of the most common algorithmic tasks. This page helps you quickly compare sorting methods, their complexity, stability, and when to use them.\n\nWhy Sorting Matters\nSorting organizes data so that searches, merges, and analyses become efficient. Many problems become simpler once the input is sorted.\n\n\nQuick Comparison Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nSpace\nStable\nIn-Place\nNotes\n\n\n\n\nBubble Sort\n\\(O(n)\\)\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(1)\\)\nYes\nYes\nSimple, educational\n\n\nSelection Sort\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(1)\\)\nNo\nYes\nFew swaps\n\n\nInsertion Sort\n\\(O(n)\\)\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\\(O(1)\\)\nYes\nYes\nGreat for small/partial sort\n\n\nMerge Sort\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n)\\)\nYes\nNo\nStable, divide and conquer\n\n\nQuick Sort\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n^2)\\)\n\\(O(\\log n)\\)\nNo\nYes\nFast average, in place\n\n\nHeap Sort\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(n\\log n)\\)\n\\(O(1)\\)\nNo\nYes\nNot stable\n\n\nCounting Sort\n\\(O(n+k)\\)\n\\(O(n+k)\\)\n\\(O(n+k)\\)\n\\(O(n+k)\\)\nYes\nNo\nInteger keys only\n\n\nRadix Sort\n\\(O(d(n+k))\\)\n\\(O(d(n+k))\\)\n\\(O(d(n+k))\\)\n\\(O(n+k)\\)\nYes\nNo\nSort by digits\n\n\nBucket Sort\n\\(O(n+k)\\)\n\\(O(n+k)\\)\n\\(O(n^2)\\)\n\\(O(n)\\)\nYes\nNo\nUniform distribution needed\n\n\n\n\n\nChoosing a Sorting Algorithm\n\n\n\nSituation\nBest Choice\n\n\n\n\nSmall array or nearly sorted data\nInsertion Sort\n\n\nStable required, general case\nMerge Sort or Timsort\n\n\nIn-place and fast on average\nQuick Sort\n\n\nGuarantee worst-case \\(O(n\\log n)\\)\nHeap Sort\n\n\nSmall integer keys or limited range\nCounting or Radix\n\n\nExternal sorting (large data)\nExternal Merge Sort\n\n\n\n\n\nTiny Code: Insertion Sort\nSimple and intuitive for beginners.\ndef insertion_sort(a):\n    for i in range(1, len(a)):\n        key = a[i]\n        j = i - 1\n        while j &gt;= 0 and a[j] &gt; key:\n            a[j + 1] = a[j]\n            j -= 1\n        a[j + 1] = key\n    return a\nComplexity: \\[T(n) = O(n^2)\\] average, \\[O(n)\\] best (already sorted)\n\n\nDivide and Conquer Sorts\n\nMerge Sort\nSplits list, sorts halves, merges results.\nRecurrence: \\[T(n) = 2T(n/2) + O(n) = O(n\\log n)\\]\nTiny Code:\ndef merge_sort(a):\n    if len(a) &lt;= 1:\n        return a\n    mid = len(a)//2\n    L = merge_sort(a[:mid])\n    R = merge_sort(a[mid:])\n    i = j = 0\n    res = []\n    while i &lt; len(L) and j &lt; len(R):\n        if L[i] &lt;= R[j]:\n            res.append(L[i]); i += 1\n        else:\n            res.append(R[j]); j += 1\n    res.extend(L[i:]); res.extend(R[j:])\n    return res\n\n\nQuick Sort\nPick pivot, partition, sort subarrays.\nRecurrence: \\[T(n) = T(k) + T(n-k-1) + O(n)\\] Average case: \\[O(n\\log n)\\] Worst case: \\[O(n^2)\\]\nTiny Code:\ndef quick_sort(a):\n    if len(a) &lt;= 1:\n        return a\n    pivot = a[len(a)//2]\n    left  = [x for x in a if x &lt; pivot]\n    mid   = [x for x in a if x == pivot]\n    right = [x for x in a if x &gt; pivot]\n    return quick_sort(left) + mid + quick_sort(right)\n\n\n\nStable vs Unstable\n\n\n\n\n\n\n\n\nProperty\nDescription\nExample\n\n\n\n\nStable\nEqual elements keep original order\nMerge Sort, Insertion\n\n\nUnstable\nMay reorder equal elements\nQuick, Heap\n\n\n\n\n\nVisualization Tips\n\n\n\nPattern\nDescription\n\n\n\n\nBubble\nCompare and swap adjacent\n\n\nSelection\nSelect min each pass\n\n\nInsertion\nGrow sorted region step by step\n\n\nMerge\nDivide, conquer, merge\n\n\nQuick\nPartition and recurse\n\n\nHeap\nBuild heap, extract repeatedly\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\n\nType\nCategory\nComplexity\nStable\nSpace\n\n\n\n\nSimple\nBubble, Selection\n\\(O(n^2)\\)\nVaries\n\\(O(1)\\)\n\n\nInsertion\nIncremental\n\\(O(n^2)\\)\nYes\n\\(O(1)\\)\n\n\nDivide/Conquer\nMerge, Quick\n\\(O(n\\log n)\\)\nMerge yes\nMerge no\n\n\nDistribution\nCounting, Radix\n\\(O(n+k)\\)\nYes\n\\(O(n+k)\\)\n\n\nHybrid\nTimsort, IntroSort\n\\(O(n\\log n)\\)\nYes\nVaries\n\n\n\nWhen in doubt, start with Timsort (Python) or std::sort (C++) which adapt dynamically.\n\n\n\nPage 4. Searching and Selection\nSearching means finding what you need from a collection. Selection means picking specific elements such as the smallest, largest, or k-th element. This page summarizes both.\n\nSearching Basics\n\n\n\n\n\n\n\n\n\nType\nDescription\nData Requirement\nComplexity\n\n\n\n\nLinear Search\nCheck one by one\nNone\n\\(O(n)\\)\n\n\nBinary Search\nDivide range by 2 each step\nSorted\n\\(O(\\log n)\\)\n\n\nJump Search\nSkip ahead fixed steps\nSorted\n\\(O(\\sqrt n)\\)\n\n\nInterpolation\nGuess position based on value\nSorted, uniform\n\\(O(\\log\\log n)\\) avg\n\n\nExponential\nExpand window, then binary search\nSorted\n\\(O(\\log n)\\)\n\n\n\n\n\nLinear Search\nSimple but slow for large inputs.\ndef linear_search(a, x):\n    for i, v in enumerate(a):\n        if v == x:\n            return i\n    return -1\nComplexity: \\[T(n) = O(n)\\]\n\n\nBinary Search\nFast on sorted lists.\ndef binary_search(a, x):\n    lo, hi = 0, len(a) - 1\n    while lo &lt;= hi:\n        mid = (lo + hi) // 2\n        if a[mid] == x:\n            return mid\n        elif a[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid - 1\n    return -1\nComplexity: \\[T(n) = T(n/2) + 1 \\Rightarrow O(\\log n)\\]\n\n\nBinary Search Variants\n\n\n\n\n\n\n\n\nVariant\nGoal\nReturn Value\n\n\n\n\nLower Bound\nFirst index where \\(a[i] \\ge x\\)\nPosition of first ≥ x\n\n\nUpper Bound\nFirst index where \\(a[i] &gt; x\\)\nPosition of first &gt; x\n\n\nCount Range\nupper_bound - lower_bound\nCount of \\(x\\) in sorted array\n\n\n\n\n\nCommon Binary Search Pitfalls\n\n\n\nProblem\nFix\n\n\n\n\nInfinite loop\nUpdate bounds correctly\n\n\nOff-by-one\nCheck mid inclusion carefully\n\n\nUnsuitable for unsorted data\nSort or use hash-based search\n\n\nOverflow (C/C++)\nmid = lo + (hi - lo) / 2\n\n\n\n\n\nExponential Search\nUsed for unbounded or large sorted lists.\n\nCheck positions \\(1, 2, 4, 8, ...\\) until \\(a[i] \\ge x\\)\nBinary search in last found interval\n\nComplexity: \\[O(\\log n)\\]\n\n\nSelection Problems\nFind the \\(k\\)-th smallest or largest element.\n\n\n\n\n\n\n\n\n\nTask\nExample Use Case\nAlgorithm\nComplexity\n\n\n\n\nMin / Max\nSmallest / largest element\nLinear Scan\n\\(O(n)\\)\n\n\nk-th Smallest\nOrder statistic\nQuickselect\nAvg \\(O(n)\\)\n\n\nMedian\nMiddle element\nQuickselect\nAvg \\(O(n)\\)\n\n\nTop-k Elements\nPartial sort\nHeap / Partition\n\\(O(n\\log k)\\)\n\n\nMedian of Medians\nWorst-case linear selection\nDeterministic\n\\(O(n)\\)\n\n\n\n\n\nTiny Code: Quickselect (k-th smallest)\nimport random\n\ndef quickselect(a, k):\n    if len(a) == 1:\n        return a[0]\n    pivot = random.choice(a)\n    left  = [x for x in a if x &lt; pivot]\n    mid   = [x for x in a if x == pivot]\n    right = [x for x in a if x &gt; pivot]\n\n    if k &lt; len(left):\n        return quickselect(left, k)\n    elif k &lt; len(left) + len(mid):\n        return pivot\n    else:\n        return quickselect(right, k - len(left) - len(mid))\nComplexity: Average \\(O(n)\\), Worst \\(O(n^2)\\)\n\n\nTiny Code: Lower Bound\ndef lower_bound(a, x):\n    lo, hi = 0, len(a)\n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if a[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid\n    return lo\n\n\nHash-Based Searching\nWhen order does not matter, hashing gives near constant lookup.\n\n\n\nOperation\nAverage\nWorst\n\n\n\n\nInsert\n\\(O(1)\\)\n\\(O(n)\\)\n\n\nSearch\n\\(O(1)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(1)\\)\n\\(O(n)\\)\n\n\n\nBest for large, unsorted collections.\n\n\nSummary Table\n\n\n\nScenario\nRecommended Approach\nComplexity\n\n\n\n\nSmall array\nLinear Search\n\\(O(n)\\)\n\n\nLarge, sorted array\nBinary Search\n\\(O(\\log n)\\)\n\n\nUnbounded range\nExponential Search\n\\(O(\\log n)\\)\n\n\nNeed k-th smallest element\nQuickselect\nAvg \\(O(n)\\)\n\n\nMany lookups\nHash Table\nAvg \\(O(1)\\)\n\n\n\n\n\nQuick Tips\n\nAlways check whether data is sorted before applying binary search.\nQuickselect is great when you only need the k-th element, not a full sort.\nUse hash maps for fast lookups on unsorted data.\n\n\n\n\nPage 5. Core Data Structures\nData structures organize data for efficient access and modification. Choosing the right one often makes an algorithm simple and fast.\n\nArrays and Lists\n\n\n\n\n\n\n\n\n\n\n\n\nStructure\nAccess\nSearch\nInsert End\nInsert Middle\nDelete\nNotes\n\n\n\n\nStatic Array\n\\(O(1)\\)\n\\(O(n)\\)\nN/A\n\\(O(n)\\)\n\\(O(n)\\)\nFixed size\n\n\nDynamic Array\n\\(O(1)\\)\n\\(O(n)\\)\nAmortized \\(O(1)\\)\n\\(O(n)\\)\n\\(O(n)\\)\nAuto-resizing\n\n\nLinked List (S)\n\\(O(n)\\)\n\\(O(n)\\)\n\\(O(1)\\) head\n\\(O(1)\\) if node known\n\\(O(1)\\) if node known\nSequential access\n\n\nLinked List (D)\n\\(O(n)\\)\n\\(O(n)\\)\n\\(O(1)\\) head/tail\n\\(O(1)\\) if node known\n\\(O(1)\\) if node known\nTwo-way traversal\n\n\n\n\nSingly linked lists: next pointer only\nDoubly linked lists: next and prev pointers\nDynamic arrays use doubling to grow capacity\n\n\n\nTiny Code: Dynamic Array Resize (Python-like)\ndef resize(arr, new_cap):\n    new = [None] * new_cap\n    for i in range(len(arr)):\n        new[i] = arr[i]\n    return new\nDoubling capacity keeps amortized append \\(O(1)\\).\n\n\nStacks and Queues\n\n\n\nStructure\nPush\nPop\nPeek\nNotes\n\n\n\n\nStack (LIFO)\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\nUndo operations, recursion\n\n\nQueue (FIFO)\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\nScheduling, BFS\n\n\nDeque\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\nInsert/remove both ends\n\n\n\n\n\nTiny Code: Stack\nstack = []\nstack.append(x)   # push\nx = stack.pop()   # pop\n\n\nTiny Code: Queue\nfrom collections import deque\n\nq = deque()\nq.append(x)   # enqueue\nx = q.popleft()  # dequeue\n\n\nPriority Queue (Heap)\nStores elements so the smallest (or largest) is always on top.\n\n\n\nOperation\nComplexity\n\n\n\n\nInsert\n\\(O(\\log n)\\)\n\n\nExtract min\n\\(O(\\log n)\\)\n\n\nPeek min\n\\(O(1)\\)\n\n\nBuild heap\n\\(O(n)\\)\n\n\n\nTiny Code:\nimport heapq\nheap = []\nheapq.heappush(heap, value)\nx = heapq.heappop(heap)\nHeaps are used in Dijkstra, Prim, and scheduling.\n\n\nHash Tables\n\n\n\nOperation\nAverage\nWorst\nNotes\n\n\n\n\nInsert\n\\(O(1)\\)\n\\(O(n)\\)\nHash collisions increase cost\n\n\nSearch\n\\(O(1)\\)\n\\(O(n)\\)\nGood hash + low load factor helps\n\n\nDelete\n\\(O(1)\\)\n\\(O(n)\\)\nUsually open addressing or chaining\n\n\n\nKey ideas:\n\nCompute index using hash function: index = hash(key) % capacity\nResolve collisions by chaining or probing\n\n\n\nTiny Code: Hash Map (Simplified)\ntable = [[] for _ in range(8)]\ndef put(key, value):\n    i = hash(key) % len(table)\n    for kv in table[i]:\n        if kv[0] == key:\n            kv[1] = value\n            return\n    table[i].append([key, value])\n\n\nSets\nA hash-based collection of unique elements.\n\n\n\nOperation\nAverage Complexity\n\n\n\n\nAdd\n\\(O(1)\\)\n\n\nSearch\n\\(O(1)\\)\n\n\nRemove\n\\(O(1)\\)\n\n\n\nUsed for membership checks and duplicates removal.\n\n\nUnion-Find (Disjoint Set)\nKeeps track of connected components. Two main operations:\n\nfind(x): get representative of x\nunion(a,b): merge sets of a and b\n\nWith path compression + union by rank → nearly \\(O(1)\\).\nTiny Code:\nclass DSU:\n    def __init__(self, n):\n        self.p = list(range(n))\n        self.r = [0]*n\n    def find(self, x):\n        if self.p[x] != x:\n            self.p[x] = self.find(self.p[x])\n        return self.p[x]\n    def union(self, a, b):\n        ra, rb = self.find(a), self.find(b)\n        if ra == rb: return\n        if self.r[ra] &lt; self.r[rb]: ra, rb = rb, ra\n        self.p[rb] = ra\n        if self.r[ra] == self.r[rb]:\n            self.r[ra] += 1\n\n\nSummary Table\n\n\n\nCategory\nStructure\nUse Case\n\n\n\n\nSequence\nArray, List\nOrdered data\n\n\nLIFO/FIFO\nStack, Queue\nRecursion, scheduling\n\n\nPriority\nHeap\nBest-first selection, PQ problems\n\n\nHash-based\nHash Table, Set\nFast lookups, uniqueness\n\n\nConnectivity\nUnion-Find\nGraph components, clustering\n\n\n\n\n\nQuick Tips\n\nChoose array when random access matters.\nChoose list when insertions/deletions frequent.\nChoose stack or queue for control flow.\nChoose heap for priority.\nChoose hash table for constant lookups.\nChoose DSU for disjoint sets or graph merging.\n\n\n\n\nPage 6. Graphs Quick Use\nGraphs model connections between objects. They appear everywhere: maps, networks, dependencies, and systems. This page gives you a compact view of common graph algorithms.\n\nGraph Basics\nA graph has vertices (nodes) and edges (connections).\n\n\n\nType\nDescription\n\n\n\n\nUndirected\nEdges go both ways\n\n\nDirected (Digraph)\nEdges have direction\n\n\nWeighted\nEdges carry cost or distance\n\n\nUnweighted\nAll edges cost 1\n\n\n\n\n\nRepresentations\n\n\n\n\n\n\n\n\n\nRepresentation\nSpace\nBest For\nNotes\n\n\n\n\nAdjacency List\n\\(O(V+E)\\)\nSparse graphs\nCommon in practice\n\n\nAdjacency Matrix\n\\(O(V^2)\\)\nDense graphs\nConstant-time edge lookup\n\n\nEdge List\n\\(O(E)\\)\nEdge-based algorithms\nEasy to iterate over edges\n\n\n\nAdjacency List Example (Python):\ngraph = {\n    0: [(1, 2), (2, 5)],\n    1: [(2, 1)],\n    2: []\n}\nEach tuple (neighbor, weight) represents an edge.\n\n\nTraversals\n\nBreadth-First Search (BFS)\nVisits layer by layer (good for shortest paths in unweighted graphs).\nfrom collections import deque\ndef bfs(adj, s):\n    dist = {s: 0}\n    q = deque([s])\n    while q:\n        u = q.popleft()\n        for v in adj[u]:\n            if v not in dist:\n                dist[v] = dist[u] + 1\n                q.append(v)\n    return dist\nComplexity: \\(O(V+E)\\)\n\n\nDepth-First Search (DFS)\nExplores deeply before backtracking.\ndef dfs(adj, u, visited):\n    visited.add(u)\n    for v in adj[u]:\n        if v not in visited:\n            dfs(adj, v, visited)\nComplexity: \\(O(V+E)\\)\n\n\n\nShortest Path Algorithms\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorks On\nNegative Edges\nComplexity\nNotes\n\n\n\n\nBFS\nUnweighted\nNo\n\\(O(V+E)\\)\nShortest hops\n\n\nDijkstra\nWeighted (nonneg)\nNo\n\\(O((V+E)\\log V)\\)\nUses priority queue\n\n\nBellman-Ford\nWeighted\nYes\n\\(O(VE)\\)\nDetects negative cycles\n\n\nFloyd-Warshall\nAll pairs\nYes\n\\(O(V^3)\\)\nDP approach\n\n\n\n\n\nTiny Code: Dijkstra’s Algorithm\nimport heapq\n\ndef dijkstra(adj, s):\n    INF = 1018\n    dist = [INF] * len(adj)\n    dist[s] = 0\n    pq = [(0, s)]\n    while pq:\n        d, u = heapq.heappop(pq)\n        if d != dist[u]: \n            continue\n        for v, w in adj[u]:\n            nd = d + w\n            if nd &lt; dist[v]:\n                dist[v] = nd\n                heapq.heappush(pq, (nd, v))\n    return dist\n\n\nTopological Sort (DAGs only)\nOrders nodes so every edge \\((u,v)\\) goes from earlier to later.\n\n\n\nMethod\nIdea\nComplexity\n\n\n\n\nDFS-based\nPost-order stack reversal\n\\(O(V+E)\\)\n\n\nKahn’s Algo\nRemove nodes with indegree 0\n\\(O(V+E)\\)\n\n\n\n\n\nMinimum Spanning Tree (MST)\nConnect all nodes with minimum total weight.\n\n\n\n\n\n\n\n\n\nAlgorithm\nIdea\nComplexity\nNotes\n\n\n\n\nKruskal\nSort edges, use Union-Find\n\\(O(E\\log E)\\)\nWorks well with edge list\n\n\nPrim\nGrow tree using PQ\n\\(O(E\\log V)\\)\nStarts from any vertex\n\n\n\n\n\nTiny Code: Kruskal MST\ndef kruskal(edges, n):\n    parent = list(range(n))\n    def find(x):\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n    res = 0\n    for w, u, v in sorted(edges):\n        ru, rv = find(u), find(v)\n        if ru != rv:\n            res += w\n            parent[rv] = ru\n    return res\n\n\nStrongly Connected Components (SCC)\nSubsets where every node can reach every other. Use Kosaraju or Tarjan algorithm, both \\(O(V+E)\\).\n\n\nCycle Detection\n\n\n\nGraph Type\nMethod\nNotes\n\n\n\n\nUndirected\nDFS with parent\nEdge to non-parent visited\n\n\nDirected\nDFS with color/state\nBack edge found = cycle\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nTask\nAlgorithm\nComplexity\nNotes\n\n\n\n\nVisit all nodes\nDFS / BFS\n\\(O(V+E)\\)\nTraversal\n\n\nShortest path (unweighted)\nBFS\n\\(O(V+E)\\)\nCounts edges\n\n\nShortest path (weighted)\nDijkstra\n\\(O(E\\log V)\\)\nNo negative weights\n\n\nNegative edges allowed\nBellman-Ford\n\\(O(VE)\\)\nDetects negative cycles\n\n\nAll-pairs shortest path\nFloyd-Warshall\n\\(O(V^3)\\)\nDP matrix\n\n\nMST\nKruskal / Prim\n\\(O(E\\log V)\\)\nMinimal connection cost\n\n\nDAG order\nTopological Sort\n\\(O(V+E)\\)\nOnly for DAGs\n\n\n\n\n\nQuick Tips\n\nUse BFS for shortest path in unweighted graphs.\nUse Dijkstra if weights are nonnegative.\nUse Union-Find for Kruskal MST.\nUse Topological Sort for dependency resolution.\nAlways check for negative edges before using Dijkstra.\n\n\n\n\nPage 7. Dynamic Programming Quick Use\nDynamic Programming (DP) is about solving big problems by breaking them into overlapping subproblems and reusing their solutions. This page helps you see patterns quickly.\n\nWhen to Use DP\nYou can usually apply DP if:\n\n\n\nSymptom\nMeaning\n\n\n\n\nOptimal Substructure\nBest solution uses best of subparts\n\n\nOverlapping Subproblems\nSame subresults appear again\n\n\nDecision + Recurrence\nState transitions can be defined\n\n\n\n\n\nDP Styles\n\n\n\n\n\n\n\n\nStyle\nDescription\nExample\n\n\n\n\nTop-down (Memo)\nRecursion + cache results\nFibonacci with memoization\n\n\nBottom-up (Tabular)\nIterative fill table\nKnapsack table\n\n\nSpace-optimized\nReuse previous row/state\nRolling arrays\n\n\n\n\n\nFibonacci Example\nRecurrence: \\[F(n)=F(n-1)+F(n-2),\\quad F(0)=0,F(1)=1\\]\n\nTop-down (Memoization)\ndef fib(n, memo={}):\n    if n &lt;= 1:\n        return n\n    if n not in memo:\n        memo[n] = fib(n-1, memo) + fib(n-2, memo)\n    return memo[n]\n\n\nBottom-up (Tabulation)\ndef fib(n):\n    dp = [0, 1]\n    for i in range(2, n + 1):\n        dp.append(dp[i-1] + dp[i-2])\n    return dp[n]\n\n\n\nSteps to Solve DP Problems\n\nDefine State Example: \\(dp[i]\\) = best answer for first \\(i\\) items\nDefine Transition Example: \\(dp[i]=\\max(dp[i-1], value[i]+dp[i-weight[i]])\\)\nSet Base Cases Example: \\(dp[0]=0\\)\nChoose Order Bottom-up or Top-down\nReturn Answer Often \\(dp[n]\\) or \\(dp[target]\\)\n\n\n\nCommon DP Categories\n\n\n\n\n\n\n\n\nCategory\nExample Problems\nState Form\n\n\n\n\nSequence\nLIS, LCS, Edit Distance\n\\(dp[i][j]\\) over prefixes\n\n\nSubset\nKnapsack, Subset Sum\n\\(dp[i][w]\\) capacity-based\n\n\nPartition\nPalindrome Partitioning, Equal Sum\n\\(dp[i]\\) cut-based\n\n\nGrid\nMin Path Sum, Unique Paths\n\\(dp[i][j]\\) over cells\n\n\nCounting\nCoin Change Count, Stairs\nAdd ways from subproblems\n\n\nInterval\nMatrix Chain, Burst Balloons\n\\(dp[i][j]\\) range subproblem\n\n\nBitmask\nTSP, Assignment\n\\(dp[mask][i]\\) subset states\n\n\nDigit\nCount numbers with constraint\n\\(dp[pos][tight][sum]\\) digits\n\n\nTree\nRerooting, Subtree DP\n\\(dp[u]\\) over children\n\n\n\n\n\nClassic Problems\n\n\n\n\n\n\n\n\nProblem\nState Definition\nTransition\n\n\n\n\nClimbing Stairs\n\\(dp[i]=\\) ways to reach step i\n\\(dp[i]=dp[i-1]+dp[i-2]\\)\n\n\nCoin Change (Count Ways)\n\\(dp[x]=\\) ways to make sum x\n\\(dp[x]+=dp[x-coin]\\)\n\n\n0/1 Knapsack\n\\(dp[w]=\\) max value under weight w\n\\(dp[w]=\\max(dp[w],dp[w-w_i]+v_i)\\)\n\n\nLongest Increasing Subseq.\n\\(dp[i]=\\) LIS ending at i\nif \\(a[j]&lt;a[i]\\), \\(dp[i]=dp[j]+1\\)\n\n\nEdit Distance\n\\(dp[i][j]=\\) edit cost\nmin(insert,delete,replace)\n\n\nMatrix Chain Multiplication\n\\(dp[i][j]=\\) min cost mult subchain\n\\(dp[i][j]=\\min_k(dp[i][k]+dp[k+1][j])\\)\n\n\n\n\n\nTiny Code: 0/1 Knapsack (1D optimized)\ndef knapsack(weights, values, W):\n    dp = [0]*(W+1)\n    for i in range(len(weights)):\n        for w in range(W, weights[i]-1, -1):\n            dp[w] = max(dp[w], dp[w-weights[i]] + values[i])\n    return dp[W]\n\n\nSequence Alignment Example\nEdit Distance Recurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1], & \\text{if } s[i] = t[j],\\\\\n1 + \\min(dp[i-1][j],\\ dp[i][j-1],\\ dp[i-1][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\nOptimization Techniques\n\n\n\n\n\n\n\n\nTechnique\nWhen to Use\nExample\n\n\n\n\nSpace Optimization\n2D → 1D states reuse\nKnapsack, LCS\n\n\nPrefix/Suffix Precomp\nRange aggregates\nSum/Min queries\n\n\nDivide & Conquer DP\nMonotonic decisions\nMatrix Chain\n\n\nConvex Hull Trick\nLinear transition minima\nDP on lines\n\n\nBitset DP\nLarge boolean states\nSubset sum optimization\n\n\n\n\n\nDebugging Tips\n\nPrint partial dp arrays to see progress.\nCheck base cases carefully.\nEnsure loops match transition dependencies.\nAlways confirm the recurrence before coding.\n\n\n\n\nPage 8. Mathematics for Algorithms Quick Use\nMathematics builds the foundation for algorithmic reasoning. This page collects essential formulas and methods every programmer should know.\n\nNumber Theory Essentials\n\n\n\n\n\n\n\n\nTopic\nDescription\nFormula / Idea\n\n\n\n\nGCD (Euclidean)\nGreatest common divisor\n\\(gcd(a,b)=gcd(b,a%b)\\)\n\n\nExtended GCD\nSolve \\(ax+by=gcd(a,b)\\)\nBacktrack coefficients\n\n\nLCM\nLeast common multiple\n\\(lcm(a,b)=\\frac{a\\cdot b}{gcd(a,b)}\\)\n\n\nModular Addition\nAdd under modulo M\n\\((a+b)\\bmod M\\)\n\n\nModular Multiply\nMultiply under modulo M\n\\((a\\cdot b)\\bmod M\\)\n\n\nModular Inverse\n\\(a^{-1}\\bmod M\\)\n\\(a^{M-2}\\bmod M\\) if M is prime\n\n\nModular Exponent\nFast exponentiation\nSquare and multiply\n\n\nCRT\nCombine congruences\nSolve system \\(x\\equiv a_i\\pmod{m_i}\\)\n\n\n\nTiny Code (Modular Exponentiation):\ndef modpow(a, n, M):\n    res = 1\n    while n:\n        if n & 1:\n            res = res * a % M\n        a = a * a % M\n        n &gt;&gt;= 1\n    return res\n\n\nPrimality and Factorization\n\n\n\n\n\n\n\n\n\nAlgorithm\nUse Case\nComplexity\nNotes\n\n\n\n\nTrial Division\nSmall n\n\\(O(\\sqrt{n})\\)\nSimple\n\n\nSieve of Eratosthenes\nGenerate primes\n\\(O(n\\log\\log n)\\)\nClassic prime sieve\n\n\nMiller–Rabin\nProbabilistic primality\n\\(O(k\\log^3 n)\\)\nFast for big n\n\n\nPollard Rho\nFactor composite\n\\(O(n^{1/4})\\)\nRandomized\n\n\nSieve of Atkin\nFaster variant\n\\(O(n)\\)\nComplex implementation\n\n\n\n\n\nCombinatorics\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\n\\(n! = n\\cdot(n-1)\\cdots1\\)\nFactorial\n\n\n\\(\\binom{n}{k}=\\dfrac{n!}{k!(n-k)!}\\)\nNumber of combinations\n\n\n\\(P(n,k)=\\dfrac{n!}{(n-k)!}\\)\nNumber of permutations\n\n\nPascal’s Rule: \\(\\binom{n}{k}=\\binom{n-1}{k}+\\binom{n-1}{k-1}\\)\nBuild Pascal’s Triangle\n\n\nCatalan: \\(C_n=\\dfrac{1}{n+1}\\binom{2n}{n}\\)\nParentheses counting\n\n\n\nTiny Code (nCr with factorials mod M):\ndef nCr(n, r, fact, inv):\n    return fact[n]*inv[r]%M*inv[n-r]%M\n\n\nProbability Basics\n\n\n\n\n\n\n\n\n\nConcept\nFormula or Idea\n\n\n\n\n\n\nProbability\n\\(P(A)=\\frac{\\text{favorable}}{\\text{total}}\\)\n\n\n\n\nComplement\n\\(P(\\bar{A})=1-P(A)\\)\n\n\n\n\nUnion\n\\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\)\n\n\n\n\nConditional\n\\(P(A                                         | B)=\\frac{P(A\\cap B)}{P(B)}\\)\n\n\n\n\nBayes’ Theorem\n\\(P(A                                         | B)=\\frac{P(B                | A)P(A)}{P(B)}\\)\n\n\n\n\nExpected Value\n\\(E[X]=\\sum x_iP(x_i)\\)\n\n\n\n\nVariance\n\\(Var(X)=E[X^2]-E[X]^2\\)\n\n\n\n\n\n\n\nLinear Algebra Core\n\n\n\nOperation\nFormula / Method\nComplexity\n\n\n\n\nGaussian Elimination\nSolve \\(Ax=b\\)\n\\(O(n^3)\\)\n\n\nDeterminant\nProduct of pivots\n\\(O(n^3)\\)\n\n\nMatrix Multiply\n\\((AB)*{ij}=\\sum_kA*{ik}B_{kj}\\)\n\\(O(n^3)\\)\n\n\nTranspose\n\\(A^T_{ij}=A_{ji}\\)\n\\(O(n^2)\\)\n\n\nLU Decomposition\n\\(A=LU\\) (lower, upper)\n\\(O(n^3)\\)\n\n\nCholesky\n\\(A=LL^T\\) (symmetric pos. def.)\n\\(O(n^3)\\)\n\n\nPower Method\nDominant eigenvalue estimation\niterative\n\n\n\nTiny Code (Gaussian Elimination Skeleton):\nfor i in range(n):\n    pivot = a[i][i]\n    for j in range(i, n+1):\n        a[i][j] /= pivot\n    for k in range(n):\n        if k != i:\n            ratio = a[k][i]\n            for j in range(i, n+1):\n                a[k][j] -= ratio*a[i][j]\n\n\nFast Transforms\n\n\n\nTransform\nUse Case\nComplexity\nNotes\n\n\n\n\nFFT\nPolynomial convolution\n\\(O(n\\log n)\\)\nComplex numbers\n\n\nNTT\nModular convolution\n\\(O(n\\log n)\\)\nPrime modulus\n\n\nFWT (XOR)\nXOR-based convolution\n\\(O(n\\log n)\\)\nSubset DP\n\n\n\nFFT Equation:\n\\[\nX_k = \\sum_{n=0}^{N-1} x_n e^{-2\\pi i kn/N}\n\\]\n\n\nNumerical Methods\n\n\n\n\n\n\n\n\nMethod\nPurpose\nFormula or Idea\n\n\n\n\nBisection\nRoot-finding\nMidpoint halve until \\(f(x)=0\\)\n\n\nNewton–Raphson\nFast convergence\n\\(x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}\\)\n\n\nSecant Method\nApprox derivative\n\\(x_{n+1}=x_n-f(x_n)\\frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}\\)\n\n\nSimpson’s Rule\nIntegration\n\\(\\int_a^bf(x)dx\\approx\\frac{h}{3}(f(a)+4f(m)+f(b))\\)\n\n\n\n\n\nOptimization and Calculus\n\n\n\n\n\n\n\nConcept\nFormula / Idea\n\n\n\n\nDerivative\n\\(f'(x)=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\\)\n\n\nGradient Descent\n\\(x_{k+1}=x_k-\\eta\\nabla f(x_k)\\)\n\n\nLagrange Multipliers\n\\(\\nabla f=\\lambda\\nabla g\\)\n\n\nConvex Function\n\\(f(\\lambda x+(1-\\lambda)y)\\le\\lambda f(x)+(1-\\lambda)f(y)\\)\n\n\n\nTiny Code (Gradient Descent):\nx = x0\nfor _ in range(1000):\n    grad = df(x)\n    x -= lr * grad\n\n\nAlgebraic Tricks\n\n\n\n\n\n\n\n\n\nTopic\nFormula / Use\n\n\n\n\n\n\nExponentiation\n\\(a^n\\) via square-multiply\n\n\n\n\nPolynomial Deriv.\n\\((ax^n)' = n\\cdot a x^{n-1}\\)\n\n\n\n\nIntegration\n\\(\\int x^n dx = \\frac{x^{n+1}}{n+1}+C\\)\n\n\n\n\nMöbius Inversion\n\\(f(n)=\\sum_{d                         | n}g(d)\\implies g(n)=\\sum_{d | n}\\mu(d)\\cdot f(n/d)\\)\n\n\n\n\n\n\n\nQuick Reference Table\n\n\n\nDomain\nMust-Know Algorithm\n\n\n\n\nNumber Theory\nGCD, Mod Exp, CRT\n\n\nCombinatorics\nPascal, Factorial, Catalan\n\n\nProbability\nBayes, Expected Value\n\n\nLinear Algebra\nGaussian Elimination\n\n\nTransforms\nFFT, NTT\n\n\nOptimization\nGradient Descent\n\n\n\n\n\n\nPage 9. Strings and Text Algorithms Quick Use\nStrings are sequences of characters used in text search, matching, and transformation. This page gives quick references to classical and modern string techniques.\n\nString Fundamentals\n\n\n\n\n\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nAlphabet\nSet of symbols\n{a, b, c}\n\n\nString Length\nNumber of characters\n\"hello\" → 5\n\n\nSubstring\nContinuous part of string\n\"ell\" in \"hello\"\n\n\nSubsequence\nOrdered subset (not necessarily cont.)\n\"hlo\" from \"hello\"\n\n\nPrefix / Suffix\nStarts / ends part of string\n\"he\", \"lo\"\n\n\n\nIndexing: Most algorithms use 0-based indexing.\n\n\nString Search Overview\n\n\n\nAlgorithm\nComplexity\nDescription\n\n\n\n\nNaive Search\n\\(O(nm)\\)\nCheck all positions\n\n\nKMP\n\\(O(n+m)\\)\nPrefix-suffix skip table\n\n\nZ-Algorithm\n\\(O(n+m)\\)\nPrecompute match lengths\n\n\nRabin–Karp\n\\(O(n+m)\\) avg\nRolling hash check\n\n\nBoyer–Moore\n\\(O(n/m)\\) avg\nBackward scan, skip mismatches\n\n\n\n\n\nKMP Prefix Function\nCompute prefix-suffix matches for pattern.\n\n\n\n\n\n\n\nStep\nMeaning\n\n\n\n\n\\(pi[i]\\)\nLongest proper prefix that is also suffix for \\(pattern[0:i]\\)\n\n\n\nTiny Code:\ndef prefix_function(p):\n    pi = [0]*len(p)\n    j = 0\n    for i in range(1, len(p)):\n        while j &gt; 0 and p[i] != p[j]:\n            j = pi[j-1]\n        if p[i] == p[j]:\n            j += 1\n        pi[i] = j\n    return pi\nSearch uses pi to skip mismatches.\n\n\nZ-Algorithm\nComputes length of substring starting at i matching prefix.\n\n\n\nStep\nMeaning\n\n\n\n\n\\(Z[i]\\)\nLongest substring starting at i matching prefix\n\n\n\nUse $S = pattern + '$' + text$ to find pattern occurrences.\n\n\nRabin–Karp Rolling Hash\n\n\n\nIdea\nCompute hash for window of text, slide, compare\n\n\n\n\n\nHash Function: \\[\nh(s) = (s_0p^{n-1} + s_1p^{n-2} + \\dots + s_{n-1}) \\bmod M\n\\]\nUpdate efficiently when sliding one character.\nTiny Code:\ndef rolling_hash(s, base=257, mod=109+7):\n    h = 0\n    for ch in s:\n        h = (h*base + ord(ch)) % mod\n    return h\n\n\nAdvanced Pattern Matching\n\n\n\nAlgorithm\nUse Case\nComplexity\n\n\n\n\nBoyer–Moore\nLarge alphabet\n\\(O(n/m)\\) avg\n\n\nSunday\nLast char shift heuristic\n\\(O(n)\\) avg\n\n\nBitap\nApproximate match\n\\(O(nm/w)\\)\n\n\nAho–Corasick\nMulti-pattern search\n\\(O(n+z)\\)\n\n\n\n\n\nAho–Corasick Automaton\nBuild a trie from patterns and compute failure links.\n\n\n\nStep\nDescription\n\n\n\n\nBuild Trie\nAdd all patterns\n\n\nFailure Link\nFallback to next prefix\n\n\nOutput Link\nRecord pattern match\n\n\n\nTiny Code Sketch:\nfrom collections import deque\n\ndef build_ac(patterns):\n    trie = [{}]\n    fail = [0]\n    for pat in patterns:\n        node = 0\n        for c in pat:\n            node = trie[node].setdefault(c, len(trie))\n            if node == len(trie):\n                trie.append({})\n                fail.append(0)\n    # compute failure links\n    q = deque()\n    for c in trie[0]:\n        q.append(trie[0][c])\n    while q:\n        u = q.popleft()\n        for c, v in trie[u].items():\n            f = fail[u]\n            while f and c not in trie[f]:\n                f = fail[f]\n            fail[v] = trie[f].get(c, 0)\n            q.append(v)\n    return trie, fail\n\n\nSuffix Structures\n\n\n\n\n\n\n\n\nStructure\nPurpose\nBuild Time\n\n\n\n\nSuffix Array\nSorted list of suffix indices\n\\(O(n\\log n)\\)\n\n\nLCP Array\nLongest Common Prefix of suffix\n\\(O(n)\\)\n\n\nSuffix Tree\nTrie of suffixes\n\\(O(n)\\) (Ukkonen)\n\n\nSuffix Automaton\nMinimal DFA of substrings\n\\(O(n)\\)\n\n\n\nSuffix Array Doubling Approach:\n\nRank substrings of length \\(2^k\\)\nSort and merge using pairs of ranks\n\nLCP via Kasai’s Algorithm: \\[\nLCP[i]=\\text{common prefix of } S[SA[i]:], S[SA[i-1]:]\n\\]\n\n\nPalindrome Detection\n\n\n\nAlgorithm\nDescription\nComplexity\n\n\n\n\nManacher’s Algorithm\nLongest palindromic substring\n\\(O(n)\\)\n\n\nDP Table\nCheck substring palindrome\n\\(O(n^2)\\)\n\n\nCenter Expansion\nExpand around center\n\\(O(n^2)\\)\n\n\n\nManacher’s Core:\n\nTransform with separators (#)\nTrack radius of palindrome around each center\n\n\n\nEdit Distance Family\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\nComplexity\n\n\n\n\nLevenshtein Distance\nInsert/Delete/Replace\n\\(O(nm)\\)\n\n\nDamerau–Levenshtein\nAdd transposition\n\\(O(nm)\\)\n\n\nHirschberg\nSpace-optimized LCS\n\\(O(nm)\\) time, \\(O(n)\\) space\n\n\n\nRecurrence: \\[\ndp[i][j]=\\min\n\\begin{cases}\ndp[i-1][j]+1 \\\ndp[i][j-1]+1 \\\ndp[i-1][j-1]+(s_i\\neq t_j)\n\\end{cases}\n\\]\n\n\nCompression Techniques\n\n\n\n\n\n\n\n\nAlgorithm\nType\nIdea\n\n\n\n\nHuffman Coding\nPrefix code\nShorter codes for frequent chars\n\n\nArithmetic Coding\nRange encoding\nFractional interval representation\n\n\nLZ77 / LZ78\nDictionary-based\nReuse earlier substrings\n\n\nBWT + MTF + RLE\nBlock sorting\nGroup similar chars before coding\n\n\n\nHuffman Principle: Shorter bit strings assigned to higher frequency symbols.\n\n\nHashing and Checksums\n\n\n\nAlgorithm\nUse Case\nNotes\n\n\n\n\nCRC32\nError detection\nSimple polynomial mod\n\n\nMD5\nHash (legacy)\nNot secure\n\n\nSHA-256\nSecure hash\nCryptographic\n\n\nRolling Hash\nSubstring compare\nUsed in Rabin–Karp\n\n\n\n\n\nQuick Reference\n\n\n\nTask\nAlgorithm\nComplexity\n\n\n\n\nSingle pattern search\nKMP / Z\n\\(O(n+m)\\)\n\n\nMulti-pattern search\nAho–Corasick\n\\(O(n+z)\\)\n\n\nApproximate search\nBitap / Wu–Manber\n\\(O(kn)\\)\n\n\nSubstring queries\nSuffix Array + LCP\n\\(O(\\log n)\\)\n\n\nPalindromes\nManacher\n\\(O(n)\\)\n\n\nCompression\nHuffman / LZ77\nvariable\n\n\nEdit distance\nDP table\n\\(O(nm)\\)\n\n\n\n\n\n\nPage 10. Geometry, Graphics, and Spatial Algorithms Quick Use\nGeometry helps us solve problems about shapes, distances, and spatial relationships. This page summarizes core computational geometry techniques with simple formulas and examples.\n\nCoordinate Basics\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nFormula / Example\n\n\n\n\n\n\n\n\nPoint Distance\nDistance between \\((x_1,y_1)\\) and \\((x_2,y_2)\\)\n\\(d=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\\)\n\n\n\n\n\n\nMidpoint\nBetween two points\n\\((\\frac{x_1+x_2}{2}, \\frac{y_1+y_2}{2})\\)\n\n\n\n\n\n\nDot Product\nAngle & projection\n\\(\\vec{a}\\cdot\\vec{b}=                    | a |   | b | \\cos\\theta\\)\n\n\n\n\n\n\nCross Product (2D)\nSigned area, orientation\n\\(a\\times b = a_xb_y - a_yb_x\\)\n\n\n\n\n\n\nOrientation Test\nCCW, CW, collinear check\n\\(\\text{sign}(a\\times b)\\)\n\n\n\n\n\n\n\nTiny Code (Orientation Test):\ndef orient(a, b, c):\n    val = (b[0]-a[0])*(c[1]-a[1]) - (b[1]-a[1])*(c[0]-a[0])\n    return 0 if val == 0 else (1 if val &gt; 0 else -1)\n\n\nConvex Hull\nFind the smallest convex polygon enclosing all points.\n\n\n\nAlgorithm\nComplexity\nNotes\n\n\n\n\nGraham Scan\n\\(O(n\\log n)\\)\nSort by angle, use stack\n\n\nAndrew’s Monotone\n\\(O(n\\log n)\\)\nSort by x, build upper/lower\n\n\nJarvis March\n\\(O(nh)\\)\nWrap hull, h = hull size\n\n\nChan’s Algorithm\n\\(O(n\\log h)\\)\nOutput-sensitive hull\n\n\n\nSteps:\n\nSort points\nBuild lower hull\nBuild upper hull\nConcatenate\n\n\n\nClosest Pair of Points\nDivide-and-conquer approach.\n\n\n\nStep\nDescription\n\n\n\n\nSplit by x\nDivide points into halves\n\n\nRecurse and merge\nTrack min distance across strip\n\n\n\nComplexity: \\(O(n\\log n)\\)\nFormula: \\[\nd(p,q)=\\sqrt{(x_p-x_q)^2+(y_p-y_q)^2}\n\\]\n\n\nLine Intersection\nTwo segments \\((p_1,p_2)\\) and \\((q_1,q_2)\\) intersect if:\n\nOrientations differ\nSegments overlap on line if collinear\n\nTiny Code:\ndef intersect(p1, p2, q1, q2):\n    o1 = orient(p1, p2, q1)\n    o2 = orient(p1, p2, q2)\n    o3 = orient(q1, q2, p1)\n    o4 = orient(q1, q2, p2)\n    return o1 != o2 and o3 != o4\n\n\nPolygon Area (Shoelace Formula)\nFor vertices \\((x_i, y_i)\\) in order:\n\\[\nA=\\frac{1}{2}\\left|\\sum_{i=0}^{n-1}(x_iy_{i+1}-x_{i+1}y_i)\\right|\n\\]\nTiny Code:\ndef area(poly):\n    s = 0\n    n = len(poly)\n    for i in range(n):\n        x1, y1 = poly[i]\n        x2, y2 = poly[(i+1)%n]\n        s += x1*y2 - x2*y1\n    return abs(s)/2\n\n\nPoint in Polygon\n\n\n\nMethod\nIdea\nComplexity\n\n\n\n\nRay Casting\nCount edge crossings\n\\(O(n)\\)\n\n\nWinding Number\nTrack signed rotations\n\\(O(n)\\)\n\n\nConvex Test\nCheck all orientations\n\\(O(n)\\)\n\n\n\nRay Casting: Odd number of crossings → inside.\n\n\nRotating Calipers\nUsed for:\n\nPolygon diameter (farthest pair)\nMinimum bounding box\nWidth and antipodal pairs\n\nIdea: Sweep around convex hull using tangents. Complexity: \\(O(n)\\) after hull.\n\n\nSweep Line Techniques\n\n\n\nProblem\nMethod\nComplexity\n\n\n\n\nClosest Pair\nActive set by y\n\\(O(n\\log n)\\)\n\n\nSegment Intersection\nEvent-based sweeping\n\\(O((n+k)\\log n)\\)\n\n\nRectangle Union Area\nVertical edge events\n\\(O(n\\log n)\\)\n\n\nSkyline Problem\nMerge by height\n\\(O(n\\log n)\\)\n\n\n\nUse balanced trees or priority queues for active sets.\n\n\nCircle Geometry\n\n\n\nConcept\nFormula\n\n\n\n\nEquation\n\\((x-x_c)^2+(y-y_c)^2=r^2\\)\n\n\nTangent Length\n\\(\\sqrt{d^2-r^2}\\)\n\n\nTwo-Circle Intersection\nDistance-based geometry\n\n\n\n\n\nSpatial Data Structures\n\n\n\nStructure\nUse Case\nNotes\n\n\n\n\nKD-Tree\nNearest neighbor search\nAxis-aligned splits\n\n\nR-Tree\nRange queries\nBounding boxes hierarchy\n\n\nQuadtree\n2D recursive subdivision\nGraphics, collision detection\n\n\nOctree\n3D extension\nVolumetric partitioning\n\n\nBSP Tree\nPlanar splits\nRendering, collision\n\n\n\n\n\nRasterization and Graphics\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nNotes\n\n\n\n\nBresenham Line\nDraw line integer grid\nNo floating point\n\n\nMidpoint Circle\nCircle rasterization\nSymmetry exploitation\n\n\nScanline Fill\nPolygon fill algorithm\nSort edges, horizontal sweep\n\n\nZ-Buffer\nHidden surface removal\nPer-pixel depth comparison\n\n\nPhong Shading\nSmooth lighting\nInterpolate normals\n\n\n\n\n\nPathfinding in Space\n\n\n\nAlgorithm\nDescription\nNotes\n\n\n\n\nA*\nHeuristic shortest path\n\\(f(n)=g(n)+h(n)\\)\n\n\nTheta*\nAny-angle path\nShortcut-based\n\n\nRRT / RRT*\nRandom exploration\nRobotics planning\n\n\nPRM\nProbabilistic roadmap\nSampled graph\n\n\nVisibility Graph\nConnect visible points\nGeometric planning\n\n\n\n\n\nQuick Summary\n\n\n\nTask\nAlgorithm\nComplexity\n\n\n\n\nConvex Hull\nGraham / Andrew\n\\(O(n\\log n)\\)\n\n\nClosest Pair\nDivide and Conquer\n\\(O(n\\log n)\\)\n\n\nSegment Intersection Detection\nSweep Line\n\\(O(n\\log n)\\)\n\n\nPoint in Polygon\nRay Casting\n\\(O(n)\\)\n\n\nPolygon Area\nShoelace Formula\n\\(O(n)\\)\n\n\nNearest Neighbor Search\nKD-Tree\n\\(O(\\log n)\\)\n\n\nPathfinding\nA*\n\\(O(E\\log V)\\)\n\n\n\n\n\nTip\n\nAlways sort points for geometry preprocessing.\nUse cross product for orientation tests.\nPrefer integer arithmetic when possible to avoid floating errors.\n\n\n\n\nPage 11. Systems, Databases, and Distributed Algorithms Quick Use\nSystems and databases rely on algorithms that manage memory, concurrency, persistence, and coordination. This page gives an overview of the most important ones.\n\nConcurrency Control\nEnsures correctness when multiple transactions or threads run at once.\n\n\n\n\n\n\n\n\nMethod\nIdea\nNotes\n\n\n\n\nTwo-Phase Locking (2PL)\nAcquire locks, then release after commit\nGuarantees serializability\n\n\nStrict 2PL\nHold all locks until commit\nPrevents cascading aborts\n\n\nConservative 2PL\nLock all before execution\nDeadlock-free but less parallel\n\n\nTimestamp Ordering\nOrder by timestamps\nMay abort late transactions\n\n\nMultiversion CC (MVCC)\nReaders get snapshots\nUsed in PostgreSQL, InnoDB\n\n\nOptimistic CC (OCC)\nValidate at commit\nBest for low conflict workloads\n\n\n\n\n\nTiny Code: Timestamp Ordering\n# Simplified\nif write_ts[x] &gt; txn_ts or read_ts[x] &gt; txn_ts:\n    abort()\nelse:\n    write_ts[x] = txn_ts\nEach object tracks read and write timestamps.\n\n\nDeadlocks\nCircular waits among transactions.\nDetection | Build Wait-For Graph, detect cycle |\nPrevention | Wait-Die (old waits) / Wound-Wait (young aborts) |\nDetection Complexity: \\(O(V+E)\\)\nTiny Code (Wait-For Graph Cycle Check):\ndef has_cycle(graph):\n    visited, stack = set(), set()\n    def dfs(u):\n        visited.add(u)\n        stack.add(u)\n        for v in graph[u]:\n            if v not in visited and dfs(v): return True\n            if v in stack: return True\n        stack.remove(u)\n        return False\n    return any(dfs(u) for u in graph)\n\n\nLogging and Recovery\n\n\n\n\n\n\n\n\nTechnique\nDescription\nNotes\n\n\n\n\nWrite-Ahead Log\nLog before data\nEnsures durability\n\n\nARIES\nAnalysis, Redo, Undo phases\nIndustry standard\n\n\nCheckpointing\nSave consistent snapshot\nSpeeds recovery\n\n\nShadow Paging\nCopy-on-write updates\nSimpler but less flexible\n\n\n\nRecovery after crash:\n\nAnalysis: find active transactions\nRedo: reapply committed changes\nUndo: revert uncommitted ones\n\n\n\nIndexing\nAccelerates lookups and range queries.\n\n\n\n\n\n\n\n\nIndex Type\nDescription\nNotes\n\n\n\n\nB-Tree / B+Tree\nBalanced multiway tree\nDisk-friendly\n\n\nHash Index\nExact match only\nNo range queries\n\n\nGiST / R-Tree\nSpatial data\nBounding box hierarchy\n\n\nInverted Index\nText search\nMaps token to document list\n\n\n\nB+Tree Complexity: \\(O(\\log_B N)\\) (B = branching factor)\nTiny Code (Binary Search in Index):\ndef search(node, key):\n    i = bisect_left(node.keys, key)\n    if i &lt; len(node.keys) and node.keys[i] == key:\n        return node.values[i]\n    if node.is_leaf:\n        return None\n    return search(node.children[i], key)\n\n\nQuery Processing\n\n\n\nStep\nDescription\n\n\n\n\nParsing\nBuild abstract syntax tree\n\n\nOptimization\nReorder joins, pick indices\n\n\nExecution Plan\nChoose algorithm per operator\n\n\nExecution\nEvaluate iterators or pipelines\n\n\n\nCommon join strategies:\n\n\n\nJoin Type\nComplexity\nNotes\n\n\n\n\nNested Loop\n\\(O(nm)\\)\nSimple, slow\n\n\nHash Join\n\\(O(n+m)\\)\nBuild + probe\n\n\nSort-Merge Join\n\\(O(n\\log n+m\\log m)\\)\nSorted inputs\n\n\n\n\n\nCaching and Replacement\n\n\n\nPolicy\nDescription\nNotes\n\n\n\n\nLRU\nEvict least recently used\nSimple, temporal locality\n\n\nLFU\nEvict least frequently used\nGood for stable patterns\n\n\nARC / LIRS\nAdaptive hybrid\nHandles mixed workloads\n\n\nRandom\nRandom eviction\nSimple, fair\n\n\n\nTiny Code (LRU using OrderedDict):\nfrom collections import OrderedDict\n\nclass LRU:\n    def __init__(self, cap):\n        self.cap = cap\n        self.cache = OrderedDict()\n    def get(self, k):\n        if k not in self.cache: return -1\n        self.cache.move_to_end(k)\n        return self.cache[k]\n    def put(self, k, v):\n        if k in self.cache: self.cache.move_to_end(k)\n        self.cache[k] = v\n        if len(self.cache) &gt; self.cap: self.cache.popitem(last=False)\n\n\nDistributed Systems Core\n\n\n\nProblem\nDescription\nTypical Solution\n\n\n\n\nConsensus\nAgree on value across nodes\nPaxos, Raft\n\n\nLeader Election\nPick coordinator\nBully, Raft\n\n\nReplication\nMaintain copies\nLog replication\n\n\nPartitioning\nSplit data\nConsistent hashing\n\n\nMembership\nDetect nodes\nGossip protocols\n\n\n\n\n\nRaft Consensus (Simplified)\n\n\n\nPhase\nAction\n\n\n\n\nElection\nNodes vote, elect leader\n\n\nReplication\nLeader appends log entries\n\n\nCommitment\nOnce majority acknowledge\n\n\n\nSafety: Committed entries never change. Liveness: New leader elected on failure.\nTiny Code Sketch:\nif vote_request.term &gt; term:\n    term = vote_request.term\n    voted_for = candidate\n\n\nConsistent Hashing\nDistributes keys across nodes smoothly.\n\n\n\nStep\nDescription\n\n\n\n\nHash each node to ring\ne.g. hash(node_id)\n\n\nHash each key\nFind next node clockwise\n\n\nAdd/remove node\nOnly nearby keys move\n\n\n\nUsed in: Dynamo, Cassandra, Memcached.\n\n\nFault Tolerance Patterns\n\n\n\nPattern\nDescription\nExample\n\n\n\n\nReplication\nMultiple copies\nPrimary-backup\n\n\nCheckpointing\nSave progress periodically\nML training\n\n\nHeartbeats\nLiveness detection\nCluster managers\n\n\nRetry + Backoff\nHandle transient failures\nAPI calls\n\n\nQuorum Reads/Writes\nRequire majority agreement\nCassandra\n\n\n\n\n\nDistributed Coordination\n\n\n\nTool / Protocol\nDescription\nExample Use\n\n\n\n\nZooKeeper\nCentralized coordination\nLocks, config\n\n\nRaft\nDistributed consensus\nLog replication\n\n\nEtcd\nKey-value store on Raft\nCluster metadata\n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nTopic\nAlgorithm / Concept\nComplexity\nNotes\n\n\n\n\nLocking\n2PL, MVCC, OCC\nvaries\nTransaction isolation\n\n\nDeadlock\nWait-Die, Detection\n\\(O(V+E)\\)\nGraph-based check\n\n\nRecovery\nARIES, WAL\nvaries\nCrash recovery\n\n\nIndexing\nB+Tree, Hash Index\n\\(O(\\log N)\\)\nFaster queries\n\n\nJoin\nHash / Sort-Merge\nvaries\nQuery optimization\n\n\nCache\nLRU, LFU\n\\(O(1)\\)\nData locality\n\n\nConsensus\nRaft, Paxos\n\\(O(n)\\) msg\nFault tolerance\n\n\nPartitioning\nConsistent Hashing\n\\(O(1)\\) avg\nScalability\n\n\n\n\n\nQuick Tips\n\nAlways ensure serializability in concurrency.\nUse MVCC for read-heavy workloads.\nARIES ensures durability via WAL.\nFor scalability, partition and replicate wisely.\nConsensus is required for shared state correctness.\n\n\n\n\nPage 12. Algorithms for AI, ML, and Optimization Quick Use\nThis page gathers classical algorithms that power modern AI and machine learning systems, from clustering and classification to gradient-based learning and metaheuristics.\n\nClassical Machine Learning Algorithms\n\n\n\n\n\n\n\n\n\nCategory\nAlgorithm\nCore Idea\nComplexity\n\n\n\n\nClustering\nk-Means\nAssign to nearest centroid, update centers\n\\(O(nkt)\\)\n\n\nClustering\nk-Medoids (PAM)\nRepresentative points as centers\n\\(O(k(n-k)^2)\\)\n\n\nClustering\nGaussian Mixture (EM)\nSoft assignments via probabilities\n\\(O(nkd)\\) per iter\n\n\nClassification\nNaive Bayes\nApply Bayes rule with feature independence\n\\(O(nd)\\)\n\n\nClassification\nLogistic Regression\nLinear + sigmoid activation\n\\(O(nd)\\)\n\n\nClassification\nSVM (Linear)\nMaximize margin via convex optimization\n\\(O(nd)\\) approx\n\n\nClassification\nk-NN\nVote from nearest neighbors\n\\(O(nd)\\) per query\n\n\nTrees\nDecision Tree (CART)\nRecursive splitting by impurity\n\\(O(nd\\log n)\\)\n\n\nProjection\nLDA / PCA\nFind projection maximizing variance or class separation\n\\(O(d^3)\\)\n\n\n\n\n\nTiny Code: k-Means\nimport random, math\n\ndef kmeans(points, k, iters=100):\n    centroids = random.sample(points, k)\n    for _ in range(iters):\n        groups = [[] for _ in range(k)]\n        for p in points:\n            idx = min(range(k), key=lambda i: (p[0]-centroids[i][0])2 + (p[1]-centroids[i][1])2)\n            groups[idx].append(p)\n        new_centroids = []\n        for g in groups:\n            if g:\n                x = sum(p[0] for p in g)/len(g)\n                y = sum(p[1] for p in g)/len(g)\n                new_centroids.append((x,y))\n            else:\n                new_centroids.append(random.choice(points))\n        if centroids == new_centroids: break\n        centroids = new_centroids\n    return centroids\n\n\nLinear Models\n\n\n\n\n\n\n\n\nModel\nFormula\nLoss Function\n\n\n\n\nLinear Regression\n\\(\\hat{y}=w^Tx+b\\)\nMSE: \\(\\frac{1}{n}\\sum(y-\\hat{y})^2\\)\n\n\nLogistic Regression\n\\(\\hat{y}=\\sigma(w^Tx+b)\\)\nCross-Entropy\n\n\nRidge Regression\nLinear + \\(L_2\\) penalty\n\\(L=\\text{MSE}+\\lambda|w|^2\\)\n\n\nLasso Regression\nLinear + \\(L_1\\) penalty\n\\(L=\\text{MSE}+\\lambda|w|_1\\)\n\n\n\nTiny Code (Gradient Descent for Linear Regression):\ndef train(X, y, lr=0.01, epochs=1000):\n    w = [0]*len(X[0])\n    b = 0\n    for _ in range(epochs):\n        for i in range(len(y)):\n            y_pred = sum(w[j]*X[i][j] for j in range(len(w))) + b\n            err = y_pred - y[i]\n            for j in range(len(w)):\n                w[j] -= lr * err * X[i][j]\n            b -= lr * err\n    return w, b\n\n\nDecision Trees and Ensembles\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\nNotes\n\n\n\n\nID3 / C4.5 / CART\nSplit by info gain or Gini\nRecursive, interpretable\n\n\nRandom Forest\nBagging + Decision Trees\nReduces variance\n\n\nGradient Boosting\nSequential residual fitting\nXGBoost, LightGBM, CatBoost\n\n\nAdaBoost\nWeighted weak learners\nSensitive to noise\n\n\n\nImpurity Measures:\n\nGini: \\(1-\\sum p_i^2\\)\nEntropy: \\(-\\sum p_i\\log_2p_i\\)\n\n\n\nSupport Vector Machines (SVM)\nFinds a maximum margin hyperplane.\nObjective: \\[\n\\min_{w,b} \\frac{1}{2}|w|^2 + C\\sum\\xi_i\n\\] subject to \\(y_i(w^Tx_i+b)\\ge1-\\xi_i\\)\nKernel trick enables nonlinear separation: \\[K(x_i,x_j)=\\phi(x_i)\\cdot\\phi(x_j)\\]\n\n\nNeural Network Fundamentals\n\n\n\nComponent\nDescription\n\n\n\n\nNeuron\n\\(y=\\sigma(w\\cdot x+b)\\)\n\n\nActivation\nSigmoid, ReLU, Tanh\n\n\nLoss\nMSE, Cross-Entropy\n\n\nTraining\nGradient Descent + Backprop\n\n\nOptimizers\nSGD, Adam, RMSProp\n\n\n\nForward Propagation: \\[a^{(l)} = \\sigma(W^{(l)}a^{(l-1)}+b^{(l)})\\] Backpropagation computes gradients layer by layer.\n\n\nGradient Descent Variants\n\n\n\nVariant\nIdea\nNotes\n\n\n\n\nBatch\nUse all data each step\nStable but slow\n\n\nStochastic\nUpdate per sample\nNoisy, fast\n\n\nMini-batch\nGroup updates\nCommon practice\n\n\nMomentum\nAdd velocity term\nFaster convergence\n\n\nAdam\nAdaptive moment estimates\nMost popular\n\n\n\nUpdate Rule: \\[\nw = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n\\]\n\n\nUnsupervised Learning\n\n\n\nAlgorithm\nDescription\nNotes\n\n\n\n\nPCA\nVariance-based projection\nEigen decomposition\n\n\nICA\nIndependent components\nSignal separation\n\n\nt-SNE\nPreserve local structure\nVisualization only\n\n\nAutoencoder\nNN reconstruction model\nDimensionality red.\n\n\n\nPCA Formula: Covariance \\(C=\\frac{1}{n}X^TX\\), eigenvectors of \\(C\\) are principal axes.\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\n\nModel\nDescription\nNotes\n\n\n\n\n\n\nNaive Bayes\nIndependence assumption\n\\(P(y                 | x)\\propto P(y)\\prod P(x_i | y)\\)\n\n\n\n\nHMM\nSequential hidden states\nViterbi for decoding\n\n\n\n\nMarkov Chains\nTransition probabilities\n\\(P(x_t               | x_{t-1})\\)\n\n\n\n\nGaussian Mixture\nSoft clustering\nEM algorithm\n\n\n\n\n\n\n\nOptimization and Metaheuristics\n\n\n\n\n\n\n\n\nAlgorithm\nCategory\nNotes\n\n\n\n\nGradient Descent\nConvex Opt.\nDifferentiable objectives\n\n\nNewton’s Method\nSecond-order\nUses Hessian\n\n\nSimulated Annealing\nProb. search\nEscape local minima\n\n\nGenetic Algorithm\nEvolutionary\nPopulation-based search\n\n\nPSO (Swarm)\nCollective move\nInspired by flocking behavior\n\n\nHill Climbing\nGreedy search\nLocal optimization\n\n\n\n\n\nReinforcement Learning Core\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nAgent\nLearner/decision maker\nRobot, policy\n\n\nEnvironment\nProvides states, rewards\nGame, simulation\n\n\nPolicy\nMapping state → action\n\\(\\pi(s)=a\\)\n\n\nValue Function\nExpected return\n\\(V(s)\\), \\(Q(s,a)\\)\n\n\n\nQ-Learning Update: \\[\nQ(s,a)\\leftarrow Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a')-Q(s,a))\n\\]\nTiny Code:\nQ[s][a] += alpha * (r + gamma * max(Q[s_next]) - Q[s][a])\n\n\nAI Search Algorithms\n\n\n\n\n\n\n\n\n\nAlgorithm\nDescription\nComplexity\nNotes\n\n\n\n\nBFS\nShortest path unweighted\n\\(O(V+E)\\)\nLevel order search\n\n\nDFS\nDeep exploration\n\\(O(V+E)\\)\nBacktracking\n\n\nA* Search\nInformed, uses heuristic\n\\(O(E\\log V)\\)\n\\(f(n)=g(n)+h(n)\\)\n\n\nIDA*\nIterative deepening A*\nMemory efficient\nOptimal if \\(h\\) admissible\n\n\nBeam Search\nKeep best k states\nApproximate\nNLP decoding\n\n\n\n\n\nEvaluation Metrics\n\n\n\n\n\n\n\n\nTask\nMetric\nFormula / Meaning\n\n\n\n\nClassification\nAccuracy, Precision, Recall\n\\(\\frac{TP}{TP+FP}\\), \\(\\frac{TP}{TP+FN}\\)\n\n\nRegression\nRMSE, MAE, \\(R^2\\)\nFit and error magnitude\n\n\nClustering\nSilhouette Score\nCohesion vs separation\n\n\nRanking\nMAP, NDCG\nOrder-sensitive\n\n\n\nConfusion Matrix:\n\n\n\n\nPred +\nPred -\n\n\n\n\nActual +\nTP\nFN\n\n\nActual -\nFP\nTN\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\nCategory\nAlgorithm Example\nNotes\n\n\n\n\nClustering\nk-Means, GMM\nUnsupervised grouping\n\n\nClassification\nLogistic, SVM, Trees\nSupervised labeling\n\n\nRegression\nLinear, Ridge, Lasso\nPredict continuous value\n\n\nOptimization\nGD, Adam, Simulated Annealing\nMinimize loss\n\n\nProbabilistic\nBayes, HMM, EM\nUncertainty modeling\n\n\nReinforcement\nQ-Learning, SARSA\nReward-based learning",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Cheatsheet</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html",
    "href": "books/en-us/book.html",
    "title": "The Book",
    "section": "",
    "text": "Chapter 1. Foundations of algorithms\nLicensed under CC BY-NC-SA 4.0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-1.-foundations-of-algorithms",
    "href": "books/en-us/book.html#chapter-1.-foundations-of-algorithms",
    "title": "The Book",
    "section": "",
    "text": "1. What Is an Algorithm?\nLet’s start at the beginning. Before code, data, or performance, we need a clear idea of what an algorithm really is.\nAn algorithm is a clear, step-by-step procedure to solve a problem. Think of it like a recipe: you have inputs (ingredients), a series of steps (instructions), and an output (the finished dish).\nAt its core, an algorithm should be:\n\nPrecise: every step is well defined and unambiguous\nFinite: it finishes after a limited number of steps\nEffective: each step is simple enough to carry out\nDeterministic (usually): the same input gives the same output\n\nWhen you write an algorithm, you are describing how to get from question to answer, not just what the answer is.\n\nExample: Sum from 1 to (n)\nSuppose you want the sum of the numbers from 1 to (n).\nNatural language steps\n\nSet total = 0\nFor each i from 1 to n, add i to total\nReturn total\n\nPseudocode\nAlgorithm SumToN(n):\n    total ← 0\n    for i ← 1 to n:\n        total ← total + i\n    return total\nC code\nint sum_to_n(int n) {\n    int total = 0;\n    for (int i = 1; i &lt;= n; i++) {\n        total += i;\n    }\n    return total;\n}\n\n\nTiny Code\nTry a quick run by hand with (n = 5):\n\nstart total = 0\nadd 1 → total = 1\nadd 2 → total = 3\nadd 3 → total = 6\nadd 4 → total = 10\nadd 5 → total = 15\n\nOutput is 15.\nYou will also see this closed-form formula soon:\n\\[\n1 + 2 + 3 + \\dots + n = \\frac{n(n+1)}{2}\n\\]\n\n\nWhy It Matters\nAlgorithms are the blueprints of computation. Every program, from a calculator to an AI model, is built from algorithms. Computers are fast at following instructions. Algorithms give those instructions structure and purpose.\n\nAlgorithms are the language of problem solving.\n\n\n\nTry It Yourself\n\nWrite an algorithm to find the maximum number in a list\nWrite an algorithm to reverse a string\nDescribe your morning routine as an algorithm: list the inputs, the steps, and the final output\n\nTip: the best way to learn is to think in small, clear steps. Break a problem into simple actions you can execute one by one.\n\n\n\n2. Measuring Time and Space\nNow that you know what an algorithm is, it’s time to ask a deeper question:\n\nHow do we know if one algorithm is better than another?\n\nIt’s not enough for an algorithm to be correct. It should also be efficient. We measure efficiency in two key ways: time and space.\n\nTime Complexity\nTime measures how long an algorithm takes to run, relative to its input size. We don’t measure in seconds, because hardware speed varies. Instead, we count steps or operations.\nExample:\nfor (int i = 0; i &lt; n; i++) {\n    printf(\"Hi\\n\");\n}\nThis loop runs \\(n\\) times, so it has time complexity \\(O(n)\\). The time grows linearly with input size.\nAnother example:\nfor (int i = 0; i &lt; n; i++)\n  for (int j = 0; j &lt; n; j++)\n    printf(\"*\");\nThis runs \\(n \\times n = n^2\\) times, so it has \\(O(n^2)\\) time complexity.\nThese Big-O symbols describe how runtime grows as the input grows.\n\n\nSpace Complexity\nSpace measures how much memory an algorithm uses.\nExample:\nint sum = 0;  // O(1) space\nThis uses a constant amount of memory, regardless of input size.\nBut if we allocate an array:\nint arr[n];   // O(n) space\nThis uses space proportional to \\(n\\).\nOften, we trade time for space:\n\nUsing a hash table speeds up lookups (more memory, less time)\nUsing a streaming algorithm saves memory (less space, more time)\n\n\n\nTiny Code\nCompare two ways to compute the sum from 1 to \\(n\\):\nMethod 1: Loop\nint sum_loop(int n) {\n    int total = 0;\n    for (int i = 1; i &lt;= n; i++) total += i;\n    return total;\n}\nTime: \\(O(n)\\) Space: \\(O(1)\\)\nMethod 2: Formula\nint sum_formula(int n) {\n    return n * (n + 1) / 2;\n}\nTime: \\(O(1)\\) Space: \\(O(1)\\)\nBoth are correct, but one is faster. Analyzing time and space helps you understand why.\n\n\nWhy It Matters\nWhen data grows huge (millions or billions), small inefficiencies explode.\nAn algorithm that takes \\(O(n^2)\\) time might feel fine for 10 elements, but impossible for 1,000,000.\nMeasuring time and space helps you:\n\nPredict performance\nCompare different solutions\nOptimize intelligently\n\nIt’s your compass for navigating complexity.\n\n\nTry It Yourself\n\nWrite a simple algorithm to find the minimum in an array. Estimate its time and space complexity.\nCompare two algorithms that solve the same problem. Which one scales better?\nThink of a daily task that feels like \\(O(n)\\). Can you imagine one that’s \\(O(1)\\)?\n\nUnderstanding these measurements early makes every future algorithm more meaningful.\n\n\n\n3. Big-O, Big-Theta, Big-Omega\nNow that you can measure time and space, let’s learn the language used to describe those measurements.\nWhen we say an algorithm is \\(O(n)\\), we’re using asymptotic notation, a way to describe how an algorithm’s running time or memory grows as input size \\(n\\) increases.\nIt’s not about exact steps, but about how the cost scales for very large \\(n\\).\n\nThe Big-O (Upper Bound)\nBig-O answers the question: “How bad can it get?” It gives an upper bound on growth, the worst-case scenario.\nIf an algorithm takes at most \\(5n + 20\\) steps, we write \\(O(n)\\). We drop constants and lower-order terms because they don’t matter at scale.\nCommon Big-O notations:\n\n\n\n\n\n\n\n\n\nName\nNotation\nGrowth\nExample\n\n\n\n\nConstant\n\\(O(1)\\)\nFlat\nAccessing array element\n\n\nLogarithmic\n\\(O(\\log n)\\)\nVery slow growth\nBinary search\n\n\nLinear\n\\(O(n)\\)\nProportional\nSingle loop\n\n\nQuadratic\n\\(O(n^2)\\)\nGrows quickly\nDouble loop\n\n\nExponential\n\\(O(2^n)\\)\nExplodes\nRecursive subset generation\n\n\n\nIf your algorithm is \\(O(n)\\), doubling input size roughly doubles runtime. If it’s \\(O(n^2)\\), doubling input size makes it about four times slower.\n\n\nThe Big-Theta (Tight Bound)\nBig-Theta (\\(\\Theta\\)) gives a tight bound, when you know the algorithm’s growth from above and below.\nIf runtime is roughly \\(3n + 2\\), then \\(T(n) = \\Theta(n)\\). That means it’s both \\(O(n)\\) and \\(\\Omega(n)\\).\n\n\nThe Big-Omega (Lower Bound)\nBig-Omega (\\(\\Omega\\)) answers: “How fast can it possibly be?” It’s the best-case growth, the lower limit.\nExample:\n\nLinear search: \\(\\Omega(1)\\) if the element is at the start\n\\(O(n)\\) in the worst case if it’s at the end\n\nSo we might say:\n\\[\nT(n) = \\Omega(1),\\quad T(n) = O(n)\n\\]\n\n\nTiny Code\nLet’s see Big-O in action.\nint sum_pairs(int n) {\n    int total = 0;\n    for (int i = 0; i &lt; n; i++)        // O(n)\n        for (int j = 0; j &lt; n; j++)    // O(n)\n            total += i + j;            // O(1)\n    return total;\n}\nTotal steps ≈ \\(n \\times n = n^2\\). So \\(T(n) = O(n^2)\\).\nIf we added a constant-time operation before or after the loops, it wouldn’t matter. Constants vanish in asymptotic notation.\n\n\nWhy It Matters\nBig-O, Big-Theta, and Big-Omega let you talk precisely about performance. They are the grammar of efficiency.\nWhen you can write:\n\nAlgorithm A runs in \\(O(n \\log n)\\) time, \\(O(n)\\) space\n\nyou’ve captured its essence clearly and compared it meaningfully.\nThey help you:\n\nPredict behavior at scale\nChoose better data structures\nCommunicate efficiency in interviews and papers\n\nIt’s not about exact timing, it’s about growth.\n\n\nTry It Yourself\n\nAnalyze this code:\nfor (int i = 1; i &lt;= n; i *= 2)\n    printf(\"%d\", i);\nWhat’s the time complexity?\nWrite an algorithm that’s \\(O(n \\log n)\\) (hint: merge sort).\nIdentify the best, worst, and average-case complexities for linear search and binary search.\n\nLearning Big-O is like learning a new language, once you’re fluent, you can see how code grows before you even run it.\n\n\n\n4. Algorithmic Paradigms (Greedy, Divide and Conquer, DP)\nOnce you can measure performance, it’s time to explore how algorithms are designed. Behind every clever solution is a guiding paradigm, a way of thinking about problems.\nThree of the most powerful are:\n\nGreedy Algorithms\nDivide and Conquer\nDynamic Programming (DP)\n\nEach represents a different mindset for problem solving.\n\n1. Greedy Algorithms\nA greedy algorithm makes the best local choice at each step, hoping it leads to a global optimum.\nThink of it like:\n\n“Take what looks best right now, and don’t worry about the future.”\n\nThey are fast and simple, but not always correct. They only work when the greedy choice property holds.\nExample: Coin Change (Greedy version) Suppose you want to make 63 cents using US coins (25, 10, 5, 1). The greedy approach:\n\nTake 25 → 38 left\nTake 25 → 13 left\nTake 10 → 3 left\nTake 1 × 3\n\nThis works here, but not always (try coins 1, 3, 4 for amount 6). Simple, but not guaranteed optimal.\nCommon greedy algorithms:\n\nKruskal’s Minimum Spanning Tree\nPrim’s Minimum Spanning Tree\nDijkstra’s Shortest Path (non-negative weights)\nHuffman Coding\n\n\n\n2. Divide and Conquer\nThis is a classic paradigm. You break the problem into smaller subproblems, solve each recursively, and then combine the results.\nIt’s like splitting a task among friends, then merging their answers.\nFormally:\n\\[\nT(n) = aT\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nExamples:\n\nMerge Sort: divide the array, sort halves, merge\nQuick Sort: partition around a pivot\nBinary Search: halve the range each step\n\nElegant and powerful, but recursion overhead can add cost if poorly structured.\n\n\n3. Dynamic Programming (DP)\nDP is for problems with overlapping subproblems and optimal substructure. You solve smaller subproblems once and store the results to avoid recomputation.\nIt’s like divide and conquer with memory.\nExample: Fibonacci Naive recursion is exponential. DP with memoization is linear.\nint fib(int n) {\n    if (n &lt;= 1) return n;\n    static int memo[1000] = {0};\n    if (memo[n]) return memo[n];\n    memo[n] = fib(n-1) + fib(n-2);\n    return memo[n];\n}\nEfficient reuse, but requires insight into subproblem structure.\n\n\nTiny Code\nQuick comparison using Fibonacci:\nNaive (Divide and Conquer)\nint fib_dc(int n) {\n    if (n &lt;= 1) return n;\n    return fib_dc(n-1) + fib_dc(n-2);  // exponential\n}\nDP (Memoization)\nint fib_dp(int n, int memo[]) {\n    if (n &lt;= 1) return n;\n    if (memo[n]) return memo[n];\n    return memo[n] = fib_dp(n-1, memo) + fib_dp(n-2, memo);\n}\n\n\nWhy It Matters\nAlgorithmic paradigms give you patterns for design:\n\nGreedy: when local choices lead to a global optimum\nDivide and Conquer: when the problem splits naturally\nDynamic Programming: when subproblems overlap\n\nOnce you recognize a problem’s structure, you’ll instantly know which mindset fits best.\nThink of paradigms as templates for reasoning, not just techniques but philosophies.\n\n\nTry It Yourself\n\nWrite a greedy algorithm to make change using coins [1, 3, 4] for amount 6. Does it work?\nImplement merge sort using divide and conquer.\nSolve Fibonacci both ways (naive vs DP) and compare speeds.\nThink of a real-life task you solve greedily.\n\nLearning paradigms is like learning styles of thought. Once you know them, every problem starts to look familiar.\n\n\n\n5. Recurrence Relations\nEvery time you break a problem into smaller subproblems, you create a recurrence, a mathematical way to describe how the total cost grows.\nRecurrence relations are the backbone of analyzing recursive algorithms. They tell us how much time or space an algorithm uses, based on the cost of its subproblems.\n\nWhat Is a Recurrence?\nA recurrence relation expresses \\(T(n)\\), the total cost for input size \\(n\\), in terms of smaller instances.\nExample (Merge Sort):\n\\[\nT(n) = 2T(n/2) + O(n)\n\\]\nThat means:\n\nIt divides the problem into 2 halves (\\(2T(n/2)\\))\nMerges results in \\(O(n)\\) time\n\nYou will often see recurrences like:\n\n\\(T(n) = T(n - 1) + O(1)\\)\n\\(T(n) = 2T(n/2) + O(n)\\)\n\\(T(n) = T(n/2) + O(1)\\)\n\nEach one represents a different structure of recursion.\n\n\nExample 1: Simple Linear Recurrence\nConsider this code:\nint count_down(int n) {\n    if (n == 0) return 0;\n    return 1 + count_down(n - 1);\n}\nThis calls itself once for each smaller input:\n\\[\nT(n) = T(n - 1) + O(1)\n\\]\nSolve it:\n\\[\nT(n) = O(n)\n\\]\nBecause it runs once per level.\n\n\nExample 2: Binary Recurrence\nFor binary recursion:\nint sum_tree(int n) {\n    if (n == 1) return 1;\n    return sum_tree(n/2) + sum_tree(n/2) + 1;\n}\nHere we do two subcalls on \\(n/2\\) and a constant amount of extra work:\n\\[\nT(n) = 2T(n/2) + O(1)\n\\]\nSolve it: \\(T(n) = O(n)\\)\nWhy? Each level doubles the number of calls but halves the size. There are \\(\\log n\\) levels, and total work adds up to \\(O(n)\\).\n\n\nSolving Recurrences\nThere are several ways to solve them:\n\nSubstitution Method Guess the solution, then prove it by induction.\nRecursion Tree Method Expand the recurrence into a tree and sum the cost per level.\nMaster Theorem Use a formula when the recurrence matches:\n\\[\nT(n) = aT(n/b) + f(n)\n\\]\n\n\n\nMaster Theorem (Quick Summary)\nIf \\(T(n) = aT(n/b) + f(n)\\), then:\n\nIf \\(f(n) = O(n^{\\log_b a - \\epsilon})\\), then \\(T(n) = \\Theta(n^{\\log_b a})\\)\nIf \\(f(n) = \\Theta(n^{\\log_b a})\\), then \\(T(n) = \\Theta(n^{\\log_b a} \\log n)\\)\nIf \\(f(n) = \\Omega(n^{\\log_b a + \\epsilon})\\), and the regularity condition holds, then \\(T(n) = \\Theta(f(n))\\)\n\nExample (Merge Sort): \\(a = 2\\), \\(b = 2\\), \\(f(n) = O(n)\\)\n\\[\nT(n) = 2T(n/2) + O(n) = O(n \\log n)\n\\]\n\n\nTiny Code\nLet’s write a quick recursive sum:\nint sum_array(int arr[], int l, int r) {\n    if (l == r) return arr[l];\n    int mid = (l + r) / 2;\n    return sum_array(arr, l, mid) + sum_array(arr, mid+1, r);\n}\nRecurrence:\n\\[\nT(n) = 2T(n/2) + O(1)\n\\]\n→ \\(O(n)\\)\nIf you added merging (like in merge sort), you would get \\(+O(n)\\):\n→ \\(O(n \\log n)\\)\n\n\nWhy It Matters\nRecurrence relations let you predict the cost of recursive solutions.\nWithout them, recursion feels like magic. With them, you can quantify efficiency.\nThey are key to understanding:\n\nDivide and Conquer\nDynamic Programming\nBacktracking\n\nOnce you can set up a recurrence, solving it becomes a game of algebra and logic.\n\n\nTry It Yourself\n\nWrite a recurrence for binary search. Solve it.\nWrite a recurrence for merge sort. Solve it.\nAnalyze this function:\nvoid fun(int n) {\n    if (n &lt;= 1) return;\n    fun(n/2);\n    fun(n/3);\n    fun(n/6);\n}\nWhat’s the recurrence? Approximate the complexity.\nExpand \\(T(n) = T(n-1) + 1\\) into its explicit sum.\n\nLearning recurrences helps you see inside recursion. They turn code into equations.\n\n\n\n6. Searching Basics\nBefore we sort or optimize, we need a way to find things. Searching is one of the most fundamental actions in computing, whether it’s looking up a name, finding a key, or checking if something exists.\nA search algorithm takes a collection (array, list, tree, etc.) and a target, and returns whether the target is present (and often its position).\nLet’s begin with two foundational techniques: Linear Search and Binary Search.\n\n1. Linear Search\nLinear search is the simplest method:\n\nStart at the beginning\nCheck each element in turn\nStop if you find the target\n\nIt works on any list, sorted or not, but can be slow for large data.\nint linear_search(int arr[], int n, int key) {\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] == key) return i;\n    }\n    return -1;\n}\nExample: If arr = [2, 4, 6, 8, 10] and key = 6, it finds it at index 2.\nComplexity:\n\nTime: \\(O(n)\\)\nSpace: \\(O(1)\\)\n\nLinear search is simple and guaranteed to find the target if it exists, but slow when lists are large.\n\n\n2. Binary Search\nWhen the list is sorted, we can do much better. Binary search repeatedly divides the search space in half.\nSteps:\n\nCheck the middle element\nIf it matches, you’re done\nIf target &lt; mid, search the left half\nElse, search the right half\n\nint binary_search(int arr[], int n, int key) {\n    int low = 0, high = n - 1;\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] == key) return mid;\n        else if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\nExample: arr = [2, 4, 6, 8, 10], key = 8\n\nmid = 6 → key &gt; mid → search right half\nmid = 8 → found\n\nComplexity:\n\nTime: \\(O(\\log n)\\)\nSpace: \\(O(1)\\)\n\nBinary search is a massive improvement; doubling input only adds one extra step.\n\n\n3. Recursive Binary Search\nBinary search can also be written recursively:\nint binary_search_rec(int arr[], int low, int high, int key) {\n    if (low &gt; high) return -1;\n    int mid = (low + high) / 2;\n    if (arr[mid] == key) return mid;\n    else if (arr[mid] &gt; key) return binary_search_rec(arr, low, mid - 1, key);\n    else return binary_search_rec(arr, mid + 1, high, key);\n}\nSame logic, different structure. Both iterative and recursive forms are equally efficient.\n\n\n4. Choosing Between Them\n\n\n\nMethod\nWorks On\nTime\nSpace\nNeeds Sorting\n\n\n\n\nLinear Search\nAny list\nO(n)\nO(1)\nNo\n\n\nBinary Search\nSorted list\nO(log n)\nO(1)\nYes\n\n\n\nIf data is unsorted or very small, linear search is fine. If data is sorted and large, binary search is far superior.\n\n\nTiny Code\nCompare the steps: For \\(n = 16\\):\n\nLinear search → up to 16 comparisons\nBinary search → \\(\\log_2 16 = 4\\) comparisons\n\nThat’s a huge difference.\n\n\nWhy It Matters\nSearching is the core of information retrieval. Every database, compiler, and system relies on it.\nUnderstanding simple searches prepares you for:\n\nHash tables (constant-time lookups)\nTree searches (ordered structures)\nGraph traversals (structured exploration)\n\nIt’s not just about finding values; it’s about learning how data structure and algorithm design fit together.\n\n\nTry It Yourself\n\nWrite a linear search that returns all indices where a target appears.\nModify binary search to return the first occurrence of a target in a sorted array.\nCompare runtime on arrays of size 10, 100, 1000.\nWhat happens if you run binary search on an unsorted list?\n\nSearch is the foundation. Once you master it, you’ll recognize its patterns everywhere.\n\n\n\n7. Sorting Basics\nSorting is one of the most studied problems in computer science. Why? Because order matters. It makes searching faster, patterns clearer, and data easier to manage.\nA sorting algorithm arranges elements in a specific order (usually ascending or descending). Once sorted, many operations (like binary search, merging, or deduplication) become much simpler.\nLet’s explore the foundational sorting methods and the principles behind them.\n\n1. What Makes a Sort Algorithm\nA sorting algorithm should define:\n\nInput: a sequence of elements\nOutput: the same elements, in sorted order\nStability: keeps equal elements in the same order (important for multi-key sorts)\nIn-place: uses only a constant amount of extra space\n\nDifferent algorithms balance speed, memory, and simplicity.\n\n\n2. Bubble Sort\nIdea: repeatedly “bubble up” the largest element to the end by swapping adjacent pairs.\nvoid bubble_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        for (int j = 0; j &lt; n - i - 1; j++) {\n            if (arr[j] &gt; arr[j + 1]) {\n                int temp = arr[j];\n                arr[j] = arr[j + 1];\n                arr[j + 1] = temp;\n            }\n        }\n    }\n}\nEach pass moves the largest remaining item to its final position.\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(1)\\)\nStable: Yes\n\nSimple but inefficient for large data.\n\n\n3. Selection Sort\nIdea: repeatedly select the smallest element and put it in the correct position.\nvoid selection_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int min_idx = i;\n        for (int j = i + 1; j &lt; n; j++) {\n            if (arr[j] &lt; arr[min_idx]) min_idx = j;\n        }\n        int temp = arr[i];\n        arr[i] = arr[min_idx];\n        arr[min_idx] = temp;\n    }\n}\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(1)\\)\nStable: No\n\nFewer swaps, but still quadratic in time.\n\n\n4. Insertion Sort\nIdea: build the sorted list one item at a time, inserting each new item in the right place.\nvoid insertion_sort(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j &gt;= 0 && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nTime: \\(O(n^2)\\) (best case \\(O(n)\\) when nearly sorted)\nSpace: \\(O(1)\\)\nStable: Yes\n\nInsertion sort is great for small or nearly sorted datasets. It is often used as a base in hybrid sorts like Timsort.\n\n\n5. Comparing the Basics\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nStable\nIn-place\n\n\n\n\nBubble Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\n\n\nSelection Sort\nO(n²)\nO(n²)\nO(n²)\nNo\nYes\n\n\nInsertion Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\n\n\n\nAll three are quadratic in time, but Insertion Sort performs best on small or partially sorted data.\n\n\nTiny Code\nQuick check with arr = [5, 3, 4, 1, 2]:\nInsertion Sort (step by step)\n\nInsert 3 before 5 → [3, 5, 4, 1, 2]\nInsert 4 → [3, 4, 5, 1, 2]\nInsert 1 → [1, 3, 4, 5, 2]\nInsert 2 → [1, 2, 3, 4, 5]\n\nSorted!\n\n\nWhy It Matters\nSorting is a gateway algorithm. It teaches you about iteration, swapping, and optimization.\nEfficient sorting is critical for:\n\nPreprocessing data for binary search\nOrganizing data for analysis\nBuilding indexes and ranking systems\n\nIt’s the first step toward deeper concepts like divide and conquer and hybrid optimization.\n\n\nTry It Yourself\n\nImplement all three: bubble, selection, insertion\nTest them on arrays of size 10, 100, 1000, and note timing differences\nTry sorting an array that’s already sorted. Which one adapts best?\nModify insertion sort to sort in descending order\n\nSorting may seem simple, but it’s a cornerstone. Mastering it will shape your intuition for almost every other algorithm.\n\n\n\n8. Data Structures Overview\nAlgorithms and data structures are two sides of the same coin. An algorithm is how you solve a problem. A data structure is where you store and organize data so that your algorithm can work efficiently.\nYou can think of data structures as containers, each one shaped for specific access patterns, trade-offs, and performance needs. Choosing the right one is often the key to designing a fast algorithm.\n\n1. Why Data Structures Matter\nImagine you want to find a book quickly.\n\nIf all books are piled randomly → you must scan every one (\\(O(n)\\))\nIf they’re sorted on a shelf → you can use binary search (\\(O(\\log n)\\))\nIf you have an index or catalog → you can find it instantly (\\(O(1)\\))\n\nDifferent structures unlock different efficiencies.\n\n\n2. The Core Data Structures\nLet’s walk through the most essential ones:\n\n\n\n\n\n\n\n\n\nType\nDescription\nKey Operations\nTypical Use\n\n\n\n\nArray\nFixed-size contiguous memory\nAccess (\\(O(1)\\)), Insert/Delete (\\(O(n)\\))\nFast index access\n\n\nLinked List\nSequence of nodes with pointers\nInsert/Delete (\\(O(1)\\)), Access (\\(O(n)\\))\nDynamic sequences\n\n\nStack\nLIFO (last-in, first-out)\npush(), pop() in \\(O(1)\\)\nUndo, recursion\n\n\nQueue\nFIFO (first-in, first-out)\nenqueue(), dequeue() in \\(O(1)\\)\nScheduling, buffers\n\n\nHash Table\nKey-value pairs via hashing\nAverage \\(O(1)\\), Worst \\(O(n)\\)\nLookup, caching\n\n\nHeap\nPartially ordered tree\nInsert \\(O(\\log n)\\), Extract-Min \\(O(\\log n)\\)\nPriority queues\n\n\nTree\nHierarchical structure\nAccess \\(O(\\log n)\\) (balanced)\nSorted storage\n\n\nGraph\nNodes + edges\nTraversal \\(O(V+E)\\)\nNetworks, paths\n\n\nSet / Map\nUnique keys or key-value pairs\n\\(O(\\log n)\\) or \\(O(1)\\)\nMembership tests\n\n\n\nEach comes with trade-offs. Arrays are fast but rigid, linked lists are flexible but slower to access, and hash tables are lightning-fast but unordered.\n\n\n3. Abstract Data Types (ADTs)\nAn ADT defines what operations you can do, not how they’re implemented. For example, a Stack ADT promises:\n\npush(x)\npop()\npeek()\n\nIt can be implemented with arrays or linked lists, the behavior stays the same.\nCommon ADTs:\n\nStack\nQueue\nDeque\nPriority Queue\nMap / Dictionary\n\nThis separation of interface and implementation helps design flexible systems.\n\n\n4. The Right Tool for the Job\nChoosing the correct data structure often decides the performance of your algorithm:\n\n\n\nProblem\nGood Choice\nReason\n\n\n\n\nUndo feature\nStack\nLIFO fits history\n\n\nScheduling tasks\nQueue\nFIFO order\n\n\nDijkstra’s algorithm\nPriority Queue\nExtract smallest distance\n\n\nCounting frequencies\nHash Map\nFast key lookup\n\n\nDynamic median\nHeap + Heap\nBalance two halves\n\n\nSearch by prefix\nTrie\nFast prefix lookups\n\n\n\nGood programmers don’t just write code. They pick the right structure.\n\n\nTiny Code\nExample: comparing array vs linked list\nArray:\nint arr[5] = {1, 2, 3, 4, 5};\nprintf(\"%d\", arr[3]); // O(1)\nLinked List:\nstruct Node { int val; struct Node* next; };\nTo get the 4th element, you must traverse → \\(O(n)\\)\nDifferent structures, different access costs.\n\n\nWhy It Matters\nEvery efficient algorithm depends on the right data structure.\n\nSearching, sorting, and storing all rely on structure\nMemory layout affects cache performance\nThe wrong choice can turn \\(O(1)\\) into \\(O(n^2)\\)\n\nUnderstanding these structures is like knowing the tools in a workshop. Once you recognize their shapes, you’ll instinctively know which to grab.\n\n\nTry It Yourself\n\nImplement a stack using an array. Then implement it using a linked list.\nWrite a queue using two stacks.\nTry storing key-value pairs in a hash table (hint: mod by table size).\nCompare access times for arrays vs linked lists experimentally.\n\nData structures aren’t just storage. They are the skeletons your algorithms stand on.\n\n\n\n9. Graphs and Trees Overview\nNow that you’ve seen linear structures like arrays and linked lists, it’s time to explore nonlinear structures, graphs and trees. These are the shapes behind networks, hierarchies, and relationships.\nThey appear everywhere: family trees, file systems, maps, social networks, and knowledge graphs all rely on them.\n\n1. Trees\nA tree is a connected structure with no cycles. It’s a hierarchy, and every node (except the root) has one parent.\n\nRoot: the top node\nChild: a node directly connected below\nLeaf: a node with no children\nHeight: the longest path from root to a leaf\n\nA binary tree is one where each node has at most two children. A binary search tree (BST) keeps elements ordered:\n\nLeft child &lt; parent &lt; right child\n\nBasic operations:\n\nInsert\nSearch\nDelete\nTraverse (preorder, inorder, postorder, level-order)\n\nExample:\nstruct Node {\n    int val;\n    struct Node *left, *right;\n};\nInsert in BST:\nstruct Node* insert(struct Node* root, int val) {\n    if (!root) return newNode(val);\n    if (val &lt; root-&gt;val) root-&gt;left = insert(root-&gt;left, val);\n    else root-&gt;right = insert(root-&gt;right, val);\n    return root;\n}\n\n\n2. Common Tree Types\n\n\n\n\n\n\n\n\nType\nDescription\nUse Case\n\n\n\n\nBinary Tree\nEach node has ≤ 2 children\nGeneral hierarchy\n\n\nBinary Search Tree\nLeft &lt; Root &lt; Right\nOrdered data\n\n\nAVL / Red-Black Tree\nSelf-balancing BST\nFast search/insert\n\n\nHeap\nComplete binary tree, parent ≥ or ≤ children\nPriority queues\n\n\nTrie\nTree of characters\nPrefix search\n\n\nSegment Tree\nTree over ranges\nRange queries\n\n\nFenwick Tree\nTree with prefix sums\nEfficient updates\n\n\n\nBalanced trees keep height \\(O(\\log n)\\), guaranteeing fast operations.\n\n\n3. Graphs\nA graph generalizes the idea of trees. In graphs, nodes (vertices) can connect freely.\nA graph is a set of vertices (\\(V\\)) and edges (\\(E\\)):\n\\[\nG = (V, E)\n\\]\nDirected vs Undirected:\n\nDirected: edges have direction (A → B)\nUndirected: edges connect both ways (A, B)\n\nWeighted vs Unweighted:\n\nWeighted: each edge has a cost\nUnweighted: all edges are equal\n\nRepresentation:\n\nAdjacency Matrix: \\(n \\times n\\) matrix; entry \\((i, j) = 1\\) if edge exists\nAdjacency List: array of lists; each vertex stores its neighbors\n\nExample adjacency list:\nvector&lt;int&gt; graph[n];\ngraph[0].push_back(1);\ngraph[0].push_back(2);\n\n\n4. Common Graph Types\n\n\n\n\n\n\n\n\nGraph Type\nDescription\nExample\n\n\n\n\nUndirected\nEdges without direction\nFriendship network\n\n\nDirected\nArrows indicate direction\nWeb links\n\n\nWeighted\nEdges have costs\nRoad network\n\n\nCyclic\nContains loops\nTask dependencies\n\n\nAcyclic\nNo loops\nFamily tree\n\n\nDAG (Directed Acyclic)\nDirected, no cycles\nScheduling, compilers\n\n\nComplete\nAll pairs connected\nDense networks\n\n\nSparse\nFew edges\nReal-world graphs\n\n\n\n\n\n5. Basic Graph Operations\n\nAdd Vertex / Edge\nTraversal: Depth-First Search (DFS), Breadth-First Search (BFS)\nPath Finding: Dijkstra, Bellman-Ford\nConnectivity: Union-Find, Tarjan (SCC)\nSpanning Trees: Kruskal, Prim\n\nEach graph problem has its own flavor, from finding shortest paths to detecting cycles.\n\n\nTiny Code\nBreadth-first search (BFS):\nvoid bfs(int start, vector&lt;int&gt; graph[], int n) {\n    bool visited[n];\n    memset(visited, false, sizeof(visited));\n    queue&lt;int&gt; q;\n    visited[start] = true;\n    q.push(start);\n    while (!q.empty()) {\n        int node = q.front(); q.pop();\n        printf(\"%d \", node);\n        for (int neighbor : graph[node]) {\n            if (!visited[neighbor]) {\n                visited[neighbor] = true;\n                q.push(neighbor);\n            }\n        }\n    }\n}\nThis explores level by level, perfect for shortest paths in unweighted graphs.\n\n\nWhy It Matters\nTrees and graphs model relationships and connections, not just sequences. They are essential for:\n\nSearch engines (web graph)\nCompilers (syntax trees, dependency DAGs)\nAI (state spaces, decision trees)\nDatabases (indexes, joins, relationships)\n\nUnderstanding them unlocks an entire world of algorithms, from DFS and BFS to Dijkstra, Kruskal, and beyond.\n\n\nTry It Yourself\n\nBuild a simple binary search tree and implement inorder traversal.\nRepresent a graph with adjacency lists and print all edges.\nWrite a DFS and BFS for a small graph.\nDraw a directed graph with a cycle and detect it manually.\n\nGraphs and trees move you beyond linear thinking. They let you explore connections, not just collections.\n\n\n\n10. Algorithm Design Patterns\nBy now, you’ve seen what algorithms are and how they’re analyzed. You’ve explored searches, sorts, structures, and recursion. The next step is learning patterns, reusable strategies that guide how you build new algorithms from scratch.\nJust like design patterns in software architecture, algorithmic design patterns give structure to your thinking. Once you recognize them, many problems suddenly feel familiar.\n\n1. Brute Force\nStart simple. Try every possibility and pick the best result. Brute force is often your baseline, clear but inefficient.\nExample: Find the maximum subarray sum by checking all subarrays.\n\nTime: \\(O(n^2)\\)\nAdvantage: easy to reason about\nDisadvantage: explodes for large input\n\nSometimes, brute force helps you see the structure needed for a better approach.\n\n\n2. Divide and Conquer\nSplit the problem into smaller parts, solve each, and combine. Ideal for problems with self-similarity.\nClassic examples:\n\nMerge Sort → split and merge\nBinary Search → halve the search space\nQuick Sort → partition and sort\n\nGeneral form:\n\\[\nT(n) = aT(n/b) + f(n)\n\\]\nUse recurrence relations and the Master Theorem to analyze them.\n\n\n3. Greedy\nMake the best local decision at each step. Works only when local optimal choices lead to a global optimum.\nExamples:\n\nActivity Selection\nHuffman Coding\nDijkstra (for non-negative weights)\n\nGreedy algorithms are simple and fast when they fit.\n\n\n4. Dynamic Programming (DP)\nWhen subproblems overlap, store results and reuse them. Think recursion plus memory.\nTwo main styles:\n\nTop-Down (Memoization): recursive with caching\nBottom-Up (Tabulation): iterative filling table\n\nUsed in:\n\nFibonacci numbers\nKnapsack\nLongest Increasing Subsequence (LIS)\nMatrix Chain Multiplication\n\nDP transforms exponential recursion into polynomial time.\n\n\n5. Backtracking\nExplore all possibilities, but prune when constraints fail. It is brute force with early exits.\nPerfect for:\n\nN-Queens\nSudoku\nPermutation generation\nSubset sums\n\nBacktracking builds solutions incrementally, abandoning paths that cannot lead to a valid result.\n\n\n6. Two Pointers\nMove two indices through a sequence to find patterns or meet conditions.\nCommon use:\n\nSorted arrays (sum pairs, partitions)\nString problems (palindromes, sliding windows)\nLinked lists (slow and fast pointers)\n\nSimple, but surprisingly powerful.\n\n\n7. Sliding Window\nMaintain a window over data, expand or shrink it as needed.\nUsed for:\n\nMaximum sum subarray (Kadane’s algorithm)\nSubstrings of length \\(k\\)\nLongest substring without repeating characters\n\nHelps reduce \\(O(n^2)\\) to \\(O(n)\\) in sequence problems.\n\n\n8. Binary Search on Answer\nSometimes, the input is not sorted, but the answer space is. If you can define a function check(mid) that is monotonic (true or false changes once), you can apply binary search on possible answers.\nExamples:\n\nMinimum capacity to ship packages in D days\nSmallest feasible value satisfying a constraint\n\nPowerful for optimization under monotonic conditions.\n\n\n9. Graph-Based\nThink in terms of nodes and edges, paths and flows.\nPatterns include:\n\nBFS and DFS (exploration)\nTopological Sort (ordering)\nDijkstra and Bellman-Ford (shortest paths)\nUnion-Find (connectivity)\nKruskal and Prim (spanning trees)\n\nGraphs often reveal relationships hidden in data.\n\n\n10. Meet in the Middle\nSplit the problem into two halves, compute all possibilities for each, and combine efficiently. Used in problems where brute force \\(O(2^n)\\) is too large but \\(O(2^{n/2})\\) is manageable.\nExamples:\n\nSubset sum (divide into two halves)\nSearch problems in combinatorics\n\nA clever compromise between brute force and efficiency.\n\n\nTiny Code\nExample: Two Pointers to find a pair sum\nint find_pair_sum(int arr[], int n, int target) {\n    int i = 0, j = n - 1;\n    while (i &lt; j) {\n        int sum = arr[i] + arr[j];\n        if (sum == target) return 1;\n        else if (sum &lt; target) i++;\n        else j--;\n    }\n    return 0;\n}\nWorks in \\(O(n)\\) for sorted arrays, elegant and fast.\n\n\nWhy It Matters\nPatterns are mental shortcuts. They turn “blank page” problems into “I’ve seen this shape before.”\nOnce you recognize the structure, you can choose a suitable pattern and adapt it. This is how top coders solve complex problems under time pressure, not by memorizing algorithms, but by seeing patterns.\n\n\nTry It Yourself\n\nWrite a brute-force and a divide-and-conquer solution for maximum subarray sum. Compare speed.\nSolve the coin change problem using both greedy and DP.\nImplement N-Queens with backtracking.\nUse two pointers to find the smallest window with a given sum.\nPick a problem you’ve solved before. Can you reframe it using a different design pattern?\n\nThe more patterns you practice, the faster you will map new problems to known strategies, and the more powerful your algorithmic intuition will become.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-2.-sorting-and-searching",
    "href": "books/en-us/book.html#chapter-2.-sorting-and-searching",
    "title": "The Book",
    "section": "Chapter 2. Sorting and Searching",
    "text": "Chapter 2. Sorting and Searching\n\n11. Elementary Sorting (Bubble, Insertion, Selection)\nBefore diving into advanced sorts like mergesort or heapsort, it’s important to understand the elementary sorting algorithms , the building blocks. They’re simple, intuitive, and great for learning how sorting works under the hood.\nIn this section, we’ll cover three classics:\n\nBubble Sort - swap adjacent out-of-order pairs- Selection Sort - select the smallest element each time- Insertion Sort - insert elements one by one in order These algorithms share ( O\\(n^2\\) ) time complexity but differ in behavior and stability.\n\n\n1. Bubble Sort\nIdea: Compare adjacent pairs and swap if they’re out of order. Repeat until the array is sorted. Each pass “bubbles” the largest element to the end.\nSteps:\n\nCompare arr[j] and arr[j+1]\nSwap if arr[j] &gt; arr[j+1]\nContinue passes until no swaps are needed\n\nCode:\nvoid bubble_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int swapped = 0;\n        for (int j = 0; j &lt; n - i - 1; j++) {\n            if (arr[j] &gt; arr[j + 1]) {\n                int temp = arr[j];\n                arr[j] = arr[j + 1];\n                arr[j + 1] = temp;\n                swapped = 1;\n            }\n        }\n        if (!swapped) break;\n    }\n}\nComplexity:\n\nBest: ( O(n) ) (already sorted)- Worst: ( O\\(n^2\\) )- Space: ( O(1) )- Stable: Yes Intuition: Imagine bubbles rising , after each pass, the largest “bubble” settles at the top.\n\n\n\n2. Selection Sort\nIdea: Find the smallest element and place it at the front.\nSteps:\n\nFor each position i, find the smallest element in the remainder of the array\nSwap it with arr[i]\n\nCode:\nvoid selection_sort(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int min_idx = i;\n        for (int j = i + 1; j &lt; n; j++) {\n            if (arr[j] &lt; arr[min_idx])\n                min_idx = j;\n        }\n        int temp = arr[i];\n        arr[i] = arr[min_idx];\n        arr[min_idx] = temp;\n    }\n}\nComplexity:\n\nBest: ( O\\(n^2\\) )- Worst: ( O\\(n^2\\) )- Space: ( O(1) )- Stable: No Intuition: Selection sort “selects” the next correct element and fixes it. It minimizes swaps but still scans all elements.\n\n\n\n3. Insertion Sort\nIdea: Build a sorted array one element at a time by inserting each new element into its correct position.\nSteps:\n\nStart from index 1\nCompare with previous elements\nShift elements greater than key to the right\nInsert key into the correct place\n\nCode:\nvoid insertion_sort(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = arr[i];\n        int j = i - 1;\n        while (j &gt;= 0 && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\nComplexity:\n\nBest: ( O(n) ) (nearly sorted)- Worst: ( O\\(n^2\\) )- Space: ( O(1) )- Stable: Yes Intuition: It’s like sorting cards in your hand , take the next card and slide it into the right place.\n\n\n\n4. Comparing the Three\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nStable\nIn-Place\nNotes\n\n\n\n\nBubble Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\nEarly exit possible\n\n\nSelection Sort\nO(n²)\nO(n²)\nO(n²)\nNo\nYes\nFew swaps\n\n\nInsertion Sort\nO(n)\nO(n²)\nO(n²)\nYes\nYes\nGreat on small or nearly sorted data\n\n\n\n\n\nTiny Code\nLet’s see how insertion sort works on [5, 3, 4, 1, 2]:\n\nStart with 3 → insert before 5 → [3, 5, 4, 1, 2]- Insert 4 → [3, 4, 5, 1, 2]- Insert 1 → [1, 3, 4, 5, 2]- Insert 2 → [1, 2, 3, 4, 5] Sorted in five passes.\n\n\n\nWhy It Matters\nElementary sorts teach you:\n\nHow comparisons and swaps drive order- The trade-off between simplicity and efficiency- How to reason about stability and adaptability While these aren’t used for large datasets in practice, they’re used inside hybrid algorithms like Timsort and IntroSort, which switch to insertion sort for small chunks.\n\n\n\nTry It Yourself\n\nImplement all three and print the array after each pass.\nTest on arrays: already sorted, reversed, random, partially sorted.\nModify bubble sort to sort descending.\nTry insertion sort on 10,000 elements and note its behavior.\nCan you detect when the list is already sorted and stop early?\n\nStart simple. Master these patterns. They’ll be your foundation for everything from merge sort to radix sort.\n\n\n\n12. Divide-and-Conquer Sorting (Merge, Quick, Heap)\nElementary sorts are great for learning, but their (O\\(n^2\\)) runtime quickly becomes a bottleneck. To scale beyond small arrays, we need algorithms that divide problems into smaller parts, sort them independently, and combine the results.\nThis is the essence of divide and conquer , break it down, solve subproblems, merge solutions. In sorting, this approach yields some of the fastest general-purpose algorithms: Merge Sort, Quick Sort, and Heap Sort.\n\n1. Merge Sort\nIdea: Split the array in half, sort each half recursively, then merge the two sorted halves.\nMerge sort is stable, works well with linked lists, and guarantees (O\\(n \\log n\\)) time.\nSteps:\n\nDivide the array into halves\nRecursively sort each half\nMerge two sorted halves into one\n\nCode:\nvoid merge(int arr[], int l, int m, int r) {\n    int n1 = m - l + 1;\n    int n2 = r - m;\n    int L[n1], R[n2];\n    for (int i = 0; i &lt; n1; i++) L[i] = arr[l + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = arr[m + 1 + j];\n    int i = 0, j = 0, k = l;\n    while (i &lt; n1 && j &lt; n2) {\n        if (L[i] &lt;= R[j]) arr[k++] = L[i++];\n        else arr[k++] = R[j++];\n    }\n    while (i &lt; n1) arr[k++] = L[i++];\n    while (j &lt; n2) arr[k++] = R[j++];\n}\n\nvoid merge_sort(int arr[], int l, int r) {\n    if (l &lt; r) {\n        int m = (l + r) / 2;\n        merge_sort(arr, l, m);\n        merge_sort(arr, m + 1, r);\n        merge(arr, l, m, r);\n    }\n}\nComplexity:\n\nTime: (O\\(n \\log n\\)) (always)- Space: (O(n)) (temporary arrays)- Stable: Yes Merge sort is predictable, making it ideal for external sorting (like sorting data on disk).\n\n\n\n2. Quick Sort\nIdea: Pick a pivot, partition the array so smaller elements go left and larger go right, then recursively sort both sides.\nQuick sort is usually the fastest in practice due to good cache locality and low constant factors.\nSteps:\n\nChoose a pivot (often middle or random)\nPartition: move smaller elements to left, larger to right\nRecursively sort the two partitions\n\nCode:\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            i++;\n            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;\n        }\n    }\n    int tmp = arr[i + 1]; arr[i + 1] = arr[high]; arr[high] = tmp;\n    return i + 1;\n}\n\nvoid quick_sort(int arr[], int low, int high) {\n    if (low &lt; high) {\n        int pi = partition(arr, low, high);\n        quick_sort(arr, low, pi - 1);\n        quick_sort(arr, pi + 1, high);\n    }\n}\nComplexity:\n\nBest / Average: (O\\(n \\log n\\))- Worst: (O\\(n^2\\)) (bad pivot, e.g. sorted input with naive pivot)- Space: (O\\(\\log n\\)) (recursion)- Stable: No (unless modified) Quick sort is often used in standard libraries due to its efficiency in real-world workloads.\n\n\n\n3. Heap Sort\nIdea: Turn the array into a heap, repeatedly extract the largest element, and place it at the end.\nA heap is a binary tree where every parent is ≥ its children (max-heap).\nSteps:\n\nBuild a max-heap\nSwap the root (max) with the last element\nReduce heap size, re-heapify\nRepeat until sorted\n\nCode:\nvoid heapify(int arr[], int n, int i) {\n    int largest = i;\n    int l = 2 * i + 1;\n    int r = 2 * i + 2;\n    if (l &lt; n && arr[l] &gt; arr[largest]) largest = l;\n    if (r &lt; n && arr[r] &gt; arr[largest]) largest = r;\n    if (largest != i) {\n        int tmp = arr[i]; arr[i] = arr[largest]; arr[largest] = tmp;\n        heapify(arr, n, largest);\n    }\n}\n\nvoid heap_sort(int arr[], int n) {\n    for (int i = n / 2 - 1; i &gt;= 0; i--)\n        heapify(arr, n, i);\n    for (int i = n - 1; i &gt; 0; i--) {\n        int tmp = arr[0]; arr[0] = arr[i]; arr[i] = tmp;\n        heapify(arr, i, 0);\n    }\n}\nComplexity:\n\nTime: (O\\(n \\log n\\))- Space: (O(1))- Stable: No Heap sort is reliable and space-efficient but less cache-friendly than quicksort.\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nSpace\nStable\nNotes\n\n\n\n\nMerge Sort\nO(n log n)\nO(n log n)\nO(n log n)\nO(n)\nYes\nPredictable, stable\n\n\nQuick Sort\nO(n log n)\nO(n log n)\nO(n²)\nO(log n)\nNo\nFast in practice\n\n\nHeap Sort\nO(n log n)\nO(n log n)\nO(n log n)\nO(1)\nNo\nIn-place, robust\n\n\n\nEach one fits a niche:\n\nMerge Sort → stability and guarantees- Quick Sort → speed and cache performance- Heap Sort → low memory usage and simplicity\n\n\n\nTiny Code\nTry sorting [5, 1, 4, 2, 8] with merge sort:\n\nSplit → [5,1,4], [2,8]\nSort each → [1,4,5], [2,8]\nMerge → [1,2,4,5,8]\n\nEach recursive split halves the problem, yielding (O\\(\\log n\\)) depth with (O(n)) work per level.\n\n\nWhy It Matters\nDivide-and-conquer sorting is the foundation for efficient order processing. It introduces ideas you’ll reuse in:\n\nBinary search (halving)- Matrix multiplication- Fast Fourier Transform- Dynamic programming These sorts teach how recursion, partitioning, and merging combine into scalable solutions.\n\n\n\nTry It Yourself\n\nImplement merge sort, quick sort, and heap sort.\nTest all three on the same random array. Compare runtime.\nModify quick sort to use a random pivot.\nBuild a stable version of heap sort.\nVisualize merge sort’s recursion tree and merging process.\n\nMastering these sorts gives you a template for solving any divide-and-conquer problem efficiently.\n\n\n\n13. Counting and Distribution Sorts (Counting, Radix, Bucket)\nSo far, we’ve seen comparison-based sorts like merge sort and quicksort. These rely on comparing elements and are bounded by the O(n log n) lower limit for comparisons.\nBut what if you don’t need to compare elements directly , what if they’re integers or values from a limited range?\nThat’s where counting and distribution sorts come in. They exploit structure, not just order, to achieve linear-time sorting in the right conditions.\n\n1. Counting Sort\nIdea: If your elements are integers in a known range ([0, k)), you can count occurrences of each value, then reconstruct the sorted output.\nCounting sort doesn’t compare , it counts.\nSteps:\n\nFind the range of input (max value (k))\nCount occurrences in a frequency array\nConvert counts to cumulative counts\nPlace elements into their sorted positions\n\nCode:\nvoid counting_sort(int arr[], int n, int k) {\n    int count[k + 1];\n    int output[n];\n    for (int i = 0; i &lt;= k; i++) count[i] = 0;\n    for (int i = 0; i &lt; n; i++) count[arr[i]]++;\n    for (int i = 1; i &lt;= k; i++) count[i] += count[i - 1];\n    for (int i = n - 1; i &gt;= 0; i--) {\n        output[count[arr[i]] - 1] = arr[i];\n        count[arr[i]]--;\n    }\n    for (int i = 0; i &lt; n; i++) arr[i] = output[i];\n}\nExample: arr = [4, 2, 2, 8, 3, 3, 1], k = 8 → count = [0,1,2,2,1,0,0,0,1] → cumulative = [0,1,3,5,6,6,6,6,7] → sorted = [1,2,2,3,3,4,8]\nComplexity:\n\nTime: (O(n + k))- Space: (O(k))- Stable: Yes When to use:\nInput is integers- Range (k) not much larger than (n)\n\n\n\n2. Radix Sort\nIdea: Sort digits one at a time, from least significant (LSD) or most significant (MSD), using a stable sub-sort like counting sort.\nRadix sort works best when all elements have fixed-length representations (e.g., integers, strings of equal length).\nSteps (LSD method):\n\nFor each digit position (from rightmost to leftmost)\nSort all elements by that digit using a stable sort (like counting sort)\n\nCode:\nint get_max(int arr[], int n) {\n    int mx = arr[0];\n    for (int i = 1; i &lt; n; i++)\n        if (arr[i] &gt; mx) mx = arr[i];\n    return mx;\n}\n\nvoid counting_sort_digit(int arr[], int n, int exp) {\n    int output[n];\n    int count[10] = {0};\n    for (int i = 0; i &lt; n; i++)\n        count[(arr[i] / exp) % 10]++;\n    for (int i = 1; i &lt; 10; i++)\n        count[i] += count[i - 1];\n    for (int i = n - 1; i &gt;= 0; i--) {\n        int digit = (arr[i] / exp) % 10;\n        output[count[digit] - 1] = arr[i];\n        count[digit]--;\n    }\n    for (int i = 0; i &lt; n; i++)\n        arr[i] = output[i];\n}\n\nvoid radix_sort(int arr[], int n) {\n    int m = get_max(arr, n);\n    for (int exp = 1; m / exp &gt; 0; exp *= 10)\n        counting_sort_digit(arr, n, exp);\n}\nExample: arr = [170, 45, 75, 90, 802, 24, 2, 66] → sort by 1s → 10s → 100s → final = [2, 24, 45, 66, 75, 90, 170, 802]\nComplexity:\n\nTime: (O(d (n + b))), where\n\n(d): number of digits - (b): base (10 for decimal)- Space: (O(n + b))- Stable: Yes When to use:\n\nFixed-length numbers- Bounded digits (e.g., base 10 or 2)\n\n\n\n3. Bucket Sort\nIdea: Divide elements into buckets based on value ranges, sort each bucket individually, then concatenate.\nWorks best when data is uniformly distributed in a known interval.\nSteps:\n\nCreate (k) buckets for value ranges\nDistribute elements into buckets\nSort each bucket (often using insertion sort)\nMerge buckets\n\nCode:\nvoid bucket_sort(float arr[], int n) {\n    vector&lt;float&gt; buckets[n];\n    for (int i = 0; i &lt; n; i++) {\n        int idx = n * arr[i]; // assuming 0 &lt;= arr[i] &lt; 1\n        buckets[idx].push_back(arr[i]);\n    }\n    for (int i = 0; i &lt; n; i++)\n        sort(buckets[i].begin(), buckets[i].end());\n    int idx = 0;\n    for (int i = 0; i &lt; n; i++)\n        for (float val : buckets[i])\n            arr[idx++] = val;\n}\nComplexity:\n\nAverage: (O(n + k))- Worst: (O\\(n^2\\)) (if all fall in one bucket)- Space: (O(n + k))- Stable: Depends on bucket sort method When to use:\nReal numbers uniformly distributed in ([0,1))\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nStable\nType\nBest Use\n\n\n\n\nCounting Sort\nO(n + k)\nO(k)\nYes\nNon-comparison\nSmall integer range\n\n\nRadix Sort\nO(d(n + b))\nO(n + b)\nYes\nNon-comparison\nFixed-length numbers\n\n\nBucket Sort\nO(n + k) avg\nO(n + k)\nOften\nDistribution-based\nUniform floats\n\n\n\nThese algorithms achieve O(n) behavior when assumptions hold , they’re specialized but incredibly fast when applicable.\n\n\nTiny Code\nLet’s walk counting sort on arr = [4, 2, 2, 8, 3, 3, 1]:\n\nCount occurrences → [1,2,2,1,0,0,0,1]- Cumulative count → positions- Place elements → [1,2,2,3,3,4,8] Sorted , no comparisons.\n\n\n\nWhy It Matters\nDistribution sorts teach a key insight:\n\nIf you know the structure of your data, you can sort faster than comparison allows.\n\nThey show how data properties , range, distribution, digit length , can drive algorithm design.\nYou’ll meet these ideas again in:\n\nHashing (bucketing)- Indexing (range partitioning)- Machine learning (binning, histogramming)\n\n\n\nTry It Yourself\n\nImplement counting sort for integers from 0 to 100.\nExtend radix sort to sort strings by character.\nVisualize bucket sort for values between 0 and 1.\nWhat happens if you use counting sort on negative numbers? Fix it.\nCompare counting vs quick sort on small integer arrays.\n\nThese are the first glimpses of linear-time sorting , harnessing knowledge about data to break the (O\\(n \\log n\\)) barrier.\n\n\n\n14. Hybrid Sorts (IntroSort, Timsort)\nIn practice, no single sorting algorithm is perfect for all cases. Some are fast on average but fail in worst cases (like Quick Sort). Others are consistent but slow due to overhead (like Merge Sort). Hybrid sorting algorithms combine multiple techniques to get the best of all worlds , practical speed, stability, and guaranteed performance.\nTwo of the most widely used hybrids in modern systems are IntroSort and Timsort , both power the sorting functions in major programming languages.\n\n1. The Idea Behind Hybrid Sorting\nReal-world data is messy: sometimes nearly sorted, sometimes random, sometimes pathological. A smart sorting algorithm should adapt to the data.\nHybrids switch between different strategies based on:\n\nInput size- Recursion depth- Degree of order- Performance thresholds So, the algorithm “introspects” or “adapts” while running.\n\n\n\n2. IntroSort\nIntroSort (short for introspective sort) begins like Quick Sort, but when recursion gets too deep , which means Quick Sort’s worst case may be coming , it switches to Heap Sort to guarantee (O\\(n \\log n\\)) time.\nSteps:\n\nUse Quick Sort as long as recursion depth &lt; \\(2 \\log n\\)\nIf depth exceeds limit → switch to Heap Sort\nFor very small subarrays → switch to Insertion Sort\n\nThis triple combo ensures:\n\nFast average case (Quick Sort)- Guaranteed upper bound (Heap Sort)- Efficiency on small arrays (Insertion Sort) Code Sketch:\n\nvoid intro_sort(int arr[], int n) {\n    int depth_limit = 2 * log(n);\n    intro_sort_util(arr, 0, n - 1, depth_limit);\n}\n\nvoid intro_sort_util(int arr[], int begin, int end, int depth_limit) {\n    int size = end - begin + 1;\n    if (size &lt; 16) {\n        insertion_sort(arr + begin, size);\n        return;\n    }\n    if (depth_limit == 0) {\n        heap_sort_range(arr, begin, end);\n        return;\n    }\n    int pivot = partition(arr, begin, end);\n    intro_sort_util(arr, begin, pivot - 1, depth_limit - 1);\n    intro_sort_util(arr, pivot + 1, end, depth_limit - 1);\n}\nComplexity:\n\nAverage: (O\\(n \\log n\\))- Worst: (O\\(n \\log n\\))- Space: (O\\(\\log n\\))- Stable: No (depends on partition scheme) Used in:\nC++ STL’s std::sort- Many systems where performance guarantees matter\n\n\n\n3. Timsort\nTimsort is a stable hybrid combining Insertion Sort and Merge Sort. It was designed to handle real-world data, which often has runs (already sorted segments).\nDeveloped by Tim Peters (Python core dev), Timsort is now used in:\n\nPython’s sorted() and .sort()- Java’s Arrays.sort() for objects Idea:\nIdentify runs , segments already ascending or descending- Reverse descending runs (to make them ascending)- Sort small runs with Insertion Sort- Merge runs with Merge Sort Timsort adapts beautifully to partially ordered data.\n\nSteps:\n\nScan array, detect runs (sequences already sorted)\nPush runs to a stack\nMerge runs using a carefully balanced merge strategy\n\nPseudocode (simplified):\ndef timsort(arr):\n    RUN = 32\n    n = len(arr)\n\n    # Step 1: sort small chunks\n    for i in range(0, n, RUN):\n        insertion_sort(arr, i, min((i + RUN - 1), n - 1))\n\n    # Step 2: merge sorted runs\n    size = RUN\n    while size &lt; n:\n        for start in range(0, n, size * 2):\n            mid = start + size - 1\n            end = min(start + size * 2 - 1, n - 1)\n            merge(arr, start, mid, end)\n        size *= 2\nComplexity:\n\nBest: (O(n)) (already sorted data)- Average: (O\\(n \\log n\\))- Worst: (O\\(n \\log n\\))- Space: (O(n))- Stable: Yes Key Strengths:\nExcellent for real-world, partially sorted data- Stable (keeps equal keys in order)- Optimized merges (adaptive merging)\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBase Methods\nStability\nBest\nAverage\nWorst\nReal Use\n\n\n\n\nIntroSort\nQuick + Heap + Insertion\nNo\nO(n log n)\nO(n log n)\nO(n log n)\nC++ STL\n\n\nTimsort\nMerge + Insertion\nYes\nO(n)\nO(n log n)\nO(n log n)\nPython, Java\n\n\n\nIntroSort prioritizes performance guarantees. Timsort prioritizes adaptivity and stability.\nBoth show that “one size fits all” sorting doesn’t exist , great systems detect what’s going on and adapt.\n\n\nTiny Code\nSuppose we run Timsort on [1, 2, 3, 7, 6, 5, 8, 9]:\n\nDetect runs: [1,2,3], [7,6,5], [8,9]- Reverse [7,6,5] → [5,6,7]- Merge runs → [1,2,3,5,6,7,8,9] Efficient because it leverages the existing order.\n\n\n\nWhy It Matters\nHybrid sorts are the real-world heroes , they combine theory with practice. They teach an important principle:\n\nWhen one algorithm’s weakness shows up, switch to another’s strength.\n\nThese are not academic curiosities , they’re in your compiler, your browser, your OS, your database. Understanding them means you understand how modern languages optimize fundamental operations.\n\n\nTry It Yourself\n\nImplement IntroSort and test on random, sorted, and reverse-sorted arrays.\nSimulate Timsort’s run detection on nearly sorted input.\nCompare sorting speed of Insertion Sort vs Timsort for small arrays.\nAdd counters to Quick Sort and see when IntroSort should switch.\nExplore Python’s sorted() with different input shapes , guess when it uses merge vs insertion.\n\nHybrid sorts remind us: good algorithms adapt , they’re not rigid, they’re smart.\n\n\n\n15. Special Sorts (Cycle, Gnome, Comb, Pancake)\nNot all sorting algorithms follow the mainstream divide-and-conquer or distribution paradigms. Some were designed to solve niche problems, to illustrate elegant ideas, or simply to experiment with different mechanisms of ordering.\nThese special sorts, Cycle Sort, Gnome Sort, Comb Sort, and Pancake Sort, are fascinating not because they’re the fastest, but because they reveal creative ways to think about permutation, local order, and in-place operations.\n\n1. Cycle Sort\nIdea: Minimize the number of writes. Cycle sort rearranges elements into cycles, placing each value directly in its correct position. It performs exactly as many writes as there are misplaced elements, making it ideal for flash memory or systems where writes are expensive.\nSteps:\n\nFor each position i, find where arr[i] belongs (its rank).\nIf it’s not already there, swap it into position.\nContinue the cycle until the current position is correct.\nMove to the next index.\n\nCode:\nvoid cycle_sort(int arr[], int n) {\n    for (int cycle_start = 0; cycle_start &lt; n - 1; cycle_start++) {\n        int item = arr[cycle_start];\n        int pos = cycle_start;\n\n        for (int i = cycle_start + 1; i &lt; n; i++)\n            if (arr[i] &lt; item) pos++;\n\n        if (pos == cycle_start) continue;\n\n        while (item == arr[pos]) pos++;\n        int temp = arr[pos];\n        arr[pos] = item;\n        item = temp;\n\n        while (pos != cycle_start) {\n            pos = cycle_start;\n            for (int i = cycle_start + 1; i &lt; n; i++)\n                if (arr[i] &lt; item) pos++;\n            while (item == arr[pos]) pos++;\n            temp = arr[pos];\n            arr[pos] = item;\n            item = temp;\n        }\n    }\n}\nComplexity:\n\nTime: (O\\(n^2\\))- Writes: minimal (exactly n-c, where c = #cycles)- Stable: No Use Case: When minimizing writes is more important than runtime.\n\n\n\n2. Gnome Sort\nIdea: A simpler variation of insertion sort. Gnome sort moves back and forth like a “gnome” tidying flower pots: if two adjacent pots are out of order, swap and step back; otherwise, move forward.\nSteps:\n\nStart at index 1\nIf arr[i] &gt;= arr[i-1], move forward\nElse, swap and step back (if possible)\nRepeat until the end\n\nCode:\nvoid gnome_sort(int arr[], int n) {\n    int i = 1;\n    while (i &lt; n) {\n        if (i == 0 || arr[i] &gt;= arr[i - 1]) i++;\n        else {\n            int temp = arr[i]; arr[i] = arr[i - 1]; arr[i - 1] = temp;\n            i--;\n        }\n    }\n}\nComplexity:\n\nTime: (O\\(n^2\\))- Space: (O(1))- Stable: Yes Use Case: Educational simplicity. It’s a readable form of insertion logic without nested loops.\n\n\n\n3. Comb Sort\nIdea: An improvement over Bubble Sort by introducing a gap between compared elements, shrinking it gradually. By jumping farther apart early, Comb Sort helps eliminate small elements that are “stuck” near the end.\nSteps:\n\nStart with gap = n\nOn each pass, shrink gap = gap / 1.3\nCompare and swap items gap apart\nStop when gap = 1 and no swaps occur\n\nCode:\nvoid comb_sort(int arr[], int n) {\n    int gap = n;\n    int swapped = 1;\n    while (gap &gt; 1 || swapped) {\n        gap = (gap * 10) / 13;\n        if (gap == 9 || gap == 10) gap = 11;\n        if (gap &lt; 1) gap = 1;\n        swapped = 0;\n        for (int i = 0; i + gap &lt; n; i++) {\n            if (arr[i] &gt; arr[i + gap]) {\n                int temp = arr[i]; arr[i] = arr[i + gap]; arr[i + gap] = temp;\n                swapped = 1;\n            }\n        }\n    }\n}\nComplexity:\n\nAverage: (O\\(n \\log n\\))- Worst: (O\\(n^2\\))- Space: (O(1))- Stable: No Use Case: When a simple, in-place, nearly linear-time alternative to bubble sort is desired.\n\n\n\n4. Pancake Sort\nIdea: Sort an array using only one operation: flip (reversing a prefix). It’s like sorting pancakes on a plate, flip the stack so the largest pancake goes to the bottom, then repeat for the rest.\nSteps:\n\nFind the maximum unsorted element\nFlip it to the front\nFlip it again to its correct position\nReduce the unsorted portion by one\n\nCode:\nvoid flip(int arr[], int i) {\n    int start = 0;\n    while (start &lt; i) {\n        int temp = arr[start];\n        arr[start] = arr[i];\n        arr[i] = temp;\n        start++;\n        i--;\n    }\n}\n\nvoid pancake_sort(int arr[], int n) {\n    for (int curr_size = n; curr_size &gt; 1; curr_size--) {\n        int mi = 0;\n        for (int i = 1; i &lt; curr_size; i++)\n            if (arr[i] &gt; arr[mi]) mi = i;\n        if (mi != curr_size - 1) {\n            flip(arr, mi);\n            flip(arr, curr_size - 1);\n        }\n    }\n}\nComplexity:\n\nTime: (O\\(n^2\\))- Space: (O(1))- Stable: No Fun Fact: Pancake sort is the only known algorithm whose operations mimic a kitchen utensil, and inspired the Burnt Pancake Problem in combinatorics and genome rearrangement theory.\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nStable\nDistinctive Trait\n\n\n\n\nCycle Sort\nO(n²)\nO(1)\nNo\nMinimal writes\n\n\nGnome Sort\nO(n²)\nO(1)\nYes\nSimple insertion-like behavior\n\n\nComb Sort\nO(n log n) avg\nO(1)\nNo\nShrinking gap, improved bubble\n\n\nPancake Sort\nO(n²)\nO(1)\nNo\nPrefix reversals only\n\n\n\nEach highlights a different design goal:\n\nCycle: minimize writes- Gnome: simplify logic- Comb: optimize comparisons- Pancake: restrict operations\n\n\n\nTiny Code\nExample (Pancake Sort on [3, 6, 1, 9]):\n\nMax = 9 at index 3 → flip(3) → [9,1,6,3]\nflip(3) → [3,6,1,9] (9 fixed)\nMax = 6 → flip(1) → [6,3,1,9]\nflip(2) → [1,3,6,9]\n\nSorted using only flips.\n\n\nWhy It Matters\nSpecial sorts show there’s more than one way to think about ordering. They’re laboratories for exploring new ideas: minimizing swaps, limiting operations, or optimizing stability. Even if they’re not the go-to in production, they deepen your intuition about sorting mechanics.\n\n\nTry It Yourself\n\nImplement each algorithm and visualize their operations step-by-step.\nMeasure how many writes Cycle Sort performs vs. others.\nCompare Gnome and Insertion sort on nearly sorted arrays.\nModify Comb Sort’s shrink factor, how does performance change?\nWrite Pancake Sort with printouts of every flip to see the “stack” in motion.\n\nThese quirky algorithms prove that sorting isn’t just science, it’s also art and experimentation.\n\n\n\n16. Linear and Binary Search\nSearching is the process of finding a target value within a collection of data. Depending on whether the data is sorted or unsorted, you’ll use different strategies.\nIn this section, we revisit two of the most fundamental searching methods , Linear Search and Binary Search , and see how they underpin many higher-level algorithms and data structures.\n\n1. Linear Search\nIdea: Check each element one by one until you find the target. This is the simplest possible search and works on unsorted data.\nSteps:\n\nStart from index 0\nCompare arr[i] with the target\nIf match, return index\nIf end reached, return -1\n\nCode:\nint linear_search(int arr[], int n, int key) {\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] == key) return i;\n    }\n    return -1;\n}\nExample: arr = [7, 2, 4, 9, 1], key = 9\n\nCompare 7, 2, 4, then 9 → found at index 3 Complexity:\nTime: ( O(n) )- Space: ( O(1) )- Best case: ( O(1) ) (first element)- Worst case: ( O(n) ) Pros:\nWorks on any data (sorted or unsorted)- Simple to implement Cons:\nInefficient on large arrays Use it when data is small or unsorted, or when simplicity matters more than speed.\n\n\n\n2. Binary Search\nIdea: If the array is sorted, you can repeatedly halve the search space. Compare the middle element to the target , if it’s greater, search left; if smaller, search right.\nSteps:\n\nFind the midpoint\nIf arr[mid] == key, done\nIf arr[mid] &gt; key, search left\nIf arr[mid] &lt; key, search right\nRepeat until range is empty\n\nIterative Version:\nint binary_search(int arr[], int n, int key) {\n    int low = 0, high = n - 1;\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] == key) return mid;\n        else if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\nRecursive Version:\nint binary_search_rec(int arr[], int low, int high, int key) {\n    if (low &gt; high) return -1;\n    int mid = (low + high) / 2;\n    if (arr[mid] == key) return mid;\n    else if (arr[mid] &gt; key)\n        return binary_search_rec(arr, low, mid - 1, key);\n    else\n        return binary_search_rec(arr, mid + 1, high, key);\n}\nExample: arr = [1, 3, 5, 7, 9, 11], key = 7\n\nmid = 5 → key &gt; mid → move right- mid = 7 → found Complexity:\nTime: ( O\\(\\log n\\) )- Space: ( O(1) ) (iterative) or ( O\\(\\log n\\) ) (recursive)- Best case: ( O(1) ) (middle element) Requirements:\nMust be sorted- Must have random access (array, not linked list) Pros:\nVery fast for large sorted arrays- Foundation for advanced searches (e.g. interpolation, exponential) Cons:\nNeeds sorted data- Doesn’t adapt to frequent insertions/deletions\n\n\n\n3. Binary Search Variants\nBinary search is a pattern as much as a single algorithm. You can tweak it to find:\n\nFirst occurrence: move left if arr[mid] == key- Last occurrence: move right if arr[mid] == key- Lower bound: first index ≥ key- Upper bound: first index &gt; key Example (Lower Bound):\n\nint lower_bound(int arr[], int n, int key) {\n    int low = 0, high = n;\n    while (low &lt; high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid;\n    }\n    return low;\n}\nUsage: These variants power functions like std::lower_bound() in C++ and binary search trees’ lookup logic.\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorks On\nTime\nSpace\nSorted Data Needed\nNotes\n\n\n\n\nLinear Search\nAny\nO(n)\nO(1)\nNo\nBest for small/unsorted\n\n\nBinary Search\nSorted\nO(log n)\nO(1)\nYes\nFastest on ordered arrays\n\n\n\nBinary search trades simplicity for power , once your data is sorted, you unlock sublinear search.\n\n\nTiny Code\nCompare on array [2, 4, 6, 8, 10], key = 8:\n\nLinear: 4 steps- Binary: 2 steps This gap grows huge with size , for \\(n = 10^6\\), linear takes up to a million steps, binary about 20.\n\n\n\nWhy It Matters\nThese two searches form the foundation of retrieval. Linear search shows brute-force iteration; binary search shows how structure (sorted order) leads to exponential improvement.\nFrom databases to compiler symbol tables to tree lookups, this principle , divide to search faster , is everywhere.\n\n\nTry It Yourself\n\nImplement linear and binary search.\nCount comparisons for ( n = 10, 100, 1000 ).\nModify binary search to return the first occurrence of a duplicate.\nTry binary search on unsorted data , what happens?\nCombine with sorting: sort array, then search.\n\nMastering these searches builds intuition for all lookup operations , they are the gateway to efficient data retrieval.\n\n\n\n17. Interpolation and Exponential Search\nLinear and binary search work well across many scenarios, but they don’t take into account how data is distributed. When values are uniformly distributed, we can estimate where the target lies, instead of always splitting the range in half. This leads to Interpolation Search, which “jumps” close to where the value should be.\nFor unbounded or infinite lists, we can’t even know the size of the array up front , that’s where Exponential Search shines, by quickly expanding its search window before switching to binary search.\nLet’s dive into both.\n\n1. Interpolation Search\nIdea: If data is sorted and uniformly distributed, you can predict where a key might be using linear interpolation. Instead of splitting at the middle, estimate the position based on the value’s proportion in the range.\nFormula: \\[\n\\text{pos} = \\text{low} + \\frac{(key - arr[low]) \\times (high - low)}{arr[high] - arr[low]}\n\\]\nThis “guesses” where the key lies. If (key = arr[pos]), we’re done. Otherwise, adjust low or high and repeat.\nSteps:\n\nCompute estimated position pos\nCompare arr[pos] with key\nNarrow range accordingly\nRepeat while low &lt;= high and key within range\n\nCode:\nint interpolation_search(int arr[], int n, int key) {\n    int low = 0, high = n - 1;\n\n    while (low &lt;= high && key &gt;= arr[low] && key &lt;= arr[high]) {\n        if (low == high) {\n            if (arr[low] == key) return low;\n            return -1;\n        }\n        int pos = low + ((double)(key - arr[low]) * (high - low)) / (arr[high] - arr[low]);\n\n        if (arr[pos] == key)\n            return pos;\n        if (arr[pos] &lt; key)\n            low = pos + 1;\n        else\n            high = pos - 1;\n    }\n    return -1;\n}\nExample: arr = [10, 20, 30, 40, 50], key = 40 pos = 0 + ((40 - 10) * (4 - 0)) / (50 - 10) = 3 → found at index 3\nComplexity:\n\nBest: (O(1))- Average: (O\\(\\log \\log n\\)) (uniform data)- Worst: (O(n)) (non-uniform or skewed data)- Space: (O(1)) When to Use:\nData is sorted and nearly uniform- Numeric data where values grow steadily Note: Interpolation search is adaptive , faster when data is predictable, slower when data is irregular.\n\n\n\n2. Exponential Search\nIdea: When you don’t know the array size (e.g., infinite streams, linked data, files), you can’t just binary search from 0 to n-1. Exponential search finds a search range dynamically by doubling its step size until it overshoots the target, then does binary search within that range.\nSteps:\n\nIf arr[0] == key, return 0\nFind a range [bound/2, bound] such that arr[bound] &gt;= key\nPerform binary search in that range\n\nCode:\nint exponential_search(int arr[], int n, int key) {\n    if (arr[0] == key) return 0;\n    int bound = 1;\n    while (bound &lt; n && arr[bound] &lt; key)\n        bound *= 2;\n    int low = bound / 2;\n    int high = (bound &lt; n) ? bound : n - 1;\n    // Binary search in [low, high]\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (arr[mid] == key) return mid;\n        else if (arr[mid] &lt; key) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\nExample: arr = [2, 4, 6, 8, 10, 12, 14, 16], key = 10\n\nStep: bound = 1 (4), 2 (6), 4 (10 ≥ key)- Binary search [2,4] → found Complexity:\nTime: (O\\(\\log i\\)), where (i) is index of the target- Space: (O(1))- Best: (O(1)) When to Use:\nUnbounded or streamed data- Unknown array size but sorted order\n\n\n\n3. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest Case\nAverage Case\nWorst Case\nData Requirement\nNotes\n\n\n\n\nLinear Search\nO(1)\nO(n)\nO(n)\nUnsorted\nWorks everywhere\n\n\nBinary Search\nO(1)\nO(log n)\nO(log n)\nSorted\nPredictable halving\n\n\nInterpolation Search\nO(1)\nO(log log n)\nO(n)\nSorted + Uniform\nAdaptive, fast on uniform data\n\n\nExponential Search\nO(1)\nO(log n)\nO(log n)\nSorted\nGreat for unknown size\n\n\n\nInterpolation improves on binary if data is smooth. Exponential shines when size is unknown.\n\n\nTiny Code\nInterpolation intuition: If your data is evenly spaced (10, 20, 30, 40, 50), the value 40 should be roughly 75% along. Instead of halving every time, we jump right near it. It’s data-aware searching.\nExponential intuition: When size is unknown, “expand until you find the wall,” then search within.\n\n\nWhy It Matters\nThese two searches show how context shapes algorithm design:\n\nDistribution (Interpolation Search)- Boundaries (Exponential Search) They teach that performance depends not only on structure (sortedness) but also metadata , how much you know about data spacing or limits.\n\nThese principles resurface in skip lists, search trees, and probabilistic indexing.\n\n\nTry It Yourself\n\nTest interpolation search on [10, 20, 30, 40, 50] , note how few steps it takes.\nTry the same on [1, 2, 4, 8, 16, 32, 64] , note slowdown.\nImplement exponential search and simulate an “infinite” array by stopping at n.\nCompare binary vs interpolation search on random vs uniform data.\nExtend exponential search to linked lists , how does complexity change?\n\nUnderstanding these searches helps you tailor lookups to the shape of your data , a key skill in algorithmic thinking.\n\n\n\n18. Selection Algorithms (Quickselect, Median of Medians)\nSometimes you don’t need to sort an entire array , you just want the k-th smallest (or largest) element. Sorting everything is overkill when you only need one specific rank. Selection algorithms solve this problem efficiently, often in linear time.\nThey’re the backbone of algorithms for median finding, percentiles, and order statistics, and they underpin operations like pivot selection in Quick Sort.\n\n1. The Selection Problem\nGiven an unsorted array of ( n ) elements and a number ( k ), find the element that would be at position ( k ) if the array were sorted.\nFor example: arr = [7, 2, 9, 4, 6], (k = 3) → Sorted = [2, 4, 6, 7, 9] → 3rd smallest = 6\nWe can solve this without sorting everything.\n\n\n2. Quickselect\nIdea: Quickselect is a selection variant of Quick Sort. It partitions the array around a pivot, but recurses only on the side that contains the k-th element.\nIt has average-case O(n) time because each partition roughly halves the search space.\nSteps:\n\nChoose a pivot (random or last element)\nPartition array into elements &lt; pivot and &gt; pivot\nLet pos be the pivot’s index after partition\nIf pos == k-1 → done\nIf pos &gt; k-1 → recurse left\nIf pos &lt; k-1 → recurse right\n\nCode:\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;\n            i++;\n        }\n    }\n    int temp = arr[i]; arr[i] = arr[high]; arr[high] = temp;\n    return i;\n}\n\nint quickselect(int arr[], int low, int high, int k) {\n    if (low == high) return arr[low];\n    int pos = partition(arr, low, high);\n    int rank = pos - low + 1;\n    if (rank == k) return arr[pos];\n    if (rank &gt; k) return quickselect(arr, low, pos - 1, k);\n    return quickselect(arr, pos + 1, high, k - rank);\n}\nExample: arr = [7, 2, 9, 4, 6], ( k = 3 )\n\nPivot = 6- Partition → [2, 4, 6, 9, 7], pos = 2- rank = 3 → found (6) Complexity:\nAverage: (O(n))- Worst: (O\\(n^2\\)) (bad pivots)- Space: (O(1))- In-place When to Use:\nFast average case- You don’t need full sorting Quickselect is used in C++’s nth_element() and many median-finding implementations.\n\n\n\n3. Median of Medians\nIdea: Guarantee worst-case ( O(n) ) time by choosing a good pivot deterministically.\nThis method ensures the pivot divides the array into reasonably balanced parts every time.\nSteps:\n\nDivide array into groups of 5\nFind the median of each group (using insertion sort)\nRecursively find the median of these medians → pivot\nPartition array around this pivot\nRecurse into the side containing the k-th element\n\nThis guarantees at least 30% of elements are eliminated each step → linear time in worst case.\nCode Sketch:\nint select_pivot(int arr[], int low, int high) {\n    int n = high - low + 1;\n    if (n &lt;= 5) {\n        insertion_sort(arr + low, n);\n        return low + n / 2;\n    }\n\n    int medians[(n + 4) / 5];\n    int i;\n    for (i = 0; i &lt; n / 5; i++) {\n        insertion_sort(arr + low + i * 5, 5);\n        medians[i] = arr[low + i * 5 + 2];\n    }\n    if (i * 5 &lt; n) {\n        insertion_sort(arr + low + i * 5, n % 5);\n        medians[i] = arr[low + i * 5 + (n % 5) / 2];\n        i++;\n    }\n    return select_pivot(medians, 0, i - 1);\n}\nYou’d then partition around pivot and recurse just like Quickselect.\nComplexity:\n\nWorst: (O(n))- Space: (O(1)) (in-place version)- Stable: No (doesn’t preserve order) Why It Matters: Median of Medians is slower in practice than Quickselect but provides theoretical guarantees , vital in real-time or critical systems.\n\n\n\n4. Special Cases\n\nMin / Max: trivial , just scan once ((O(n)))- Median: \\(k = \\lceil n/2 \\rceil\\) , can use Quickselect or Median of Medians- Top-k Elements: use partial selection or heaps (k smallest/largest) Example: To get top 5 scores from a million entries, use Quickselect to find 5th largest, then filter ≥ threshold.\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nBest\nAverage\nWorst\nStable\nIn-Place\nNotes\n\n\n\n\nQuickselect\nO(n)\nO(n)\nO(n²)\nNo\nYes\nFast in practice\n\n\nMedian of Medians\nO(n)\nO(n)\nO(n)\nNo\nYes\nDeterministic\n\n\nSorting\nO(n log n)\nO(n log n)\nO(n log n)\nDepends\nDepends\nOverkill for single element\n\n\n\nQuickselect is fast and simple; Median of Medians is safe and predictable.\n\n\nTiny Code\nFind 4th smallest in [9, 7, 2, 5, 4, 3]:\n\nPivot = 4 → partition [2,3,4,9,7,5]- 4 at position 2 → rank = 3 &lt; 4 → recurse right- New range [9,7,5], ( k = 1 ) → smallest = 5 Result: 5\n\n\n\nWhy It Matters\nSelection algorithms reveal a key insight:\n\nSometimes you don’t need everything , just what matters.\n\nThey form the basis for:\n\nMedian filters in signal processing- Partitioning steps in sorting- k-th order statistics- Robust statistics and quantile computation They embody a “partial work, full answer” philosophy , do exactly enough.\n\n\n\nTry It Yourself\n\nImplement Quickselect and find k-th smallest for various k.\nCompare runtime vs full sorting.\nModify Quickselect to find k-th largest.\nImplement Median of Medians pivot selection.\nUse Quickselect to find median of 1,000 random elements.\n\nMastering selection algorithms helps you reason about efficiency , you’ll learn when to stop sorting and start selecting.\n\n\n\n19. Range Searching and Nearest Neighbor\nSearching isn’t always about finding a single key. Often, you need to find all elements within a given range , or the closest match to a query point.\nThese problems are central to databases, computational geometry, and machine learning (like k-NN classification). This section introduces algorithms for range queries (e.g. find all values between L and R) and nearest neighbor searches (e.g. find the point closest to query q).\n\n1. Range Searching\nIdea: Given a set of data points (1D or multidimensional), quickly report all points within a specified range.\nIn 1D (simple arrays), range queries can be handled by binary search and prefix sums. In higher dimensions, we need trees designed for efficient spatial querying.\n\n\nA. 1D Range Query (Sorted Array)\nGoal: Find all elements in [L, R].\nSteps:\n\nUse lower bound to find first element ≥ L\nUse upper bound to find first element &gt; R\nOutput all elements in between\n\nCode (C++-style pseudo):\nint l = lower_bound(arr, arr + n, L) - arr;\nint r = upper_bound(arr, arr + n, R) - arr;\nfor (int i = l; i &lt; r; i++)\n    printf(\"%d \", arr[i]);\nTime Complexity:\n\nBinary search bounds: (O\\(\\log n\\))- Reporting results: (O(k)) where (k) = number of elements in range → Total: (O\\(\\log n + k\\))\n\n\n\nB. Prefix Sum Range Query (For sums)\nIf you just need the sum (not the actual elements), use prefix sums:\n\\[\n\\text{prefix}[i] = a_0 + a_1 + \\ldots + a_i\n\\]\nThen range sum: \\[\n\\text{sum}(L, R) = \\text{prefix}[R] - \\text{prefix}[L - 1]\n\\]\nCode:\nint prefix[n];\nprefix[0] = arr[0];\nfor (int i = 1; i &lt; n; i++)\n    prefix[i] = prefix[i - 1] + arr[i];\n\nint range_sum(int L, int R) {\n    return prefix[R] - (L &gt; 0 ? prefix[L - 1] : 0);\n}\nTime: (O(1)) per query after (O(n)) preprocessing.\nUsed in:\n\nDatabases for fast range aggregation- Fenwick trees, segment trees\n\n\n\nC. 2D Range Queries (Rectangular Regions)\nFor points ((x, y)), queries like:\n\n“Find all points where \\(L_x ≤ x ≤ R_x\\) and \\(L_y ≤ y ≤ R_y\\)”\n\nUse specialized structures:\n\nRange Trees (balanced BSTs per dimension)- Fenwick Trees / Segment Trees (for 2D arrays)- KD-Trees (spatial decomposition) Time: (O\\(\\log^2 n + k\\)) typical for 2D Space: (O\\(n \\log n\\))\n\n\n\n2. Nearest Neighbor Search\nIdea: Given a set of points, find the one closest to query (q). Distance is often Euclidean, but can be any metric.\nBrute Force: Check all points → (O(n)) per query. Too slow for large datasets.\nWe need structures that let us prune far regions fast.\n\n\nA. KD-Tree\nKD-tree = K-dimensional binary tree. Each level splits points by one coordinate, alternating axes. Used for efficient nearest neighbor search in low dimensions (2D-10D).\nConstruction:\n\nChoose axis = depth % k\nSort points by axis\nPick median → root\nRecursively build left and right\n\nQuery (Nearest Neighbor):\n\nTraverse down tree based on query position\nBacktrack , check whether hypersphere crosses splitting plane\nKeep track of best (closest) distance\n\nComplexity:\n\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\log n\\)) avg, (O(n)) worst Use Cases:\nNearest city lookup- Image / feature vector matching- Game AI spatial queries Code Sketch (2D Example):\n\nstruct Point { double x, y; };\n\ndouble dist(Point a, Point b) {\n    return sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y));\n}\n(Full KD-tree implementation omitted for brevity , idea is recursive partitioning.)\n\n\nB. Ball Tree / VP-Tree\nFor high-dimensional data, KD-trees degrade. Alternatives like Ball Trees (split by hyperspheres) or VP-Trees (Vantage Point Trees) perform better.\nThey split based on distance metrics, not coordinate axes.\n\n\nC. Approximate Nearest Neighbor (ANN)\nFor large-scale, high-dimensional data (e.g. embeddings, vectors):\n\nLocality Sensitive Hashing (LSH)- HNSW (Hierarchical Navigable Small World Graphs) These trade exactness for speed, common in:\nVector databases- Recommendation systems- AI model retrieval\n\n\n\n3. Summary\n\n\n\n\n\n\n\n\n\n\nProblem\nBrute Force\nOptimized\nTime (Query)\nNotes\n\n\n\n\n1D Range Query\nScan O(n)\nBinary Search\nO(log n + k)\nSorted data\n\n\nRange Sum\nO(n)\nPrefix Sum\nO(1)\nStatic data\n\n\n2D Range Query\nO(n)\nRange Tree\nO(log² n + k)\nSpatial filtering\n\n\nNearest Neighbor\nO(n)\nKD-Tree\nO(log n) avg\nExact, low-dim\n\n\nNearest Neighbor (high-dim)\nO(n)\nHNSW / LSH\n~O(1)\nApproximate\n\n\n\n\n\nTiny Code\nSimple 1D range query:\nint arr[] = {1, 3, 5, 7, 9, 11};\nint L = 4, R = 10;\nint l = lower_bound(arr, arr + 6, L) - arr;\nint r = upper_bound(arr, arr + 6, R) - arr;\nfor (int i = l; i &lt; r; i++)\n    printf(\"%d \", arr[i]); // 5 7 9\nOutput: 5 7 9\n\n\nWhy It Matters\nRange and nearest-neighbor queries power:\n\nDatabases (SQL range filters, BETWEEN)- Search engines (spatial indexing)- ML (k-NN classifiers, vector similarity)- Graphics / Games (collision detection, spatial queries) These are not just searches , they’re geometric lookups, linking algorithms to spatial reasoning.\n\n\n\nTry It Yourself\n\nWrite a function to return all numbers in [L, R] using binary search.\nBuild a prefix sum array and answer 5 range-sum queries in O(1).\nImplement a KD-tree for 2D points and query nearest neighbor.\nCompare brute-force vs KD-tree search on 1,000 random points.\nExplore Python’s scipy.spatial.KDTree or sklearn.neighbors.\n\nThese algorithms bridge searching with geometry and analytics, forming the backbone of spatial computation.\n\n\n\n20. Search Optimizations and Variants\nWe’ve explored the main search families , linear, binary, interpolation, exponential , each fitting a different data shape or constraint. Now let’s move one step further: optimizing search for performance and adapting it to specialized scenarios.\nThis section introduces practical variants and enhancements used in real systems, databases, and competitive programming, including jump search, fibonacci search, ternary search, and exponential + binary combinations.\n\n1. Jump Search\nIdea: If data is sorted, we can “jump” ahead by fixed steps instead of scanning linearly. It’s like hopping through the array in blocks , when you overshoot the target, you step back and linearly search that block.\nIt strikes a balance between linear and binary search , fewer comparisons without the recursion or halving of binary search.\nSteps:\n\nChoose jump size = \\(\\sqrt{n}\\)\nJump by blocks until arr[step] &gt; key\nLinear search in previous block\n\nCode:\nint jump_search(int arr[], int n, int key) {\n    int step = sqrt(n);\n    int prev = 0;\n\n    while (arr[min(step, n) - 1] &lt; key) {\n        prev = step;\n        step += sqrt(n);\n        if (prev &gt;= n) return -1;\n    }\n\n    for (int i = prev; i &lt; min(step, n); i++) {\n        if (arr[i] == key) return i;\n    }\n    return -1;\n}\nExample: arr = [1, 3, 5, 7, 9, 11, 13, 15], key = 11\n\nstep = 2- Jump 5, 7, 9, 11 → found Complexity:\nTime: (O\\(\\sqrt{n}\\))- Space: (O(1))- Works on sorted data When to Use: For moderately sized sorted lists when you want fewer comparisons but minimal overhead.\n\n\n\n2. Fibonacci Search\nIdea: Similar to binary search, but it splits the array based on Fibonacci numbers instead of midpoints. This allows using only addition and subtraction (no division), useful on hardware where division is costly.\nAlso, like binary search, it halves (roughly) the search space each iteration.\nSteps:\n\nFind the smallest Fibonacci number ≥ n\nUse it to compute probe index\nCompare and move interval accordingly\n\nCode (Sketch):\nint fibonacci_search(int arr[], int n, int key) {\n    int fibMMm2 = 0; // (m-2)'th Fibonacci\n    int fibMMm1 = 1; // (m-1)'th Fibonacci\n    int fibM = fibMMm2 + fibMMm1; // m'th Fibonacci\n\n    while (fibM &lt; n) {\n        fibMMm2 = fibMMm1;\n        fibMMm1 = fibM;\n        fibM = fibMMm2 + fibMMm1;\n    }\n\n    int offset = -1;\n    while (fibM &gt; 1) {\n        int i = min(offset + fibMMm2, n - 1);\n        if (arr[i] &lt; key) {\n            fibM = fibMMm1;\n            fibMMm1 = fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n            offset = i;\n        } else if (arr[i] &gt; key) {\n            fibM = fibMMm2;\n            fibMMm1 = fibMMm1 - fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n        } else return i;\n    }\n    if (fibMMm1 && arr[offset + 1] == key)\n        return offset + 1;\n    return -1;\n}\nComplexity:\n\nTime: (O\\(\\log n\\))- Space: (O(1))- Sorted input required Fun Fact: Fibonacci search was originally designed for tape drives , where random access is expensive, and predictable jumps matter.\n\n\n\n3. Ternary Search\nIdea: When the function or sequence is unimodal (strictly increasing then decreasing), you can locate a maximum or minimum by splitting the range into three parts instead of two.\nUsed not for discrete lookup but for optimization on sorted functions.\nSteps:\n\nDivide range into thirds\nEvaluate at two midpoints m1, m2\nEliminate one-third based on comparison\nRepeat until range is small\n\nCode:\ndouble ternary_search(double low, double high, double (*f)(double)) {\n    for (int i = 0; i &lt; 100; i++) {\n        double m1 = low + (high - low) / 3;\n        double m2 = high - (high - low) / 3;\n        if (f(m1) &lt; f(m2))\n            low = m1;\n        else\n            high = m2;\n    }\n    return (low + high) / 2;\n}\nExample: Find minimum of ( f(x) = (x-3)^2 ) between [0,10]. After iterations, converges to (x ≈ 3).\nComplexity:\n\nTime: \\(O(\\log\\text{range})\\)\nSpace: \\(O(1)\\)\nWorks for unimodal functions\n\nUsed in:\n\nMathematical optimization\nSearch-based tuning\nGame AI decision models\n\n\n\n4. Binary Search Variants (Review)\nBinary search can be tailored to answer richer queries:\n\nLower Bound: first index ≥ key- Upper Bound: first index &gt; key- Equal Range: range of all equal elements- Rotated Arrays: find element in rotated sorted array- Infinite Arrays: use exponential expansion Rotated Example: arr = [6,7,9,1,3,4], key = 3 → Find pivot, then binary search correct side.\n\n\n\n5. Combined Searches\nReal systems often chain algorithms:\n\nExponential + Binary Search → when bounds unknown- Interpolation + Linear Search → when near target- Jump + Linear Search → hybrid iteration These hybrids use context switching , pick a fast search, then fall back to simple scan in a narrowed window.\n\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nData Requirement\nSpecial Strength\n\n\n\n\nJump Search\nO(√n)\nO(1)\nSorted\nFewer comparisons\n\n\nFibonacci Search\nO(log n)\nO(1)\nSorted\nDivision-free\n\n\nTernary Search\nO(log range)\nO(1)\nUnimodal\nOptimization\n\n\nBinary Variants\nO(log n)\nO(1)\nSorted\nBound finding\n\n\nCombined Searches\nAdaptive\nO(1)\nMixed\nPractical hybrids\n\n\n\n\n\nTiny Code\nJump Search intuition:\n// Blocks of size sqrt(n)\n[1, 3, 5, 7, 9, 11, 13, 15]\nStep: 3 → 7 &gt; 6 → search previous block\nJumps reduce comparisons dramatically vs linear scan.\n\n\nWhy It Matters\nSearch optimization is about adapting structure to context. You don’t always need a fancy data structure , sometimes a tweak like fixed-step jumping or Fibonacci spacing yields massive gains.\nThese ideas influence:\n\nIndexing in databases- Compilers’ symbol resolution- Embedded systems with low-level constraints They embody the principle: search smarter, not harder.\n\n\n\nTry It Yourself\n\nImplement Jump Search and test vs Binary Search on 1M elements.\nWrite a Fibonacci Search , compare steps taken.\nUse Ternary Search to find min of a convex function.\nModify binary search to find element in rotated array.\nCombine Jump + Linear , how does it behave for small n?\n\nUnderstanding these variants arms you with flexibility , the heart of algorithmic mastery.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-3.-data-structures-in-actions",
    "href": "books/en-us/book.html#chapter-3.-data-structures-in-actions",
    "title": "The Book",
    "section": "Chapter 3. Data Structures in Actions",
    "text": "Chapter 3. Data Structures in Actions\n\n21. Arrays, Linked Lists, Stacks, Queues\nEvery data structure is built on top of a few core foundations , the ones that teach you how data is stored, accessed, and moved. In this section, we’ll revisit the essentials: arrays, linked lists, stacks, and queues.\nThey’re simple, but they show you the most important design trade-offs in algorithms:\n\nContiguity vs. flexibility- Speed vs. dynamic growth- Last-in-first-out vs. first-in-first-out access\n\n\n1. Arrays\nIdea: A contiguous block of memory storing elements of the same type. Access by index in O(1) time , that’s their superpower.\nOperations:\n\nAccess arr[i]: (O(1))- Update arr[i]: (O(1))- Insert at end: (O(1)) (amortized for dynamic arrays)- Insert in middle: (O(n))- Delete: (O(n)) Example:\n\nint arr[5] = {10, 20, 30, 40, 50};\nprintf(\"%d\", arr[2]); // 30\nStrengths:\n\nFast random access- Cache-friendly (contiguous memory)- Simple, predictable Weaknesses:\nFixed size (unless using dynamic array)- Costly inserts/deletes Dynamic Arrays: Languages provide resizable arrays (like vector in C++ or ArrayList in Java) using doubling strategy , when full, allocate new array twice as big and copy. This gives amortized (O(1)) insertion at end.\n\n\n\n2. Linked Lists\nIdea: A chain of nodes, where each node stores a value and a pointer to the next. No contiguous memory required.\nOperations:\n\nAccess: (O(n))- Insert/Delete at head: (O(1))- Search: (O(n)) Example:\n\ntypedef struct Node {\n    int data;\n    struct Node* next;\n} Node;\n\nNode* head = NULL;\nTypes:\n\nSingly Linked List: one pointer (next)- Doubly Linked List: two pointers (next, prev)- Circular Linked List: last node points back to first Strengths:\nDynamic size- Fast insert/delete (no shifting) Weaknesses:\nSlow access- Extra memory for pointers- Poor cache locality Linked lists shine when memory is fragmented or frequent insertions/deletions are needed.\n\n\n\n3. Stack\nIdea: A Last-In-First-Out (LIFO) structure , the most recently added element is the first to be removed.\nUsed in:\n\nFunction call stacks- Expression evaluation- Undo operations Operations:\npush(x): add element on top- pop(): remove top element- peek(): view top element Example (Array-based Stack):\n\n#define MAX 100\nint stack[MAX], top = -1;\n\nvoid push(int x) { stack[++top] = x; }\nint pop() { return stack[top--]; }\nint peek() { return stack[top]; }\nComplexity: All (O(1)): push, pop, peek\nVariants:\n\nLinked-list-based stack (no fixed size)- Min-stack (tracks minimums) Stacks also appear implicitly , in recursion and backtracking algorithms.\n\n\n\n4. Queue\nIdea: A First-In-First-Out (FIFO) structure , the first added element leaves first.\nUsed in:\n\nTask scheduling- BFS traversal- Producer-consumer pipelines Operations:\nenqueue(x): add to rear- dequeue(): remove from front- front(): view front Example (Array-based Queue):\n\n#define MAX 100\nint queue[MAX], front = 0, rear = 0;\n\nvoid enqueue(int x) { queue[rear++] = x; }\nint dequeue() { return queue[front++]; }\nThis simple implementation can waste space. A circular queue fixes that by wrapping indices modulo MAX:\nrear = (rear + 1) % MAX;\nComplexity: All (O(1)): enqueue, dequeue\nVariants:\n\nDeque (double-ended queue): push/pop from both ends- Priority Queue: dequeue highest priority (not strictly FIFO)\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nStructure\nAccess\nInsert\nDelete\nOrder\nMemory\nNotes\n\n\n\n\nArray\nO(1)\nO(n)\nO(n)\nIndexed\nContiguous\nFast access\n\n\nLinked List\nO(n)\nO(1)*\nO(1)*\nSequential\nPointers\nFlexible size\n\n\nStack\nO(1)\nO(1)\nO(1)\nLIFO\nMinimal\nCall stack, parsing\n\n\nQueue\nO(1)\nO(1)\nO(1)\nFIFO\nMinimal\nScheduling, BFS\n\n\n\n(* at head or tail with pointer)\n\n\nTiny Code\nSimple stack example:\npush(10);\npush(20);\nprintf(\"%d\", pop()); // 20\nSimple queue example:\nenqueue(5);\nenqueue(8);\nprintf(\"%d\", dequeue()); // 5\nThese short routines appear in almost every algorithm , from recursion stacks to graph traversals.\n\n\nWhy It Matters\nThese four structures form the spine of data structures:\n\nArrays teach indexing and memory- Linked lists teach pointers and dynamic allocation- Stacks teach recursion and reversal- Queues teach scheduling and order maintenance Every complex structure (trees, heaps, graphs) builds on these.\n\nMaster them, and every algorithm will feel more natural.\n\n\nTry It Yourself\n\nImplement a linked list with insert_front and delete_value.\nBuild a stack and use it to reverse an array.\nImplement a queue for a round-robin scheduler.\nConvert infix expression to postfix using a stack.\nCompare time taken to insert 1000 elements in array vs linked list.\n\nUnderstanding these foundations gives you the vocabulary of structure , the way algorithms organize their thoughts in memory.\n\n\n\n22. Hash Tables and Variants (Cuckoo, Robin Hood, Consistent)\nWhen you need lightning-fast lookups, insertions, and deletions, few data structures match the raw efficiency of a hash table. They’re everywhere , from symbol tables and caches to compilers and databases , powering average-case O(1) access.\nIn this section, we’ll unpack how hash tables work, their collision strategies, and explore modern variants like Cuckoo Hashing, Robin Hood Hashing, and Consistent Hashing, each designed to handle different real-world needs.\n\n1. The Core Idea\nA hash table maps keys to values using a hash function that transforms the key into an index in an array.\n\\[\n\\text{index} = h(\\text{key}) \\bmod \\text{table\\_size}\n\\]\nIf no two keys hash to the same index, all operations are (O(1)). But in practice, collisions happen , two keys may map to the same slot , and we must handle them smartly.\n\n\n2. Collision Resolution Strategies\nA. Separate Chaining Each table slot holds a linked list (or dynamic array) of entries with the same hash.\nPros: Simple, handles load factor &gt; 1 Cons: Extra pointers, memory overhead\nCode Sketch:\ntypedef struct Node {\n    int key, value;\n    struct Node* next;\n} Node;\n\nNode* table[SIZE];\n\nint hash(int key) { return key % SIZE; }\n\nvoid insert(int key, int value) {\n    int idx = hash(key);\n    Node* node = malloc(sizeof(Node));\n    node-&gt;key = key; node-&gt;value = value;\n    node-&gt;next = table[idx];\n    table[idx] = node;\n}\nB. Open Addressing All keys live directly in the table. On collision, find another slot.\nThree main strategies:\n\nLinear probing: try next slot (+1)- Quadratic probing: step size increases quadratically- Double hashing: second hash decides step size Example (Linear Probing):\n\nint hash(int key) { return key % SIZE; }\nint insert(int key, int value) {\n    int idx = hash(key);\n    while (table[idx].used)\n        idx = (idx + 1) % SIZE;\n    table[idx] = (Entry){key, value, 1};\n}\nLoad Factor \\(\\alpha = \\frac{n}{m}\\) affects performance , when too high, rehash to larger size.\n\n\n3. Modern Variants\nClassic hash tables can degrade under heavy collisions. Modern variants reduce probe chains and balance load more evenly.\n\n\nA. Cuckoo Hashing\nIdea: Each key has two possible locations , if both full, evict one (“kick out the cuckoo”) and reinsert. Ensures constant lookup , at most two probes.\nSteps:\n\nCompute two hashes (h_1(key)), (h_2(key))\nIf slot 1 empty → place\nElse evict occupant, reinsert it using alternate hash\nRepeat until placed or cycle detected (rehash if needed)\n\nCode Sketch (Conceptual):\nint h1(int key) { return key % SIZE; }\nint h2(int key) { return (key / SIZE) % SIZE; }\n\nvoid insert(int key) {\n    int pos1 = h1(key);\n    if (!table1[pos1]) { table1[pos1] = key; return; }\n    int displaced = table1[pos1]; table1[pos1] = key;\n\n    int pos2 = h2(displaced);\n    if (!table2[pos2]) { table2[pos2] = displaced; return; }\n    // continue evicting if needed\n}\nPros:\n\nWorst-case O(1) lookup (constant probes)- Predictable latency Cons:\nRehash needed on insertion failure- More complex logic Used in high-performance caches and real-time systems.\n\n\n\nB. Robin Hood Hashing\nIdea: Steal slots from richer (closer) keys to ensure fairness. When inserting, if you find someone with smaller probe distance, swap , “steal from the rich, give to the poor.”\nThis balances probe lengths and improves variance and average lookup time.\nKey Principle: \\[\n\\text{If new\\_probe\\_distance} &gt; \\text{existing\\_probe\\_distance} \\Rightarrow \\text{swap}\n\\]\nCode Sketch:\nint insert(int key) {\n    int idx = hash(key);\n    int dist = 0;\n    while (table[idx].used) {\n        if (table[idx].dist &lt; dist) {\n            // swap entries\n            Entry tmp = table[idx];\n            table[idx] = (Entry){key, dist, 1};\n            key = tmp.key;\n            dist = tmp.dist;\n        }\n        idx = (idx + 1) % SIZE;\n        dist++;\n    }\n    table[idx] = (Entry){key, dist, 1};\n}\nPros:\n\nReduced variance- Better performance under high load Cons:\nSlightly slower insertion Used in modern languages like Rust (hashbrown) and Swift.\n\n\n\nC. Consistent Hashing\nIdea: When distributing keys across multiple nodes, you want minimal movement when adding/removing a node. Consistent hashing maps both keys and nodes onto a circular hash ring.\nSteps:\n\nHash nodes into a ring\nHash keys into same ring\nEach key belongs to the next node clockwise\n\nWhen a node is added or removed, only nearby keys move.\nUsed in:\n\nDistributed caches (Memcached, DynamoDB)- Load balancing- Sharding in databases Code (Conceptual):\n\nRing: 0 -------------------------------- 2^32\nNodes: N1 at hash(\"A\"), N2 at hash(\"B\")\nKey: hash(\"User42\") → assign to next node clockwise\nPros:\n\nMinimal rebalancing- Scalable Cons:\nMore complex setup- Requires virtual nodes for even distribution\n\n\n\n4. Complexity Overview\n\n\n\n\n\n\n\n\n\n\n\nVariant\nInsert\nSearch\nDelete\nMemory\nNotes\n\n\n\n\nChaining\nO(1) avg\nO(1) avg\nO(1) avg\nHigh\nSimple, dynamic\n\n\nLinear Probing\nO(1) avg\nO(1) avg\nO(1) avg\nLow\nClustering risk\n\n\nCuckoo\nO(1)\nO(1)\nO(1)\nMedium\nTwo tables, predictable\n\n\nRobin Hood\nO(1)\nO(1)\nO(1)\nLow\nBalanced probes\n\n\nConsistent\nO(log n)\nO(log n)\nO(log n)\nDepends\nDistributed keys\n\n\n\n\n\nTiny Code\nSimple hash table with linear probing:\n#define SIZE 10\nint keys[SIZE], values[SIZE], used[SIZE];\n\nint hash(int key) { return key % SIZE; }\n\nvoid insert(int key, int value) {\n    int idx = hash(key);\n    while (used[idx]) idx = (idx + 1) % SIZE;\n    keys[idx] = key; values[idx] = value; used[idx] = 1;\n}\nLookup:\nint get(int key) {\n    int idx = hash(key);\n    while (used[idx]) {\n        if (keys[idx] == key) return values[idx];\n        idx = (idx + 1) % SIZE;\n    }\n    return -1;\n}\n\n\nWhy It Matters\nHash tables show how structure and randomness combine for speed. They embody the idea that a good hash function + smart collision handling = near-constant performance.\nVariants like Cuckoo and Robin Hood are examples of modern engineering trade-offs , balancing performance, memory, and predictability. Consistent hashing extends these ideas to distributed systems.\n\n\nTry It Yourself\n\nImplement a hash table with chaining and test collision handling.\nModify it to use linear probing , measure probe lengths.\nSimulate Cuckoo hashing with random inserts.\nImplement Robin Hood swapping logic , observe fairness.\nDraw a consistent hash ring with 3 nodes and 10 keys , track movement when adding one node.\n\nOnce you master these, you’ll see hashing everywhere , from dictionaries to distributed databases.\n\n\n\n23. Heaps (Binary, Fibonacci, Pairing)\nHeaps are priority-driven data structures , they always give you fast access to the most important element, typically the minimum or maximum. They’re essential for priority queues, scheduling, graph algorithms (like Dijkstra), and streaming analytics.\nIn this section, we’ll start from the basic binary heap and then explore more advanced ones like Fibonacci and pairing heaps, which trade off simplicity, speed, and amortized guarantees.\n\n1. The Heap Property\nA heap is a tree-based structure (often represented as an array) that satisfies:\n\nMin-Heap: Every node ≤ its children- Max-Heap: Every node ≥ its children This ensures the root always holds the smallest (or largest) element.\n\nComplete Binary Tree: All levels filled except possibly the last, which is filled left to right.\nExample (Min-Heap):\n        2\n      /   \\\n     4     5\n    / \\   /\n   9  10 15\nHere, the smallest element (2) is at the root.\n\n\n2. Binary Heap\nStorage: Stored compactly in an array. For index i (0-based):\n\nParent = (i - 1) / 2- Left child = 2i + 1- Right child = 2i + 2 Operations:\n\n\n\n\nOperation\nDescription\nTime\n\n\n\n\npush(x)\nInsert element\n(O\\(\\log n\\))\n\n\npop()\nRemove root\n(O\\(\\log n\\))\n\n\npeek()\nGet root\n(O(1))\n\n\nheapify()\nBuild heap\n(O(n))\n\n\n\n\n\nA. Insertion (Push)\nInsert at the end, then bubble up until heap property is restored.\nCode:\nvoid push(int heap[], int *n, int x) {\n    int i = (*n)++;\n    heap[i] = x;\n    while (i &gt; 0 && heap[(i - 1)/2] &gt; heap[i]) {\n        int tmp = heap[i];\n        heap[i] = heap[(i - 1)/2];\n        heap[(i - 1)/2] = tmp;\n        i = (i - 1) / 2;\n    }\n}\n\n\nB. Removal (Pop)\nReplace root with last element, then bubble down (heapify).\nCode:\nvoid heapify(int heap[], int n, int i) {\n    int smallest = i, l = 2*i + 1, r = 2*i + 2;\n    if (l &lt; n && heap[l] &lt; heap[smallest]) smallest = l;\n    if (r &lt; n && heap[r] &lt; heap[smallest]) smallest = r;\n    if (smallest != i) {\n        int tmp = heap[i]; heap[i] = heap[smallest]; heap[smallest] = tmp;\n        heapify(heap, n, smallest);\n    }\n}\nPop:\nint pop(int heap[], int *n) {\n    int root = heap[0];\n    heap[0] = heap[--(*n)];\n    heapify(heap, *n, 0);\n    return root;\n}\n\n\nC. Building a Heap\nHeapify bottom-up from last non-leaf: (O(n))\nfor (int i = n/2 - 1; i &gt;= 0; i--)\n    heapify(heap, n, i);\n\n\nD. Applications\n\nHeapsort: Repeatedly pop min (O(n log n))- Priority Queue: Fast access to smallest/largest- Graph Algorithms: Dijkstra, Prim- Streaming: Median finding using two heaps\n\n\n\n3. Fibonacci Heap\nIdea: A heap optimized for algorithms that do many decrease-key operations (like Dijkstra’s). It stores a collection of trees with lazy merging, giving amortized bounds:\n\n\n\nOperation\nAmortized Time\n\n\n\n\nInsert\n(O(1))\n\n\nFind-Min\n(O(1))\n\n\nExtract-Min\n(O\\(\\log n\\))\n\n\nDecrease-Key\n(O(1))\n\n\nMerge\n(O(1))\n\n\n\nIt achieves this by delaying structural fixes until absolutely necessary (using potential method in amortized analysis).\nStructure:\n\nA circular linked list of roots- Each node can have multiple children- Consolidation on extract-min ensures minimal degree duplication Used in theoretical optimizations where asymptotic complexity matters (e.g. Dijkstra in (O\\(E + V \\log V\\)) vs (O\\(E \\log V\\))).\n\n\n\n4. Pairing Heap\nIdea: A simpler, practical alternative to Fibonacci heaps. Self-adjusting structure using a tree with multiple children.\nOperations:\n\nInsert: (O(1))- Extract-Min: (O\\(\\log n\\)) amortized- Decrease-Key: (O\\(\\log n\\)) amortized Steps:\nmerge two heaps: attach one as child of the other- extract-min: remove root, merge children in pairs, then merge all results Why It’s Popular:\nEasier to implement- Great real-world performance- Used in functional programming and priority schedulers\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nHeap Type\nInsert\nExtract-Min\nDecrease-Key\nMerge\nSimplicity\nUse Case\n\n\n\n\nBinary Heap\nO(log n)\nO(log n)\nO(log n)\nO(n)\nEasy\nGeneral-purpose\n\n\nFibonacci Heap\nO(1)\nO(log n)\nO(1)\nO(1)\nComplex\nTheoretical optimality\n\n\nPairing Heap\nO(1)\nO(log n)\nO(log n)\nO(1)\nModerate\nPractical alternative\n\n\n\n\n\nTiny Code\nBinary Heap Demo:\nint heap[100], n = 0;\npush(heap, &n, 10);\npush(heap, &n, 4);\npush(heap, &n, 7);\nprintf(\"%d \", pop(heap, &n)); // 4\nOutput: 4\n\n\nWhy It Matters\nHeaps show how to prioritize elements dynamically. From sorting to scheduling, they’re the backbone of many “choose the best next” algorithms. Variants like Fibonacci and Pairing Heaps demonstrate how amortized analysis can unlock deeper efficiency , crucial in graph theory and large-scale optimization.\n\n\nTry It Yourself\n\nImplement a binary min-heap with push, pop, and peek.\nUse a heap to sort a list (Heapsort).\nBuild a priority queue for task scheduling.\nStudy how Dijkstra changes when replacing arrays with heaps.\nExplore Fibonacci heap pseudo-code , trace decrease-key.\n\nMastering heaps gives you a deep sense of priority-driven design , how to keep “the best” element always within reach.\n\n\n\n24. Balanced Trees (AVL, Red-Black, Splay, Treap)\nUnbalanced trees can degrade into linear lists, turning your beautiful (O\\(\\log n\\)) search into a sad (O(n)) crawl. Balanced trees solve this , they keep the height logarithmic, guaranteeing fast lookups, insertions, and deletions.\nIn this section, you’ll learn how different balancing philosophies work , AVL (strict balance), Red-Black (relaxed balance), Splay (self-adjusting), and Treap (randomized balance).\n\n1. The Idea of Balance\nFor a binary search tree (BST):\n\\[\n\\text{height} = O(\\log n)\n\\]\nonly if it’s balanced , meaning the number of nodes in left and right subtrees differ by a small factor.\nUnbalanced BST (bad):\n1\n \\\n  2\n   \\\n    3\nBalanced BST (good):\n  2\n / \\\n1   3\nBalance ensures efficient:\n\nsearch(x) → (O\\(\\log n\\))- insert(x) → (O\\(\\log n\\))- delete(x) → (O\\(\\log n\\))\n\n\n\n2. AVL Tree (Adelson-Velsky & Landis)\nInvented in 1962, AVL is the first self-balancing BST. It enforces strict balance: \\[\n| \\text{height(left)} - \\text{height(right)} | \\le 1\n\\]\nWhenever this condition breaks, rotations fix it.\nRotations:\n\nLL (Right Rotation): imbalance on left-left- RR (Left Rotation): imbalance on right-right- LR / RL: double rotation cases Code (Rotation Example):\n\nNode* rotateRight(Node* y) {\n    Node* x = y-&gt;left;\n    Node* T = x-&gt;right;\n    x-&gt;right = y;\n    y-&gt;left = T;\n    return x;\n}\nHeight & Balance Factor:\nint height(Node* n) { return n ? n-&gt;h : 0; }\nint balance(Node* n) { return height(n-&gt;left) - height(n-&gt;right); }\nProperties:\n\nStrict height bound: (O\\(\\log n\\))- More rotations (slower insertions)- Excellent lookup speed Used when lookups &gt; updates (databases, indexing).\n\n\n\n3. Red-Black Tree\nIdea: A slightly looser balance for faster insertions. Each node has a color (Red/Black) with these rules:\n\nRoot is black\nRed node’s children are black\nEvery path has same number of black nodes\nNull nodes are black\n\nBalance through color flips + rotations\nCompared to AVL:\n\nFewer rotations (faster insert/delete)- Slightly taller (slower lookup)- Simpler amortized balance Used in:\nC++ std::map, std::set- Java TreeMap, Linux scheduler Complexity: All major operations (O\\(\\log n\\))\n\n\n\n4. Splay Tree\nIdea: Bring recently accessed node to root via splaying (rotations). It adapts to access patterns , the more you access a key, the faster it becomes.\nSplaying Steps:\n\nZig: one rotation (root child)- Zig-Zig: two rotations (same side)- Zig-Zag: two rotations (different sides) Code (Conceptual):\n\nNode* splay(Node* root, int key) {\n    if (!root || root-&gt;key == key) return root;\n    if (key &lt; root-&gt;key) {\n        if (!root-&gt;left) return root;\n        // splay in left subtree\n        if (key &lt; root-&gt;left-&gt;key)\n            root-&gt;left-&gt;left = splay(root-&gt;left-&gt;left, key),\n            root = rotateRight(root);\n        else if (key &gt; root-&gt;left-&gt;key)\n            root-&gt;left-&gt;right = splay(root-&gt;left-&gt;right, key),\n            root-&gt;left = rotateLeft(root-&gt;left);\n        return rotateRight(root);\n    } else {\n        if (!root-&gt;right) return root;\n        // symmetric\n    }\n}\nWhy It’s Cool: No strict balance, but amortized (O\\(\\log n\\)). Frequently accessed elements stay near top.\nUsed in self-adjusting caches, rope data structures, memory allocators.\n\n\n5. Treap (Tree + Heap)\nIdea: Each node has two keys:\n\nBST key → order property- Priority → heap property Insertion = normal BST insert + heap fix via rotation.\n\nBalance comes from randomization , random priorities ensure expected (O\\(\\log n\\)) height.\nCode Sketch:\ntypedef struct Node {\n    int key, priority;\n    struct Node *left, *right;\n} Node;\n\nNode* insert(Node* root, int key) {\n    if (!root) return newNode(key, rand());\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else root-&gt;right = insert(root-&gt;right, key);\n\n    if (root-&gt;left && root-&gt;left-&gt;priority &gt; root-&gt;priority)\n        root = rotateRight(root);\n    if (root-&gt;right && root-&gt;right-&gt;priority &gt; root-&gt;priority)\n        root = rotateLeft(root);\n    return root;\n}\nAdvantages:\n\nSimple logic- Random balancing- Expected (O\\(\\log n\\)) Used in randomized algorithms and functional programming.\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nTree\nBalance Type\nRotations\nHeight\nInsert/Delete\nLookup\nNotes\n\n\n\n\nAVL\nStrict\nMore\n(O\\(\\log n\\))\nMedium\nFast\nLookup-heavy\n\n\nRed-Black\nRelaxed\nFewer\n(O\\(\\log n\\))\nFast\nMedium\nLibrary std\n\n\nSplay\nAdaptive\nVariable\nAmortized (O\\(\\log n\\))\nFast\nFast (amortized)\nAccess patterns\n\n\nTreap\nRandomized\nAvg few\n(O\\(\\log n\\)) expected\nSimple\nSimple\nProbabilistic\n\n\n\n\n\nTiny Code\nAVL Insert (Skeleton):\nNode* insert(Node* root, int key) {\n    if (!root) return newNode(key);\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else root-&gt;right = insert(root-&gt;right, key);\n    root-&gt;h = 1 + max(height(root-&gt;left), height(root-&gt;right));\n    int b = balance(root);\n    if (b &gt; 1 && key &lt; root-&gt;left-&gt;key) return rotateRight(root);\n    if (b &lt; -1 && key &gt; root-&gt;right-&gt;key) return rotateLeft(root);\n    // other cases...\n    return root;\n}\n\n\nWhy It Matters\nBalanced trees guarantee predictable performance under dynamic updates. Each variant represents a philosophy:\n\nAVL: precision- Red-Black: practicality- Splay: adaptability- Treap: randomness Together, they teach one core idea , keep height in check, no matter the operations.\n\n\n\nTry It Yourself\n\nImplement an AVL tree and visualize rotations.\nInsert keys [10, 20, 30, 40, 50] and trace Red-Black color changes.\nSplay after each access , see which keys stay near top.\nBuild a Treap with random priorities , measure average height.\nCompare performance of BST vs AVL on sorted input.\n\nBalanced trees are the architects of order , always keeping chaos one rotation away.\n\n\n\n25. Segment Trees and Fenwick Trees\nWhen you need to answer range queries quickly (like sum, min, max) and support updates to individual elements, simple prefix sums won’t cut it anymore.\nYou need something smarter , data structures that can divide and conquer over ranges, updating and combining results efficiently.\nThat’s exactly what Segment Trees and Fenwick Trees (Binary Indexed Trees) do:\n\nQuery over a range in (O\\(\\log n\\))- Update elements in (O\\(\\log n\\)) They’re the backbone of competitive programming, signal processing, and database analytics.\n\n\n1. The Problem\nGiven an array A[0..n-1], support:\n\nupdate(i, x) → change A[i] to x\nquery(L, R) → compute sum (or min, max) of A[L..R]\n\nNaive approach:\n\nUpdate: (O(1))- Query: (O(n)) Prefix sums fix one but not both. Segment and Fenwick trees fix both.\n\n\n\n2. Segment Tree\nIdea: Divide the array into segments (intervals) recursively. Each node stores an aggregate (sum, min, max) of its range. You can combine child nodes to get any range result.\nStructure (Sum Example):\n           [0,7] sum=36\n         /           \\\n   [0,3]=10         [4,7]=26\n   /     \\           /      \\\n[0,1]=3 [2,3]=7  [4,5]=11  [6,7]=15\nEach node represents a range [L,R]. Leaf nodes = single elements.\n\n\nA. Build\nRecursive Construction: Time: (O(n))\nvoid build(int node, int L, int R) {\n    if (L == R) tree[node] = arr[L];\n    else {\n        int mid = (L + R) / 2;\n        build(2*node, L, mid);\n        build(2*node+1, mid+1, R);\n        tree[node] = tree[2*node] + tree[2*node+1];\n    }\n}\n\n\nB. Query (Range Sum)\nQuery [l, r] recursively:\n\nIf current range [L, R] fully inside [l, r], return node value- If disjoint, return 0- Else combine children\n\nint query(int node, int L, int R, int l, int r) {\n    if (r &lt; L || R &lt; l) return 0;\n    if (l &lt;= L && R &lt;= r) return tree[node];\n    int mid = (L + R) / 2;\n    return query(2*node, L, mid, l, r)\n         + query(2*node+1, mid+1, R, l, r);\n}\n\n\nC. Update\nChange arr[i] = x and update tree nodes covering i.\nvoid update(int node, int L, int R, int i, int x) {\n    if (L == R) tree[node] = x;\n    else {\n        int mid = (L + R)/2;\n        if (i &lt;= mid) update(2*node, L, mid, i, x);\n        else update(2*node+1, mid+1, R, i, x);\n        tree[node] = tree[2*node] + tree[2*node+1];\n    }\n}\nComplexities:\n\nBuild: (O(n))- Query: (O\\(\\log n\\))- Update: (O\\(\\log n\\))- Space: (O(4n))\n\n\n\nD. Variants\nSegment trees are flexible:\n\nRange minimum/maximum- Range GCD- Lazy propagation → range updates- 2D segment tree for grids\n\n\n\n3. Fenwick Tree (Binary Indexed Tree)\nIdea: Stores cumulative frequencies using bit manipulation. Each node covers a range size = LSB(index).\nSimpler, smaller, but supports only associative ops (sum, xor, etc.)\nIndexing:\n\nParent: i + (i & -i)- Child: i - (i & -i) Build: Initialize with zero, then add elements one by one.\n\nAdd / Update:\nvoid add(int i, int x) {\n    for (; i &lt;= n; i += i & -i)\n        bit[i] += x;\n}\nPrefix Sum:\nint sum(int i) {\n    int res = 0;\n    for (; i &gt; 0; i -= i & -i)\n        res += bit[i];\n    return res;\n}\nRange Sum [L, R]: \\[\n\\text{sum}(R) - \\text{sum}(L-1)\n\\]\nComplexities:\n\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\log n\\))- Update: (O\\(\\log n\\))- Space: (O(n))\n\n\n\n4. Comparison\n\n\n\nFeature\nSegment Tree\nFenwick Tree\n\n\n\n\nSpace\nO(4n)\nO(n)\n\n\nBuild\nO(n)\nO(n log n)\n\n\nQuery\nO(log n)\nO(log n)\n\n\nUpdate\nO(log n)\nO(log n)\n\n\nRange Update\nWith Lazy\nTricky\n\n\nRange Query\nFlexible\nSum/XOR only\n\n\nImplementation\nModerate\nSimple\n\n\n\n\n\n5. Applications\n\nSum / Min / Max / XOR queries- Frequency counts- Inversions counting- Order statistics- Online problems where array updates over time Used in:\nCompetitive programming- Databases (analytics on changing data)- Time series queries- Games (damage/range updates)\n\n\n\nTiny Code\nFenwick Tree Example:\nint bit[1001], n;\n\nvoid update(int i, int val) {\n    for (; i &lt;= n; i += i & -i)\n        bit[i] += val;\n}\n\nint query(int i) {\n    int res = 0;\n    for (; i &gt; 0; i -= i & -i)\n        res += bit[i];\n    return res;\n}\n\n// range sum\nint range_sum(int L, int R) { return query(R) - query(L - 1); }\n\n\nWhy It Matters\nSegment and Fenwick trees embody divide-and-conquer over data , balancing dynamic updates with range queries. They’re how modern systems aggregate live data efficiently.\nThey teach a powerful mindset:\n\n“If you can split a problem, you can solve it fast.”\n\n\n\nTry It Yourself\n\nBuild a segment tree for sum queries.\nAdd range minimum queries (RMQ).\nImplement a Fenwick tree , test with prefix sums.\nSolve: number of inversions in array using Fenwick tree.\nAdd lazy propagation to segment tree for range updates.\n\nOnce you master these, range queries will never scare you again , you’ll slice through them in logarithmic time.\n\n\n\n26. Disjoint Set Union (Union-Find)\nMany problems involve grouping elements into sets and efficiently checking whether two elements belong to the same group , like connected components in a graph, network connectivity, Kruskal’s MST, or even social network clustering.\nFor these, the go-to structure is the Disjoint Set Union (DSU), also called Union-Find. It efficiently supports two operations:\n\nfind(x) → which set does x belong to?\nunion(x, y) → merge the sets containing x and y.\n\nWith path compression and union by rank, both operations run in near-constant time, specifically (O((n))), where \\(\\alpha\\) is the inverse Ackermann function (practically ≤ 4).\n\n1. The Problem\nSuppose you have (n) elements initially in separate sets. Over time, you want to:\n\nMerge two sets- Check if two elements share the same set Example:\n\nSets: {1}, {2}, {3}, {4}, {5}\nUnion(1,2) → {1,2}, {3}, {4}, {5}\nUnion(3,4) → {1,2}, {3,4}, {5}\nFind(2) == Find(1)? Yes\nFind(5) == Find(3)? No\n\n\n2. Basic Implementation\nEach element has a parent pointer. Initially, every node is its own parent.\nParent array representation:\nint parent[N];\n\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find(int v) {\n    if (v == parent[v]) return v;\n    return find(parent[v]);\n}\n\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b)\n        parent[b] = a;\n}\nThis works, but deep trees can form , making find slow. We fix that with path compression.\n\n\n3. Path Compression\nEvery time we call find(v), we make all nodes along the path point directly to the root. This flattens the tree dramatically.\nOptimized Find:\nint find(int v) {\n    if (v == parent[v]) return v;\n    return parent[v] = find(parent[v]);\n}\nSo next time, lookups will be (O(1)) for those nodes.\n\n\n4. Union by Rank / Size\nWhen merging, always attach the smaller tree to the larger to keep depth small.\nUnion by Rank:\nint parent[N], rank[N];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b])\n            swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\nUnion by Size (Alternative): Track size of each set and attach smaller to larger.\nint size[N];\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b) {\n        if (size[a] &lt; size[b]) swap(a, b);\n        parent[b] = a;\n        size[a] += size[b];\n    }\n}\n\n\n5. Complexity\nWith both path compression and union by rank, all operations are effectively constant time: \\[\nO(\\alpha(n)) \\approx O(1)\n\\]\nFor all practical (n), ((n) ).\n\n\n\nOperation\nTime\n\n\n\n\nMake set\nO(1)\n\n\nFind\nO(α(n))\n\n\nUnion\nO(α(n))\n\n\n\n\n\n6. Applications\n\nGraph Connectivity: determine connected components- Kruskal’s MST: add edges, avoid cycles- Dynamic connectivity- Image segmentation- Network clustering- Cycle detection in undirected graphs Example: Kruskal’s Algorithm\n\nsort(edges.begin(), edges.end());\nfor (edge e : edges)\n    if (find(e.u) != find(e.v)) {\n        union_sets(e.u, e.v);\n        mst_weight += e.w;\n    }\n\n\n7. Example\nint parent[6], rank[6];\n\nvoid init() {\n    for (int i = 1; i &lt;= 5; i++) {\n        parent[i] = i;\n        rank[i] = 0;\n    }\n}\n\nint main() {\n    init();\n    union_sets(1, 2);\n    union_sets(3, 4);\n    union_sets(2, 3);\n    printf(\"%d\\n\", find(4)); // prints representative of {1,2,3,4}\n}\nResult: {1,2,3,4}, {5}\n\n\n8. Visualization\nBefore compression:\n1\n \\\n  2\n   \\\n    3\n\nAfter compression:\n1\n├─2\n└─3\nEvery find call makes future queries faster.\n\n\n9. Comparison\n\n\n\nVariant\nFind\nUnion\nNotes\n\n\n\n\nBasic\nO(n)\nO(n)\nDeep trees\n\n\nPath Compression\nO(α(n))\nO(α(n))\nVery fast\n\n\n+ Rank / Size\nO(α(n))\nO(α(n))\nBalanced\n\n\nPersistent DSU\nO(log n)\nO(log n)\nUndo/rollback support\n\n\n\n\n\nTiny Code\nFull DSU with path compression + rank:\nint parent[1000], rank[1000];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 0;\n}\n\nint find(int v) {\n    if (v != parent[v])\n        parent[v] = find(parent[v]);\n    return parent[v];\n}\n\nvoid union_sets(int a, int b) {\n    a = find(a);\n    b = find(b);\n    if (a != b) {\n        if (rank[a] &lt; rank[b]) swap(a, b);\n        parent[b] = a;\n        if (rank[a] == rank[b])\n            rank[a]++;\n    }\n}\n\n\nWhy It Matters\nUnion-Find embodies structural sharing and lazy optimization , you don’t balance eagerly, but just enough. It’s one of the most elegant demonstrations of how constant-time algorithms are possible through clever organization.\nIt teaches a key algorithmic lesson:\n\n“Work only when necessary, and fix structure as you go.”\n\n\n\nTry It Yourself\n\nImplement DSU and test find/union.\nBuild a program that counts connected components.\nSolve Kruskal’s MST using DSU.\nAdd get_size(v) to return component size.\nTry rollback DSU (keep stack of changes).\n\nUnion-Find is the quiet powerhouse behind many graph and connectivity algorithms , simple, fast, and deeply elegant.\n\n\n\n27. Probabilistic Data Structures (Bloom, Count-Min, HyperLogLog)\nWhen you work with massive data streams , billions of elements, too big for memory , you can’t store everything. But what if you don’t need perfect answers, just fast and tiny approximate ones?\nThat’s where probabilistic data structures shine. They trade a bit of accuracy for huge space savings and constant-time operations.\nIn this section, we’ll explore three of the most famous:\n\nBloom Filters → membership queries- Count-Min Sketch → frequency estimation- HyperLogLog → cardinality estimation Each of them answers “How likely is X?” or “How many?” efficiently , perfect for modern analytics, caching, and streaming systems.\n\n\n1. Bloom Filter , “Is this element probably in the set?”\nA Bloom filter answers:\n\n“Is x in the set?” with either maybe yes or definitely no.\n\nNo false negatives, but some false positives.\n\n\nA. Idea\nUse an array of bits (size m), all initialized to 0. Use k different hash functions.\nTo insert an element:\n\nCompute k hashes: ( h_1(x), h_2(x), , h_k(x) )\nSet each bit position \\(b_i = 1\\)\n\nTo query an element:\n\nCompute same k hashes\nIf all bits are 1 → maybe yes\nIf any bit is 0 → definitely no\n\n\n\nB. Example\nInsert dog:\n\n(h_1(dog)=2, h_2(dog)=5, h_3(dog)=9) Set bits 2, 5, 9 → 1\n\nCheck cat:\n\nIf any hash bit = 0 → not present\n\n\n\nC. Complexity\n\n\n\nOperation\nTime\nSpace\nAccuracy\n\n\n\n\nInsert\nO(k)\nO(m)\nTunable\n\n\nQuery\nO(k)\nO(m)\nFalse positives\n\n\n\nFalse positive rate ≈ ( \\(1 - e^{-kn/m}\\)^k )\nChoose m and k based on expected n and acceptable error.\n\n\nD. Code\n#define M 1000\nint bitset[M];\n\nint hash1(int x) { return (x * 17) % M; }\nint hash2(int x) { return (x * 31 + 7) % M; }\n\nvoid add(int x) {\n    bitset[hash1(x)] = 1;\n    bitset[hash2(x)] = 1;\n}\n\nbool contains(int x) {\n    return bitset[hash1(x)] && bitset[hash2(x)];\n}\nUsed in:\n\nCaches (check before disk lookup)- Spam filters- Databases (join filtering)- Blockchain and peer-to-peer networks\n\n\n\n2. Count-Min Sketch , “How often has this appeared?”\nTracks frequency counts in a stream, using sub-linear memory.\nInstead of a full table, it uses a 2D array of counters, each row hashed with a different hash function.\n\n\nA. Insert\nFor each row i:\n\nCompute hash (h_i(x))- Increment count[i][h_i(x)]++ #### B. Query\n\nFor element x:\n\nCompute all (h_i(x))- Take min(count[i][h_i(x)]) across rows → gives an upper-bounded estimate of true frequency\n\n\n\nC. Code\n#define W 1000\n#define D 5\nint count[D][W];\n\nint hash(int i, int x) {\n    return (x * (17*i + 3)) % W;\n}\n\nvoid add(int x) {\n    for (int i = 0; i &lt; D; i++)\n        count[i][hash(i, x)]++;\n}\n\nint query(int x) {\n    int res = INT_MAX;\n    for (int i = 0; i &lt; D; i++)\n        res = min(res, count[i][hash(i, x)]);\n    return res;\n}\n\n\nD. Complexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(D)\nO(W×D)\n\n\nQuery\nO(D)\nO(W×D)\n\n\n\nError controlled by: \\[\n\\varepsilon = \\frac{1}{W}, \\quad \\delta = 1 - e^{-D}\n\\]\nUsed in:\n\nFrequency counting in streams- Hot-key detection- Network flow analysis- Trending topics\n\n\n\n3. HyperLogLog , “How many unique items?”\nEstimates cardinality (number of distinct elements) with very small memory (~1.5 KB for millions).\n\n\nA. Idea\nHash each element uniformly → 32-bit value. Split hash into:\n\nPrefix bits → bucket index- Suffix bits → count leading zeros Each bucket stores the max leading zero count seen. At the end, use harmonic mean of counts to estimate distinct values.\n\n\n\nB. Formula\n\\[\nE = \\alpha_m \\cdot m^2 \\cdot \\Big(\\sum_{i=1}^m 2^{-M[i]}\\Big)^{-1}\n\\]\nwhere M[i] is the zero count in bucket i, and \\(\\alpha_m\\) is a correction constant.\nAccuracy: ~1.04 / √m\n\n\nC. Complexity\n\n\n\nOperation\nTime\nSpace\nError\n\n\n\n\nAdd\nO(1)\nO(m)\n~1.04/√m\n\n\nMerge\nO(m)\nO(m)\n,\n\n\n\nUsed in:\n\nWeb analytics (unique visitors)- Databases (COUNT DISTINCT)- Distributed systems (mergeable estimates)\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nStructure\nPurpose\nQuery\nMemory\nError\nNotes\n\n\n\n\nBloom\nMembership\nO(k)\nTiny\nFalse positives\nNo deletions\n\n\nCount-Min\nFrequency\nO(D)\nSmall\nOverestimate\nStreaming counts\n\n\nHyperLogLog\nCardinality\nO(1)\nVery small\n~1%\nMergeable\n\n\n\n\n\nTiny Code\nBloom Filter Demo:\nadd(42);\nadd(17);\nprintf(\"%d\\n\", contains(42)); // 1 (maybe yes)\nprintf(\"%d\\n\", contains(99)); // 0 (definitely no)\n\n\nWhy It Matters\nProbabilistic data structures show how approximation beats impossibility when resources are tight. They make it feasible to process massive streams in real time, when storing everything is impossible.\nThey teach a deeper algorithmic truth:\n\n“A bit of uncertainty can buy you a world of scalability.”\n\n\n\nTry It Yourself\n\nImplement a Bloom filter with 3 hash functions.\nMeasure false positive rate for 10K elements.\nBuild a Count-Min Sketch and test frequency estimation.\nApproximate unique elements using HyperLogLog logic.\nExplore real-world systems: Redis (Bloom/CM Sketch), PostgreSQL (HyperLogLog).\n\nThese tiny probabilistic tools are how big data becomes tractable.\n\n\n\n28. Skip Lists and B-Trees\nWhen you want fast search, insert, and delete but need a structure that’s easier to code than trees or optimized for disk and memory blocks, two clever ideas step in:\n\nSkip Lists → randomized, layered linked lists that behave like balanced BSTs- B-Trees → multi-way trees that minimize disk I/O and organize large data blocks Both guarantee (O\\(\\log n\\)) operations, but they shine in very different environments , Skip Lists in-memory, B-Trees on disk.\n\n\n1. Skip Lists\nInvented by: William Pugh (1990) Goal: Simulate binary search using linked lists with probabilistic shortcuts.\n\n\nA. Idea\nA skip list is a stack of linked lists, each level skipping over more elements.\nExample:\nLevel 3:        ┌───────&gt; 50 ───────┐\nLevel 2:   ┌──&gt; 10 ─────&gt; 30 ─────&gt; 50 ───┐\nLevel 1:  5 ──&gt; 10 ──&gt; 20 ──&gt; 30 ──&gt; 40 ──&gt; 50\nHigher levels are sparser and let you “skip” large chunks of the list.\nYou search top-down:\n\nMove right while next ≤ target- Drop down when you can’t go further This mimics binary search , logarithmic layers, logarithmic hops.\n\n\n\nB. Construction\nEach inserted element is given a random height, with geometric distribution:\n\nLevel 1 (base) always exists- Level 2 with probability ½- Level 3 with ¼, etc. Expected total nodes = 2n, Expected height = (O\\(\\log n\\))\n\n\n\nC. Operations\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nSearch\n(O\\(\\log n\\))\nO(n)\nRandomized balance\n\n\nInsert\n(O\\(\\log n\\))\nO(n)\nRebuild towers\n\n\nDelete\n(O\\(\\log n\\))\nO(n)\nRewire pointers\n\n\n\nSearch Algorithm:\nNode* search(SkipList* sl, int key) {\n    Node* cur = sl-&gt;head;\n    for (int lvl = sl-&gt;level; lvl &gt;= 0; lvl--) {\n        while (cur-&gt;forward[lvl] && cur-&gt;forward[lvl]-&gt;key &lt; key)\n            cur = cur-&gt;forward[lvl];\n    }\n    cur = cur-&gt;forward[0];\n    if (cur && cur-&gt;key == key) return cur;\n    return NULL;\n}\nSkip Lists are simple, fast, and probabilistically balanced , no rotations, no rebalancing.\n\n\nD. Why Use Skip Lists?\n\nEasier to implement than balanced trees- Support concurrent access well- Randomized, not deterministic , but highly reliable Used in:\nRedis (sorted sets)- LevelDB / RocksDB internals- Concurrent maps\n\n\n\n2. B-Trees\nInvented by: Rudolf Bayer & Ed McCreight (1972) Goal: Reduce disk access by grouping data in blocks.\nA B-Tree is a generalization of a BST:\n\nEach node holds multiple keys and children- Keys are kept sorted- Child subtrees span ranges between keys\n\n\n\nA. Structure\nA B-Tree of order m:\n\nEach node has ≤ m children- Each internal node has k-1 keys if it has k children- All leaves at the same depth Example (order 3):\n\n        [17 | 35]\n       /    |     \\\n [5 10] [20 25 30] [40 45 50]\n\n\nB. Operations\n\nSearch\n\nTraverse from root - Binary search in each node’s key array - Follow appropriate child → (O\\(\\log_m n\\))\n\nInsert\n\nInsert in leaf - If overflow → split node - Promote median key to parent\n\nDelete\n\nBorrow or merge if node underflows Each split or merge keeps height minimal.\n\n\n\n\nC. Complexity\n\n\n\nOperation\nTime\nDisk Accesses\nNotes\n\n\n\n\nSearch\n(O\\(\\log_m n\\))\n(O\\(\\log_m n\\))\nm = branching factor\n\n\nInsert\n(O\\(\\log_m n\\))\n(O(1)) splits\nBalanced\n\n\nDelete\n(O\\(\\log_m n\\))\n(O(1)) merges\nBalanced\n\n\n\nHeight ≈ \\(\\log_m n\\) → very shallow when (m) large (e.g. 100).\n\n\nD. B+ Tree Variant\nIn B+ Trees:\n\nAll data in leaves (internal nodes = indexes)- Leaves linked → efficient range queries Used in:\nDatabases (MySQL, PostgreSQL)- File systems (NTFS, HFS+)- Key-value stores\n\n\n\nE. Example Flow\nInsert 25:\n[10 | 20 | 30] → overflow\nSplit → [10] [30]\nPromote 20\nRoot: [20]\n\n\n3. Comparison\n\n\n\nFeature\nSkip List\nB-Tree\n\n\n\n\nBalancing\nRandomized\nDeterministic\n\n\nFanout\n2 (linked)\nm-way\n\n\nEnvironment\nIn-memory\nDisk-based\n\n\nSearch\nO(log n)\nO\\(log_m n\\)\n\n\nInsert/Delete\nO(log n)\nO\\(log_m n\\)\n\n\nConcurrency\nEasy\nComplex\n\n\nRange Queries\nSequential scan\nLinked leaves (B+)\n\n\n\n\n\nTiny Code\nSkip List Search (Conceptual):\nNode* search(SkipList* list, int key) {\n    Node* cur = list-&gt;head;\n    for (int lvl = list-&gt;level; lvl &gt;= 0; lvl--) {\n        while (cur-&gt;next[lvl] && cur-&gt;next[lvl]-&gt;key &lt; key)\n            cur = cur-&gt;next[lvl];\n    }\n    cur = cur-&gt;next[0];\n    return (cur && cur-&gt;key == key) ? cur : NULL;\n}\nB-Tree Node (Skeleton):\n#define M 4\ntypedef struct {\n    int keys[M-1];\n    Node* child[M];\n    int n;\n} Node;\n\n\nWhy It Matters\nSkip Lists and B-Trees show two paths to balance:\n\nRandomized simplicity (Skip List)- Block-based order (B-Tree) Both offer logarithmic guarantees, but one optimizes pointer chasing, the other I/O.\n\nThey’re fundamental to:\n\nIn-memory caches (Skip List)- On-disk indexes (B-Tree, B+ Tree)- Sorted data structures across systems\n\n\n\nTry It Yourself\n\nBuild a basic skip list and insert random keys.\nTrace a search path across levels.\nImplement B-Tree insert and split logic.\nCompare height of BST vs B-Tree for 1,000 keys.\nExplore how Redis and MySQL use these internally.\n\nTogether, they form the bridge between linked lists and balanced trees, uniting speed, structure, and scalability.\n\n\n\n29. Persistent and Functional Data Structures\nMost data structures are ephemeral , when you update them, the old version disappears. But sometimes, you want to keep all past versions, so you can go back in time, undo operations, or run concurrent reads safely.\nThat’s the magic of persistent data structures: every update creates a new version while sharing most of the old structure.\nThis section introduces the idea of persistence, explores how to make classic structures like arrays and trees persistent, and explains why functional programming loves them.\n\n1. What Is Persistence?\nA persistent data structure preserves previous versions after updates. You can access any version , past or present , without side effects.\nThree levels:\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nPartial\nCan access past versions, but only modify the latest\nUndo stack\n\n\nFull\nCan access and modify any version\nImmutable map\n\n\nConfluent\nCan combine different versions\nGit-like merges\n\n\n\nThis is essential in functional programming, undo systems, version control, persistent segment trees, and immutable databases.\n\n\n2. Ephemeral vs Persistent\nEphemeral:\narr[2] = 7; // old value lost forever\nPersistent:\nnew_arr = update(arr, 2, 7); // old_arr still exists\nPersistent structures use structural sharing , unchanged parts are reused, not copied.\n\n\n3. Persistent Linked List\nEasiest example: each update creates a new head, reusing the tail.\nstruct Node { int val; Node* next; };\n\nNode* push(Node* head, int x) {\n    Node* newHead = malloc(sizeof(Node));\n    newHead-&gt;val = x;\n    newHead-&gt;next = head;\n    return newHead;\n}\nNow both old_head and new_head coexist. Each version is immutable , you never change existing nodes.\nAccess: old and new lists share most of their structure:\nv0: 1 → 2 → 3\nv1: 0 → 1 → 2 → 3\nOnly one new node was created.\n\n\n4. Persistent Binary Tree\nFor trees, updates create new paths from the root to the modified node, reusing the rest.\ntypedef struct Node {\n    int key;\n    struct Node *left, *right;\n} Node;\n\nNode* update(Node* root, int pos, int val) {\n    if (!root) return newNode(val);\n    Node* node = malloc(sizeof(Node));\n    *node = *root; // copy\n    if (pos &lt; root-&gt;key) node-&gt;left = update(root-&gt;left, pos, val);\n    else node-&gt;right = update(root-&gt;right, pos, val);\n    return node;\n}\nEach update creates a new version , only (O\\(\\log n\\)) new nodes per change.\nThis is the core of persistent segment trees used in competitive programming.\n\n\n5. Persistent Array (Functional Trick)\nArrays are trickier because of random access. Solutions:\n\nUse balanced binary trees as array replacements- Each update replaces one node- Persistent vector = tree of small arrays (used in Clojure, Scala) This gives:\nAccess: (O\\(\\log n\\))- Update: (O\\(\\log n\\))- Space: (O\\(\\log n\\)) per update\n\n\n\n6. Persistent Segment Tree\nUsed for versioned range queries:\n\nEach update = new root- Each version = snapshot of history Example: Track how array changes over time, query “sum in range [L,R] at version t”.\n\nBuild:\nNode* build(int L, int R) {\n    if (L == R) return newNode(arr[L]);\n    int mid = (L+R)/2;\n    return newNode(\n        build(L, mid),\n        build(mid+1, R),\n        sum\n    );\n}\nUpdate: only (O\\(\\log n\\)) new nodes\nNode* update(Node* prev, int L, int R, int pos, int val) {\n    if (L == R) return newNode(val);\n    int mid = (L+R)/2;\n    if (pos &lt;= mid)\n        return newNode(update(prev-&gt;left, L, mid, pos, val), prev-&gt;right);\n    else\n        return newNode(prev-&gt;left, update(prev-&gt;right, mid+1, R, pos, val));\n}\nEach version = new root; old ones still valid.\n\n\n7. Functional Perspective\nIn functional programming, data is immutable by default. Instead of mutating, you create a new version.\nThis allows:\n\nThread-safety (no races)- Time-travel debugging- Undo/redo systems- Concurrency without locks Languages like Haskell, Clojure, and Elm build everything this way.\n\nFor example, Clojure’s persistent vector uses path copying and branching factor 32 for (O\\(\\log_{32} n\\)) access.\n\n\n8. Applications\n\nUndo / Redo stacks (text editors, IDEs)- Version control (Git trees)- Immutable databases (Datomic)- Segment trees over time (competitive programming)- Snapshots in memory allocators or games\n\n\n\n9. Complexity\n\n\n\n\n\n\n\n\n\n\nStructure\nUpdate\nAccess\nSpace per Update\nNotes\n\n\n\n\nPersistent Linked List\nO(1)\nO(1)\nO(1)\nSimple sharing\n\n\nPersistent Tree\nO(log n)\nO(log n)\nO(log n)\nPath copying\n\n\nPersistent Array\nO(log n)\nO(log n)\nO(log n)\nTree-backed\n\n\nPersistent Segment Tree\nO(log n)\nO(log n)\nO(log n)\nVersioned queries\n\n\n\n\n\nTiny Code\nPersistent Linked List Example:\nNode* v0 = NULL;\nv0 = push(v0, 3);\nv0 = push(v0, 2);\nNode* v1 = push(v0, 1);\n// v0 = [2,3], v1 = [1,2,3]\n\n\nWhy It Matters\nPersistence is about time as a first-class citizen. It lets you:\n\nRoll back- Compare versions- Work immutably and safely It’s the algorithmic foundation behind functional programming, time-travel debugging, and immutable data systems.\n\nIt teaches this powerful idea:\n\n“Never destroy , always build upon what was.”\n\n\n\nTry It Yourself\n\nImplement a persistent stack using linked lists.\nWrite a persistent segment tree for range sums.\nTrack array versions after each update and query old states.\nCompare space/time with an ephemeral one.\nExplore persistent structures in Clojure (conj, assoc) or Rust (im crate).\n\nPersistence transforms data from fleeting state into a history you can navigate , a timeline of structure and meaning.\n\n\n\n30. Advanced Trees and Range Queries\nSo far, you’ve seen balanced trees (AVL, Red-Black, Treap) and segment-based structures (Segment Trees, Fenwick Trees). Now it’s time to combine those ideas and step into advanced trees , data structures that handle dynamic sets, order statistics, intervals, ranges, and geometry-like queries in logarithmic time.\nThis chapter is about trees that go beyond search , they store order, track ranges, and answer complex queries efficiently.\nWe’ll explore:\n\nOrder Statistic Trees (k-th element, rank queries)- Interval Trees (range overlaps)- Range Trees (multi-dimensional search)- KD-Trees (spatial partitioning)- Merge Sort Trees (offline range queries)\n\n\n1. Order Statistic Tree\nGoal: find the k-th smallest element, or the rank of an element, in (O\\(\\log n\\)).\nBuilt on top of a balanced BST (e.g. Red-Black) by storing subtree sizes.\n\n\nA. Augmented Tree Nodes\nEach node keeps:\n\nkey: element value- left, right: children- size: number of nodes in subtree\n\ntypedef struct Node {\n    int key, size;\n    struct Node *left, *right;\n} Node;\nWhenever you rotate or insert, update size:\nint get_size(Node* n) { return n ? n-&gt;size : 0; }\nvoid update_size(Node* n) {\n    if (n) n-&gt;size = get_size(n-&gt;left) + get_size(n-&gt;right) + 1;\n}\n\n\nB. Find k-th Element\nRecursively use subtree sizes:\nNode* kth(Node* root, int k) {\n    int left = get_size(root-&gt;left);\n    if (k == left + 1) return root;\n    else if (k &lt;= left) return kth(root-&gt;left, k);\n    else return kth(root-&gt;right, k - left - 1);\n}\nTime: (O\\(\\log n\\))\n\n\nC. Find Rank\nFind position of a key (number of smaller elements):\nint rank(Node* root, int key) {\n    if (!root) return 0;\n    if (key &lt; root-&gt;key) return rank(root-&gt;left, key);\n    if (key &gt; root-&gt;key) return get_size(root-&gt;left) + 1 + rank(root-&gt;right, key);\n    return get_size(root-&gt;left) + 1;\n}\nUsed in:\n\nDatabases (ORDER BY, pagination)- Quantile queries- Online median maintenance\n\n\n\n2. Interval Tree\nGoal: find all intervals overlapping with a given point or range.\nUsed in computational geometry, scheduling, and genomic data.\n\n\nA. Structure\nBST ordered by interval low endpoint. Each node stores:\n\nlow, high: interval bounds- max: maximum high in its subtree\n\ntypedef struct {\n    int low, high, max;\n    struct Node *left, *right;\n} Node;\n\n\nB. Query Overlap\nCheck if x overlaps node-&gt;interval: If not, go left or right based on max values.\nbool overlap(Interval a, Interval b) {\n    return a.low &lt;= b.high && b.low &lt;= a.high;\n}\n\nNode* overlap_search(Node* root, Interval q) {\n    if (!root) return NULL;\n    if (overlap(root-&gt;interval, q)) return root;\n    if (root-&gt;left && root-&gt;left-&gt;max &gt;= q.low)\n        return overlap_search(root-&gt;left, q);\n    return overlap_search(root-&gt;right, q);\n}\nTime: (O\\(\\log n\\)) average\n\n\nC. Use Cases\n\nCalendar/schedule conflict detection- Collision detection- Genome region lookup- Segment intersection\n\n\n\n3. Range Tree\nGoal: multi-dimensional queries like\n\n“How many points fall inside rectangle [x1, x2] × [y1, y2]?”\n\nStructure:\n\nPrimary BST on x- Each node stores secondary BST on y Query time: (O\\(\\log^2 n\\)) Space: (O\\(n \\log n\\))\n\nUsed in:\n\n2D search- Computational geometry- Databases (spatial joins)\n\n\n\n4. KD-Tree\nGoal: efficiently search points in k-dimensional space.\nAlternate splitting dimensions at each level:\n\nLevel 0 → split by x- Level 1 → split by y- Level 2 → split by z Each node stores:\nPoint (vector)- Split axis Used for:\nNearest neighbor search- Range queries- ML (k-NN classifiers) Time:\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\sqrt{n}\\)) average in 2D\n\n\n\n5. Merge Sort Tree\nGoal: query “number of elements ≤ k in range [L, R]”\nBuilt like a segment tree, but each node stores a sorted list of its range.\nBuild: merge children lists Query: binary search in node lists\nTime:\n\nBuild: (O\\(n \\log n\\))- Query: (O\\(\\log^2 n\\)) Used in offline queries and order-statistics over ranges.\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nTree Type\nUse Case\nQuery\nUpdate\nNotes\n\n\n\n\nOrder Statistic\nk-th, rank\nO(log n)\nO(log n)\nAugmented BST\n\n\nInterval\nOverlaps\nO(log n + k)\nO(log n)\nStore intervals\n\n\nRange Tree\n2D range\nO(log² n + k)\nO(log² n)\nMulti-dim\n\n\nKD-Tree\nSpatial\nO(√n) avg\nO(log n)\nNearest neighbor\n\n\nMerge Sort Tree\nOffline rank\nO(log² n)\nStatic\nBuilt from sorted segments\n\n\n\n\n\nTiny Code\nOrder Statistic Example:\nNode* root = NULL;\nroot = insert(root, 10);\nroot = insert(root, 20);\nroot = insert(root, 30);\nprintf(\"%d\", kth(root, 2)-&gt;key); // 20\nInterval Query:\nInterval q = {15, 17};\nNode* res = overlap_search(root, q);\nif (res) printf(\"Overlap: [%d, %d]\\n\", res-&gt;low, res-&gt;high);\n\n\nWhy It Matters\nThese trees extend balance into dimensions and ranges. They let you query ordered data efficiently: “How many?”, “Which overlaps?”, “Where is k-th smallest?”.\nThey teach a deeper design principle:\n\n“Augment structure with knowledge , balance plus metadata equals power.”\n\n\n\nTry It Yourself\n\nImplement an order statistic tree , test rank/k-th queries.\nInsert intervals and test overlap detection.\nBuild a simple KD-tree for 2D points.\nSolve rectangle counting with a range tree.\nPrecompute a merge sort tree for offline queries.\n\nThese advanced trees form the final evolution of structured queries , blending geometry, order, and logarithmic precision.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-4.-graph-algorithms",
    "href": "books/en-us/book.html#chapter-4.-graph-algorithms",
    "title": "The Book",
    "section": "Chapter 4. Graph Algorithms",
    "text": "Chapter 4. Graph Algorithms\n\n31. Traversals (DFS, BFS, Iterative Deepening)\nGraphs are everywhere , maps, networks, dependencies, state spaces. Before you can analyze them, you need a way to visit their vertices , systematically, without getting lost or looping forever.\nThat’s where graph traversals come in. They’re the foundation for everything that follows: connected components, shortest paths, spanning trees, topological sorts, and more.\nThis section walks through the three pillars:\n\nDFS (Depth-First Search) , explore deeply before backtracking- BFS (Breadth-First Search) , explore level by level- Iterative Deepening , a memory-friendly hybrid\n\n\n1. Representing Graphs\nBefore traversal, you need a good structure.\nAdjacency List (most common):\n#define MAX 1000\nvector&lt;int&gt; adj[MAX];\nAdd edges:\nvoid add_edge(int u, int v) {\n    adj[u].push_back(v);\n    adj[v].push_back(u); // omit if directed\n}\nTrack visited vertices:\nbool visited[MAX];\n\n\n2. Depth-First Search (DFS)\nDFS dives deep, following one branch fully before exploring others. It’s recursive, like exploring a maze by always turning left until you hit a wall.\n\n\nA. Recursive Form\nvoid dfs(int u) {\n    visited[u] = true;\n    for (int v : adj[u]) {\n        if (!visited[v])\n            dfs(v);\n    }\n}\nStart it:\ndfs(start_node);\n\n\nB. Iterative Form (with Stack)\nvoid dfs_iter(int start) {\n    stack&lt;int&gt; s;\n    s.push(start);\n    while (!s.empty()) {\n        int u = s.top(); s.pop();\n        if (visited[u]) continue;\n        visited[u] = true;\n        for (int v : adj[u]) s.push(v);\n    }\n}\n\n\nC. Complexity\n\n\n\nGraph Type\nTime\nSpace\n\n\n\n\nAdjacency List\nO(V + E)\nO(V)\n\n\n\nDFS is used in:\n\nConnected components- Cycle detection- Topological sort- Backtracking & search- Articulation points / bridges\n\n\n\n3. Breadth-First Search (BFS)\nBFS explores neighbors first , it’s like expanding in waves. This guarantees shortest path in unweighted graphs.\n\n\nA. BFS with Queue\nvoid bfs(int start) {\n    queue&lt;int&gt; q;\n    q.push(start);\n    visited[start] = true;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (int v : adj[u]) {\n            if (!visited[v]) {\n                visited[v] = true;\n                q.push(v);\n            }\n        }\n    }\n}\n\n\nB. Track Distance\nint dist[MAX];\nvoid bfs_dist(int s) {\n    fill(dist, dist + MAX, -1);\n    dist[s] = 0;\n    queue&lt;int&gt; q; q.push(s);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (int v : adj[u]) {\n            if (dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n}\nNow dist[v] gives shortest distance from s.\n\n\nC. Complexity\nSame as DFS:\n\n\n\nTime\nSpace\n\n\n\n\nO(V + E)\nO(V)\n\n\n\nUsed in:\n\nShortest paths (unweighted)- Level-order traversal- Bipartite check- Connected components\n\n\n\n4. Iterative Deepening Search (IDS)\nDFS is memory-light but might go too deep. BFS is optimal but can use a lot of memory. Iterative Deepening Search (IDS) combines both.\nIt performs DFS with increasing depth limits:\nbool dls(int u, int target, int depth) {\n    if (u == target) return true;\n    if (depth == 0) return false;\n    for (int v : adj[u])\n        if (dls(v, target, depth - 1)) return true;\n    return false;\n}\n\nbool ids(int start, int target, int max_depth) {\n    for (int d = 0; d &lt;= max_depth; d++)\n        if (dls(start, target, d)) return true;\n    return false;\n}\nUsed in:\n\nAI search problems (state spaces)- Game trees (chess, puzzles)\n\n\n\n5. Traversal Order Examples\nFor a graph:\n1 - 2 - 3\n|   |\n4 - 5\nDFS (starting at 1): 1 → 2 → 3 → 5 → 4 BFS (starting at 1): 1 → 2 → 4 → 3 → 5\n\n\n6. Directed vs Undirected\n\nUndirected: mark both directions- Directed: follow edge direction only DFS on directed graphs is core to:\nSCC (Strongly Connected Components)- Topological Sorting- Reachability analysis\n\n\n\n7. Traversal Trees\nEach traversal implicitly builds a spanning tree:\n\nDFS Tree: based on recursion- BFS Tree: based on levels Use them to:\nDetect cross edges, back edges- Classify edges (important for algorithms like Tarjan’s)\n\n\n\n8. Comparison\n\n\n\n\n\n\n\n\nAspect\nDFS\nBFS\n\n\n\n\nStrategy\nDeep first\nLevel-wise\n\n\nSpace\nO(V) (stack)\nO(V) (queue)\n\n\nPath Optimality\nNot guaranteed\nYes (unweighted)\n\n\nApplications\nCycle detection, backtracking\nShortest path, level order\n\n\n\n\n\nTiny Code\nDFS + BFS Combo:\nvoid traverse(int n) {\n    for (int i = 1; i &lt;= n; i++) visited[i] = false;\n    dfs(1);\n    for (int i = 1; i &lt;= n; i++) visited[i] = false;\n    bfs(1);\n}\n\n\nWhy It Matters\nDFS and BFS are the roots of graph theory in practice. Every algorithm you’ll meet later , shortest paths, flows, SCCs , builds upon them.\nThey teach you how to navigate structure, how to systematically explore unknowns, and how search lies at the heart of computation.\n\n\nTry It Yourself\n\nBuild an adjacency list and run DFS/BFS from vertex 1.\nTrack discovery and finish times in DFS.\nUse BFS to compute shortest paths in an unweighted graph.\nModify DFS to count connected components.\nImplement IDS for a puzzle like the 8-puzzle.\n\nGraph traversal is the art of exploration , once you master it, the rest of graph theory falls into place.\n\n\n\n32. Strongly Connected Components (Tarjan, Kosaraju)\nIn directed graphs, edges have direction, so connectivity gets tricky. It’s not enough for vertices to be reachable , you need mutual reachability.\nThat’s the essence of a strongly connected component (SCC):\n\nA set of vertices where every vertex can reach every other vertex.\n\nThink of SCCs as islands of mutual connectivity , inside, you can go anywhere; outside, you can’t. They’re the building blocks for simplifying directed graphs into condensation DAGs (no cycles).\nWe’ll explore two classic algorithms:\n\nKosaraju’s Algorithm , clean, intuitive, two-pass- Tarjan’s Algorithm , one-pass, stack-based elegance\n\n\n1. Definition\nA Strongly Connected Component (SCC) in a directed graph ( G = (V, E) ) is a maximal subset of vertices \\(C \\subseteq V\\) such that for every pair ( (u, v) C ): \\(u \\to v\\) and \\(v \\to u\\).\nIn other words, every node in (C) is reachable from every other node in (C).\nExample:\n1 → 2 → 3 → 1   forms an SCC  \n4 → 5           separate SCCs\n\n\n2. Applications\n\nCondensation DAG: compress SCCs into single nodes , no cycles remain.- Component-based reasoning: topological sorting on DAG of SCCs.- Program analysis: detecting cycles, dependencies.- Web graphs: find clusters of mutually linked pages.- Control-flow: loops and strongly connected subroutines.\n\n\n\n3. Kosaraju’s Algorithm\nA simple two-pass algorithm using DFS and graph reversal.\nSteps:\n\nRun DFS and push nodes onto a stack in finish-time order.\nReverse the graph (edges flipped).\nPop nodes from stack; DFS on reversed graph; each DFS = one SCC.\n\n\n\nA. Implementation\nvector&lt;int&gt; adj[MAX], rev[MAX];\nbool visited[MAX];\nstack&lt;int&gt; st;\nvector&lt;vector&lt;int&gt;&gt; sccs;\n\nvoid dfs1(int u) {\n    visited[u] = true;\n    for (int v : adj[u])\n        if (!visited[v])\n            dfs1(v);\n    st.push(u);\n}\n\nvoid dfs2(int u, vector&lt;int&gt;& comp) {\n    visited[u] = true;\n    comp.push_back(u);\n    for (int v : rev[u])\n        if (!visited[v])\n            dfs2(v, comp);\n}\n\nvoid kosaraju(int n) {\n    // Pass 1: order by finish time\n    for (int i = 1; i &lt;= n; i++)\n        if (!visited[i]) dfs1(i);\n\n    // Reverse graph\n    for (int u = 1; u &lt;= n; u++)\n        for (int v : adj[u])\n            rev[v].push_back(u);\n\n    // Pass 2: collect SCCs\n    fill(visited, visited + n + 1, false);\n    while (!st.empty()) {\n        int u = st.top(); st.pop();\n        if (!visited[u]) {\n            vector&lt;int&gt; comp;\n            dfs2(u, comp);\n            sccs.push_back(comp);\n        }\n    }\n}\nTime Complexity: (O(V + E)) , two DFS passes.\nSpace Complexity: (O(V + E))\n\n\nB. Example\nGraph:\n1 → 2 → 3  \n↑   ↓   ↓  \n5 ← 4 ← 6\nSCCs:\n\n{1,2,4,5}- {3,6}\n\n\n\n4. Tarjan’s Algorithm\nMore elegant: one DFS pass, no reversal, stack-based. It uses discovery times and low-link values to detect SCC roots.\n\n\nA. Idea\n\ndisc[u]: discovery time of node u- low[u]: smallest discovery time reachable from u- A node is root of an SCC if disc[u] == low[u] Maintain a stack of active nodes (in current DFS path).\n\n\n\nB. Implementation\nvector&lt;int&gt; adj[MAX];\nint disc[MAX], low[MAX], timer;\nbool inStack[MAX];\nstack&lt;int&gt; st;\nvector&lt;vector&lt;int&gt;&gt; sccs;\n\nvoid dfs_tarjan(int u) {\n    disc[u] = low[u] = ++timer;\n    st.push(u);\n    inStack[u] = true;\n\n    for (int v : adj[u]) {\n        if (!disc[v]) {\n            dfs_tarjan(v);\n            low[u] = min(low[u], low[v]);\n        } else if (inStack[v]) {\n            low[u] = min(low[u], disc[v]);\n        }\n    }\n\n    if (disc[u] == low[u]) {\n        vector&lt;int&gt; comp;\n        while (true) {\n            int v = st.top(); st.pop();\n            inStack[v] = false;\n            comp.push_back(v);\n            if (v == u) break;\n        }\n        sccs.push_back(comp);\n    }\n}\n\nvoid tarjan(int n) {\n    for (int i = 1; i &lt;= n; i++)\n        if (!disc[i])\n            dfs_tarjan(i);\n}\nTime Complexity: (O(V + E))\nSpace Complexity: (O(V))\n\n\nC. Walkthrough\nGraph:\n1 → 2 → 3  \n↑   ↓   ↓  \n5 ← 4 ← 6\nDFS visits nodes in order; when it finds a node whose disc == low, it pops from the stack to form an SCC.\nResult:\nSCC1: 1 2 4 5\nSCC2: 3 6\n\n\n5. Comparison\n\n\n\nFeature\nKosaraju\nTarjan\n\n\n\n\nDFS Passes\n2\n1\n\n\nReversal Needed\nYes\nNo\n\n\nStack\nYes (finish order)\nYes (active path)\n\n\nImplementation\nSimple conceptually\nCompact, efficient\n\n\nTime\nO(V + E)\nO(V + E)\n\n\n\n\n\n6. Condensation Graph\nOnce SCCs are found, you can build a DAG: Each SCC becomes a node, edges represent cross-SCC connections. Topological sorting now applies.\nUsed in:\n\nDependency analysis- Strong component compression- DAG dynamic programming\n\n\n\nTiny Code\nPrint SCCs (Tarjan):\ntarjan(n);\nfor (auto &comp : sccs) {\n    for (int x : comp) printf(\"%d \", x);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\nSCC algorithms turn chaotic directed graphs into structured DAGs. They’re the key to reasoning about cycles, dependencies, and modularity.\nUnderstanding them reveals a powerful truth:\n\n“Every complex graph can be reduced to a simple hierarchy , once you find its strongly connected core.”\n\n\n\nTry It Yourself\n\nImplement both Kosaraju and Tarjan , verify they match.\nBuild SCC DAG and run topological sort on it.\nDetect cycles via SCC size &gt; 1.\nUse SCCs to solve 2-SAT (boolean satisfiability).\nVisualize condensation of a graph with 6 nodes.\n\nOnce you can find SCCs, you can tame directionality , transforming messy networks into ordered systems.\n\n\n\n33. Shortest Paths (Dijkstra, Bellman-Ford, A*, Johnson)\nOnce you can traverse a graph, the next natural question is:\n\n“What is the shortest path between two vertices?”\n\nShortest path algorithms are the heart of routing, navigation, planning, and optimization. They compute minimal cost paths , whether distance, time, or weight , and adapt to different edge conditions (non-negative, negative, heuristic).\nThis section covers the most essential algorithms:\n\nDijkstra’s Algorithm , efficient for non-negative weights- Bellman-Ford Algorithm , handles negative edges- A* , best-first with heuristics- Johnson’s Algorithm , all-pairs shortest paths in sparse graphs\n\n\n1. The Shortest Path Problem\nGiven a weighted graph ( G = (V, E) ) and a source ( s ), find \\(\\text{dist}[v]\\), the minimum total weight to reach every vertex ( v ).\nVariants:\n\nSingle-source shortest path (SSSP) , one source to all- Single-pair , one source to one target- All-pairs shortest path (APSP) , every pair- Dynamic shortest path , with updates\n\n\n\n2. Dijkstra’s Algorithm\nBest for non-negative weights. Idea: explore vertices in increasing distance order, like water spreading.\n\n\nA. Steps\n\nInitialize all distances to infinity.\nSet source distance = 0.\nUse a priority queue to always pick the node with smallest tentative distance.\nRelax all outgoing edges.\n\n\n\nB. Implementation (Adjacency List)\n#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n\nconst int INF = 1e9;\nvector&lt;pair&lt;int,int&gt;&gt; adj[1000]; // (neighbor, weight)\nint dist[1000];\n\nvoid dijkstra(int n, int s) {\n    fill(dist, dist + n + 1, INF);\n    dist[s] = 0;\n    priority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq;\n    pq.push({0, s});\n\n    while (!pq.empty()) {\n        auto [d, u] = pq.top(); pq.pop();\n        if (d != dist[u]) continue;\n        for (auto [v, w] : adj[u]) {\n            if (dist[v] &gt; dist[u] + w) {\n                dist[v] = dist[u] + w;\n                pq.push({dist[v], v});\n            }\n        }\n    }\n}\nComplexity:\n\nUsing priority queue (binary heap): \\(O((V + E)\\log V)\\)\nSpace: \\(O(V + E)\\)\n\n\n\nC. Example\nGraph:\n1 →(2) 2 →(3) 3\n↓(4)       ↑(1)\n4 →(2)─────┘\ndijkstra(1) gives shortest distances:\ndist[1] = 0  \ndist[2] = 2  \ndist[3] = 5  \ndist[4] = 4\n\n\nD. Properties\n\nWorks only if all edges \\(w \\ge 0\\)- Can reconstruct path via parent[v]- Used in:\n\nGPS and routing systems - Network optimization - Scheduling with positive costs\n\n\n\n\n3. Bellman-Ford Algorithm\nHandles negative edge weights, and detects negative cycles.\n\n\nA. Idea\nRelax all edges (V-1) times. If on (V)-th iteration you can still relax → negative cycle exists.\n\n\nB. Implementation\nstruct Edge { int u, v, w; };\nvector&lt;Edge&gt; edges;\nint dist[1000];\n\nbool bellman_ford(int n, int s) {\n    fill(dist, dist + n + 1, INF);\n    dist[s] = 0;\n    for (int i = 1; i &lt;= n - 1; i++) {\n        for (auto e : edges) {\n            if (dist[e.u] + e.w &lt; dist[e.v])\n                dist[e.v] = dist[e.u] + e.w;\n        }\n    }\n    // Check for negative cycle\n    for (auto e : edges)\n        if (dist[e.u] + e.w &lt; dist[e.v])\n            return false; // negative cycle\n    return true;\n}\nComplexity: (O(VE)) Works even when (w &lt; 0).\n\n\nC. Example\nGraph:\n1 →(2) 2 →(-5) 3 →(2) 4\nBellman-Ford finds path 1→2→3→4 with total cost (-1).\nIf a cycle reduces total weight indefinitely, algorithm detects it.\n\n\nD. Use Cases\n\nCurrency exchange arbitrage- Game graphs with penalties- Detecting impossible constraints\n\n\n\n4. A* Search Algorithm\nHeuristic-guided shortest path, perfect for pathfinding (AI, maps, games).\nIt combines actual cost and estimated cost: \\[\nf(v) = g(v) + h(v)\n\\] where\n\n(g(v)): known cost so far- (h(v)): heuristic estimate (must be admissible)\n\n\n\nA. Pseudocode\npriority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq;\ng[start] = 0;\npq.push({h[start], start});\n\nwhile (!pq.empty()) {\n    auto [f, u] = pq.top(); pq.pop();\n    if (u == goal) break;\n    for (auto [v, w] : adj[u]) {\n        int new_g = g[u] + w;\n        if (new_g &lt; g[v]) {\n            g[v] = new_g;\n            pq.push({g[v] + h[v], v});\n        }\n    }\n}\nHeuristic Example:\n\nEuclidean distance (for grids)- Manhattan distance (for 4-direction movement)\n\n\n\nB. Use Cases\n\nGame AI (pathfinding)- Robot motion planning- Map navigation Complexity: (O(E)) in best case, depends on heuristic quality.\n\n\n\n5. Johnson’s Algorithm\nGoal: All-Pairs Shortest Path in sparse graphs with negative edges (no negative cycles).\nIdea:\n\nAdd new vertex q connected to all others with edge weight 0\nRun Bellman-Ford from q to get potential h(v)\nReweight edges: (w’(u, v) = w(u, v) + h(u) - h(v)) (now all weights ≥ 0)\nRun Dijkstra from each vertex\n\nComplexity: (O\\(VE + V^2 \\log V\\))\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nHandles Negative Weights\nDetects Negative Cycle\nHeuristic\nComplexity\nUse Case\n\n\n\n\nDijkstra\nNo\nNo\nNo\nO((V+E) log V)\nNon-negative weights\n\n\nBellman-Ford\nYes\nYes\nNo\nO(VE)\nNegative edges\n\n\nA*\nNo (unless careful)\nNo\nYes\nDepends\nPathfinding\n\n\nJohnson\nYes (no neg. cycles)\nYes\nNo\nO(VE + V log V)\nAll-pairs, sparse\n\n\n\n\n\nTiny Code\nDijkstra Example:\ndijkstra(n, 1);\nfor (int i = 1; i &lt;= n; i++)\n    printf(\"dist[%d] = %d\\n\", i, dist[i]);\n\n\nWhy It Matters\nShortest paths are the essence of optimization , not just in graphs, but in reasoning: finding minimal cost, minimal distance, minimal risk.\nThese algorithms teach:\n\n“The path to a goal isn’t random , it’s guided by structure, weight, and knowledge.”\n\n\n\nTry It Yourself\n\nBuild a weighted graph and compare Dijkstra vs Bellman-Ford.\nIntroduce a negative edge and observe Bellman-Ford detecting it.\nImplement A* on a grid with obstacles.\nUse Dijkstra to plan routes in a city map dataset.\nTry Johnson’s algorithm for all-pairs shortest paths.\n\nMaster these, and you master direction + cost = intelligence in motion.\n\n\n\n34. Shortest Path Variants (0-1 BFS, Bidirectional, Heuristic A*)\nSometimes the classic shortest path algorithms aren’t enough. You might have special edge weights (only 0 or 1), a need for faster searches, or extra structure you can exploit.\nThat’s where shortest path variants come in , they’re optimized adaptations of the big three (BFS, Dijkstra, A*) for specific scenarios.\nIn this section, we’ll explore:\n\n0-1 BFS → when edge weights are only 0 or 1- Bidirectional Search → meet-in-the-middle for speed- Heuristic A* → smarter exploration guided by estimates Each shows how structure in your problem can yield speed-ups.\n\n\n1. 0-1 BFS\nIf all edge weights are either 0 or 1, you don’t need a priority queue. A deque (double-ended queue) is enough for (O(V + E)) time.\nWhy? Because edges with weight 0 should be processed immediately, while edges with weight 1 can wait one step longer.\n\n\nA. Algorithm\nUse a deque.\n\nWhen relaxing an edge with weight 0, push to front.- When relaxing an edge with weight 1, push to back.\n\nconst int INF = 1e9;\nvector&lt;pair&lt;int,int&gt;&gt; adj[1000]; // (v, w)\nint dist[1000];\n\nvoid zero_one_bfs(int n, int s) {\n    fill(dist, dist + n + 1, INF);\n    deque&lt;int&gt; dq;\n    dist[s] = 0;\n    dq.push_front(s);\n\n    while (!dq.empty()) {\n        int u = dq.front(); dq.pop_front();\n        for (auto [v, w] : adj[u]) {\n            if (dist[v] &gt; dist[u] + w) {\n                dist[v] = dist[u] + w;\n                if (w == 0) dq.push_front(v);\n                else dq.push_back(v);\n            }\n        }\n    }\n}\n\n\nB. Example\nGraph:\n1 -0-&gt; 2 -1-&gt; 3  \n|              ^  \n1              |  \n+--------------+\nShortest path from 1 to 3 = 1 (via edge 1-2-3). Deque ensures weight-0 edges don’t get delayed.\n\n\nC. Complexity\n\n\n\nTime\nSpace\nNotes\n\n\n\n\nO(V + E)\nO(V)\nOptimal for binary weights\n\n\n\nUsed in:\n\nLayered BFS- Grid problems with binary costs- BFS with teleportation (weight 0 edges)\n\n\n\n2. Bidirectional Search\nSometimes you just need one path , from source to target , in an unweighted graph. Instead of expanding from one side, expand from both ends and stop when they meet.\nThis reduces search depth from (O\\(b^d\\)) to (O\\(b^{d/2}\\)) (huge gain for large graphs).\n\n\nA. Idea\nRun BFS from both source and target simultaneously. When their frontiers intersect, you’ve found the shortest path.\n\n\nB. Implementation\nbool visited_from_s[MAX], visited_from_t[MAX];\nqueue&lt;int&gt; qs, qt;\n\nint bidirectional_bfs(int s, int t) {\n    qs.push(s); visited_from_s[s] = true;\n    qt.push(t); visited_from_t[t] = true;\n\n    while (!qs.empty() && !qt.empty()) {\n        if (step(qs, visited_from_s, visited_from_t)) return 1;\n        if (step(qt, visited_from_t, visited_from_s)) return 1;\n    }\n    return 0;\n}\n\nbool step(queue&lt;int&gt;& q, bool vis[], bool other[]) {\n    int size = q.size();\n    while (size--) {\n        int u = q.front(); q.pop();\n        if (other[u]) return true;\n        for (int v : adj[u]) {\n            if (!vis[v]) {\n                vis[v] = true;\n                q.push(v);\n            }\n        }\n    }\n    return false;\n}\n\n\nC. Complexity\n\n\n\nTime\nSpace\nNotes\n\n\n\n\nO\\(b^{d/2}\\)\nO\\(b^{d/2}\\)\nDoubly fast in practice\n\n\n\nUsed in:\n\nMaze solvers- Shortest paths in large sparse graphs- Social network “degrees of separation”\n\n\n\n3. Heuristic A* (Revisited)\nA* generalizes Dijkstra with goal-directed search using heuristics. We revisit it here to show how heuristics change exploration order.\n\n\nA. Cost Function\n\\[\nf(v) = g(v) + h(v)\n\\]\n\n(g(v)): cost so far- (h(v)): estimated cost to goal- (h(v)) must be admissible ((h(v) ))\n\n\n\nB. Implementation\nstruct Node {\n    int v; int f, g;\n    bool operator&gt;(const Node& o) const { return f &gt; o.f; }\n};\n\npriority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt;&gt; pq;\n\nvoid astar(int start, int goal) {\n    g[start] = 0;\n    h[start] = heuristic(start, goal);\n    pq.push({start, g[start] + h[start], g[start]});\n\n    while (!pq.empty()) {\n        auto [u, f_u, g_u] = pq.top(); pq.pop();\n        if (u == goal) break;\n        for (auto [v, w] : adj[u]) {\n            int new_g = g[u] + w;\n            if (new_g &lt; g[v]) {\n                g[v] = new_g;\n                int f_v = new_g + heuristic(v, goal);\n                pq.push({v, f_v, new_g});\n            }\n        }\n    }\n}\n\n\nC. Example Heuristics\n\nGrid map: Manhattan distance (h(x, y) = |x - x_g| + |y - y_g|)\nNavigation: straight-line (Euclidean)- Game tree: evaluation function\n\n\n\nD. Performance\n\n\n\nHeuristic\nEffect\n\n\n\n\nPerfect (h = true cost)\nOptimal, visits minimal nodes\n\n\nAdmissible but weak\nStill correct, more nodes\n\n\nOverestimate\nMay fail (non-admissible)\n\n\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWeight Type\nStrategy\nTime\nSpace\nNotes\n\n\n\n\n0-1 BFS\n0 or 1\nDeque-based\nO(V+E)\nO(V)\nNo heap\n\n\nBidirectional BFS\nUnweighted\nTwo-way search\nO\\(b^{d/2}\\)\nO\\(b^{d/2}\\)\nMeets in middle\n\n\nA*\nNon-negative\nHeuristic search\nDepends\nO(V)\nGuided\n\n\n\n\n\n5. Example Scenario\n\n\n\n\n\n\n\nProblem\nVariant\n\n\n\n\nGrid with teleport (cost 0)\n0-1 BFS\n\n\nHuge social graph (find shortest chain)\nBidirectional BFS\n\n\nGame AI pathfinding\nA* with Manhattan heuristic\n\n\n\n\n\nTiny Code\n0-1 BFS Quick Demo:\nadd_edge(1, 2, 0);\nadd_edge(2, 3, 1);\nzero_one_bfs(3, 1);\nprintf(\"%d\\n\", dist[3]); // shortest = 1\n\n\nWhy It Matters\nSpecial cases deserve special tools. These variants show that understanding structure (like edge weights or symmetry) can yield huge gains.\nThey embody a principle:\n\n“Don’t just run faster , run smarter, guided by what you know.”\n\n\n\nTry It Yourself\n\nImplement 0-1 BFS for a grid with cost 0 teleports.\nCompare BFS vs Bidirectional BFS on a large maze.\nWrite A* for an 8x8 chessboard knight’s move puzzle.\nTune heuristics , see how overestimating breaks A*.\nCombine A* and 0-1 BFS for hybrid search.\n\nWith these in hand, you can bend shortest path search to the shape of your problem , efficient, elegant, and exact.\n\n\n\n35. Minimum Spanning Trees (Kruskal, Prim, Borůvka)\nWhen a graph connects multiple points with weighted edges, sometimes you don’t want the shortest path, but the cheapest network that connects everything.\nThat’s the Minimum Spanning Tree (MST) problem:\n\nGiven a connected, weighted, undirected graph, find a subset of edges that connects all vertices with minimum total weight and no cycles.\n\nMSTs are everywhere , from building networks and designing circuits to clustering and approximation algorithms.\nThree cornerstone algorithms solve it beautifully:\n\nKruskal’s , edge-based, union-find- Prim’s , vertex-based, greedy expansion- Borůvka’s , component merging in parallel\n\n\n1. What Is a Spanning Tree?\nA spanning tree connects all vertices with exactly (V-1) edges. Among all spanning trees, the one with minimum total weight is the MST.\nProperties:\n\nContains no cycles- Connects all vertices- Edge count = (V - 1)- Unique if all weights distinct\n\n\n\n2. MST Applications\n\nNetwork design (roads, cables, pipelines)- Clustering (e.g., hierarchical clustering)- Image segmentation- Approximation (e.g., TSP ~ 2 × MST)- Graph simplification\n\n\n\n3. Kruskal’s Algorithm\nBuild the MST edge-by-edge, in order of increasing weight. Use Union-Find (Disjoint Set Union) to avoid cycles.\n\n\nA. Steps\n\nSort all edges by weight.\nInitialize each vertex as its own component.\nFor each edge (u, v):\n\nIf u and v are in different components → include edge - Union their sets Stop when (V-1) edges chosen.\n\n\n\n\nB. Implementation\nstruct Edge { int u, v, w; };\nvector&lt;Edge&gt; edges;\nint parent[MAX], rank_[MAX];\n\nint find(int x) {\n    return parent[x] == x ? x : parent[x] = find(parent[x]);\n}\nbool unite(int a, int b) {\n    a = find(a); b = find(b);\n    if (a == b) return false;\n    if (rank_[a] &lt; rank_[b]) swap(a, b);\n    parent[b] = a;\n    if (rank_[a] == rank_[b]) rank_[a]++;\n    return true;\n}\n\nint kruskal(int n) {\n    iota(parent, parent + n + 1, 0);\n    sort(edges.begin(), edges.end(), [](Edge a, Edge b){ return a.w &lt; b.w; });\n    int total = 0;\n    for (auto &e : edges)\n        if (unite(e.u, e.v))\n            total += e.w;\n    return total;\n}\nComplexity:\n\nSorting edges: (O\\(E \\log E\\))- Union-Find operations: (O((V))) (almost constant)- Total: (O\\(E \\log E\\))\n\n\n\nC. Example\nGraph:\n1 -4- 2  \n|     |  \n2     3  \n \\-1-/\nEdges sorted: (1-3,1), (1-2,4), (2-3,3)\nPick 1-3, 2-3 → MST weight = 1 + 3 = 4\n\n\n4. Prim’s Algorithm\nGrow MST from a starting vertex, adding the smallest outgoing edge each step.\nSimilar to Dijkstra , but pick edges, not distances.\n\n\nA. Steps\n\nStart with one vertex, mark as visited.\nUse priority queue for candidate edges.\nPick smallest edge that connects to an unvisited vertex.\nAdd vertex to MST, repeat until all visited.\n\n\n\nB. Implementation\nvector&lt;pair&lt;int,int&gt;&gt; adj[MAX]; // (v, w)\nbool used[MAX];\nint prim(int n, int start) {\n    priority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq;\n    pq.push({0, start});\n    int total = 0;\n\n    while (!pq.empty()) {\n        auto [w, u] = pq.top(); pq.pop();\n        if (used[u]) continue;\n        used[u] = true;\n        total += w;\n        for (auto [v, w2] : adj[u])\n            if (!used[v]) pq.push({w2, v});\n    }\n    return total;\n}\nComplexity:\n\n\\(O((V+E) \\log V)\\) with binary heap\n\nUsed when:\n\nGraph is dense\nEasier to grow tree than sort all edges\n\n\n\nC. Example\nGraph:\n1 -2- 2  \n|     |  \n4     1  \n \\-3-/\nStart at 1 → choose (1-2), (1-3) → MST weight = 2 + 3 = 5\n\n\n5. Borůvka’s Algorithm\nLess famous, but elegant , merges cheapest outgoing edge per component in parallel.\nEach component picks one cheapest outgoing edge, adds it, merges components. Repeat until one component left.\nComplexity: (O\\(E \\log V\\))\nUsed in parallel/distributed MST computations.\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nStrategy\nTime\nSpace\nBest For\n\n\n\n\nKruskal\nEdge-based, sort all edges\nO(E log E)\nO(E)\nSparse graphs\n\n\nPrim\nVertex-based, grow tree\nO(E log V)\nO(V+E)\nDense graphs\n\n\nBorůvka\nComponent merging\nO(E log V)\nO(E)\nParallel MST\n\n\n\n\n\n7. MST Properties\n\nCut Property: For any cut, smallest crossing edge ∈ MST.- Cycle Property: For any cycle, largest edge not ∈ MST.- MST may not be unique if equal weights.\n\n\n\n8. Building the Tree\nStore MST edges:\nvector&lt;Edge&gt; mst_edges;\nif (unite(e.u, e.v)) mst_edges.push_back(e);\nThen use MST for:\n\nPath queries- Clustering (remove largest edge)- Approximation TSP (preorder traversal)\n\n\n\nTiny Code\nKruskal Example:\nedges.push_back({1,2,4});\nedges.push_back({1,3,1});\nedges.push_back({2,3,3});\nprintf(\"MST = %d\\n\", kruskal(3)); // 4\n\n\nWhy It Matters\nMSTs model connection without redundancy. They’re about efficiency , connecting everything at minimal cost, a principle that appears in infrastructure, data, and even ideas.\nThey teach:\n\n“You can connect the whole with less , if you choose wisely.”\n\n\n\nTry It Yourself\n\nImplement Kruskal’s algorithm using union-find.\nRun Prim’s algorithm and compare output.\nBuild MST on random weighted graph , visualize tree.\nRemove heaviest edge from MST to form two clusters.\nExplore Borůvka for parallel execution.\n\nMSTs are how you span complexity with minimal effort , a tree of balance, economy, and order.\n\n\n\n36. Flows (Ford-Fulkerson, Edmonds-Karp, Dinic)\nSome graphs don’t just connect , they carry something. Imagine water flowing through pipes, traffic through roads, data through a network. Each edge has a capacity, and you want to know:\n\n“How much can I send from source to sink before the system clogs?”\n\nThat’s the Maximum Flow problem , a cornerstone of combinatorial optimization, powering algorithms for matching, cuts, scheduling, and more.\nThis section covers the big three:\n\nFord-Fulkerson , the primal idea- Edmonds-Karp , BFS-based implementation- Dinic’s Algorithm , layered speed\n\n\n1. Problem Definition\nGiven a directed graph ( G = (V, E) ), each edge ( (u, v) ) has a capacity ( c(u, v) ).\nWe have:\n\nSource ( s )- Sink ( t ) We want the maximum flow from ( s ) to ( t ): a function ( f(u, v) ) that satisfies:\n\n\nCapacity constraint: ( 0 f(u, v) c(u, v) )\nFlow conservation: For every vertex \\(v \\neq s, t\\): (f(u, v) = f(v, w))\n\nTotal flow = (f(s, v))\n\n\n2. The Big Picture\nMax Flow - Min Cut Theorem:\n\nThe value of the maximum flow equals the capacity of the minimum cut.\n\nSo finding a max flow is equivalent to finding the bottleneck.\n\n\n3. Ford-Fulkerson Method\nThe idea:\n\nWhile there exists a path from (s) to (t) with available capacity, push flow along it.\n\nEach step:\n\nFind augmenting path\nSend flow = min residual capacity along it\nUpdate residual capacities\n\nRepeat until no augmenting path.\n\n\nA. Residual Graph\nResidual capacity: \\[\nr(u, v) = c(u, v) - f(u, v)\n\\] If ( f(u, v) &gt; 0 ), then add reverse edge ( (v, u) ) with capacity ( f(u, v) ).\nThis allows undoing flow if needed.\n\n\nB. Implementation (DFS-style)\nconst int INF = 1e9;\nvector&lt;pair&lt;int,int&gt;&gt; adj[MAX];\nint cap[MAX][MAX];\n\nint dfs(int u, int t, int flow, vector&lt;int&gt;& vis) {\n    if (u == t) return flow;\n    vis[u] = 1;\n    for (auto [v, _] : adj[u]) {\n        if (!vis[v] && cap[u][v] &gt; 0) {\n            int pushed = dfs(v, t, min(flow, cap[u][v]), vis);\n            if (pushed &gt; 0) {\n                cap[u][v] -= pushed;\n                cap[v][u] += pushed;\n                return pushed;\n            }\n        }\n    }\n    return 0;\n}\n\nint ford_fulkerson(int s, int t, int n) {\n    int flow = 0;\n    while (true) {\n        vector&lt;int&gt; vis(n + 1, 0);\n        int pushed = dfs(s, t, INF, vis);\n        if (pushed == 0) break;\n        flow += pushed;\n    }\n    return flow;\n}\nComplexity: (O\\(E \\cdot \\text{max flow}\\)) , depends on flow magnitude.\n\n\n4. Edmonds-Karp Algorithm\nA refinement:\n\nAlways choose shortest augmenting path (by edges) using BFS.\n\nGuarantees polynomial time.\n\n\nA. Implementation (BFS + parent tracking)\nint bfs(int s, int t, vector&lt;int&gt;& parent, int n) {\n    fill(parent.begin(), parent.end(), -1);\n    queue&lt;pair&lt;int,int&gt;&gt; q;\n    q.push({s, INF});\n    parent[s] = -2;\n    while (!q.empty()) {\n        auto [u, flow] = q.front(); q.pop();\n        for (auto [v, _] : adj[u]) {\n            if (parent[v] == -1 && cap[u][v] &gt; 0) {\n                int new_flow = min(flow, cap[u][v]);\n                parent[v] = u;\n                if (v == t) return new_flow;\n                q.push({v, new_flow});\n            }\n        }\n    }\n    return 0;\n}\n\nint edmonds_karp(int s, int t, int n) {\n    int flow = 0;\n    vector&lt;int&gt; parent(n + 1);\n    int new_flow;\n    while ((new_flow = bfs(s, t, parent, n))) {\n        flow += new_flow;\n        int v = t;\n        while (v != s) {\n            int u = parent[v];\n            cap[u][v] -= new_flow;\n            cap[v][u] += new_flow;\n            v = u;\n        }\n    }\n    return flow;\n}\nComplexity: (O\\(VE^2\\)) Always terminates (no dependence on flow values).\n\n\n5. Dinic’s Algorithm\nA modern classic , uses BFS to build level graph, and DFS to send blocking flow.\nIt works layer-by-layer, avoiding useless exploration.\n\n\nA. Steps\n\nBuild level graph via BFS (assign levels to reachable nodes).\nDFS sends flow along level-respecting paths.\nRepeat until no path remains.\n\n\n\nB. Implementation\nvector&lt;int&gt; level, ptr;\n\nbool bfs_level(int s, int t, int n) {\n    fill(level.begin(), level.end(), -1);\n    queue&lt;int&gt; q;\n    q.push(s);\n    level[s] = 0;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (auto [v, _] : adj[u])\n            if (level[v] == -1 && cap[u][v] &gt; 0) {\n                level[v] = level[u] + 1;\n                q.push(v);\n            }\n    }\n    return level[t] != -1;\n}\n\nint dfs_flow(int u, int t, int pushed) {\n    if (u == t || pushed == 0) return pushed;\n    for (int &cid = ptr[u]; cid &lt; (int)adj[u].size(); cid++) {\n        int v = adj[u][cid].first;\n        if (level[v] == level[u] + 1 && cap[u][v] &gt; 0) {\n            int tr = dfs_flow(v, t, min(pushed, cap[u][v]));\n            if (tr &gt; 0) {\n                cap[u][v] -= tr;\n                cap[v][u] += tr;\n                return tr;\n            }\n        }\n    }\n    return 0;\n}\n\nint dinic(int s, int t, int n) {\n    int flow = 0;\n    level.resize(n + 1);\n    ptr.resize(n + 1);\n    while (bfs_level(s, t, n)) {\n        fill(ptr.begin(), ptr.end(), 0);\n        while (int pushed = dfs_flow(s, t, INF))\n            flow += pushed;\n    }\n    return flow;\n}\nComplexity: (O\\(EV^2\\)) worst case, (O\\(E \\sqrt{V}\\)) in practice.\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nStrategy\nHandles\nTime\nNotes\n\n\n\n\nFord-Fulkerson\nDFS augmenting paths\nIntegral capacities\nO\\(E × max_flow\\)\nSimple, may loop on reals\n\n\nEdmonds-Karp\nBFS augmenting paths\nAll capacities\nO(VE²)\nAlways terminates\n\n\nDinic\nLevel graph + DFS\nAll capacities\nO(V²E)\nFast in practice\n\n\n\n\n\n7. Applications\n\nNetwork routing- Bipartite matching- Task assignment (flows = people → jobs)- Image segmentation (min-cut)- Circulation with demands- Data pipelines, max throughput systems\n\n\n\nTiny Code\nFord-Fulkerson Example:\nadd_edge(1, 2, 3);\nadd_edge(1, 3, 2);\nadd_edge(2, 3, 5);\nadd_edge(2, 4, 2);\nadd_edge(3, 4, 3);\nprintf(\"Max flow = %d\\n\", ford_fulkerson(1, 4, 4)); // 5\n\n\nWhy It Matters\nFlow algorithms transform capacity constraints into solvable systems. They reveal the deep unity between optimization and structure: every maximum flow defines a minimum bottleneck cut.\nThey embody a timeless truth:\n\n“To understand limits, follow the flow.”\n\n\n\nTry It Yourself\n\nImplement Ford-Fulkerson using DFS.\nSwitch to Edmonds-Karp and observe performance gain.\nBuild Dinic’s level graph and visualize layers.\nModel job assignment as bipartite flow.\nVerify Max Flow = Min Cut on small examples.\n\nOnce you master flows, you’ll see them hidden in everything that moves , from data to decisions.\n\n\n\n37. Cuts (Stoer-Wagner, Karger, Gomory-Hu)\nWhere flow problems ask “How much can we send?”, cut problems ask “Where does it break?”\nA cut splits a graph into two disjoint sets. The minimum cut is the smallest set of edges whose removal disconnects the graph , the tightest “bottleneck” holding it together.\nThis chapter explores three major algorithms:\n\nStoer-Wagner , deterministic min-cut for undirected graphs- Karger’s Randomized Algorithm , fast, probabilistic- Gomory-Hu Tree , compress all-pairs min-cuts into one tree Cuts reveal hidden structure , clusters, vulnerabilities, boundaries , and form the dual to flows via the Max-Flow Min-Cut Theorem.\n\n\n1. The Min-Cut Problem\nGiven a weighted undirected graph ( G = (V, E) ): Find the minimum total weight of edges whose removal disconnects the graph.\nEquivalent to:\n\nThe smallest sum of edge weights crossing any partition ( \\(S, V \\setminus S\\) ).\n\nFor directed graphs, you use max-flow methods; For undirected graphs, specialized algorithms exist.\n\n\n2. Applications\n\nNetwork reliability , weakest link detection- Clustering , partition graph by minimal interconnection- Circuit design , splitting components- Image segmentation , separating regions- Community detection , sparse connections between groups\n\n\n\n3. Stoer-Wagner Algorithm (Deterministic)\nA clean, deterministic method for global minimum cut in undirected graphs.\n\n\nA. Idea\n\nStart with the full vertex set ( V ).\nRepeatedly run Maximum Adjacency Search:\n\nStart from a vertex - Grow a set by adding the most tightly connected vertex - The last added vertex defines a cut3. Contract the last two added vertices into one.\n\nKeep track of smallest cut seen.\n\nRepeat until one vertex remains.\n\n\nB. Implementation (Adjacency Matrix)\nconst int INF = 1e9;\nint g[MAX][MAX], w[MAX];\nbool added[MAX], exist[MAX];\n\nint stoer_wagner(int n) {\n    int best = INF;\n    vector&lt;int&gt; v(n);\n    iota(v.begin(), v.end(), 0);\n\n    while (n &gt; 1) {\n        fill(w, w + n, 0);\n        fill(added, added + n, false);\n        int prev = 0;\n        for (int i = 0; i &lt; n; i++) {\n            int sel = -1;\n            for (int j = 0; j &lt; n; j++)\n                if (!added[j] && (sel == -1 || w[j] &gt; w[sel])) sel = j;\n            if (i == n - 1) {\n                best = min(best, w[sel]);\n                for (int j = 0; j &lt; n; j++)\n                    g[prev][j] = g[j][prev] += g[sel][j];\n                v.erase(v.begin() + sel);\n                n--;\n                break;\n            }\n            added[sel] = true;\n            for (int j = 0; j &lt; n; j++) w[j] += g[sel][j];\n            prev = sel;\n        }\n    }\n    return best;\n}\nComplexity: (O\\(V^3\\)), or (O\\(VE + V^2 \\log V\\)) with heaps Input: weighted undirected graph Output: global min cut value\n\n\nC. Example\nGraph:\n1 -3- 2  \n|     |  \n4     2  \n \\-5-/\nCuts:\n\n{1,2}|{3} → 7- {1,3}|{2} → 5 Min cut = 5\n\n\n\n4. Karger’s Algorithm (Randomized)\nA simple, elegant probabilistic method. Repeatedly contract random edges until two vertices remain; the remaining crossing edges form a cut.\nRun multiple times → high probability of finding min cut.\n\n\nA. Algorithm\n\nWhile ( |V| &gt; 2 ):\n\nChoose random edge ((u, v)) - Contract (u, v) into one node - Remove self-loops2. Return number of edges between remaining nodes\n\n\nRepeat (O\\(n^2 \\log n\\)) times for high confidence.\n\n\nB. Implementation Sketch\nstruct Edge { int u, v; };\nvector&lt;Edge&gt; edges;\nint parent[MAX];\n\nint find(int x) { return parent[x] == x ? x : parent[x] = find(parent[x]); }\nvoid unite(int a, int b) { parent[find(b)] = find(a); }\n\nint karger(int n) {\n    int m = edges.size();\n    iota(parent, parent + n, 0);\n    int vertices = n;\n    while (vertices &gt; 2) {\n        int i = rand() % m;\n        int u = find(edges[i].u), v = find(edges[i].v);\n        if (u == v) continue;\n        unite(u, v);\n        vertices--;\n    }\n    int cuts = 0;\n    for (auto e : edges)\n        if (find(e.u) != find(e.v)) cuts++;\n    return cuts;\n}\nExpected Time: (O\\(n^2\\)) per run Probability of success: (2 / (n(n-1))) per run Run multiple trials and take minimum.\n\n\nC. Use Case\nGreat for large sparse graphs, or when approximate solutions are acceptable. Intuitive: the min cut survives random contractions if chosen carefully enough.\n\n\n5. Gomory-Hu Tree\nA compact way to store all-pairs min-cuts. It compresses (O\\(V^2\\)) flow computations into V-1 cuts.\n\n\nA. Idea\n\nBuild a tree where the min cut between any two vertices = the minimum edge weight on their path in the tree.\n\n\n\nB. Algorithm\n\nPick vertex (s).\nFor each vertex \\(t \\neq s\\),\n\nRun max flow to find min cut between (s, t). - Partition vertices accordingly.3. Connect partitions to form a tree.\n\n\nResult: Gomory-Hu tree (V-1 edges).\nNow any pair’s min cut = smallest edge on path between them.\nComplexity: (O(V)) max flow runs.\n\n\nC. Uses\n\nQuickly answer all-pairs cut queries- Network reliability- Hierarchical clustering\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nType\nRandomized\nGraph\nComplexity\nOutput\n\n\n\n\nStoer-Wagner\nDeterministic\nNo\nUndirected\nO(V³)\nGlobal min cut\n\n\nKarger\nRandomized\nYes\nUndirected\nO(n² log n) (multi-run)\nProbabilistic min cut\n\n\nGomory-Hu\nDeterministic\nNo\nUndirected\nO(V × MaxFlow)\nAll-pairs min cuts\n\n\n\n\n\n7. Relationship to Flows\nBy Max-Flow Min-Cut, min-cut capacity = max-flow value.\nSo you can find:\n\ns-t min cut = via max flow- global min cut = min over all (s, t) pairs Specialized algorithms just make it faster.\n\n\n\nTiny Code\nStoer-Wagner Example:\nprintf(\"Global Min Cut = %d\\n\", stoer_wagner(n));\nKarger Multi-Run:\nint ans = INF;\nfor (int i = 0; i &lt; 100; i++)\n    ans = min(ans, karger(n));\nprintf(\"Approx Min Cut = %d\\n\", ans);\n\n\nWhy It Matters\nCuts show you fragility , the weak seams of connection. While flows tell you how much can pass, cuts reveal where it breaks first.\nThey teach:\n\n“To understand strength, study what happens when you pull things apart.”\n\n\n\nTry It Yourself\n\nImplement Stoer-Wagner and test on small graphs.\nRun Karger 100 times and track success rate.\nBuild a Gomory-Hu tree and answer random pair queries.\nVerify Max-Flow = Min-Cut equivalence on examples.\nUse cuts for community detection in social graphs.\n\nMastering cuts gives you both grip and insight , where systems hold, and where they give way.\n\n\n\n38. Matchings (Hopcroft-Karp, Hungarian, Blossom)\nIn many problems, we need to pair up elements efficiently: students to schools, jobs to workers, tasks to machines.\nThese are matching problems , find sets of edges with no shared endpoints that maximize cardinality or weight.\nDepending on graph type, different algorithms apply:\n\nHopcroft-Karp , fast matching in bipartite graphs- Hungarian Algorithm , optimal weighted assignment- Edmonds’ Blossom Algorithm , general graphs (non-bipartite) Matching is a fundamental combinatorial structure, appearing in scheduling, flow networks, and resource allocation.\n\n\n1. Terminology\n\nMatching: set of edges with no shared vertices- Maximum Matching: matching with largest number of edges- Perfect Matching: covers all vertices (each vertex matched once)- Maximum Weight Matching: matching with largest total edge weight Graph Types:\nBipartite: vertices split into two sets (L, R); edges only between sets- General: arbitrary connections (may contain odd cycles)\n\n\n\n2. Applications\n\nJob assignment- Network flows- Resource allocation- Student-project pairing- Stable marriages (with preferences)- Computer vision (feature correspondence)\n\n\n\n3. Hopcroft-Karp Algorithm (Bipartite Matching)\nA highly efficient algorithm for maximum cardinality matching in bipartite graphs.\nIt uses layered BFS + DFS to find multiple augmenting paths simultaneously.\n\n\nA. Idea\n\nInitialize matching empty.\nWhile augmenting paths exist:\n\nBFS builds layer graph (shortest augmenting paths). - DFS finds all augmenting paths along those layers. Each phase increases matching size significantly.\n\n\n\n\nB. Complexity\n\\[\nO(E \\sqrt{V})\n\\]\nMuch faster than augmenting one path at a time (like Ford-Fulkerson).\n\n\nC. Implementation\nLet pairU[u] = matched vertex in R, or 0 if unmatched pairV[v] = matched vertex in L, or 0 if unmatched\nvector&lt;int&gt; adjL[MAX];\nint pairU[MAX], pairV[MAX], dist[MAX];\nint nL, nR;\n\nbool bfs() {\n    queue&lt;int&gt; q;\n    for (int u = 1; u &lt;= nL; u++) {\n        if (!pairU[u]) dist[u] = 0, q.push(u);\n        else dist[u] = INF;\n    }\n    int found = INF;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        if (dist[u] &lt; found) {\n            for (int v : adjL[u]) {\n                if (!pairV[v]) found = dist[u] + 1;\n                else if (dist[pairV[v]] == INF) {\n                    dist[pairV[v]] = dist[u] + 1;\n                    q.push(pairV[v]);\n                }\n            }\n        }\n    }\n    return found != INF;\n}\n\nbool dfs(int u) {\n    for (int v : adjL[u]) {\n        if (!pairV[v] || (dist[pairV[v]] == dist[u] + 1 && dfs(pairV[v]))) {\n            pairU[u] = v;\n            pairV[v] = u;\n            return true;\n        }\n    }\n    dist[u] = INF;\n    return false;\n}\n\nint hopcroft_karp() {\n    int matching = 0;\n    while (bfs()) {\n        for (int u = 1; u &lt;= nL; u++)\n            if (!pairU[u] && dfs(u)) matching++;\n    }\n    return matching;\n}\n\n\nD. Example\nGraph:\nU = {1,2,3}, V = {a,b}\nEdges: 1–a, 2–a, 3–b\nMatching: {1-a, 3-b} (size 2)\n\n\n4. Hungarian Algorithm (Weighted Bipartite Matching)\nSolves assignment problem , given cost matrix \\(c_{ij}\\), assign each (i) to one (j) minimizing total cost (or maximizing profit).\n\n\nA. Idea\nSubtract minimums row- and column-wise → expose zeros → find minimal zero-cover → adjust matrix → repeat.\nEquivalent to solving min-cost perfect matching on a bipartite graph.\n\n\nB. Complexity\n\\[\nO(V^3)\n\\]\nWorks for dense graphs, moderate sizes.\n\n\nC. Implementation Sketch (Matrix Form)\nint hungarian(const vector&lt;vector&lt;int&gt;&gt;& cost) {\n    int n = cost.size();\n    vector&lt;int&gt; u(n+1), v(n+1), p(n+1), way(n+1);\n    for (int i = 1; i &lt;= n; i++) {\n        p[0] = i; int j0 = 0;\n        vector&lt;int&gt; minv(n+1, INF);\n        vector&lt;char&gt; used(n+1, false);\n        do {\n            used[j0] = true;\n            int i0 = p[j0], delta = INF, j1;\n            for (int j = 1; j &lt;= n; j++) if (!used[j]) {\n                int cur = cost[i0-1][j-1] - u[i0] - v[j];\n                if (cur &lt; minv[j]) minv[j] = cur, way[j] = j0;\n                if (minv[j] &lt; delta) delta = minv[j], j1 = j;\n            }\n            for (int j = 0; j &lt;= n; j++)\n                if (used[j]) u[p[j]] += delta, v[j] -= delta;\n                else minv[j] -= delta;\n            j0 = j1;\n        } while (p[j0]);\n        do { int j1 = way[j0]; p[j0] = p[j1]; j0 = j1; } while (j0);\n    }\n    return -v[0]; // minimal cost\n}\n\n\nD. Example\nCost matrix:\n  a  b  c\n1 3  2  1\n2 2  3  2\n3 3  2  3\nOptimal assignment = 1-c, 2-a, 3-b Cost = 1 + 2 + 2 = 5\n\n\n5. Edmonds’ Blossom Algorithm (General Graphs)\nFor non-bipartite graphs, simple augmenting path logic breaks down (odd cycles). Blossom algorithm handles this via contraction of blossoms (odd cycles).\n\n\nA. Idea\n\nFind augmenting paths- When odd cycle encountered (blossom), shrink it into one vertex- Continue search- Expand blossoms at end\n\n\n\nB. Complexity\n\\[\nO(V^3)\n\\]\nThough complex to implement, it’s the general-purpose solution for matchings.\n\n\nC. Use Cases\n\nNon-bipartite job/task assignments- General pairing problems- Network design\n\n\n\n6. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nGraph Type\nWeighted\nComplexity\nOutput\n\n\n\n\nHopcroft-Karp\nBipartite\nNo\nO(E√V)\nMax cardinality\n\n\nHungarian\nBipartite\nYes\nO(V³)\nMin/Max cost matching\n\n\nBlossom\nGeneral\nYes\nO(V³)\nMax cardinality or weight\n\n\n\n\n\n7. Relation to Flows\nBipartite matching = max flow on network:\n\nLeft → Source edges (capacity 1)- Right → Sink edges (capacity 1)- Between sets → edges (capacity 1) Matching size = flow value\n\n\n\nTiny Code\nHopcroft-Karp Demo:\nnL = 3; nR = 2;\nadjL[1] = {1};\nadjL[2] = {1};\nadjL[3] = {2};\nprintf(\"Max Matching = %d\\n\", hopcroft_karp()); // 2\n\n\nWhy It Matters\nMatchings are the language of pairing and assignment. They express cooperation without overlap , a structure of balance.\nThey reveal a deep duality:\n\n“Every match is a flow, every assignment an optimization.”\n\n\n\nTry It Yourself\n\nBuild a bipartite graph and run Hopcroft-Karp.\nSolve an assignment problem with Hungarian algorithm.\nExplore Blossom’s contraction idea conceptually.\nCompare max-flow vs matching approach.\nUse matching to model scheduling (people ↔︎ tasks).\n\nMatching teaches how to pair without conflict, a lesson both mathematical and universal.\n\n\n\n39. Tree Algorithms (LCA, HLD, Centroid Decomposition)\nTrees are the backbone of many algorithms , they are connected, acyclic, and wonderfully structured.\nBecause of their simplicity, they allow elegant divide-and-conquer, dynamic programming, and query techniques. This section covers three fundamental patterns:\n\nLowest Common Ancestor (LCA) , answer ancestor queries fast- Heavy-Light Decomposition (HLD) , break trees into chains for segment trees / path queries- Centroid Decomposition , recursively split tree by balance for divide-and-conquer Each reveals a different way to reason about trees , by depth, by chains, or by balance.\n\n\n1. Lowest Common Ancestor (LCA)\nGiven a tree, two nodes (u, v). The LCA is the lowest node (farthest from root) that is an ancestor of both.\nApplications:\n\nDistance queries- Path decomposition- RMQ / binary lifting- Tree DP and rerooting\n\n\n\nA. Naive Approach\nClimb ancestors until they meet. But this is (O(n)) per query , too slow for many queries.\n\n\nB. Binary Lifting\nPrecompute ancestors at powers of 2. Then jump up by powers to align depths.\nPreprocessing:\n\nDFS to record depth\nup[v][k] = 2^k-th ancestor of v\n\nAnswering query:\n\nLift deeper node up to same depth\nLift both together while up[u][k] != up[v][k]\nReturn parent\n\nCode:\nconst int LOG = 20;\nvector&lt;int&gt; adj[MAX];\nint up[MAX][LOG], depth[MAX];\n\nvoid dfs(int u, int p) {\n    up[u][0] = p;\n    for (int k = 1; k &lt; LOG; k++)\n        up[u][k] = up[up[u][k-1]][k-1];\n    for (int v : adj[u]) if (v != p) {\n        depth[v] = depth[u] + 1;\n        dfs(v, u);\n    }\n}\n\nint lca(int u, int v) {\n    if (depth[u] &lt; depth[v]) swap(u, v);\n    int diff = depth[u] - depth[v];\n    for (int k = 0; k &lt; LOG; k++)\n        if (diff & (1 &lt;&lt; k)) u = up[u][k];\n    if (u == v) return u;\n    for (int k = LOG-1; k &gt;= 0; k--)\n        if (up[u][k] != up[v][k])\n            u = up[u][k], v = up[v][k];\n    return up[u][0];\n}\nComplexity:\n\nPreprocess: (O\\(n \\log n\\))- Query: (O\\(\\log n\\))\n\n\n\nC. Example\nTree:\n    1\n   / \\\n  2   3\n / \\\n4   5\n\nLCA(4,5) = 2- LCA(4,3) = 1\n\n\n\n2. Heavy-Light Decomposition (HLD)\nWhen you need to query paths (sum, max, min, etc.) on trees efficiently, you can use Heavy-Light Decomposition.\n\n\nA. Idea\nDecompose the tree into chains:\n\nHeavy edge = edge to child with largest subtree- Light edges = others Result: Every path from root to leaf crosses at most (O\\(\\log n\\)) light edges.\n\nSo, a path query can be broken into (O\\(\\log^2 n\\)) segment tree queries.\n\n\nB. Steps\n\nDFS to compute subtree sizes and identify heavy child\nDecompose into chains\nAssign IDs for segment tree\nUse Segment Tree / BIT on linearized array\n\nKey functions:\n\ndfs_sz(u) → compute subtree sizes- decompose(u, head) → assign chain heads Code (core):\n\nint parent[MAX], depth[MAX], heavy[MAX], head[MAX], pos[MAX];\nint cur_pos = 0;\n\nint dfs_sz(int u) {\n    int size = 1, max_sz = 0;\n    for (int v : adj[u]) if (v != parent[u]) {\n        parent[v] = u;\n        depth[v] = depth[u] + 1;\n        int sz = dfs_sz(v);\n        if (sz &gt; max_sz) max_sz = sz, heavy[u] = v;\n        size += sz;\n    }\n    return size;\n}\n\nvoid decompose(int u, int h) {\n    head[u] = h;\n    pos[u] = cur_pos++;\n    if (heavy[u] != -1) decompose(heavy[u], h);\n    for (int v : adj[u])\n        if (v != parent[u] && v != heavy[u])\n            decompose(v, v);\n}\nQuery path(u, v):\n\nWhile heads differ, move up chain by chain- Query segment tree in [pos[head[u]], pos[u]]- When in same chain, query segment [pos[v], pos[u]] Complexity:\nBuild: (O(n))- Query/Update: (O\\(\\log^2 n\\))\n\n\n\nC. Use Cases\n\nPath sums- Path maximums- Edge updates- Subtree queries\n\n\n\n3. Centroid Decomposition\nCentroid = node that splits tree into subtrees ≤ n/2 each. By removing centroids recursively, we form a centroid tree.\nUsed for divide-and-conquer on trees.\n\n\nA. Steps\n\nFind centroid\n\nDFS to compute subtree sizes - Choose node where largest subtree ≤ n/22. Decompose:\nRemove centroid - Recurse on subtrees Code (core):\n\n\nint subtree[MAX];\nbool removed[MAX];\nvector&lt;int&gt; adj[MAX];\n\nint dfs_size(int u, int p) {\n    subtree[u] = 1;\n    for (int v : adj[u])\n        if (v != p && !removed[v])\n            subtree[u] += dfs_size(v, u);\n    return subtree[u];\n}\n\nint find_centroid(int u, int p, int n) {\n    for (int v : adj[u])\n        if (v != p && !removed[v])\n            if (subtree[v] &gt; n / 2)\n                return find_centroid(v, u, n);\n    return u;\n}\n\nvoid decompose(int u, int p) {\n    int n = dfs_size(u, -1);\n    int c = find_centroid(u, -1, n);\n    removed[c] = true;\n    // process centroid here\n    for (int v : adj[c])\n        if (!removed[v])\n            decompose(v, c);\n}\nComplexity: (O\\(n \\log n\\))\n\n\nB. Applications\n\nDistance queries (decompose + store distance to centroid)- Tree problems solvable by divide-and-conquer- Dynamic queries (add/remove nodes)\n\n\n\n4. Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nQuery\nPreprocess\nComplexity\nNotes\n\n\n\n\nLCA\nAncestor query\n(O\\(\\log n\\))\n(O\\(n \\log n\\))\nFast ancestor lookup\n\n\n\nHLD\nPath queries\n(O\\(\\log^2 n\\))\n(O(n))\nSegment tree-friendly\n\n\n\nCentroid Decomposition\nDivide tree\n-\n(O\\(n \\log n\\))\nBalanced splits\n\n\n\n\n\n\n5. Interconnections\n\nHLD often uses LCA internally.- Centroid decomposition may use distance to ancestor (via LCA).- All exploit tree structure to achieve sublinear queries.\n\n\n\nTiny Code\nLCA(4,5):\ndfs(1,1);\nprintf(\"%d\\n\", lca(4,5)); // 2\nHLD Path Sum: Build segment tree on pos[u] order, query along chains.\nCentroid: decompose(1, -1);\n\n\nWhy It Matters\nTree algorithms show how structure unlocks efficiency. They transform naive traversals into fast, layered, or recursive solutions.\nTo master data structures, you must learn to “climb” and “cut” trees intelligently.\n\n“Every rooted path hides a logarithm.”\n\n\n\nTry It Yourself\n\nImplement binary lifting LCA and test queries.\nAdd segment tree over HLD and run path sums.\nDecompose tree by centroid and count nodes at distance k.\nCombine LCA + HLD for path min/max.\nDraw centroid tree of a simple graph.\n\nMaster these, and trees will stop being “just graphs” , they’ll become tools.\n\n\n\n40. Advanced Graph Algorithms and Tricks\nBy now you’ve seen the big families , traversals, shortest paths, flows, matchings, cuts, and trees. But real-world graphs often bring extra constraints: dynamic updates, multiple sources, layered structures, or special properties (planar, DAG, sparse).\nThis section gathers powerful advanced graph techniques , tricks and patterns that appear across problems once you’ve mastered the basics.\nWe’ll explore:\n\nTopological Sorting & DAG DP- Strongly Connected Components (Condensation Graphs)- Articulation Points & Bridges (2-Edge/Vertex Connectivity)- Eulerian & Hamiltonian Paths- Graph Coloring & Bipartiteness Tests- Cycle Detection & Directed Acyclic Reasoning- Small-to-Large Merging, DSU on Tree, Mo’s Algorithm on Trees- Bitmask DP on Graphs- Dynamic Graphs (Incremental/Decremental BFS/DFS)- Special Graphs (Planar, Sparse, Dense) These aren’t just algorithms , they’re patterns that let you attack harder graph problems with insight.\n\n\n1. Topological Sorting & DAG DP\nIn a DAG (Directed Acyclic Graph), edges always point forward. This makes it possible to order vertices linearly so all edges go from left to right , a topological order.\nUse cases:\n\nTask scheduling- Dependency resolution- DP on DAG (longest/shortest path, counting paths) Algorithm (Kahn’s):\n\nvector&lt;int&gt; topo_sort(int n) {\n    vector&lt;int&gt; indeg(n+1), res;\n    queue&lt;int&gt; q;\n    for (int u = 1; u &lt;= n; u++)\n        for (int v : adj[u]) indeg[v]++;\n    for (int u = 1; u &lt;= n; u++)\n        if (!indeg[u]) q.push(u);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        res.push_back(u);\n        for (int v : adj[u])\n            if (--indeg[v] == 0) q.push(v);\n    }\n    return res;\n}\nDAG DP:\nvector&lt;int&gt; dp(n+1, 0);\nfor (int u : topo_order)\n    for (int v : adj[u])\n        dp[v] = max(dp[v], dp[u] + weight(u,v));\nComplexity: O(V + E)\n\n\n2. Strongly Connected Components (Condensation)\nIn directed graphs, vertices may form SCCs (mutually reachable components). Condensing SCCs yields a DAG, often easier to reason about.\nUse:\n\nComponent compression- Meta-graph reasoning- Cycle condensation Tarjan’s Algorithm: DFS with low-link values, single pass.\n\nKosaraju’s Algorithm: Two passes , DFS on graph and reversed graph.\nComplexity: O(V + E)\nOnce SCCs are built, you can run DP or topological sort on the condensed DAG.\n\n\n3. Articulation Points & Bridges\nFind critical vertices/edges whose removal disconnects the graph.\n\nArticulation point: vertex whose removal increases component count- Bridge: edge whose removal increases component count Algorithm: Tarjan’s DFS Track discovery time tin[u] and lowest reachable ancestor low[u].\n\nvoid dfs(int u, int p) {\n    tin[u] = low[u] = ++timer;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        if (!tin[v]) {\n            dfs(v, u);\n            low[u] = min(low[u], low[v]);\n            if (low[v] &gt; tin[u]) bridge(u, v);\n            if (low[v] &gt;= tin[u] && p != -1) cut_vertex(u);\n        } else low[u] = min(low[u], tin[v]);\n    }\n}\nApplications:\n\nNetwork reliability- Biconnected components- 2-edge/vertex connectivity tests\n\n\n\n4. Eulerian & Hamiltonian Paths\n\nEulerian Path: visits every edge exactly once\n\nExists if graph is connected and 0 or 2 vertices have odd degree- Hamiltonian Path: visits every vertex exactly once (NP-hard) Euler Tour Construction: Hierholzer’s algorithm (O(E))\n\n\nApplications:\n\nRoute reconstruction (e.g., word chains)- Postman problems\n\n\n\n5. Graph Coloring & Bipartiteness\nBipartite Check: DFS/ BFS alternating colors Fails if odd cycle found.\nbool bipartite(int n) {\n    vector&lt;int&gt; color(n+1, -1);\n    for (int i = 1; i &lt;= n; i++) if (color[i] == -1) {\n        queue&lt;int&gt; q; q.push(i); color[i] = 0;\n        while (!q.empty()) {\n            int u = q.front(); q.pop();\n            for (int v : adj[u]) {\n                if (color[v] == -1)\n                    color[v] = color[u] ^ 1, q.push(v);\n                else if (color[v] == color[u])\n                    return false;\n            }\n        }\n    }\n    return true;\n}\nApplications:\n\n2-SAT reduction- Planar graph coloring- Conflict-free assignment\n\n\n\n6. Cycle Detection\n\nDFS + recursion stack for directed graphs- Union-Find for undirected graphs Used to test acyclicity, detect back edges, or find cycles for rollback or consistency checks.\n\n\n\n7. DSU on Tree (Small-to-Large Merging)\nFor queries like “count distinct colors in subtree,” merge results from smaller to larger subtrees to maintain O(n log n).\nPattern:\n\nDFS through children\nKeep large child’s data structure\nMerge small child’s data in\n\nApplications:\n\nOffline subtree queries- Heavy subproblem caching\n\n\n\n8. Mo’s Algorithm on Trees\nOffline algorithm to answer path queries efficiently:\n\nConvert path queries to ranges via Euler Tour- Use Mo’s ordering to process in O((N + Q)√N) Useful when online updates aren’t required.\n\n\n\n9. Bitmask DP on Graphs\nFor small graphs (n ≤ 20): State = subset of vertices e.g., Traveling Salesman Problem (TSP)\ndp[mask][u] = min cost to visit mask, end at u\nTransition:\ndp[mask | (1&lt;&lt;v)][v] = min(dp[mask][u] + cost[u][v])\nComplexity: O(n² 2ⁿ)\n\n\n10. Dynamic Graphs\nGraphs that change:\n\nIncremental BFS: maintain distances as edges added- Decremental connectivity: union-find rollback or dynamic trees Used in online queries, evolving networks, or real-time systems.\n\n\n\n11. Special Graph Classes\n\nPlanar graphs: ≤ 3V-6E; use face counting- Sparse graphs: adjacency lists best- Dense graphs: adjacency matrix / bitset Optimizations often hinge on density.\n\n\n\nTiny Code\nTopological Order:\nauto order = topo_sort(n);\nfor (int u : order) printf(\"%d \", u);\nBridge Check: if (low[v] &gt; tin[u]) edge is a bridge.\nEuler Path Check: Count odd-degree nodes == 0 or 2.\n\n\nWhy It Matters\nThese advanced techniques complete your toolkit. They’re not isolated , they combine to solve real-world puzzles: dependency graphs, robust networks, optimized paths, compressed states.\nThey teach a mindset:\n\n“Graphs are not obstacles , they’re shapes of possibility.”\n\n\n\nTry It Yourself\n\nImplement topological sort and DAG DP.\nFind SCCs and build condensation graph.\nDetect articulation points and bridges.\nCheck Euler path conditions on random graphs.\nTry DSU on tree for subtree statistics.\nSolve TSP via bitmask DP for n ≤ 15.\n\nOnce you can mix and match these tools, you’re no longer just navigating graphs , you’re shaping them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-5.-dynamic-programming",
    "href": "books/en-us/book.html#chapter-5.-dynamic-programming",
    "title": "The Book",
    "section": "Chapter 5. Dynamic Programming",
    "text": "Chapter 5. Dynamic Programming\n\n41. DP Basics and State Transitions\nDynamic Programming (DP) is one of the most powerful ideas in algorithm design. It’s about breaking a big problem into smaller overlapping subproblems, solving each once, and reusing their answers.\nWhen brute force explodes exponentially, DP brings it back under control. This section introduces the mindset, the mechanics, and the math behind DP.\n\n1. The Core Idea\nMany problems have two key properties:\n\nOverlapping subproblems: The same smaller computations repeat many times.\nOptimal substructure: The optimal solution to a problem can be built from optimal solutions to its subproblems.\n\nDP solves each subproblem once, stores the result, and reuses it. This saves exponential time , often reducing ( O\\(2^n\\) ) to ( O\\(n^2\\) ) or ( O(n) ).\n\n\n2. The Recipe\nWhen approaching a DP problem, follow this pattern:\n\nDefine the state. Decide what subproblems you’ll solve. Example: dp[i] = best answer for first i elements.\nWrite the recurrence. Express each state in terms of smaller ones. Example: dp[i] = dp[i-1] + cost(i)\nSet the base cases. Where does the recursion start? Example: dp[0] = 0\nDecide the order. Bottom-up (iterative) or top-down (recursive with memoization).\nReturn the final answer. Often dp[n] or max(dp[i]).\n\n\n\n3. Example: Fibonacci Numbers\nLet’s begin with a classic , the nth Fibonacci number ( F(n) = F(n-1) + F(n-2) ).\nRecursive (slow):\nint fib(int n) {\n    if (n &lt;= 1) return n;\n    return fib(n - 1) + fib(n - 2);\n}\nThis recomputes the same values over and over , exponential time.\nTop-Down DP (Memoization):\nint dp[MAX];\nint fib(int n) {\n    if (n &lt;= 1) return n;\n    if (dp[n] != -1) return dp[n];\n    return dp[n] = fib(n-1) + fib(n-2);\n}\nBottom-Up DP (Tabulation):\nint fib(int n) {\n    int dp[n+1];\n    dp[0] = 0; dp[1] = 1;\n    for (int i = 2; i &lt;= n; i++)\n        dp[i] = dp[i-1] + dp[i-2];\n    return dp[n];\n}\nSpace Optimized:\nint fib(int n) {\n    int a = 0, b = 1, c;\n    for (int i = 2; i &lt;= n; i++) {\n        c = a + b;\n        a = b;\n        b = c;\n    }\n    return b;\n}\n\n\n4. States, Transitions, and Dependencies\nA DP table is a map from states to answers. Each state depends on others via a transition function.\nThink of it like a graph , each edge represents a recurrence relation.\nExample:\n\nState: dp[i] = number of ways to reach step i- Transition: dp[i] = dp[i-1] + dp[i-2] (like stairs)- Base: dp[0] = 1\n\n\n\n5. Common DP Patterns\n\n1D Linear DP\n\nProblems like Fibonacci, climbing stairs, LIS.\n\n2D DP\n\nGrids, sequences, or combinations (LCS, knapsack).\n\nBitmask DP\n\nSubsets, TSP, combinatorial optimization.\n\nDP on Trees\n\nSubtree computations (sum, diameter).\n\nDigit DP\n\nCounting numbers with properties in a range.\n\nSegment DP\n\nMatrix chain multiplication, interval merges.\n\n\n\n\n6. Top-Down vs Bottom-Up\n\n\n\n\n\n\n\n\n\nApproach\nMethod\nPros\nCons\n\n\n\n\nTop-Down\nRecursion + Memoization\nEasy to write, intuitive\nStack overhead, needs memo\n\n\nBottom-Up\nIteration\nFast, space-optimizable\nHarder to derive order\n\n\n\nWhen dependencies are simple and acyclic, bottom-up shines. When they’re complex, top-down is easier.\n\n\n7. Example 2: Climbing Stairs\nYou can climb 1 or 2 steps at a time. How many distinct ways to reach step ( n )?\nState: dp[i] = ways to reach step i Transition: dp[i] = dp[i-1] + dp[i-2] Base: dp[0] = 1, dp[1] = 1\nCode:\nint climb(int n) {\n    int dp[n+1];\n    dp[0] = dp[1] = 1;\n    for (int i = 2; i &lt;= n; i++)\n        dp[i] = dp[i-1] + dp[i-2];\n    return dp[n];\n}\n\n\n8. Debugging DP\nTo debug DP:\n\nPrint intermediate states.- Visualize table (especially 2D).- Check base cases.- Trace one small example by hand.\n\n\n\n9. Complexity\nMost DP algorithms are linear or quadratic in number of states:\n\nTime = (#states) × (work per transition)- Space = (#states) Example: Fibonacci: ( O(n) ) time, ( O(1) ) space Knapsack: ( O\\(n \\times W\\) ) LCS: ( O\\(n \\times m\\) )\n\n\n\nTiny Code\nFibonacci (tabulated):\nint dp[100];\ndp[0] = 0; dp[1] = 1;\nfor (int i = 2; i &lt;= n; i++)\n    dp[i] = dp[i-1] + dp[i-2];\nprintf(\"%d\", dp[n]);\n\n\nWhy It Matters\nDP is the art of remembering. It transforms recursion into iteration, chaos into order.\nFrom optimization to counting, from paths to sequences , once you see substructure, DP becomes your hammer.\n\n“Every repetition hides a recurrence.”\n\n\n\nTry It Yourself\n\nWrite top-down and bottom-up Fibonacci.\nCount ways to climb stairs with steps {1,2,3}.\nCompute number of paths in an n×m grid.\nTry to spot state, recurrence, base in each problem.\nDraw dependency graphs to visualize transitions.\n\nDP isn’t a formula , it’s a mindset: break problems into parts, remember the past, and build from it.\n\n\n\n42. Classic Problems (Knapsack, Subset Sum, Coin Change)\nNow that you know what dynamic programming is, let’s dive into the classic trio , problems that every programmer meets early on:\n\nKnapsack (maximize value under weight constraint)- Subset Sum (can we form a given sum?)- Coin Change (how many ways or fewest coins to reach a total) These are the training grounds of DP: each shows how to define states, transitions, and base cases clearly.\n\n\n1. 0/1 Knapsack Problem\nProblem: You have n items, each with weight w[i] and value v[i]. A knapsack with capacity W. Pick items (each at most once) to maximize total value, without exceeding weight.\n\n\nA. State\ndp[i][w] = max value using first i items with capacity w\n\n\nB. Recurrence\nFor item i:\n\nIf we don’t take it: dp[i-1][w]- If we take it (if w[i] ≤ w): dp[i-1][w - w[i]] + v[i] So, \\[\ndp[i][w] = \\max(dp[i-1][w], dp[i-1][w - w[i]] + v[i])\n\\]\n\n\n\nC. Base Case\ndp[0][w] = 0 for all w (no items = no value)\n\n\nD. Implementation\nint knapsack(int n, int W, int w[], int v[]) {\n    int dp[n+1][W+1];\n    for (int i = 0; i &lt;= n; i++) {\n        for (int j = 0; j &lt;= W; j++) {\n            if (i == 0 || j == 0) dp[i][j] = 0;\n            else if (w[i-1] &lt;= j)\n                dp[i][j] = max(dp[i-1][j], dp[i-1][j - w[i-1]] + v[i-1]);\n            else\n                dp[i][j] = dp[i-1][j];\n        }\n    }\n    return dp[n][W];\n}\nComplexity: Time: (O(nW)) Space: (O(nW)) (can be optimized to 1D (O(W)))\n\n\nE. Space Optimization (1D DP)\nint dp[W+1] = {0};\nfor (int i = 0; i &lt; n; i++)\n    for (int w = W; w &gt;= weight[i]; w--)\n        dp[w] = max(dp[w], dp[w - weight[i]] + value[i]);\n\n\nF. Example\nItems:\nw = [2, 3, 4, 5]\nv = [3, 4, 5, 6]\nW = 5\nBest: take items 1 + 2 → value 7\n\n\n2. Subset Sum\nProblem: Given a set S of integers, can we pick some to sum to target?\n\n\nA. State\ndp[i][sum] = true if we can form sum sum using first i elements.\n\n\nB. Recurrence\n\nDon’t take: dp[i-1][sum]- Take (if a[i] ≤ sum): dp[i-1][sum - a[i]] So, \\[\ndp[i][sum] = dp[i-1][sum] ; || ; dp[i-1][sum - a[i]]\n\\]\n\n\n\nC. Base Case\ndp[0][0] = true (sum 0 possible with no elements) dp[0][sum] = false for sum &gt; 0\n\n\nD. Implementation\nbool subset_sum(int a[], int n, int target) {\n    bool dp[n+1][target+1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = true;\n    for (int j = 1; j &lt;= target; j++) dp[0][j] = false;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= target; j++) {\n            if (a[i-1] &gt; j) dp[i][j] = dp[i-1][j];\n            else dp[i][j] = dp[i-1][j] || dp[i-1][j - a[i-1]];\n        }\n    }\n    return dp[n][target];\n}\nComplexity: Time: (O\\(n \\cdot target\\))\n\n\nE. Example\nS = [3, 34, 4, 12, 5, 2], target = 9 Yes → 4 + 5\n\n\n3. Coin Change\nTwo variants:\n\n\n(a) Count Ways (Unbounded Coins)\n“How many ways to make total T with coins c[]?”\nOrder doesn’t matter.\nState: dp[i][t] = ways using first i coins for total t\nRecurrence:\n\nSkip coin: dp[i-1][t]- Take coin (unlimited): dp[i][t - c[i]] \\[\ndp[i][t] = dp[i-1][t] + dp[i][t - c[i]]\n\\]\n\nBase: dp[0][0] = 1\n1D Simplified:\nint dp[T+1] = {0};\ndp[0] = 1;\nfor (int coin : coins)\n    for (int t = coin; t &lt;= T; t++)\n        dp[t] += dp[t - coin];\n\n\n(b) Min Coins (Fewest Coins to Reach Total)\nState: dp[t] = min coins to reach t\nRecurrence: \\[\ndp[t] = \\min_{c_i \\le t}(dp[t - c_i] + 1)\n\\]\nBase: dp[0] = 0, rest = INF\nint dp[T+1];\nfill(dp, dp+T+1, INF);\ndp[0] = 0;\nfor (int t = 1; t &lt;= T; t++)\n    for (int c : coins)\n        if (t &gt;= c) dp[t] = min(dp[t], dp[t - c] + 1);\n\n\nExample\nCoins = [1,2,5], Total = 5\n\nWays: 4 (5; 2+2+1; 2+1+1+1; 1+1+1+1+1)- Min Coins: 1 (5)\n\n\n\n4. Summary\n\n\n\n\n\n\n\n\n\n\nProblem\nType\nState\nTransition\nComplexity\n\n\n\n\n0/1 Knapsack\nMax value\ndp[i][w]\nmax(take, skip)\nO(nW)\n\n\nSubset Sum\nFeasibility\ndp[i][sum]\nOR of include/exclude\nO(n * sum)\n\n\nCoin Change (ways)\nCounting\ndp[t]\ndp[t] + dp[t - coin]\nO(nT)\n\n\nCoin Change (min)\nOptimization\ndp[t]\nmin(dp[t - coin] + 1)\nO(nT)\n\n\n\n\n\nTiny Code\nMin Coin Change (1D):\nint dp[T+1];\nfill(dp, dp+T+1, INF);\ndp[0] = 0;\nfor (int c : coins)\n    for (int t = c; t &lt;= T; t++)\n        dp[t] = min(dp[t], dp[t - c] + 1);\nprintf(\"%d\\n\", dp[T]);\n\n\nWhy It Matters\nThese three are archetypes:\n\nKnapsack: optimize under constraint- Subset Sum: choose feasibility- Coin Change: count or minimize Once you master them, you can spot their patterns in harder problems , from resource allocation to pathfinding.\n\n\n“Every constraint hides a choice; every choice hides a state.”\n\n\n\nTry It Yourself\n\nImplement 0/1 Knapsack (2D and 1D).\nSolve Subset Sum for target 30 with random list.\nCount coin combinations for amount 10.\nCompare “min coins” vs “ways to form.”\nWrite down state-transition diagram for each.\n\nThese three form your DP foundation , the grammar for building more complex algorithms.\n\n\n\n43. Sequence Problems (LIS, LCS, Edit Distance)\nSequence problems form the heart of dynamic programming. They appear in strings, arrays, genomes, text comparison, and version control. Their power comes from comparing prefixes , building large answers from aligned smaller ones.\nThis section explores three cornerstones:\n\nLIS (Longest Increasing Subsequence)- LCS (Longest Common Subsequence)- Edit Distance (Levenshtein Distance) Each teaches a new way to think about subproblems, transitions, and structure.\n\n\n1. Longest Increasing Subsequence (LIS)\nProblem: Given an array, find the length of the longest subsequence that is strictly increasing.\nA subsequence isn’t necessarily contiguous , you can skip elements.\nExample: [10, 9, 2, 5, 3, 7, 101, 18] → LIS is [2, 3, 7, 18] → length 4\n\n\nA. State\ndp[i] = length of LIS ending at index i\n\n\nB. Recurrence\n\\[\ndp[i] = 1 + \\max_{j &lt; i \\land a[j] &lt; a[i]} dp[j]\n\\]\nIf no smaller a[j], then dp[i] = 1.\n\n\nC. Base\ndp[i] = 1 for all i (each element alone is an LIS)\n\n\nD. Implementation\nint lis(int a[], int n) {\n    int dp[n], best = 0;\n    for (int i = 0; i &lt; n; i++) {\n        dp[i] = 1;\n        for (int j = 0; j &lt; i; j++)\n            if (a[j] &lt; a[i])\n                dp[i] = max(dp[i], dp[j] + 1);\n        best = max(best, dp[i]);\n    }\n    return best;\n}\nComplexity: (O\\(n^2\\))\n\n\nE. Binary Search Optimization\nUse a tail array:\n\ntail[len] = min possible ending value of LIS of length len For each x:\nReplace tail[idx] via lower_bound\n\nint lis_fast(vector&lt;int&gt;& a) {\n    vector&lt;int&gt; tail;\n    for (int x : a) {\n        auto it = lower_bound(tail.begin(), tail.end(), x);\n        if (it == tail.end()) tail.push_back(x);\n        else *it = x;\n    }\n    return tail.size();\n}\nComplexity: (O\\(n \\log n\\))\n\n\n2. Longest Common Subsequence (LCS)\nProblem: Given two strings, find the longest subsequence present in both.\nExample: s1 = \"ABCBDAB\", s2 = \"BDCABA\" LCS = “BCBA” → length 4\n\n\nA. State\ndp[i][j] = LCS length between s1[0..i-1] and s2[0..j-1]\n\n\nB. Recurrence\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1] + 1, & \\text{if } s_1[i-1] = s_2[j-1], \\\\\n\\max(dp[i-1][j],\\, dp[i][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\nC. Base\ndp[0][*] = dp[*][0] = 0 (empty string)\n\n\nD. Implementation\nint lcs(string a, string b) {\n    int n = a.size(), m = b.size();\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++)\n        for (int j = 0; j &lt;= m; j++)\n            if (i == 0 || j == 0) dp[i][j] = 0;\n            else if (a[i-1] == b[j-1])\n                dp[i][j] = dp[i-1][j-1] + 1;\n            else\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1]);\n    return dp[n][m];\n}\nComplexity: (O(nm))\n\n\nE. Reconstruct LCS\nTrace back from dp[n][m]:\n\nIf chars equal → take it and move diagonally- Else move toward larger neighbor\n\n\n\nF. Example\na = “AGGTAB”, b = “GXTXAYB” LCS = “GTAB” → 4\n\n\n3. Edit Distance (Levenshtein Distance)\nProblem: Minimum operations (insert, delete, replace) to convert string a → b.\nExample: kitten → sitting = 3 (replace k→s, insert i, insert g)\n\n\nA. State\ndp[i][j] = min edits to convert a[0..i-1] → b[0..j-1]\n\n\nB. Recurrence\nIf a[i-1] == b[j-1]: \\[\ndp[i][j] = dp[i-1][j-1]\n\\]\nElse: \\[\ndp[i][j] = 1 + \\min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n\\] (Delete, Insert, Replace)\n\n\nC. Base\n\ndp[0][j] = j (insert all)- dp[i][0] = i (delete all)\n\n\n\nD. Implementation\nint edit_distance(string a, string b) {\n    int n = a.size(), m = b.size();\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++)\n        for (int j = 0; j &lt;= m; j++) {\n            if (i == 0) dp[i][j] = j;\n            else if (j == 0) dp[i][j] = i;\n            else if (a[i-1] == b[j-1])\n                dp[i][j] = dp[i-1][j-1];\n            else\n                dp[i][j] = 1 + min({dp[i-1][j], dp[i][j-1], dp[i-1][j-1]});\n        }\n    return dp[n][m];\n}\nComplexity: (O(nm))\n\n\nE. Example\na = “horse”, b = “ros”\n\nreplace h→r, delete r, delete e → 3\n\n\n\n4. Summary\n\n\n\n\n\n\n\n\n\n\nProblem\nType\nState\nTransition\nComplexity\n\n\n\n\nLIS\nSingle seq\ndp[i]\n1 + max(dp[j])\nO(n²) / O(n log n)\n\n\nLCS\nTwo seqs\ndp[i][j]\nif match +1 else max\nO(nm)\n\n\nEdit Distance\nTwo seqs\ndp[i][j]\nif match 0 else 1 + min\nO(nm)\n\n\n\n\n\n5. Common Insights\n\nLIS builds upward , from smaller sequences.- LCS aligns two sequences , compare prefixes.- Edit Distance quantifies difference , minimal edits. They’re templates for bioinformatics, text diffing, version control, and more.\n\n\n\nTiny Code\nLCS:\nif (a[i-1] == b[j-1])\n    dp[i][j] = dp[i-1][j-1] + 1;\nelse\n    dp[i][j] = max(dp[i-1][j], dp[i][j-1]);\n\n\nWhy It Matters\nSequence DPs teach you how to compare progressions , how structure and similarity evolve over time.\nThey transform vague “compare these” tasks into crisp recurrence relations.\n\n“To align is to understand.”\n\n\n\nTry It Yourself\n\nImplement LIS (O(n²) and O(n log n))\nFind LCS of two given strings\nCompute edit distance between “intention” and “execution”\nModify LCS to print one valid subsequence\nTry to unify LCS and Edit Distance in a single table\n\nMaster these, and you can handle any DP on sequences , the DNA of algorithmic thinking.\n\n\n\n44. Matrix and Chain Problems\nDynamic programming shines when a problem involves choices over intervals , which order, which split, which parenthesis. This chapter explores a class of problems built on chains and matrices, where order matters and substructure is defined by intervals.\nWe’ll study:\n\nMatrix Chain Multiplication (MCM) - optimal parenthesization- Polygon Triangulation - divide shape into minimal-cost triangles- Optimal BST / Merge Patterns - weighted merging decisions These problems teach interval DP, where each state represents a segment ([i, j]).\n\n\n1. Matrix Chain Multiplication (MCM)\nProblem: Given matrices \\(A_1, A_2, ..., A_n\\), find the parenthesization that minimizes total scalar multiplications.\nMatrix \\(A_i\\) has dimensions \\(p[i-1] \\times p[i]\\). We can multiply \\(A_i \\cdot A_{i+1}\\) only if inner dimensions match.\nGoal: Minimize operations: \\[\n\\text{cost}(i, j) = \\min_k \\big(\\text{cost}(i, k) + \\text{cost}(k+1, j) + p[i-1] \\cdot p[k] \\cdot p[j]\\big)\n\\]\n\n\nA. State\ndp[i][j] = min multiplications to compute \\(A_i...A_j\\)\n\n\nB. Base\ndp[i][i] = 0 (single matrix needs no multiplication)\n\n\nC. Recurrence\n\\[\ndp[i][j] = \\min_{i \\le k &lt; j} { dp[i][k] + dp[k+1][j] + p[i-1] \\times p[k] \\times p[j] }\n\\]\n\n\nD. Implementation\nint matrix_chain(int p[], int n) {\n    int dp[n][n];\n    for (int i = 1; i &lt; n; i++) dp[i][i] = 0;\n\n    for (int len = 2; len &lt; n; len++) {\n        for (int i = 1; i + len - 1 &lt; n; i++) {\n            int j = i + len - 1;\n            dp[i][j] = INT_MAX;\n            for (int k = i; k &lt; j; k++)\n                dp[i][j] = min(dp[i][j],\n                    dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]);\n        }\n    }\n    return dp[1][n-1];\n}\nComplexity: (O\\(n^3\\)) time, (O\\(n^2\\)) space\n\n\nE. Example\np = [10, 20, 30, 40, 30] Optimal order: ((A1A2)A3)A4 → cost 30000\n\n\n2. Polygon Triangulation\nGiven a convex polygon with n vertices, connect non-intersecting diagonals to minimize total cost. Cost of a triangle = perimeter or product of side weights.\nThis is the same structure as MCM , divide polygon by diagonals.\n\n\nA. State\ndp[i][j] = min triangulation cost for polygon vertices from i to j.\n\n\nB. Recurrence\n\\[\ndp[i][j] = \\min_{i &lt; k &lt; j} (dp[i][k] + dp[k][j] + cost(i, j, k))\n\\]\nBase: dp[i][i+1] = 0 (fewer than 3 points)\n\n\nC. Implementation\ndouble polygon_triangulation(vector&lt;Point&gt; &p) {\n    int n = p.size();\n    double dp[n][n];\n    for (int i = 0; i &lt; n; i++) for (int j = 0; j &lt; n; j++) dp[i][j] = 0;\n    for (int len = 2; len &lt; n; len++) {\n        for (int i = 0; i + len &lt; n; i++) {\n            int j = i + len;\n            dp[i][j] = 1e18;\n            for (int k = i+1; k &lt; j; k++)\n                dp[i][j] = min(dp[i][j],\n                    dp[i][k] + dp[k][j] + dist(p[i],p[k])+dist(p[k],p[j])+dist(p[j],p[i]));\n        }\n    }\n    return dp[0][n-1];\n}\nComplexity: (O\\(n^3\\))\n\n\n3. Optimal Binary Search Tree (OBST)\nGiven sorted keys \\(k_1 &lt; k_2 &lt; \\dots &lt; k_n\\) with search frequencies ( f[i] ), construct a BST with minimal expected search cost.\nThe more frequently accessed nodes should be nearer the root.\n\n\nA. State\ndp[i][j] = min cost to build BST from keys i..j sum[i][j] = sum of frequencies from i to j (precomputed)\n\n\nB. Recurrence\n\\[\ndp[i][j] = \\min_{k=i}^{j} (dp[i][k-1] + dp[k+1][j] + sum[i][j])\n\\]\nEach root adds one to depth of its subtrees → extra cost = sum[i][j]\n\n\nC. Implementation\nint optimal_bst(int freq[], int n) {\n    int dp[n][n], sum[n][n];\n    for (int i = 0; i &lt; n; i++) {\n        dp[i][i] = freq[i];\n        sum[i][i] = freq[i];\n        for (int j = i+1; j &lt; n; j++)\n            sum[i][j] = sum[i][j-1] + freq[j];\n    }\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 0; i+len-1 &lt; n; i++) {\n            int j = i + len - 1;\n            dp[i][j] = INT_MAX;\n            for (int r = i; r &lt;= j; r++) {\n                int left = (r &gt; i) ? dp[i][r-1] : 0;\n                int right = (r &lt; j) ? dp[r+1][j] : 0;\n                dp[i][j] = min(dp[i][j], left + right + sum[i][j]);\n            }\n        }\n    }\n    return dp[0][n-1];\n}\nComplexity: (O\\(n^3\\))\n\n\n4. Merge Pattern Problems\nMany problems , merging files, joining ropes, Huffman coding , involve repeatedly combining elements with minimal total cost.\nAll follow this template: \\[\ndp[i][j] = \\min_{k} (dp[i][k] + dp[k+1][j] + \\text{merge cost})\n\\]\nSame structure as MCM.\n\n\n5. Key Pattern: Interval DP\nState: dp[i][j] = best answer for subarray [i..j] Transition: Try all splits k between i and j\nTemplate:\nfor (len = 2; len &lt;= n; len++)\n for (i = 0; i + len - 1 &lt; n; i++) {\n    j = i + len - 1;\n    dp[i][j] = INF;\n    for (k = i; k &lt; j; k++)\n       dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j,k));\n }\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\nProblem\nState\nRecurrence\nComplexity\n\n\n\n\nMCM\ndp[i][j]\nmin(dp[i][k]+dp[k+1][j]+p[i-1]p[k]p[j])\nO(n³)\n\n\nPolygon Triangulation\ndp[i][j]\nmin(dp[i][k]+dp[k][j]+cost)\nO(n³)\n\n\nOBST\ndp[i][j]\nmin(dp[i][k-1]+dp[k+1][j]+sum[i][j])\nO(n³)\n\n\nMerge Problems\ndp[i][j]\nmin(dp[i][k]+dp[k+1][j]+merge cost)\nO(n³)\n\n\n\n\n\nTiny Code\nMatrix Chain (Compact):\nfor (len = 2; len &lt; n; len++)\n  for (i = 1; i + len - 1 &lt; n; i++) {\n    j = i + len - 1; dp[i][j] = INF;\n    for (k = i; k &lt; j; k++)\n      dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]);\n  }\n\n\nWhy It Matters\nThese problems are DP in 2D , reasoning over intervals and splits. They train your ability to “cut the problem” at every possible point.\n\n“Between every start and end lies a choice of where to divide.”\n\n\n\nTry It Yourself\n\nImplement MCM and print parenthesization.\nSolve polygon triangulation with edge weights.\nBuild OBST for frequencies [34, 8, 50].\nVisualize DP table diagonally.\nGeneralize to merging k segments at a time.\n\nMaster these, and you’ll see interval DP patterns hiding in parsing, merging, and even AI planning.\n\n\n\n45. Bitmask DP and Traveling Salesman\nSome dynamic programming problems require you to track which items have been used, or which subset of elements is active at a given point. This is where Bitmask DP shines. It encodes subsets as binary masks, allowing you to represent state space efficiently.\nThis technique is a must-know for:\n\nTraveling Salesman Problem (TSP)- Subset covering / visiting problems- Permutations and combinations of sets- Game states and toggles\n\n\n1. The Idea of Bitmask DP\nA bitmask is an integer whose binary representation encodes a subset.\nFor ( n ) elements:\n\nThere are \\(2^n\\) subsets.- A subset is represented by a mask from 0 to (1 &lt;&lt; n) - 1. Example for n = 4:\n\n\n\n\nSubset\nMask (binary)\nMask (decimal)\n\n\n\n\n∅\n0000\n0\n\n\n{0}\n0001\n1\n\n\n{1}\n0010\n2\n\n\n{0,1,3}\n1011\n11\n\n\n\nWe can check membership:\n\nmask & (1 &lt;&lt; i) → whether element i is in subset We can add elements:\nmask | (1 &lt;&lt; i) → add element i We can remove elements:\nmask & ~(1 &lt;&lt; i) → remove element i\n\n\n\n2. Example: Traveling Salesman Problem (TSP)\nProblem: Given n cities and cost matrix cost[i][j], find the minimum cost Hamiltonian cycle visiting all cities exactly once and returning to start.\n\n\nA. State\ndp[mask][i] = minimum cost to reach city i having visited subset mask\n\nmask → set of visited cities- i → current city\n\n\n\nB. Base Case\ndp[1&lt;&lt;0][0] = 0 (start at city 0, only 0 visited)\n\n\nC. Transition\nFor each subset mask and city i in mask, try moving from i to j not in mask:\n\\[\ndp[mask \\cup (1 &lt;&lt; j)][j] = \\min \\big(dp[mask \\cup (1 &lt;&lt; j)][j], dp[mask][i] + cost[i][j]\\big)\n\\]\n\n\nD. Implementation\nint tsp(int n, int cost[20][20]) {\n    int N = 1 &lt;&lt; n;\n    const int INF = 1e9;\n    int dp[N][n];\n    for (int m = 0; m &lt; N; m++)\n        for (int i = 0; i &lt; n; i++)\n            dp[m][i] = INF;\n\n    dp[1][0] = 0; // start at city 0\n\n    for (int mask = 1; mask &lt; N; mask++) {\n        for (int i = 0; i &lt; n; i++) {\n            if (!(mask & (1 &lt;&lt; i))) continue;\n            for (int j = 0; j &lt; n; j++) {\n                if (mask & (1 &lt;&lt; j)) continue;\n                int next = mask | (1 &lt;&lt; j);\n                dp[next][j] = min(dp[next][j], dp[mask][i] + cost[i][j]);\n            }\n        }\n    }\n\n    int ans = INF;\n    for (int i = 1; i &lt; n; i++)\n        ans = min(ans, dp[N-1][i] + cost[i][0]);\n    return ans;\n}\nComplexity:\n\nStates: ( O\\(n \\cdot 2^n\\) )- Transitions: ( O(n) )- Total: ( O\\(n^2 \\cdot 2^n\\) )\n\n\n\nE. Example\nn = 4\ncost = {\n {0, 10, 15, 20},\n {10, 0, 35, 25},\n {15, 35, 0, 30},\n {20, 25, 30, 0}\n}\nOptimal path: 0 → 1 → 3 → 2 → 0 Cost = 80\n\n\n3. Other Common Bitmask DP Patterns\n\nSubset Sum / Partition dp[mask] = true if subset represented by mask satisfies property\nCounting Set Bits __builtin_popcount(mask) gives number of elements in subset.\nIterating Over Submasks\n\nfor (int sub = mask; sub; sub = (sub-1) & mask)\n    // handle subset sub\n\nAssigning Tasks (Assignment Problem)\n\n\nEach mask represents set of workers assigned.- State: dp[mask] = min cost for assigned tasks.\n\nfor (mask) for (task)\n if (!(mask & (1 &lt;&lt; task)))\n   dp[mask | (1 &lt;&lt; task)] = min(dp[mask | (1 &lt;&lt; task)],\n        dp[mask] + cost[__builtin_popcount(mask)][task]);\n\n\n4. Memory Tricks\n\nIf only previous masks needed, use rolling arrays:\n\ndp[next][j] = ...\nswap(dp, next_dp)\n\nCompress dimensions: (O\\(2^n\\)) memory for small n\n\n\n\n5. Summary\n\n\n\n\n\n\n\n\n\nProblem\nState\nTransition\nComplexity\n\n\n\n\nTSP\ndp[mask][i]\nmin(dp[mask][i] + cost[i][j])\nO(n²·2ⁿ)\n\n\nAssignment\ndp[mask]\nadd one new element\nO(n²·2ⁿ)\n\n\nSubset Sum\ndp[mask]\nunion of valid subsets\nO(2ⁿ·n)\n\n\n\n\n\nTiny Code\nCore Transition:\nfor (mask)\n  for (i)\n    if (mask & (1&lt;&lt;i))\n      for (j)\n        if (!(mask & (1&lt;&lt;j)))\n          dp[mask|(1&lt;&lt;j)][j] = min(dp[mask|(1&lt;&lt;j)][j], dp[mask][i] + cost[i][j]);\n\n\nWhy It Matters\nBitmask DP is how you enumerate subsets efficiently. It bridges combinatorics and optimization, solving exponential problems with manageable constants.\n\n“Every subset is a story, and bits are its alphabet.”\n\n\n\nTry It Yourself\n\nSolve TSP with 4 cities (hand-trace the table).\nImplement Assignment Problem using bitmask DP.\nCount subsets with even sum.\nUse bitmask DP to find maximum compatible set of tasks.\nExplore how to optimize memory with bit tricks.\n\nBitmask DP unlocks the world of subset-based reasoning , the foundation of combinatorial optimization.\n\n\n\n46. Digit DP and SOS DP\nIn some problems, you don’t iterate over indices or subsets , you iterate over digits or masks to count or optimize over structured states. Two major flavors stand out:\n\nDigit DP - counting numbers with certain properties (e.g. digit sum, constraints)- SOS DP (Sum Over Subsets) - efficiently computing functions over all subsets These are essential techniques when brute force would require enumerating every number or subset, which quickly becomes impossible.\n\n\n1. Digit DP (Counting with Constraints)\nDigit DP is used to count or sum over all numbers ≤ N that satisfy a condition, such as:\n\nThe sum of digits equals a target.- The number doesn’t contain a forbidden digit.- The number has certain parity or divisibility. Instead of iterating over all numbers (up to 10¹⁸!), we iterate digit-by-digit.\n\n\n\nA. State Design\nTypical DP state:\ndp[pos][sum][tight][leading_zero]\n\npos: current digit index (from most significant to least)- sum: property tracker (e.g. sum of digits, remainder)- tight: whether we’re still restricted by N’s prefix- leading_zero: whether we’ve started placing nonzero digits\n\n\n\nB. Transition\nAt each digit position, we choose a digit d:\nlimit = tight ? (digit at pos in N) : 9\nfor (d = 0; d &lt;= limit; d++) {\n    new_tight = tight && (d == limit)\n    new_sum = sum + d\n    // or new_mod = (mod * 10 + d) % M\n}\nTransition accumulates results across valid choices.\n\n\nC. Base Case\nWhen pos == len(N) (end of digits):\n\nReturn 1 if condition holds (e.g. sum == target), else 0\n\n\n\nD. Example: Count numbers ≤ N with digit sum = S\nlong long dp[20][200][2];\n\nlong long solve(string s, int pos, int sum, bool tight) {\n    if (pos == s.size()) return sum == 0;\n    if (sum &lt; 0) return 0;\n    if (dp[pos][sum][tight] != -1) return dp[pos][sum][tight];\n\n    int limit = tight ? (s[pos] - '0') : 9;\n    long long res = 0;\n    for (int d = 0; d &lt;= limit; d++)\n        res += solve(s, pos+1, sum-d, tight && (d==limit));\n\n    return dp[pos][sum][tight] = res;\n}\nUsage:\nstring N = \"12345\";\nint S = 9;\nmemset(dp, -1, sizeof dp);\ncout &lt;&lt; solve(N, 0, S, 1);\nComplexity: O(number of digits × sum × 2) → typically O(20 × 200 × 2)\n\n\nE. Example Variants\n\nCount numbers divisible by 3 → track remainder: new_rem = (rem*10 + d) % 3\nCount numbers without consecutive equal digits → add last_digit to state.\nCount beautiful numbers (like palindromes, no repeated digits) → track bitmask of used digits.\n\n\n\nF. Summary\n\n\n\n\n\n\n\n\n\nProblem\nState\nTransition\nComplexity\n\n\n\n\nSum of digits = S\ndp[pos][sum][tight]\nsum-d\nO(len·S)\n\n\nDivisible by k\ndp[pos][rem][tight]\n(rem*10+d)%k\nO(len·k)\n\n\nNo repeated digits\ndp[pos][mask][tight]\nmask\nO(len·2¹⁰)\n\n\n\n\n\nTiny Code\nfor (int d = 0; d &lt;= limit; d++)\n    res += solve(pos+1, sum-d, tight && (d==limit));\n\n\n2. SOS DP (Sum Over Subsets)\nWhen dealing with functions on subsets, we sometimes need to compute:\n\\[\nf(S) = \\sum_{T \\subseteq S} g(T)\n\\]\nNaively O(3ⁿ). SOS DP reduces it to O(n·2ⁿ).\n\n\nA. Setup\nLet f[mask] = g[mask] initially. For each bit i:\nfor (mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n    if (mask & (1&lt;&lt;i))\n        f[mask] += f[mask^(1&lt;&lt;i)];\nAfter this, f[mask] = sum of g[sub] for all sub ⊆ mask.\n\n\nB. Example\nGiven array a[mask], compute sum[mask] = sum_{sub ⊆ mask} a[sub]\nint n = 3;\nint N = 1 &lt;&lt; n;\nint f[N], a[N];\n// initialize a[]\nfor (int mask = 0; mask &lt; N; mask++) f[mask] = a[mask];\nfor (int i = 0; i &lt; n; i++)\n  for (int mask = 0; mask &lt; N; mask++)\n    if (mask & (1 &lt;&lt; i))\n        f[mask] += f[mask ^ (1 &lt;&lt; i)];\n\n\nC. Why It Works\nEach iteration adds contributions from subsets differing by one bit. By processing all bits, every subset’s contribution propagates upward.\n\n\nD. Variants\n\nSum over supersets: reverse direction.- Max instead of sum: replace += with max=.- XOR convolution: combine values under XOR subset relation.\n\n\n\nE. Applications\n\nInclusion-exclusion acceleration- Precomputing subset statistics- DP over masks with subset transitions\n\n\n\nF. Complexity\n\n\n\nProblem\nNaive\nSOS DP\n\n\n\n\nSubset sum\nO(3ⁿ)\nO(n·2ⁿ)\n\n\nSuperset sum\nO(3ⁿ)\nO(n·2ⁿ)\n\n\n\n\n\nWhy It Matters\nDigit DP teaches counting under constraints , thinking digit by digit. SOS DP teaches subset propagation , spreading information efficiently.\nTogether, they show how to tame exponential state spaces with structure.\n\n“When the search space explodes, symmetry and structure are your compass.”\n\n\n\nTry It Yourself\n\nCount numbers ≤ 10⁹ whose digit sum = 10.\nCount numbers ≤ 10⁶ without repeated digits.\nCompute f[mask] = sum_{sub⊆mask} a[sub] for n=4.\nUse SOS DP to find how many subsets of bits have even sum.\nModify Digit DP to handle leading zeros explicitly.\n\nMaster these, and you can handle structured exponential problems with elegance and speed.\n\n\n\n47. DP Optimizations (Divide & Conquer, Convex Hull Trick, Knuth)\nDynamic Programming often starts with a simple recurrence, but naïve implementations can be too slow (e.g., ( O\\(n^2\\) ) or worse). When the recurrence has special structure , such as monotonicity or convexity , we can exploit it to reduce time complexity drastically.\nThis chapter introduces three powerful optimization families:\n\nDivide and Conquer DP\nConvex Hull Trick (CHT)\nKnuth Optimization\n\nEach one is based on discovering order or geometry hidden inside transitions.\n\n1. Divide and Conquer Optimization\nIf you have a recurrence like: \\[\ndp[i] = \\min_{k &lt; i} { dp[k] + C(k, i) }\n\\]\nand the optimal k for dp[i] ≤ optimal k for dp[i+1], you can use divide & conquer to compute dp in ( O\\(n \\log n\\) ) or ( O\\(n \\log^2 n\\) ).\nThis property is called monotonicity of argmin.\n\n\nA. Conditions\nLet ( C(k, i) ) be the cost to transition from ( k ) to ( i ). Divide and conquer optimization applies if:\n\\[\nopt(i) \\le opt(i+1)\n\\]\nand ( C ) satisfies quadrangle inequality (or similar convex structure).\n\n\nB. Template\nvoid compute(int l, int r, int optL, int optR) {\n    if (l &gt; r) return;\n    int mid = (l + r) / 2;\n    pair&lt;long long,int&gt; best = {INF, -1};\n    for (int k = optL; k &lt;= min(mid, optR); k++) {\n        long long val = dp_prev[k] + cost(k, mid);\n        if (val &lt; best.first) best = {val, k};\n    }\n    dp[mid] = best.first;\n    int opt = best.second;\n    compute(l, mid-1, optL, opt);\n    compute(mid+1, r, opt, optR);\n}\nYou call it as:\ncompute(1, n, 0, n-1);\n\n\nC. Example: Divide Array into K Segments\nGiven array a[1..n], divide into k parts to minimize \\[\ndp[i][k] = \\min_{j &lt; i} dp[j][k-1] + cost(j+1, i)\n\\] If cost satisfies quadrangle inequality, you can optimize each layer in ( O\\(n \\log n\\) ).\n\n\nD. Complexity\nNaive: ( O\\(n^2\\) ) → Optimized: ( O\\(n \\log n\\) )\n\n\n2. Convex Hull Trick (CHT)\nApplies when DP recurrence is linear in i and k: \\[\ndp[i] = \\min_{k &lt; i} (m_k \\cdot x_i + b_k)\n\\]\nwhere:\n\n\\(m_k\\) is slope (depends on k)- ( b_k = dp[k] + c(k) )- \\(x_i\\) is known (monotonic) You can maintain lines \\(y = m_k x + b_k\\) in a convex hull, and query min efficiently.\n\n\n\nA. Conditions\n\nSlopes \\(m_k\\) are monotonic (either increasing or decreasing)- Query points \\(x_i\\) are sorted If both monotonic, we can use pointer walk in O(1) amortized per query. Otherwise, use Li Chao Tree (O(log n)).\n\n\n\nB. Implementation (Monotonic Slopes)\nstruct Line { long long m, b; };\ndeque&lt;Line&gt; hull;\n\nbool bad(Line l1, Line l2, Line l3) {\n    return (l3.b - l1.b)*(l1.m - l2.m) &lt;= (l2.b - l1.b)*(l1.m - l3.m);\n}\n\nvoid add(long long m, long long b) {\n    Line l = {m, b};\n    while (hull.size() &gt;= 2 && bad(hull[hull.size()-2], hull.back(), l))\n        hull.pop_back();\n    hull.push_back(l);\n}\n\nlong long query(long long x) {\n    while (hull.size() &gt;= 2 && \n          hull[0].m*x + hull[0].b &gt;= hull[1].m*x + hull[1].b)\n        hull.pop_front();\n    return hull.front().m*x + hull.front().b;\n}\n\n\nC. Example: DP for Line-Based Recurrence\n\\[\ndp[i] = a_i^2 + \\min_{j &lt; i} (dp[j] + b_j \\cdot a_i)\n\\] Here \\(m_j = b_j\\), \\(x_i = a_i\\), \\(b_j = dp[j]\\)\n\n\nD. Complexity\n\nNaive: ( O\\(n^2\\) )- CHT: ( O(n) ) or ( O\\(n \\log n\\) )\n\n\n\n3. Knuth Optimization\nUsed in interval DP problems (like Matrix Chain, Merging Stones).\nIf:\n\n\\(dp[i][j] = \\min_{k=i}^{j-1} (dp[i][k] + dp[k+1][j] + w(i,j))\\)\nThe cost \\(w(i,j)\\) satisfies the quadrangle inequality: \\[\nw(a,c) + w(b,d) \\le w(a,d) + w(b,c)\n\\]\nAnd the monotonicity condition: \\[\nopt[i][j-1] \\le opt[i][j] \\le opt[i+1][j]\n\\]\n\nThen you can reduce the search space from ( O(n) ) to ( O(1) ) per cell, making total complexity ( O\\(n^2\\) ) instead of ( O\\(n^3\\) ).\n\n\nA. Implementation\nfor (int len = 2; len &lt;= n; len++) {\n    for (int i = 1; i + len - 1 &lt;= n; i++) {\n        int j = i + len - 1;\n        dp[i][j] = INF;\n        for (int k = opt[i][j-1]; k &lt;= opt[i+1][j]; k++) {\n            long long val = dp[i][k] + dp[k+1][j] + cost(i,j);\n            if (val &lt; dp[i][j]) {\n                dp[i][j] = val;\n                opt[i][j] = k;\n            }\n        }\n    }\n}\n\n\nB. Example\nOptimal Binary Search Tree or Merging Stones (with additive cost). Typical improvement: ( O\\(n^3\\) O\\(n^2\\) )\n\n\n4. Summary\n\n\n\n\n\n\n\n\n\nTechnique\nApplies To\nKey Property\nComplexity\n\n\n\n\nDivide & Conquer DP\n1D transitions\nMonotonic argmin\nO(n log n)\n\n\nConvex Hull Trick\nLinear transitions\nMonotonic slopes\nO(n) / O(n log n)\n\n\nKnuth Optimization\nInterval DP\nQuadrangle + Monotonicity\nO(n²)\n\n\n\n\n\nTiny Code\nDivide & Conquer Template\nvoid compute(int l, int r, int optL, int optR);\nCHT Query\nwhile (size&gt;=2 && f[1](x) &lt; f[0](x)) pop_front();\n\n\nWhy It Matters\nThese optimizations show that DP isn’t just brute force with memory , it’s mathematical reasoning on structure.\nOnce you spot monotonicity or linearity, you can shrink a quadratic solution into near-linear time.\n\n“Optimization is the art of seeing simplicity hiding in structure.”\n\n\n\nTry It Yourself\n\nOptimize Matrix Chain DP using Knuth.\nApply Divide & Conquer on dp[i] = min_{k&lt;i}(dp[k]+(i-k)^2).\nSolve Slope DP with CHT for convex cost functions.\nCompare runtime vs naive DP on random data.\nDerive conditions for opt monotonicity in your custom recurrence.\n\nMaster these techniques, and you’ll turn your DPs from slow prototypes into lightning-fast solutions.\n\n\n\n48. Tree DP and Rerooting\nDynamic Programming on trees is one of the most elegant and powerful patterns in algorithm design. Unlike linear arrays or grids, trees form hierarchical structures, where each node depends on its children or parent. Tree DP teaches you how to aggregate results up and down the tree, handling problems where subtrees interact.\nIn this section, we’ll cover:\n\nBasic Tree DP (rooted trees)\nDP over children (bottom-up aggregation)\nRerooting technique (top-down propagation)\nCommon applications and examples\n\n\n1. Basic Tree DP: The Idea\nWe define dp[u] to represent some property of the subtree rooted at u. Then we combine children’s results to compute dp[u].\nThis bottom-up approach is like postorder traversal.\nExample structure:\nfunction dfs(u, parent):\n    dp[u] = base_value\n    for v in adj[u]:\n        if v == parent: continue\n        dfs(v, u)\n        dp[u] = combine(dp[u], dp[v])\n\n\nExample 1: Size of Subtree\nLet dp[u] = number of nodes in subtree rooted at u\nvoid dfs(int u, int p) {\n    dp[u] = 1;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u] += dp[v];\n    }\n}\nKey idea: Combine children’s sizes to get parent size. Complexity: ( O(n) )\n\n\nExample 2: Height of Tree\nLet dp[u] = height of subtree rooted at u\nvoid dfs(int u, int p) {\n    dp[u] = 0;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u] = max(dp[u], 1 + dp[v]);\n    }\n}\nThis gives you the height when rooted at any node.\n\n\n2. DP Over Children (Bottom-Up Aggregation)\nTree DP is all about merging children.\nFor example, if you want the number of ways to color or number of independent sets, you compute children’s dp and merge results at parent.\n\n\nExample 3: Counting Independent Sets\nIn a tree, an independent set is a set of nodes with no two adjacent.\nState:\n\ndp[u][0] = ways if u is not selected- dp[u][1] = ways if u is selected Recurrence: \\[\ndp[u][0] = \\prod_{v \\in children(u)} (dp[v][0] + dp[v][1])\n\\]\n\n\\[\ndp[u][1] = \\prod_{v \\in children(u)} dp[v][0]\n\\]\nImplementation:\nvoid dfs(int u, int p) {\n    dp[u][0] = dp[u][1] = 1;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u][0] *= (dp[v][0] + dp[v][1]);\n        dp[u][1] *= dp[v][0];\n    }\n}\nFinal answer = dp[root][0] + dp[root][1]\n\n\nExample 4: Maximum Path Sum in Tree\nLet dp[u] = max path sum starting at u and going down To find best path anywhere, store a global max over child pairs.\nint ans = 0;\nint dfs(int u, int p) {\n    int best1 = 0, best2 = 0;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        int val = dfs(v, u) + weight(u, v);\n        if (val &gt; best1) swap(best1, val);\n        if (val &gt; best2) best2 = val;\n    }\n    ans = max(ans, best1 + best2);\n    return best1;\n}\nThis gives tree diameter or max path sum.\n\n\n3. Rerooting Technique\nRerooting DP allows you to compute answers for every node as root, without recomputing from scratch ( O\\(n^2\\) ). It’s also known as DP on trees with re-rooting.\n\n\nIdea\n\nFirst, compute dp_down[u] = answer for subtree when rooted at u.\nThen, propagate info from parent to child (dp_up[u]), so each node gets info from outside its subtree.\nCombine dp_down and dp_up to get dp_all[u].\n\n\n\nExample 5: Sum of Distances from Each Node\nLet’s find ans[u] = sum of distances from u to all nodes.\n\nRoot the tree at 0.\nCompute subtree sizes and total distance from root.\nReroot to adjust distances using parent’s info.\n\nStep 1: Bottom-up:\nvoid dfs1(int u, int p) {\n    sz[u] = 1;\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dfs1(v, u);\n        sz[u] += sz[v];\n        dp[u] += dp[v] + sz[v];\n    }\n}\nStep 2: Top-down:\nvoid dfs2(int u, int p) {\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        dp[v] = dp[u] - sz[v] + (n - sz[v]);\n        dfs2(v, u);\n    }\n}\nAfter dfs2, dp[u] = sum of distances from node u.\nComplexity: ( O(n) )\n\n\n4. General Rerooting Template\n// 1. Postorder: compute dp_down[u] from children\nvoid dfs_down(u, p):\n    dp_down[u] = base\n    for v in adj[u]:\n        if v != p:\n            dfs_down(v, u)\n            dp_down[u] = merge(dp_down[u], dp_down[v])\n\n// 2. Preorder: use parent's dp_up to compute dp_all[u]\nvoid dfs_up(u, p, dp_up_parent):\n    ans[u] = merge(dp_down[u], dp_up_parent)\n    prefix, suffix = prefix products of children\n    for each child v:\n        dp_up_v = merge(prefix[v-1], suffix[v+1], dp_up_parent)\n        dfs_up(v, u, dp_up_v)\nThis template generalizes rerooting to many problems:\n\nMaximum distance from each node- Number of ways to select subtrees- Sum of subtree sizes seen from each root\n\n\n\n5. Summary\n\n\n\nPattern\nDescription\nComplexity\n\n\n\n\nBasic Tree DP\nCombine child subresults\nO(n)\n\n\nDP Over Children\nEach node depends on children\nO(n)\n\n\nRerooting DP\nCompute result for every root\nO(n)\n\n\nMultiple States\nTrack choices (e.g. include/exclude)\nO(n·state)\n\n\n\n\n\nTiny Code\nSubtree Size\nvoid dfs(int u, int p) {\n    dp[u] = 1;\n    for (int v: adj[u]) if (v != p) {\n        dfs(v,u);\n        dp[u] += dp[v];\n    }\n}\nReroot Sum Distances\ndp[v] = dp[u] - sz[v] + (n - sz[v]);\n\n\nWhy It Matters\nTree DP is how we think recursively over structure , each node’s truth emerges from its children. Rerooting expands this idea globally, giving every node its own perspective.\n\n“In the forest of states, each root sees a different world , yet all follow the same law.”\n\n\n\nTry It Yourself\n\nCount number of nodes in each subtree.\nCompute sum of depths from each node.\nFind tree diameter using DP.\nCount number of independent sets modulo 1e9+7.\nImplement rerooting to find max distance from each node.\n\nTree DP turns recursive patterns into universal strategies for hierarchical data.\n\n\n\n49. DP Reconstruction and Traceback\nSo far, we’ve focused on computing optimal values (min cost, max score, count of ways). But in most real problems, we don’t just want the number , we want to know how we got it.\nThat’s where reconstruction comes in: once you’ve filled your DP table, you can trace back the decisions that led to the optimal answer.\nThis chapter shows you how to:\n\nRecord transitions during DP computation\nReconstruct paths, subsets, or sequences\nHandle multiple reconstructions (paths, sets, parent links)\nUnderstand traceback in 1D, 2D, and graph-based DPs\n\n\n1. The Core Idea\nEach DP state comes from a choice. If you store which choice was best, you can walk backward from the final state to rebuild the solution.\nThink of it as:\ndp[i] = best over options\nchoice[i] = argmin or argmax option\nThen:\nreconstruction_path = []\ni = n\nwhile i &gt; 0:\n    reconstruction_path.push(choice[i])\n    i = choice[i].prev\nYou’re not just solving , you’re remembering the path.\n\n\n2. Reconstruction in 1D DP\n\n\nExample: Coin Change (Minimum Coins)\nProblem: Find minimum number of coins to make value n.\nRecurrence: \\[\ndp[x] = 1 + \\min_{c \\in coins, c \\le x} dp[x-c]\n\\]\nTo reconstruct which coins were used:\nint dp[MAXN], prev_coin[MAXN];\ndp[0] = 0;\nfor (int x = 1; x &lt;= n; x++) {\n    dp[x] = INF;\n    for (int c : coins) {\n        if (x &gt;= c && dp[x-c] + 1 &lt; dp[x]) {\n            dp[x] = dp[x-c] + 1;\n            prev_coin[x] = c;\n        }\n    }\n}\nReconstruction:\nvector&lt;int&gt; used;\nint cur = n;\nwhile (cur &gt; 0) {\n    used.push_back(prev_coin[cur]);\n    cur -= prev_coin[cur];\n}\nOutput: coins used in one optimal solution.\n\n\nExample: LIS Reconstruction\nYou know how to find LIS length. Now reconstruct the sequence.\nint dp[n], prev[n];\nint best_end = 0;\nfor (int i = 0; i &lt; n; i++) {\n    dp[i] = 1; prev[i] = -1;\n    for (int j = 0; j &lt; i; j++)\n        if (a[j] &lt; a[i] && dp[j] + 1 &gt; dp[i]) {\n            dp[i] = dp[j] + 1;\n            prev[i] = j;\n        }\n    if (dp[i] &gt; dp[best_end]) best_end = i;\n}\nRebuild LIS:\nvector&lt;int&gt; lis;\nfor (int i = best_end; i != -1; i = prev[i])\n    lis.push_back(a[i]);\nreverse(lis.begin(), lis.end());\n\n\n3. Reconstruction in 2D DP\n\n\nExample: LCS (Longest Common Subsequence)\nWe have dp[i][j] filled using:\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1] + 1, & \\text{if } a[i-1] = b[j-1], \\\\\n\\max(dp[i-1][j], dp[i][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\nTo reconstruct LCS:\nint i = n, j = m;\nstring lcs = \"\";\nwhile (i &gt; 0 && j &gt; 0) {\n    if (a[i-1] == b[j-1]) {\n        lcs.push_back(a[i-1]);\n        i--; j--;\n    }\n    else if (dp[i-1][j] &gt; dp[i][j-1]) i--;\n    else j--;\n}\nreverse(lcs.begin(), lcs.end());\nOutput: one valid LCS string.\n\n\nExample: Edit Distance\nOperations: insert, delete, replace.\nYou can store the operation:\nif (a[i-1] == b[j-1]) op[i][j] = \"match\";\nelse if (dp[i][j] == dp[i-1][j-1] + 1) op[i][j] = \"replace\";\nelse if (dp[i][j] == dp[i-1][j] + 1) op[i][j] = \"delete\";\nelse op[i][j] = \"insert\";\nThen backtrack to list operations:\nwhile (i &gt; 0 || j &gt; 0) {\n    if (op[i][j] == \"match\") i--, j--;\n    else if (op[i][j] == \"replace\") { print(\"Replace\"); i--; j--; }\n    else if (op[i][j] == \"delete\") { print(\"Delete\"); i--; }\n    else { print(\"Insert\"); j--; }\n}\n\n\n4. Reconstruction in Path Problems\nWhen DP tracks shortest paths, you can keep parent pointers.\n\n\nExample: Bellman-Ford Path Reconstruction\nint dist[n], parent[n];\ndist[src] = 0;\nfor (int k = 0; k &lt; n-1; k++)\n  for (auto [u,v,w] : edges)\n    if (dist[u] + w &lt; dist[v]) {\n        dist[v] = dist[u] + w;\n        parent[v] = u;\n    }\n\nvector&lt;int&gt; path;\nfor (int v = dest; v != src; v = parent[v])\n    path.push_back(v);\npath.push_back(src);\nreverse(path.begin(), path.end());\nYou now have the actual shortest path.\n\n\n5. Handling Multiple Solutions\nSometimes multiple optimal paths exist. You can:\n\nStore all predecessors instead of one- Backtrack recursively to enumerate all solutions- Tie-break deterministically (e.g., lexicographically smallest) Example:\n\nif (new_val == dp[i]) parents[i].push_back(j);\nThen recursively generate all possible paths.\n\n\n6. Visualization\nDP reconstruction often looks like following arrows in a grid or graph:\n\nLCS: diagonal (↖), up (↑), left (←)- Shortest path: parent edges- LIS: predecessor chain You’re walking through decisions, not just numbers.\n\n\n\n7. Summary\n\n\n\nType\nState\nReconstruction\n\n\n\n\n1D DP\nprev[i]\nTrace chain\n\n\n2D DP\nop[i][j]\nFollow choices\n\n\nGraph DP\nparent[v]\nFollow edges\n\n\nCounting DP\noptional\nRecover counts / paths\n\n\n\n\n\nTiny Code\nGeneral pattern:\nfor (state)\n  for (choice)\n    if (better) {\n        dp[state] = value;\n        parent[state] = choice;\n    }\nThen:\nwhile (state != base) {\n    path.push_back(parent[state]);\n    state = parent[state];\n}\n\n\nWhy It Matters\nSolving DP gets you the score , reconstructing shows you the story. It’s the difference between knowing the answer and understanding the reasoning.\n\n“Numbers tell you the outcome; pointers tell you the path.”\n\n\n\nTry It Yourself\n\nReconstruct one LIS path.\nPrint all LCSs for small strings.\nShow edit operations to transform “cat” → “cut”.\nTrack subset used in Knapsack to reach exact weight.\nRecover optimal merge order in Matrix Chain DP.\n\nReconstruction turns DP from a static table into a narrative of decisions , a map back through the maze of optimal choices.\n\n\n\n50. Meta-DP and Optimization Templates\nWe’ve now explored many flavors of dynamic programming , on sequences, grids, trees, graphs, subsets, and digits. This final chapter in the DP arc zooms out to the meta-level: how to see DP patterns, generalize them, and turn them into reusable templates.\nIf classical DP is about solving one problem, meta-DP is about recognizing families of problems that share structure. You’ll learn how to build your own DP frameworks, use common templates, and reason from first principles.\n\n1. What Is Meta-DP?\nA Meta-DP is a high-level abstraction of a dynamic programming pattern. It encodes:\n\nState definition pattern- Transition pattern- Optimization structure- Dimensional dependencies By learning these patterns, you can design DPs faster, reuse logic across problems, and spot optimizations early.\n\nThink of Meta-DP as:\n\n“Instead of memorizing 100 DPs, master 10 DP blueprints.”\n\n\n\n2. The Four Building Blocks\nEvery DP has the same core ingredients:\n\nState: what subproblem you’re solving\n\nOften dp[i], dp[i][j], or dp[mask] - Represents smallest unit of progress2. Transition: how to build larger subproblems from smaller ones\nE.g. dp[i] = min(dp[j] + cost(j, i))3. Base Case: known trivial answers\nE.g. dp[0] = 04. Order: how to fill the states\nE.g. increasing i, decreasing i, or topological order Once you can describe a problem in these four, it is a DP.\n\n\n\n\n3. Meta-Templates for Common Structures\nBelow are generalized templates to use and adapt.\n\n\nA. Line DP (1D Sequential)\nShape: linear progression Examples:\n\nFibonacci- Knapsack (1D capacity)- LIS (sequential dependency)\n\nfor (int i = 1; i &lt;= n; i++) {\n    dp[i] = base;\n    for (int j : transitions(i))\n        dp[i] = min(dp[i], dp[j] + cost(j, i));\n}\nVisualization: → → → Each state depends on previous positions.\n\n\nB. Grid DP (2D Spatial)\nShape: grid or matrix Examples:\n\nPaths in a grid- Edit Distance- Counting paths with obstacles\n\nfor (i = 0; i &lt; n; i++)\n  for (j = 0; j &lt; m; j++)\n    dp[i][j] = combine(dp[i-1][j], dp[i][j-1]);\nVisualization: ⬇️ ⬇️ ➡️ Moves from top-left to bottom-right.\n\n\nC. Interval DP\nShape: segments or subarrays Examples:\n\nMatrix Chain Multiplication- Optimal BST- Merging Stones\n\nfor (len = 2; len &lt;= n; len++)\n  for (i = 0; i + len - 1 &lt; n; i++) {\n      j = i + len - 1;\n      dp[i][j] = INF;\n      for (k = i; k &lt; j; k++)\n          dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j));\n  }\nKey Insight: overlapping intervals, split points.\n\n\nD. Subset DP\nShape: subsets of a set Examples:\n\nTraveling Salesman (TSP)- Assignment problem- SOS DP\n\nfor (mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n  for (sub = mask; sub; sub = (sub-1)&mask)\n      dp[mask] = combine(dp[mask], dp[sub]);\nKey Insight: use bitmasks to represent subsets.\n\n\nE. Tree DP\nShape: hierarchical dependencies Examples:\n\nSubtree sizes- Independent sets- Rerooting\n\nvoid dfs(u, p):\n  dp[u] = base\n  for (v in children)\n    if (v != p)\n      dfs(v, u)\n      dp[u] = merge(dp[u], dp[v])\n\n\nF. Graph DP (Topological Order)\nShape: DAG structure Examples:\n\nLongest path in DAG- Counting paths- DAG shortest path\n\nfor (u in topo_order)\n  for (v in adj[u])\n    dp[v] = combine(dp[v], dp[u] + weight(u,v));\nKey: process nodes in topological order.\n\n\nG. Digit DP\nShape: positional digits, constrained transitions Examples:\n\nCount numbers satisfying digit conditions- Divisibility / digit sum problems\n\ndp[pos][sum][tight] = ∑ dp[next_pos][new_sum][new_tight];\n\n\nH. Knuth / Divide & Conquer / Convex Hull Trick\nShape: optimization over monotone or convex transitions Examples:\n\nCost-based splits- Line-based transitions\n\ndp[i] = min_k (dp[k] + cost(k, i))\nKey: structure in opt[i] or slope.\n\n\n4. Recognizing DP Type\nAsk these diagnostic questions:\n\n\n\n\n\n\n\nQuestion\nClue\n\n\n\n\n“Does each step depend on smaller subproblems?”\nDP\n\n\n“Do I split a segment?”\nInterval DP\n\n\n“Do I choose subsets?”\nSubset / Bitmask DP\n\n\n“Do I move along positions?”\nLine DP\n\n\n“Do I merge children?”\nTree DP\n\n\n“Do I process in a DAG?”\nGraph DP\n\n\n“Do I track digits or constraints?”\nDigit DP\n\n\n\n\n\n5. Optimization Layer\nOnce you have a working DP, ask:\n\nCan transitions be reduced (monotonicity)?- Can overlapping cost be cached (prefix sums)?- Can dimensions be compressed (rolling arrays)?- Can you reuse solutions for each segment (Divide & Conquer / Knuth)? This transforms your DP from conceptual to efficient.\n\n\n\n6. Meta-DP: Transformations\n\nCompress dimensions: if only dp[i-1] needed, use 1D array.- Invert loops: bottom-up ↔︎ top-down.- Change base: prefix-sums for range queries.- State lifting: add dimension for new property (like remainder, parity, bitmask). &gt; “When stuck, add a dimension. When slow, remove one.”\n\n\n\n7. Common Template Snippets\nRolling 1D Knapsack:\nfor (c = C; c &gt;= w[i]; c--)\n  dp[c] = max(dp[c], dp[c-w[i]] + val[i]);\nTop-Down Memoization:\nint solve(state):\n  if (visited[state]) return dp[state];\n  dp[state] = combine(solve(next_states));\nIterative DP:\nfor (state in order)\n  dp[state] = merge(prev_states);\n\n\n8. Building Your Own DP Framework\nYou can design a generic DP(state, transition) class:\nstruct DP {\n    vector&lt;long long&gt; dp;\n    function&lt;long long(int,int)&gt; cost;\n    DP(int n, auto cost): dp(n, INF), cost(cost) {}\n    void solve() { for (int i=1; i&lt;n; i++) for (int j=0; j&lt;i; j++) \n         dp[i] = min(dp[i], dp[j] + cost(j, i)); }\n};\nReusable, readable, flexible.\n\n\n9. Summary\n\n\n\nDP Type\nCore State\nShape\nTypical Complexity\n\n\n\n\nLine DP\ndp[i]\nLinear\nO(n²) → O(n)\n\n\nGrid DP\ndp[i][j]\nMatrix\nO(n·m)\n\n\nInterval DP\ndp[i][j]\nTriangular\nO(n³)\n\n\nSubset DP\ndp[mask]\nExponential\nO(n·2ⁿ)\n\n\nTree DP\ndp[u]\nTree\nO(n)\n\n\nDigit DP\ndp[pos][sum]\nRecursive\nO(len·sum)\n\n\nGraph DP\ndp[v]\nDAG\nO(V+E)\n\n\n\n\n\nTiny Code\nfor (state in order)\n  dp[state] = combine(all_prev(state));\n\n\nWhy It Matters\nMeta-DP turns your thinking from case-by-case to pattern-by-pattern. You stop memorizing formulas and start seeing shapes: lines, grids, intervals, trees, masks.\nOnce you can name the shape, you can write the DP.\n\n“DP is not about filling tables. It’s about recognizing structure.”\n\n\n\nTry It Yourself\n\nClassify each classic DP problem into a type.\nWrite one template per pattern (Line, Grid, Tree, etc.).\nCreate a dp_solve(state, transitions) function to generalize logic.\nFor each pattern, practice a small example.\nBuild your own “Little Book of DP Patterns” with code snippets.\n\nThis is your bridge from concrete solutions to algorithmic fluency , the foundation for mastering the next 950 algorithms ahead.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-6.-strings-and-text-algorithms",
    "href": "books/en-us/book.html#chapter-6.-strings-and-text-algorithms",
    "title": "The Book",
    "section": "Chapter 6. Strings and Text Algorithms",
    "text": "Chapter 6. Strings and Text Algorithms\n\n51. Number Theory (GCD, Modular Arithmetic, CRT)\nNumber theory forms the mathematical backbone of many algorithms , from hashing and cryptography to modular combinatorics and primality testing. In algorithmic problem-solving, it’s all about working with integers, divisibility, and modular systems efficiently.\nThis section covers the essential toolkit:\n\nGCD and Extended Euclidean Algorithm- Modular Arithmetic (addition, subtraction, multiplication, inverse)- Modular Exponentiation- Chinese Remainder Theorem (CRT)\n\n\n1. The Greatest Common Divisor (GCD)\nThe GCD of two integers \\(a\\) and \\(b\\), denoted \\(\\gcd(a, b)\\), is the largest integer that divides both. It’s the cornerstone for fraction simplification, Diophantine equations, and modular inverses.\n\n\nA. Euclidean Algorithm\nBased on: \\[\n\\gcd(a, b) = \\gcd(b, a \\bmod b)\n\\]\nint gcd(int a, int b) {\n    return b == 0 ? a : gcd(b, a % b);\n}\nTime complexity: \\(O(\\log \\min(a,b))\\)\n\n\nB. Extended Euclidean Algorithm\nFinds integers ( x, y ) such that: \\[\nax + by = \\gcd(a, b)\n\\]\nThis is critical for finding modular inverses.\nint ext_gcd(int a, int b, int &x, int &y) {\n    if (b == 0) {\n        x = 1; y = 0;\n        return a;\n    }\n    int x1, y1;\n    int g = ext_gcd(b, a % b, x1, y1);\n    x = y1;\n    y = x1 - (a / b) * y1;\n    return g;\n}\n\n\nC. Bezout’s Identity\nIf \\(g = \\gcd(a,b)\\), then \\(ax + by = g\\) has integer solutions. If \\(g = 1\\), \\(x\\) is the modular inverse of \\(a modulo b\\).\n\n\n2. Modular Arithmetic\nA modular system “wraps around” after a certain value ( m ).\nWe write: \\[\na \\equiv b \\pmod{m} \\quad \\text{if } m \\mid (a - b)\n\\]\nIt behaves like ordinary arithmetic, with the rule:\n\n\\((a + b) \\bmod m = ((a \\bmod m) + (b \\bmod m)) \\bmod m\\)\n\\((a \\cdot b) \\bmod m = ((a \\bmod m) \\cdot (b \\bmod m)) \\bmod m\\)\n\\((a - b) \\bmod m = ((a \\bmod m) - (b \\bmod m) + m) \\bmod m\\)\n\n\n\nA. Modular Exponentiation\nCompute \\(a^b \\bmod m\\) efficiently using binary exponentiation.\nlong long modpow(long long a, long long b, long long m) {\n    long long res = 1;\n    a %= m;\n    while (b &gt; 0) {\n        if (b & 1) res = (res * a) % m;\n        a = (a * a) % m;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\nComplexity: ( O\\(\\log b\\) )\n\n\nB. Modular Inverse\nGiven ( a ), find \\(a^{-1}\\) such that: \\[\na \\cdot a^{-1} \\equiv 1 \\pmod{m}\n\\]\nCase 1: If ( m ) is prime, use Fermat’s Little Theorem: \\[\na^{-1} \\equiv a^{m-2} \\pmod{m}\n\\]\nint modinv(int a, int m) {\n    return modpow(a, m-2, m);\n}\nCase 2: If ( a ) and ( m ) are coprime, use Extended GCD:\nint inv(int a, int m) {\n    int x, y;\n    int g = ext_gcd(a, m, x, y);\n    if (g != 1) return -1; // no inverse\n    return (x % m + m) % m;\n}\n\n\nC. Modular Division\nTo divide \\(a / b \\bmod m\\): \\[\na / b \\equiv a \\cdot b^{-1} \\pmod{m}\n\\]\nSo compute the inverse and multiply.\n\n\n3. Chinese Remainder Theorem (CRT)\nThe CRT solves systems of congruences: \\[\nx \\equiv a_1 \\pmod{m_1}\n\\]\n\\[\nx \\equiv a_2 \\pmod{m_2}\n\\] If moduli \\(m_1, m_2, \\dots, m_k\\) are pairwise coprime, there exists a unique solution modulo \\(M = m_1 m_2 \\dots m_k\\).\n\n\nA. 2-Equation Example\nSolve: \\[\nx \\equiv a_1 \\pmod{m_1}, \\quad x \\equiv a_2 \\pmod{m_2}\n\\]\nLet:\n\n\\(M = m_1 m_2\\)- \\(M_1 = M / m_1\\)- \\(M_2 = M / m_2\\) Find inverses \\(inv_1 = M_1^{-1} \\bmod m_1\\), \\(inv_2 = M_2^{-1} \\bmod m_2\\)\n\nThen: \\[\nx = (a_1 \\cdot M_1 \\cdot inv_1 + a_2 \\cdot M_2 \\cdot inv_2) \\bmod M\n\\]\n\n\nB. Implementation\nlong long crt(vector&lt;int&gt; a, vector&lt;int&gt; m) {\n    long long M = 1;\n    for (int mod : m) M *= mod;\n    long long res = 0;\n    for (int i = 0; i &lt; a.size(); i++) {\n        long long Mi = M / m[i];\n        long long inv = modinv(Mi % m[i], m[i]);\n        res = (res + 1LL * a[i] * Mi % M * inv % M) % M;\n    }\n    return (res % M + M) % M;\n}\n\n\nC. Example\nSolve:\nx ≡ 2 (mod 3)\nx ≡ 3 (mod 5)\nx ≡ 2 (mod 7)\nSolution: ( x = 23 ) (mod 105)\nCheck:\n\n( 23 % 3 = 2 )- ( 23 % 5 = 3 )- ( 23 % 7 = 2 )\n\n\n\n4. Tiny Code\nGCD\nint gcd(int a, int b) { return b ? gcd(b, a % b) : a; }\nModular Power\nmodpow(a, b, m)\nModular Inverse\nmodinv(a, m)\nCRT\ncrt(a[], m[])\n\n\n5. Summary\n\n\n\n\n\n\n\n\nConcept\nFormula\nPurpose\n\n\n\n\nGCD\n\\(\\gcd(a,b) = \\gcd(b, a \\bmod b)\\)\nSimplify ratios\n\n\nExtended GCD\n\\(ax + by = \\gcd(a,b)\\)\nFind modular inverse\n\n\nModular Inverse\n\\(a^{-1} \\equiv a^{m-2} \\pmod{m}\\)\nSolve modular equations\n\n\nModular Exp\n\\(a^b \\bmod m\\)\nFast exponentiation\n\n\nCRT\nCombine congruences\nMulti-mod system\n\n\n\n\n\nWhy It Matters\nNumber theory lets algorithms speak the language of integers , turning huge computations into modular games. From hashing to RSA, from combinatorics to cryptography, it’s everywhere.\n\n“When numbers wrap around, math becomes modular , and algorithms become magical.”\n\n\n\nTry It Yourself\n\nCompute gcd(48, 180).\nFind inverse of 7 mod 13.\nSolve \\(x ≡ 1 \\pmod{2}, x ≡ 2 \\pmod{3}, x ≡ 3 \\pmod{5}\\).\nImplement modular division \\(a / b \\bmod m\\).\nUse modpow to compute \\(3^{200} \\bmod 13\\).\n\nThese basics unlock higher algorithms in cryptography, combinatorics, and beyond.\n\n\n\n52. Primality and Factorization (Miller-Rabin, Pollard Rho)\nPrimality and factorization are core to number theory, cryptography, and competitive programming. Many modern systems (RSA, ECC) rely on the hardness of factoring large numbers. Here, we learn how to test if a number is prime and break it into factors efficiently.\nWe’ll cover:\n\nTrial Division\nSieve of Eratosthenes (for precomputation)\nProbabilistic Primality Test (Miller-Rabin)\nInteger Factorization (Pollard Rho)\n\n\n1. Trial Division\nThe simplest way to test primality is by dividing by all integers up to √n.\nbool is_prime(long long n) {\n    if (n &lt; 2) return false;\n    if (n % 2 == 0) return n == 2;\n    for (long long i = 3; i * i &lt;= n; i += 2)\n        if (n % i == 0) return false;\n    return true;\n}\nTime Complexity: ( O\\(\\sqrt{n}\\) ) Good for \\(n \\le 10^6\\), impractical for large ( n ).\n\n\n2. Sieve of Eratosthenes\nFor checking many numbers at once, use a sieve.\nIdea: Mark all multiples of each prime starting from 2.\nvector&lt;bool&gt; sieve(int n) {\n    vector&lt;bool&gt; is_prime(n+1, true);\n    is_prime[0] = is_prime[1] = false;\n    for (int i = 2; i * i &lt;= n; i++)\n        if (is_prime[i])\n            for (int j = i * i; j &lt;= n; j += i)\n                is_prime[j] = false;\n    return is_prime;\n}\nTime Complexity: ( O\\(n \\log \\log n\\) )\nUseful for generating primes up to \\(10^7\\).\n\n\n3. Modular Multiplication\nBefore we do probabilistic tests or factorization, we need safe modular multiplication for large numbers.\nlong long modmul(long long a, long long b, long long m) {\n    __int128 res = (__int128)a * b % m;\n    return (long long)res;\n}\nAvoid overflow for \\(n \\approx 10^{18}\\).\n\n\n4. Miller-Rabin Primality Test\nA probabilistic test that can check if ( n ) is prime or composite in ( O\\(k \\log^3 n\\) ).\nIdea: For a prime ( n ): \\[\na^{n-1} \\equiv 1 \\pmod{n}\n\\] But for composites, most ( a ) fail this.\nWe write \\(n - 1 = 2^s \\cdot d\\), ( d ) odd.\nFor each base ( a ):\n\nCompute \\(x = a^d \\bmod n\\)- If ( x = 1 ) or ( x = n - 1 ), pass- Else, square ( s-1 ) times- If none equal ( n - 1 ), composite\n\nbool miller_rabin(long long n) {\n    if (n &lt; 2) return false;\n    for (long long p : {2,3,5,7,11,13,17,19,23,29,31,37})\n        if (n % p == 0) return n == p;\n    long long d = n - 1, s = 0;\n    while ((d & 1) == 0) d &gt;&gt;= 1, s++;\n    auto modpow = [&](long long a, long long b) {\n        long long r = 1;\n        while (b) {\n            if (b & 1) r = modmul(r, a, n);\n            a = modmul(a, a, n);\n            b &gt;&gt;= 1;\n        }\n        return r;\n    };\n    for (long long a : {2, 325, 9375, 28178, 450775, 9780504, 1795265022}) {\n        if (a % n == 0) continue;\n        long long x = modpow(a, d);\n        if (x == 1 || x == n - 1) continue;\n        bool composite = true;\n        for (int r = 1; r &lt; s; r++) {\n            x = modmul(x, x, n);\n            if (x == n - 1) {\n                composite = false;\n                break;\n            }\n        }\n        if (composite) return false;\n    }\n    return true;\n}\nDeterministic for:\n\n\\(n &lt; 2^{64}\\) with bases above. Complexity: ( O\\(k \\log^3 n\\) )\n\n\n\n5. Pollard Rho Factorization\nEfficient for finding nontrivial factors of large composites. Based on Floyd’s cycle detection (Tortoise and Hare).\nIdea: Define a pseudo-random function: \\[\nf(x) = (x^2 + c) \\bmod n\n\\] Then find \\(\\gcd(|x - y|, n)\\) where \\(x, y\\) move at different speeds.\nlong long pollard_rho(long long n) {\n    if (n % 2 == 0) return 2;\n    auto f = [&](long long x, long long c) {\n        return (modmul(x, x, n) + c) % n;\n    };\n    while (true) {\n        long long x = rand() % (n - 2) + 2;\n        long long y = x;\n        long long c = rand() % (n - 1) + 1;\n        long long d = 1;\n        while (d == 1) {\n            x = f(x, c);\n            y = f(f(y, c), c);\n            d = gcd(abs(x - y), n);\n        }\n        if (d != n) return d;\n    }\n}\nUse:\n\nCheck if ( n ) is prime (Miller-Rabin)\nIf not, find a factor using Pollard Rho\nRecurse on factors\n\nComplexity: ~ ( O\\(n^{1/4}\\) ) average\n\n\n6. Example\nFactorize ( n = 8051 ):\n\nMiller-Rabin → composite\nPollard Rho → factor 83\n( 8051 / 83 = 97 )\nBoth primes ⇒ ( 8051 = 83 × 97 )\n\n\n\n7. Tiny Code\nvoid factor(long long n, vector&lt;long long&gt; &f) {\n    if (n == 1) return;\n    if (miller_rabin(n)) {\n        f.push_back(n);\n        return;\n    }\n    long long d = pollard_rho(n);\n    factor(d, f);\n    factor(n / d, f);\n}\nCall factor(n, f) to get prime factors.\n\n\n8. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nComplexity\nType\n\n\n\n\nTrial Division\nSmall primes\n( O\\(\\sqrt{n}\\) )\nDeterministic\n\n\nSieve\nPrecompute primes\n( O\\(n \\log \\log n\\) )\nDeterministic\n\n\nMiller-Rabin\nPrimality test\n( O\\(k \\log^3 n\\) )\nProbabilistic\n\n\nPollard Rho\nFactorization\n( O\\(n^{1/4}\\) )\nProbabilistic\n\n\n\n\n\nWhy It Matters\nModern security, number theory problems, and many algorithmic puzzles depend on knowing when a number is prime and factoring it quickly when it isn’t. These tools are the entry point to RSA, modular combinatorics, and advanced cryptography.\n\n\nTry It Yourself\n\nCheck if 97 is prime using trial division and Miller-Rabin.\nFactorize 5959 (should yield 59 × 101).\nGenerate all primes ≤ 100 using a sieve.\nWrite a recursive factorizer using Pollard Rho + Miller-Rabin.\nMeasure performance difference between \\(\\sqrt{n}\\) trial and Pollard Rho for \\(n \\approx 10^{12}\\).\n\nThese techniques make huge numbers approachable , one factor at a time.\n\n\n\n53. Combinatorics (Permutations, Combinations, Subsets)\nCombinatorics is the art of counting structures , how many ways can we arrange, select, or group things? In algorithms, it’s everywhere: DP transitions, counting paths, bitmask enumeration, and probabilistic reasoning. Here we’ll build a toolkit for computing factorials, nCr, nPr, and subset counts, both exactly and under a modulus.\n\n1. Factorials and Precomputation\nMost combinatorial formulas rely on factorials: \\[\nn! = 1 \\times 2 \\times 3 \\times \\dots \\times n\n\\]\nWe can precompute them modulo ( m ) (often \\(10^9+7\\)) for efficiency.\nconst int MOD = 1e9 + 7;\nconst int MAXN = 1e6;\nlong long fact[MAXN + 1], invfact[MAXN + 1];\n\nlong long modpow(long long a, long long b) {\n    long long res = 1;\n    while (b &gt; 0) {\n        if (b & 1) res = res * a % MOD;\n        a = a * a % MOD;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\n\nvoid init_factorials() {\n    fact[0] = 1;\n    for (int i = 1; i &lt;= MAXN; i++)\n        fact[i] = fact[i - 1] * i % MOD;\n    invfact[MAXN] = modpow(fact[MAXN], MOD - 2);\n    for (int i = MAXN - 1; i &gt;= 0; i--)\n        invfact[i] = invfact[i + 1] * (i + 1) % MOD;\n}\nNow you can compute ( nCr ) and ( nPr ) in ( O(1) ) time.\n\n\n2. Combinations ( nCr )\nThe number of ways to choose r items from ( n ) items: \\[\nC(n, r) = \\frac{n!}{r!(n-r)!}\n\\]\nlong long nCr(int n, int r) {\n    if (r &lt; 0 || r &gt; n) return 0;\n    return fact[n] * invfact[r] % MOD * invfact[n - r] % MOD;\n}\nProperties:\n\n\\((C(n, 0) = 1),\\ (C(n, n) = 1)\\)\n\\(C(n, r) = C(n, n - r)\\)\nPascal’s Rule: \\(C(n, r) = C(n - 1, r - 1) + C(n - 1, r)\\)\n\n\n\nExample\n( C(5, 2) = 10 ) There are 10 ways to pick 2 elements from a 5-element set.\n\n\n3. Permutations ( nPr )\nNumber of ways to arrange r elements chosen from ( n ): \\[\nP(n, r) = \\frac{n!}{(n-r)!}\n\\]\nlong long nPr(int n, int r) {\n    if (r &lt; 0 || r &gt; n) return 0;\n    return fact[n] * invfact[n - r] % MOD;\n}\n\n\nExample\n( P(5, 2) = 20 ) Choosing 2 out of 5 elements and arranging them yields 20 orders.\n\n\n4. Subsets and Power Set\nEach element has 2 choices: include or exclude. Hence, number of subsets: \\[\n2^n\n\\]\nlong long subsets_count(int n) {\n    return modpow(2, n);\n}\nEnumerating subsets using bitmasks:\nfor (int mask = 0; mask &lt; (1 &lt;&lt; n); mask++) {\n    for (int i = 0; i &lt; n; i++)\n        if (mask & (1 &lt;&lt; i))\n            ; // include element i\n}\nTotal: \\(2^n\\) subsets, ( O\\(n2^n\\) ) time to enumerate.\n\n\n5. Multisets and Repetition\nNumber of ways to choose ( r ) items from ( n ) with repetition: \\[\nC(n + r - 1, r)\n\\]\nFor example, number of ways to give 5 candies to 3 kids (each can get 0): ( C(3+5-1, 5) = C(7,5) = 21 )\n\n\n6. Modular Combinatorics\nWhen working modulo ( p ): - Use modular inverse for division. - \\(C(n, r) \\bmod p = fact[n] \\cdot invfact[r] \\cdot invfact[n - r] \\bmod p\\)\nWhen \\(n \\ge p\\), use Lucas’ Theorem: \\[\nC(n, r) \\bmod p = C(n/p, r/p) \\cdot C(n%p, r%p) \\bmod p\n\\]\n\n\n7. Stirling and Bell Numbers (Advanced)\n\nStirling Numbers of 2nd Kind: ways to partition ( n ) items into ( k ) non-empty subsets \\[\nS(n,k) = k \\cdot S(n-1,k) + S(n-1,k-1)\n\\]\nBell Numbers: total number of partitions \\[\nB(n) = \\sum_{k=0}^{n} S(n,k)\n\\]\n\nUsed in set partition and grouping problems.\n\n\n8. Tiny Code\ninit_factorials();\nprintf(\"%lld\\n\", nCr(10, 3));  // 120\nprintf(\"%lld\\n\", nPr(10, 3));  // 720\nprintf(\"%lld\\n\", subsets_count(5)); // 32\n\n\n9. Summary\n\n\n\n\n\n\n\n\n\nConcept\nFormula\nMeaning\nExample\n\n\n\n\nFactorial\n\\(n!\\)\nAll arrangements\n\\(5! = 120\\)\n\n\nCombination\n\\(C(n, r) = \\frac{n!}{r!(n - r)!}\\)\nChoose\n\\(C(5, 2) = 10\\)\n\n\nPermutation\n\\(P(n, r) = \\frac{n!}{(n - r)!}\\)\nArrange\n\\(P(5, 2) = 20\\)\n\n\nSubsets\n\\(2^n\\)\nAll combinations\n\\(2^3 = 8\\)\n\n\nMultisets\n\\(C(n + r - 1, r)\\)\nRepetition allowed\n\\(C(4, 2) = 6\\)\n\n\n\n\n\nWhy It Matters\nCombinatorics underlies probability, DP counting, and modular problems. You can’t master competitive programming or algorithm design without counting possibilities correctly. It teaches how structure emerges from choice , and how to count it efficiently.\n\n\nTry It Yourself\n\nCompute \\(C(1000, 500) \\bmod (10^9 + 7)\\).\nCount the number of 5-bit subsets with exactly 3 bits on, i.e. \\(C(5, 3)\\).\nWrite a loop to print all subsets of {a, b, c, d}.\nUse Lucas’ theorem for \\(C(10^6, 1000) \\bmod 13\\).\nImplement Stirling recursion and print \\(S(5, 2)\\).\n\nEvery algorithmic counting trick , from Pascal’s triangle to binomial theorem , starts right here.\n\n\n\n54. Probability and Randomized Algorithms\nProbability introduces controlled randomness into computation. Instead of deterministic steps, randomized algorithms use random choices to achieve speed, simplicity, or robustness. This section bridges probability theory and algorithm design , teaching how to model, analyze, and exploit randomness.\nWe’ll cover:\n\nProbability Basics\nExpected Value\nMonte Carlo and Las Vegas Algorithms\nRandomized Data Structures and Algorithms\n\n\n1. Probability Basics\nEvery event has a probability between 0 and 1.\nIf a sample space has \\(n\\) equally likely outcomes and \\(k\\) of them satisfy a condition, then\n\\[\nP(E) = \\frac{k}{n}\n\\]\nExamples\n\nRolling a fair die: \\(P(\\text{even}) = \\frac{3}{6} = \\frac{1}{2}\\)\nDrawing an ace from a deck: \\(P(\\text{ace}) = \\frac{4}{52} = \\frac{1}{13}\\)\n\nKey Rules\n\nComplement: \\(P(\\bar{E}) = 1 - P(E)\\)\n\nAddition: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\nMultiplication: \\(P(A \\cap B) = P(A) \\cdot P(B \\mid A)\\)\n\n\n\n2. Expected Value\nThe expected value is the weighted average of outcomes.\n\\[\nE[X] = \\sum_{i} P(x_i) \\cdot x_i\n\\]\nExample: Expected value of a die: \\[\nE[X] = \\frac{1+2+3+4+5+6}{6} = 3.5\n\\]\nProperties:\n\nLinearity: \\(E[aX + bY] = aE[X] + bE[Y]\\)\nUseful for average-case analysis\n\nIn algorithms:\n\nExpected number of comparisons in QuickSort is \\(O(n \\log n)\\)\nExpected time for hash table lookup is \\(O(1)\\)\n\n\n\n3. Monte Carlo vs Las Vegas\nRandomized algorithms are broadly two types:\n\n\n\n\n\n\n\n\n\nType\nOutput\nRuntime\nExample\n\n\n\n\nMonte Carlo\nMay be wrong (probabilistically)\nFixed\nMiller-Rabin Primality\n\n\nLas Vegas\nAlways correct\nRandom runtime\nRandomized QuickSort\n\n\n\nMonte Carlo:\n\nFaster, approximate\nYou can control error probability\nE.g. primality test returns “probably prime”\n\nLas Vegas:\n\nOutput guaranteed correct\nRuntime varies by luck\nE.g. QuickSort with random pivot\n\n\n\n4. Randomization in Algorithms\nRandomization helps break worst-case patterns.\n\n\nA. Randomized QuickSort\nPick a random pivot instead of first element. Expected time becomes ( O\\(n \\log n\\) ) regardless of input order.\nint partition(int a[], int l, int r) {\n    int pivot = a[l + rand() % (r - l + 1)];\n    // move pivot to end, then normal partition\n}\nThis avoids adversarial inputs like sorted arrays.\n\n\nB. Randomized Hashing\nHash collisions can be exploited by adversaries. Using random coefficients in hash functions makes attacks infeasible.\nlong long hash(long long x, long long a, long long b, long long p) {\n    return (a * x + b) % p;\n}\nPick random ( a, b ) for robustness.\n\n\nC. Randomized Data Structures\n\nSkip List: uses random levels for nodes Expected ( O\\(\\log n\\) ) search/insert/delete\nTreap: randomized heap priority + BST order Maintains balance in expectation\n\nstruct Node {\n    int key, priority;\n    Node *l, *r;\n};\nRandomized balancing gives good average performance without rotation logic.\n\n\nD. Random Sampling\nPick random elements efficiently:\n\nReservoir Sampling: sample ( k ) items from a stream of unknown size- Shuffle: Fisher-Yates Algorithm\n\nfor (int i = n - 1; i &gt; 0; i--) {\n    int j = rand() % (i + 1);\n    swap(a[i], a[j]);\n}\n\n\n5. Probabilistic Guarantees\nRandomized algorithms often use Chernoff bounds and Markov’s inequality to bound errors:\n\nMarkov: \\(P(X \\ge kE[X]) \\le \\frac{1}{k}\\)\nChebyshev: \\(P(|X - E[X]| \\ge c\\sigma) \\le \\frac{1}{c^2}\\)\nChernoff: Exponentially small tail bounds\n\nThese ensure “with high probability” (\\(1 - \\frac{1}{n^c}\\)) guarantees.\n\n\n6. Tiny Code\nRandomized QuickSort:\nint partition(int arr[], int low, int high) {\n    int pivotIdx = low + rand() % (high - low + 1);\n    swap(arr[pivotIdx], arr[high]);\n    int pivot = arr[high], i = low;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) swap(arr[i++], arr[j]);\n    }\n    swap(arr[i], arr[high]);\n    return i;\n}\n\nvoid quicksort(int arr[], int low, int high) {\n    if (low &lt; high) {\n        int pi = partition(arr, low, high);\n        quicksort(arr, low, pi - 1);\n        quicksort(arr, pi + 1, high);\n    }\n}\n\n\n7. Summary\n\n\n\n\n\n\n\n\nConcept\nKey Idea\nUse Case\n\n\n\n\nExpected Value\nWeighted average outcome\nAnalyze average case\n\n\nMonte Carlo\nProbabilistic correctness\nPrimality test\n\n\nLas Vegas\nProbabilistic runtime\nQuickSort\n\n\nRandom Pivot\nBreak worst-case\nSorting\n\n\nSkip List / Treap\nRandom balancing\nData Structures\n\n\nReservoir Sampling\nStream selection\nLarge data\n\n\n\n\n\nWhy It Matters\nRandomization is not “luck” , it’s a design principle. It transforms rigid algorithms into adaptive, robust systems. In complexity theory, randomness helps achieve bounds impossible deterministically.\n\n“A bit of randomness turns worst cases into best friends.”\n\n\n\nTry It Yourself\n\nSimulate rolling two dice and compute expected sum.\nImplement randomized QuickSort and measure average runtime.\nWrite a Monte Carlo primality checker.\nCreate a random hash function for integers.\nImplement reservoir sampling for a large input stream.\n\nThese experiments show how uncertainty can become a powerful ally in algorithm design.\n\n\n\n55. Sieve Methods and Modular Math\nSieve methods are essential tools in number theory for generating prime numbers, prime factors, and function values (φ, μ) efficiently. Combined with modular arithmetic, they form the backbone of algorithms in cryptography, combinatorics, and competitive programming.\nThis section introduces:\n\nSieve of Eratosthenes- Optimized Linear Sieve- Sieve for Smallest Prime Factor (SPF)- Euler’s Totient Function (φ)- Modular Applications\n\n\n1. The Sieve of Eratosthenes\nThe classic algorithm to find all primes ≤ ( n ).\nIdea: Start from 2, mark all multiples as composite. Continue to √n.\nvector&lt;int&gt; sieve(int n) {\n    vector&lt;int&gt; primes;\n    vector&lt;bool&gt; is_prime(n + 1, true);\n    is_prime[0] = is_prime[1] = false;\n    for (int i = 2; i * i &lt;= n; i++)\n        if (is_prime[i])\n            for (int j = i * i; j &lt;= n; j += i)\n                is_prime[j] = false;\n    for (int i = 2; i &lt;= n; i++)\n        if (is_prime[i]) primes.push_back(i);\n    return primes;\n}\nTime Complexity: ( O\\(n \\log \\log n\\) )\nSpace: ( O(n) )\nExample: Primes up to 20 → 2, 3, 5, 7, 11, 13, 17, 19\n\n\n2. Linear Sieve (O(n))\nUnlike the basic sieve, each number is marked exactly once by its smallest prime factor (SPF).\nIdea:\n\nFor each prime ( p ), mark \\(p \\times i\\) only once.- Use spf[i] to store smallest prime factor.\n\nconst int N = 1e6;\nint spf[N + 1];\nvector&lt;int&gt; primes;\n\nvoid linear_sieve() {\n    for (int i = 2; i &lt;= N; i++) {\n        if (!spf[i]) {\n            spf[i] = i;\n            primes.push_back(i);\n        }\n        for (int p : primes) {\n            if (p &gt; spf[i] || 1LL * i * p &gt; N) break;\n            spf[i * p] = p;\n        }\n    }\n}\nBenefits:\n\nGet primes, SPF, and factorizations in ( O(n) ).- Ideal for problems needing many factorizations.\n\n\n\n3. Smallest Prime Factor (SPF) Table\nWith spf[], factorization becomes ( O\\(\\log n\\) ).\nvector&lt;int&gt; factorize(int x) {\n    vector&lt;int&gt; f;\n    while (x != 1) {\n        f.push_back(spf[x]);\n        x /= spf[x];\n    }\n    return f;\n}\nExample: spf[12] = 2 → factors = [2, 2, 3]\n\n\n4. Euler’s Totient Function ( (n) )\nThe number of integers ≤ ( n ) that are coprime with ( n ).\nFormula: \\[\n\\varphi(n) = n \\prod_{p|n} \\left(1 - \\frac{1}{p}\\right)\n\\]\nProperties:\n\n\\(\\varphi(p) = p - 1\\) if \\(p\\) is prime\nMultiplicative: if \\(\\gcd(a, b) = 1\\), then \\(\\varphi(ab) = \\varphi(a)\\varphi(b)\\)\n\nImplementation (Linear Sieve):\nconst int N = 1e6;\nint phi[N + 1];\nbool is_comp[N + 1];\nvector&lt;int&gt; primes;\n\nvoid phi_sieve() {\n    phi[1] = 1;\n    for (int i = 2; i &lt;= N; i++) {\n        if (!is_comp[i]) {\n            primes.push_back(i);\n            phi[i] = i - 1;\n        }\n        for (int p : primes) {\n            if (1LL * i * p &gt; N) break;\n            is_comp[i * p] = true;\n            if (i % p == 0) {\n                phi[i * p] = phi[i] * p;\n                break;\n            } else {\n                phi[i * p] = phi[i] * (p - 1);\n            }\n        }\n    }\n}\nExample:\n\n\\(\\varphi(6) = 6(1 - \\frac{1}{2})(1 - \\frac{1}{3}) = 2\\)\nNumbers coprime with 6: 1, 5\n\n\n\n5. Modular Math Applications\nOnce we have primes and totients, we can do many modular computations.\n\n\nA. Fermat’s Little Theorem\nIf ( p ) is prime, \\[\na^{p-1} \\equiv 1 \\pmod{p}\n\\] Hence, \\[\na^{-1} \\equiv a^{p-2} \\pmod{p}\n\\]\nUsed in: modular inverses, combinatorics.\n\n\nB. Euler’s Theorem\nIf \\(\\gcd(a, n) = 1\\), then\n\\[\na^{\\varphi(n)} \\equiv 1 \\pmod{n}\n\\]\nGeneralizes Fermat’s theorem to composite moduli.\n\n\nC. Modular Exponentiation with Totient Reduction\nFor very large powers:\n\\[\na^b \\bmod n = a^{b \\bmod \\varphi(n)} \\bmod n\n\\]\n(when \\(a\\) and \\(n\\) are coprime)\n\n\n6. Tiny Code\nPrimes up to n:\nauto primes = sieve(100);\nTotients up to n:\nphi_sieve();\ncout &lt;&lt; phi[10]; // 4\nFactorization:\nauto f = factorize(60); // [2, 2, 3, 5]\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nTime\nUse\n\n\n\n\nEratosthenes\nMark multiples\n(O\\(n \\log \\log n\\))\nSimple prime gen\n\n\nLinear Sieve\nMark once\n(O(n))\nPrime + SPF\n\n\nSPF Table\nSmallest prime factor\n(O(1)) query\nFast factorization\n\n\nφ(n)\nCoprime count\n(O(n))\nModular exponent\n\n\nFermat / Euler\nInverses, reduction\n(O\\(\\log n\\))\nModular arithmetic\n\n\n\n\n\nWhy It Matters\nSieve methods are the fastest way to preprocess arithmetic information. They unlock efficient solutions to problems involving primes, divisors, modular equations, and cryptography.\n\n“Before you can reason about numbers, you must first sieve them clean.”\n\n\n\nTry It Yourself\n\nGenerate all primes \\(\\le 10^6\\) using a linear sieve.\nFactorize \\(840\\) using the SPF array.\nCompute \\(\\varphi(n)\\) for \\(n = 1..20\\).\nVerify \\(a^{\\varphi(n)} \\equiv 1 \\pmod{n}\\) for \\(a = 3\\), \\(n = 10\\).\nSolve \\(a^b \\bmod n\\) with \\(b\\) very large using \\(\\varphi(n)\\).\n\nSieve once, and modular math becomes effortless forever after.\n\n\n\n56. Linear Algebra (Gaussian Elimination, LU, SVD)\nLinear algebra gives algorithms their mathematical backbone. From solving equations to powering ML models, it’s the hidden engine behind optimization, geometry, and numerical computation.\nIn this section, we’ll focus on the algorithmic toolkit:\n\nGaussian Elimination (solve systems, invert matrices)\nLU Decomposition (efficient repeated solving)\nSVD (Singular Value Decomposition) overview\n\nYou’ll see how algebra becomes code , step by step.\n\n1. Systems of Linear Equations\nWe want to solve: \\[\nA \\cdot x = b\n\\] where ( A ) is an \\(n \\times n\\) matrix, and ( x, b ) are vectors.\nFor example: \\[\\begin{cases}\n2x + 3y = 8 \\\nx + 2y = 5\n\\end{cases}\\]\nThe solution is the intersection of two lines. In general, \\(A^{-1}b\\) gives ( x ), but we usually solve it more directly using Gaussian elimination.\n\n\n2. Gaussian Elimination (Row Reduction)\nIdea: Transform ( [A|b] ) (augmented matrix) into upper-triangular form, then back-substitute.\nSteps:\n\nFor each row, select a pivot (non-zero leading element).\nEliminate below it using row operations.\nAfter all pivots, back-substitute to get the solution.\n\n\n\nA. Implementation (C)\nconst double EPS = 1e-9;\n\nvector&lt;double&gt; gauss(vector&lt;vector&lt;double&gt;&gt; A, vector&lt;double&gt; b) {\n    int n = A.size();\n    for (int i = 0; i &lt; n; i++) {\n        // 1. Find pivot\n        int pivot = i;\n        for (int j = i + 1; j &lt; n; j++)\n            if (fabs(A[j][i]) &gt; fabs(A[pivot][i]))\n                pivot = j;\n        swap(A[i], A[pivot]);\n        swap(b[i], b[pivot]);\n\n        // 2. Normalize pivot row\n        double div = A[i][i];\n        if (fabs(div) &lt; EPS) continue;\n        for (int k = i; k &lt; n; k++) A[i][k] /= div;\n        b[i] /= div;\n\n        // 3. Eliminate below\n        for (int j = i + 1; j &lt; n; j++) {\n            double factor = A[j][i];\n            for (int k = i; k &lt; n; k++) A[j][k] -= factor * A[i][k];\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // 4. Back substitution\n    vector&lt;double&gt; x(n);\n    for (int i = n - 1; i &gt;= 0; i--) {\n        x[i] = b[i];\n        for (int j = i + 1; j &lt; n; j++)\n            x[i] -= A[i][j] * x[j];\n    }\n    return x;\n}\nTime complexity: ( O\\(n^3\\) )\n\n\nB. Example\nSolve: \\[\\begin{cases}\n2x + 3y = 8 \\\nx + 2y = 5\n\\end{cases}\\]\nAugmented matrix: \\[\\begin{bmatrix}\n2 & 3 & | & 8 \\\n1 & 2 & | & 5\n\\end{bmatrix}\\]\nReduce:\n\nRow2 ← Row2 − ½ Row1 → \\([1, 2 | 5] \\to [0, 0.5 | 1]\\)- Back substitute → ( y = 2, x = 1 )\n\n\n\n3. LU Decomposition\nLU factorization expresses: \\[\nA = L \\cdot U\n\\] where ( L ) is lower-triangular (1s on diagonal), ( U ) is upper-triangular.\nThis allows solving ( A x = b ) in two triangular solves:\n\nSolve ( L y = b )\nSolve ( U x = y )\n\nEfficient when solving for multiple b’s (same A).\n\n\nA. Decomposition Algorithm\nvoid lu_decompose(vector&lt;vector&lt;double&gt;&gt;& A, vector&lt;vector&lt;double&gt;&gt;& L, vector&lt;vector&lt;double&gt;&gt;& U) {\n    int n = A.size();\n    L.assign(n, vector&lt;double&gt;(n, 0));\n    U.assign(n, vector&lt;double&gt;(n, 0));\n\n    for (int i = 0; i &lt; n; i++) {\n        // Upper\n        for (int k = i; k &lt; n; k++) {\n            double sum = 0;\n            for (int j = 0; j &lt; i; j++)\n                sum += L[i][j] * U[j][k];\n            U[i][k] = A[i][k] - sum;\n        }\n        // Lower\n        for (int k = i; k &lt; n; k++) {\n            if (i == k) L[i][i] = 1;\n            else {\n                double sum = 0;\n                for (int j = 0; j &lt; i; j++)\n                    sum += L[k][j] * U[j][i];\n                L[k][i] = (A[k][i] - sum) / U[i][i];\n            }\n        }\n    }\n}\nSolve with forward + backward substitution.\n\n\n4. Singular Value Decomposition (SVD)\nSVD generalizes diagonalization for non-square matrices: \\[\nA = U \\Sigma V^T\n\\]\nWhere:\n\n( U ): left singular vectors (orthogonal)- \\(\\Sigma\\): diagonal of singular values- \\(V^T\\): right singular vectors Applications:\nData compression (PCA)- Noise reduction- Rank estimation- Pseudoinverse \\(A^+ = V \\Sigma^{-1} U^T\\) In practice, use libraries (e.g. LAPACK, Eigen).\n\n\n\n5. Numerical Stability and Pivoting\nIn floating-point math:\n\nAlways pick largest pivot (partial pivoting)- Avoid dividing by small numbers- Use EPS = 1e-9 threshold Small numerical errors can amplify quickly , stability is key.\n\n\n\n6. Tiny Code\nvector&lt;vector&lt;double&gt;&gt; A = {{2, 3}, {1, 2}};\nvector&lt;double&gt; b = {8, 5};\nauto x = gauss(A, b);\n// Output: x = [1, 2]\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nComplexity\nNotes\n\n\n\n\nGaussian Elimination\nSolve Ax=b\n(O\\(n^3\\))\nDirect method\n\n\nLU Decomposition\nRepeated solves\n(O\\(n^3\\))\nTriangular factorization\n\n\nSVD\nGeneral decomposition\n(O\\(n^3\\))\nRobust, versatile\n\n\n\n\n\nWhy It Matters\nLinear algebra is the language of algorithms , it solves equations, optimizes functions, and projects data. Whether building solvers or neural networks, these methods are your foundation.\n\n“Every algorithm lives in a vector space , it just needs a basis to express itself.”\n\n\n\nTry It Yourself\n\nSolve a 3×3 linear system with Gaussian elimination.\nImplement LU decomposition and test \\(L \\cdot U = A\\).\nUse LU to solve multiple ( b ) vectors.\nExplore SVD using a math library; compute singular values of a 2×2 matrix.\nCompare results between naive and pivoted elimination for unstable systems.\n\nStart with row operations , and you’ll see how geometry and algebra merge into code.\n\n\n\n57. FFT and NTT (Fast Transforms)\nThe Fast Fourier Transform (FFT) is one of the most beautiful and practical algorithms ever invented. It converts data between time (or coefficient) domain and frequency (or point) domain efficiently. The Number Theoretic Transform (NTT) is its modular counterpart for integer arithmetic , ideal for polynomial multiplication under a modulus.\nThis section covers:\n\nWhy we need transforms- Discrete Fourier Transform (DFT)- Cooley-Tukey FFT (complex numbers)- NTT (modular version)- Applications (polynomial multiplication, convolution)\n\n\n1. Motivation\nSuppose you want to multiply two polynomials: \\[\nA(x) = a_0 + a_1x + a_2x^2\n\\]\n\\[\nB(x) = b_0 + b_1x + b_2x^2\n\\]\nTheir product has coefficients: \\[\nc_k = \\sum_{i+j=k} a_i \\cdot b_j\n\\]\nThis is convolution: \\[\nC = A * B\n\\] Naively, this takes ( O\\(n^2\\) ). FFT reduces it to ( O\\(n \\log n\\) ).\n\n\n2. Discrete Fourier Transform (DFT)\nThe DFT maps coefficients \\(a_0, a_1, \\ldots, a_{n-1}\\) to evaluations at ( n )-th roots of unity:\n\\[\nA_k = \\sum_{j=0}^{n-1} a_j \\cdot e^{-2\\pi i \\cdot jk / n}\n\\]\nand the inverse transform recovers \\(a_j\\) from \\(A_k\\).\n\n\n3. Cooley-Tukey FFT\nKey idea: recursively split the sum into even and odd parts:\n\\[\nA_k = A_{even}(w_n^2) + w_n^k \\cdot A_{odd}(w_n^2)\n\\]\nWhere \\(w_n = e^{-2\\pi i / n}\\) is an ( n )-th root of unity.\n\n\nImplementation (C++)\n#include &lt;complex&gt;\n#include &lt;vector&gt;\n#include &lt;cmath&gt;\nusing namespace std;\n\nusing cd = complex&lt;double&gt;;\nconst double PI = acos(-1);\n\nvoid fft(vector&lt;cd&gt; &a, bool invert) {\n    int n = a.size();\n    for (int i = 1, j = 0; i &lt; n; i++) {\n        int bit = n &gt;&gt; 1;\n        for (; j & bit; bit &gt;&gt;= 1) j ^= bit;\n        j ^= bit;\n        if (i &lt; j) swap(a[i], a[j]);\n    }\n\n    for (int len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        double ang = 2 * PI / len * (invert ? -1 : 1);\n        cd wlen(cos(ang), sin(ang));\n        for (int i = 0; i &lt; n; i += len) {\n            cd w(1);\n            for (int j = 0; j &lt; len / 2; j++) {\n                cd u = a[i + j], v = a[i + j + len / 2] * w;\n                a[i + j] = u + v;\n                a[i + j + len / 2] = u - v;\n                w *= wlen;\n            }\n        }\n    }\n\n    if (invert) {\n        for (cd &x : a) x /= n;\n    }\n}\n\n\nPolynomial Multiplication with FFT\nvector&lt;long long&gt; multiply(vector&lt;int&gt; const& a, vector&lt;int&gt; const& b) {\n    vector&lt;cd&gt; fa(a.begin(), a.end()), fb(b.begin(), b.end());\n    int n = 1;\n    while (n &lt; (int)a.size() + (int)b.size()) n &lt;&lt;= 1;\n    fa.resize(n);\n    fb.resize(n);\n\n    fft(fa, false);\n    fft(fb, false);\n    for (int i = 0; i &lt; n; i++) fa[i] *= fb[i];\n    fft(fa, true);\n\n    vector&lt;long long&gt; result(n);\n    for (int i = 0; i &lt; n; i++)\n        result[i] = llround(fa[i].real());\n    return result;\n}\nComplexity: ( O\\(n \\log n\\) )\n\n\n4. Number Theoretic Transform (NTT)\nFFT uses complex numbers , NTT uses modular arithmetic with roots of unity mod p. We need a prime ( p ) such that: \\[\np = c \\cdot 2^k + 1\n\\] so a primitive root ( g ) exists.\nPopular choices:\n\n( p = 998244353, g = 3 )- ( p = 7340033, g = 3 )\n\n\n\nImplementation (NTT)\nconst int MOD = 998244353;\nconst int G = 3;\n\nint modpow(int a, int b) {\n    long long res = 1;\n    while (b) {\n        if (b & 1) res = res * a % MOD;\n        a = 1LL * a * a % MOD;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\n\nvoid ntt(vector&lt;int&gt; &a, bool invert) {\n    int n = a.size();\n    for (int i = 1, j = 0; i &lt; n; i++) {\n        int bit = n &gt;&gt; 1;\n        for (; j & bit; bit &gt;&gt;= 1) j ^= bit;\n        j ^= bit;\n        if (i &lt; j) swap(a[i], a[j]);\n    }\n    for (int len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        int wlen = modpow(G, (MOD - 1) / len);\n        if (invert) wlen = modpow(wlen, MOD - 2);\n        for (int i = 0; i &lt; n; i += len) {\n            long long w = 1;\n            for (int j = 0; j &lt; len / 2; j++) {\n                int u = a[i + j];\n                int v = (int)(a[i + j + len / 2] * w % MOD);\n                a[i + j] = u + v &lt; MOD ? u + v : u + v - MOD;\n                a[i + j + len / 2] = u - v &gt;= 0 ? u - v : u - v + MOD;\n                w = w * wlen % MOD;\n            }\n        }\n    }\n    if (invert) {\n        int inv_n = modpow(n, MOD - 2);\n        for (int &x : a) x = 1LL * x * inv_n % MOD;\n    }\n}\n\n\n5. Applications\n\nPolynomial Multiplication: ( O\\(n \\log n\\) )\nConvolution: digital signal processing\nBig Integer Multiplication (Karatsuba, FFT)\nSubset Convolution and combinatorial transforms\nNumber-theoretic sums (NTT-friendly modulus)\n\n\n\n6. Tiny Code\nvector&lt;int&gt; A = {1, 2, 3};\nvector&lt;int&gt; B = {4, 5, 6};\n// Result = {4, 13, 28, 27, 18}\nauto C = multiply(A, B);\n\n\n7. Summary\n\n\n\nAlgorithm\nDomain\nComplexity\nType\n\n\n\n\nDFT\nComplex\n( O\\(n^2\\) )\nNaive\n\n\nFFT\nComplex\n( O\\(n \\log n\\) )\nRecursive\n\n\nNTT\nModular\n( O\\(n \\log n\\) )\nInteger arithmetic\n\n\n\n\n\nWhy It Matters\nFFT and NTT bring polynomial algebra to life. They turn slow convolutions into lightning-fast transforms. From multiplying huge integers to compressing signals, they’re the ultimate divide-and-conquer on structure.\n\n“To multiply polynomials fast, you first turn them into music , then back again.”\n\n\n\nTry It Yourself\n\nMultiply (\\(1 + 2x + 3x^2\\)) and (\\(4 + 5x + 6x^2\\)) using FFT.\nImplement NTT over 998244353 and verify results mod p.\nCompare ( O\\(n^2\\) ) vs FFT performance for n = 1024.\nExperiment with inverse FFT (invert = true).\nExplore circular convolution for signal data.\n\nOnce you master FFT/NTT, you hold the power of speed in algebraic computation.\n\n\n\n58. Numerical Methods (Newton, Simpson, Runge-Kutta)\nNumerical methods let us approximate solutions when exact algebraic answers are hard or impossible. They are the foundation of scientific computing, simulation, and optimization , bridging the gap between continuous math and discrete computation.\nIn this section, we’ll explore three classics:\n\nNewton-Raphson: root finding- Simpson’s Rule: numerical integration- Runge-Kutta (RK4): solving differential equations These algorithms showcase how iteration, approximation, and convergence build powerful tools.\n\n\n1. Newton-Raphson Method\nUsed to find a root of ( f(x) = 0 ). Starting from a guess \\(x_0\\), iteratively refine:\n\\[\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\\]\nConvergence is quadratic if ( f ) is smooth and \\(x_0\\) is close enough.\n\n\nA. Example\nSolve ( f(x) = x^2 - 2 = 0 ) We know root = \\(\\sqrt{2}\\)\nStart \\(x_0 = 1\\)\n\n\n\nIter\n\\(x_n\\)\n(f\\(x_n\\))\n(f’\\(x_n\\))\n\\(x_{n+1}\\)\n\n\n\n\n0\n1.000\n-1.000\n2.000\n1.500\n\n\n1\n1.500\n0.250\n3.000\n1.417\n\n\n2\n1.417\n0.006\n2.834\n1.414\n\n\n\nConverged: \\(1.414 \\approx \\sqrt{2}\\)\n\n\nB. Implementation\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\ndouble f(double x) { return x * x - 2; }\ndouble df(double x) { return 2 * x; }\n\ndouble newton(double x0) {\n    for (int i = 0; i &lt; 20; i++) {\n        double fx = f(x0);\n        double dfx = df(x0);\n        if (fabs(fx) &lt; 1e-9) break;\n        x0 = x0 - fx / dfx;\n    }\n    return x0;\n}\n\nint main() {\n    printf(\"Root: %.6f\\n\", newton(1.0)); // 1.414214\n}\nTime Complexity: ( O(k) ) iterations, each ( O(1) )\n\n\n2. Simpson’s Rule (Numerical Integration)\nWhen you can’t integrate ( f(x) ) analytically, approximate the area under the curve.\nDivide interval ([a, b]) into even ( n ) subintervals (width ( h )).\n\\[\nI \\approx \\frac{h}{3} \\Big( f(a) + 4 \\sum f(x_{odd}) + 2 \\sum f(x_{even}) + f(b) \\Big)\n\\]\n\n\nA. Implementation\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\ndouble f(double x) { return x * x; } // integrate x^2\n\ndouble simpson(double a, double b, int n) {\n    double h = (b - a) / n;\n    double s = f(a) + f(b);\n    for (int i = 1; i &lt; n; i++) {\n        double x = a + i * h;\n        s += f(x) * (i % 2 == 0 ? 2 : 4);\n    }\n    return s * h / 3;\n}\n\nint main() {\n    printf(\"∫₀¹ x² dx ≈ %.6f\\n\", simpson(0, 1, 100)); // ~0.333333\n}\nAccuracy: ( O\\(h^4\\) ) Note: ( n ) must be even.\n\n\nB. Example\n\\[\n\\int_0^1 x^2 dx = \\frac{1}{3}\n\\] With ( n = 100 ), Simpson gives ( 0.333333 ).\n\n\n3. Runge-Kutta (RK4)\nUsed to solve first-order ODEs: \\[\ny' = f(x, y), \\quad y(x_0) = y_0\n\\]\nRK4 Formula:\n\\[\\begin{aligned}\nk_1 &= f(x_n, y_n) \\\nk_2 &= f(x_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1) \\\nk_3 &= f(x_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2) \\\nk_4 &= f(x_n + h, y_n + hk_3) \\\ny_{n+1} &= y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n\\end{aligned}\\]\nAccuracy: ( O\\(h^4\\) )\n\n\nA. Example\nSolve ( y’ = x + y ), ( y(0) = 1 ), step ( h = 0.1 ).\nEach iteration refines ( y ) with weighted slope average.\n\n\nB. Implementation\n#include &lt;stdio.h&gt;\n\ndouble f(double x, double y) {\n    return x + y;\n}\n\ndouble runge_kutta(double x0, double y0, double h, double xn) {\n    double x = x0, y = y0;\n    while (x &lt; xn) {\n        double k1 = f(x, y);\n        double k2 = f(x + h / 2, y + h * k1 / 2);\n        double k3 = f(x + h / 2, y + h * k2 / 2);\n        double k4 = f(x + h, y + h * k3);\n        y += h * (k1 + 2*k2 + 2*k3 + k4) / 6;\n        x += h;\n    }\n    return y;\n}\n\nint main() {\n    printf(\"y(0.1) ≈ %.6f\\n\", runge_kutta(0, 1, 0.1, 0.1));\n}\n\n\n4. Tiny Code Summary\n\n\n\n\n\n\n\n\n\n\nMethod\nPurpose\nFormula\nAccuracy\nType\n\n\n\n\nNewton-Raphson\nRoot finding\n\\(x_{n+1}=x_n-\\frac{f}{f'}\\)\nQuadratic\nIterative\n\n\nSimpson’s Rule\nIntegration\n(h/3(…))\n(O\\(h^4\\))\nDeterministic\n\n\nRunge-Kutta (RK4)\nODEs\nWeighted slope avg\n(O\\(h^4\\))\nIterative\n\n\n\n\n\n5. Numerical Stability\n\nSmall step ( h ): better accuracy, more cost- Large ( h ): faster, less stable- Always check convergence (\\(|x_{n+1} - x_n| &lt; \\varepsilon\\))- Avoid division by small derivatives in Newton’s method\n\n\n\nWhy It Matters\nNumerical methods let computers simulate the continuous world. From physics to AI training, they solve what calculus cannot symbolically.\n\n“When equations won’t talk, iterate , and they’ll whisper their answers.”\n\n\n\nTry It Yourself\n\nUse Newton’s method for \\(\\cos x - x = 0\\).\nApproximate \\(\\displaystyle \\int_0^{\\pi/2} \\sin x\\,dx\\) with Simpson’s rule.\nSolve \\(y' = y - x^2 + 1,\\ y(0) = 0.5\\) using RK4.\nCompare RK4 with Euler’s method for the same ODE.\nExperiment with step sizes \\(h \\in \\{0.1, 0.01, 0.001\\}\\) and observe convergence.\n\nNumerical thinking turns continuous problems into iterative algorithms , precise enough to power every simulation and solver you’ll ever write.\n\n\n\n59. Mathematical Optimization (Simplex, Gradient, Convex)\nMathematical optimization is about finding the best solution , smallest cost, largest profit, shortest path , under given constraints. It’s the heart of machine learning, operations research, and engineering design.\nIn this section, we’ll explore three pillars:\n\nSimplex Method , for linear programs\nGradient Descent , for continuous optimization\nConvex Optimization , the theory ensuring global optima\n\n\n1. What Is Optimization?\nA general optimization problem looks like:\n\\[\n\\min_x \\ f(x)\n\\] subject to constraints: \\[\ng_i(x) \\le 0, \\quad h_j(x) = 0\n\\]\nWhen ( f ) and \\(g_i, h_j\\) are linear, it’s a Linear Program (LP). When ( f ) is differentiable, we can use gradients. When ( f ) is convex, every local minimum is global , the ideal world.\n\n\n2. The Simplex Method (Linear Programming)\nA linear program has the form:\n\\[\n\\max \\ c^T x\n\\] subject to \\[\nA x \\le b, \\quad x \\ge 0\n\\]\nGeometrically, each constraint forms a half-space. The feasible region is a convex polytope, and the optimum lies at a vertex.\n\n\nA. Example\nMaximize ( z = 3x + 2y ) subject to \\[\\begin{cases}\n2x + y \\le 18 \\\n2x + 3y \\le 42 \\\nx, y \\ge 0\n\\end{cases}\\]\nSolution: ( x=9, y=8 ), ( z=43 )\n\n\nB. Algorithm (Sketch)\n\nConvert inequalities to equalities by adding slack variables.\nInitialize at a vertex (basic feasible solution).\nAt each step:\n\nChoose entering variable (most negative coefficient in objective). - Choose leaving variable (min ratio test). - Pivot to new vertex.4. Repeat until optimal.\n\n\n\n\nC. Implementation (Simplified Pseudocode)\n// Basic simplex-like outline\nwhile (exists negative coefficient in objective row) {\n    choose entering column j;\n    choose leaving row i (min b[i]/a[i][j]);\n    pivot(i, j);\n}\nLibraries (like GLPK or Eigen) handle full implementations.\nTime Complexity: worst ( O\\(2^n\\) ), but fast in practice.\n\n\n3. Gradient Descent\nFor differentiable ( f(x) ), we move opposite the gradient:\n\\[\nx_{k+1} = x_k - \\eta \\nabla f(x_k)\n\\]\nwhere \\(\\eta\\) is the learning rate.\nIntuition: ( f(x) ) points uphill, so step opposite it.\n\n\nA. Example\nMinimize ( f(x) = (x-3)^2 )\n\\[\nf'(x) = 2(x-3)\n\\]\nStart \\(x_0 = 0\\), \\(\\eta = 0.1\\)\n\n\n\nIter\n\\(x_k\\)\n(f\\(x_k\\))\nGradient\nNew (x)\n\n\n\n\n0\n0\n9\n-6\n0.6\n\n\n1\n0.6\n5.76\n-4.8\n1.08\n\n\n2\n1.08\n3.69\n-3.84\n1.46\n\n\n…\n→3\n→0\n→0\n→3\n\n\n\nConverges to ( x = 3 )\n\n\nB. Implementation\n#include &lt;math.h&gt;\n#include &lt;stdio.h&gt;\n\ndouble f(double x) { return (x - 3) * (x - 3); }\ndouble df(double x) { return 2 * (x - 3); }\n\ndouble gradient_descent(double x0, double lr) {\n    for (int i = 0; i &lt; 100; i++) {\n        double g = df(x0);\n        if (fabs(g) &lt; 1e-6) break;\n        x0 -= lr * g;\n    }\n    return x0;\n}\n\nint main() {\n    printf(\"Min at x = %.6f\\n\", gradient_descent(0, 0.1));\n}\n\n\nC. Variants\n\nMomentum: ( v = v + \\(1-\\beta\\)f(x) )- Adam: adaptive learning rates- Stochastic Gradient Descent (SGD): random subset of data All used heavily in machine learning.\n\n\n\n4. Convex Optimization\nA function ( f ) is convex if: \\[\nf(\\lambda x + (1-\\lambda)y) \\le \\lambda f(x) + (1-\\lambda)f(y)\n\\]\nThis means any local minimum is global.\nExamples:\n\n( f(x) = x^2 ) (convex)- ( f(x) = x^3 ) (not convex) For convex functions with linear constraints, gradient-based methods always converge to the global optimum.\n\n\n\nA. Checking Convexity\n\n1D: ( f’’(x) )- Multivariate: Hessian ( ^2 f(x) ) is positive semidefinite\n\n\n\n5. Applications\n\nLinear Programming (Simplex): logistics, scheduling- Quadratic Programming: portfolio optimization- Gradient Methods: ML, curve fitting- Convex Programs: control systems, regularization\n\n\n\n6. Tiny Code\nSimple gradient descent to minimize ( f(x,y)=x2+y2 ):\ndouble f(double x, double y) { return x*x + y*y; }\nvoid grad(double x, double y, double *gx, double *gy) {\n    *gx = 2*x; *gy = 2*y;\n}\n\nvoid optimize() {\n    double x=5, y=3, lr=0.1;\n    for(int i=0; i&lt;100; i++){\n        double gx, gy;\n        grad(x, y, &gx, &gy);\n        x -= lr * gx;\n        y -= lr * gy;\n    }\n    printf(\"Min at (%.3f, %.3f)\\n\", x, y);\n}\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nDomain\nComplexity\nNotes\n\n\n\n\nSimplex\nLinear\nPolynomial (average case)\nLP solver\n\n\nGradient Descent\nContinuous\n\\(O(k)\\)\nNeeds step size\n\n\nConvex Methods\nConvex\n\\(O(k \\log \\frac{1}{\\varepsilon})\\)\nGlobal optima guaranteed\n\n\n\n\n\nWhy It Matters\nOptimization turns math into decisions. From fitting curves to planning resources, it formalizes trade-offs and efficiency. It’s where computation meets purpose , finding the best in all possible worlds.\n\n“Every algorithm is, at heart, an optimizer , searching for something better.”\n\n\n\nTry It Yourself\n\nSolve a linear program with 2 constraints manually via Simplex.\nImplement gradient descent for \\(f(x) = (x - 5)^2 + 2\\).\nAdd momentum to your gradient descent loop.\nCheck convexity by plotting \\(f(x) = x^4 - 3x^2\\).\nExperiment with learning rates: too small leads to slow convergence; too large can diverge.\n\nMastering optimization means mastering how algorithms improve themselves , step by step, iteration by iteration.\n\n\n\n60. Algebraic Tricks and Transform Techniques\nIn algorithm design, algebra isn’t just theory , it’s a toolbox for transforming problems. By expressing computations algebraically, we can simplify, accelerate, or generalize solutions. This section surveys common algebraic techniques that turn hard problems into manageable ones.\nWe’ll explore:\n\nAlgebraic identities and factorizations\nGenerating functions and transforms\nConvolution tricks\nPolynomial methods and FFT applications\nMatrix and linear transforms for acceleration\n\n\n1. Algebraic Identities\nThese let you rewrite or decompose expressions to reveal structure or reduce complexity.\nClassic Forms:\n\nDifference of squares: \\[\na^2 - b^2 = (a-b)(a+b)\n\\]\nSum of cubes: \\[\na^3 + b^3 = (a+b)(a^2 - ab + b^2)\n\\]\nSquare of sum: \\[\n(a+b)^2 = a^2 + 2ab + b^2\n\\]\n\nUsed in dynamic programming, geometry, and optimization when simplifying recurrence terms or constraints.\nExample: Transforming \\((x+y)^2\\) lets you compute both \\(x^2 + y^2\\) and cross terms efficiently.\n\n\n2. Generating Functions\nA generating function encodes a sequence \\(a_0, a_1, a_2, \\ldots\\) into a formal power series:\n\\[\nG(x) = a_0 + a_1x + a_2x^2 + \\ldots\n\\]\nThey turn recurrence relations and counting problems into algebraic equations.\nExample: Fibonacci sequence \\[\nF(x) = F_0 + F_1x + F_2x^2 + \\ldots\n\\] with recurrence \\(F_n = F_{n-1} + F_{n-2}\\)\nSolve algebraically: \\[\nF(x) = \\frac{x}{1 - x - x^2}\n\\]\nApplications: combinatorics, probability, counting partitions.\n\n\n3. Convolution Tricks\nConvolution arises in combining sequences: \\[\n(c_n) = (a * b)*n = \\sum*{i=0}^{n} a_i b_{n-i}\n\\]\nNaive computation: ( O\\(n^2\\) ) Using Fast Fourier Transform (FFT): ( O\\(n \\log n\\) )\nExample: Polynomial multiplication Let \\[\nA(x) = a_0 + a_1x + a_2x^2, \\quad B(x) = b_0 + b_1x + b_2x^2\n\\] Then ( C(x) = A(x)B(x) ) gives coefficients by convolution.\nThis trick is used in:\n\nLarge integer multiplication- Pattern matching (cross-correlation)- Subset sum acceleration\n\n\n\n4. Polynomial Methods\nMany algorithmic problems can be represented as polynomials, where coefficients encode combinatorial structure.\n\n\nA. Polynomial Interpolation\nGiven ( n+1 ) points, there’s a unique degree-( n ) polynomial passing through them.\nUsed in error correction, FFT-based reconstruction, and number-theoretic transforms.\nLagrange Interpolation: \\[\nP(x) = \\sum_i y_i \\prod_{j \\ne i} \\frac{x - x_j}{x_i - x_j}\n\\]\n\n\nB. Root Representation\nSolve equations or check identities by working modulo a polynomial. Used in finite fields and coding theory (e.g., Reed-Solomon).\n\n\n5. Transform Techniques\nTransforms convert problems to simpler domains where operations become efficient.\n\n\n\n\n\n\n\n\n\nTransform\nConverts\nKey Property\nUsed In\n\n\n\n\nFFT / NTT\nTime ↔︎ Frequency\nConvolution → Multiplication\nSignal, polynomial mult\n\n\nZ-Transform\nSequence ↔︎ Function\nRecurrence solving\nDSP, control\n\n\nLaplace Transform\nFunction ↔︎ Algebraic\nDiff. eq. → Algebraic eq.\nContinuous systems\n\n\nWalsh-Hadamard Transform\nBoolean vectors\nXOR convolution\nSubset sum, SOS DP\n\n\n\nExample: Subset Convolution via FWT\nFor all subsets ( S ): \\[\nf'(S) = \\sum_{T \\subseteq S} f(T)\n\\]\nUse Fast Walsh-Hadamard Transform (FWHT) to compute in ( O\\(n2^n\\) ) instead of ( O\\(3^n\\) ).\n\n\n6. Matrix Tricks\nMatrix algebra enables transformations and compact formulations.\n\nMatrix exponentiation: solve recurrences in \\(O(\\log n)\\)\nDiagonalization: \\(A = P D P^{-1}\\), then \\(A^k = P D^k P^{-1}\\)\nFast power: speeds up Fibonacci, linear recurrences, Markov chains\n\nExample: Fibonacci\n\\[\n\\begin{bmatrix}\nF_{n+1} \\\\\nF_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0\n\\end{bmatrix}^n\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\n\\]\n\n\n7. Tiny Code\nPolynomial Multiplication via FFT (Pseudo-C):\n// Outline using complex FFT library\nfft(A, false);\nfft(B, false);\nfor (int i = 0; i &lt; n; i++)\n    C[i] = A[i] * B[i];\nfft(C, true); // inverse\nMatrix Power (Fibonacci):\nvoid matmul(long long A[2][2], long long B[2][2]) {\n    long long C[2][2] = {{0}};\n    for (int i=0;i&lt;2;i++)\n        for (int j=0;j&lt;2;j++)\n            for (int k=0;k&lt;2;k++)\n                C[i][j] += A[i][k]*B[k][j];\n    memcpy(A, C, sizeof(C));\n}\n\nvoid matpow(long long A[2][2], int n) {\n    long long R[2][2] = {{1,0},{0,1}};\n    while(n){\n        if(n&1) matmul(R,A);\n        matmul(A,A);\n        n&gt;&gt;=1;\n    }\n    memcpy(A, R, sizeof(R));\n}\n\n\n8. Summary\n\n\n\n\n\n\n\n\nTechnique\nPurpose\nSpeedup\n\n\n\n\nAlgebraic Identities\nSimplify expressions\nConstant factor\n\n\nGenerating Functions\nSolve recurrences\nConceptual\n\n\nFFT / Convolution\nCombine sequences fast\n(O\\(n^2\\) O\\(n \\log n\\))\n\n\nPolynomial Interpolation\nReconstruction\n(O\\(n^2\\) O\\(n \\log^2 n\\))\n\n\nMatrix Tricks\nAccelerate recurrences\n(O(n) O\\(\\log n\\))\n\n\n\n\n\nWhy It Matters\nAlgebra turns computation into structure. By rewriting problems in algebraic form, you reveal hidden symmetries, exploit fast transforms, and find elegant solutions. It’s not magic , it’s the math beneath performance.\n\n“The smartest code is often the one that solves itself on paper first.”\n\n\n\nTry It Yourself\n\nMultiply two polynomials using FFT.\nRepresent Fibonacci as a matrix and compute \\(F_{100}\\).\nUse generating functions to count coin change ways.\nImplement subset sum via Walsh-Hadamard transform.\nDerive a recurrence and solve it algebraically.\n\nUnderstanding algebraic tricks makes you not just a coder, but a mathematical engineer , bending structure to will.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-7.-strings-and-text-algorithms",
    "href": "books/en-us/book.html#chapter-7.-strings-and-text-algorithms",
    "title": "The Book",
    "section": "Chapter 7. Strings and Text Algorithms",
    "text": "Chapter 7. Strings and Text Algorithms\n\n61. String Matching (KMP, Z, Rabin-Karp, Boyer-Moore)\nString matching is one of the oldest and most fundamental problems in computer science: given a text ( T ) of length ( n ) and a pattern ( P ) of length ( m ), find all positions where ( P ) appears in ( T ).\nThis section walks you through both naive and efficient algorithms , from the straightforward brute-force method to elegant linear-time solutions like KMP and Z-algorithm, and clever heuristics like Boyer-Moore and Rabin-Karp.\n\n1. Problem Setup\nWe’re given:\n\nText: \\(T = t_1 t_2 \\ldots t_n\\)- Pattern: \\(P = p_1 p_2 \\ldots p_m\\) Goal: find all ( i ) such that \\[\nT[i \\ldots i+m-1] = P[1 \\ldots m]\n\\]\n\nNaive solution: compare ( P ) with every substring of ( T ) Time complexity: ( O(nm) )\nWe’ll now see how to reduce it to ( O(n + m) ) or close.\n\n\n2. Knuth-Morris-Pratt (KMP)\nKMP avoids rechecking characters by precomputing overlaps within the pattern.\nIt builds a prefix-function (also called failure function), which tells how much to shift when a mismatch happens.\n\n\nA. Prefix Function\nFor each position ( i ), compute \\(\\pi[i]\\) = length of longest prefix that’s also a suffix of ( P[1..i] ).\nExample: Pattern ababc\n\n\n\ni\nP[i]\nπ[i]\n\n\n\n\n1\na\n0\n\n\n2\nb\n0\n\n\n3\na\n1\n\n\n4\nb\n2\n\n\n5\nc\n0\n\n\n\n\n\nB. Search Phase\nUse \\(\\pi[]\\) to skip mismatched prefixes in the text.\nTime Complexity: ( O(n + m) ) Space: ( O(m) )\nTiny Code (C)\nvoid compute_pi(char *p, int m, int pi[]) {\n    pi[0] = 0;\n    for (int i = 1, k = 0; i &lt; m; i++) {\n        while (k &gt; 0 && p[k] != p[i]) k = pi[k-1];\n        if (p[k] == p[i]) k++;\n        pi[i] = k;\n    }\n}\n\nvoid kmp_search(char *t, char *p) {\n    int n = strlen(t), m = strlen(p);\n    int pi[m]; compute_pi(p, m, pi);\n    for (int i = 0, k = 0; i &lt; n; i++) {\n        while (k &gt; 0 && p[k] != t[i]) k = pi[k-1];\n        if (p[k] == t[i]) k++;\n        if (k == m) {\n            printf(\"Found at %d\\n\", i - m + 1);\n            k = pi[k-1];\n        }\n    }\n}\n\n\n3. Z-Algorithm\nZ-algorithm computes the Z-array,\nwhere \\(Z[i]\\) = length of the longest substring starting at \\(i\\) that matches the prefix of \\(P\\).\nTo match \\(P\\) in \\(T\\), build the string:\n\\[\nS = P + \\# + T\n\\]\nThen every \\(i\\) where \\(Z[i] = |P|\\) corresponds to a match.\nTime: \\(O(n + m)\\)\nSimple and elegant.\nExample:\nP = \"aba\", T = \"ababa\"\nS = \"aba#ababa\"\nZ = [0,0,1,0,3,0,1,0]\nMatch at index 0, 2\n\n\n4. Rabin-Karp (Rolling Hash)\nInstead of comparing strings character-by-character, compute a hash for each window in ( T ), and compare hashes.\n\\[\nh(s_1s_2\\ldots s_m) = (s_1b^{m-1} + s_2b^{m-2} + \\ldots + s_m) \\bmod M\n\\]\nUse a rolling hash to update in ( O(1) ) per shift.\nTime: average ( O(n + m) ), worst ( O(nm) ) Good for multiple pattern search.\nTiny Code (Rolling Hash)\n#define B 256\n#define M 101\n\nvoid rabin_karp(char *t, char *p) {\n    int n = strlen(t), m = strlen(p);\n    int h = 1, pHash = 0, tHash = 0;\n    for (int i = 0; i &lt; m-1; i++) h = (h*B) % M;\n    for (int i = 0; i &lt; m; i++) {\n        pHash = (B*pHash + p[i]) % M;\n        tHash = (B*tHash + t[i]) % M;\n    }\n    for (int i = 0; i &lt;= n-m; i++) {\n        if (pHash == tHash && strncmp(&t[i], p, m) == 0)\n            printf(\"Found at %d\\n\", i);\n        if (i &lt; n-m)\n            tHash = (B*(tHash - t[i]*h) + t[i+m]) % M;\n        if (tHash &lt; 0) tHash += M;\n    }\n}\n\n\n5. Boyer-Moore (Heuristic Skipping)\nBoyer-Moore compares from right to left and uses two heuristics:\n\nBad Character Rule When mismatch at ( j ), shift pattern so next occurrence of ( T[i] ) in ( P ) aligns.\nGood Suffix Rule Shift pattern so a suffix of matched portion aligns with another occurrence.\n\nTime: ( O(n/m) ) on average Practical and fast, especially for English text.\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nIdea\nBest For\n\n\n\n\nNaive\n(O(nm))\n(O(1))\nDirect compare\nSimple cases\n\n\nKMP\n(O(n+m))\n(O(m))\nPrefix overlap\nGeneral use\n\n\nZ\n(O(n+m))\n(O(n+m))\nPrefix matching\nPattern prep\n\n\nRabin-Karp\n(O(n+m)) avg\n(O(1))\nHashing\nMulti-pattern\n\n\nBoyer-Moore\n(O(n/m)) avg\n(O\\(m+\\sigma\\))\nRight-to-left skip\nLong texts\n\n\n\n\n\nWhy It Matters\nString matching powers text editors, DNA search, spam filters, and search engines. These algorithms show how structure and clever preprocessing turn brute force into elegance.\n\n“To find is human, to match efficiently is divine.”\n\n\n\nTry It Yourself\n\nImplement KMP and print all matches in a sentence.\nUse Rabin-Karp to find multiple keywords.\nCompare running times on large text files.\nModify KMP for case-insensitive matching.\nVisualize prefix function computation step-by-step.\n\nBy mastering these, you’ll wield the foundation of pattern discovery , the art of finding order in streams of symbols.\n\n\n\n62. Multi-Pattern Search (Aho-Corasick)\nSo far, we’ve matched one pattern against a text. But what if we have many patterns , say, a dictionary of keywords , and we want to find all occurrences of all patterns in a single pass?\nThat’s where the Aho-Corasick algorithm shines. It builds a trie with failure links, turning multiple patterns into one efficient automaton. Think of it as “KMP for many words at once.”\n\n1. Problem Setup\nGiven:\n\nA text ( T ) of length ( n )- A set of patterns \\({ P_1, P_2, \\ldots, P_k }\\) with total length \\(m = \\sum |P_i|\\)\n\nGoal: find all occurrences of every \\(P_i\\) in ( T ).\nNaive solution: Run KMP for each pattern , ( O(kn) )\nBetter idea: Merge all patterns into a trie, and use failure links to transition on mismatches.\nAho-Corasick achieves O(n + m + z), where ( z ) = number of matches reported.\n\n\n2. Trie Construction\nEach pattern is inserted into a trie node-by-node.\nExample Patterns:\nhe, she, his, hers\nTrie:\n(root)\n ├─ h ─ e*\n │   └─ r ─ s*\n ├─ s ─ h ─ e*\n └─ h ─ i ─ s*\nEach node may mark an output (end of pattern).\n\n\n3. Failure Links\nFailure link of a node points to the longest proper suffix that’s also a prefix in the trie.\nThese links let us “fall back” like KMP.\nWhen mismatch happens, follow failure link to find next possible match.\n\n\nBuilding Failure Links (BFS)\n\nRoot’s failure = null\nChildren of root → failure = root\nBFS over nodes:\n\nFor each edge ( (u, c) → v ): follow failure links from ( u ) until you find ( f ) with edge ( c ) then \\(v.\\text{fail} = f.c\\)\n\n\n\n\nExample\nFor “he”, “she”, “his”, “hers”:\n\nfail(\"he\") = root- fail(\"hers\") = \"rs\" path invalid → fallback to \"s\" if exists So failure links connect partial suffixes.\n\n\n\n4. Matching Phase\nNow we can process the text in one pass:\nstate = root\nfor each character c in text:\n    while state has no child c and state != root:\n        state = state.fail\n    if state has child c:\n        state = state.child[c]\n    else:\n        state = root\n    if state.output:\n        report matches at this position\nEach transition costs O(1) amortized. No backtracking , fully linear time.\n\n\n5. Example Walkthrough\nPatterns: he, she, his, hers Text: ahishers\nAt each character:\na → root (no match)\nh → go to h\ni → go to hi\ns → go to his → output \"his\"\nh → fallback → h\ne → he → output \"he\"\nr → her → continue\ns → hers → output \"hers\"\nOutputs: \"his\", \"he\", \"hers\"\n\n\n6. Tiny Code (C Implementation Sketch)\n#define ALPHA 26\n\ntypedef struct Node {\n    struct Node *next[ALPHA];\n    struct Node *fail;\n    int out;\n} Node;\n\nNode* newNode() {\n    Node *n = calloc(1, sizeof(Node));\n    return n;\n}\n\nvoid insert(Node *root, char *p) {\n    for (int i = 0; p[i]; i++) {\n        int c = p[i] - 'a';\n        if (!root-&gt;next[c]) root-&gt;next[c] = newNode();\n        root = root-&gt;next[c];\n    }\n    root-&gt;out = 1;\n}\n\nvoid build_failures(Node *root) {\n    Node *q[10000];\n    int front=0, back=0;\n    root-&gt;fail = root;\n    q[back++] = root;\n    while (front &lt; back) {\n        Node *u = q[front++];\n        for (int c=0; c&lt;ALPHA; c++) {\n            Node *v = u-&gt;next[c];\n            if (!v) continue;\n            Node *f = u-&gt;fail;\n            while (f != root && !f-&gt;next[c]) f = f-&gt;fail;\n            if (f-&gt;next[c] && f-&gt;next[c] != v) v-&gt;fail = f-&gt;next[c];\n            else v-&gt;fail = root;\n            if (v-&gt;fail-&gt;out) v-&gt;out = 1;\n            q[back++] = v;\n        }\n    }\n}\n\n\n7. Complexity\n\n\n\nPhase\nTime\nSpace\n\n\n\n\nTrie Build\n( O(m) )\n( O(m) )\n\n\nFailure Links\n( O(m) )\n( O(m) )\n\n\nSearch\n( O(n + z) )\n( O(1) )\n\n\n\nTotal: O(n + m + z)\n\n\n8. Summary\n\n\n\nStep\nPurpose\n\n\n\n\nTrie\nMerge patterns\n\n\nFail Links\nHandle mismatches\n\n\nOutputs\nCollect matches\n\n\nBFS\nBuild efficiently\n\n\nOne Pass\nMatch all patterns\n\n\n\n\n\nWhy It Matters\nAho-Corasick is the core of:\n\nSpam filters- Intrusion detection (e.g., Snort IDS)- Keyword search in compilers- DNA sequence scanners It’s a masterclass in blending automata theory with practical efficiency.\n\n\n“Why search one word at a time when your algorithm can read the whole dictionary?”\n\n\n\nTry It Yourself\n\nBuild an automaton for words {“he”, “she”, “hers”} and trace it manually.\nModify code for uppercase letters.\nExtend to report overlapping matches.\nMeasure runtime vs. naive multi-search.\nVisualize the failure links in a graph.\n\nOnce you grasp Aho-Corasick, you’ll see pattern search not as a loop , but as a machine that reads and recognizes.\n\n\n\n63. Suffix Structures (Suffix Array, Suffix Tree, LCP)\nSuffix-based data structures are among the most powerful tools in string algorithms. They enable fast searching, substring queries, pattern matching, and lexicographic operations , all from one fundamental idea:\n\nRepresent all suffixes of a string in a structured form.\n\nIn this section, we explore three key constructs:\n\nSuffix Array (SA) - lexicographically sorted suffix indices- Longest Common Prefix (LCP) array - shared prefix lengths between neighbors- Suffix Tree - compressed trie of all suffixes Together, they power many advanced algorithms in text processing, bioinformatics, and compression.\n\n\n1. Suffix Array (SA)\nA suffix array stores all suffixes of a string in lexicographic order, represented by their starting indices.\nExample: String banana$ All suffixes:\n\n\n\nIndex\nSuffix\n\n\n\n\n0\nbanana$\n\n\n1\nanana$\n\n\n2\nnana$\n\n\n3\nana$\n\n\n4\nna$\n\n\n5\na$\n\n\n6\n$\n\n\n\nSort them:\n\n\n\nSorted Order\nSuffix\nIndex\n\n\n\n\n0\n$\n6\n\n\n1\na$\n5\n\n\n2\nana$\n3\n\n\n3\nanana$\n1\n\n\n4\nbanana$\n0\n\n\n5\nna$\n4\n\n\n6\nnana$\n2\n\n\n\nSuffix Array: [6, 5, 3, 1, 0, 4, 2]\n\n\nConstruction (Prefix Doubling)\nWe iteratively sort suffixes by first 2ⁱ characters, using radix sort on pairs of ranks.\nSteps:\n\nAssign initial rank by character.\nSort by (rank[i], rank[i+k]).\nRepeat doubling \\(k \\leftarrow 2k\\) until all ranks distinct.\n\nTime Complexity: ( O\\(n \\log n\\) ) Space: ( O(n) )\nTiny Code (C, Sketch)\ntypedef struct { int idx, rank[2]; } Suffix;\nint cmp(Suffix a, Suffix b) {\n    return (a.rank[0]==b.rank[0]) ? (a.rank[1]-b.rank[1]) : (a.rank[0]-b.rank[0]);\n}\n\nvoid buildSA(char *s, int n, int sa[]) {\n    Suffix suf[n];\n    for (int i = 0; i &lt; n; i++) {\n        suf[i].idx = i;\n        suf[i].rank[0] = s[i];\n        suf[i].rank[1] = (i+1&lt;n) ? s[i+1] : -1;\n    }\n    for (int k = 2; k &lt; 2*n; k *= 2) {\n        qsort(suf, n, sizeof(Suffix), cmp);\n        int r = 0, rank[n]; rank[suf[0].idx]=0;\n        for (int i=1;i&lt;n;i++) {\n            if (suf[i].rank[0]!=suf[i-1].rank[0] || suf[i].rank[1]!=suf[i-1].rank[1]) r++;\n            rank[suf[i].idx]=r;\n        }\n        for (int i=0;i&lt;n;i++){\n            suf[i].rank[0] = rank[suf[i].idx];\n            suf[i].rank[1] = (suf[i].idx+k/2&lt;n)?rank[suf[i].idx+k/2]:-1;\n        }\n    }\n    for (int i=0;i&lt;n;i++) sa[i]=suf[i].idx;\n}\n\n\n2. Longest Common Prefix (LCP)\nThe LCP array stores the length of the longest common prefix between consecutive suffixes in SA order.\nExample: banana$\n\n\n\nSA\nSuffix\nLCP\n\n\n\n\n6\n$\n0\n\n\n5\na$\n0\n\n\n3\nana$\n1\n\n\n1\nanana$\n3\n\n\n0\nbanana$\n0\n\n\n4\nna$\n0\n\n\n2\nnana$\n2\n\n\n\nSo LCP = [0,0,1,3,0,0,2]\n\n\nKasai’s Algorithm (Build in O(n))\nWe compute LCP in one pass using inverse SA:\nvoid buildLCP(char *s, int n, int sa[], int lcp[]) {\n    int rank[n];\n    for (int i=0;i&lt;n;i++) rank[sa[i]]=i;\n    int k=0;\n    for (int i=0;i&lt;n;i++) {\n        if (rank[i]==n-1) { k=0; continue; }\n        int j = sa[rank[i]+1];\n        while (i+k&lt;n && j+k&lt;n && s[i+k]==s[j+k]) k++;\n        lcp[rank[i]]=k;\n        if (k&gt;0) k--;\n    }\n}\nTime Complexity: ( O(n) )\n\n\n3. Suffix Tree\nA suffix tree is a compressed trie of all suffixes.\nEach edge holds a substring interval, not individual characters. This gives:\n\nConstruction in ( O(n) ) (Ukkonen’s algorithm)- Pattern search in ( O(m) )- Many advanced uses (e.g., longest repeated substring) Example: String: banana$ Suffix tree edges:\n\n(root)\n ├─ b[0:0] → ...\n ├─ a[1:1] → ...\n ├─ n[2:2] → ...\nEdges compress consecutive letters into intervals like [start:end].\n\n\nComparison\n\n\n\nStructure\nSpace\nBuild Time\nSearch\n\n\n\n\nSuffix Array\n( O(n) )\n( O\\(n \\log n\\) )\n( O\\(m \\log n\\) )\n\n\nLCP Array\n( O(n) )\n( O(n) )\nRange queries\n\n\nSuffix Tree\n( O(n) )\n( O(n) )\n( O(m) )\n\n\n\nSuffix Array + LCP ≈ compact Suffix Tree.\n\n\n4. Applications\n\nSubstring search - binary search in SA\nLongest repeated substring - max(LCP)\nLexicographic order - direct from SA\nDistinct substrings count = ( n(n+1)/2 - LCP[i] )\nPattern frequency - range query in SA using LCP\n\n\n\n5. Tiny Code (Search via SA)\nint searchSA(char *t, int n, char *p, int sa[]) {\n    int l=0, r=n-1, m=strlen(p);\n    while (l &lt;= r) {\n        int mid = (l+r)/2;\n        int cmp = strncmp(t+sa[mid], p, m);\n        if (cmp==0) return sa[mid];\n        else if (cmp&lt;0) l=mid+1;\n        else r=mid-1;\n    }\n    return -1;\n}\n\n\n6. Summary\n\n\n\nConcept\nPurpose\nComplexity\n\n\n\n\nSuffix Array\nSorted suffix indices\n( O\\(n \\log n\\) )\n\n\nLCP Array\nAdjacent suffix overlap\n( O(n) )\n\n\nSuffix Tree\nCompressed trie of suffixes\n( O(n) )\n\n\n\nTogether they form the core of advanced string algorithms.\n\n\nWhy It Matters\nSuffix structures reveal hidden order in strings. They turn raw text into searchable, analyzable data , ideal for compression, search engines, and DNA analysis.\n\n“All suffixes, perfectly sorted , the DNA of text.”\n\n\n\nTry It Yourself\n\nBuild suffix array for banana$ by hand.\nWrite code to compute LCP and longest repeated substring.\nSearch multiple patterns using binary search on SA.\nCount distinct substrings from SA + LCP.\nCompare SA-based vs. tree-based search performance.\n\nMastering suffix structures equips you to tackle problems that were once “too big” for brute force , now solvable with elegance and order.\n\n\n\n64. Palindromes and Periodicity (Manacher)\nPalindromes are symmetric strings that read the same forwards and backwards , like “level”, “racecar”, or “madam”. They arise naturally in text analysis, bioinformatics, and even in data compression.\nThis section introduces efficient algorithms to detect and analyze palindromic structure and periodicity in strings, including the legendary Manacher’s Algorithm, which finds all palindromic substrings in linear time.\n\n1. What Is a Palindrome?\nA string ( S ) is a palindrome if: \\[\nS[i] = S[n - i + 1] \\quad \\text{for all } i\n\\]\nExamples:\n\n\"abba\" is even-length palindrome- \"aba\" is odd-length palindrome A string may contain many palindromic substrings , our goal is to find all centers efficiently.\n\n\n\n2. Naive Approach\nFor each center (between characters or at characters), expand outward while characters match.\nfor each center c:\n    expand left, right while S[l] == S[r]\nComplexity: ( O\\(n^2\\) ) , too slow for large strings.\nWe need something faster , that’s where Manacher’s Algorithm steps in.\n\n\n3. Manacher’s Algorithm (O(n))\nManacher’s Algorithm finds the radius of the longest palindrome centered at each position in linear time.\nIt cleverly reuses previous computations using mirror symmetry and a current right boundary.\n\n\nStep-by-Step\n\nPreprocess string to handle even-length palindromes: Insert # between characters.\nExample:\nS = \"abba\"\nT = \"^#a#b#b#a#$\"\n(^ and $ are sentinels)\nMaintain:\n\nC: center of rightmost palindrome - R: right boundary - P[i]: palindrome radius at i\n\nFor each position i:\n\nmirror position mirror = 2*C - i - initialize P[i] = min(R - i, P[mirror]) - expand around i while characters match - if new palindrome extends past R, update C and R\n\nThe maximum value of P[i] gives the longest palindrome.\n\n\n\nExample\nS = \"abba\"\nT = \"^#a#b#b#a#$\"\nP = [0,0,1,0,3,0,3,0,1,0,0]\nLongest radius = 3 → \"abba\"\nTiny Code (C Implementation)\nint manacher(char *s) {\n    int n = strlen(s);\n    char t[2*n + 3];\n    int p[2*n + 3];\n    int m = 0;\n    t[m++] = '^';\n    for (int i=0;i&lt;n;i++) {\n        t[m++] = '#';\n        t[m++] = s[i];\n    }\n    t[m++] = '#'; t[m++] = '$';\n    t[m] = '\\0';\n    \n    int c = 0, r = 0, maxLen = 0;\n    for (int i=1; i&lt;m-1; i++) {\n        int mirror = 2*c - i;\n        if (i &lt; r)\n            p[i] = (r - i &lt; p[mirror]) ? (r - i) : p[mirror];\n        else p[i] = 0;\n        while (t[i + 1 + p[i]] == t[i - 1 - p[i]])\n            p[i]++;\n        if (i + p[i] &gt; r) {\n            c = i;\n            r = i + p[i];\n        }\n        if (p[i] &gt; maxLen) maxLen = p[i];\n    }\n    return maxLen;\n}\nTime Complexity: ( O(n) ) Space: ( O(n) )\n\n\n4. Periodicity and Repetition\nA string ( S ) has a period ( p ) if: \\[\nS[i] = S[i + p] \\text{ for all valid } i\n\\]\nExample: abcabcabc has period 3 (abc).\nChecking Periodicity:\n\nBuild prefix function (as in KMP).\nLet ( n = |S| ), \\(p = n - \\pi[n-1]\\).\nIf \\(n \\mod p = 0\\), period = ( p ).\n\nExample:\nS = \"ababab\"\nπ = [0,0,1,2,3,4]\np = 6 - 4 = 2\n6 mod 2 = 0 → periodic\nTiny Code (Check Periodicity)\nint period(char *s) {\n    int n = strlen(s), pi[n];\n    pi[0]=0;\n    for(int i=1,k=0;i&lt;n;i++){\n        while(k&gt;0 && s[k]!=s[i]) k=pi[k-1];\n        if(s[k]==s[i]) k++;\n        pi[i]=k;\n    }\n    int p = n - pi[n-1];\n    return (n % p == 0) ? p : n;\n}\n\n\n5. Applications\n\nPalindrome Queries: is substring ( S[l:r] ) palindrome? → precompute radii- Longest Palindromic Substring- DNA Symmetry Analysis- Pattern Compression / Period Detection- String Regularity Tests\n\n\n\n6. Summary\n\n\n\nConcept\nPurpose\nTime\n\n\n\n\nNaive Expand\nSimple palindrome check\n( O\\(n^2\\) )\n\n\nManacher\nLongest palindromic substring\n( O(n) )\n\n\nKMP Prefix\nPeriod detection\n( O(n) )\n\n\n\n\n\nWhy It Matters\nPalindromes reveal hidden symmetries. Manacher’s algorithm is a gem , a linear-time mirror-based solution to a quadratic problem.\n\n“In every word, there may hide a reflection.”\n\n\n\nTry It Yourself\n\nRun Manacher’s algorithm on \"abacdfgdcaba\".\nModify code to print all palindromic substrings.\nUse prefix function to find smallest period.\nCombine both to find palindromic periodic substrings.\nCompare runtime vs. naive expand method.\n\nUnderstanding palindromes and periodicity teaches how structure emerges from repetition , a central theme in all of algorithmic design.\n\n\n\n65. Edit Distance and Alignment\nEdit distance measures how different two strings are , the minimal number of operations needed to turn one into the other. It’s a cornerstone of spell checking, DNA sequence alignment, plagiarism detection, and fuzzy search.\nThe most common form is the Levenshtein distance, using:\n\nInsertion (add a character)- Deletion (remove a character)- Substitution (replace a character) We’ll also touch on alignment, which generalizes this idea with custom scoring and penalties.\n\n\n1. Problem Definition\nGiven two strings ( A ) and ( B ), find the minimum number of edits to convert \\(A \\to B\\).\nIf ( A = “kitten” ) ( B = “sitting” )\nOne optimal sequence:\nkitten → sitten (substitute 'k'→'s')\nsitten → sittin (substitute 'e'→'i')\nsittin → sitting (insert 'g')\nSo edit distance = 3.\n\n\n2. Dynamic Programming Solution\nLet \\(dp[i][j]\\) be the minimum edits to convert \\(A[0..i-1] \\to B[0..j-1]\\).\nRecurrence: \\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j-1], & \\text{if } A[i-1] = B[j-1], \\\\\n1 + \\min\\big(dp[i-1][j],\\, dp[i][j-1],\\, dp[i-1][j-1]\\big), & \\text{otherwise}\n\\end{cases}\n\\]\nWhere: - \\(dp[i-1][j]\\): delete from \\(A\\) - \\(dp[i][j-1]\\): insert into \\(A\\) - \\(dp[i-1][j-1]\\): substitute\nBase cases: \\[\ndp[0][j] = j,\\quad dp[i][0] = i\n\\]\nTime complexity: \\(O(|A||B|)\\)\n\n\nExample\nA = kitten, B = sitting\n\n\n\n\n“”\ns\ni\nt\nt\ni\nn\ng\n\n\n\n\n“”\n0\n1\n2\n3\n4\n5\n6\n7\n\n\nk\n1\n1\n2\n3\n4\n5\n6\n7\n\n\ni\n2\n2\n1\n2\n3\n4\n5\n6\n\n\nt\n3\n3\n2\n1\n2\n3\n4\n5\n\n\nt\n4\n4\n3\n2\n1\n2\n3\n4\n\n\ne\n5\n5\n4\n3\n2\n2\n3\n4\n\n\nn\n6\n6\n5\n4\n3\n3\n2\n3\n\n\n\nEdit distance = 3\nTiny Code (C)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#define MIN3(a,b,c) ((a&lt;b)?((a&lt;c)?a:c):((b&lt;c)?b:c))\n\nint edit_distance(char *A, char *B) {\n    int n = strlen(A), m = strlen(B);\n    int dp[n+1][m+1];\n    for (int i=0;i&lt;=n;i++) dp[i][0]=i;\n    for (int j=0;j&lt;=m;j++) dp[0][j]=j;\n    for (int i=1;i&lt;=n;i++)\n        for (int j=1;j&lt;=m;j++)\n            if (A[i-1]==B[j-1])\n                dp[i][j]=dp[i-1][j-1];\n            else\n                dp[i][j]=1+MIN3(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]);\n    return dp[n][m];\n}\n\nint main() {\n    printf(\"%d\\n\", edit_distance(\"kitten\",\"sitting\")); // 3\n}\n\n\n3. Space Optimization\nWe only need the previous row to compute the current row.\nSo,\nSpace complexity: \\(O(\\min(|A|, |B|))\\)\nint edit_distance_opt(char *A, char *B) {\n    int n=strlen(A), m=strlen(B);\n    int prev[m+1], curr[m+1];\n    for(int j=0;j&lt;=m;j++) prev[j]=j;\n    for(int i=1;i&lt;=n;i++){\n        curr[0]=i;\n        for(int j=1;j&lt;=m;j++){\n            if(A[i-1]==B[j-1]) curr[j]=prev[j-1];\n            else curr[j]=1+MIN3(prev[j], curr[j-1], prev[j-1]);\n        }\n        memcpy(prev,curr,sizeof(curr));\n    }\n    return prev[m];\n}\n\n\n4. Alignment\nAlignment shows which characters correspond between two strings. Used in bioinformatics (e.g., DNA sequence alignment).\nEach operation has a cost:\n\nMatch: 0\nMismatch: 1\nGap (insert/delete): 1 We fill the DP table similarly, but track choices to trace back alignment.\n\n\n\nExample Alignment\nA: kitten-\nB: sitt-ing\nWe can visualize the transformation path by backtracking dp table.\n\n\nScoring Alignment (General Form)\nWe can generalize: \\[\ndp[i][j] = \\min \\begin{cases}\ndp[i-1][j-1] + cost(A_i,B_j) \\\ndp[i-1][j] + gap \\\ndp[i][j-1] + gap\n\\end{cases}\n\\]\nUsed in Needleman-Wunsch (global alignment) and Smith-Waterman (local alignment).\n\n\n5. Variants\n\nDamerau-Levenshtein: adds transposition (swap adjacent chars)- Hamming Distance: only substitutions, equal-length strings- Weighted Distance: different operation costs- Local Alignment: only best matching substrings\n\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\nMethod\nOperations\nTime\nUse\n\n\n\n\nLevenshtein\ninsert, delete, replace\n(O(nm))\nSpell check, fuzzy search\n\n\nHamming\nsubstitution only\n(O(n))\nDNA, binary strings\n\n\nAlignment (Needleman-Wunsch)\nwith scoring\n(O(nm))\nBioinformatics\n\n\nLocal Alignment (Smith-Waterman)\nbest substring\n(O(nm))\nDNA regions\n\n\n\n\n\nWhy It Matters\nEdit distance transforms “difference” into data. It quantifies how far apart two strings are, enabling flexible, robust comparisons.\n\n“Similarity isn’t perfection , it’s the cost of becoming alike.”\n\n\n\nTry It Yourself\n\nCompute edit distance between “intention” and “execution”.\nTrace back operations to show alignment.\nModify costs (insertion=2, deletion=1, substitution=2) and compare results.\nImplement Hamming distance for equal-length strings.\nExplore Smith-Waterman for longest common substring.\n\nOnce you master edit distance, you can build tools that understand typos, align genomes, and search imperfectly , perfectly.\n\n\n\n66. Compression (Huffman, Arithmetic, LZ77, BWT)\nCompression algorithms let us encode information efficiently, reducing storage or transmission cost without losing meaning. They turn patterns and redundancy into shorter representations , the essence of data compression.\nThis section introduces the key families of lossless compression algorithms that form the backbone of formats like ZIP, PNG, and GZIP.\nWe’ll explore:\n\nHuffman Coding (prefix-free variable-length codes)\nArithmetic Coding (fractional interval encoding)\nLZ77 / LZ78 (dictionary-based methods)\nBurrows-Wheeler Transform (BWT) (reversible sorting transform)\n\n\n1. Huffman Coding\nHuffman coding assigns shorter codes to frequent symbols, and longer codes to rare ones , achieving optimal compression among prefix-free codes.\n\n\nA. Algorithm\n\nCount frequencies of all symbols.\nBuild a min-heap of nodes (symbol, freq).\nWhile heap size &gt; 1:\n\nExtract two smallest nodes a, b. - Create new node with freq = a.freq + b.freq. - Push back into heap.4. Assign 0 to left, 1 to right.\n\nTraverse tree , collect codes.\n\nEach symbol gets a unique prefix code (no code is prefix of another).\n\n\nB. Example\nText: ABRACADABRA\nFrequencies:\n\n\n\nSymbol\nCount\n\n\n\n\nA\n5\n\n\nB\n2\n\n\nR\n2\n\n\nC\n1\n\n\nD\n1\n\n\n\nBuilding tree gives codes like:\nA: 0  \nB: 101  \nR: 100  \nC: 1110  \nD: 1111\nEncoded text: 0 101 100 0 1110 0 1111 0 101 100 0 Compression achieved!\nTiny Code (C, Sketch)\ntypedef struct Node {\n    char ch;\n    int freq;\n    struct Node *left, *right;\n} Node;\nUse a min-heap (priority queue) to build the tree. Traverse recursively to print codewords.\nComplexity: (O\\(n \\log n\\))\n\n\n2. Arithmetic Coding\nInstead of mapping symbols to bit strings, arithmetic coding maps the entire message to a single number in [0,1).\nWe start with interval ([0,1)), and iteratively narrow it based on symbol probabilities.\n\n\nExample\nSymbols: {A: 0.5, B: 0.3, C: 0.2} Message: ABC\nIntervals:\nStart: [0, 1)\nA → [0, 0.5)\nB → [0.25, 0.4)\nC → [0.34, 0.37)\nFinal code = any number in [0.34, 0.37) (e.g. 0.35)\nDecoding reverses this process.\nAdvantage: achieves near-optimal entropy compression. Used in: JPEG2000, H.264\nTime Complexity: ( O(n) )\n\n\n3. LZ77 (Sliding Window Compression)\nLZ77 replaces repeated substrings with back-references (offset, length, next_char) pointing into a sliding window.\n\n\nExample\nText: abcabcabcx\nWindow slides; when abc repeats:\n(0,0,'a'), (0,0,'b'), (0,0,'c'),\n(3,3,'x')  // \"abc\" repeats from 3 chars back\nSo sequence is compressed as references to earlier substrings.\nUsed in: DEFLATE (ZIP, GZIP), PNG\nTime: ( O(n) ), Space: proportional to window size.\n\n\nTiny Code (Conceptual)\nstruct Token { int offset, length; char next; };\nSearch previous window for longest match before emitting token.\n\n\n4. LZ78 (Dictionary-Based)\nInstead of sliding window, LZ78 builds an explicit dictionary of substrings.\nAlgorithm:\n\nStart with empty dictionary.- Read input, find longest prefix in dictionary.- Output (index, next_char) and insert new entry. Example:\n\nInput: ABAABABAABAB\nOutput: (0,A), (0,B), (1,B), (2,A), (4,A), (3,B)\nUsed in: LZW (GIF, TIFF)\n\n\n5. Burrows-Wheeler Transform (BWT)\nBWT is not compression itself , it permutes text to cluster similar characters, making it more compressible by run-length or Huffman coding.\n\n\nSteps\n\nGenerate all rotations of string.\nSort them lexicographically.\nTake last column as output.\n\nExample: banana$\n\n\n\nRotations\nSorted\n\n\n\n\nbanana$\n\\(banana |\n| anana\\)b\n\n\na\\(banan   | na\\)bana\n\n\n\n\\(banana   | nana\\)ba\n\n\n\n\nLast column: annb$aa BWT(“banana\\(\") = \"annb\\)aa”\nReversible with index of original row.\nUsed in: bzip2, FM-index (bioinformatics)\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nIdea\nComplexity\nUse\n\n\n\n\nHuffman\nVariable-length prefix codes\n(O\\(n \\log n\\))\nGeneral compression\n\n\nArithmetic\nInterval encoding\n(O(n))\nNear-optimal entropy\n\n\nLZ77\nSliding window matches\n(O(n))\nZIP, PNG\n\n\nLZ78\nDictionary building\n(O(n))\nGIF, TIFF\n\n\nBWT\nPermute for clustering\n(O\\(n \\log n\\))\nbzip2\n\n\n\n\n\nWhy It Matters\nCompression algorithms reveal structure in data , they exploit patterns that humans can’t see. They’re also a window into information theory, showing how close we can get to the entropy limit.\n\n“To compress is to understand , every bit saved is a pattern found.”\n\n\n\nTry It Yourself\n\nBuild a Huffman tree for MISSISSIPPI.\nImplement a simple LZ77 encoder for repeating patterns.\nApply BWT and observe clustering of symbols.\nCompare Huffman and Arithmetic outputs on same input.\nExplore DEFLATE format combining LZ77 + Huffman.\n\nUnderstanding compression means learning to see redundancy , the key to efficient storage, transmission, and understanding itself.\n\n\n\n67. Cryptographic Hashes and Checksums\nIn algorithms, hashing helps us map data to fixed-size values. But when used for security and verification, hashing becomes a cryptographic tool. This section explores cryptographic hashes and checksums , algorithms that verify integrity, detect corruption, and secure data.\nWe’ll look at:\n\nSimple checksums (parity, CRC)- Cryptographic hash functions (MD5, SHA family, BLAKE3)- Properties like collision resistance and preimage resistance- Practical uses in verification, signing, and storage\n\n\n1. Checksums\nChecksums are lightweight methods to detect accidental errors in data (not secure against attackers). They’re used in filesystems, networking, and storage to verify integrity.\n\n\nA. Parity Bit\nAdds one bit to make total 1s even or odd. Used in memory or communication to detect single-bit errors.\nExample: Data = 1011 → has three 1s. Add parity bit 1 to make total 4 (even parity).\nLimitation: Only detects odd number of bit errors.\n\n\nB. Modular Sum (Simple Checksum)\nSum all bytes (mod 256 or 65536).\nTiny Code (C)\nuint8_t checksum(uint8_t *data, int n) {\n    uint32_t sum = 0;\n    for (int i = 0; i &lt; n; i++) sum += data[i];\n    return (uint8_t)(sum % 256);\n}\nUse: Simple file or packet validation.\n\n\nC. CRC (Cyclic Redundancy Check)\nCRCs treat bits as coefficients of a polynomial. Divide by a generator polynomial, remainder = CRC code.\nUsed in Ethernet, ZIP, and PNG.\nExample: CRC-32, CRC-16.\nFast hardware and table-driven implementations available.\nKey Property:\n\nDetects most burst errors- Not cryptographically secure\n\n\n\n2. Cryptographic Hash Functions\nA cryptographic hash function ( h(x) ) maps any input to a fixed-size output such that:\n\nDeterministic: same input → same output\nFast computation\nPreimage resistance: hard to find ( x ) given ( h(x) )\nSecond-preimage resistance: hard to find \\(x' \\neq x\\) with ( h(x’) = h(x) )\nCollision resistance: hard to find any two distinct inputs with same hash\n\n\n\n\nAlgorithm\nOutput (bits)\nNotes\n\n\n\n\nMD5\n128\nBroken (collisions found)\n\n\nSHA-1\n160\nDeprecated\n\n\nSHA-256\n256\nStandard (SHA-2 family)\n\n\nSHA-3\n256\nKeccak-based sponge\n\n\nBLAKE3\n256\nFast, parallel, modern\n\n\n\n\n\nExample\nh(\"hello\") = 2cf24dba5fb0a... (SHA-256)\nChange one letter, hash changes completely (avalanche effect):\nh(\"Hello\") = 185f8db32271f... \nEven small changes → big differences.\n\n\nTiny Code (C, using pseudo-interface)\n#include &lt;openssl/sha.h&gt;\n\nunsigned char hash[SHA256_DIGEST_LENGTH];\nSHA256((unsigned char*)\"hello\", 5, hash);\nPrint hash as hex string to verify.\n\n\n3. Applications\n\nData integrity: verify files (e.g., SHA256SUM)- Digital signatures: sign hashes, not raw data- Password storage: store hashes, not plaintext- Deduplication: detect identical files via hashes- Blockchain: link blocks with hash pointers- Git: stores objects via SHA-1 identifiers\n\n\n\n4. Hash Collisions\nA collision occurs when ( h(x) = h(y) ) for \\(x \\neq y\\). Good cryptographic hashes make this computationally infeasible.\nBy the birthday paradox, collisions appear after \\(2^{n/2}\\) operations for an ( n )-bit hash.\nHence, SHA-256 → ~\\(2^{128}\\) effort to collide.\n\n\n5. Checksums vs Hashes\n\n\n\nFeature\nChecksum\nCryptographic Hash\n\n\n\n\nGoal\nDetect errors\nEnsure integrity and authenticity\n\n\nResistance\nLow\nHigh\n\n\nOutput Size\nSmall\n128-512 bits\n\n\nPerformance\nVery fast\nFast but secure\n\n\nExample\nCRC32\nSHA-256, BLAKE3\n\n\n\n\n\nWhy It Matters\nChecksums catch accidental corruption, hashes protect against malicious tampering. Together, they guard the trustworthiness of data , the foundation of secure systems.\n\n“Integrity is invisible , until it’s lost.”\n\n\n\nTry It Yourself\n\nCompute CRC32 of a text file, flip one bit, and recompute.\nUse sha256sum to verify file integrity.\nExperiment: change one character in input, observe avalanche.\nCompare performance of SHA-256 and BLAKE3.\nResearch how Git uses SHA-1 to track versions.\n\nBy learning hashes, you master one of the pillars of security , proof that something hasn’t changed, even when everything else does.\n\n\n\n68. Approximate and Streaming Matching\nExact string matching (like KMP or Boyer-Moore) demands perfect alignment between pattern and text. But what if errors, noise, or incomplete data exist?\nThat’s where approximate matching and streaming matching come in. These algorithms let you search efficiently even when:\n\nThe pattern might contain typos or mutations- The text arrives in a stream (too large to store entirely)- You want to match “close enough,” not “exactly” They’re crucial in search engines, spell checkers, bioinformatics, and real-time monitoring systems.\n\n\n1. Approximate String Matching\nApproximate string matching finds occurrences of a pattern in a text allowing mismatches, insertions, or deletions , often measured by edit distance.\n\n\nA. Dynamic Programming (Levenshtein Distance)\nGiven two strings \\(A\\) and \\(B\\), the edit distance is the minimum number of insertions, deletions, or substitutions to turn \\(A\\) into \\(B\\).\nWe can build a DP table \\(dp[i][j]\\):\n\n\\(dp[i][0] = i\\) (delete all characters)\n\n\\(dp[0][j] = j\\) (insert all characters)\n\nIf \\(A[i] = B[j]\\), then \\(dp[i][j] = dp[i-1][j-1]\\)\n\nElse \\(dp[i][j] = 1 + \\min(dp[i-1][j],\\, dp[i][j-1],\\, dp[i-1][j-1])\\)\n\nTiny Code (C)\nint edit_distance(char *a, char *b) {\n    int n = strlen(a), m = strlen(b);\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= m; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= n; i++)\n        for (int j = 1; j &lt;= m; j++)\n            if (a[i-1] == b[j-1]) dp[i][j] = dp[i-1][j-1];\n            else dp[i][j] = 1 + fmin(fmin(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]);\n    return dp[n][m];\n}\nThis computes Levenshtein distance in ( O(nm) ) time.\n\n\nB. Bitap Algorithm (Shift-Or)\nWhen pattern length is small, Bitap uses bitmasks to track mismatches. It efficiently supports up to k errors and runs in near linear time for small patterns.\nUsed in grep -E, ag, and fuzzy searching systems.\nIdea: Maintain a bitmask where 1 = mismatch, 0 = match. Shift and OR masks as we scan text.\n\n\nC. k-Approximate Matching\nFind all positions where edit distance ≤ k. Efficient for small ( k ) (e.g., spell correction: edit distance ≤ 2).\nApplications:\n\nTypo-tolerant search- DNA sequence matching- Autocomplete systems\n\n\n\n2. Streaming Matching\nIn streaming, the text is too large or unbounded, so we must process input online. We can’t store everything , only summaries or sketches.\n\n\nA. Rolling Hash (Rabin-Karp style)\nMaintains a moving hash of recent characters. When new character arrives, update hash in ( O(1) ). Compare with pattern’s hash for possible match.\nGood for sliding window matching.\nExample:\nhash = (base * (hash - old_char * base^(m-1)) + new_char) % mod;\n\n\nB. Fingerprinting (Karp-Rabin Fingerprint)\nA compact representation of a substring. If fingerprints match, do full verification (avoid false positives). Used in streaming algorithms and chunking.\n\n\nC. Sketch-Based Matching\nAlgorithms like Count-Min Sketch or SimHash build summaries of large data. They help approximate similarity between streams.\nApplications:\n\nNear-duplicate detection (SimHash in Google)- Network anomaly detection- Real-time log matching\n\n\n\n3. Approximate Matching in Practice\n\n\n\n\n\n\n\n\nDomain\nUse Case\nAlgorithm\n\n\n\n\nSpell Checking\n“recieve” → “receive”\nEdit Distance\n\n\nDNA Alignment\nFind similar sequences\nSmith-Waterman\n\n\nAutocomplete\nSuggest close matches\nFuzzy Search\n\n\nLogs & Streams\nOnline pattern alerts\nStreaming Bitap, Karp-Rabin\n\n\nNear-Duplicate\nDetect similar text\nSimHash, MinHash\n\n\n\n\n\n4. Complexity\n\n\n\nAlgorithm\nTime\nSpace\nNotes\n\n\n\n\nLevenshtein DP\n(O(nm))\n(O(nm))\nExact distance\n\n\nBitap\n(O(n))\n(O(1))\nFor small patterns\n\n\nRolling Hash\n(O(n))\n(O(1))\nProbabilistic match\n\n\nSimHash\n(O(n))\n(O(1))\nApproximate similarity\n\n\n\n\n\nWhy It Matters\nReal-world data is messy , typos, noise, loss, corruption. Approximate matching lets you build algorithms that forgive errors and adapt to streams. It powers everything from search engines to genomics, ensuring your algorithms stay practical in an imperfect world.\n\n\nTry It Yourself\n\nCompute edit distance between “kitten” and “sitting.”\nImplement fuzzy search that returns words with ≤1 typo.\nUse rolling hash to detect repeated substrings in a stream.\nExperiment with SimHash to compare document similarity.\nObserve how small typos affect fuzzy vs exact search.\n\n\n\n\n69. Bioinformatics Alignment (Needleman-Wunsch, Smith-Waterman)\nIn bioinformatics, comparing DNA, RNA, or protein sequences is like comparing strings , but with biological meaning. Each sequence is made of letters (A, C, G, T for DNA; amino acids for proteins). To analyze similarity, scientists use sequence alignment algorithms that handle insertions, deletions, and substitutions.\nTwo fundamental methods dominate:\n\nNeedleman-Wunsch for global alignment- Smith-Waterman for local alignment\n\n\n1. Sequence Alignment\nAlignment means placing two sequences side by side to maximize matches and minimize gaps or mismatches.\nFor example:\nA C G T G A\n| | |   | |\nA C G A G A\nHere, mismatches and gaps may occur, but the alignment finds the best possible match under a scoring system.\n\n\nScoring System\nAlignment uses scores instead of just counts. Typical scheme:\n\nMatch: +1- Mismatch: -1- Gap (insertion or deletion): -2 You can adjust weights depending on the biological context.\n\n\n\n2. Needleman-Wunsch (Global Alignment)\nUsed when you want to align entire sequences , from start to end.\nIt uses dynamic programming to build a score table ( dp[i][j] ), where each cell represents the best score for aligning prefixes ( A[1..i] ) and ( B[1..j] ).\nRecurrence:\n\\[dp[i][j] = \\max\n\\begin{cases}\ndp[i-1][j-1] + \\text{score}(A_i, B_j) \\\ndp[i-1][j] + \\text{gap penalty} \\\ndp[i][j-1] + \\text{gap penalty}\n\\end{cases}\\]\nBase cases: \\[\ndp[0][j] = j \\times \\text{gap penalty}, \\quad dp[i][0] = i \\times \\text{gap penalty}\n\\]\nTiny Code (C)\nint max3(int a, int b, int c) {\n    return a &gt; b ? (a &gt; c ? a : c) : (b &gt; c ? b : c);\n}\n\nint needleman_wunsch(char *A, char *B, int match, int mismatch, int gap) {\n    int n = strlen(A), m = strlen(B);\n    int dp[n+1][m+1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = i * gap;\n    for (int j = 0; j &lt;= m; j++) dp[0][j] = j * gap;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= m; j++) {\n            int s = (A[i-1] == B[j-1]) ? match : mismatch;\n            dp[i][j] = max3(dp[i-1][j-1] + s, dp[i-1][j] + gap, dp[i][j-1] + gap);\n        }\n    }\n    return dp[n][m];\n}\nExample:\nA = \"ACGT\"\nB = \"AGT\"\nmatch = +1, mismatch = -1, gap = -2\nProduces optimal alignment:\nA C G T\nA - G T\n\n\n3. Smith-Waterman (Local Alignment)\nUsed when sequences may have similar segments, not full-length similarity. Perfect for finding motifs or conserved regions.\nRecurrence is similar, but with local reset to zero:\n\\[dp[i][j] = \\max\n\\begin{cases}\n0, \\\ndp[i-1][j-1] + \\text{score}(A_i, B_j), \\\ndp[i-1][j] + \\text{gap penalty}, \\\ndp[i][j-1] + \\text{gap penalty}\n\\end{cases}\\]\nFinal answer = maximum value in the table (not necessarily at the end).\nIt finds the best substring alignment.\n\n\nExample\nA = \"ACGTTG\"\nB = \"CGT\"\nSmith-Waterman finds best local match:\nA C G T\n  | | |\n  C G T\nUnlike global alignment, extra prefixes or suffixes are ignored.\n\n\n4. Variants and Extensions\n\n\n\n\n\n\n\n\nAlgorithm\nType\nNotes\n\n\n\n\nNeedleman-Wunsch\nGlobal\nAligns full sequences\n\n\nSmith-Waterman\nLocal\nFinds similar subsequences\n\n\nGotoh Algorithm\nGlobal\nUses affine gap penalty (opening + extension)\n\n\nBLAST\nHeuristic\nSpeeds up search for large databases\n\n\n\nBLAST (Basic Local Alignment Search Tool) uses word seeds and extension, trading exactness for speed , essential for large genome databases.\n\n\n5. Complexity\nBoth Needleman-Wunsch and Smith-Waterman run in:\n\nTime: ( O(nm) )- Space: ( O(nm) ) But optimized versions use banded DP or Hirschberg’s algorithm to cut memory to ( O(n + m) ).\n\n\n\nWhy It Matters\nSequence alignment bridges computer science and biology. It’s how we:\n\nCompare species- Identify genes- Detect mutations- Trace ancestry- Build phylogenetic trees The idea of “minimum edit cost” echoes everywhere , from spell checkers to DNA analysis.\n\n\n“In biology, similarity is a story. Alignment is how we read it.”\n\n\n\nTry It Yourself\n\nImplement Needleman-Wunsch for short DNA sequences.\nChange gap penalties , see how alignment shifts.\nCompare outputs from global and local alignment.\nUse real sequences from GenBank to test.\nExplore BLAST online and compare to exact alignment results.\n\n\n\n\n70. Text Indexing and Search Structures\nWhen text becomes large , think books, databases, or the entire web , searching naively for patterns (O(nm)) is far too slow. We need indexing structures that let us search fast, often in O(m) or O(log n) time.\nThis section covers the backbone of search engines and string processing:\n\nSuffix Arrays- Suffix Trees- Inverted Indexes- Tries and Prefix Trees- Compressed Indexes like FM-Index (Burrows-Wheeler)\n\n\n1. Why Index?\nA text index is like a table of contents , it doesn’t store the book, but lets you jump straight to what you want.\nIf you have a text of length ( n ), and you’ll run many queries, it’s worth building an index (even if it costs ( O\\(n \\log n\\) ) to build).\nWithout indexing: each query takes ( O(nm) ). With indexing: each query can take ( O(m) ) or less.\n\n\n2. Suffix Array\nA suffix array is a sorted array of all suffixes of a string.\nFor text \"banana\", suffixes are:\n0: banana  \n1: anana  \n2: nana  \n3: ana  \n4: na  \n5: a\nSorted lexicographically:\n5: a  \n3: ana  \n1: anana  \n0: banana  \n4: na  \n2: nana\nSuffix Array = [5, 3, 1, 0, 4, 2]\nTo search, binary search over the suffix array using your pattern , ( O\\(m \\log n\\) ).\nTiny Code (C) (naive construction)\nint cmp(const void *a, const void *b, void *txt) {\n    int i = *(int*)a, j = *(int*)b;\n    return strcmp((char*)txt + i, (char*)txt + j);\n}\n\nvoid build_suffix_array(char *txt, int n, int sa[]) {\n    for (int i = 0; i &lt; n; i++) sa[i] = i;\n    qsort_r(sa, n, sizeof(int), cmp, txt);\n}\nModern methods like prefix doubling or radix sort build it in ( O\\(n \\log n\\) ).\nApplications:\n\nFast substring search- Longest common prefix (LCP) array- Pattern matching in DNA sequences- Plagiarism detection\n\n\n\n3. Suffix Tree\nA suffix tree is a compressed trie of all suffixes , each edge stores multiple characters.\nFor \"banana\", you’d build a tree where each leaf corresponds to a suffix index.\nAdvantages:\n\nPattern search in ( O(m) )- Space ( O(n) ) (with compression) Built using Ukkonen’s algorithm in ( O(n) ).\n\nUse Suffix Array + LCP as a space-efficient alternative.\n\n\n4. FM-Index (Burrows-Wheeler Transform)\nUsed in compressed full-text search (e.g., Bowtie, BWA). Combines:\n\nBurrows-Wheeler Transform (BWT)- Rank/select bitvectors Supports pattern search in O(m) time with very low memory.\n\nIdea: transform text so similar substrings cluster together, enabling compression and backward search.\nApplications:\n\nDNA alignment- Large text archives- Memory-constrained search\n\n\n\n5. Inverted Index\nUsed in search engines. Instead of suffixes, it indexes words.\nFor example, text corpus:\ndoc1: quick brown fox  \ndoc2: quick red fox\nInverted index:\n\"quick\" → [doc1, doc2]\n\"brown\" → [doc1]\n\"red\"   → [doc2]\n\"fox\"   → [doc1, doc2]\nNow searching “quick fox” becomes set intersection of lists.\nUsed with ranking functions (TF-IDF, BM25).\n\n\n6. Tries and Prefix Trees\nA trie stores strings character by character. Each node = prefix.\ntypedef struct Node {\n    struct Node *child[26];\n    int end;\n} Node;\nPerfect for:\n\nAutocomplete- Prefix search- Spell checkers Search: O(m), where m = pattern length.\n\nCompressed tries (Patricia trees) reduce space.\n\n\n7. Comparing Structures\n\n\n\nStructure\nSearch Time\nBuild Time\nSpace\nNotes\n\n\n\n\nTrie\nO(m)\nO(n)\nHigh\nPrefix queries\n\n\nSuffix Array\nO(m log n)\nO(n log n)\nMedium\nSorted suffixes\n\n\nSuffix Tree\nO(m)\nO(n)\nHigh\nRich structure\n\n\nFM-Index\nO(m)\nO(n)\nLow\nCompressed\n\n\nInverted Index\nO(k)\nO(N)\nMedium\nWord-based\n\n\n\n\n\nWhy It Matters\nText indexing is the backbone of search engines, DNA alignment, and autocomplete systems. Without it, Google searches, code lookups, or genome scans would take minutes, not milliseconds.\n\n“Indexing turns oceans of text into navigable maps.”\n\n\n\nTry It Yourself\n\nBuild a suffix array for “banana” and perform binary search for “ana.”\nConstruct a trie for a dictionary and query prefixes.\nWrite a simple inverted index for a few documents.\nCompare memory usage of suffix tree vs suffix array.\nExperiment with FM-index using an online demo (like BWT explorer).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-8.-geometry-graphics-and-spatial-algorithms",
    "href": "books/en-us/book.html#chapter-8.-geometry-graphics-and-spatial-algorithms",
    "title": "The Book",
    "section": "Chapter 8. Geometry, Graphics, and Spatial Algorithms",
    "text": "Chapter 8. Geometry, Graphics, and Spatial Algorithms\n\n71. Convex Hull (Graham, Andrew, Chan)\nIn computational geometry, the convex hull of a set of points is the smallest convex polygon that contains all the points. Intuitively, imagine stretching a rubber band around a set of nails on a board , the shape the band takes is the convex hull.\nConvex hulls are foundational for many geometric algorithms, like closest pair, Voronoi diagrams, and collision detection.\nIn this section, we’ll explore three classical algorithms:\n\nGraham Scan - elegant and simple (O(n log n))- Andrew’s Monotone Chain - robust and practical (O(n log n))- Chan’s Algorithm - advanced and optimal (O(n log h), where h = number of hull points)\n\n\n1. Definition\nGiven a set of points \\(P = {p_1, p_2, ..., p_n}\\), the convex hull, ( (P) ), is the smallest convex polygon enclosing all points.\nFormally: \\[\n\\text{CH}(P) = \\bigcap {C \\subseteq \\mathbb{R}^2 \\mid C \\text{ is convex and } P \\subseteq C }\n\\]\nA polygon is convex if every line segment between two points of the polygon lies entirely inside it.\n\n\n2. Orientation Test\nAll convex hull algorithms rely on an orientation test using cross product: Given three points ( a, b, c ):\n\\[\n\\text{cross}(a,b,c) = (b_x - a_x)(c_y - a_y) - (b_y - a_y)(c_x - a_x)\n\\]\n\n&gt; 0 → counter-clockwise turn- &lt; 0 → clockwise turn- = 0 → collinear\n\n\n\n3. Graham Scan\nOne of the earliest convex hull algorithms.\nIdea:\n\nPick the lowest point (and leftmost if tie).\nSort all other points by polar angle with respect to it.\nTraverse points and maintain a stack:\n\nAdd point - While last three points make a right turn, pop middle one4. Remaining points form convex hull in CCW order.\n\n\nTiny Code (C)\ntypedef struct { double x, y; } Point;\n\ndouble cross(Point a, Point b, Point c) {\n    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);\n}\n\nint cmp(const void *p1, const void *p2) {\n    Point *a = (Point*)p1, *b = (Point*)p2;\n    // Compare by polar angle or distance\n    return (a-&gt;y != b-&gt;y) ? (a-&gt;y - b-&gt;y) : (a-&gt;x - b-&gt;x);\n}\n\nint graham_scan(Point pts[], int n, Point hull[]) {\n    qsort(pts, n, sizeof(Point), cmp);\n    int top = 0;\n    for (int i = 0; i &lt; n; i++) {\n        while (top &gt;= 2 && cross(hull[top-2], hull[top-1], pts[i]) &lt;= 0)\n            top--;\n        hull[top++] = pts[i];\n    }\n    return top; // number of hull points\n}\nComplexity:\n\nSorting: ( O\\(n \\log n\\) )- Scanning: ( O(n) ) → Total: O(n log n)\n\n\n\nExample\nInput:\n(0, 0), (1, 1), (2, 2), (2, 0), (0, 2)\nHull (CCW):\n(0,0) → (2,0) → (2,2) → (0,2)\n\n\n4. Andrew’s Monotone Chain\nSimpler and more robust for floating-point coordinates. Builds lower and upper hulls separately.\nSteps:\n\nSort points lexicographically (x, then y).\nBuild lower hull (left-to-right)\nBuild upper hull (right-to-left)\nConcatenate (excluding duplicates)\n\nTiny Code (C)\nint monotone_chain(Point pts[], int n, Point hull[]) {\n    qsort(pts, n, sizeof(Point), cmp);\n    int k = 0;\n    // Lower hull\n    for (int i = 0; i &lt; n; i++) {\n        while (k &gt;= 2 && cross(hull[k-2], hull[k-1], pts[i]) &lt;= 0) k--;\n        hull[k++] = pts[i];\n    }\n    // Upper hull\n    for (int i = n-2, t = k+1; i &gt;= 0; i--) {\n        while (k &gt;= t && cross(hull[k-2], hull[k-1], pts[i]) &lt;= 0) k--;\n        hull[k++] = pts[i];\n    }\n    return k-1; // last point == first point\n}\nTime Complexity: ( O\\(n \\log n\\) )\n\n\n5. Chan’s Algorithm\nWhen \\(h \\ll n\\), Chan’s method achieves ( O\\(n \\log h\\) ):\n\nPartition points into groups of size ( m ).\nCompute hulls for each group (Graham).\nMerge hulls with Jarvis March (gift wrapping).\nChoose ( m ) cleverly (\\(m = 2^k\\)) to ensure ( O\\(n \\log h\\) ).\n\nUsed in: large-scale geometric processing.\n\n\n6. Applications\n\n\n\nDomain\nUse\n\n\n\n\nComputer Graphics\nShape boundary, hitboxes\n\n\nGIS / Mapping\nRegion boundaries\n\n\nRobotics\nObstacle envelopes\n\n\nClustering\nOutlier detection\n\n\nData Analysis\nMinimal bounding shape\n\n\n\n\n\n7. Complexity Summary\n\n\n\nAlgorithm\nTime\nSpace\nNotes\n\n\n\n\nGraham Scan\n( O\\(n \\log n\\) )\n( O(n) )\nSimple, classic\n\n\nMonotone Chain\n( O\\(n \\log n\\) )\n( O(n) )\nStable, robust\n\n\nChan’s Algorithm\n( O\\(n \\log h\\) )\n( O(n) )\nBest asymptotic\n\n\n\n\n\nWhy It Matters\nConvex hulls are one of the cornerstones of computational geometry. They teach sorting, cross products, and geometric reasoning , and form the basis for many spatial algorithms.\n\n“Every scattered set hides a simple shape. The convex hull is that hidden simplicity.”\n\n\n\nTry It Yourself\n\nImplement Graham Scan for 10 random points.\nPlot the points and verify the hull.\nCompare results with Andrew’s Monotone Chain.\nTest with collinear and duplicate points.\nExplore 3D convex hulls (QuickHull, Gift Wrapping) next.\n\n\n\n\n72. Closest Pair and Segment Intersection\nGeometric problems often ask: what’s the shortest distance between two points? or do these segments cross? These are classic building blocks in computational geometry , essential for collision detection, graphics, clustering, and path planning.\nThis section covers two foundational problems:\n\nClosest Pair of Points - find two points with minimum Euclidean distance- Segment Intersection - determine if (and where) two line segments intersect\n\n\n1. Closest Pair of Points\nGiven ( n ) points in 2D, find a pair with the smallest distance. The brute force solution is ( O\\(n^2\\) ), but using Divide and Conquer, we can solve it in O(n log n).\n\n\nA. Divide and Conquer Algorithm\nIdea:\n\nSort points by x-coordinate.\nSplit into left and right halves.\nRecursively find closest pairs in each half (distance = ( d )).\nMerge step: check pairs across the split line within ( d ).\n\nIn merge step, we only need to check at most 6 neighbors per point (by geometric packing).\nTiny Code (C, Sketch)\n#include &lt;math.h&gt;\ntypedef struct { double x, y; } Point;\n\ndouble dist(Point a, Point b) {\n    double dx = a.x - b.x, dy = a.y - b.y;\n    return sqrt(dx*dx + dy*dy);\n}\n\ndouble brute_force(Point pts[], int n) {\n    double d = 1e9;\n    for (int i = 0; i &lt; n; i++)\n        for (int j = i + 1; j &lt; n; j++)\n            d = fmin(d, dist(pts[i], pts[j]));\n    return d;\n}\nRecursive divide and merge:\ndouble closest_pair(Point pts[], int n) {\n    if (n &lt;= 3) return brute_force(pts, n);\n    int mid = n / 2;\n    double d = fmin(closest_pair(pts, mid),\n                    closest_pair(pts + mid, n - mid));\n    // merge step: check strip points within distance d\n    // sort by y, check neighbors\n    return d;\n}\nTime Complexity: ( O\\(n \\log n\\) )\nExample:\nPoints:\n(2,3), (12,30), (40,50), (5,1), (12,10), (3,4)\nClosest pair: (2,3) and (3,4), distance = √2\n\n\nB. Sweep Line Variant\nAnother method uses a line sweep and a balanced tree to keep active points. As you move from left to right, maintain a window of recent points within ( d ).\nUsed in large-scale spatial systems.\n\n\nApplications\n\n\n\nDomain\nUse\n\n\n\n\nClustering\nFind nearest neighbors\n\n\nRobotics\nAvoid collisions\n\n\nGIS\nNearest city search\n\n\nNetworking\nSensor proximity\n\n\n\n\n\n2. Segment Intersection\nGiven two segments ( AB ) and ( CD ), determine whether they intersect. It’s the core of geometry engines and vector graphics systems.\n\n\nA. Orientation Test\nWe use the cross product (orientation) test again. Two segments ( AB ) and ( CD ) intersect if and only if:\n\nThe segments straddle each other: \\[\n\\text{orient}(A, B, C) \\neq \\text{orient}(A, B, D)\n\\]\n\n\\[\n\\text{orient}(C, D, A) \\neq \\text{orient}(C, D, B)\n\\] 2. Special cases for collinear points (check bounding boxes).\nTiny Code (C)\ndouble cross(Point a, Point b, Point c) {\n    return (b.x - a.x)*(c.y - a.y) - (b.y - a.y)*(c.x - a.x);\n}\n\nint on_segment(Point a, Point b, Point c) {\n    return fmin(a.x, b.x) &lt;= c.x && c.x &lt;= fmax(a.x, b.x) &&\n           fmin(a.y, b.y) &lt;= c.y && c.y &lt;= fmax(a.y, b.y);\n}\n\nint intersect(Point a, Point b, Point c, Point d) {\n    double o1 = cross(a, b, c);\n    double o2 = cross(a, b, d);\n    double o3 = cross(c, d, a);\n    double o4 = cross(c, d, b);\n    if (o1*o2 &lt; 0 && o3*o4 &lt; 0) return 1; // general case\n    if (o1 == 0 && on_segment(a,b,c)) return 1;\n    if (o2 == 0 && on_segment(a,b,d)) return 1;\n    if (o3 == 0 && on_segment(c,d,a)) return 1;\n    if (o4 == 0 && on_segment(c,d,b)) return 1;\n    return 0;\n}\n\n\nB. Line Sweep Algorithm (Bentley-Ottmann)\nFor multiple segments, check all intersections efficiently. Algorithm:\n\nSort all endpoints by x-coordinate.\nSweep from left to right.\nMaintain active set (balanced BST).\nCheck neighboring segments for intersections.\n\nTime complexity: \\(O((n + k) \\log n)\\), where \\(k\\) is the number of intersections.\nUsed in CAD, map rendering, and collision systems.\n\n\n3. Complexity Summary\n\n\n\n\n\n\n\n\n\nProblem\nNaive\nOptimal\nTechnique\n\n\n\n\nClosest Pair\n\\(O(n^2)\\)\n\\(O(n \\log n)\\)\nDivide & Conquer\n\n\nSegment Intersection\n\\(O(n^2)\\)\n\\(O((n + k) \\log n)\\)\nSweep Line\n\n\n\n\n\nWhy It Matters\nGeometric algorithms like these teach how to reason spatially , blending math, sorting, and logic. They power real-world systems where precision matters: from self-driving cars to game engines.\n\n“Every point has a neighbor; every path may cross another , geometry finds the truth in space.”\n\n\n\nTry It Yourself\n\nImplement the closest pair algorithm using divide and conquer.\nVisualize all pairwise distances , see which pairs are minimal.\nTest segment intersection on random pairs.\nModify for 3D line segments using vector cross products.\nTry building a line sweep visualizer to catch intersections step-by-step.\n\n\n\n\n73. Line Sweep and Plane Sweep Algorithms\nThe sweep line (or plane sweep) technique is one of the most powerful paradigms in computational geometry. It transforms complex spatial problems into manageable one-dimensional events , by sweeping a line (or plane) across the input and maintaining a dynamic set of active elements.\nThis method underlies many geometric algorithms:\n\nEvent sorting → handle things in order- Active set maintenance → track current structure- Updates and queries → respond as the sweep progresses Used for intersection detection, closest pair, rectangle union, computational geometry in graphics and GIS.\n\n\n1. The Core Idea\nImagine a vertical line sweeping from left to right across the plane. At each “event” (like a point or segment endpoint), we update the set of objects the line currently touches , the active set.\nEach event may trigger queries, insertions, or removals.\nThis approach works because geometry problems often depend only on local relationships between nearby elements as the sweep advances.\n\n\nA. Sweep Line Template\nA general structure looks like this:\nstruct Event { double x; int type; Object *obj; };\nsort(events.begin(), events.end());\n\nActiveSet S;\n\nfor (Event e : events) {\n    if (e.type == START) S.insert(e.obj);\n    else if (e.type == END) S.erase(e.obj);\n    else if (e.type == QUERY) handle_query(S, e.obj);\n}\nSorting ensures events are processed in order of increasing x (or another dimension).\n\n\n2. Classic Applications\nLet’s explore three foundational problems solvable by sweep techniques.\n\n\nA. Segment Intersection (Bentley-Ottmann)\nGoal: detect all intersections among ( n ) line segments.\nSteps:\n\nSort endpoints by x-coordinate.\nSweep from left to right.\nMaintain an ordered set of active segments (sorted by y).\nWhen a new segment starts, check intersection with neighbors above and below.\nWhen segments intersect, record intersection and insert a new event at the x-coordinate of intersection.\n\nComplexity: \\(O((n + k)\\log n)\\), where \\(k\\) is the number of intersections.\n\n\nB. Closest Pair of Points\nSweep line version sorts by x, then slides a vertical line while maintaining active points within a strip of width ( d ) (current minimum). Only need to check at most 6-8 nearby points in strip.\nComplexity: ( O\\(n \\log n\\) )\n\n\nC. Rectangle Union Area\nGiven axis-aligned rectangles, compute total area covered.\nIdea:\n\nTreat vertical edges as events (entering/exiting rectangles).- Sweep line moves along x-axis.- Maintain y-intervals in active set (using a segment tree or interval tree).- At each step, multiply current width × height of union of active intervals. Complexity: ( O\\(n \\log n\\) )\n\nTiny Code Sketch (C)\ntypedef struct { double x, y1, y2; int type; } Event;\nEvent events[MAX];\nint n_events;\n\nqsort(events, n_events, sizeof(Event), cmp_by_x);\n\ndouble prev_x = events[0].x, area = 0;\nSegmentTree T;\n\nfor (int i = 0; i &lt; n_events; i++) {\n    double dx = events[i].x - prev_x;\n    area += dx * T.total_length(); // current union height\n    if (events[i].type == START)\n        T.insert(events[i].y1, events[i].y2);\n    else\n        T.remove(events[i].y1, events[i].y2);\n    prev_x = events[i].x;\n}\n\n\n3. Other Applications\n\n\n\n\n\n\n\n\nProblem\nDescription\nTime\n\n\n\n\nK-closest points\nMaintain top \\(k\\) in active set\n\\(O(n \\log n)\\)\n\n\nUnion of rectangles\nCompute covered area\n\\(O(n \\log n)\\)\n\n\nPoint location\nLocate point in planar subdivision\n\\(O(\\log n)\\)\n\n\nVisibility graph\nTrack visible edges\n\\(O(n \\log n)\\)\n\n\n\n\n\n4. Plane Sweep Extensions\nWhile line sweep moves in one dimension (x), plane sweep handles 2D or higher-dimensional spaces, where:\n\nEvents are 2D cells or regions.- Sweep front is a plane instead of a line. Used in 3D collision detection, computational topology, and CAD systems.\n\n\n\nConceptual Visualization\n\nSort events by one axis (say, x).\nMaintain structure (set, tree, or heap) of intersecting or active elements.\nUpdate at each event and record desired output (intersection, union, coverage).\n\nThe key is the locality principle: only neighbors in the sweep structure can change outcomes.\n\n\n5. Complexity\n\n\n\nPhase\nComplexity\n\n\n\n\nSorting events\n\\(O(n \\log n)\\)\n\n\nProcessing events\n\\(O(n \\log n)\\)\n\n\nTotal\n\\(O(n \\log n)\\) (typical)\n\n\n\n\n\nWhy It Matters\nThe sweep line method transforms geometric chaos into order , turning spatial relationships into sorted sequences. It’s the bridge between geometry and algorithms, blending structure with motion.\n\n“A sweep line sees everything , not all at once, but just in time.”\n\n\n\nTry It Yourself\n\nImplement a sweep-line segment intersection finder.\nCompute the union area of 3 rectangles with overlaps.\nAnimate the sweep line to visualize event processing.\nModify for circular or polygonal objects.\nExplore how sweep-line logic applies to time-based events in scheduling.\n\n\n\n\n74. Delaunay and Voronoi Diagrams\nIn geometry and spatial computing, Delaunay triangulations and Voronoi diagrams are duals , elegant structures that capture proximity, territory, and connectivity among points.\nThey’re used everywhere: from mesh generation, pathfinding, geospatial analysis, to computational biology. This section introduces both, their relationship, and algorithms to construct them efficiently.\n\n1. Voronoi Diagram\nGiven a set of sites (points) \\(P = {p_1, p_2, \\ldots, p_n}\\), the Voronoi diagram partitions the plane into regions , one per point , so that every location in a region is closer to its site than to any other.\nFormally, the Voronoi cell for \\(p_i\\) is: \\[\nV(p_i) = {x \\in \\mathbb{R}^2 \\mid d(x, p_i) \\le d(x, p_j), \\forall j \\neq i }\n\\]\nEach region is convex, and boundaries are formed by perpendicular bisectors.\n\n\nExample\nFor points ( A, B, C ):\n\nDraw bisectors between each pair.- Intersection points define Voronoi vertices.- Resulting polygons cover the plane, one per site. Used to model nearest neighbor regions , “which tower serves which area?”\n\n\n\nProperties\n\nEvery cell is convex.- Neighboring cells share edges.- The diagram’s vertices are centers of circumcircles through three sites.- Dual graph = Delaunay triangulation.\n\n\n\n2. Delaunay Triangulation\nThe Delaunay triangulation (DT) connects points so that no point lies inside the circumcircle of any triangle.\nEquivalently, it’s the dual graph of the Voronoi diagram.\nIt tends to avoid skinny triangles , maximizing minimum angles, creating well-shaped meshes.\n\n\nFormal Definition\nA triangulation ( T ) of ( P ) is Delaunay if for every triangle \\(\\triangle abc \\in T\\), no point \\(p \\in P \\setminus {a,b,c}\\) lies inside the circumcircle of \\(\\triangle abc\\).\nWhy It Matters:\n\nAvoids sliver triangles.- Used in finite element meshes, terrain modeling, and path planning.- Leads to natural neighbor interpolation and smooth surfaces.\n\n\n\n3. Relationship\nVoronoi and Delaunay are geometric duals:\n\n\n\nVoronoi\nDelaunay\n\n\n\n\nRegions = proximity zones\nTriangles = neighbor connections\n\n\nEdges = bisectors\nEdges = neighbor pairs\n\n\nVertices = circumcenters\nFaces = circumcircles\n\n\n\nConnecting neighboring Voronoi cells gives Delaunay edges.\n\n\n4. Algorithms\nSeveral algorithms can build these diagrams efficiently.\n\n\nA. Incremental Insertion\n\nStart with a super-triangle enclosing all points.\nInsert points one by one.\nRemove triangles whose circumcircle contains the point.\nRe-triangulate the resulting polygonal hole.\n\nTime Complexity: ( O\\(n^2\\) ), improved to ( O\\(n \\log n\\) ) with randomization.\n\n\nB. Divide and Conquer\n\nSort points by x.\nRecursively build DT for left and right halves.\nMerge by finding common tangents.\n\nTime Complexity: ( O\\(n \\log n\\) ) Elegant, structured, and deterministic.\n\n\nC. Fortune’s Sweep Line Algorithm\nFor Voronoi diagrams, Fortune’s algorithm sweeps a line from top to bottom. Maintains a beach line of parabolic arcs and event queue.\nEach event (site or circle) updates the structure , building Voronoi edges incrementally.\nTime Complexity: ( O\\(n \\log n\\) )\n\n\nD. Bowyer-Watson (Delaunay via Circumcircle Test)\nA practical incremental version widely used in graphics and simulation.\nSteps:\n\nStart with supertriangle- Insert point- Remove all triangles whose circumcircle contains point- Reconnect the resulting cavity\n\nTiny Code (Conceptual)\ntypedef struct { double x, y; } Point;\n\ntypedef struct { Point a, b, c; } Triangle;\n\nbool in_circle(Point a, Point b, Point c, Point p) {\n    double A[3][3] = {\n        {a.x - p.x, a.y - p.y, (a.x*a.x + a.y*a.y) - (p.x*p.x + p.y*p.y)},\n        {b.x - p.x, b.y - p.y, (b.x*b.x + b.y*b.y) - (p.x*p.x + p.y*p.y)},\n        {c.x - p.x, c.y - p.y, (c.x*c.x + c.y*c.y) - (p.x*p.x + p.y*p.y)}\n    };\n    return determinant(A) &gt; 0;\n}\nThis test ensures Delaunay property.\n\n\n5. Applications\n\n\n\nDomain\nApplication\n\n\n\n\nGIS\nNearest facility, region partition\n\n\nMesh Generation\nFinite element methods\n\n\nRobotics\nVisibility graphs, navigation\n\n\nComputer Graphics\nTerrain triangulation\n\n\nClustering\nSpatial neighbor structure\n\n\n\n\n\n6. Complexity Summary\n\n\n\nAlgorithm\nType\nTime\nNotes\n\n\n\n\nFortune\nVoronoi\n( O\\(n \\log n\\) )\nSweep line\n\n\nBowyer-Watson\nDelaunay\n( O\\(n \\log n\\) )\nIncremental\n\n\nDivide & Conquer\nDelaunay\n( O\\(n \\log n\\) )\nRecursive\n\n\n\n\n\nWhy It Matters\nVoronoi and Delaunay diagrams reveal natural structure in point sets. They convert distance into geometry, showing how space is divided and connected. If geometry is the shape of space, these diagrams are its skeleton.\n\n“Every point claims its territory; every territory shapes its network.”\n\n\n\nTry It Yourself\n\nDraw Voronoi regions for 5 random points by hand.\nBuild Delaunay triangles (connect neighboring sites).\nVerify the empty circumcircle property.\nUse a library (CGAL / SciPy) to visualize both structures.\nExplore how adding new points reshapes the diagrams.\n\n\n\n\n75. Point in Polygon and Polygon Triangulation\nGeometry often asks two fundamental questions:\n\nIs a point inside or outside a polygon?\nHow can a complex polygon be broken into triangles for computation?\n\nThese are the building blocks of spatial analysis, computer graphics, and computational geometry.\n\n1. Point in Polygon (PIP)\nGiven a polygon defined by vertices ( \\(x_1, y_1\\), \\(x_2, y_2\\), , \\(x_n, y_n\\) ) and a test point ( (x, y) ), we want to determine if the point lies inside, on the boundary, or outside the polygon.\n\n\nMethods\n\n\nA. Ray Casting Algorithm\nShoot a ray horizontally to the right of the point. Count how many times it intersects polygon edges.\n\nOdd count → Inside- Even count → Outside This is based on the even-odd rule.\n\nTiny Code (Ray Casting in C)\nbool point_in_polygon(Point p, Point poly[], int n) {\n    bool inside = false;\n    for (int i = 0, j = n - 1; i &lt; n; j = i++) {\n        if (((poly[i].y &gt; p.y) != (poly[j].y &gt; p.y)) &&\n            (p.x &lt; (poly[j].x - poly[i].x) * \n                   (p.y - poly[i].y) / \n                   (poly[j].y - poly[i].y) + poly[i].x))\n            inside = !inside;\n    }\n    return inside;\n}\nThis toggles inside every time a crossing is found.\n\n\nB. Winding Number Algorithm\nCounts how many times the polygon winds around the point.\n\nNonzero winding number → Inside- Zero → Outside More robust for complex polygons with holes or self-intersections.\n\n\n\n\nMethod\nTime Complexity\nRobustness\n\n\n\n\nRay Casting\n(O(n))\nSimple, may fail on edge cases\n\n\nWinding Number\n(O(n))\nMore accurate for complex shapes\n\n\n\n\n\nEdge Cases\nHandle:\n\nPoints on edges or vertices- Horizontal edges (special treatment to avoid double counting) Numerical precision is key.\n\n\n\nApplications\n\nHit testing in computer graphics- GIS spatial queries- Collision detection\n\n\n\n2. Polygon Triangulation\nA polygon triangulation divides a polygon into non-overlapping triangles whose union equals the polygon.\nWhy triangulate?\n\nTriangles are simple, stable, and efficient for rendering and computation.- Used in graphics pipelines, area computation, physics, and mesh generation.\n\n\n\nA. Triangulation Basics\nFor a simple polygon with ( n ) vertices,\n\nAlways possible- Always yields ( n - 2 ) triangles Goal: Find a triangulation efficiently and stably.\n\n\n\nB. Ear Clipping Algorithm\nAn intuitive and widely used method for triangulation.\n\n\nIdea\n\nFind an ear: a triangle formed by three consecutive vertices ( \\(v_{i-1}, v_i, v_{i+1}\\) ) such that:\n\nIt is convex - Contains no other vertex inside\n\nClip the ear (remove vertex \\(v_i\\))\nRepeat until only one triangle remains\n\nTime Complexity: ( O\\(n^2\\) )\nTiny Code (Ear Clipping Sketch)\nwhile (n &gt; 3) {\n    for (i = 0; i &lt; n; i++) {\n        if (is_ear(i)) {\n            add_triangle(i-1, i, i+1);\n            remove_vertex(i);\n            break;\n        }\n    }\n}\nHelper is_ear() checks convexity and emptiness.\n\n\nC. Dynamic Programming for Convex Polygons\nIf the polygon is convex, use DP triangulation:\n\\[\ndp[i][j] = \\min_{k \\in (i,j)} dp[i][k] + dp[k][j] + cost(i, j, k)\n\\]\nCost: perimeter or area (for minimum-weight triangulation)\nTime Complexity: ( O\\(n^3\\) ) Space: ( O\\(n^2\\) )\n\n\nD. Divide and Conquer\nRecursively split polygon and triangulate sub-polygons. Useful for convex or near-convex shapes.\n\n\n\nAlgorithm\nTime\nNotes\n\n\n\n\nEar Clipping\n(O\\(n^2\\))\nSimple polygons\n\n\nDP Triangulation\n(O\\(n^3\\))\nWeighted cost\n\n\nConvex Polygon\n(O(n))\nStraightforward\n\n\n\n\n\n3. Applications\n\n\n\nDomain\nUsage\n\n\n\n\nComputer Graphics\nRendering, rasterization\n\n\nComputational Geometry\nArea computation, integration\n\n\nFinite Element Analysis\nMesh subdivision\n\n\nRobotics\nPath planning, map decomposition\n\n\n\n\n\nWhy It Matters\nPoint-in-polygon answers where you are. Triangulation tells you how space is built. Together, they form the foundation of geometric reasoning.\n\n“From a single point to a thousand triangles, geometry turns space into structure.”\n\n\n\nTry It Yourself\n\nDraw a non-convex polygon and test random points using the ray casting rule.\nImplement the ear clipping algorithm for a simple polygon.\nVisualize how each step removes an ear and simplifies the shape.\nCompare triangulation results for convex vs concave shapes.\n\n\n\n\n76. Spatial Data Structures (KD, R-tree)\nWhen working with geometric data, points, rectangles, or polygons, efficient lookup and organization are crucial. Spatial data structures are designed to answer queries like:\n\nWhich objects are near a given point?- Which shapes intersect a region?- What’s the nearest neighbor? They form the backbone of computational geometry, computer graphics, GIS, and search systems.\n\n\n1. Motivation\nBrute force approaches that check every object have ( O(n) ) or worse performance. Spatial indexing structures, like KD-Trees and R-Trees, enable efficient range queries, nearest neighbor searches, and spatial joins.\n\n\n2. KD-Tree (k-dimensional tree)\nA KD-tree is a binary tree that recursively partitions space using axis-aligned hyperplanes.\nEach node splits the data by one coordinate axis (x, y, z, …).\n\n\nStructure\n\nEach node represents a point.- Each level splits by a different axis (x, y, x, y, …).- Left child contains points with smaller coordinate.- Right child contains larger coordinate.\n\nTiny Code (KD-tree Construction in 2D)\ntypedef struct {\n    double x, y;\n} Point;\n\nint axis; // 0 for x, 1 for y\n\nKDNode* build(Point points[], int n, int depth) {\n    if (n == 0) return NULL;\n    axis = depth % 2;\n    int mid = n / 2;\n    nth_element(points, points + mid, points + n, compare_by_axis);\n    KDNode* node = new_node(points[mid]);\n    node-&gt;left  = build(points, mid, depth + 1);\n    node-&gt;right = build(points + mid + 1, n - mid - 1, depth + 1);\n    return node;\n}\nSearch Complexity:\n\nAverage: ( O\\(\\log n\\) )- Worst-case: ( O(n) )\n\n\n\nQueries\n\nRange query: Find points in a region.- Nearest neighbor: Search branches that might contain closer points.- K-nearest neighbors: Use priority queues.\n\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nEfficient for static data\nCostly updates\n\n\nGood for low dimensions\nDegrades with high dimensions\n\n\n\n\n\nApplications\n\nNearest neighbor in ML- Collision detection- Clustering (e.g., k-means acceleration)\n\n\n\n3. R-Tree (Rectangle Tree)\nAn R-tree is a height-balanced tree for rectangular bounding boxes. It’s the spatial analog of a B-tree.\n\n\nIdea\n\nStore objects or bounding boxes in leaf nodes.- Internal nodes store MBRs (Minimum Bounding Rectangles) that cover child boxes.- Query by traversing overlapping MBRs.\n\nTiny Code (R-Tree Node Sketch)\ntypedef struct {\n    Rectangle mbr;\n    Node* children[MAX_CHILDREN];\n    int count;\n} Node;\nInsertion chooses the child whose MBR expands least to accommodate the new entry.\n\n\nOperations\n\nInsert: Choose subtree → Insert → Adjust MBRs- Search: Descend into nodes whose MBR intersects query- Split: When full, use heuristics (linear, quadratic, R*-Tree) Complexity:\nQuery: ( O\\(\\log n\\) )- Insert/Delete: ( O\\(\\log n\\) ) average\n\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nSupports dynamic data\nOverlaps can degrade performance\n\n\nIdeal for rectangles\nComplex split rules\n\n\n\n\n\nVariants\n\nR*-Tree: Optimized reinsertion, better packing- R+ Tree: Non-overlapping partitions- Hilbert R-Tree: Uses space-filling curves\n\n\n\n4. Comparison\n\n\n\nFeature\nKD-Tree\nR-Tree\n\n\n\n\nData Type\nPoints\nRectangles / Regions\n\n\nDimensionality\nLow (2-10)\nMedium\n\n\nUse Case\nNN, range queries\nSpatial joins, overlap queries\n\n\nUpdates\nExpensive\nDynamic-friendly\n\n\nBalance\nRecursive median\nB-tree-like\n\n\n\n\n\n5. Other Spatial Structures\n\n\n\nStructure\nDescription\n\n\n\n\nQuadtree\nRecursive 2D subdivision into 4 quadrants\n\n\nOctree\n3D analog of quadtree\n\n\nBSP Tree\nBinary partition using arbitrary planes\n\n\nGrid Index\nDivide space into uniform grid cells\n\n\n\n\n\n6. Applications\n\n\n\nDomain\nUsage\n\n\n\n\nGIS\nRegion queries, map intersections\n\n\nGraphics\nRay tracing acceleration\n\n\nRobotics\nCollision and path planning\n\n\nML\nNearest neighbor search\n\n\nDatabases\nSpatial indexing\n\n\n\n\n\nWhy It Matters\nSpatial structures turn geometry into searchable data. They enable efficient algorithms for where and what’s near, vital for real-time systems.\n\n“Divide space wisely, and queries become whispers instead of shouts.”\n\n\n\nTry It Yourself\n\nBuild a KD-tree for 10 random 2D points.\nImplement nearest neighbor search.\nInsert rectangles into a simple R-tree and query intersection with a bounding box.\nCompare query time vs brute force.\n\n\n\n\n77. Rasterization and Scanline Techniques\nWhen you draw shapes on a screen, triangles, polygons, circles, they must be converted into pixels. This conversion is called rasterization. It’s the bridge between geometric math and visible images.\nRasterization and scanline algorithms are foundational to computer graphics, game engines, and rendering pipelines.\n\n1. What Is Rasterization?\nRasterization transforms vector shapes (continuous lines and surfaces) into discrete pixels on a grid.\nFor example, a triangle defined by vertices (x1, y1), (x2, y2), (x3, y3) must be filled pixel by pixel.\n\n\n2. Core Idea\nEach shape (line, polygon, circle) is sampled over a grid. The algorithm decides which pixels are inside, on, or outside the shape.\nA rasterizer answers:\n\nWhich pixels should be lit?- What color or depth should each pixel have?\n\n\n\n3. Line Rasterization (Bresenham’s Algorithm)\nA classic method for drawing straight lines with integer arithmetic.\nKey Idea: Move from one pixel to the next, choosing the pixel closest to the true line path.\nvoid draw_line(int x0, int y0, int x1, int y1) {\n    int dx = abs(x1 - x0), dy = abs(y1 - y0);\n    int sx = (x0 &lt; x1) ? 1 : -1;\n    int sy = (y0 &lt; y1) ? 1 : -1;\n    int err = dx - dy;\n    while (true) {\n        plot(x0, y0); // draw pixel\n        if (x0 == x1 && y0 == y1) break;\n        int e2 = 2 * err;\n        if (e2 &gt; -dy) { err -= dy; x0 += sx; }\n        if (e2 &lt; dx) { err += dx; y0 += sy; }\n    }\n}\nWhy it works: Bresenham avoids floating-point math and keeps the line visually continuous.\n\n\n4. Polygon Rasterization\nTo fill shapes, we need scanline algorithms, they sweep a horizontal line (y-axis) across the shape and fill pixels in between edges.\n\n\nScanline Fill Steps\n\nSort edges by their y-coordinates.\nScan each line (y).\nFind intersections with polygon edges.\nFill between intersection pairs.\n\nThis guarantees correct filling for convex and concave polygons.\n\n\nExample (Simple Triangle Rasterization)\nfor (int y = y_min; y &lt;= y_max; y++) {\n    find all x-intersections with polygon edges;\n    sort x-intersections;\n    for (int i = 0; i &lt; count; i += 2)\n        draw_line(x[i], y, x[i+1], y);\n}\n\n\n5. Circle Rasterization (Midpoint Algorithm)\nUse symmetry, a circle is symmetric in 8 octants.\nEach step calculates the error term to decide whether to move horizontally or diagonally.\nvoid draw_circle(int xc, int yc, int r) {\n    int x = 0, y = r, d = 3 - 2 * r;\n    while (y &gt;= x) {\n        plot_circle_points(xc, yc, x, y);\n        x++;\n        if (d &gt; 0) { y--; d += 4 * (x - y) + 10; }\n        else d += 4 * x + 6;\n    }\n}\n\n\n6. Depth and Shading\nIn 3D graphics, rasterization includes depth testing (Z-buffer) and color interpolation. Each pixel stores its depth; new pixels overwrite only if closer.\nInterpolated shading (Gouraud, Phong) computes smooth color transitions across polygons.\n\n\n7. Hardware Rasterization\nModern GPUs perform rasterization in parallel:\n\nVertex Shader → Projection- Rasterizer → Pixel Grid- Fragment Shader → Color & Depth Each pixel is processed in fragment shaders for lighting, texture, and effects.\n\n\n\n8. Optimizations\n\n\n\nTechnique\nPurpose\n\n\n\n\nBounding Box Clipping\nSkip off-screen regions\n\n\nEarly Z-Culling\nDiscard hidden pixels early\n\n\nEdge Functions\nFast inside-test for triangles\n\n\nBarycentric Coordinates\nInterpolate depth/color smoothly\n\n\n\n\n\n9. Why It Matters\nRasterization turns math into imagery. It’s the foundation of all visual computing, renderers, CAD, games, and GUIs. Even with ray tracing rising, rasterization remains dominant for real-time rendering.\n\n“Every pixel you see began as math, it’s just geometry painted by light.”\n\n\n\n10. Try It Yourself\n\nImplement Bresenham’s algorithm for lines.\nWrite a scanline polygon fill for triangles.\nExtend it with color interpolation using barycentric coordinates.\nCompare performance vs brute force (looping over all pixels).\n\n\n\n\n78. Computer Vision (Canny, Hough, SIFT)\nComputer vision is where algorithms learn to see, to extract structure, shape, and meaning from images. Behind every object detector, edge map, and keypoint matcher lies a handful of powerful geometric algorithms.\nIn this section, we explore four pillars of classical vision: Canny edge detection, Hough transform, and SIFT (Scale-Invariant Feature Transform).\n\n1. The Vision Pipeline\nMost vision algorithms follow a simple pattern:\n\nInput: Raw pixels (grayscale or color)\nPreprocess: Smoothing or filtering\nFeature extraction: Edges, corners, blobs\nDetection or matching: Shapes, keypoints\nInterpretation: Object recognition, tracking\n\nCanny, Hough, and SIFT live in the feature extraction and detection stages.\n\n\n2. Canny Edge Detector\nEdges mark places where intensity changes sharply, the outlines of objects. The Canny algorithm (1986) is one of the most robust and widely used edge detectors.\n\n\nSteps\n\nSmoothing: Apply Gaussian blur to reduce noise.\nGradient computation:\n\nCompute \\(G_x\\) and \\(G_y\\) via Sobel filters\n\nGradient magnitude: \\(G = \\sqrt{G_x^2 + G_y^2}\\)\n\nGradient direction: \\(\\theta = \\tan^{-1}\\frac{G_y}{G_x}\\)\n\nNon-maximum suppression:\n\nKeep only local maxima along the gradient direction\n\nDouble thresholding:\n\nStrong edges (high gradient)\n\nWeak edges (connected to strong ones)\n\nEdge tracking by hysteresis:\n\nConnect weak edges linked to strong edges\n\n\n\n\nTiny Code (Pseudocode)\nImage canny(Image input) {\n    Image smoothed = gaussian_blur(input);\n    Gradient grad = sobel(smoothed);\n    Image suppressed = non_max_suppression(grad);\n    Image edges = hysteresis_threshold(suppressed, low, high);\n    return edges;\n}\n\n\nWhy Canny Works\nCanny maximizes three criteria:\n\nGood detection (low false negatives)\nGood localization (edges close to true edges)\nSingle response (no duplicates)\n\nIt’s a careful balance between sensitivity and stability.\n\n\n3. Hough Transform\nCanny finds edge points, Hough connects them into shapes.\nThe Hough transform detects lines, circles, and other parametric shapes using voting in parameter space.\n\n\nLine Detection\nEquation of a line: \\[\n\\rho = x\\cos\\theta + y\\sin\\theta\n\\]\nEach edge point votes for all (\\(\\rho, \\theta\\)) combinations it could belong to. Peaks in the accumulator array correspond to strong lines.\nTiny Code (Hough Transform)\nfor each edge point (x, y):\n  for theta in [0, 180):\n    rho = x*cos(theta) + y*sin(theta);\n    accumulator[rho, theta]++;\nThen pick (\\(\\rho, \\theta\\)) with highest votes.\n\n\nCircle Detection\nUse 3D accumulator \\(center_x, center_y, radius\\). Each edge pixel votes for possible circle centers.\n\n\nApplications\n\nLane detection in self-driving- Shape recognition (circles, ellipses)- Document analysis (lines, grids)\n\n\n\n4. SIFT (Scale-Invariant Feature Transform)\nSIFT finds keypoints that remain stable under scale, rotation, and illumination changes.\nIt’s widely used for image matching, panoramas, 3D reconstruction, and object recognition.\n\n\nSteps\n\nScale-space extrema detection\n\nUse Difference of Gaussians (DoG) across scales. - Detect maxima/minima in space-scale neighborhood.2. Keypoint localization\nRefine keypoint position and discard unstable ones.3. Orientation assignment\nAssign dominant gradient direction.4. Descriptor generation\nBuild a 128D histogram of gradient orientations in a local patch.\n\n\nTiny Code (Outline)\nfor each octave:\n  build scale-space pyramid\n  find DoG extrema\n  localize keypoints\n  assign orientations\n  compute 128D descriptor\n\n\nProperties\n\n\n\nProperty\nDescription\n\n\n\n\nScale Invariant\nDetects features at multiple scales\n\n\nRotation Invariant\nUses local orientation\n\n\nRobust\nHandles lighting, noise, affine transforms\n\n\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\nAlgorithm\nPurpose\nOutput\nRobustness\n\n\n\n\nCanny\nEdge detection\nBinary edge map\nSensitive to thresholds\n\n\nHough\nShape detection\nLines, circles\nNeeds clean edges\n\n\nSIFT\nFeature detection\nKeypoints, descriptors\nVery robust\n\n\n\n\n\n6. Applications\n\n\n\nDomain\nUse Case\n\n\n\n\nRobotics\nVisual SLAM, localization\n\n\nAR / VR\nMarker tracking\n\n\nSearch\nImage matching\n\n\nMedical\nEdge segmentation\n\n\nIndustry\nQuality inspection\n\n\n\n\n\n7. Modern Successors\n\nORB (FAST + BRIEF): Efficient for real-time- SURF: Faster SIFT alternative- Harris / FAST: Corner detectors- Deep features: CNN-based descriptors\n\n\n\nWhy It Matters\nThese algorithms gave machines their first eyes, before deep learning, they were how computers recognized structure. Even today, they’re used in preprocessing, embedded systems, and hybrid pipelines.\n\n“Before neural nets could dream, vision began with gradients, geometry, and votes.”\n\n\n\nTry It Yourself\n\nImplement Canny using Sobel and hysteresis.\nUse Hough transform to detect lines in a synthetic image.\nTry OpenCV SIFT to match keypoints between two rotated images.\nCompare edge maps before and after Gaussian blur.\n\n\n\n\n79. Pathfinding in Space (A*, RRT, PRM)\nWhen navigating a maze, driving an autonomous car, or moving a robot arm, the question is the same: How do we find a path from start to goal efficiently and safely?\nPathfinding algorithms answer this question, balancing optimality, speed, and adaptability. In this section, we explore three foundational families:\n\nA*: Heuristic search in grids and graphs- RRT (Rapidly-Exploring Random Tree): Sampling-based exploration- PRM (Probabilistic Roadmap): Precomputed navigation networks\n\n\n1. The Pathfinding Problem\nGiven:\n\nA space (grid, graph, or continuous)- A start node and goal node- A cost function (distance, time, energy)- Optional obstacles Find a collision-free, low-cost path.\n\n\n\n2. A* (A-star) Search\nA* combines Dijkstra’s algorithm with a heuristic that estimates remaining cost. It’s the most popular graph-based pathfinding algorithm.\n\n\nKey Idea\nEach node ( n ) has: \\[\nf(n) = g(n) + h(n)\n\\]\n\n( g(n) ): cost so far- ( h(n) ): estimated cost to goal- ( f(n) ): total estimated cost\n\nAlgorithm\n\nInitialize priority queue with start node\nWhile queue not empty:\n\nPop node with smallest ( f(n) ) - If goal reached → return path - For each neighbor:\n\nCompute new ( g ), ( f ) - Update queue if better\n\n\n\nTiny Code (Grid A*)\ntypedef struct { int x, y; double g, f; } Node;\n\ndouble heuristic(Node a, Node b) {\n    return fabs(a.x - b.x) + fabs(a.y - b.y); // Manhattan\n}\n\nvoid a_star(Node start, Node goal) {\n    PriorityQueue open;\n    push(open, start);\n    while (!empty(open)) {\n        Node cur = pop_min(open);\n        if (cur == goal) return reconstruct_path();\n        for (Node n : neighbors(cur)) {\n            double tentative_g = cur.g + dist(cur, n);\n            if (tentative_g &lt; n.g) {\n                n.g = tentative_g;\n                n.f = n.g + heuristic(n, goal);\n                push(open, n);\n            }\n        }\n    }\n}\n\n\nComplexity\n\nTime: ( O\\(E \\log V\\) )- Space: ( O(V) )- Optimal if ( h(n) ) is admissible (never overestimates)\n\n\n\nVariants\n\n\n\nVariant\nDescription\n\n\n\n\nDijkstra\nA* with ( h(n) = 0 )\n\n\nGreedy Best-First\nUses ( h(n) ) only\n\n\nWeighted A*\nSpeeds up with tradeoff on optimality\n\n\nJump Point Search\nOptimized for uniform grids\n\n\n\n\n\n3. RRT (Rapidly-Exploring Random Tree)\nA* struggles in continuous or high-dimensional spaces (e.g. robot arms). RRT tackles this with randomized exploration.\n\n\nCore Idea\n\nGrow a tree from the start by randomly sampling points.- Extend tree toward each sample (step size \\(\\epsilon\\)).- Stop when near the goal.\n\nTiny Code (RRT Sketch)\nTree T = {start};\nfor (int i = 0; i &lt; MAX_ITERS; i++) {\n    Point q_rand = random_point();\n    Point q_near = nearest(T, q_rand);\n    Point q_new = steer(q_near, q_rand, step_size);\n    if (collision_free(q_near, q_new))\n        add_edge(T, q_near, q_new);\n    if (distance(q_new, goal) &lt; eps)\n        return path;\n}\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nWorks in continuous space\nPaths are suboptimal\n\n\nHandles high dimensions\nRandomness may miss narrow passages\n\n\nSimple and fast\nNeeds post-processing (smoothing)\n\n\n\n\n\nVariants\n\n\n\nVariant\nDescription\n\n\n\n\nRRT*\nAsymptotically optimal\n\n\nBi-RRT\nGrow from both start and goal\n\n\nInformed RRT*\nFocus on promising regions\n\n\n\n\n\n4. PRM (Probabilistic Roadmap)\nPRM builds a graph of feasible configurations, a roadmap, then searches it.\n\n\nSteps\n\nSample random points in free space\nConnect nearby points with collision-free edges\nSearch roadmap (e.g., with A*)\n\nTiny Code (PRM Sketch)\nGraph G = {};\nfor (int i = 0; i &lt; N; i++) {\n    Point p = random_free_point();\n    G.add_vertex(p);\n}\nfor each p in G:\n    for each q near p:\n        if (collision_free(p, q))\n            G.add_edge(p, q);\npath = a_star(G, start, goal);\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nPrecomputes reusable roadmap\nNeeds many samples for coverage\n\n\nGood for multiple queries\nPoor for single-query planning\n\n\nWorks in high-dim spaces\nMay need post-smoothing\n\n\n\n\n\n5. Comparison\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nSpace\nNature\nOptimal\nUse Case\n\n\n\n\nA*\nDiscrete\nDeterministic\nYes\nGrids, graphs\n\n\nRRT\nContinuous\nRandomized\nNo (RRT* = Yes)\nRobotics, motion planning\n\n\nPRM\nContinuous\nRandomized\nApprox.\nMulti-query planning\n\n\n\n\n\n6. Applications\n\n\n\nDomain\nUse Case\n\n\n\n\nRobotics\nArm motion, mobile navigation\n\n\nGames\nNPC pathfinding, AI navigation mesh\n\n\nAutonomous vehicles\nRoute planning\n\n\nAerospace\nDrone and spacecraft trajectory\n\n\nLogistics\nWarehouse robot movement\n\n\n\n\n\nWhy It Matters\nPathfinding is decision-making in space, it gives agents the ability to move, explore, and act purposefully. From Pac-Man to Mars rovers, every journey starts with an algorithm.\n\n“To move with purpose, one must first see the paths that are possible.”\n\n\n\nTry It Yourself\n\nImplement A* on a 2D grid with walls.\nGenerate an RRT in a 2D obstacle field.\nBuild a PRM for a continuous space and run A* on the roadmap.\nCompare speed and path smoothness across methods.\n\n\n\n\n80. Computational Geometry Variants and Applications\nComputational geometry is the study of algorithms on geometric data, points, lines, polygons, circles, and higher-dimensional shapes. By now, you’ve seen core building blocks: convex hulls, intersections, nearest neighbors, triangulations, and spatial indexing.\nThis final section brings them together through variants, generalizations, and real-world applications, showing how geometry quietly powers modern computing.\n\n1. Beyond the Plane\nMost examples so far assumed 2D geometry. But real systems often live in 3D or N-D spaces.\n\n\n\n\n\n\n\n\nDimension\nExample Problems\nTypical Uses\n\n\n\n\n2D\nConvex hull, polygon area, line sweep\nGIS, CAD, mapping\n\n\n3D\nConvex polyhedra, mesh intersection, visibility\nGraphics, simulation\n\n\nN-D\nVoronoi in high-D, KD-trees, optimization\nML, robotics, data science\n\n\n\nHigher dimensions add complexity (and sometimes impossibility):\n\nExact geometry often replaced by approximations.- Volume, distance, and intersection tests become more expensive.\n\n\n\n2. Approximate and Robust Geometry\nReal-world geometry faces numerical errors (floating point) and degenerate cases (collinear, overlapping). To handle this, algorithms adopt robustness and approximation strategies.\n\nEpsilon comparisons: treat values within tolerance as equal- Orientation tests: robustly compute turn direction via cross product- Exact arithmetic: rational or symbolic computation- Grid snapping: quantize space for stability Approximate geometry accepts small error for large speed-up, essential in graphics and machine learning.\n\n\n\n3. Geometric Duality\nA powerful tool for reasoning about problems: map points to lines, lines to points. For example:\n\nA point ( (a, b) ) maps to line ( y = ax - b ).- A line ( y = mx + c ) maps to point ( (m, -c) ). Applications:\nTransforming line intersection problems into point location problems- Simplifying half-plane intersections- Enabling arrangement algorithms in computational geometry Duality is a common trick: turn geometry upside-down to make it simpler.\n\n\n\n4. Geometric Data Structures\nRecap of core spatial structures and what they’re best at:\n\n\n\n\n\n\n\n\n\nStructure\nStores\nQueries\nUse Case\n\n\n\n\nKD-Tree\nPoints\nNN, range\nLow-D search\n\n\nR-Tree\nRectangles\nOverlaps\nSpatial DB\n\n\nQuad/Octree\nSpace partitions\nPoint lookup\nGraphics, GIS\n\n\nBSP Tree\nPolygons\nVisibility\nRendering\n\n\nDelaunay Triangulation\nPoints\nNeighbors\nMesh generation\n\n\nSegment Tree\nIntervals\nRange\nSweep-line events\n\n\n\n\n\n5. Randomized Geometry\nRandomness simplifies deterministic geometry:\n\nRandomized incremental construction (Convex Hulls, Delaunay)- Random sampling for approximation (ε-nets, VC dimension)- Monte Carlo geometry for probabilistic intersection and coverage Example: randomized incremental convex hull builds expected ( O\\(n \\log n\\) ) structures with elegant proofs.\n\n\n\n6. Computational Topology\nBeyond geometry lies shape connectivity, studied by topology. Algorithms compute connected components, holes, homology, and Betti numbers.\nApplications include:\n\n3D printing (watertightness)- Data analysis (persistent homology)- Robotics (free space topology) Geometry meets topology in alpha-shapes, simplicial complexes, and manifold reconstruction.\n\n\n\n7. Geometry Meets Machine Learning\nMany ML methods are geometric at heart:\n\nNearest neighbor → Voronoi diagram- SVM → hyperplane separation- K-means → Voronoi partitioning- Manifold learning → low-dim geometry- Convex optimization → geometric feasibility Visualization tools (t-SNE, UMAP) rely on spatial embedding and distance geometry.\n\n\n\n8. Applications Across Fields\n\n\n\n\n\n\n\n\nField\nApplication\nGeometric Core\n\n\n\n\nGraphics\nRendering, collision\nTriangulation, ray tracing\n\n\nGIS\nMaps, roads\nPolygons, point-in-region\n\n\nRobotics\nPath planning\nObstacles, configuration space\n\n\nArchitecture\nModeling\nMesh operations\n\n\nVision\nObject boundaries\nContours, convexity\n\n\nAI\nClustering, similarity\nDistance metrics\n\n\nPhysics\nSimulation\nParticle collision\n\n\nDatabases\nSpatial joins\nR-Trees, indexing\n\n\n\nGeometry underpins structure, position, and relationship, the backbone of spatial reasoning.\n\n\n9. Complexity and Open Problems\nSome problems still challenge efficient solutions:\n\nPoint location in dynamic settings- Visibility graphs in complex polygons- Motion planning in high dimensions- Geometric median / center problems- Approximation guarantees in robust settings These remain active areas in computational geometry research.\n\n\n\nTiny Code (Point-in-Polygon via Ray Casting)\nbool inside(Point p, Polygon poly) {\n    int cnt = 0;\n    for (int i = 0; i &lt; poly.n; i++) {\n        Point a = poly[i], b = poly[(i + 1) % poly.n];\n        if (intersect_ray(p, a, b)) cnt++;\n    }\n    return cnt % 2 == 1; // odd crossings = inside\n}\nThis small routine appears everywhere, maps, games, GUIs, and physics engines.\n\n\n10. Why It Matters\nComputational geometry is more than shape, it’s the mathematics of space, powering visual computing, spatial data, and intelligent systems. Everywhere something moves, collides, maps, or recognizes form, geometry is the invisible hand guiding it.\n\n“All computation lives somewhere, and geometry is how we understand the where.”\n\n\n\nTry It Yourself\n\nImplement point-in-polygon and test on convex vs concave shapes.\nVisualize a Delaunay triangulation and its Voronoi dual.\nExperiment with KD-trees for nearest neighbor queries.\nWrite a small convex hull in 3D using incremental insertion.\nSketch an RRT path over a geometric map.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-9.-systems-databases-and-distributed-algorithms",
    "href": "books/en-us/book.html#chapter-9.-systems-databases-and-distributed-algorithms",
    "title": "The Book",
    "section": "Chapter 9. Systems, Databases, and Distributed Algorithms",
    "text": "Chapter 9. Systems, Databases, and Distributed Algorithms\n\n81. Concurrency Control (2PL, MVCC, OCC)\nIn multi-user or multi-threaded systems, many operations want to read or write shared data at the same time. Without discipline, this leads to chaos, lost updates, dirty reads, or even inconsistent states.\nConcurrency control ensures correctness under parallelism, so that the result is as if each transaction ran alone (a property called serializability).\nThis section introduces three foundational techniques:\n\n2PL - Two-Phase Locking- MVCC - Multi-Version Concurrency Control- OCC - Optimistic Concurrency Control\n\n\n1. The Goal: Serializability\nWe want transactions to behave as if executed in some serial order, even though they’re interleaved.\nA schedule is serializable if it yields the same result as some serial order of transactions.\nConcurrency control prevents problems like:\n\nLost Update: Two writes overwrite each other.- Dirty Read: Read uncommitted data.- Non-repeatable Read: Data changes mid-transaction.- Phantom Read: New rows appear after a query.\n\n\n\n2. Two-Phase Locking (2PL)\nIdea: Use locks to coordinate access. Each transaction has two phases:\n\nGrowing phase: acquire locks (shared or exclusive)\nShrinking phase: release locks (no new locks allowed after release)\n\nThis ensures conflict-serializability.\n\n\nLock Types\n\n\n\nType\nOperation\nShared?\nExclusive?\n\n\n\n\nShared (S)\nRead\nYes\nNo\n\n\nExclusive (X)\nWrite\nNo\nNo\n\n\n\nIf a transaction needs to read: request S-lock. If it needs to write: request X-lock.\nTiny Code (Lock Manager Sketch)\nvoid acquire_lock(Transaction *T, Item *X, LockType type) {\n    while (conflict_exists(X, type))\n        wait();\n    add_lock(X, T, type);\n}\n\nvoid release_all(Transaction *T) {\n    for (Lock *l in T-&gt;locks)\n        unlock(l);\n}\n\n\nExample\nT1: read(A); write(A)\nT2: read(A); write(A)\nWithout locks → race condition. With 2PL → one must wait → consistent.\n\n\nVariants\n\n\n\n\n\n\n\nVariant\nDescription\n\n\n\n\nStrict 2PL\nHolds all locks until commit → avoids cascading aborts\n\n\nRigorous 2PL\nSame as Strict, all locks released at end\n\n\nConservative 2PL\nAcquires all locks before execution\n\n\n\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nGuarantees serializability\nCan cause deadlocks\n\n\nSimple concept\nBlocking, contention under load\n\n\n\n\n\n3. Multi-Version Concurrency Control (MVCC)\nIdea: Readers don’t block writers, and writers don’t block readers. Each write creates a new version of data with a timestamp.\nTransactions read from a consistent snapshot based on their start time.\n\n\nSnapshot Isolation\n\nReaders see the latest committed version at transaction start.- Writers produce new versions; conflicts detected at commit time. Each record stores:\nvalue- created_at- deleted_at (if applicable)\n\nTiny Code (Version Chain)\nstruct Version {\n    int value;\n    Timestamp created;\n    Timestamp deleted;\n    Version *next;\n};\nRead finds version with created &lt;= tx.start && deleted &gt; tx.start.\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nNo read locks\nHigher memory (multiple versions)\n\n\nReaders never block\nWrite conflicts at commit\n\n\nGreat for OLTP systems\nGC of old versions needed\n\n\n\n\n\nUsed In\n\nPostgreSQL- Oracle- MySQL (InnoDB)- Spanner\n\n\n\n4. Optimistic Concurrency Control (OCC)\nIdea: Assume conflicts are rare. Let transactions run without locks. At commit time, validate, if conflicts exist, rollback.\n\n\nPhases\n\nRead phase - execute, read data, buffer writes.\nValidation phase - check if conflicts occurred.\nWrite phase - apply changes if valid, else abort.\n\nTiny Code (OCC Validation)\nbool validate(Transaction *T) {\n    for (Transaction *U in committed_since(T.start))\n        if (conflict(T, U))\n            return false;\n    return true;\n}\n\n\nPros & Cons\n\n\n\nPros\nCons\n\n\n\n\nNo locks → no deadlocks\nHigh abort rate under contention\n\n\nGreat for low-conflict workloads\nWasted work on abort\n\n\n\n\n\nUsed In\n\nIn-memory DBs- Distributed systems- STM (Software Transactional Memory)\n\n\n\n5. Choosing a Strategy\n\n\n\nSystem Type\nPreferred Control\n\n\n\n\nOLTP (many reads/writes)\nMVCC\n\n\nOLAP (read-heavy)\nMVCC or OCC\n\n\nReal-time systems\n2PL (predictable)\n\n\nLow contention\nOCC\n\n\nHigh contention\n2PL / MVCC\n\n\n\n\n\n6. Why It Matters\nConcurrency control is the backbone of consistency in databases, distributed systems, and even multi-threaded programs. It enforces correctness amid chaos, ensuring your data isn’t silently corrupted.\n\n“Without order, parallelism is noise. Concurrency control is its conductor.”\n\n\n\nTry It Yourself\n\nSimulate 2PL with two transactions updating shared data.\nImplement a toy MVCC table with version chains.\nWrite an OCC validator for three concurrent transactions.\nExperiment: under high conflict, which model performs best?\n\n\n\n\n82. Logging, Recovery, and Commit Protocols\nNo matter how elegant your algorithms or how fast your storage, failures happen. Power cuts, crashes, and network splits are inevitable. What matters is recovery, restoring the system to a consistent state without losing committed work.\nLogging, recovery, and commit protocols form the backbone of reliable transactional systems, ensuring durability and correctness in the face of crashes.\n\n1. The Problem\nWe need to guarantee the ACID properties:\n\nAtomicity - all or nothing- Consistency - valid before and after- Isolation - no interference- Durability - once committed, always safe If a crash occurs mid-transaction, how do we roll back or redo correctly?\n\nThe answer: Log everything, then replay or undo after failure.\n\n\n2. Write-Ahead Logging (WAL)\nThe golden rule:\n\n“Write log entries before modifying the database.”\n\nEvery action is recorded in a sequential log on disk, ensuring the system can reconstruct the state.\n\n\nLog Record Format\nEach log entry typically includes:\n\nLSN (Log Sequence Number)- Transaction ID- Operation (update, insert, delete)- Before image (old value)- After image (new value)\n\nstruct LogEntry {\n    int lsn;\n    int tx_id;\n    char op[10];\n    Value before, after;\n};\nWhen a transaction commits, the system first flushes logs to disk (fsync). Only then is the commit acknowledged.\n\n\n3. Recovery Actions\nWhen the system restarts, it reads logs and applies a recovery algorithm.\n\n\nThree Phases (ARIES Model)\n\nAnalysis - determine state at crash (active vs committed)\nRedo - repeat all actions from last checkpoint\nUndo - rollback incomplete transactions\n\nARIES (Algorithm for Recovery and Isolation Exploiting Semantics) is the most widely used approach (IBM DB2, PostgreSQL, SQL Server).\n\n\nRedo Rule\nIf the system committed before crash → redo all updates so data is preserved.\n\n\nUndo Rule\nIf the system didn’t commit → undo to maintain atomicity.\nTiny Code (Simplified Recovery Sketch)\nvoid recover(Log log) {\n    for (Entry e : log) {\n        if (e.committed)\n            apply(e.after);\n        else\n            apply(e.before);\n    }\n}\n\n\n4. Checkpointing\nInstead of replaying the entire log, systems take checkpoints, periodic snapshots marking a safe state.\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSharp checkpoint\nStop all transactions briefly, flush data + log\n\n\nFuzzy checkpoint\nMark consistent LSN; continue running\n\n\n\nCheckpoints reduce recovery time: only replay after the last checkpoint.\n\n\n5. Commit Protocols\nIn distributed systems, multiple nodes must agree to commit or abort together. This is handled by atomic commit protocols.\n\n\nTwo-Phase Commit (2PC)\nGoal: All participants either commit or abort in unison.\nSteps:\n\nPrepare phase (voting):\n\nCoordinator asks all participants to “prepare” - Each replies yes/no2. Commit phase (decision):\nIf all say yes → commit - Else → abort\n\nCoordinator: PREPARE  \nParticipants: VOTE YES / NO  \nCoordinator: COMMIT / ABORT\n\nIf the coordinator crashes after prepare, participants must wait → blocking protocol.\nTiny Code (2PC Pseudocode)\nbool two_phase_commit(Participants P) {\n    for each p in P:\n        if (!p.prepare()) return abort_all();\n    for each p in P:\n        p.commit();\n    return true;\n}\n\n\nThree-Phase Commit (3PC)\nImproves on 2PC by adding an intermediate phase to avoid indefinite blocking. More complex, used in systems with reliable failure detection.\n\n\n6. Logging in Distributed Systems\nEach participant maintains its own WAL. To recover globally:\n\nUse coordinated checkpoints- Maintain global commit logs- Consensus-based protocols (Paxos Commit, Raft) can replace 2PC for high availability\n\n\n\n7. Example Timeline\n\n\n\nStep\nAction\n\n\n\n\nT1 updates record A\nWAL entry written\n\n\nT1 updates record B\nWAL entry written\n\n\nT1 commits\nWAL flush, commit record\n\n\nCrash!\nDisk may be inconsistent\n\n\nRestart\nRecovery scans log, redoes T1\n\n\n\n\n\n8. Pros and Cons\n\n\n\nApproach\nStrength\nWeakness\n\n\n\n\nWAL\nSimple, durable\nWrite overhead\n\n\nCheckpointing\nFaster recovery\nI/O spikes\n\n\n2PC\nGlobal atomicity\nBlocking\n\n\n3PC / Consensus\nNon-blocking\nComplex, slower\n\n\n\n\n\n9. Real Systems\n\n\n\nSystem\nStrategy\n\n\n\n\nPostgreSQL\nWAL + ARIES + Checkpoint\n\n\nMySQL (InnoDB)\nWAL + Fuzzy checkpoint\n\n\nSpanner\nWAL + 2PC + TrueTime\n\n\nKafka\nWAL for durability\n\n\nRocksDB\nWAL + LSM checkpoints\n\n\n\n\n\n10. Why It Matters\nLogging and commit protocols make data survive crashes and stay consistent across machines. Without them, every failure risks corruption.\n\n“Persistence is not about never failing, it’s about remembering how to stand back up.”\n\n\n\nTry It Yourself\n\nWrite a toy WAL system that logs before writes.\nSimulate a crash mid-transaction and replay the log.\nImplement a simple 2PC coordinator with two participants.\nCompare recovery time with vs without checkpoints.\n\n\n\n\n83. Scheduling (Round Robin, EDF, Rate-Monotonic)\nIn operating systems and real-time systems, scheduling determines the order in which tasks or processes run. Since resources like CPU time are limited, a good scheduler aims to balance fairness, efficiency, and responsiveness.\n\n1. The Goal of Scheduling\nEvery system has tasks competing for the CPU. Scheduling decides:\n\nWhich task runs next- How long it runs- When it yields or preempts Different goals apply in different domains:\n\n\n\n\nDomain\nObjective\n\n\n\n\nGeneral-purpose OS\nFairness, responsiveness\n\n\nReal-time systems\nMeeting deadlines\n\n\nEmbedded systems\nPredictability\n\n\nHigh-performance servers\nThroughput, latency balance\n\n\n\nA scheduler’s policy can be preemptive (interrupts tasks) or non-preemptive (waits for voluntary yield).\n\n\n2. Round Robin Scheduling\nRound Robin (RR) is one of the simplest preemptive schedulers. Each process gets a fixed time slice (quantum) and runs in a circular queue.\nIf a process doesn’t finish, it’s put back at the end of the queue.\n\n\nTiny Code: Round Robin (Pseudocode)\nqueue processes;\nwhile (!empty(processes)) {\n    process = dequeue(processes);\n    run_for_quantum(process);\n    if (!process.finished)\n        enqueue(processes, process);\n}\n\n\nCharacteristics\n\nFair: Every process gets CPU time.- Responsive: Short tasks don’t starve.- Downside: Context switching overhead if quantum is too small. #### Example\n\n\n\n\nProcess\nBurst Time\n\n\n\n\n\nP1\n4\n\n\n\nP2\n3\n\n\n\nP3\n2\n\n\n\n\nQuantum = 1 Order: P1, P2, P3, P1, P2, P3, P1, P2 → all finish fairly.\n\n\n3. Priority Scheduling\nEach task has a priority. The scheduler always picks the highest-priority ready task.\n\nPreemptive: A higher-priority task can interrupt a lower one.- Non-preemptive: The CPU is released voluntarily. #### Problems\nStarvation: Low-priority tasks may never run.- Solution: Aging - gradually increase waiting task priority.\n\n\n\n4. Earliest Deadline First (EDF)\nEDF is a dynamic priority scheduler for real-time systems. Each task has a deadline, and the task with the earliest deadline runs first.\n\n\nRule\nAt any time, run the ready task with the closest deadline.\n\n\nExample\n\n\n\nTask\nExecution Time\nDeadline\n\n\n\n\nT1\n1\n3\n\n\nT2\n2\n5\n\n\nT3\n1\n2\n\n\n\nOrder: T3 → T1 → T2\nEDF is optimal for preemptive scheduling of independent tasks on a single processor.\n\n\n5. Rate-Monotonic Scheduling (RMS)\nIn periodic real-time systems, tasks repeat at fixed intervals. RMS assigns higher priority to tasks with shorter periods.\n\n\n\nTask\nPeriod\nPriority\n\n\n\n\nT1\n2 ms\nHigh\n\n\nT2\n5 ms\nMedium\n\n\nT3\n10 ms\nLow\n\n\n\nIt’s static (priorities don’t change) and optimal among fixed-priority schedulers.\n\n\nUtilization Bound\nFor n tasks, RMS is guaranteed schedulable if:\n\\[\nU = \\sum_{i=1}^{n} \\frac{C_i}{T_i} \\le n(2^{1/n} - 1)\n\\]\nFor example, for 3 tasks, \\(U \\le 0.78\\).\n\n\n6. Shortest Job First (SJF)\nRun the task with the shortest burst time first.\n\nNon-preemptive SJF: Once started, runs to completion.- Preemptive SJF (Shortest Remaining Time First): Preempts if a shorter job arrives. Advantage: Minimizes average waiting time. Disadvantage: Needs knowledge of future job lengths.\n\n\n\n7. Multilevel Queue Scheduling\nDivide processes into classes (interactive, batch, system). Each class has its own queue with own policy, e.g.:\n\nQueue 1: System → RR (quantum = 10ms)- Queue 2: Interactive → RR (quantum = 50ms)- Queue 3: Batch → FCFS (First-Come-First-Serve) CPU is assigned based on queue priority.\n\n\n\n8. Multilevel Feedback Queue (MLFQ)\nProcesses move between queues based on behavior.\n\nCPU-bound → move down (lower priority)- I/O-bound → move up (higher priority) Goal: Adaptive scheduling that rewards interactive tasks.\n\nUsed in modern OS kernels (Linux, Windows).\n\n\n9. Scheduling Metrics\n\n\n\nMetric\nMeaning\n\n\n\n\nTurnaround Time\nCompletion − Arrival\n\n\nWaiting Time\nTime spent in ready queue\n\n\nResponse Time\nTime from arrival to first execution\n\n\nThroughput\nCompleted tasks per unit time\n\n\nCPU Utilization\n% of time CPU is busy\n\n\n\nSchedulers balance these based on design goals.\n\n\n10. Why It Matters\nSchedulers shape how responsive, efficient, and fair a system feels. In operating systems, they govern multitasking. In real-time systems, they ensure deadlines are met. In servers, they keep latency low and throughput high.\n\n“Scheduling is not just about time. It’s about fairness, foresight, and flow.”\n\n\n\nTry It Yourself\n\nSimulate Round Robin with quantum = 2, compare average waiting time.\nImplement EDF for a set of periodic tasks with deadlines.\nCheck schedulability under RMS for 3 periodic tasks.\nExplore Linux CFS (Completely Fair Scheduler) source code.\nCompare SJF and RR for CPU-bound vs I/O-bound workloads.\n\n\n\n\n84. Caching and Replacement (LRU, LFU, CLOCK)\nCaching is the art of remembering the past to speed up the future. In computing, caches store recently used or frequently accessed data to reduce latency and load on slower storage (like disks or networks). The challenge: caches have limited capacity, so when full, we must decide what to evict. That’s where replacement policies come in.\n\n1. The Need for Caching\nCaches appear everywhere:\n\nCPU: L1, L2, L3 caches speed up memory access- Databases: query results or index pages- Web browsers / CDNs: recently fetched pages- Operating systems: page cache for disk blocks The principle guiding all caches is locality:\nTemporal locality: recently used items are likely used again soon- Spatial locality: nearby items are likely needed next\n\n\n\n2. Cache Replacement Problem\nWhen the cache is full, which item should we remove?\nWe want to minimize cache misses (requests not found in cache).\nFormally:\n\nGiven a sequence of accesses, find a replacement policy that minimizes misses.\n\nTheoretical optimal policy (OPT): always evict the item used farthest in the future. But OPT requires future knowledge, so we rely on heuristics like LRU, LFU, CLOCK.\n\n\n3. Least Recently Used (LRU)\nLRU evicts the least recently accessed item. It assumes recently used = likely to be used again.\n\n\nImplementation Approaches\n\nStack (list): move item to top on access- Hash map + doubly linked list: O(1) insert, delete, lookup #### Tiny Code: LRU (Simplified)\n\ntypedef struct Node {\n    int key;\n    struct Node *prev, *next;\n} Node;\n\nHashMap cache;\nList lru_list;\n\nvoid access(int key) {\n    if (in_cache(key)) move_to_front(key);\n    else {\n        if (cache_full()) remove_lru();\n        insert_front(key);\n    }\n}\n\n\nPros\n\nGood for workloads with strong temporal locality #### Cons\nCostly in hardware or massive caches (metadata overhead)\n\n\n\n4. Least Frequently Used (LFU)\nLFU evicts the least frequently accessed item.\nTracks usage count for each item:\n\nIncrement on each access- Evict lowest-count item #### Example\n\n\n\n\nItem\nAccesses\nFrequency\n\n\n\n\nA\n3\n3\n\n\nB\n1\n1\n\n\nC\n2\n2\n\n\n\nEvict B.\n\n\nVariants\n\nLFU with aging: gradually reduce counts to adapt to new trends- Approximate LFU: counters in ranges (for memory efficiency) #### Pros\nGreat for stable, repetitive workloads #### Cons\nPoor for workloads with shifting popularity (slow adaptation)\n\n\n\n5. FIFO (First In First Out)\nSimple but naive:\n\nEvict the oldest item, ignoring usage Used in simple hardware caches. Good when access pattern is cyclic, bad otherwise.\n\n\n\n6. Random Replacement (RR)\nEvict a random entry.\nSurprisingly competitive in some high-concurrency systems, and trivial to implement. Used in memcached (as an option).\n\n\n7. CLOCK Algorithm\nA practical approximation of LRU, widely used in OS page replacement.\nEach page has a reference bit (R). Pages form a circular list.\nAlgorithm:\n\nClock hand sweeps over pages.\nIf R = 0, evict page.\nIf R = 1, set R = 0 and skip.\n\nThis mimics LRU with O(1) cost and low overhead.\n\n\n8. Second-Chance and Enhanced CLOCK\nSecond-Chance: give recently used pages a “second chance” before eviction. Enhanced CLOCK: also uses modify bit (M) to prefer clean pages.\nUsed in Linux’s page replacement (with Active/Inactive lists).\n\n\n9. Adaptive Algorithms\nModern systems use hybrid or adaptive policies:\n\nARC (Adaptive Replacement Cache) - balances recency and frequency- CAR (Clock with Adaptive Replacement) - CLOCK-style adaptation- TinyLFU - frequency sketch + admission policy- Hyperbolic caching - popularity decay for large-scale systems These adapt dynamically to changing workloads.\n\n\n\n10. Why It Matters\nCaching is the backbone of system speed:\n\nOS uses it for paging- Databases for buffer pools- CPUs for memory hierarchies- CDNs for global acceleration Choosing the right eviction policy can mean orders of magnitude improvement in latency and throughput.\n\n\n“A good cache remembers what matters, and forgets what no longer does.”\n\n\n\nTry It Yourself\n\nSimulate a cache of size 3 with sequence: A B C A B D A B C D Compare LRU, LFU, and FIFO miss counts.\nImplement LRU with a doubly-linked list and hash map in C.\nTry CLOCK with reference bits, simulate a sweep.\nExperiment with ARC and TinyLFU for dynamic workloads.\nMeasure hit ratios for different access patterns (sequential, random, looping).\n\n\n\n\n85. Networking (Routing, Congestion Control)\nNetworking algorithms make sure data finds its way through vast, connected systems, efficiently, reliably, and fairly. Two core pillars of network algorithms are routing (deciding where packets go) and congestion control (deciding how fast to send them).\nTogether, they ensure the internet functions under heavy load, dynamic topology, and unpredictable demand.\n\n1. The Goals of Networking Algorithms\n\nCorrectness: all destinations are reachable if paths exist- Efficiency: use minimal resources (bandwidth, latency, hops)- Scalability: support large, dynamic networks- Robustness: recover from failures- Fairness: avoid starving flows\n\n\n\n2. Types of Routing\nRouting decides paths packets should follow through a graph-like network.\n\n\nStatic vs Dynamic Routing\n\nStatic: fixed routes, manual configuration (good for small networks)- Dynamic: routes adjust automatically as topology changes (internet-scale) #### Unicast, Multicast, Broadcast\nUnicast: one-to-one (most traffic)- Multicast: one-to-many (video streaming, gaming)- Broadcast: one-to-all (local networks)\n\n\n\n3. Shortest Path Routing\nMost routing relies on shortest path algorithms:\n\n\nDijkstra’s Algorithm\n\nBuilds shortest paths from one source- Complexity: O(E log V) with priority queue Used in:\nOSPF (Open Shortest Path First)- IS-IS (Intermediate System to Intermediate System) #### Bellman-Ford Algorithm\nHandles negative edges- Basis for Distance-Vector routing (RIP) #### Tiny Code: Dijkstra for Routing\n\n#define INF 1e9\nint dist[MAX], visited[MAX];\nvector&lt;pair&lt;int,int&gt;&gt; adj[MAX];\n\nvoid dijkstra(int s, int n) {\n    for (int i = 0; i &lt; n; i++) dist[i] = INF;\n    dist[s] = 0;\n    priority_queue&lt;pair&lt;int,int&gt;&gt; pq;\n    pq.push({0, s});\n    while (!pq.empty()) {\n        int u = pq.top().second; pq.pop();\n        if (visited[u]) continue;\n        visited[u] = 1;\n        for (auto [v, w]: adj[u]) {\n            if (dist[v] &gt; dist[u] + w) {\n                dist[v] = dist[u] + w;\n                pq.push({-dist[v], v});\n            }\n        }\n    }\n}\n\n\n4. Distance-Vector vs Link-State\n\n\n\nFeature\nDistance-Vector (RIP)\nLink-State (OSPF)\n\n\n\n\nInfo Shared\nDistance to neighbors\nFull topology map\n\n\nConvergence\nSlower (loops possible)\nFast (SPF computation)\n\n\nComplexity\nLower\nHigher\n\n\nExamples\nRIP, BGP (conceptually)\nOSPF, IS-IS\n\n\n\nRIP uses Bellman-Ford. OSPF floods link-state updates, runs Dijkstra at each node.\n\n\n5. Hierarchical Routing\nLarge-scale networks (like the Internet) use hierarchical routing:\n\nRouters grouped into Autonomous Systems (AS)- Intra-AS routing: OSPF, IS-IS- Inter-AS routing: BGP (Border Gateway Protocol) BGP exchanges reachability info, not shortest paths, and prefers policy-based routing (e.g., cost, contracts, peering).\n\n\n\n6. Congestion Control\nEven with good routes, we can’t flood links. Congestion control ensures fair and efficient use of bandwidth.\nImplemented primarily at the transport layer (TCP).\n\n\nTCP Congestion Control\nKey components:\n\nAdditive Increase, Multiplicative Decrease (AIMD)- Slow Start: probe capacity- Congestion Avoidance: grow cautiously- Fast Retransmit / Recovery Modern variants:\nTCP Reno: classic AIMD- TCP Cubic: non-linear growth for high-speed networks- BBR (Bottleneck Bandwidth + RTT): model-based control #### Algorithm Sketch (AIMD)\n\nOn ACK: cwnd += 1/cwnd  // increase slowly\nOn loss: cwnd /= 2      // halve window\n\n\n7. Queue Management\nRouters maintain queues. Too full? =&gt; Packet loss, latency spikes, tail drop.\nSolutions:\n\nRED (Random Early Detection) - drop packets early- CoDel (Controlled Delay) - monitor queue delay, drop adaptively These prevent bufferbloat, improving latency for real-time traffic.\n\n\n\n8. Flow Control vs Congestion Control\n\nFlow Control: prevent sender from overwhelming receiver- Congestion Control: prevent sender from overwhelming network TCP uses both: receive window (rwnd) and congestion window (cwnd). Actual sending rate = min(rwnd, cwnd).\n\n\n\n9. Data Plane vs Control Plane\n\nControl Plane: decides routes (OSPF, BGP)- Data Plane: forwards packets (fast path) Modern networking (e.g. SDN, Software Defined Networking) separates these:\nController computes routes- Switches act on flow rules\n\n\n\n10. Why It Matters\nRouting and congestion control shape the performance of:\n\nThe Internet backbone- Data center networks (with load balancing)- Cloud services and microservice meshes- Content delivery networks (CDNs) Every packet’s journey, from your laptop to a global data center, relies on these ideas.\n\n\n“Networking is not magic. It’s algorithms moving data through time and space.”\n\n\n\nTry It Yourself\n\nImplement Dijkstra’s algorithm for a small network graph.\nSimulate RIP (Distance Vector): each node updates from neighbors.\nModel TCP AIMD window growth; visualize with Python.\nTry RED: drop packets when queue length &gt; threshold.\nCompare TCP Reno, Cubic, BBR throughput in simulation.\n\n\n\n\n86. Distributed Consensus (Paxos, Raft, PBFT)\nIn a distributed system, multiple nodes must agree on a single value, for example, the state of a log, a database entry, or a blockchain block. This agreement process is called consensus.\nConsensus algorithms let distributed systems act as one reliable system, even when some nodes fail, crash, or lie (Byzantine faults).\n\n1. Why Consensus?\nImagine a cluster managing a shared log (like in databases or Raft). Each node might:\n\nSee different requests,- Fail and recover,- Communicate over unreliable links. We need all non-faulty nodes to agree on the same order of operations.\n\nA valid consensus algorithm must satisfy:\n\nAgreement: all correct nodes choose the same value- Validity: the chosen value was proposed by a node- Termination: every correct node eventually decides- Fault Tolerance: works despite failures\n\n\n\n2. The FLP Impossibility\nThe FLP theorem (Fischer, Lynch, Paterson, 1985) says:\n\nIn an asynchronous system with even one faulty process, no deterministic algorithm can guarantee consensus.\n\nSo practical algorithms use:\n\nRandomization, or- Partial synchrony (timeouts, retries)\n\n\n\n3. Paxos: The Classical Algorithm\nPaxos, by Leslie Lamport, is the theoretical foundation for distributed consensus.\nIt revolves around three roles:\n\nProposers: suggest values- Acceptors: vote on proposals- Learners: learn the final decision Consensus proceeds in two phases.\n\n\n\nPhase 1 (Prepare)\n\nProposer picks a proposal number n and sends (Prepare, n) to acceptors.\nAcceptors respond with their highest accepted proposal (if any).\n\n\n\nPhase 2 (Accept)\n\nIf proposer receives a majority of responses, it sends (Accept, n, v) with value v (highest seen or new).\nAcceptors accept if they haven’t promised higher n.\n\nWhen a majority accept, value v is chosen.\n\n\nGuarantees\n\nSafety: no two different values chosen- Liveness: possible under stable leadership #### Drawbacks\nComplex to implement correctly- High messaging overhead &gt; “Paxos is for theorists; Raft is for engineers.”\n\n\n\n4. Raft: Understandable Consensus\nRaft was designed to be simpler and more practical than Paxos, focusing on replicated logs.\n\n\nRoles\n\nLeader: coordinates all changes- Followers: replicate leader’s log- Candidates: during elections #### Workflow\n\n\nLeader Election\n\nTimeout triggers candidate election. - Each follower votes; majority wins.2. Log Replication\nLeader appends entries, sends AppendEntries RPCs. - Followers acknowledge; leader commits when majority ack.3. Safety\nLogs are consistent across majority. - Followers accept only valid prefixes. Raft ensures:\n\n\n\nAt most one leader per term- Committed entries never lost- Logs stay consistent #### Pseudocode Sketch\n\non timeout -&gt; become_candidate()\nsend RequestVote(term, id)\nif majority_votes -&gt; become_leader()\n\non AppendEntries(term, entries):\n    if term &gt;= current_term:\n        append(entries)\n        reply success\n\n\n5. PBFT: Byzantine Fault Tolerance\nPaxos and Raft assume crash faults (nodes stop, not lie). For Byzantine faults (arbitrary behavior), we use PBFT (Practical Byzantine Fault Tolerance).\nTolerates up to f faulty nodes out of 3f + 1 total.\n\n\nPhases\n\nPre-Prepare: Leader proposes value\nPrepare: Nodes broadcast proposal hashes\nCommit: Nodes confirm receipt by 2f+1 votes\n\nUsed in blockchains and critical systems (space, finance).\n\n\n6. Quorum Concept\nConsensus often relies on quorums (majorities):\n\nTwo quorums always intersect, ensuring consistency.- Write quorum + read quorum ≥ total nodes. In Raft/Paxos:\nMajority = N/2 + 1- Guarantees overlap even if some nodes fail.\n\n\n\n7. Log Replication and State Machines\nConsensus underlies Replicated State Machines (RSM):\n\nEvery node applies the same commands in the same order.- Guarantees deterministic, identical states. This model powers:\nDatabases (etcd, Spanner, TiKV)- Coordination systems (ZooKeeper, Consul)- Kubernetes control planes\n\n\n\n8. Leader Election\nAll practical consensus systems need leaders:\n\nSimplifies coordination- Reduces conflicts- Heartbeats detect failures- New elections restore progress Algorithms:\nRaft Election (random timeouts)- Bully Algorithm- Chang-Roberts Ring Election\n\n\n\n9. Performance and Optimization\n\nBatching: amortize RPC overhead- Pipeline: parallelize appends- Read-only optimizations: serve from followers (stale reads)- Witness nodes: participate in quorum without full data Advanced:\nMulti-Paxos: reuse leader, fewer rounds- Fast Paxos: shortcut some phases- Viewstamped Replication: Paxos-like log replication\n\n\n\n10. Why It Matters\nConsensus is the backbone of reliability in modern distributed systems. Every consistent database, service registry, or blockchain depends on it.\nSystems using consensus:\n\netcd, Consul, ZooKeeper - cluster coordination- Raft in Kubernetes - leader election- PBFT in blockchains - fault-tolerant ledgers- Spanner, TiDB - consistent databases &gt; “Consensus is how machines learn to agree, and trust.”\n\n\n\nTry It Yourself\n\nImplement Raft leader election in C or Python.\nSimulate Paxos on 5 nodes with message drops.\nExplore PBFT: try failing nodes and Byzantine behavior.\nCompare performance of Raft vs Paxos under load.\nBuild a replicated key-value store with Raft.\n\n\n\n\n87. Load Balancing and Rate Limiting\nWhen systems scale, no single server can handle all requests alone. Load balancing distributes incoming traffic across multiple servers to improve throughput, reduce latency, and prevent overload. Meanwhile, rate limiting protects systems by controlling how often requests are allowed, ensuring fairness, stability, and security.\nThese two ideas, spreading the load and controlling the flow, are cornerstones of modern distributed systems and APIs.\n\n1. Why Load Balancing Matters\nImagine a web service receiving thousands of requests per second. If every request went to one machine, it would crash. A load balancer (LB) acts as a traffic director, spreading requests across many backends.\nGoals:\n\nEfficiency - fully utilize servers- Reliability - no single point of failure- Scalability - handle growing workloads- Flexibility - add/remove servers dynamically\n\n\n\n2. Types of Load Balancers\n\n\n1. Layer 4 (Transport Layer)\nBalances based on IP and port. Fast and protocol-agnostic (works for TCP/UDP).\nExample: Linux IPVS, Envoy, HAProxy\n\n\n2. Layer 7 (Application Layer)\nUnderstands protocols like HTTP. Can route by URL path, headers, cookies.\nExample: Nginx, Envoy, AWS ALB\n\n\n3. Load Balancing Algorithms\n\n\nRound Robin\nCycles through backends in order.\nReq1 → ServerA  \nReq2 → ServerB  \nReq3 → ServerC\nSimple, fair (if all nodes equal).\n\n\nWeighted Round Robin\nAssigns weights to reflect capacity. Example: ServerA(2x), ServerB(1x)\n\n\nLeast Connections\nSend request to server with fewest active connections.\n\n\nLeast Response Time\nSelect backend with lowest latency (monitored dynamically).\n\n\nHash-Based (Consistent Hashing)\nDeterministically route based on request key (like user ID).\n\nKeeps cache locality- Used in CDNs, distributed caches (e.g. memcached) #### Random\n\nPick a random backend, surprisingly effective under uniform load.\n\n\n4. Consistent Hashing (In Depth)\nUsed for sharding and sticky sessions.\nKey idea:\n\nMap servers to a hash ring- A request’s key is hashed onto the ring- Assigned to next clockwise server When servers join/leave, only small fraction of keys move.\n\nUsed in:\n\nCDNs- Distributed caches (Redis Cluster, DynamoDB)- Load-aware systems\n\n\n\n5. Health Checks and Failover\nA smart LB monitors health of each server:\n\nHeartbeat pings (HTTP/TCP)- Auto-remove unhealthy servers- Rebalance traffic instantly Example: If ServerB fails, remove from rotation:\n\nHealthy: [ServerA, ServerC]\nAlso supports active-passive failover: hot standby servers take over when active fails.\n\n\n6. Global Load Balancing\nAcross regions or data centers:\n\nGeoDNS: route to nearest region- Anycast: advertise same IP globally; routing picks nearest- Latency-based routing: measure and pick lowest RTT Used by CDNs, cloud services, multi-region apps\n\n\n\n7. Rate Limiting: The Other Side\nIf load balancing spreads the work, rate limiting keeps total work reasonable.\nIt prevents:\n\nAbuse (bots, DDoS)- Overload (too many requests)- Fairness issues (no user dominates resources) Policies:\nPer-user, per-IP, per-API-key- Global or per-endpoint\n\n\n\n8. Rate Limiting Algorithms\n\n\nToken Bucket\n\nBucket holds tokens (capacity = burst limit)- Each request consumes 1 token- Tokens refill at constant rate (rate limit)- If empty → reject or delay Good for bursty traffic.\n\nif (tokens &gt; 0) {\n    tokens--;\n    allow();\n} else reject();\n\n\nLeaky Bucket\n\nRequests flow into a bucket, drain at fixed rate- Excess = overflow = dropped Smooths bursts; used for shaping.\n\n\n\nFixed Window Counter\n\nCount requests in fixed interval (e.g. 1s)- Reset every window- Simple but unfair around boundaries #### Sliding Window Log / Sliding Window Counter\nMaintain timestamps of requests- Remove old ones beyond time window- More accurate and fair\n\n\n\n9. Combining Both\nA full system might:\n\nUse rate limiting per user or service- Use load balancing across nodes- Apply circuit breakers when overload persists Together, they form resilient architectures that stay online even under spikes.\n\n\n\n10. Why It Matters\nThese techniques make large-scale systems:\n\nScalable - handle millions of users- Stable - prevent cascading failures- Fair - each client gets a fair share- Resilient - recover gracefully from spikes or node loss Used in:\nAPI Gateways (Kong, Envoy, Nginx)- Cloud Load Balancers (AWS ALB, GCP LB)- Kubernetes Ingress and Service Meshes- Distributed Caches and Databases &gt; “Balance keeps systems alive. Limits keep them sane.”\n\n\n\nTry It Yourself\n\nSimulate Round Robin and Least Connections balancing across 3 servers.\nImplement a Token Bucket rate limiter in C or Python.\nTest burst traffic, observe drops or delays.\nCombine Consistent Hashing with Token Bucket for user-level control.\nVisualize how load balancing + rate limiting keep system latency low.\n\n\n\n\n88. Search and Indexing (Inverted, BM25, WAND)\nSearch engines, whether web-scale like Google or local like SQLite’s FTS, rely on efficient indexing and ranking to answer queries fast. Instead of scanning all documents, they use indexes (structured lookup tables) to quickly find relevant matches.\nThis section explores inverted indexes, ranking algorithms (TF-IDF, BM25), and efficient retrieval techniques like WAND.\n\n1. The Search Problem\nGiven:\n\nA corpus of documents- A query (e.g., “machine learning algorithms”) We want to return:\nRelevant documents- Ranked by importance and similarity Naive search → O(N × M) comparisons Inverted indexes → O(K log N), where K = terms in query\n\n\n\n2. Inverted Index: The Heart of Search\nAn inverted index maps terms → documents containing them.\n\n\nExample\n\n\n\nTerm\nPostings List\n\n\n\n\n“data”\n[1, 4, 5]\n\n\n“algorithm”\n[2, 3, 5]\n\n\n“machine”\n[1, 2]\n\n\n\nEach posting may include:\n\ndocID- term frequency (tf)- positions (for phrase search) #### Construction Steps\n\n\nTokenize documents → words\nNormalize (lowercase, stemming, stopword removal)\nBuild postings: term → [docIDs, tf, positions]\nSort & compress for storage efficiency\n\nUsed by:\n\nElasticsearch, Lucene, Whoosh, Solr\n\n\n\n3. Boolean Retrieval\nSimplest model:\n\nQuery = Boolean expression e.g. (machine AND learning) OR AI\n\nUse set operations on postings:\n\nAND → intersection- OR → union- NOT → difference Fast intersection uses merge algorithm on sorted lists.\n\nvoid intersect(int A[], int B[], int n, int m) {\n    int i = 0, j = 0;\n    while (i &lt; n && j &lt; m) {\n        if (A[i] == B[j]) { print(A[i]); i++; j++; }\n        else if (A[i] &lt; B[j]) i++;\n        else j++;\n    }\n}\nBut Boolean search doesn’t rank results, so we need scoring models.\n\n\n4. Vector Space Model\nRepresent documents and queries as term vectors. Each dimension = term weight (tf-idf).\n\ntf: term frequency in document- idf: inverse document frequency \\(idf = \\log\\frac{N}{df_t}\\)\n\nCosine similarity measures relevance: \\[\n\\text{score}(q, d) = \\frac{q \\cdot d}{|q| |d|}\n\\]\nSimple, interpretable, forms basis of BM25 and modern embeddings.\n\n\n5. BM25: The Classic Ranking Function\nBM25 (Best Match 25) is the de facto standard in information retrieval.\n\\[\n\\text{score}(q, d) = \\sum_{t \\in q} IDF(t) \\cdot \\frac{f(t, d) \\cdot (k_1 + 1)}{f(t, d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{avgdl})}\n\\]\nWhere:\n\n( f(t, d) ): term frequency- ( |d| ): doc length- ( avgdl ): average doc length- \\(k_1, b\\): tunable params (typ. 1.2-2.0, 0.75) #### Advantages\nBalances term frequency, document length, and rarity- Fast and effective baseline- Still used in Elasticsearch, Lucene, OpenSearch\n\n\n\n6. Efficiency Tricks: WAND, Block-Max WAND\nRanking involves merging multiple postings. We can skip irrelevant documents early with WAND (Weak AND).\n\n\nWAND Principle\n\nEach term has upper-bound score- Maintain pointers in each posting- Compute potential max score- If max &lt; current threshold, skip doc Improves latency for top-k retrieval.\n\nVariants:\n\nBMW (Block-Max WAND) - uses block-level score bounds- MaxScore - simpler thresholding- Dynamic pruning - skip unpromising candidates\n\n\n\n7. Index Compression\nPostings lists are long, compression is crucial.\nCommon schemes:\n\nDelta encoding: store gaps between docIDs- Variable-byte (VB) or Gamma coding- Frame of Reference (FOR) and SIMD-BP128 for vectorized decoding Goal: smaller storage + faster decompression\n\n\n\n8. Advanced Retrieval\n\n\nProximity Search\nRequire words appear near each other. Use positional indexes.\n\n\nPhrase Search\nMatch exact sequences using positions: “machine learning” ≠ “learning machine”\n\n\nFuzzy / Approximate Search\nAllow typos: Use Levenshtein automata, n-grams, or k-approximate matching\n\n\nFielded Search\nScore per field (title, body, tags) Weighted combination\n\n\n9. Learning-to-Rank and Semantic Search\nModern search adds ML-based re-ranking:\n\nLearning to Rank (LTR): use features (tf, idf, BM25, clicks)- Neural re-ranking: BERT-style embeddings for semantic similarity- Hybrid retrieval: combine BM25 + dense vectors (e.g. ColBERT, RRF) Also: ANN (Approximate Nearest Neighbor) for vector-based search.\n\n\n\n10. Why It Matters\nEfficient search powers:\n\nWeb search engines- IDE symbol lookup- Log search, code search- Database full-text search- AI retrieval pipelines (RAG) It’s where algorithms meet language and scale.\n\n\n“Search is how we connect meaning to memory.”\n\n\n\nTry It Yourself\n\nBuild a tiny inverted index in C or Python.\nImplement Boolean AND and OR queries.\nCompute TF-IDF and BM25 scores for a toy dataset.\nAdd WAND pruning for top-k retrieval.\nCompare BM25 vs semantic embeddings for relevance.\n\n\n\n\n89. Compression and Encoding in Systems\nCompression and encoding algorithms are the quiet workhorses of computing, shrinking data to save space, bandwidth, and time. They allow systems to store more, transmit faster, and process efficiently. From files and databases to networks and logs, compression shapes nearly every layer of system design.\n\n1. Why Compression Matters\nCompression is everywhere:\n\nDatabases - column stores, indexes, logs- Networks - HTTP, TCP, QUIC payloads- File systems - ZFS, NTFS, btrfs compression- Streaming - video/audio codecs- Logs & telemetry - reduce I/O and storage cost Benefits:\nSmaller data = faster I/O- Less storage = lower cost- Less transfer = higher throughput Trade-offs:\nCPU overhead (compression/decompression)- Latency (especially for small data)- Suitability (depends on entropy and structure)\n\n\n\n2. Key Concepts\n\n\nEntropy\nMinimum bits needed to represent data (Shannon). High entropy → less compressible.\n\n\nRedundancy\nCompression exploits repetition and patterns.\n\n\nLossless vs Lossy\n\nLossless: reversible (ZIP, PNG, LZ4)- Lossy: approximate (JPEG, MP3, H.264) In system contexts, lossless dominates.\n\n\n\n3. Common Lossless Compression Families\n\n\nHuffman Coding\n\nPrefix-free variable-length codes- Frequent symbols = short codes- Optimal under symbol-level model Used in: DEFLATE, JPEG, MP3\n\n\n\nArithmetic Coding\n\nEncodes sequence as fractional interval- More efficient than Huffman for skewed distributions- Used in: H.264, bzip2, AV1 #### Dictionary-Based (LZ77, LZ78)\nReplace repeated substrings with references- Core of ZIP, gzip, zlib, LZMA, Snappy #### LZ77 Sketch\n\nwhile (not EOF) {\n    find longest match in sliding window;\n    output (offset, length, next_char);\n}\nVariants:\n\nLZ4 - fast, lower ratio- Snappy - optimized for speed- Zstandard (Zstd) - tunable speed/ratio, dictionary support #### Burrows-Wheeler Transform (BWT)\nReorders data to group similar symbols- Followed by Move-To-Front + Huffman- Used in bzip2, BWT-based compressors #### Run-Length Encoding (RLE)\nReplace consecutive repeats with (symbol, count)- Great for structured or sparse data Example: AAAAABBBCC → (A,5)(B,3)(C,2)\n\n\n\n4. Specialized Compression in Systems\n\n\nColumnar Databases\nCompress per column:\n\nDictionary encoding - map strings → ints- Run-length encoding - good for sorted columns- Delta encoding - store differences (time series)- Bit-packing - fixed-width integers in minimal bits Combine multiple for optimal ratio.\n\nExample (time deltas):\n[100, 102, 103, 107] → [100, +2, +1, +4]\n\n\nLog and Telemetry Compression\n\nStructured formats → fieldwise encoding- Often Snappy or LZ4 for fast decode- Aggregators (Fluentd, Loki, Kafka) rely heavily on them #### Data Lakes and Files\nParquet, ORC, Arrow → columnar + compressed- Choose codec per column: LZ4 for speed, Zstd for ratio\n\n\n\n5. Streaming and Chunked Compression\nLarge data often processed in chunks:\n\nEnables random access and parallelism- Needed for network streams, distributed files Example: zlib block, Zstd frame, gzip chunk\n\nUsed in:\n\nHTTP chunked encoding- Kafka log segments- MapReduce shuffle\n\n\n\n6. Encoding Schemes\nCompression ≠ encoding. Encoding ensures safe transport.\n\n\nBase64\n\nMaps 3 bytes → 4 chars- 33% overhead- Used for binary → text (emails, JSON APIs) #### URL Encoding\nEscape unsafe chars with %xx #### Delta Encoding\nStore differences, not full values #### Varint / Zigzag Encoding\nCompact integers (e.g. protobufs)- Smaller numbers → fewer bytes Example:\n\nwhile (x &gt;= 0x80) { emit((x & 0x7F) | 0x80); x &gt;&gt;= 7; }\nemit(x);\n\n\n7. Adaptive and Context Models\nModern compressors adapt to local patterns:\n\nPPM (Prediction by Partial Matching)- Context mixing (PAQ)- Zstd uses FSE (Finite State Entropy) coding Balance between speed, memory, and compression ratio.\n\n\n\n8. Hardware Acceleration\nCompression can be offloaded to:\n\nCPUs with SIMD (AVX2, SSE4.2)- GPUs (parallel encode/decode)- NICs / SmartNICs- ASICs (e.g., Intel QAT) Critical for high-throughput databases, network appliances, storage systems.\n\n\n\n9. Design Trade-offs\n\n\n\nGoal\nBest Choice\n\n\n\n\nMax speed\nLZ4, Snappy\n\n\nMax ratio\nZstd, LZMA\n\n\nBalance\nZstd (tunable)\n\n\nColumn store\nRLE, Delta, Dict\n\n\nLogs / telemetry\nSnappy, LZ4\n\n\nArchival\nbzip2, xz\n\n\nReal-time\nLZ4, Brotli (fast mode)\n\n\n\nChoose based on CPU budget, I/O cost, latency tolerance.\n\n\n10. Why It Matters\nCompression is a first-class optimization:\n\nSaves petabytes in data centers- Boosts throughput across networks- Powers cloud storage (S3, BigQuery, Snowflake)- Enables efficient analytics and ML pipelines &gt; “Every byte saved is time earned.”\n\n\n\nTry It Yourself\n\nCompress text using Huffman coding (build frequency table).\nCompare gzip, Snappy, and Zstd on a 1GB dataset.\nImplement delta encoding and RLE for numeric data.\nTry dictionary encoding on repetitive strings.\nMeasure compression ratio, speed, and CPU usage trade-offs.\n\n\n\n\n90. Fault Tolerance and Replication\nModern systems must survive hardware crashes, network partitions, or data loss without stopping. Fault tolerance ensures that a system continues to function, even when parts fail. Replication underpins this resilience, duplicating data or computation across multiple nodes for redundancy, performance, and consistency.\nTogether, they form the backbone of reliability in distributed systems.\n\n1. Why Fault Tolerance?\nNo system is perfect:\n\nServers crash- Disks fail- Networks partition- Power goes out The question isn’t if failure happens, but when. Fault-tolerant systems detect, contain, and recover from failure automatically.\n\nGoals:\n\nAvailability - keep serving requests- Durability - never lose data- Consistency - stay correct across replicas\n\n\n\n2. Failure Models\n\n\nCrash Faults\nNode stops responding but doesn’t misbehave. Handled by restarts or replication (Raft, Paxos).\n\n\nOmission Faults\nLost messages or dropped updates. Handled with retries and acknowledgments.\n\n\nByzantine Faults\nArbitrary/malicious behavior. Handled by Byzantine Fault Tolerance (PBFT), expensive but robust.\n\n\n3. Redundancy: The Core Strategy\nFault tolerance = redundancy + detection + recovery\nRedundancy types:\n\nHardware: multiple power supplies, disks (RAID)- Software: replicated services, retries- Data: multiple copies, erasure codes- Temporal: retry or checkpoint and replay\n\n\n\n4. Replication Models\n\n\n1. Active Replication\nAll replicas process requests in parallel (lockstep). Results must match. Used in real-time and Byzantine systems.\n\n\n2. Passive (Primary-Backup)\nOne leader (primary) handles requests. Backups replicate log, take over on failure. Used in Raft, ZooKeeper, PostgreSQL streaming.\n\n\n3. Quorum Replication\nWrites and reads contact majority of replicas. Ensures overlap → consistency. Used in Cassandra, DynamoDB, Etcd.\n\n\n5. Consistency Models\nReplication introduces a trade-off between consistency and availability (CAP theorem).\n\n\nStrong Consistency\nAll clients see the same value immediately. Example: Raft, Etcd, Spanner.\n\n\nEventual Consistency\nReplicas converge over time. Example: DynamoDB, Cassandra.\n\n\nCausal Consistency\nPreserves causal order of events. Example: Vector clocks, CRDTs.\nChoice depends on workload:\n\nBanking → strong- Social feeds → eventual- Collaborative editing → causal\n\n\n\n6. Checkpointing and Recovery\nTo recover after crash:\n\nPeriodically checkpoint state- On restart, replay log of missed events Example: Databases → Write-Ahead Log (WAL) Stream systems → Kafka checkpoints\n\n1. Save state to disk\n2. Record latest log position\n3. On restart → reload + replay\n\n\n7. Erasure Coding\nInstead of full copies, store encoded fragments. With ( k ) data blocks, ( m ) parity blocks → tolerate ( m ) failures.\nExample: Reed-Solomon (used in HDFS, Ceph)\n\n\n\nk\nm\nTotal\nFault Tolerance\n\n\n\n\n4\n2\n6\n2 failures\n\n\n\nBetter storage efficiency than 3× replication.\n\n\n8. Failure Detection\nDetecting failure is tricky in distributed systems (because of latency). Common techniques:\n\nHeartbeats - periodic “I’m alive” messages- Timeouts - suspect node if no heartbeat- Gossip protocols - share failure info among peers Used in Consul, Cassandra, Kubernetes health checks.\n\n\n\n9. Self-Healing Systems\nAfter failure:\n\nDetect it\nIsolate faulty component\nReplace or restart\nRebalance load or re-replicate data\n\nPatterns:\n\nSupervisor trees (Erlang/Elixir)- Self-healing clusters (Kubernetes)- Rebalancing (Cassandra ring repair) “Never trust a single machine, trust the system.”\n\n\n\n10. Why It Matters\nFault tolerance turns fragile infrastructure into reliable services.\nUsed in:\n\nDatabases (replication + WAL)- Distributed storage (HDFS, Ceph, S3)- Orchestration (Kubernetes controllers)- Streaming systems (Kafka, Flink) Without replication and fault tolerance, large-scale systems would collapse under failure.\n\n\n“Resilience is built, not assumed.”\n\n\n\nTry It Yourself\n\nBuild a primary-backup key-value store: leader writes, follower replicates.\nAdd heartbeat + timeout detection to trigger failover.\nSimulate partition: explore behavior under strong vs eventual consistency.\nImplement checkpoint + replay recovery for a small app.\nCompare 3× replication vs Reed-Solomon (4+2) for space and reliability.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/book.html#chapter-10.-ai-ml-and-optimization",
    "href": "books/en-us/book.html#chapter-10.-ai-ml-and-optimization",
    "title": "The Book",
    "section": "Chapter 10. AI, ML and Optimization",
    "text": "Chapter 10. AI, ML and Optimization\n\n91. Classical ML (k-means, Naive Bayes, SVM, Decision Trees)\nClassical machine learning is built on interpretable mathematics and solid optimization foundations. Long before deep learning, these algorithms powered search engines, spam filters, and recommendation systems. They’re still used today, fast, explainable, and easy to deploy.\nThis section covers the four pillars of classical ML:\n\nk-means - unsupervised clustering- Naive Bayes - probabilistic classification- SVM - margin-based classification- Decision Trees - rule-based learning\n\n\n1. The Essence of Classical ML\nClassical ML is about learning from data using statistical principles, often without huge compute. Given dataset ( D = {\\(x_i, y_i\\)} ), the task is to:\n\nPredict ( y ) from ( x )- Generalize beyond seen data- Balance bias and variance\n\n\n\n2. k-means Clustering\nGoal: partition data into ( k ) groups (clusters) such that intra-cluster distance is minimized.\n\n\nObjective\n\\[\n\\min_{C} \\sum_{i=1}^k \\sum_{x \\in C_i} |x - \\mu_i|^2\n\\] Where \\(\\mu_i\\) = centroid of cluster ( i ).\n\n\nAlgorithm\n\nChoose ( k ) random centroids\nAssign each point to nearest centroid\nRecompute centroids\nRepeat until stable\n\n\n\nTiny Code (C-style)\nfor (iter = 0; iter &lt; max_iter; iter++) {\n    assign_points_to_clusters();\n    recompute_centroids();\n}\n\n\nPros\n\nSimple, fast (( O(nkd) ))- Works well for spherical clusters #### Cons\nRequires ( k )- Sensitive to initialization, outliers Variants:\nk-means++ (better initialization)- Mini-batch k-means (scalable)\n\n\n\n3. Naive Bayes Classifier\nA probabilistic model using Bayes’ theorem under independence assumptions.\n\\[\nP(y|x) \\propto P(y) \\prod_{i=1}^n P(x_i | y)\n\\]\n\n\nAlgorithm\n\nCompute prior ( P(y) )\nCompute likelihood ( P\\(x_i | y\\) )\nPredict class with max posterior\n\n\n\nTypes\n\nMultinomial NB - text (bag of words)- Gaussian NB - continuous features- Bernoulli NB - binary features #### Example (Spam Detection)\n\nP(spam | \"win money\") ∝ P(spam) * P(\"win\"|spam) * P(\"money\"|spam)\n\n\nPros\n\nFast, works well for text- Needs little data- Probabilistic interpretation #### Cons\nAssumes feature independence- Poor for correlated features\n\n\n\n4. Support Vector Machines (SVM)\nSVM finds the max-margin hyperplane separating classes.\n\n\nObjective\nMaximize margin = distance between boundary and nearest points.\n\\[\n\\min_{w, b} \\frac{1}{2} |w|^2 \\quad \\text{s.t.} \\quad y_i(w \\cdot x_i + b) \\ge 1\n\\]\nCan be solved via Quadratic Programming.\n\n\nIntuition\n\nEach data point → vector- Hyperplane: \\(w \\cdot x + b = 0\\)- Support vectors = boundary points #### Kernel Trick\n\nTransform input via kernel ( K\\(x_i, x_j\\) = \\(x_i\\) \\(x_j\\) ):\n\nLinear: dot product- Polynomial: ( \\(x_i \\cdot x_j + c\\)^d )- RBF: \\(e^{-\\gamma |x_i - x_j|^2}\\) #### Pros\nEffective in high dimensions- Can model nonlinear boundaries- Few hyperparameters #### Cons\nSlow on large data- Harder to tune kernel parameters\n\n\n\n5. Decision Trees\nIf-else structure for classification/regression.\nAt each node:\n\nPick feature ( f ) and threshold ( t )- Split to maximize information gain #### Metrics\nEntropy: \\(H = -\\sum p_i \\log p_i\\)- Gini: \\(G = 1 - \\sum p_i^2\\) #### Pseudocode\n\nif (feature &lt; threshold)\n    go left;\nelse\n    go right;\nBuild recursively until:\n\nMax depth- Min samples per leaf- Pure nodes #### Pros\nInterpretable- Handles mixed data- No scaling needed #### Cons\nProne to overfitting- Unstable (small data changes) Fixes:\nPruning (reduce depth)- Ensembles: Random Forests, Gradient Boosting\n\n\n\n6. Bias-Variance Tradeoff\n\n\n\nAlgorithm\nBias\nVariance\n\n\n\n\nk-means\nHigh\nLow\n\n\nNaive Bayes\nHigh\nLow\n\n\nSVM\nLow\nMedium\n\n\nDecision Tree\nLow\nHigh\n\n\n\nBalancing both = good generalization.\n\n\n7. Evaluation Metrics\nFor classification:\n\nAccuracy, Precision, Recall, F1-score- ROC-AUC, Confusion Matrix For clustering:\nInertia, Silhouette Score Always use train/test split or cross-validation.\n\n\n\n8. Scaling to Large Data\nTechniques:\n\nMini-batch training- Online updates (SGD)- Dimensionality reduction (PCA)- Approximation (Random Projections) Libraries:\nscikit-learn (Python)- liblinear, libsvm (C/C++)- MLlib (Spark)\n\n\n\n9. When to Use What\n\n\n\nTask\nRecommended Algorithm\n\n\n\n\nText classification\nNaive Bayes\n\n\nClustering\nk-means\n\n\nNonlinear classification\nSVM (RBF)\n\n\nTabular data\nDecision Tree\n\n\nQuick baseline\nLogistic Regression / NB\n\n\n\n\n\n10. Why It Matters\nThese algorithms are fast, interpretable, and theoretical foundations of modern ML. They remain the go-to choice for:\n\nSmall to medium datasets- Real-time classification- Explainable AI &gt; “Classical ML is the art of solving problems with math you can still write on a whiteboard.”\n\n\n\nTry It Yourself\n\nCluster 2D points with k-means, plot centroids.\nTrain Naive Bayes on a spam/ham dataset.\nClassify linearly separable data with SVM.\nBuild a Decision Tree from scratch (entropy, Gini).\nCompare models’ accuracy and interpretability.\n\n\n\n\n92. Ensemble Methods (Bagging, Boosting, Random Forests)\nEnsemble methods combine multiple weak learners to build a strong predictor. Instead of relying on one model, ensembles vote, average, or boost multiple models, improving stability and accuracy.\nThey are the bridge between classical and modern ML , simple models, combined smartly, become powerful.\n\n1. The Core Idea\n\n“Many weak learners, when combined, can outperform a single strong one.”\n\nMathematically, if \\(f_1, f_2, \\ldots, f_k\\) are weak learners, an ensemble predictor is:\n\\[\nF(x) = \\frac{1}{k}\\sum_{i=1}^k f_i(x)\n\\]\nFor classification, combine via majority vote. For regression, combine via average.\n\n\n2. Bagging (Bootstrap Aggregating)\nBagging reduces variance by training models on different samples.\n\n\nSteps\n\nDraw ( B ) bootstrap samples from dataset ( D ).\nTrain one model per sample.\nAggregate predictions by averaging or voting.\n\n\\[\n\\hat{f}*{bag}(x) = \\frac{1}{B} \\sum*{b=1}^B f_b(x)\n\\]\nEach \\(f_b\\) is trained on a random subset (with replacement).\n\n\nExample\n\nBase learner: Decision Tree- Ensemble: Bagged Trees- Famous instance: Random Forest #### Tiny Code (C-style Pseudocode)\n\nfor (int b = 0; b &lt; B; b++) {\n    D_b = bootstrap_sample(D);\n    model[b] = train_tree(D_b);\n}\nprediction = average_predictions(model, x);\n\n\nPros\n\nReduces variance- Works well with high-variance learners- Parallelizable #### Cons\nIncreases computation- Doesn’t reduce bias\n\n\n\n3. Random Forest\nA bagging-based ensemble of decision trees with feature randomness.\n\n\nKey Ideas\n\nEach tree trained on bootstrap sample.- At each split, consider random subset of features.- Final prediction = majority vote or average. This decorrelates trees, improving generalization.\n\n\\[\nF(x) = \\frac{1}{B} \\sum_{b=1}^{B} T_b(x)\n\\]\n\n\nPros\n\nHandles large feature sets- Low overfitting- Good default for tabular data #### Cons\nLess interpretable- Slower on huge datasets OOB (Out-of-Bag) error = internal validation from unused samples.\n\n\n\n4. Boosting\nBoosting focuses on reducing bias by sequentially training models , each one corrects errors from the previous.\n\n\nSteps\n\nStart with weak learner ( f_1(x) )\nTrain next learner ( f_2(x) ) on residuals/errors\nCombine with weighted sum\n\n\\[\nF_m(x) = F_{m-1}(x) + \\alpha_m f_m(x)\n\\]\nWeights \\(\\alpha_m\\) focus on difficult examples.\n\n\nTiny Code (Conceptual)\nF = 0;\nfor (int m = 0; m &lt; M; m++) {\n    residual = y - predict(F, x);\n    f_m = train_weak_learner(x, residual);\n    F += alpha[m] * f_m;\n}\n\n\n5. AdaBoost (Adaptive Boosting)\nAdaBoost adapts weights on samples after each iteration.\n\n\nAlgorithm\n\nInitialize weights: \\(w_i = \\frac{1}{n}\\)\nTrain weak classifier \\(f_t\\)\nCompute error: \\(\\epsilon_t\\)\nUpdate weights: \\[\nw_i \\leftarrow w_i \\cdot e^{\\alpha_t \\cdot I(y_i \\ne f_t(x_i))}\n\\] where \\(\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\\)\nNormalize weights\n\nFinal classifier: \\[\nF(x) = \\text{sign}\\left( \\sum_t \\alpha_t f_t(x) \\right)\n\\]\n\n\nPros\n\nHigh accuracy on clean data- Simple and interpretable weights #### Cons\nSensitive to outliers- Sequential → not easily parallelizable\n\n\n\n6. Gradient Boosting\nA modern version of boosting using gradient descent on loss.\nAt each step, fit new model to negative gradient of loss function.\n\n\nObjective\n\\[\nF_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n\\]\nwhere \\(h_m(x) \\approx -\\frac{\\partial L(y, F(x))}{\\partial F(x)}\\)\n\n\nCommon Libraries\n\nXGBoost\nLightGBM\nCatBoost #### Pros\nHigh performance on tabular data- Flexible (custom loss)- Handles mixed feature types #### Cons\nSlower to train- Sensitive to hyperparameters\n\n\n\n7. Stacking (Stacked Generalization)\nCombine multiple models (base learners) via a meta-model.\n\n\nSteps\n\nTrain base models (SVM, Tree, NB, etc.)\nCollect their predictions\nTrain meta-model (e.g. Logistic Regression) on outputs\n\n\\[\n\\hat{y} = f_{meta}(f_1(x), f_2(x), \\ldots, f_k(x))\n\\]\n\n\n8. Bagging vs Boosting\n\n\n\nFeature\nBagging\nBoosting\n\n\n\n\nStrategy\nParallel\nSequential\n\n\nGoal\nReduce variance\nReduce bias\n\n\nWeighting\nUniform\nAdaptive\n\n\nExample\nRandom Forest\nAdaBoost, XGBoost\n\n\n\n\n\n9. Bias-Variance Behavior\n\nBagging: ↓ variance- Boosting: ↓ bias- Random Forest: balanced- Stacking: flexible but complex\n\n\n\n10. Why It Matters\nEnsemble methods are the workhorses of classical ML competitions and real-world tabular problems. They blend interpretability, flexibility, and predictive power.\n\n“One tree may fall, but a forest stands strong.”\n\n\n\nTry It Yourself\n\nTrain a Random Forest on the Iris dataset.\nImplement AdaBoost from scratch using decision stumps.\nCompare Bagging vs Boosting accuracy.\nTry XGBoost with different learning rates.\nVisualize feature importance across models.\n\n\n\n\n93. Gradient Methods (SGD, Adam, RMSProp)\nGradient-based optimization is the heartbeat of machine learning. These methods update parameters iteratively by following the negative gradient of the loss function. They power everything from linear regression to deep neural networks.\n\n1. The Core Idea\nWe want to minimize a loss function ( L\\(\\theta\\) ). Starting from some initial parameters \\(\\theta_0\\), we move in the opposite direction of the gradient:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta L(\\theta_t)\n\\]\nwhere \\(\\eta\\) is the learning rate (step size).\nThe gradient tells us which way the function increases fastest , we move the other way.\n\n\n2. Batch Gradient Descent\nUses the entire dataset to compute the gradient.\n\\[\n\\nabla_\\theta L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\ell_i(\\theta)\n\\]\n\nAccurate but slow for large ( N )- Each update is expensive Tiny Code\n\nfor (int t = 0; t &lt; T; t++) {\n    grad = compute_full_gradient(data, theta);\n    theta = theta - eta * grad;\n}\nGood for: small datasets or convex problems\n\n\n3. Stochastic Gradient Descent (SGD)\nInstead of full data, use one random sample per step.\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\ell_i(\\theta_t)\n\\]\n\nNoisy but faster updates- Can escape local minima- Great for online learning Tiny Code\n\nfor each sample (x_i, y_i):\n    grad = grad_loss(theta, x_i, y_i);\n    theta -= eta * grad;\nPros\n\nFast convergence- Works on large datasets Cons\nNoisy updates- Requires learning rate tuning\n\n\n\n4. Mini-Batch Gradient Descent\nCompromise between batch and stochastic.\nUse small subset (mini-batch) of samples:\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{1}{m} \\sum_{i=1}^m \\nabla_\\theta \\ell_i(\\theta_t)\n\\]\nUsually batch size = 32 or 64. Faster, more stable updates.\n\n\n5. Momentum\nAdds velocity to smooth oscillations.\n\\[\nv_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_\\theta L(\\theta_t)\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\eta v_t\n\\]\nThis accumulates past gradients to speed movement in consistent directions.\nThink of it like a heavy ball rolling down a hill.\n\n\n6. Nesterov Accelerated Gradient (NAG)\nImproves momentum by looking ahead:\n\\[\nv_t = \\beta v_{t-1} + \\eta \\nabla_\\theta L(\\theta_t - \\beta v_{t-1})\n\\]\nIt anticipates the future position before computing the gradient.\nFaster convergence in convex settings.\n\n\n7. RMSProp\nAdjusts learning rate per parameter using exponential average of squared gradients:\n\\[\nE[g^2]*t = \\rho E[g^2]*{t-1} + (1 - \\rho) g_t^2\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n\\]\nThis helps when gradients vary in magnitude.\nGood for: non-stationary objectives, deep networks\n\n\n8. Adam (Adaptive Moment Estimation)\nCombines momentum + RMSProp:\n\\[\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n\\]\n\\[\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n\\]\nBias-corrected estimates:\n\\[\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n\\]\nUpdate rule:\n\\[\n\\theta_{t+1} = \\theta_t - \\frac{\\eta \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\]\nTiny Code (Conceptual)\nm = 0; v = 0;\nfor (int t = 1; t &lt;= T; t++) {\n    g = grad(theta);\n    m = beta1 * m + (1 - beta1) * g;\n    v = beta2 * v + (1 - beta2) * g * g;\n    m_hat = m / (1 - pow(beta1, t));\n    v_hat = v / (1 - pow(beta2, t));\n    theta -= eta * m_hat / (sqrt(v_hat) + eps);\n}\nPros\n\nWorks well out of the box- Adapts learning rate- Great for deep learning Cons\nMay not converge exactly- Needs decay schedule for stability\n\n\n\n9. Learning Rate Schedules\nControl \\(\\eta\\) over time:\n\nStep decay: \\(\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}\\)- Exponential decay: \\(\\eta_t = \\eta_0 e^{-\\lambda t}\\)- Cosine annealing: smooth periodic decay- Warm restarts: reset learning rate periodically\n\n\n\n10. Why It Matters\nAll modern deep learning is built on gradients. Choosing the right optimizer can mean faster training and better accuracy.\n\n\n\nOptimizer\nAdaptive\nMomentum\nCommon Use\n\n\n\n\nSGD\nNo\nOptional\nSimple tasks\n\n\nSGD + Momentum\nNo\nYes\nConvNets\n\n\nRMSProp\nYes\nNo\nRNNs\n\n\nAdam\nYes\nYes\nTransformers, DNNs\n\n\n\n\n“Optimization is the art of taking small steps in the right direction , many times over.”\n\n\n\nTry It Yourself\n\nImplement SGD and Adam on a linear regression task.\nCompare training curves for SGD, Momentum, RMSProp, and Adam.\nExperiment with learning rate schedules.\nVisualize optimization paths on a 2D contour plot.\n\n\n\n\n94. Deep Learning (Backpropagation, Dropout, Normalization)\nDeep learning is about stacking layers of computation so that the network can learn hierarchical representations. From raw pixels to abstract features, deep nets build meaning through composition of functions.\nAt the core of this process are three ideas: backpropagation, regularization (dropout), and normalization.\n\n1. The Essence of Deep Learning\nA neural network is a chain of functions:\n\\[\nf(x; \\theta) = f_L(f_{L-1}(\\cdots f_1(x)))\n\\]\nEach layer transforms its input and passes it on.\nTraining involves finding parameters \\(\\theta\\) that minimize a loss ( L(f\\(x; \\theta\\), y) ).\n\n\n2. Backpropagation\nBackpropagation is the algorithm that teaches neural networks.\nIt uses the chain rule of calculus to efficiently compute gradients layer by layer.\nFor each layer ( i ):\n\\[\n\\frac{\\partial L}{\\partial \\theta_i} = \\frac{\\partial L}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial \\theta_i}\n\\]\nand propagate backward:\n\\[\n\\frac{\\partial L}{\\partial a_{i-1}} = \\frac{\\partial L}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial a_{i-1}}\n\\]\nSo every neuron learns how much it contributed to the error.\nTiny Code\n// Pseudocode for 2-layer network\nforward:\n    z1 = W1*x + b1;\n    a1 = relu(z1);\n    z2 = W2*a1 + b2;\n    y_hat = softmax(z2);\n    loss = cross_entropy(y_hat, y);\n\nbackward:\n    dz2 = y_hat - y;\n    dW2 = dz2 * a1.T;\n    db2 = sum(dz2);\n    da1 = W2.T * dz2;\n    dz1 = da1 * relu_grad(z1);\n    dW1 = dz1 * x.T;\n    db1 = sum(dz1);\nEach gradient is computed by local differentiation and multiplied back.\n\n\n3. Activation Functions\nNonlinear activations let networks approximate nonlinear functions.\n\n\n\nFunction\nFormula\nUse\n\n\n\n\nReLU\n\\(\\max(0, x)\\)\nDefault, fast\n\n\nSigmoid\n\\(\\frac{1}{1 + e^{-x}}\\)\nProbabilities\n\n\nTanh\n\\(\\tanh(x)\\)\nCentered activations\n\n\nGELU\n\\(x \\, \\Phi(x)\\)\nModern transformers\n\n\n\nWithout nonlinearity, stacking layers is just one big linear transformation.\n\n\n4. Dropout\nDropout is a regularization technique that prevents overfitting. During training, randomly turn off neurons:\n\\[\n\\tilde{a}_i = a_i \\cdot m_i, \\quad m_i \\sim \\text{Bernoulli}(p)\n\\]\nAt inference, scale activations by ( p ) (keep probability).\nIt forces the network to not rely on any single path.\nTiny Code\nfor (int i = 0; i &lt; n; i++) {\n    if (rand_uniform() &lt; p) a[i] = 0;\n    else a[i] /= p; // scaling\n}\n\n\n5. Normalization\nNormalization stabilizes and speeds up training by reducing internal covariate shift.\n\n\nBatch Normalization\nNormalize activations per batch:\n\\[\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\]\n\\[\ny = \\gamma \\hat{x} + \\beta\n\\]\nLearnable parameters \\(\\gamma, \\beta\\) restore flexibility.\nBenefits:\n\nSmooth gradients- Allows higher learning rates- Acts as regularizer #### Layer Normalization\n\nUsed in transformers (normalizes across features, not batch).\n\n\n6. Initialization\nProper initialization helps gradients flow.\n\n\n\nScheme\nFormula\nUse\n\n\n\n\nXavier\n\\(\\text{Var}(W) = \\frac{1}{n_{in}}\\)\nTanh\n\n\nHe\n\\(\\text{Var}(W) = \\frac{2}{n_{in}}\\)\nReLU\n\n\n\nPoor initialization can lead to vanishing or exploding gradients.\n\n\n7. Training Pipeline\n\nInitialize weights\nForward pass\nCompute loss\nBackward pass (backprop)\nUpdate weights (e.g. with Adam)\n\nRepeat until convergence.\n\n\n8. Deep Architectures\n\n\n\nModel\nKey Idea\nTypical Use\n\n\n\n\nMLP\nFully connected\nTabular data\n\n\nCNN\nConvolutions\nImages\n\n\nRNN\nSequential recurrence\nTime series, text\n\n\nTransformer\nSelf-attention\nLanguage, vision\n\n\n\nEach architecture stacks linear operations and nonlinearities in different ways.\n\n\n9. Overfitting and Regularization\nCommon fixes:\n\nDropout- Weight decay (\\(L_2\\) regularization)- Data augmentation- Early stopping The key is to improve generalization, not just minimize training loss.\n\n\n\n10. Why It Matters\nBackpropagation turned neural networks from theory to practice. Normalization made them train faster. Dropout made them generalize better.\nTogether, they unlocked the deep learning revolution.\n\n“Depth gives power, but gradients give life.”\n\n\n\nTry It Yourself\n\nImplement a 2-layer network with ReLU and softmax.\nAdd dropout and batch normalization.\nVisualize training with and without dropout.\nCompare performance on MNIST with and without normalization.\n\n\n\n\n95. Sequence Models (Viterbi, Beam Search, CTC)\nSequence models process data where order matters, text, speech, DNA, time series. They capture dependencies across positions, predicting the next step from context.\nThis section explores three fundamental tools: Viterbi, Beam Search, and CTC (Connectionist Temporal Classification).\n\n1. The Nature of Sequential Data\nSequential data has temporal or structural order. Each element \\(x_t\\) depends on past inputs \\(x_{1:t-1}\\).\nCommon sequence tasks:\n\nTagging (POS tagging, named entity recognition)- Transcription (speech → text)- Decoding (translation, path reconstruction) To handle such problems, we need models that remember.\n\n\n\n2. Hidden Markov Models (HMMs)\nA Hidden Markov Model assumes:\n\nA sequence of hidden states \\(z_1, z_2, \\ldots, z_T\\)- Each state emits an observation \\(x_t\\)- Transition and emission probabilities govern the process \\[\nP(z_t | z_{t-1}) = A_{z_{t-1}, z_t}, \\quad P(x_t | z_t) = B_{z_t}(x_t)\n\\]\n\nGoal: find the most likely sequence of hidden states given observations.\n\n\n3. The Viterbi Algorithm\nViterbi is a dynamic programming algorithm to decode the most probable path:\n\\[\n\\delta_t(i) = \\max_{z_{1:t-1}} P(z_{1:t-1}, z_t = i, x_{1:t})\n\\]\nRecurrence:\n\\[\n\\delta_t(i) = \\max_j \\big( \\delta_{t-1}(j) \\cdot A_{j,i} \\big) \\cdot B_i(x_t)\n\\]\nTrack backpointers to reconstruct the best sequence.\nTime complexity: \\(O(T \\cdot N^2)\\),\nwhere \\(N\\) = number of states, \\(T\\) = sequence length.\nTiny Code\nfor (t = 1; t &lt; T; t++) {\n    for (i = 0; i &lt; N; i++) {\n        double best = -INF;\n        int argmax = -1;\n        for (j = 0; j &lt; N; j++) {\n            double score = delta[t-1][j] * A[j][i];\n            if (score &gt; best) { best = score; argmax = j; }\n        }\n        delta[t][i] = best * B[i][x[t]];\n        backptr[t][i] = argmax;\n    }\n}\nUse backptr to trace back the optimal path.\n\n\n4. Beam Search\nFor many sequence models (e.g. neural machine translation), exhaustive search is impossible. Beam search keeps only the top-k best hypotheses at each step.\nAlgorithm:\n\nStart with an empty sequence and score 0\nAt each step, expand each candidate with all possible next tokens\nKeep only k best sequences (beam size)\nStop when all sequences end or reach max length\n\nBeam size controls trade-off:\n\nLarger beam → better accuracy, slower- Smaller beam → faster, riskier\n\nTiny Code\nfor (step = 0; step &lt; max_len; step++) {\n    vector&lt;Candidate&gt; new_beam;\n    for (c in beam) {\n        probs = model_next(c.seq);\n        for (token, p in probs)\n            new_beam.push({c.seq + token, c.score + log(p)});\n    }\n    beam = top_k(new_beam, k);\n}\nUse log probabilities to avoid underflow.\n\n\n5. Connectionist Temporal Classification (CTC)\nUsed in speech recognition and handwriting recognition where input and output lengths differ.\nCTC learns to align input frames with output symbols without explicit alignment.\nAdd a special blank symbol (∅) to allow flexible alignment.\nExample (CTC decoding):\n\n\n\nFrame\nOutput\nAfter Collapse\n\n\n\n\nA ∅ A A\nA ∅ A\nA A\n\n\nH ∅ ∅ H\nH H\nH\n\n\n\nLoss: \\[\nP(y | x) = \\sum_{\\pi \\in \\text{Align}(x, y)} P(\\pi | x)\n\\] where \\(\\pi\\) are all alignments that reduce to ( y ).\nCTC uses dynamic programming to compute forward-backward probabilities.\n\n\n6. Comparing Methods\n\n\n\n\n\n\n\n\n\nMethod\nUsed In\nKey Idea\nHandles Alignment?\n\n\n\n\nViterbi\nHMMs\nMost probable state path\nYes\n\n\nBeam Search\nNeural decoders\nApproximate search\nImplicit\n\n\nCTC\nSpeech / seq2seq\nSum over alignments\nYes\n\n\n\n\n\n7. Use Cases\n\nViterbi: POS tagging, speech decoding- Beam Search: translation, text generation- CTC: ASR, OCR, gesture recognition\n\n\n\n8. Implementation Tips\n\nUse log-space for probabilities- In beam search, apply length normalization- In CTC, use dynamic programming tables- Combine CTC + beam search for speech decoding\n\n\n\n9. Common Pitfalls\n\nViterbi assumes Markov property (limited memory)- Beam Search can miss global optimum- CTC can confuse repeated characters without blanks\n\n\n\n10. Why It Matters\nSequence models are the bridge between structure and time. They show how to decode hidden meaning in ordered data.\nFrom decoding Morse code to transcribing speech, these algorithms give machines the gift of sequence understanding.\n\n\nTry It Yourself\n\nImplement Viterbi for a 3-state HMM.\nCompare greedy decoding vs beam search on a toy language model.\nBuild a CTC loss table for a short sequence (like “HELLO”).\n\n\n\n\n96. Metaheuristics (GA, SA, PSO, ACO)\nMetaheuristics are general-purpose optimization strategies that search through vast, complex spaces when exact methods are too slow or infeasible. They don’t guarantee the perfect answer but often find good-enough solutions fast.\nThis section covers four classics:\n\nGA (Genetic Algorithm)- SA (Simulated Annealing)- PSO (Particle Swarm Optimization)- ACO (Ant Colony Optimization)\n\n\n1. The Metaheuristic Philosophy\nMetaheuristics draw inspiration from nature and physics. They combine exploration (searching widely) and exploitation (refining promising spots).\nThey’re ideal for:\n\nNP-hard problems (TSP, scheduling)- Continuous optimization (parameter tuning)- Black-box functions (no gradients) They trade mathematical guarantees for practical power.\n\n\n\n2. Genetic Algorithm (GA)\nInspired by natural selection, GAs evolve a population of solutions.\n\n\nCore Steps\n\nInitialize population randomly\nEvaluate fitness of each\nSelect parents\nCrossover to produce offspring\nMutate to add variation\nReplace worst with new candidates\n\nRepeat until convergence.\nTiny Code\nfor (gen = 0; gen &lt; max_gen; gen++) {\n    evaluate(pop);\n    parents = select_best(pop);\n    offspring = crossover(parents);\n    mutate(offspring);\n    pop = select_survivors(pop, offspring);\n}\nOperators\n\nSelection: tournament, roulette-wheel- Crossover: one-point, uniform- Mutation: bit-flip, Gaussian Strengths: global search, diverse exploration Weakness: may converge slowly\n\n\n\n3. Simulated Annealing (SA)\nMimics cooling of metals, start hot (high randomness), slowly cool.\nAt each step:\n\nPropose random neighbor\nAccept if better\nIf worse, accept with probability \\[\nP = e^{-\\frac{\\Delta E}{T}}\n\\]\nGradually lower ( T )\n\nTiny Code\nT = T_init;\nstate = random_state();\nwhile (T &gt; T_min) {\n    next = neighbor(state);\n    dE = cost(next) - cost(state);\n    if (dE &lt; 0 || exp(-dE/T) &gt; rand_uniform())\n        state = next;\n    T *= alpha; // cooling rate\n}\nStrengths: escapes local minima Weakness: sensitive to cooling schedule\n\n\n4. Particle Swarm Optimization (PSO)\nInspired by bird flocking. Each particle adjusts velocity based on:\n\nIts own best position- The global best found \\[\nv_i \\leftarrow w v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i)\n\\]\n\n\\[\nx_i \\leftarrow x_i + v_i\n\\]\nTiny Code\nfor each particle i:\n    v[i] = w*v[i] + c1*r1*(pbest[i]-x[i]) + c2*r2*(gbest-x[i]);\n    x[i] += v[i];\n    update_best(i);\nStrengths: continuous domains, easy Weakness: premature convergence\n\n\n5. Ant Colony Optimization (ACO)\nInspired by ant foraging, ants deposit pheromones on paths. The stronger the trail, the more likely others follow.\nSteps:\n\nInitialize pheromone on all edges\nEach ant builds a solution (prob. ∝ pheromone)\nEvaluate paths\nEvaporate pheromone\nReinforce good paths\n\n\\[\n\\tau_{ij} \\leftarrow (1 - \\rho)\\tau_{ij} + \\sum_k \\Delta\\tau_{ij}^k\n\\]\nTiny Code\nfor each iteration:\n    for each ant:\n        path = build_solution(pheromone);\n        score = evaluate(path);\n    evaporate(pheromone);\n    deposit(pheromone, best_paths);\nStrengths: combinatorial problems (TSP) Weakness: parameter tuning, slower convergence\n\n\n6. Comparing the Four\n\n\n\n\n\n\n\n\n\nMethod\nInspiration\nBest For\nKey Idea\n\n\n\n\nGA\nEvolution\nDiscrete search\nSelection, crossover, mutation\n\n\nSA\nThermodynamics\nLocal optima escape\nCooling + randomness\n\n\nPSO\nSwarm behavior\nContinuous search\nLocal + global attraction\n\n\nACO\nAnt foraging\nGraph paths\nPheromone reinforcement\n\n\n\n\n\n7. Design Patterns\nCommon metaheuristic pattern:\n\nRepresent solution- Define fitness / cost function- Define neighbor / mutation operators- Balance randomness and greediness Tuning parameters often matters more than equations.\n\n\n\n8. Hybrid Metaheuristics\nCombine strengths:\n\nGA + SA: evolve population, fine-tune locally- PSO + DE: use swarm + differential evolution- ACO + Local Search: reinforce with hill-climbing These hybrids often outperform single methods.\n\n\n\n9. Common Pitfalls\n\nPoor representation → weak search- Over-exploitation → stuck in local optima- Bad parameters → chaotic or stagnant behavior Always visualize progress (fitness over time).\n\n\n\n10. Why It Matters\nMetaheuristics give us adaptive intelligence, searching without gradients, equations, or complete knowledge. They reflect nature’s way of solving complex puzzles: iterate, adapt, survive.\n\n“Optimization is not about perfection. It’s about progress guided by curiosity.”\n\n\n\nTry It Yourself\n\nImplement Simulated Annealing for the Traveling Salesman Problem.\nCreate a Genetic Algorithm for knapsack optimization.\nTune PSO parameters to fit a function \\(f(x) = x^2 + 10\\sin x\\).\nCompare ACO paths for TSP at different evaporation rates.\n\n\n\n\n97. Reinforcement Learning (Q-learning, Policy Gradients)\nReinforcement Learning (RL) is about learning through interaction , an agent explores an environment, takes actions, and learns from rewards. Unlike supervised learning (where correct labels are given), RL learns what to do by trial and error.\nThis section introduces two core approaches:\n\nQ-learning (value-based)- Policy Gradient (policy-based)\n\n\n1. The Reinforcement Learning Setting\nAn RL problem is modeled as a Markov Decision Process (MDP):\n\nStates \\(S\\)\nActions \\(A\\)\nTransition \\(P(s' \\mid s, a)\\)\nReward \\(R(s, a)\\)\nDiscount factor \\(\\gamma\\)\n\nThe agent’s goal is to find a policy \\(\\pi(a \\mid s)\\) that maximizes expected return:\n\\[\nG_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n\\]\n\n\n2. Value Functions\nThe value function measures how good a state (or state-action pair) is.\n\nState-value: \\[\nV^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]\n\\]\nAction-value (Q-function): \\[\nQ^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]\n\\]\n\n\n\n3. Bellman Equation\nThe Bellman equation relates a state’s value to its neighbors:\n\\[\nQ^*(s,a) = R(s,a) + \\gamma \\max_{a'} Q^*(s',a')\n\\]\nThis recursive definition drives value iteration and Q-learning.\n\n\n4. Q-Learning\nQ-learning learns the optimal action-value function off-policy (independent of behavior policy):\nUpdate Rule: \\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\big]\n\\]\nTiny Code\nQ[s][a] += alpha * (r + gamma * max(Q[s_next]) - Q[s][a]);\ns = s_next;\nRepeat while exploring (e.g., \\(\\varepsilon\\)-greedy):\n\nWith probability \\(\\varepsilon\\), choose a random action\nWith probability \\(1 - \\varepsilon\\), choose the best action\n\nOver time, \\(Q\\) converges to \\(Q^*\\).\n\n\n5. Exploration vs Exploitation\nRL is a balancing act:\n\nExploration: try new actions to gather knowledge- Exploitation: use current best knowledge to maximize reward Strategies:\nε-greedy- Softmax action selection- Upper Confidence Bound (UCB)\n\n\n\n6. Policy Gradient Methods\nInstead of learning Q-values, learn the policy directly. Represent policy with parameters \\(\\theta\\):\n\\[\n\\pi_\\theta(a|s) = P(a | s; \\theta)\n\\]\nGoal: maximize expected return \\[\nJ(\\theta) = \\mathbb{E}*{\\pi*\\theta}[G_t]\n\\]\nGradient ascent update: \\[\n\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n\\]\nREINFORCE Algorithm: \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}\\big[ G_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\big]\n\\]\nTiny Code\ntheta += alpha * G_t * grad_logpi(a_t, s_t);\n\n\n7. Actor-Critic Architecture\nCombines policy gradient (actor) + value estimation (critic).\n\nActor: updates policy- Critic: estimates value (baseline) Update: \\[\n\\theta \\leftarrow \\theta + \\alpha_\\theta \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n\\]\n\n\\[\nw \\leftarrow w + \\alpha_w \\delta_t \\nabla_w V_w(s_t)\n\\]\nwith TD error: \\[\n\\delta_t = r + \\gamma V(s') - V(s)\n\\]\n\n\n8. Comparing Methods\n\n\n\n\n\n\n\n\n\n\n\nMethod\nType\nLearns\nOn/Off Policy\nContinuous?\n\n\n\n\n\nQ-learning\nValue-based\nQ(s, a)\nOff-policy\nNo\n\n\n\nPolicy Gradient\nPolicy-based\nπ(a\ns)\nOn-policy\nYes\n\n\nActor-Critic\nHybrid\nBoth\nOn-policy\nYes\n\n\n\n\n\n\n9. Extensions\n\nDeep Q-Networks (DQN): use neural nets for Q(s, a)- PPO / A3C: advanced actor-critic methods- TD(λ): tradeoff between MC and TD learning- Double Q-learning: reduce overestimation- Entropy regularization: encourage exploration\n\n\n\n10. Why It Matters\nReinforcement learning powers autonomous agents, game AIs, and control systems. It’s the foundation of AlphaGo, robotics control, and adaptive decision systems.\n\n“An agent learns not from instruction but from experience.”\n\n\n\nTry It Yourself\n\nImplement Q-learning for a grid-world maze.\nAdd ε-greedy exploration.\nVisualize the learned policy.\nTry REINFORCE with a simple policy (e.g. softmax over actions).\nCompare convergence of Q-learning vs Policy Gradient.\n\n\n\n\n98. Approximation and Online Algorithms\nIn the real world, we often can’t wait for a perfect solution , data arrives on the fly, or the problem is too hard to solve exactly. That’s where approximation and online algorithms shine. They aim for good-enough results, fast and adaptively, under uncertainty.\n\n1. The Big Picture\n\nApproximation algorithms: Solve NP-hard problems with provable bounds.- Online algorithms: Make immediate decisions without knowing the future. Both trade optimality for efficiency or adaptability.\n\n\n\n2. Approximation Algorithms\nAn approximation algorithm finds a solution within a factor \\(\\rho\\) of the optimal.\nIf ( C ) is cost of the algorithm, and \\(C^*\\) is optimal cost:\n\\[\n\\rho = \\max\\left(\\frac{C}{C^*}, \\frac{C^*}{C}\\right)\n\\]\nExample: \\(\\rho = 2\\) → solution at most twice worse than optimal.\n\n\n3. Example: Vertex Cover\nProblem: Given graph ( G(V,E) ), choose smallest set of vertices covering all edges.\nAlgorithm (2-approximation):\n\nInitialize cover = ∅\nWhile edges remain:\n\nPick any edge (u, v) - Add both u, v to cover - Remove all edges incident on u or v Guarantee: At most 2× optimal size.\n\n\nTiny Code\ncover = {};\nwhile (!edges.empty()) {\n    (u, v) = edges.pop();\n    cover.add(u);\n    cover.add(v);\n    remove_incident_edges(u, v);\n}\n\n\n4. Example: Metric TSP (Triangle Inequality)\nAlgorithm (Christofides):\n\nFind MST\nFind odd-degree vertices\nFind min perfect matching\nCombine + shortcut to get tour\n\nGuarantee: ≤ 1.5 × optimal.\n\n\n5. Greedy Approximation: Set Cover\nGoal: Cover universe ( U ) with minimum sets \\(S_i\\).\nGreedy Algorithm: Pick set covering most uncovered elements each time. Guarantee: \\(H_n \\approx \\ln n\\) factor approximation.\n\n\n6. Online Algorithms\nOnline algorithms must decide now, before future input is known.\nGoal: Minimize competitive ratio:\n\\[\n\\text{CR} = \\max_{\\text{input}} \\frac{\\text{Cost}*{\\text{online}}}{\\text{Cost}*{\\text{optimal offline}}}\n\\]\nLower CR → better adaptability.\n\n\n7. Classic Example: Online Paging\nYou have k pages in cache, sequence of page requests.\n\nIf page in cache → hit- Else → miss, must evict one page Strategies:\nLRU (Least Recently Used): evict oldest- FIFO: evict first loaded- Random: pick randomly Competitive Ratio:\nLRU: ≤ ( k )- Random: ≤ ( 2k-1 )\n\nTiny Code\ncache = LRUCache(k);\nfor (page in requests) {\n    if (!cache.contains(page))\n        cache.evict_oldest();\n    cache.add(page);\n}\n\n\n8. Online Bipartite Matching (Karp-Vazirani-Vazirani)\nGiven offline set U and online set V (arrives one by one), match greedily. Competitive ratio: \\(1 - \\frac{1}{e}\\)\nUsed in ad allocation and resource assignment.\n\n\n9. Approximation + Online Together\nModern algorithms blend both:\n\nStreaming algorithms: One pass, small memory (Count-Min, reservoir sampling)- Online learning: Update models incrementally (SGD, perceptron)- Approximate dynamic programming: RL and heuristic search These are approximate online solvers , both quick and adaptive.\n\n\n\n10. Why It Matters\nApproximation algorithms give us provable near-optimal answers. Online algorithms give us real-time adaptivity. Together, they model intelligence under limits , when time and information are scarce.\n\n“Sometimes, good and on time beats perfect and late.”\n\n\n\nTry It Yourself\n\nImplement 2-approx vertex cover on a small graph.\nSimulate online paging with LRU vs Random.\nBuild a greedy set cover solver.\nMeasure competitive ratio on test sequences.\nCombine ideas: streaming + approximation for big data filtering.\n\n\n\n\n99. Fairness, Causal Inference, and Robust Optimization\nAs algorithms increasingly shape decisions , from hiring to lending to healthcare , we must ensure they’re fair, causally sound, and robust to uncertainty. This section blends ideas from ethics, statistics, and optimization to make algorithms not just efficient, but responsible and reliable.\n\n1. Why Fairness Matters\nMachine learning systems often inherit biases from data. Without intervention, they can amplify inequality or discrimination.\nFairness-aware algorithms explicitly measure and correct these effects.\nCommon sources of bias:\n\nHistorical bias (biased data)- Measurement bias (imprecise features)- Selection bias (skewed samples) The goal: equitable treatment across sensitive groups (gender, race, region, etc.)\n\n\n\n2. Formal Fairness Criteria\nSeveral fairness notions exist, often conflicting:\n\n\n\n\n\n\n\n\n\n\nCriterion\nDescription\nExample\n\n\n\n\n\n\nDemographic Parity\n( P\\(\\hat{Y}=1                      | A=a\\) = P\\(\\hat{Y}=1                               | A=b\\) )\nEqual positive rate\n\n\n\n\nEqual Opportunity\nEqual true positive rates\nSame recall for all groups\n\n\n\n\nEqualized Odds\nEqual TPR & FPR\nBalanced errors\n\n\n\n\nCalibration\nSame predicted probability meaning\nIf model says 70%, all groups should achieve 70%\n\n\n\n\n\nNo single measure fits all , fairness depends on context and trade-offs.\n\n\n3. Algorithmic Fairness Techniques\n\nPre-processing Rebalance or reweight data before training. Example: reweighing, sampling.\nIn-processing Add fairness constraints to loss function. Example: adversarial debiasing.\nPost-processing Adjust predictions after training. Example: threshold shifting.\n\nTiny Code (Adversarial Debiasing Skeleton)\nfor x, a, y in data:\n    y_pred = model(x)\n    loss_main = loss_fn(y_pred, y)\n    loss_adv = adv_fn(y_pred, a)\n    loss_total = loss_main - λ * loss_adv\n    update(loss_total)\nHere, the adversary tries to predict sensitive attribute, encouraging invariance.\n\n\n4. Causal Inference Basics\nCorrelation ≠ causation. To reason about fairness and robustness, we need causal understanding , what would happen if we changed something.\nCausal inference models relationships via Directed Acyclic Graphs (DAGs):\n\nNodes: variables- Edges: causal influence\n\n\n\n5. Counterfactual Reasoning\nA counterfactual asks:\n\n“What would the outcome be if we intervened differently?”\n\nFormally: \\[\nP(Y_{do(X=x)})\n\\]\nUsed in:\n\nFairness (counterfactual fairness)- Policy evaluation- Robust decision making\n\n\n\n6. Counterfactual Fairness\nAn algorithm is counterfactually fair if prediction stays the same under hypothetical changes to sensitive attributes.\n\\[\n\\hat{Y}*{A \\leftarrow a}(U) = \\hat{Y}*{A \\leftarrow a'}(U)\n\\]\nThis requires causal models , not just data.\n\n\n7. Robust Optimization\nIn uncertain environments, we want solutions that hold up under worst-case conditions.\nFormulation: \\[\n\\min_x \\max_{\\xi \\in \\Xi} f(x, \\xi)\n\\]\nwhere \\(\\Xi\\) is the uncertainty set.\nExample: Design a portfolio that performs well under varying market conditions.\nTiny Code\ndouble robust_objective(double x[], Scenario Xi[], int N) {\n    double worst = -INF;\n    for (i=0; i&lt;N; i++)\n        worst = max(worst, f(x, Xi[i]));\n    return worst;\n}\nThis searches for a solution minimizing worst-case loss.\n\n\n8. Distributional Robustness\nInstead of worst-case instances, protect against worst-case distributions:\n\\[\n\\min_\\theta \\sup_{Q \\in \\mathcal{B}(P)} \\mathbb{E}_{x \\sim Q}[L(\\theta, x)]\n\\]\nUsed in adversarial training and domain adaptation.\nExample: Add noise or perturbations to improve resilience:\nx_adv = x + ε * sign(grad(loss, x))\n\n\n9. Balancing Fairness, Causality, and Robustness\n\n\n\n\n\n\n\n\nGoal\nMethod\nChallenge\n\n\n\n\nFairness\nParity, Adversarial, Counterfactual\nCompeting definitions\n\n\nCausality\nDAGs, do-calculus, SCMs\nIdentifying true structure\n\n\nRobustness\nMin-max, DRO, Adversarial Training\nTrade-off with accuracy\n\n\n\nReal-world design involves balancing trade-offs.\nSometimes improving fairness reduces accuracy, or robustness increases conservatism.\n\n\n10. Why It Matters\nAlgorithms don’t exist in isolation , they affect people. Embedding fairness, causality, and robustness ensures systems are trustworthy, interpretable, and just.\n\n“The goal is not just intelligent algorithms , but responsible ones.”\n\n\n\nTry It Yourself\n\nTrain a simple classifier on biased data.\nApply reweighing or adversarial debiasing.\nDraw a causal DAG of your data features.\nCompute counterfactual fairness for a sample.\nImplement a robust loss using adversarial perturbations.\n\n\n\n\n100. AI Planning, Search, and Learning Systems\nAI systems are not just pattern recognizers , they are decision makers. They plan, search, and learn in structured environments, choosing actions that lead to long-term goals. This section explores how modern AI combines planning, search, and learning to solve complex tasks.\n\n1. What Is AI Planning?\nAI planning is about finding a sequence of actions that transforms an initial state into a goal state.\nFormally, a planning problem consists of:\n\nStates ( S )- Actions ( A )- Transition function ( T(s, a) s’ )- Goal condition \\(G \\subseteq S\\)- Cost function ( c(a) ) The objective: Find a plan \\(\\pi = [a_1, a_2, \\ldots, a_n]\\) minimizing total cost or maximizing reward.\n\n\n\n2. Search-Based Planning\nAt the heart of planning lies search. Search explores possible action sequences, guided by heuristics.\n\n\n\nAlgorithm\nType\nDescription\n\n\n\n\nDFS\nUninformed\nDeep exploration, no guarantee\n\n\nBFS\nUninformed\nFinds shortest path\n\n\nDijkstra\nWeighted\nOptimal if costs ≥ 0\n\n\nA*\nHeuristic\nCombines cost + heuristic\n\n\n\nA* Search Formula: \\[\nf(n) = g(n) + h(n)\n\\] where:\n\n( g(n) ): cost so far- ( h(n) ): heuristic estimate to goal If ( h ) is admissible, A* is optimal.\n\nTiny Code (A* Skeleton)\npriority_queue&lt;Node&gt; open;\ng[start] = 0;\nopen.push({start, h(start)});\n\nwhile (!open.empty()) {\n    n = open.pop_min();\n    if (goal(n)) break;\n    for (a in actions(n)) {\n        s = step(n, a);\n        cost = g[n] + c(n, a);\n        if (cost &lt; g[s]) {\n            g[s] = cost;\n            f[s] = g[s] + h(s);\n            open.push({s, f[s]});\n        }\n    }\n}\n\n\n3. Heuristics and Admissibility\nA heuristic ( h(s) ) estimates distance to the goal.\n\nAdmissible: never overestimates- Consistent: satisfies triangle inequality Examples:\nManhattan distance (grids)- Euclidean distance (geometry)- Pattern databases (puzzles) Good heuristics = faster convergence.\n\n\n\n4. Classical Planning (STRIPS)\nIn symbolic AI, states are represented by facts (predicates), and actions have preconditions and effects.\nExample:\nAction: Move(x, y)\nPrecondition: At(x), Clear(y)\nEffect: ¬At(x), At(y)\nSearch happens in logical state space.\nPlanners:\n\nForward search (progression)- Backward search (regression)- Heuristic planners (FF, HSP)\n\n\n\n5. Hierarchical Planning\nBreak complex goals into subgoals.\n\nHTN (Hierarchical Task Networks): Define high-level tasks broken into subtasks.\n\nExample: “Make dinner” → [Cook rice, Stir-fry vegetables, Set table]\nHierarchy makes planning modular and interpretable.\n\n\n6. Probabilistic Planning\nWhen actions are uncertain:\n\nMDPs: full observability, stochastic transitions- POMDPs: partial observability Use value iteration, policy iteration, or Monte Carlo planning.\n\n\n\n7. Learning to Plan\nCombine learning with search:\n\nLearned heuristics: neural networks approximate ( h(s) )- AlphaZero-style planning: learn value + policy, guide tree search- Imitation learning: mimic expert demonstrations This bridges classical AI and modern ML.\n\nTiny Code (Learning-Guided A*)\nf = g + alpha * learned_heuristic(s)\nNeural net learns ( h_(s) ) from solved examples.\n\n\n8. Integrated Systems\nModern AI stacks combine:\n\nSearch (planning backbone)- Learning (policy, heuristic, model)- Simulation (data generation) Examples:\nAlphaZero: self-play + MCTS + neural nets- MuZero: learns model + value + policy jointly- Large Language Agents: use reasoning + memory + search\n\n\n\n9. Real-World Applications\n\nRobotics: motion planning, pathfinding- Games: Go, Chess, strategy games- Logistics: route optimization- Autonomy: drones, vehicles, AI assistants- Synthesis: program and query generation Each blends symbolic reasoning and statistical learning.\n\n\n\n10. Why It Matters\nPlanning, search, and learning form the triad of intelligence:\n\nSearch explores possibilities- Planning sequences actions toward goals- Learning adapts heuristics from experience Together, they power systems that think, adapt, and act.\n\n\n“Intelligence is not just knowing , it is choosing wisely under constraints.”\n\n\n\nTry It Yourself\n\nImplement A* search on a grid maze.\nAdd a Manhattan heuristic.\nExtend to probabilistic transitions (simulate noise).\nBuild a simple planner with preconditions and effects.\nTrain a neural heuristic to guide search on puzzles.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-us/plan.html",
    "href": "books/en-us/plan.html",
    "title": "The Plan",
    "section": "",
    "text": "Chapter 1. Foundations of Algorithms\n\n1. What Is an Algorithm?\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n1\nEuclid’s GCD\nOldest known algorithm for greatest common divisor\n\n\n2\nSieve of Eratosthenes\nGenerate primes efficiently\n\n\n3\nBinary Search\nDivide and conquer search\n\n\n4\nExponentiation by Squaring\nFast power computation\n\n\n5\nLong Division\nClassic step-by-step arithmetic\n\n\n6\nModular Addition Algorithm\nWrap-around arithmetic\n\n\n7\nBase Conversion Algorithm\nConvert between number systems\n\n\n8\nFactorial Computation\nRecursive and iterative approaches\n\n\n9\nFibonacci Sequence\nRecursive vs. dynamic computation\n\n\n10\nTower of Hanoi\nRecursive problem-solving pattern\n\n\n\n\n\n2. Measuring Time and Space\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n11\nCounting Operations\nManual step-counting for complexity\n\n\n12\nLoop Analysis\nEvaluate time cost of loops\n\n\n13\nRecurrence Expansion\nAnalyze recursive costs\n\n\n14\nAmortized Analysis\nAverage per-operation cost\n\n\n15\nSpace Counting\nStack and heap tracking\n\n\n16\nMemory Footprint Estimator\nTrack per-variable usage\n\n\n17\nTime Complexity Table\nMap O(1)…O(n²)…O(2ⁿ)\n\n\n18\nSpace-Time Tradeoff\nCache vs. recomputation\n\n\n19\nProfiling Algorithm\nEmpirical time measurement\n\n\n20\nBenchmarking Framework\nCompare algorithm performance\n\n\n\n\n\n3. Big-O, Big-Theta, Big-Omega\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n21\nGrowth Rate Comparator\nCompare asymptotic behaviors\n\n\n22\nDominant Term Extractor\nSimplify runtime expressions\n\n\n23\nLimit-Based Complexity Test\nUsing limits for asymptotics\n\n\n24\nSummation Simplifier\nSum of arithmetic/geometric sequences\n\n\n25\nRecurrence Tree Method\nVisualize recursive costs\n\n\n26\nMaster Theorem Evaluator\nSolve T(n) recurrences\n\n\n27\nBig-Theta Proof Builder\nBounding upper and lower limits\n\n\n28\nBig-Omega Case Finder\nBest-case scenario analysis\n\n\n29\nEmpirical Complexity Estimator\nMeasure via doubling experiments\n\n\n30\nComplexity Class Identifier\nMatch runtime to known class\n\n\n\n\n\n4. Algorithmic Paradigms (Greedy, Divide and Conquer, DP)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n31\nGreedy Coin Change\nLocal optimal step-by-step\n\n\n32\nHuffman Coding\nGreedy compression tree\n\n\n33\nMerge Sort\nDivide and conquer sort\n\n\n34\nBinary Search\nDivide and conquer search\n\n\n35\nKaratsuba Multiplication\nRecursive divide & conquer\n\n\n36\nMatrix Chain Multiplication\nDP with substructure\n\n\n37\nLongest Common Subsequence\nClassic DP problem\n\n\n38\nRod Cutting\nDP optimization\n\n\n39\nActivity Selection\nGreedy scheduling\n\n\n40\nOptimal Merge Patterns\nGreedy file merging\n\n\n\n\n\n5. Recurrence Relations\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n41\nLinear Recurrence Solver\nClosed-form for linear recurrences\n\n\n42\nMaster Theorem\nDivide-and-conquer complexity\n\n\n43\nSubstitution Method\nInductive proof approach\n\n\n44\nIteration Method\nExpand recurrence step-by-step\n\n\n45\nGenerating Functions\nTransform recurrences\n\n\n46\nMatrix Exponentiation\nSolve linear recurrences fast\n\n\n47\nRecurrence to DP Table\nTabulation approach\n\n\n48\nDivide & Combine Template\nConvert recurrence into algorithm\n\n\n49\nMemoized Recursive Solver\nStore overlapping results\n\n\n50\nCharacteristic Polynomial\nSolve homogeneous recurrence\n\n\n\n\n\n6. Searching Basics\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n51\nLinear Search\nSequential element scan\n\n\n52\nBinary Search\nMidpoint halving\n\n\n53\nJump Search\nBlock skip linear\n\n\n54\nExponential Search\nDoubling step size\n\n\n55\nInterpolation Search\nEstimate position by value\n\n\n56\nTernary Search\nDivide into thirds\n\n\n57\nFibonacci Search\nGolden ratio search\n\n\n58\nSentinel Search\nEarly termination optimization\n\n\n59\nBidirectional Search\nMeet-in-the-middle\n\n\n60\nSearch in Rotated Array\nAdapted binary search\n\n\n\n\n\n7. Sorting Basics\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n61\nBubble Sort\nAdjacent swap sort\n\n\n62\nSelection Sort\nFind minimum each pass\n\n\n63\nInsertion Sort\nIncremental build sort\n\n\n64\nShell Sort\nGap-based insertion\n\n\n65\nMerge Sort\nDivide-and-conquer\n\n\n66\nQuick Sort\nPartition-based\n\n\n67\nHeap Sort\nBinary heap order\n\n\n68\nCounting Sort\nInteger key distribution\n\n\n69\nRadix Sort\nDigit-by-digit\n\n\n70\nBucket Sort\nGroup into ranges\n\n\n\n\n\n8. Data Structures Overview\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n71\nStack Push/Pop\nLIFO operations\n\n\n72\nQueue Enqueue/Dequeue\nFIFO operations\n\n\n73\nSingly Linked List\nLinear node chain\n\n\n74\nDoubly Linked List\nBidirectional traversal\n\n\n75\nHash Table Insertion\nKey-value indexing\n\n\n76\nBinary Search Tree Insert\nOrdered node placement\n\n\n77\nHeapify\nBuild heap in-place\n\n\n78\nUnion-Find Operations\nDisjoint-set management\n\n\n79\nGraph Adjacency List Build\nSparse representation\n\n\n80\nTrie Insertion/Search\nPrefix tree for strings\n\n\n\n\n\n9. Graphs and Trees Overview\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n81\nDFS Traversal\nDepth-first exploration\n\n\n82\nBFS Traversal\nLevel-order exploration\n\n\n83\nTopological Sort\nDAG ordering\n\n\n84\nMinimum Spanning Tree\nKruskal/Prim overview\n\n\n85\nDijkstra’s Shortest Path\nWeighted graph shortest route\n\n\n86\nBellman-Ford\nHandle negative edges\n\n\n87\nFloyd-Warshall\nAll-pairs shortest path\n\n\n88\nUnion-Find for MST\nEdge grouping\n\n\n89\nTree Traversals\nInorder, Preorder, Postorder\n\n\n90\nLCA (Lowest Common Ancestor)\nCommon node in tree\n\n\n\n\n\n10. Algorithm Design Patterns\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n91\nBrute Force\nTry all possibilities\n\n\n92\nGreedy Choice\nLocal optimum per step\n\n\n93\nDivide and Conquer\nBreak and merge\n\n\n94\nDynamic Programming\nReuse subproblems\n\n\n95\nBacktracking\nExplore with undo\n\n\n96\nBranch and Bound\nPrune search space\n\n\n97\nRandomized Algorithm\nInject randomness\n\n\n98\nApproximation Algorithm\nNear-optimal solution\n\n\n99\nOnline Algorithm\nStep-by-step decision\n\n\n100\nHybrid Strategy\nCombine paradigms\n\n\n\n\n\n\nChapter 2. Sorting and Searching\n\n11. Elementary Sorting (Bubble, Insertion, Selection)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n101\nBubble Sort\nSwap adjacent out-of-order elements\n\n\n102\nImproved Bubble Sort\nStop early if already sorted\n\n\n103\nCocktail Shaker Sort\nBidirectional bubble pass\n\n\n104\nSelection Sort\nSelect smallest element each pass\n\n\n105\nDouble Selection Sort\nFind both min and max each pass\n\n\n106\nInsertion Sort\nInsert each element into correct spot\n\n\n107\nBinary Insertion Sort\nUse binary search for position\n\n\n108\nGnome Sort\nSimple insertion-like with swaps\n\n\n109\nOdd-Even Sort\nParallel-friendly comparison sort\n\n\n110\nStooge Sort\nRecursive quirky educational sort\n\n\n\n\n\n12. Divide-and-Conquer Sorting (Merge, Quick, Heap)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n111\nMerge Sort\nRecursive divide and merge\n\n\n112\nIterative Merge Sort\nBottom-up non-recursive version\n\n\n113\nQuick Sort\nPartition-based recursive sort\n\n\n114\nHoare Partition Scheme\nClassic quicksort partition\n\n\n115\nLomuto Partition Scheme\nSimpler but less efficient\n\n\n116\nRandomized Quick Sort\nAvoid worst-case pivot\n\n\n117\nHeap Sort\nHeapify + extract max repeatedly\n\n\n118\n3-Way Quick Sort\nHandle duplicates efficiently\n\n\n119\nExternal Merge Sort\nDisk-based merge for large data\n\n\n120\nParallel Merge Sort\nDivide work among threads\n\n\n\n\n\n13. Counting and Distribution Sorts (Counting, Radix, Bucket)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n121\nCounting Sort\nCount key occurrences\n\n\n122\nStable Counting Sort\nPreserve order of equals\n\n\n123\nRadix Sort (LSD)\nLeast significant digit first\n\n\n124\nRadix Sort (MSD)\nMost significant digit first\n\n\n125\nBucket Sort\nDistribute into buckets\n\n\n126\nPigeonhole Sort\nSimple bucket variant\n\n\n127\nFlash Sort\nDistribution with in-place correction\n\n\n128\nPostman Sort\nStable multi-key sort\n\n\n129\nAddress Calculation Sort\nHash-like distribution\n\n\n130\nSpread Sort\nHybrid radix/quick strategy\n\n\n\n\n\n14. Hybrid Sorts (IntroSort, Timsort)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n131\nIntroSort\nQuick + Heap fallback\n\n\n132\nTimSort\nMerge + Insertion + Runs\n\n\n133\nDual-Pivot QuickSort\nModern quicksort optimization\n\n\n134\nSmoothSort\nHeap-like adaptive sort\n\n\n135\nBlock Merge Sort\nCache-efficient merge variant\n\n\n136\nAdaptive Merge Sort\nAdjusts to partially sorted data\n\n\n137\nPDQSort\nPattern-defeating quicksort\n\n\n138\nWikiSort\nStable in-place merge\n\n\n139\nGrailSort\nIn-place stable mergesort\n\n\n140\nAdaptive Hybrid Sort\nDynamically selects strategy\n\n\n\n\n\n15. Special Sorts (Cycle, Gnome, Comb, Pancake)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n141\nCycle Sort\nMinimal writes\n\n\n142\nComb Sort\nShrinking gap bubble\n\n\n143\nGnome Sort\nInsertion-like with swaps\n\n\n144\nCocktail Sort\nTwo-way bubble\n\n\n145\nPancake Sort\nFlip-based sorting\n\n\n146\nBitonic Sort\nParallel network sorting\n\n\n147\nOdd-Even Merge Sort\nSorting network design\n\n\n148\nSleep Sort\nUses timing as order key\n\n\n149\nBead Sort\nSimulates gravity\n\n\n150\nBogo Sort\nRandomly permute until sorted\n\n\n\n\n\n16. Linear and Binary Search\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n151\nLinear Search\nScan sequentially\n\n\n152\nLinear Search (Sentinel)\nGuard element at end\n\n\n153\nBinary Search (Iterative)\nHalve interval each loop\n\n\n154\nBinary Search (Recursive)\nHalve interval via recursion\n\n\n155\nBinary Search (Lower Bound)\nFirst &gt;= target\n\n\n156\nBinary Search (Upper Bound)\nFirst &gt; target\n\n\n157\nExponential Search\nDouble step size\n\n\n158\nJump Search\nJump fixed steps then linear\n\n\n159\nFibonacci Search\nGolden-ratio style jumps\n\n\n160\nUniform Binary Search\nAvoid recomputing midpoints\n\n\n\n\n\n17. Interpolation and Exponential Search\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n161\nInterpolation Search\nEstimate index by value\n\n\n162\nRecursive Interpolation Search\nDivide by estimated midpoint\n\n\n163\nExponential Search\nDouble and binary refine\n\n\n164\nDoubling Search\nGeneric exponential pattern\n\n\n165\nGalloping Search\nUsed in TimSort merges\n\n\n166\nUnbounded Binary Search\nFind bounds dynamically\n\n\n167\nRoot-Finding Bisection\nSearch zero-crossing\n\n\n168\nGolden Section Search\nOptimize unimodal function\n\n\n169\nFibonacci Search (Optimum)\nSimilar to golden search\n\n\n170\nJump + Binary Hybrid\nCombined probing strategy\n\n\n\n\n\n18. Selection Algorithms (Quickselect, Median of Medians)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n171\nQuickselect\nPartition-based selection\n\n\n172\nMedian of Medians\nDeterministic pivot\n\n\n173\nRandomized Select\nRandom pivot version\n\n\n174\nBinary Search on Answer\nRange-based selection\n\n\n175\nOrder Statistics Tree\nBST with rank queries\n\n\n176\nTournament Tree Selection\nHierarchical comparison\n\n\n177\nHeap Select (Min-Heap)\nMaintain top-k elements\n\n\n178\nPartial QuickSort\nSort partial prefix\n\n\n179\nBFPRT Algorithm\nLinear-time selection\n\n\n180\nKth Largest Stream\nStreaming selection\n\n\n\n\n\n19. Range Searching and Nearest Neighbor\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n181\nBinary Search Range\nFind lower and upper bounds\n\n\n182\nSegment Tree Query\nSum/min/max over interval\n\n\n183\nFenwick Tree Query\nEfficient prefix sums\n\n\n184\nInterval Tree Search\nOverlap queries\n\n\n185\nKD-Tree Search\nSpatial nearest neighbor\n\n\n186\nR-Tree Query\nRange search in geometry\n\n\n187\nRange Minimum Query (RMQ)\nSparse table approach\n\n\n188\nMo’s Algorithm\nOffline query reordering\n\n\n189\nSweep Line Range Search\nSort + scan technique\n\n\n190\nBall Tree Nearest Neighbor\nMetric-space search\n\n\n\n\n\n20. Search Optimizations and Variants\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n191\nBinary Search with Tolerance\nFor floating values\n\n\n192\nTernary Search\nUnimodal optimization\n\n\n193\nHash-Based Search\nO(1) expected lookup\n\n\n194\nBloom Filter Lookup\nProbabilistic membership\n\n\n195\nCuckoo Hash Search\nDual-hash relocation\n\n\n196\nRobin Hood Hashing\nEqualize probe lengths\n\n\n197\nJump Consistent Hashing\nStable hash assignment\n\n\n198\nPrefix Search in Trie\nAuto-completion lookup\n\n\n199\nPattern Search in Suffix Array\nFast substring lookup\n\n\n200\nSearch in Infinite Array\nDynamic bound finding\n\n\n\n\n\n\nChapter 3. Data Structures in Action\n\n21. Arrays, Linked Lists, Stacks, Queues\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n201\nDynamic Array Resize\nDoubling strategy for capacity\n\n\n202\nCircular Array Implementation\nWrap-around indexing\n\n\n203\nSingly Linked List Insert/Delete\nBasic node manipulation\n\n\n204\nDoubly Linked List Insert/Delete\nTwo-way linkage\n\n\n205\nStack Push/Pop\nLIFO structure\n\n\n206\nQueue Enqueue/Dequeue\nFIFO structure\n\n\n207\nDeque Implementation\nDouble-ended queue\n\n\n208\nCircular Queue\nFixed-size queue with wrap-around\n\n\n209\nStack via Queue\nImplement stack using two queues\n\n\n210\nQueue via Stack\nImplement queue using two stacks\n\n\n\n\n\n22. Hash Tables and Variants (Cuckoo, Robin Hood, Consistent)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n211\nHash Table Insertion\nKey-value pair with modulo\n\n\n212\nLinear Probing\nResolve collisions sequentially\n\n\n213\nQuadratic Probing\nNonlinear probing sequence\n\n\n214\nDouble Hashing\nAlternate hash on collision\n\n\n215\nCuckoo Hashing\nTwo-table relocation strategy\n\n\n216\nRobin Hood Hashing\nEqualize probe length fairness\n\n\n217\nChained Hash Table\nLinked list buckets\n\n\n218\nPerfect Hashing\nNo-collision mapping\n\n\n219\nConsistent Hashing\nStable distribution across nodes\n\n\n220\nDynamic Rehashing\nResize on load factor threshold\n\n\n\n\n\n23. Heaps (Binary, Fibonacci, Pairing)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n221\nBinary Heap Insert\nBubble-up maintenance\n\n\n222\nBinary Heap Delete\nHeapify-down maintenance\n\n\n223\nBuild Heap (Heapify)\nBottom-up O(n) build\n\n\n224\nHeap Sort\nExtract max repeatedly\n\n\n225\nMin Heap Implementation\nFor smallest element access\n\n\n226\nMax Heap Implementation\nFor largest element access\n\n\n227\nFibonacci Heap Insert/Delete\nAmortized efficient operations\n\n\n228\nPairing Heap Merge\nLightweight mergeable heap\n\n\n229\nBinomial Heap Merge\nMerge trees of equal order\n\n\n230\nLeftist Heap Merge\nMaintain rank-skewed heap\n\n\n\n\n\n24. Balanced Trees (AVL, Red-Black, Splay, Treap)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n231\nAVL Tree Insert\nRotate to maintain balance\n\n\n232\nAVL Tree Delete\nBalance after deletion\n\n\n233\nRed-Black Tree Insert\nColor fix and rotations\n\n\n234\nRed-Black Tree Delete\nMaintain invariants\n\n\n235\nSplay Tree Access\nMove accessed node to root\n\n\n236\nTreap Insert\nPriority-based rotation\n\n\n237\nTreap Delete\nRandomized balance\n\n\n238\nWeight Balanced Tree\nMaintain subtree weights\n\n\n239\nScapegoat Tree Rebuild\nRebalance on size threshold\n\n\n240\nAA Tree\nSimplified red-black variant\n\n\n\n\n\n25. Segment Trees and Fenwick Trees\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n241\nBuild Segment Tree\nRecursive construction\n\n\n242\nRange Sum Query\nRecursive or iterative query\n\n\n243\nRange Update\nLazy propagation technique\n\n\n244\nPoint Update\nModify single element\n\n\n245\nFenwick Tree Build\nIncremental binary index\n\n\n246\nFenwick Update\nUpdate cumulative sums\n\n\n247\nFenwick Query\nPrefix sum retrieval\n\n\n248\nSegment Tree Merge\nCombine child results\n\n\n249\nPersistent Segment Tree\nMaintain history of versions\n\n\n250\n2D Segment Tree\nFor matrix range queries\n\n\n\n\n\n26. Disjoint Set Union (Union-Find)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n251\nMake-Set\nInitialize each element\n\n\n252\nFind\nLocate representative\n\n\n253\nUnion\nMerge two sets\n\n\n254\nUnion by Rank\nAttach smaller tree to larger\n\n\n255\nPath Compression\nFlatten tree structure\n\n\n256\nDSU with Rollback\nSupport undo operations\n\n\n257\nDSU on Tree\nTrack subtree connectivity\n\n\n258\nKruskal’s MST\nEdge selection with DSU\n\n\n259\nConnected Components\nGroup graph nodes\n\n\n260\nOffline Query DSU\nHandle dynamic unions\n\n\n\n\n\n27. Probabilistic Data Structures (Bloom, Count-Min, HyperLogLog)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n261\nBloom Filter Insert\nHash to bit array\n\n\n262\nBloom Filter Query\nProbabilistic membership check\n\n\n263\nCounting Bloom Filter\nSupport deletions via counters\n\n\n264\nCuckoo Filter\nSpace-efficient alternative\n\n\n265\nCount-Min Sketch\nApproximate frequency table\n\n\n266\nHyperLogLog\nCardinality estimation\n\n\n267\nFlajolet-Martin\nEarly probabilistic counting\n\n\n268\nMinHash\nEstimate Jaccard similarity\n\n\n269\nReservoir Sampling\nRandom k-sample stream\n\n\n270\nSkip Bloom Filter\nRange queries on Bloom\n\n\n\n\n\n28. Skip Lists and B-Trees\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n271\nSkip List Insert\nProbabilistic layered list\n\n\n272\nSkip List Delete\nAdjust pointers\n\n\n273\nSkip List Search\nJump via tower levels\n\n\n274\nB-Tree Insert\nSplit on overflow\n\n\n275\nB-Tree Delete\nMerge on underflow\n\n\n276\nB+ Tree Search\nLeaf-based sequential scan\n\n\n277\nB+ Tree Range Query\nEfficient ordered access\n\n\n278\nB* Tree\nMore space-efficient variant\n\n\n279\nAdaptive Radix Tree\nByte-wise branching\n\n\n280\nTrie Compression\nPath compression optimization\n\n\n\n\n\n29. Persistent and Functional Data Structures\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n281\nPersistent Stack\nKeep all versions\n\n\n282\nPersistent Array\nCopy-on-write segments\n\n\n283\nPersistent Segment Tree\nVersioned updates\n\n\n284\nPersistent Linked List\nImmutable nodes\n\n\n285\nFunctional Queue\nAmortized reverse lists\n\n\n286\nFinger Tree\nFast concat and split\n\n\n287\nZipper Structure\nLocalized mutation\n\n\n288\nRed-Black Persistent Tree\nImmutable balanced tree\n\n\n289\nTrie with Versioning\nHistorical string lookup\n\n\n290\nPersistent Union-Find\nTime-travel connectivity\n\n\n\n\n\n30. Advanced Trees and Range Queries\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n291\nSparse Table Build\nStatic range min/max\n\n\n292\nCartesian Tree\nRMQ to LCA transformation\n\n\n293\nSegment Tree Beats\nHandle complex queries\n\n\n294\nMerge Sort Tree\nRange count queries\n\n\n295\nWavelet Tree\nRank/select by value\n\n\n296\nKD-Tree\nMultidimensional queries\n\n\n297\nRange Tree\nOrthogonal range queries\n\n\n298\nFenwick 2D Tree\nMatrix prefix sums\n\n\n299\nTreap Split/Merge\nRange-based treap ops\n\n\n300\nMo’s Algorithm on Tree\nOffline subtree queries\n\n\n\n\n\n\nChapter 4. Graph Algorithms\n\n31. Traversals (DFS, BFS, Iterative Deepening)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n301\nDepth-First Search (Recursive)\nExplore deeply before backtracking\n\n\n302\nDepth-First Search (Iterative)\nStack-based exploration\n\n\n303\nBreadth-First Search (Queue)\nLevel-order traversal\n\n\n304\nIterative Deepening DFS\nCombine depth-limit + completeness\n\n\n305\nBidirectional BFS\nSearch from both ends\n\n\n306\nDFS on Grid\nMaze solving / connected components\n\n\n307\nBFS on Grid\nShortest path in unweighted graph\n\n\n308\nMulti-Source BFS\nParallel layer expansion\n\n\n309\nTopological Sort (DFS-based)\nDAG ordering\n\n\n310\nTopological Sort (Kahn’s Algorithm)\nIn-degree tracking\n\n\n\n\n\n32. Strongly Connected Components (Tarjan, Kosaraju)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n311\nKosaraju’s Algorithm\nTwo-pass DFS\n\n\n312\nTarjan’s Algorithm\nLow-link discovery\n\n\n313\nGabow’s Algorithm\nStack pair tracking\n\n\n314\nSCC DAG Construction\nCondensed component graph\n\n\n315\nSCC Online Merge\nIncremental condensation\n\n\n316\nComponent Label Propagation\nIterative labeling\n\n\n317\nPath-Based SCC\nDFS with path stack\n\n\n318\nKosaraju Parallel Version\nSCC via parallel DFS\n\n\n319\nDynamic SCC Maintenance\nAdd/remove edges\n\n\n320\nSCC for Weighted Graph\nCombine with edge weights\n\n\n\n\n\n33. Shortest Paths (Dijkstra, Bellman-Ford, A*, Johnson)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n321\nDijkstra (Binary Heap)\nGreedy edge relaxation\n\n\n322\nDijkstra (Fibonacci Heap)\nImproved priority queue\n\n\n323\nBellman-Ford\nNegative weights support\n\n\n324\nSPFA (Queue Optimization)\nFaster average Bellman-Ford\n\n\n325\nA* Search\nHeuristic-guided path\n\n\n326\nFloyd–Warshall\nAll-pairs shortest path\n\n\n327\nJohnson’s Algorithm\nAll-pairs using reweighting\n\n\n328\n0-1 BFS\nDeque-based shortest path\n\n\n329\nDial’s Algorithm\nInteger weight buckets\n\n\n330\nMulti-Source Dijkstra\nMultiple starting points\n\n\n\n\n\n34. Shortest Path Variants (0–1 BFS, Bidirectional, Heuristic A*)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n331\n0–1 BFS\nFor edges with weight 0 or 1\n\n\n332\nBidirectional Dijkstra\nMeet in the middle\n\n\n333\nA* with Euclidean Heuristic\nSpatial shortest path\n\n\n334\nALT Algorithm\nA* landmarks + triangle inequality\n\n\n335\nContraction Hierarchies\nPreprocessing for road networks\n\n\n336\nCH Query Algorithm\nShortcut-based routing\n\n\n337\nBellman-Ford Queue Variant\nEarly termination\n\n\n338\nDijkstra with Early Stop\nHalt on target\n\n\n339\nGoal-Directed Search\nRestrict expansion direction\n\n\n340\nYen’s K Shortest Paths\nEnumerate multiple best paths\n\n\n\n\n\n35. Minimum Spanning Trees (Kruskal, Prim, Borůvka)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n341\nKruskal’s Algorithm\nSort edges + union-find\n\n\n342\nPrim’s Algorithm (Heap)\nGrow MST from seed\n\n\n343\nPrim’s Algorithm (Adj Matrix)\nDense graph variant\n\n\n344\nBorůvka’s Algorithm\nComponent merging\n\n\n345\nReverse-Delete MST\nRemove heavy edges\n\n\n346\nMST via Dijkstra Trick\nFor positive weights\n\n\n347\nDynamic MST Maintenance\nHandle edge updates\n\n\n348\nMinimum Bottleneck Spanning Tree\nMax edge minimization\n\n\n349\nManhattan MST\nGrid graph optimization\n\n\n350\nEuclidean MST (Kruskal + Geometry)\nUse Delaunay graph\n\n\n\n\n\n36. Flows (Ford–Fulkerson, Edmonds–Karp, Dinic)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n351\nFord–Fulkerson\nAugmenting path method\n\n\n352\nEdmonds–Karp\nBFS-based Ford–Fulkerson\n\n\n353\nDinic’s Algorithm\nLevel graph + blocking flow\n\n\n354\nPush–Relabel\nLocal preflow push\n\n\n355\nCapacity Scaling\nSpeed-up with capacity tiers\n\n\n356\nCost Scaling\nMin-cost optimization\n\n\n357\nMin-Cost Max-Flow (Bellman-Ford)\nCosted augmenting paths\n\n\n358\nMin-Cost Max-Flow (SPFA)\nFaster average\n\n\n359\nCirculation with Demands\nGeneralized flow formulation\n\n\n360\nSuccessive Shortest Path\nIncremental min-cost updates\n\n\n\n\n\n37. Cuts (Stoer–Wagner, Karger, Gomory–Hu)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n361\nStoer–Wagner Minimum Cut\nGlobal min cut\n\n\n362\nKarger’s Randomized Cut\nContract edges randomly\n\n\n363\nKarger–Stein\nRecursive randomized cut\n\n\n364\nGomory–Hu Tree\nAll-pairs min-cut\n\n\n365\nMax-Flow Min-Cut\nDuality theorem application\n\n\n366\nStoer–Wagner Repeated Phase\nMultiple passes\n\n\n367\nDynamic Min Cut\nMaintain on edge update\n\n\n368\nMinimum s–t Cut (Edmonds–Karp)\nBased on flow\n\n\n369\nApproximate Min Cut\nRandom sampling\n\n\n370\nMin k-Cut\nPartition graph into k parts\n\n\n\n\n\n38. Matchings (Hopcroft–Karp, Hungarian, Blossom)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n371\nBipartite Matching (DFS)\nSimple augmenting path\n\n\n372\nHopcroft–Karp\nO(E√V) bipartite matching\n\n\n373\nHungarian Algorithm\nWeighted assignment\n\n\n374\nKuhn–Munkres\nMax-weight matching\n\n\n375\nBlossom Algorithm\nGeneral graph matching\n\n\n376\nEdmonds’ Blossom Shrinking\nOdd cycle contraction\n\n\n377\nGreedy Matching\nFast approximate\n\n\n378\nStable Marriage (Gale–Shapley)\nStable pairing\n\n\n379\nWeighted b-Matching\nCapacity-constrained\n\n\n380\nMaximal Matching\nLocal greedy maximal set\n\n\n\n\n\n39. Tree Algorithms (LCA, HLD, Centroid Decomposition)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n381\nEuler Tour LCA\nFlatten tree to array\n\n\n382\nBinary Lifting LCA\nJump powers of two\n\n\n383\nTarjan’s LCA (Offline DSU)\nQuery via union-find\n\n\n384\nHeavy-Light Decomposition\nDecompose paths\n\n\n385\nCentroid Decomposition\nRecursive split on centroid\n\n\n386\nTree Diameter (DFS Twice)\nFarthest pair\n\n\n387\nTree DP\nSubtree-based optimization\n\n\n388\nRerooting DP\nCompute all roots’ answers\n\n\n389\nBinary Search on Tree\nEdge weight constraints\n\n\n390\nVirtual Tree\nBuild on query subset\n\n\n\n\n\n40. Advanced Graph Algorithms and Tricks\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n391\nTopological DP\nDP on DAG order\n\n\n392\nSCC Condensed Graph DP\nMeta-graph processing\n\n\n393\nEulerian Path\nTrail covering all edges\n\n\n394\nHamiltonian Path\nNP-complete exploration\n\n\n395\nChinese Postman\nEulerian circuit with repeats\n\n\n396\nHierholzer’s Algorithm\nConstruct Eulerian circuit\n\n\n397\nJohnson’s Cycle Finding\nEnumerate all cycles\n\n\n398\nTransitive Closure (Floyd–Warshall)\nReachability matrix\n\n\n399\nGraph Coloring (Backtracking)\nConstraint satisfaction\n\n\n400\nArticulation Points & Bridges\nCritical structure detection\n\n\n\n\n\n\nChapter 5. Dynamic Programming\n\n41. DP Basics and State Transitions\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n401\nFibonacci DP\nClassic top-down vs bottom-up\n\n\n402\nClimbing Stairs\nCount paths with small steps\n\n\n403\nGrid Paths\nDP over 2D lattice\n\n\n404\nMin Cost Path\nAccumulate minimal sums\n\n\n405\nCoin Change (Count Ways)\nCombinatorial sums\n\n\n406\nCoin Change (Min Coins)\nMinimize step count\n\n\n407\nKnapsack 0/1\nSelect items under weight limit\n\n\n408\nKnapsack Unbounded\nRepeatable items\n\n\n409\nLongest Increasing Subsequence (DP)\nSubsequence optimization\n\n\n410\nEdit Distance (Levenshtein)\nMeasure similarity step-by-step\n\n\n\n\n\n42. Classic Problems (Knapsack, Subset Sum, Coin Change)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n411\n0/1 Knapsack\nValue maximization under capacity\n\n\n412\nSubset Sum\nBoolean feasibility DP\n\n\n413\nEqual Partition\nDivide set into equal halves\n\n\n414\nCount of Subsets with Sum\nCounting variant\n\n\n415\nTarget Sum\nDP with +/- transitions\n\n\n416\nUnbounded Knapsack\nReuse items\n\n\n417\nFractional Knapsack\nGreedy + DP comparison\n\n\n418\nCoin Change (Min Coins)\nDP shortest path\n\n\n419\nCoin Change (Count Ways)\nCombinatorial counting\n\n\n420\nMulti-Dimensional Knapsack\nCapacity in multiple dimensions\n\n\n\n\n\n43. Sequence Problems (LIS, LCS, Edit Distance)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n421\nLongest Increasing Subsequence\nO(n²) DP\n\n\n422\nLIS (Patience Sorting)\nO(n log n) optimized\n\n\n423\nLongest Common Subsequence\nTwo-sequence DP\n\n\n424\nEdit Distance (Levenshtein)\nTransform operations\n\n\n425\nLongest Palindromic Subsequence\nSymmetric DP\n\n\n426\nShortest Common Supersequence\nMerge sequences\n\n\n427\nLongest Repeated Subsequence\nDP with overlap\n\n\n428\nString Interleaving\nMerge with order preservation\n\n\n429\nSequence Alignment (Bioinformatics)\nGap penalties\n\n\n430\nDiff Algorithm (Myers/DP)\nMinimal edit path\n\n\n\n\n\n44. Matrix and Chain Problems\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n431\nMatrix Chain Multiplication\nParenthesization cost\n\n\n432\nBoolean Parenthesization\nCount true outcomes\n\n\n433\nBurst Balloons\nInterval DP\n\n\n434\nOptimal BST\nWeighted search cost\n\n\n435\nPolygon Triangulation\nDP over partitions\n\n\n436\nMatrix Path Sum\nDP on 2D grid\n\n\n437\nLargest Square Submatrix\nDynamic growth check\n\n\n438\nMax Rectangle in Binary Matrix\nHistogram + DP\n\n\n439\nSubmatrix Sum Queries\nPrefix sum DP\n\n\n440\nPalindrome Partitioning\nDP with cuts\n\n\n\n\n\n45. Bitmask DP and Traveling Salesman\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n441\nTraveling Salesman (TSP)\nVisit all cities\n\n\n442\nSubset DP\nOver subsets of states\n\n\n443\nHamiltonian Path DP\nState compression\n\n\n444\nAssignment Problem DP\nMask over tasks\n\n\n445\nPartition into Two Sets\nBalanced load\n\n\n446\nCount Hamiltonian Cycles\nBitmask enumeration\n\n\n447\nSteiner Tree DP\nMinimal connection of terminals\n\n\n448\nSOS DP (Sum Over Subsets)\nPrecompute sums\n\n\n449\nBitmask Knapsack\nState compression\n\n\n450\nBitmask Independent Set\nGraph subset optimization\n\n\n\n\n\n46. Digit DP and SOS DP\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n451\nCount Numbers with Property\nDigit-state transitions\n\n\n452\nCount Without Adjacent Duplicates\nAdjacent constraints\n\n\n453\nSum of Digits in Range\nCarry-dependent states\n\n\n454\nCount with Mod Condition\nDP over digit sum mod M\n\n\n455\nCount of Increasing Digits\nOrdered constraint\n\n\n456\nCount with Forbidden Digits\nExclusion transitions\n\n\n457\nSOS DP Subset Sum\nSum over bitmask subsets\n\n\n458\nSOS DP Superset Sum\nSum over supersets\n\n\n459\nXOR Basis DP\nCombine digit and bit DP\n\n\n460\nDigit DP for Palindromes\nSymmetric digit state\n\n\n\n\n\n47. DP Optimizations (Divide & Conquer, Convex Hull Trick, Knuth)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n461\nDivide & Conquer DP\nMonotone decision property\n\n\n462\nKnuth Optimization\nDP with quadrangle inequality\n\n\n463\nConvex Hull Trick\nLinear recurrence min queries\n\n\n464\nLi Chao Tree\nSegment-based hull maintenance\n\n\n465\nSlope Trick\nPiecewise-linear optimization\n\n\n466\nMonotonic Queue Optimization\nSliding DP state\n\n\n467\nBitset DP\nSpeed via bit-parallel\n\n\n468\nOffline DP Queries\nPreprocessing state\n\n\n469\nDP + Segment Tree\nRange-based optimization\n\n\n470\nDivide & Conquer Knapsack\nSplit-space DP\n\n\n\n\n\n48. Tree DP and Rerooting\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n471\nSubtree Sum DP\nAggregate values\n\n\n472\nDiameter DP\nMax path via child states\n\n\n473\nIndependent Set DP\nChoose or skip nodes\n\n\n474\nVertex Cover DP\nTree constraint problem\n\n\n475\nPath Counting DP\nCount root-leaf paths\n\n\n476\nDP on Rooted Tree\nBottom-up aggregation\n\n\n477\nRerooting Technique\nCompute for all roots\n\n\n478\nDistance Sum Rerooting\nEfficient recomputation\n\n\n479\nTree Coloring DP\nCombinatorial counting\n\n\n480\nBinary Search on Tree DP\nMonotonic transitions\n\n\n\n\n\n49. DP Reconstruction and Traceback\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n481\nReconstruct LCS\nBacktrack table\n\n\n482\nReconstruct LIS\nTrack predecessors\n\n\n483\nReconstruct Knapsack\nRecover selected items\n\n\n484\nEdit Distance Alignment\nTrace insert/delete/substitute\n\n\n485\nMatrix Chain Parentheses\nRebuild parenthesization\n\n\n486\nCoin Change Reconstruction\nBacktrack last used coin\n\n\n487\nPath Reconstruction DP\nTrace minimal route\n\n\n488\nSequence Reconstruction\nRebuild from states\n\n\n489\nMulti-Choice Reconstruction\nCombine best subpaths\n\n\n490\nTraceback Visualization\nVisual DP backtrack tool\n\n\n\n\n\n50. Meta-DP and Optimization Templates\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n491\nState Compression Template\nRepresent subsets compactly\n\n\n492\nTransition Optimization Template\nPrecompute transitions\n\n\n493\nSpace Optimization Template\nRolling arrays\n\n\n494\nMulti-Dimensional DP Template\nNested loops version\n\n\n495\nDecision Monotonicity\nOptimization hint\n\n\n496\nMonge Array Optimization\nMatrix property leverage\n\n\n497\nDivide & Conquer Template\nHalf-split recursion\n\n\n498\nRerooting Template\nGeneralized tree DP\n\n\n499\nIterative DP Pattern\nBottom-up unrolling\n\n\n500\nMemoization Template\nRecursive caching skeleton\n\n\n\n\n\n\nChapter 6. Mathematics for Algorithms\n\n51. Number Theory (GCD, Modular Arithmetic, CRT)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n501\nEuclidean Algorithm\nCompute gcd(a, b)\n\n\n502\nExtended Euclidean Algorithm\nSolve ax + by = gcd(a, b)\n\n\n503\nModular Addition\nAdd under modulo M\n\n\n504\nModular Multiplication\nMultiply under modulo M\n\n\n505\nModular Exponentiation\nFast power mod M\n\n\n506\nModular Inverse\nCompute a⁻¹ mod M\n\n\n507\nChinese Remainder Theorem\nCombine modular systems\n\n\n508\nBinary GCD (Stein’s Algorithm)\nBitwise gcd\n\n\n509\nModular Reduction\nNormalize residues\n\n\n510\nModular Linear Equation Solver\nSolve ax ≡ b (mod m)\n\n\n\n\n\n52. Primality and Factorization (Miller–Rabin, Pollard Rho)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n511\nTrial Division\nSimple prime test\n\n\n512\nSieve of Eratosthenes\nGenerate primes up to n\n\n\n513\nSieve of Atkin\nFaster sieve variant\n\n\n514\nMiller–Rabin Primality Test\nProbabilistic primality\n\n\n515\nFermat Primality Test\nModular power check\n\n\n516\nPollard’s Rho\nRandomized factorization\n\n\n517\nPollard’s p−1 Method\nFactor using smoothness\n\n\n518\nWheel Factorization\nSkip known composites\n\n\n519\nAKS Primality Test\nDeterministic polynomial test\n\n\n520\nSegmented Sieve\nPrime generation for large n\n\n\n\n\n\n53. Combinatorics (Permutations, Combinations, Subsets)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n521\nFactorial Precomputation\nBuild n! table\n\n\n522\nnCr Computation\nUse Pascal’s or factorials\n\n\n523\nPascal’s Triangle\nBinomial coefficients\n\n\n524\nMultiset Combination\nRepetition allowed\n\n\n525\nPermutation Generation\nLexicographic order\n\n\n526\nNext Permutation\nSTL-style increment\n\n\n527\nSubset Generation\nBitmask or recursion\n\n\n528\nGray Code Generation\nSingle-bit flips\n\n\n529\nCatalan Number DP\nCount valid parentheses\n\n\n530\nStirling Numbers\nPartition counting\n\n\n\n\n\n54. Probability and Randomized Algorithms\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n531\nMonte Carlo Simulation\nApproximate via randomness\n\n\n532\nLas Vegas Algorithm\nAlways correct, variable time\n\n\n533\nReservoir Sampling\nUniform sampling from stream\n\n\n534\nRandomized QuickSort\nExpected O(n log n)\n\n\n535\nRandomized QuickSelect\nRandom pivot\n\n\n536\nBirthday Paradox Simulation\nProbability collision\n\n\n537\nRandom Hashing\nReduce collision chance\n\n\n538\nRandom Walk Simulation\nState transitions\n\n\n539\nCoupon Collector Estimation\nExpected trials\n\n\n540\nMarkov Chain Simulation\nTransition matrix sampling\n\n\n\n\n\n55. Sieve Methods and Modular Math\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n541\nSieve of Eratosthenes\nBase prime sieve\n\n\n542\nLinear Sieve\nO(n) sieve variant\n\n\n543\nSegmented Sieve\nRange prime generation\n\n\n544\nSPF (Smallest Prime Factor) Table\nFactorization via sieve\n\n\n545\nMöbius Function Sieve\nMultiplicative function calc\n\n\n546\nEuler’s Totient Sieve\nCompute φ(n) for all n\n\n\n547\nDivisor Count Sieve\nCount divisors efficiently\n\n\n548\nModular Precomputation\nStore inverses, factorials\n\n\n549\nFermat Little Theorem\na^(p−1) ≡ 1 mod p\n\n\n550\nWilson’s Theorem\nPrime test via factorial mod p\n\n\n\n\n\n56. Linear Algebra (Gaussian Elimination, LU, SVD)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n551\nGaussian Elimination\nSolve Ax = b\n\n\n552\nGauss-Jordan Elimination\nReduced row echelon\n\n\n553\nLU Decomposition\nFactor A into L·U\n\n\n554\nCholesky Decomposition\nA = L·Lᵀ for SPD\n\n\n555\nQR Decomposition\nOrthogonal factorization\n\n\n556\nMatrix Inversion (Gauss-Jordan)\nFind A⁻¹\n\n\n557\nDeterminant by Elimination\nProduct of pivots\n\n\n558\nRank of Matrix\nCount non-zero rows\n\n\n559\nEigenvalue Power Method\nApproximate dominant eigenvalue\n\n\n560\nSingular Value Decomposition\nA = UΣVᵀ\n\n\n\n\n\n57. FFT and NTT (Fast Transforms)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n561\nDiscrete Fourier Transform (DFT)\nO(n²) baseline\n\n\n562\nFast Fourier Transform (FFT)\nO(n log n) convolution\n\n\n563\nCooley–Tukey FFT\nRecursive divide and conquer\n\n\n564\nIterative FFT\nIn-place bit reversal\n\n\n565\nInverse FFT\nRecover time-domain\n\n\n566\nConvolution via FFT\nPolynomial multiplication\n\n\n567\nNumber Theoretic Transform (NTT)\nModulo prime FFT\n\n\n568\nInverse NTT\nModular inverse transform\n\n\n569\nBluestein’s Algorithm\nFFT of arbitrary size\n\n\n570\nFFT-Based Multiplication\nBig integer product\n\n\n\n\n\n58. Numerical Methods (Newton, Simpson, Runge–Kutta)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n571\nNewton–Raphson\nRoot finding via tangent\n\n\n572\nBisection Method\nInterval halving\n\n\n573\nSecant Method\nApproximate derivative\n\n\n574\nFixed-Point Iteration\nx = f(x) convergence\n\n\n575\nGaussian Quadrature\nWeighted integration\n\n\n576\nSimpson’s Rule\nPiecewise quadratic integral\n\n\n577\nTrapezoidal Rule\nLinear interpolation integral\n\n\n578\nRunge–Kutta (RK4)\nODE solver\n\n\n579\nEuler’s Method\nStep-by-step ODE\n\n\n580\nGradient Descent (1D)\nNumerical optimization\n\n\n\n\n\n59. Mathematical Optimization (Simplex, Gradient, Convex)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n581\nSimplex Method\nLinear programming solver\n\n\n582\nDual Simplex Method\nSolve dual constraints\n\n\n583\nInterior-Point Method\nConvex optimization\n\n\n584\nGradient Descent\nUnconstrained optimization\n\n\n585\nStochastic Gradient Descent\nSample-based updates\n\n\n586\nNewton’s Method (Multivariate)\nQuadratic convergence\n\n\n587\nConjugate Gradient\nSolve SPD systems\n\n\n588\nLagrange Multipliers\nConstrained optimization\n\n\n589\nKKT Conditions Solver\nConvex constraint handling\n\n\n590\nCoordinate Descent\nSequential variable updates\n\n\n\n\n\n60. Algebraic Tricks and Transform Techniques\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n591\nPolynomial Multiplication (FFT)\nFast convolution\n\n\n592\nPolynomial Inversion\nNewton iteration\n\n\n593\nPolynomial Derivative\nTerm-wise multiply by index\n\n\n594\nPolynomial Integration\nDivide by index+1\n\n\n595\nFormal Power Series Composition\nSubstitute series\n\n\n596\nExponentiation by Squaring\nFast powering\n\n\n597\nModular Exponentiation\nFast power mod M\n\n\n598\nFast Walsh–Hadamard Transform\nXOR convolution\n\n\n599\nZeta Transform\nSubset summation\n\n\n600\nMöbius Inversion\nRecover original from sums\n\n\n\n\n\n\nChapter 7. Strings and Text Algorithms\n\n61. String Matching (KMP, Z, Rabin–Karp, Boyer–Moore)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n601\nNaive String Matching\nCompare every position\n\n\n602\nKnuth–Morris–Pratt (KMP)\nPrefix function skipping\n\n\n603\nZ-Algorithm\nMatch using Z-values\n\n\n604\nRabin–Karp\nRolling hash comparison\n\n\n605\nBoyer–Moore\nBackward skip based on mismatch\n\n\n606\nBoyer–Moore–Horspool\nSimplified shift table\n\n\n607\nSunday Algorithm\nLast-character shift\n\n\n608\nFinite Automaton Matching\nDFA-based matching\n\n\n609\nBitap Algorithm\nBitmask approximate matching\n\n\n610\nTwo-Way Algorithm\nOptimal linear matching\n\n\n\n\n\n62. Multi-Pattern Search (Aho–Corasick)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n611\nAho–Corasick Automaton\nTrie + failure links\n\n\n612\nTrie Construction\nPrefix tree build\n\n\n613\nFailure Link Computation\nBFS for transitions\n\n\n614\nOutput Link Management\nHandle overlapping patterns\n\n\n615\nMulti-Pattern Search\nFind all keywords\n\n\n616\nDictionary Matching\nFind multiple substrings\n\n\n617\nDynamic Aho–Corasick\nAdd/remove patterns\n\n\n618\nParallel AC Search\nMulti-threaded traversal\n\n\n619\nCompressed AC Automaton\nMemory-optimized\n\n\n620\nExtended AC with Wildcards\nFlexible matching\n\n\n\n\n\n63. Suffix Structures (Suffix Array, Suffix Tree, LCP)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n621\nSuffix Array (Naive)\nSort all suffixes\n\n\n622\nSuffix Array (Doubling)\nO(n log n) rank-based\n\n\n623\nKasai’s LCP Algorithm\nLongest common prefix\n\n\n624\nSuffix Tree (Ukkonen)\nLinear-time online\n\n\n625\nSuffix Automaton\nMinimal DFA of substrings\n\n\n626\nSA-IS Algorithm\nO(n) suffix array\n\n\n627\nLCP RMQ Query\nRange minimum for substring\n\n\n628\nGeneralized Suffix Array\nMultiple strings\n\n\n629\nEnhanced Suffix Array\nCombine SA + LCP\n\n\n630\nSparse Suffix Tree\nSpace-efficient variant\n\n\n\n\n\n64. Palindromes and Periodicity (Manacher)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n631\nNaive Palindrome Check\nExpand around center\n\n\n632\nManacher’s Algorithm\nO(n) longest palindrome\n\n\n633\nLongest Palindromic Substring\nCenter expansion\n\n\n634\nPalindrome DP Table\nSubstring boolean matrix\n\n\n635\nPalindromic Tree (Eertree)\nTrack distinct palindromes\n\n\n636\nPrefix Function Periodicity\nDetect repetition patterns\n\n\n637\nZ-Function Periodicity\nIdentify periodic suffix\n\n\n638\nKMP Prefix Period Check\nShortest repeating unit\n\n\n639\nLyndon Factorization\nDecompose string into Lyndon words\n\n\n640\nMinimal Rotation (Booth’s Algorithm)\nLexicographically minimal shift\n\n\n\n\n\n65. Edit Distance and Alignment\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n641\nLevenshtein Distance\nInsert/delete/replace cost\n\n\n642\nDamerau–Levenshtein\nSwap included\n\n\n643\nHamming Distance\nCount differing bits\n\n\n644\nNeedleman–Wunsch\nGlobal alignment\n\n\n645\nSmith–Waterman\nLocal alignment\n\n\n646\nHirschberg’s Algorithm\nMemory-optimized alignment\n\n\n647\nEdit Script Reconstruction\nBacktrack operations\n\n\n648\nAffine Gap Penalty DP\nVarying gap cost\n\n\n649\nMyers Bit-Vector Algorithm\nFast edit distance\n\n\n650\nLongest Common Subsequence\nAlignment by inclusion\n\n\n\n\n\n66. Compression (Huffman, Arithmetic, LZ77, BWT)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n651\nHuffman Coding\nOptimal prefix tree\n\n\n652\nCanonical Huffman\nDeterministic ordering\n\n\n653\nArithmetic Coding\nInterval probability coding\n\n\n654\nShannon–Fano Coding\nEarly prefix method\n\n\n655\nRun-Length Encoding (RLE)\nRepeat compression\n\n\n656\nLZ77\nSliding-window match\n\n\n657\nLZ78\nDictionary building\n\n\n658\nLZW\nVariant used in GIF\n\n\n659\nBurrows–Wheeler Transform\nBlock reordering\n\n\n660\nMove-to-Front Encoding\nLocality boosting transform\n\n\n\n\n\n67. Cryptographic Hashes and Checksums\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n661\nRolling Hash\nPolynomial mod-based\n\n\n662\nCRC32\nCyclic redundancy check\n\n\n663\nAdler-32\nLightweight checksum\n\n\n664\nMD5\nLegacy cryptographic hash\n\n\n665\nSHA-1\nDeprecated hash function\n\n\n666\nSHA-256\nSecure hash standard\n\n\n667\nSHA-3 (Keccak)\nSponge construction\n\n\n668\nHMAC\nKeyed message authentication\n\n\n669\nMerkle Tree\nHierarchical hashing\n\n\n670\nHash Collision Detection\nBirthday bound simulation\n\n\n\n\n\n68. Approximate and Streaming Matching\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n671\nK-Approximate Matching\nAllow k mismatches\n\n\n672\nBitap Algorithm\nBitwise dynamic programming\n\n\n673\nLandau–Vishkin Algorithm\nEdit distance ≤ k\n\n\n674\nFiltering Algorithm\nFast approximate search\n\n\n675\nWu–Manber\nMulti-pattern approximate search\n\n\n676\nStreaming KMP\nOnline prefix updates\n\n\n677\nRolling Hash Sketch\nSliding window hashing\n\n\n678\nSketch-based Similarity\nMinHash / LSH variants\n\n\n679\nWeighted Edit Distance\nWeighted operations\n\n\n680\nOnline Levenshtein\nDynamic stream update\n\n\n\n\n\n69. Bioinformatics Alignment (Needleman–Wunsch, Smith–Waterman)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n681\nNeedleman–Wunsch\nGlobal sequence alignment\n\n\n682\nSmith–Waterman\nLocal alignment\n\n\n683\nGotoh Algorithm\nAffine gap penalties\n\n\n684\nHirschberg Alignment\nLinear-space alignment\n\n\n685\nMultiple Sequence Alignment (MSA)\nProgressive methods\n\n\n686\nProfile Alignment\nAlign sequence to profile\n\n\n687\nHidden Markov Model Alignment\nProbabilistic alignment\n\n\n688\nBLAST\nHeuristic local search\n\n\n689\nFASTA\nWord-based alignment\n\n\n690\nPairwise DP Alignment\nGeneral DP framework\n\n\n\n\n\n70. Text Indexing and Search Structures\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n691\nInverted Index Build\nWord-to-document mapping\n\n\n692\nPositional Index\nStore word positions\n\n\n693\nTF-IDF Weighting\nImportance scoring\n\n\n694\nBM25 Ranking\nModern ranking formula\n\n\n695\nTrie Index\nPrefix search structure\n\n\n696\nSuffix Array Index\nSubstring search\n\n\n697\nCompressed Suffix Array\nSpace-optimized\n\n\n698\nFM-Index\nBWT-based compressed index\n\n\n699\nDAWG (Directed Acyclic Word Graph)\nShared suffix graph\n\n\n700\nWavelet Tree for Text\nRank/select on sequences\n\n\n\n\n\n\nChapter 8. Geometry, Graphics, and Spatial Algorithms\n\n71. Convex Hull (Graham, Andrew, Chan)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n701\nGift Wrapping (Jarvis March)\nWrap hull one point at a time\n\n\n702\nGraham Scan\nSort by angle, maintain stack\n\n\n703\nAndrew’s Monotone Chain\nSort by x, upper + lower hull\n\n\n704\nChan’s Algorithm\nOutput-sensitive O(n log h)\n\n\n705\nQuickHull\nDivide-and-conquer hull\n\n\n706\nIncremental Convex Hull\nAdd points one by one\n\n\n707\nDivide & Conquer Hull\nMerge two partial hulls\n\n\n708\n3D Convex Hull\nExtend to 3D geometry\n\n\n709\nDynamic Convex Hull\nMaintain hull with inserts\n\n\n710\nRotating Calipers\nCompute diameter, width, antipodal pairs\n\n\n\n\n\n72. Closest Pair and Segment Intersection\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n711\nClosest Pair (Divide & Conquer)\nSplit, merge minimal distance\n\n\n712\nClosest Pair (Sweep Line)\nMaintain active window\n\n\n713\nBrute Force Closest Pair\nCheck all O(n²) pairs\n\n\n714\nBentley–Ottmann\nFind all line intersections\n\n\n715\nSegment Intersection Test\nCross product orientation\n\n\n716\nLine Sweep for Segments\nEvent-based intersection\n\n\n717\nIntersection via Orientation\nCCW test\n\n\n718\nCircle Intersection\nGeometry of two circles\n\n\n719\nPolygon Intersection\nClip overlapping polygons\n\n\n720\nNearest Neighbor Pair\nCombine KD-tree + search\n\n\n\n\n\n73. Line Sweep and Plane Sweep Algorithms\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n721\nSweep Line for Events\nProcess sorted events\n\n\n722\nInterval Scheduling\nSelect non-overlapping intervals\n\n\n723\nRectangle Union Area\nSweep edges to count area\n\n\n724\nSegment Intersection (Bentley–Ottmann)\nDetect all crossings\n\n\n725\nSkyline Problem\nMerge height profiles\n\n\n726\nClosest Pair Sweep\nMaintain active set\n\n\n727\nCircle Arrangement\nSweep and count regions\n\n\n728\nSweep for Overlapping Rectangles\nDetect collisions\n\n\n729\nRange Counting\nCount points in rectangle\n\n\n730\nPlane Sweep for Triangles\nPolygon overlay computation\n\n\n\n\n\n74. Delaunay and Voronoi Diagrams\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n731\nDelaunay Triangulation (Incremental)\nAdd points, maintain Delaunay\n\n\n732\nDelaunay (Divide & Conquer)\nMerge triangulations\n\n\n733\nDelaunay (Fortune’s Sweep)\nO(n log n) construction\n\n\n734\nVoronoi Diagram (Fortune’s)\nSweep line beachline\n\n\n735\nIncremental Voronoi\nUpdate on insertion\n\n\n736\nBowyer–Watson\nEmpty circle criterion\n\n\n737\nDuality Transform\nConvert between Voronoi/Delaunay\n\n\n738\nPower Diagram\nWeighted Voronoi\n\n\n739\nLloyd’s Relaxation\nSmooth Voronoi cells\n\n\n740\nVoronoi Nearest Neighbor\nRegion-based lookup\n\n\n\n\n\n75. Point in Polygon and Polygon Triangulation\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n741\nRay Casting\nCount edge crossings\n\n\n742\nWinding Number\nAngle sum method\n\n\n743\nConvex Polygon Point Test\nOrientation checks\n\n\n744\nEar Clipping Triangulation\nRemove ears iteratively\n\n\n745\nMonotone Polygon Triangulation\nSweep line triangulation\n\n\n746\nDelaunay Triangulation\nOptimal triangle quality\n\n\n747\nConvex Decomposition\nSplit into convex parts\n\n\n748\nPolygon Area (Shoelace Formula)\nSigned area computation\n\n\n749\nMinkowski Sum\nAdd shapes geometrically\n\n\n750\nPolygon Intersection (Weiler–Atherton)\nClip overlapping shapes\n\n\n\n\n\n76. Spatial Data Structures (KD, R-tree)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n751\nKD-Tree Build\nRecursive median split\n\n\n752\nKD-Tree Search\nAxis-aligned query\n\n\n753\nRange Search KD-Tree\nOrthogonal query\n\n\n754\nNearest Neighbor KD-Tree\nClosest point search\n\n\n755\nR-Tree Build\nBounding box hierarchy\n\n\n756\nR*-Tree\nOptimized split strategy\n\n\n757\nQuad Tree\nSpatial decomposition\n\n\n758\nOctree\n3D spatial decomposition\n\n\n759\nBSP Tree (Binary Space Partition)\nSplit by planes\n\n\n760\nMorton Order (Z-Curve)\nSpatial locality index\n\n\n\n\n\n77. Rasterization and Scanline Techniques\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n761\nBresenham’s Line Algorithm\nEfficient integer drawing\n\n\n762\nMidpoint Circle Algorithm\nCircle rasterization\n\n\n763\nScanline Fill\nPolygon interior fill\n\n\n764\nEdge Table Fill\nSort edges by y\n\n\n765\nZ-Buffer Algorithm\nHidden surface removal\n\n\n766\nPainter’s Algorithm\nSort by depth\n\n\n767\nGouraud Shading\nVertex interpolation shading\n\n\n768\nPhong Shading\nNormal interpolation\n\n\n769\nAnti-Aliasing (Supersampling)\nSmooth jagged edges\n\n\n770\nScanline Polygon Clipping\nEfficient clipping\n\n\n\n\n\n78. Computer Vision (Canny, Hough, SIFT)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n771\nCanny Edge Detector\nGradient + hysteresis\n\n\n772\nSobel Operator\nGradient magnitude filter\n\n\n773\nHough Transform (Lines)\nAccumulator for line detection\n\n\n774\nHough Transform (Circles)\nRadius-based accumulator\n\n\n775\nHarris Corner Detector\nEigenvalue-based corners\n\n\n776\nFAST Corner Detector\nIntensity circle test\n\n\n777\nSIFT (Scale-Invariant Feature Transform)\nKeypoint detection\n\n\n778\nSURF (Speeded-Up Robust Features)\nFaster descriptor\n\n\n779\nORB (Oriented FAST + BRIEF)\nBinary robust feature\n\n\n780\nRANSAC\nRobust model fitting\n\n\n\n\n\n79. Pathfinding in Space (A*, RRT, PRM)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n781\nA* Search\nHeuristic pathfinding\n\n\n782\nDijkstra for Grid\nWeighted shortest path\n\n\n783\nTheta*\nAny-angle pathfinding\n\n\n784\nJump Point Search\nGrid acceleration\n\n\n785\nRRT (Rapidly-Exploring Random Tree)\nRandom sampling tree\n\n\n786\nRRT*\nOptimal variant with rewiring\n\n\n787\nPRM (Probabilistic Roadmap)\nGraph sampling planner\n\n\n788\nVisibility Graph\nConnect visible vertices\n\n\n789\nPotential Field Pathfinding\nGradient-based navigation\n\n\n790\nBug Algorithms\nSimple obstacle avoidance\n\n\n\n\n\n80. Computational Geometry Variants and Applications\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n791\nConvex Polygon Intersection\nClip convex sets\n\n\n792\nMinkowski Sum\nShape convolution\n\n\n793\nRotating Calipers\nClosest/farthest pair\n\n\n794\nHalf-Plane Intersection\nFeasible region\n\n\n795\nLine Arrangement\nCount regions\n\n\n796\nPoint Location (Trapezoidal Map)\nQuery region lookup\n\n\n797\nVoronoi Nearest Facility\nRegion query\n\n\n798\nDelaunay Mesh Generation\nTriangulation refinement\n\n\n799\nSmallest Enclosing Circle\nWelzl’s algorithm\n\n\n800\nCollision Detection (SAT)\nSeparating axis theorem\n\n\n\n\n\n\nChapter 9. Systems, Databases, and Distributed Algorithms\n\n81. Concurrency Control (2PL, MVCC, OCC)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n801\nTwo-Phase Locking (2PL)\nAcquire-then-release locks\n\n\n802\nStrict 2PL\nHold locks until commit\n\n\n803\nConservative 2PL\nPrevent deadlocks via prelock\n\n\n804\nTimestamp Ordering\nSchedule by timestamps\n\n\n805\nMultiversion Concurrency Control (MVCC)\nSnapshot isolation\n\n\n806\nOptimistic Concurrency Control (OCC)\nValidate at commit\n\n\n807\nSerializable Snapshot Isolation\nMerge read/write sets\n\n\n808\nLock-Free Algorithm\nAtomic CAS updates\n\n\n809\nWait-Die / Wound-Wait\nDeadlock prevention policies\n\n\n810\nDeadlock Detection (Wait-for Graph)\nCycle detection in waits\n\n\n\n\n\n82. Logging, Recovery, and Commit Protocols\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n811\nWrite-Ahead Logging (WAL)\nLog before commit\n\n\n812\nARIES Recovery\nRe-do/undo with LSNs\n\n\n813\nShadow Paging\nCopy-on-write persistence\n\n\n814\nTwo-Phase Commit (2PC)\nCoordinator-driven commit\n\n\n815\nThree-Phase Commit (3PC)\nNon-blocking variant\n\n\n816\nCheckpointing\nSave state for recovery\n\n\n817\nUndo Logging\nRollback uncommitted\n\n\n818\nRedo Logging\nReapply committed\n\n\n819\nQuorum Commit\nMajority agreement\n\n\n820\nConsensus Commit\nCombine 2PC + Paxos\n\n\n\n\n\n83. Scheduling (Round Robin, EDF, Rate-Monotonic)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n821\nFirst-Come First-Served (FCFS)\nSequential job order\n\n\n822\nShortest Job First (SJF)\nOptimal average wait\n\n\n823\nRound Robin (RR)\nTime-slice fairness\n\n\n824\nPriority Scheduling\nWeighted selection\n\n\n825\nMultilevel Queue\nTiered priority queues\n\n\n826\nEarliest Deadline First (EDF)\nReal-time optimal\n\n\n827\nRate Monotonic Scheduling (RMS)\nFixed periodic priority\n\n\n828\nLottery Scheduling\nProbabilistic fairness\n\n\n829\nMultilevel Feedback Queue\nAdaptive behavior\n\n\n830\nFair Queuing (FQ)\nFlow-based proportional sharing\n\n\n\n\n\n84. Caching and Replacement (LRU, LFU, CLOCK)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n831\nLRU (Least Recently Used)\nEvict oldest used\n\n\n832\nLFU (Least Frequently Used)\nEvict lowest frequency\n\n\n833\nFIFO Cache\nSimple queue eviction\n\n\n834\nCLOCK Algorithm\nApproximate LRU\n\n\n835\nARC (Adaptive Replacement Cache)\nMix of recency + frequency\n\n\n836\nTwo-Queue (2Q)\nSeparate recent/frequent\n\n\n837\nLIRS (Low Inter-reference Recency Set)\nPredict reuse distance\n\n\n838\nTinyLFU\nFrequency sketch admission\n\n\n839\nRandom Replacement\nSimple stochastic policy\n\n\n840\nBelady’s Optimal\nEvict farthest future use\n\n\n\n\n\n85. Networking (Routing, Congestion Control)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n841\nDijkstra’s Routing\nShortest path routing\n\n\n842\nBellman–Ford Routing\nDistance-vector routing\n\n\n843\nLink-State Routing (OSPF)\nGlobal view routing\n\n\n844\nDistance-Vector Routing (RIP)\nLocal neighbor updates\n\n\n845\nPath Vector (BGP)\nRoute advertisement\n\n\n846\nFlooding\nBroadcast to all nodes\n\n\n847\nSpanning Tree Protocol\nLoop-free topology\n\n\n848\nCongestion Control (AIMD)\nTCP window control\n\n\n849\nRandom Early Detection (RED)\nQueue preemptive drop\n\n\n850\nECN (Explicit Congestion Notification)\nMark packets early\n\n\n\n\n\n86. Distributed Consensus (Paxos, Raft, PBFT)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n851\nBasic Paxos\nMajority consensus\n\n\n852\nMulti-Paxos\nSequence of agreements\n\n\n853\nRaft\nLog replication + leader election\n\n\n854\nViewstamped Replication\nAlternative consensus design\n\n\n855\nPBFT (Practical Byzantine Fault Tolerance)\nByzantine safety\n\n\n856\nZab (Zookeeper Atomic Broadcast)\nBroadcast + ordering\n\n\n857\nEPaxos\nLeaderless fast path\n\n\n858\nVRR (Virtual Ring Replication)\nLog around ring\n\n\n859\nTwo-Phase Commit with Consensus\nTransactional commit\n\n\n860\nChain Replication\nOrdered state replication\n\n\n\n\n\n87. Load Balancing and Rate Limiting\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n861\nRound Robin Load Balancing\nSequential distribution\n\n\n862\nWeighted Round Robin\nProportional to weight\n\n\n863\nLeast Connections\nPick least loaded node\n\n\n864\nConsistent Hashing\nMap requests stably\n\n\n865\nPower of Two Choices\nSample and choose lesser load\n\n\n866\nRandom Load Balancing\nSimple uniform random\n\n\n867\nToken Bucket\nRate-based limiter\n\n\n868\nLeaky Bucket\nSteady flow shaping\n\n\n869\nSliding Window Counter\nRolling time window\n\n\n870\nFixed Window Counter\nResettable counter limiter\n\n\n\n\n\n88. Search and Indexing (Inverted, BM25, WAND)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n871\nInverted Index Construction\nWord → document list\n\n\n872\nPositional Index Build\nStore term positions\n\n\n873\nTF-IDF Scoring\nTerm frequency weighting\n\n\n874\nBM25 Ranking\nModern scoring model\n\n\n875\nBoolean Retrieval\nLogical AND/OR/NOT\n\n\n876\nWAND Algorithm\nEfficient top-k retrieval\n\n\n877\nBlock-Max WAND (BMW)\nEarly skipping optimization\n\n\n878\nImpact-Ordered Indexing\nSort by contribution\n\n\n879\nTiered Indexing\nPrioritize high-score docs\n\n\n880\nDAAT vs SAAT Evaluation\nDocument vs score-at-a-time\n\n\n\n\n\n89. Compression and Encoding in Systems\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n881\nRun-Length Encoding (RLE)\nSimple repetition encoding\n\n\n882\nHuffman Coding\nOptimal variable-length code\n\n\n883\nArithmetic Coding\nFractional interval coding\n\n\n884\nDelta Encoding\nStore differences\n\n\n885\nVariable Byte Encoding\nCompact integers\n\n\n886\nElias Gamma Coding\nPrefix integer encoding\n\n\n887\nRice Coding\nUnary + remainder scheme\n\n\n888\nSnappy\nFast block compression\n\n\n889\nZstandard (Zstd)\nModern adaptive codec\n\n\n890\nLZ4\nHigh-speed dictionary compressor\n\n\n\n\n\n90. Fault Tolerance and Replication\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n891\nPrimary–Backup Replication\nOne leader, one standby\n\n\n892\nQuorum Replication\nMajority write/read rule\n\n\n893\nChain Replication\nOrdered consistency\n\n\n894\nGossip Protocol\nEpidemic state exchange\n\n\n895\nAnti-Entropy Repair\nPeriodic reconciliation\n\n\n896\nErasure Coding\nRedundant data blocks\n\n\n897\nChecksum Verification\nDetect corruption\n\n\n898\nHeartbeat Monitoring\nLiveness detection\n\n\n899\nLeader Election (Bully)\nHighest ID wins\n\n\n900\nLeader Election (Ring)\nToken-based rotation\n\n\n\n\n\n\nChapter 10. AI, ML, and Optimization\n\n91. Classical ML (k-means, Naive Bayes, SVM, Decision Trees)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n901\nk-Means Clustering\nPartition by centroid iteration\n\n\n902\nk-Medoids (PAM)\nCluster by exemplars\n\n\n903\nGaussian Mixture Model (EM)\nSoft probabilistic clustering\n\n\n904\nNaive Bayes Classifier\nProbabilistic feature independence\n\n\n905\nLogistic Regression\nSigmoid linear classifier\n\n\n906\nPerceptron\nOnline linear separator\n\n\n907\nDecision Tree (CART)\nRecursive partition by impurity\n\n\n908\nID3 Algorithm\nInformation gain splitting\n\n\n909\nk-Nearest Neighbors (kNN)\nDistance-based classification\n\n\n910\nLinear Discriminant Analysis (LDA)\nProjection for separation\n\n\n\n\n\n92. Ensemble Methods (Bagging, Boosting, Random Forests)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n911\nBagging\nBootstrap aggregation\n\n\n912\nRandom Forest\nEnsemble of decision trees\n\n\n913\nAdaBoost\nWeighted error correction\n\n\n914\nGradient Boosting\nSequential residual fitting\n\n\n915\nXGBoost\nOptimized gradient boosting\n\n\n916\nLightGBM\nHistogram-based leaf growth\n\n\n917\nCatBoost\nOrdered boosting for categoricals\n\n\n918\nStacking\nMeta-model ensemble\n\n\n919\nVoting Classifier\nMajority aggregation\n\n\n920\nSnapshot Ensemble\nAveraged checkpoints\n\n\n\n\n\n93. Gradient Methods (SGD, Adam, RMSProp)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n921\nGradient Descent\nBatch full-gradient step\n\n\n922\nStochastic Gradient Descent (SGD)\nSample-wise updates\n\n\n923\nMini-Batch SGD\nTradeoff speed and variance\n\n\n924\nMomentum\nAdd velocity to descent\n\n\n925\nNesterov Accelerated Gradient\nLookahead correction\n\n\n926\nAdaGrad\nAdaptive per-parameter rate\n\n\n927\nRMSProp\nExponential moving average\n\n\n928\nAdam\nMomentum + adaptive rate\n\n\n929\nAdamW\nDecoupled weight decay\n\n\n930\nL-BFGS\nLimited-memory quasi-Newton\n\n\n\n\n\n94. Deep Learning (Backpropagation, Dropout, Normalization)\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n931\nBackpropagation\nGradient chain rule\n\n\n932\nXavier/He Initialization\nScaled variance init\n\n\n933\nDropout\nRandom neuron deactivation\n\n\n934\nBatch Normalization\nNormalize per batch\n\n\n935\nLayer Normalization\nNormalize per feature\n\n\n936\nGradient Clipping\nPrevent explosion\n\n\n937\nEarly Stopping\nPrevent overfitting\n\n\n938\nWeight Decay\nRegularization via penalty\n\n\n939\nLearning Rate Scheduling\nDynamic LR adjustment\n\n\n940\nResidual Connections\nSkip layer improvement\n\n\n\n\n\n95. Sequence Models (Viterbi, Beam Search, CTC)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n941\nHidden Markov Model (Forward–Backward)\nProbabilistic sequence model\n\n\n942\nViterbi Algorithm\nMost probable path\n\n\n943\nBaum–Welch\nEM training for HMMs\n\n\n944\nBeam Search\nTop-k path exploration\n\n\n945\nGreedy Decoding\nFast approximate decoding\n\n\n946\nConnectionist Temporal Classification (CTC)\nUnaligned sequence training\n\n\n947\nAttention Mechanism\nWeighted context aggregation\n\n\n948\nTransformer Decoder\nSelf-attention stack\n\n\n949\nSeq2Seq with Attention\nEncoder-decoder framework\n\n\n950\nPointer Network\nOutput index selection\n\n\n\n\n\n96. Metaheuristics (GA, SA, PSO, ACO)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n951\nGenetic Algorithm (GA)\nEvolutionary optimization\n\n\n952\nSimulated Annealing (SA)\nTemperature-controlled search\n\n\n953\nTabu Search\nMemory of forbidden moves\n\n\n954\nParticle Swarm Optimization (PSO)\nVelocity-based search\n\n\n955\nAnt Colony Optimization (ACO)\nPheromone-guided path\n\n\n956\nDifferential Evolution (DE)\nVector-based mutation\n\n\n957\nHarmony Search\nMusic-inspired improvisation\n\n\n958\nFirefly Algorithm\nBrightness-attraction movement\n\n\n959\nBee Colony Optimization\nExplore-exploit via scouts\n\n\n960\nHill Climbing\nLocal incremental improvement\n\n\n\n\n\n97. Reinforcement Learning (Q-learning, Policy Gradients)\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n961\nMonte Carlo Control\nAverage returns\n\n\n962\nTemporal Difference (TD) Learning\nBootstrap updates\n\n\n963\nSARSA\nOn-policy TD learning\n\n\n964\nQ-Learning\nOff-policy TD learning\n\n\n965\nDouble Q-Learning\nReduce overestimation\n\n\n966\nDeep Q-Network (DQN)\nNeural Q approximator\n\n\n967\nREINFORCE\nPolicy gradient by sampling\n\n\n968\nActor–Critic\nValue-guided policy update\n\n\n969\nPPO (Proximal Policy Optimization)\nClipped surrogate objective\n\n\n970\nDDPG / SAC\nContinuous action RL\n\n\n\n\n\n98. Approximation and Online Algorithms\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n971\nGreedy Set Cover\nln(n)-approximation\n\n\n972\nVertex Cover Approximation\nDouble-matching heuristic\n\n\n973\nTraveling Salesman Approximation\nMST-based 2-approx\n\n\n974\nk-Center Approximation\nFarthest-point heuristic\n\n\n975\nOnline Paging (LRU)\nCompetitive analysis\n\n\n976\nOnline Matching (Ranking)\nAdversarial input resilience\n\n\n977\nOnline Knapsack\nRatio-based acceptance\n\n\n978\nCompetitive Ratio Evaluation\nBound worst-case performance\n\n\n979\nPTAS / FPTAS Schemes\nPolynomial approximation\n\n\n980\nPrimal–Dual Method\nApproximate combinatorial optimization\n\n\n\n\n\n99. Fairness, Causal Inference, and Robust Optimization\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n981\nReweighting for Fairness\nAdjust sample weights\n\n\n982\nDemographic Parity Constraint\nEqualize positive rates\n\n\n983\nEqualized Odds\nAlign error rates\n\n\n984\nAdversarial Debiasing\nLearn fair representations\n\n\n985\nCausal DAG Discovery\nGraphical causal inference\n\n\n986\nPropensity Score Matching\nEstimate treatment effect\n\n\n987\nInstrumental Variable Estimation\nHandle confounders\n\n\n988\nRobust Optimization\nWorst-case aware optimization\n\n\n989\nDistributionally Robust Optimization\nMinimax over uncertainty sets\n\n\n990\nCounterfactual Fairness\nSimulate do-interventions\n\n\n\n\n\n100. AI Planning, Search, and Learning Systems\n\n\n\n\n\n\n\n\n#\nAlgorithm\nNote\n\n\n\n\n991\nBreadth-First Search (BFS)\nUninformed search\n\n\n992\nDepth-First Search (DFS)\nBacktracking search\n\n\n993\nA* Search\nHeuristic guided\n\n\n994\nIterative Deepening A* (IDA*)\nMemory-bounded heuristic\n\n\n995\nUniform Cost Search\nExpand by path cost\n\n\n996\nMonte Carlo Tree Search (MCTS)\nExploration vs exploitation\n\n\n997\nMinimax\nGame tree evaluation\n\n\n998\nAlpha–Beta Pruning\nPrune unneeded branches\n\n\n999\nSTRIPS Planning\nAction-based state transition\n\n\n1000\nHierarchical Task Network (HTN)\nStructured AI planning",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Plan</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html",
    "href": "books/en-us/list-1.html",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "",
    "text": "Section 1. What is an algorithms?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-1.-what-is-an-algorithms",
    "href": "books/en-us/list-1.html#section-1.-what-is-an-algorithms",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "",
    "text": "1 Euclid’s GCD\nEuclid’s algorithm is one of the oldest and most elegant procedures in mathematics. It computes the greatest common divisor (GCD) of two integers by repeatedly applying a simple rule: replace the larger number with its remainder when divided by the smaller. When the remainder becomes zero, the smaller number at that step is the GCD.\n\nWhat Problem Are We Solving?\nWe want the greatest common divisor of two integers \\(a\\) and \\(b\\): the largest number that divides both without a remainder.\nA naive way would be to check all numbers from \\(\\min(a,b)\\) down to 1. That’s \\(O(\\min(a,b))\\) steps, which is too slow for large inputs. Euclid’s insight gives a much faster recursive method using division:\n\\[\n\\gcd(a, b) =\n\\begin{cases}\na, & \\text{if } b = 0, \\\\\n\\gcd(b, a \\bmod b), & \\text{otherwise.}\n\\end{cases}\n\\]\n\n\nHow It Works (Plain Language)\nImagine two sticks of lengths \\(a\\) and \\(b\\). You can keep cutting the longer stick by the shorter one until one divides evenly. The length of the last nonzero remainder is the GCD.\nSteps:\n\nTake \\(a, b\\) with \\(a \\ge b\\).\nReplace \\(a\\) by \\(b\\), and \\(b\\) by \\(a \\bmod b\\).\nRepeat until \\(b = 0\\).\nReturn \\(a\\).\n\nThis process always terminates, since \\(b\\) strictly decreases each step.\n\n\nExample Step by Step\nFind \\(\\gcd(48, 18)\\):\n\n\n\nStep\n\\(a\\)\n\\(b\\)\n\\(a \\bmod b\\)\n\n\n\n\n1\n48\n18\n12\n\n\n2\n18\n12\n6\n\n\n3\n12\n6\n0\n\n\n\nWhen \\(b = 0\\), \\(a = 6\\). So \\(\\gcd(48, 18) = 6\\).\n\n\nTiny Code (Python)\ndef gcd(a, b):\n    while b != 0:\n        a, b = b, a % b\n    return a\n\nprint(gcd(48, 18))  # Output: 6\n\n\nWhy It Matters\n\nFoundational example of algorithmic thinking\nCore building block in modular arithmetic, number theory, and cryptography\nEfficient: runs in \\(O(\\log \\min(a,b))\\) steps\nEasy to implement iteratively or recursively\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(a = bq + r\\), any common divisor of \\(a\\) and \\(b\\) also divides \\(r\\), since \\(r = a - bq\\). Thus, the set of common divisors of \\((a, b)\\) and \\((b, r)\\) is the same, and their greatest element (the GCD) is unchanged.\nRepeatedly applying this property leads to \\(b = 0\\), where \\(\\gcd(a, 0) = a\\).\n\n\nTry It Yourself\n\nCompute \\(\\gcd(270, 192)\\) step by step.\nImplement the recursive version:\n\n\\[\n\\gcd(a, b) = \\gcd(b,, a \\bmod b)\n\\]\n\nExtend to find \\(\\gcd(a, b, c)\\) using \\(\\gcd(\\gcd(a, b), c)\\).\n\n\n\nTest Cases\n\n\n\nInput \\((a, b)\\)\nExpected Output\n\n\n\n\n(48, 18)\n6\n\n\n(270, 192)\n6\n\n\n(7, 3)\n1\n\n\n(10, 0)\n10\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nGCD\n\\(O(\\log \\min(a,b))\\)\n\\(O(1)\\)\n\n\n\nEuclid’s GCD algorithm is where algorithmic elegance begins, a timeless loop of division that turns mathematics into motion.\n\n\n\n2 Sieve of Eratosthenes\nThe Sieve of Eratosthenes is a classic ancient algorithm for finding all prime numbers up to a given limit. It works by iteratively marking the multiples of each prime, starting from 2. The numbers that remain unmarked at the end are primes.\n\nWhat Problem Are We Solving?\nWe want to find all prime numbers less than or equal to \\(n\\). A naive method checks each number \\(k\\) by testing divisibility from \\(2\\) to \\(\\sqrt{k}\\), which is too slow for large \\(n\\). The sieve improves this by using elimination instead of repeated checking.\nWe aim for an algorithm with time complexity close to \\(O(n \\log \\log n)\\).\n\n\nHow It Works (Plain Language)\n\nCreate a list is_prime[0..n] and mark all as true.\nMark 0 and 1 as non-prime.\nStarting from \\(p = 2\\), if \\(p\\) is still marked prime:\n\nMark all multiples of \\(p\\) (from \\(p^2\\) to \\(n\\)) as non-prime.\n\nIncrement \\(p\\) and repeat until \\(p^2 &gt; n\\).\nAll indices still marked true are primes.\n\nThis process “filters out” composite numbers step by step, just like passing sand through finer and finer sieves.\n\n\nExample Step by Step\nFind all primes up to \\(30\\):\nStart: \\([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\\)\n\n\\(p = 2\\): cross out multiples of 2\n\\(p = 3\\): cross out multiples of 3\n\\(p = 5\\): cross out multiples of 5\n\nRemaining numbers: \\(2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\)\n\n\nTiny Code (Python)\ndef sieve(n):\n    is_prime = [True] * (n + 1)\n    is_prime[0] = is_prime[1] = False\n\n    p = 2\n    while p * p &lt;= n:\n        if is_prime[p]:\n            for i in range(p * p, n + 1, p):\n                is_prime[i] = False\n        p += 1\n\n    return [i for i in range(2, n + 1) if is_prime[i]]\n\nprint(sieve(30))  # [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n\n\nWhy It Matters\n\nOne of the earliest and most efficient ways to generate primes\nForms the basis for number-theoretic algorithms and cryptographic systems\nConceptually simple yet mathematically deep\nDemonstrates elimination instead of brute force\n\n\n\nA Gentle Proof (Why It Works)\nEvery composite number \\(n\\) has a smallest prime divisor \\(p \\le \\sqrt{n}\\). Thus, when we mark multiples of primes up to \\(\\sqrt{n}\\), every composite number is crossed out by its smallest prime factor. Numbers that remain unmarked are prime by definition.\n\n\nTry It Yourself\n\nRun the sieve for \\(n = 50\\) and list primes.\nModify to count primes instead of listing them.\nCompare runtime with naive primality tests for large \\(n\\).\nExtend to a segmented sieve for \\(n &gt; 10^7\\).\n\n\n\nTest Cases\n\n\n\nInput \\(n\\)\nExpected Primes\n\n\n\n\n10\n[2, 3, 5, 7]\n\n\n20\n[2, 3, 5, 7, 11, 13, 17, 19]\n\n\n30\n[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSieve\n\\(O(n \\log \\log n)\\)\n\\(O(n)\\)\n\n\n\nThe Sieve of Eratosthenes turns the search for primes into a graceful pattern of elimination, simple loops revealing the hidden order of numbers.\n\n\n\n3 Linear Step Trace\nA Linear Step Trace is a simple yet powerful visualization tool for understanding how an algorithm progresses line by line. It records each step of execution, showing how variables change over time, helping beginners see the flow of computation.\n\nWhat Problem Are We Solving?\nWhen learning algorithms, it’s easy to lose track of what’s happening after each instruction. A Linear Step Trace helps us see execution in motion, one step, one update at a time.\nInstead of abstract reasoning alone, we follow the exact state changes that occur during the run, making debugging and reasoning far easier.\n\n\nHow It Works (Plain Language)\n\nWrite down your pseudocode or code.\nCreate a table with columns for step number, current line, and variable values.\nEach time a line executes, record the line number and updated variables.\nContinue until the program finishes.\n\nThis method is algorithm-agnostic, it works for loops, recursion, conditionals, and all flow patterns.\n\n\nExample Step by Step\nLet’s trace a simple loop:\nsum = 0\nfor i in 1..4:\n    sum = sum + i\n\n\n\nStep\nLine\ni\nsum\nNote\n\n\n\n\n1\n1\n-\n0\nInitialize sum\n\n\n2\n2\n1\n0\nLoop start\n\n\n3\n3\n1\n1\nsum = 0 + 1\n\n\n4\n2\n2\n1\nNext iteration\n\n\n5\n3\n2\n3\nsum = 1 + 2\n\n\n6\n2\n3\n3\nNext iteration\n\n\n7\n3\n3\n6\nsum = 3 + 3\n\n\n8\n2\n4\n6\nNext iteration\n\n\n9\n3\n4\n10\nsum = 6 + 4\n\n\n10\n-\n-\n10\nEnd\n\n\n\nFinal result: \\(sum = 10\\).\n\n\nTiny Code (Python)\nsum = 0\ntrace = []\n\nfor i in range(1, 5):\n    trace.append((i, sum))\n    sum += i\n\ntrace.append((\"final\", sum))\nprint(trace)\n# [(1, 0), (2, 1), (3, 3), (4, 6), ('final', 10)]\n\n\nWhy It Matters\n\nBuilds step-by-step literacy in algorithm reading\nGreat for teaching loops, conditions, and recursion\nReveals hidden assumptions and logic errors\nIdeal for debugging and analysis\n\n\n\nA Gentle Proof (Why It Works)\nEvery algorithm can be expressed as a sequence of state transitions. If each transition is recorded, we obtain a complete trace of computation. Thus, correctness can be verified by comparing expected vs. actual state sequences. This is equivalent to an inductive proof: each step matches the specification.\n\n\nTry It Yourself\n\nTrace a recursive factorial function step by step.\nAdd a “call stack” column to visualize recursion depth.\nTrace an array-sorting loop and mark swaps.\nCompare traces before and after optimization.\n\n\n\nTest Cases\n\n\n\nProgram\nExpected Final State\n\n\n\n\nsum of 1..4\nsum = 10\n\n\nsum of 1..10\nsum = 55\n\n\nfactorial(5)\nresult = 120\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nTrace Recording\n\\(O(n)\\)\n\\(O(n)\\)\n\n\n\nA Linear Step Trace transforms invisible logic into a visible path, a story of each line’s journey, one state at a time.\n\n\n\n4 Algorithm Flow Diagram Builder\nAn Algorithm Flow Diagram Builder turns abstract pseudocode into a visual map, a diagram of control flow that shows where decisions branch, where loops repeat, and where computations end. It’s the bridge between code and comprehension.\n\nWhat Problem Are We Solving?\nWhen an algorithm becomes complex, it’s easy to lose track of its structure. We may know what each line does, but not how control moves through the program.\nA flow diagram lays out that control structure explicitly, revealing loops, branches, merges, and exits at a glance.\n\n\nHow It Works (Plain Language)\n\nIdentify actions and decisions\n\nActions: assignments, computations\nDecisions: if, while, for, switch\n\nRepresent them with symbols\n\nRectangle → action\nDiamond → decision\nArrow → flow of control\n\nConnect nodes based on what happens next\nLoop back arrows for iterations, and mark exit points\n\nThis yields a graph of control, a shape you can follow from start to finish.\n\n\nExample Step by Step\nLet’s draw the flow for finding the sum of numbers \\(1\\) to \\(n\\):\nPseudocode:\nsum = 0\ni = 1\nwhile i ≤ n:\n    sum = sum + i\n    i = i + 1\nprint(sum)\nFlow Outline:\n\nStart\nInitialize sum = 0, i = 1\nDecision: i ≤ n?\n\nYes → Update sum, Increment i → Loop back\nNo → Print sum → End\n\n\nTextual Diagram:\n  [Start]\n     |\n[sum=0, i=1]\n     |\n  (i ≤ n?) ----No----&gt; [Print sum] -&gt; [End]\n     |\n    Yes\n     |\n [sum = sum + i]\n     |\n [i = i + 1]\n     |\n   (Back to i ≤ n?)\n\n\nTiny Code (Python)\ndef sum_to_n(n):\n    sum = 0\n    i = 1\n    while i &lt;= n:\n        sum += i\n        i += 1\n    return sum\nUse this code to generate flow diagrams automatically with libraries like graphviz or pyflowchart.\n\n\nWhy It Matters\n\nReveals structure at a glance\nMakes debugging easier by visualizing possible paths\nHelps design before coding\nUniversal representation (language-agnostic)\n\n\n\nA Gentle Proof (Why It Works)\nEach algorithm’s execution path can be modeled as a directed graph:\n\nVertices = program states or actions\nEdges = transitions (next step)\n\nA flow diagram is simply this control graph rendered visually. It preserves correctness since each edge corresponds to a valid jump in control flow.\n\n\nTry It Yourself\n\nDraw a flowchart for binary search.\nMark all possible comparison outcomes.\nAdd loopbacks for mid-point updates.\nCompare with recursive version, note structural difference.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nKey Decision Node\nExpected Paths\n\n\n\n\nSum loop\n\\(i \\le n\\)\n2 (continue, exit)\n\n\nBinary search\n\\(key == mid?\\)\n3 (left, right, found)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDiagram Construction\n\\(O(n)\\) nodes\n\\(O(n)\\) edges\n\n\n\nAn Algorithm Flow Diagram is a lens, it turns invisible execution paths into a map you can walk, from “Start” to “End.”\n\n\n\n5 Long Division\nLong Division is a step-by-step algorithm for dividing one integer by another. It’s one of the earliest examples of a systematic computational procedure, showing how large problems can be solved through a sequence of local, repeatable steps.\n\nWhat Problem Are We Solving?\nWe want to compute the quotient and remainder when dividing two integers \\(a\\) (dividend) and \\(b\\) (divisor).\nNaively, repeated subtraction would take \\(O(a/b)\\) steps, far too many for large numbers. Long Division improves this by grouping subtractions by powers of 10, performing digit-wise computation efficiently.\n\n\nHow It Works (Plain Language)\n\nAlign digits of \\(a\\) (the dividend).\nCompare current portion of \\(a\\) to \\(b\\).\nFind the largest multiple of \\(b\\) that fits in the current portion.\nSubtract, write the quotient digit, and bring down the next digit.\nRepeat until all digits have been processed.\nThe digits written form the quotient; what’s left is the remainder.\n\nThis method extends naturally to decimals, just continue bringing down zeros.\n\n\nExample Step by Step\nCompute \\(153 \\div 7\\):\n\n\n\n\n\n\n\n\n\n\nStep\nPortion\nQuotient Digit\nRemainder\nAction\n\n\n\n\n1\n15\n2\n1\n\\(7 \\times 2 = 14\\), subtract \\(15 - 14 = 1\\)\n\n\n2\nBring down 3 → 13\n1\n6\n\\(7 \\times 1 = 7\\), subtract \\(13 - 7 = 6\\)\n\n\n3\nNo more digits\n,\n6\nDone\n\n\n\nResult: Quotient \\(= 21\\), Remainder \\(= 6\\) Check: \\(7 \\times 21 + 6 = 153\\)\n\n\nTiny Code (Python)\ndef long_division(a, b):\n    quotient = 0\n    remainder = 0\n    for digit in str(a):\n        remainder = remainder * 10 + int(digit)\n        q = remainder // b\n        remainder = remainder % b\n        quotient = quotient * 10 + q\n    return quotient, remainder\n\nprint(long_division(153, 7))  # (21, 6)\n\n\nWhy It Matters\n\nIntroduces loop invariants and digit-by-digit reasoning\nFoundation for division in arbitrary-precision arithmetic\nCore to implementing division in CPUs and big integer libraries\nDemonstrates decomposing a large task into simple, local operations\n\n\n\nA Gentle Proof (Why It Works)\nAt each step:\n\nThe current remainder \\(r_i\\) satisfies \\(0 \\le r_i &lt; b\\).\nThe algorithm maintains the invariant: \\[\na = b \\times Q_i + r_i\n\\] where \\(Q_i\\) is the partial quotient so far.\nEach step reduces the unprocessed part of \\(a\\), ensuring termination with correct \\(Q\\) and \\(r\\).\n\n\n\nTry It Yourself\n\nPerform \\(2345 \\div 13\\) by hand.\nVerify with Python’s divmod(2345, 13).\nExtend your code to produce decimal expansions.\nCompare digit-wise trace with manual process.\n\n\n\nTest Cases\n\n\n\nDividend \\(a\\)\nDivisor \\(b\\)\nExpected Output \\((Q, R)\\)\n\n\n\n\n153\n7\n(21, 6)\n\n\n100\n8\n(12, 4)\n\n\n99\n9\n(11, 0)\n\n\n23\n5\n(4, 3)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLong Division\n\\(O(d)\\)\n\\(O(1)\\)\n\n\n\nwhere \\(d\\) is the number of digits in \\(a\\).\nLong Division is more than arithmetic, it’s the first encounter with algorithmic thinking: state, iteration, and correctness unfolding one digit at a time.\n\n\n\n6 Modular Addition\nModular addition is arithmetic on a clock, we add numbers, then wrap around when reaching a fixed limit. It’s the simplest example of modular arithmetic, a system that underlies cryptography, hashing, and cyclic data structures.\n\nWhat Problem Are We Solving?\nWe want to add two integers \\(a\\) and \\(b\\), but keep the result within a fixed modulus \\(m\\). That means we compute the remainder after dividing the sum by \\(m\\).\nFormally, we want:\n\\[\n(a + b) \\bmod m\n\\]\nThis ensures results always lie in the range \\([0, m - 1]\\), regardless of how large \\(a\\) or \\(b\\) become.\n\n\nHow It Works (Plain Language)\n\nCompute the sum \\(s = a + b\\).\nDivide \\(s\\) by \\(m\\) to find the remainder.\nThe remainder is the modular sum.\n\nIf \\(s \\ge m\\), we “wrap around” by subtracting \\(m\\) until it fits in the modular range.\nThis idea is like hours on a clock: \\(10 + 5\\) hours on a \\(12\\)-hour clock → \\(3\\).\n\n\nExample Step by Step\nLet \\(a = 10\\), \\(b = 7\\), \\(m = 12\\).\n\nCompute \\(s = 10 + 7 = 17\\).\n\\(17 \\bmod 12 = 5\\).\nSo \\((10 + 7) \\bmod 12 = 5\\).\n\nCheck: \\(17 - 12 = 5\\), fits in \\([0, 11]\\).\n\n\nTiny Code (Python)\ndef mod_add(a, b, m):\n    return (a + b) % m\n\nprint(mod_add(10, 7, 12))  # 5\n\n\nWhy It Matters\n\nFoundation of modular arithmetic\nUsed in hashing, cyclic buffers, and number theory\nCrucial for secure encryption (RSA, ECC)\nDemonstrates wrap-around logic in bounded systems\n\n\n\nA Gentle Proof (Why It Works)\nBy definition of modulus:\n\\[\nx \\bmod m = r \\quad \\text{such that } x = q \\times m + r,\\ 0 \\le r &lt; m\n\\]\nThus, for \\(a + b = q \\times m + r\\), we have \\((a + b) \\bmod m = r\\). All equivalent sums differ by a multiple of \\(m\\), so modular addition preserves congruence:\n\\[\n(a + b) \\bmod m \\equiv (a \\bmod m + b \\bmod m) \\bmod m\n\\]\n\n\nTry It Yourself\n\nCompute \\((15 + 8) \\bmod 10\\).\nVerify \\((a + b) \\bmod m = ((a \\bmod m) + (b \\bmod m)) \\bmod m\\).\nTest with negative values: \\((−3 + 5) \\bmod 7\\).\nApply to time arithmetic: what is \\(11 + 5\\) on a \\(12\\)-hour clock?\n\n\n\nTest Cases\n\n\n\n\\(a\\)\n\\(b\\)\n\\(m\\)\nResult\n\n\n\n\n10\n7\n12\n5\n\n\n5\n5\n10\n0\n\n\n8\n15\n10\n3\n\n\n11\n5\n12\n4\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nModular Addition\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nModular addition teaches the rhythm of modular arithmetic, every sum wraps back into harmony, always staying within its finite world.\n\n\n\n7 Base Conversion\nBase conversion is the algorithmic process of expressing a number in a different numeral system. It’s how we translate between decimal, binary, octal, hexadecimal, or any base, the language of computers and mathematics alike.\n\nWhat Problem Are We Solving?\nWe want to represent an integer \\(n\\) in base \\(b\\). In base 10, digits go from 0 to 9. In base 2, only 0 and 1. In base 16, digits are \\(0 \\ldots 9\\) and \\(A \\ldots F\\).\nThe goal is to find a sequence of digits \\(d_k d_{k-1} \\ldots d_0\\) such that:\n\\[\nn = \\sum_{i=0}^{k} d_i \\cdot b^i\n\\]\nwhere \\(0 \\le d_i &lt; b\\).\n\n\nHow It Works (Plain Language)\n\nStart with the integer \\(n\\).\nRepeatedly divide \\(n\\) by \\(b\\).\nRecord the remainder each time (these are the digits).\nStop when \\(n = 0\\).\nThe base-\\(b\\) representation is the remainders read in reverse order.\n\nThis works because division extracts digits starting from the least significant position.\n\n\nExample Step by Step\nConvert \\(45\\) to binary (\\(b = 2\\)):\n\n\n\nStep\n\\(n\\)\n\\(n \\div 2\\)\nRemainder\n\n\n\n\n1\n45\n22\n1\n\n\n2\n22\n11\n0\n\n\n3\n11\n5\n1\n\n\n4\n5\n2\n1\n\n\n5\n2\n1\n0\n\n\n6\n1\n0\n1\n\n\n\nRead remainders upward: 101101\nSo \\(45_{10} = 101101_2\\).\nCheck: \\(1 \\cdot 2^5 + 0 \\cdot 2^4 + 1 \\cdot 2^3 + 1 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0 = 32 + 0 + 8 + 4 + 0 + 1 = 45\\) ✅\n\n\nTiny Code (Python)\ndef to_base(n, b):\n    digits = []\n    while n &gt; 0:\n        digits.append(n % b)\n        n //= b\n    return digits[::-1] or [0]\n\nprint(to_base(45, 2))  # [1, 0, 1, 1, 0, 1]\n\n\nWhy It Matters\n\nConverts numbers between human and machine representations\nCore in encoding, compression, and cryptography\nBuilds intuition for positional number systems\nUsed in parsing, serialization, and digital circuits\n\n\n\nA Gentle Proof (Why It Works)\nEach division step produces one digit \\(r_i = n_i \\bmod b\\). We have:\n\\[\nn_i = b \\cdot n_{i+1} + r_i\n\\]\nUnfolding the recurrence gives:\n\\[\nn = \\sum_{i=0}^{k} r_i b^i\n\\]\nSo collecting remainders in reverse order reconstructs \\(n\\) exactly.\n\n\nTry It Yourself\n\nConvert \\(100_{10}\\) to base 8.\nConvert \\(255_{10}\\) to base 16.\nVerify by recombining digits via \\(\\sum d_i b^i\\).\nWrite a reverse converter: base-\\(b\\) to decimal.\n\n\n\nTest Cases\n\n\n\nDecimal \\(n\\)\nBase \\(b\\)\nRepresentation\n\n\n\n\n45\n2\n101101\n\n\n100\n8\n144\n\n\n255\n16\nFF\n\n\n31\n5\n111\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBase Conversion\n\\(O(\\log_b n)\\)\n\\(O(\\log_b n)\\)\n\n\n\nBase conversion is arithmetic storytelling, peeling away remainders until only digits remain, revealing the same number through a different lens.\n\n\n\n8 Factorial Computation\nFactorial computation is the algorithmic act of multiplying a sequence of consecutive integers, a simple rule that grows explosively. It lies at the foundation of combinatorics, probability, and mathematical analysis.\n\nWhat Problem Are We Solving?\nWe want to compute the factorial of a non-negative integer \\(n\\), written \\(n!\\), defined as:\n\\[\nn! = n \\times (n - 1) \\times (n - 2) \\times \\cdots \\times 1\n\\]\nwith the base case:\n\\[\n0! = 1\n\\]\nFactorial counts the number of ways to arrange \\(n\\) distinct objects, the building block of permutations and combinations.\n\n\nHow It Works (Plain Language)\nThere are two main ways:\nIterative:\n\nStart with result = 1\nMultiply by each \\(i\\) from 1 to \\(n\\)\nReturn result\n\nRecursive:\n\n\\(n! = n \\times (n - 1)!\\)\nStop when \\(n = 0\\)\n\nBoth methods produce the same result; recursion mirrors the mathematical definition, iteration avoids call overhead.\n\n\nExample Step by Step\nCompute \\(5!\\):\n\n\n\nStep\n\\(n\\)\nProduct\n\n\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n3\n6\n\n\n4\n4\n24\n\n\n5\n5\n120\n\n\n\nSo \\(5! = 120\\) ✅\n\n\nTiny Code (Python)\nIterative Version\ndef factorial_iter(n):\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\nprint(factorial_iter(5))  # 120\nRecursive Version\ndef factorial_rec(n):\n    if n == 0:\n        return 1\n    return n * factorial_rec(n - 1)\n\nprint(factorial_rec(5))  # 120\n\n\nWhy It Matters\n\nCore operation in combinatorics, calculus, and probability\nDemonstrates recursion, iteration, and induction\nGrows rapidly, useful for testing overflow and asymptotics\nAppears in binomial coefficients, Taylor series, and permutations\n\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\(n! = n \\times (n - 1)!\\). Assume \\((n - 1)!\\) is correctly computed. Then multiplying by \\(n\\) yields \\(n!\\).\nBy induction:\n\nBase case: \\(0! = 1\\)\nStep: if \\((n - 1)!\\) is correct, so is \\(n!\\)\n\nThus, the recursive and iterative definitions are equivalent and correct.\n\n\nTry It Yourself\n\nCompute \\(6!\\) both iteratively and recursively.\nPrint intermediate products to trace the growth.\nCompare runtime for \\(n = 1000\\) using both methods.\nExplore factorial in floating point (math.gamma) for non-integers.\n\n\n\nTest Cases\n\n\n\nInput \\(n\\)\nExpected Output \\(n!\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n3\n6\n\n\n5\n120\n\n\n6\n720\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nIterative\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nRecursive\n\\(O(n)\\)\n\\(O(n)\\) (stack)\n\n\n\nFactorial computation is where simplicity meets infinity, a single rule that scales from 1 to astronomical numbers with graceful inevitability.\n\n\n\n9 Iterative Process Tracer\nAn Iterative Process Tracer is a diagnostic algorithm that follows each iteration of a loop, recording variable states, conditions, and updates. It helps visualize the evolution of a program’s internal state, turning looping logic into a clear timeline.\n\nWhat Problem Are We Solving?\nWhen writing iterative algorithms, it’s easy to lose sight of what happens at each step. Are variables updating correctly? Are loop conditions behaving as expected? A tracer captures this process, step by step, so we can verify correctness, find bugs, and teach iteration with clarity.\n\n\nHow It Works (Plain Language)\n\nIdentify the loop (for or while).\nBefore or after each iteration, record:\n\nThe iteration number\nKey variable values\nCondition evaluations\n\nStore these snapshots in a trace table.\nAfter execution, review how values evolve over time.\n\nThink of it as an “execution diary”, every iteration gets a journal entry.\n\n\nExample Step by Step\nLet’s trace a simple accumulation:\nsum = 0\nfor i in 1..5:\n    sum = sum + i\n\n\n\nStep\n\\(i\\)\n\\(sum\\)\nDescription\n\n\n\n\n1\n1\n1\nAdd first number\n\n\n2\n2\n3\nAdd second number\n\n\n3\n3\n6\nAdd third number\n\n\n4\n4\n10\nAdd fourth number\n\n\n5\n5\n15\nAdd fifth number\n\n\n\nFinal result: \\(sum = 15\\)\n\n\nTiny Code (Python)\ndef trace_sum(n):\n    sum = 0\n    trace = []\n    for i in range(1, n + 1):\n        sum += i\n        trace.append((i, sum))\n    return trace\n\nprint(trace_sum(5))\n# [(1, 1), (2, 3), (3, 6), (4, 10), (5, 15)]\n\n\nWhy It Matters\n\nTurns hidden state changes into visible data\nIdeal for debugging loops and verifying invariants\nSupports algorithm teaching and step-by-step reasoning\nUseful in profiling, logging, and unit testing\n\n\n\nA Gentle Proof (Why It Works)\nAn iterative algorithm is a sequence of deterministic transitions:\n\\[\nS_{i+1} = f(S_i)\n\\]\nRecording \\(S_i\\) at each iteration yields the complete trajectory of execution. The trace table captures all intermediate states, ensuring reproducibility and clarity, a form of operational proof.\n\n\nTry It Yourself\n\nTrace variable updates in a multiplication loop.\nAdd condition checks (e.g. early exits).\nRecord both pre- and post-update states.\nCompare traces of iterative vs recursive versions.\n\n\n\nTest Cases\n\n\n\nInput \\(n\\)\nExpected Trace\n\n\n\n\n3\n[(1, 1), (2, 3), (3, 6)]\n\n\n4\n[(1, 1), (2, 3), (3, 6), (4, 10)]\n\n\n5\n[(1, 1), (2, 3), (3, 6), (4, 10), (5, 15)]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nTracing\n\\(O(n)\\)\n\\(O(n)\\)\n\n\n\nAn Iterative Process Tracer makes thinking visible, a loop’s internal rhythm laid out, step by step, until the final note resolves.\n\n\n\n10 Tower of Hanoi\nThe Tower of Hanoi is a legendary recursive puzzle that beautifully illustrates how complex problems can be solved through simple repeated structure. It’s a timeless example of divide and conquer thinking in its purest form.\n\nWhat Problem Are We Solving?\nWe want to move \\(n\\) disks from a source peg to a target peg, using one auxiliary peg. Rules:\n\nMove only one disk at a time.\nNever place a larger disk on top of a smaller one.\n\nThe challenge is to find the minimal sequence of moves that achieves this.\n\n\nHow It Works (Plain Language)\nThe key insight: To move \\(n\\) disks, first move \\(n-1\\) disks aside, move the largest one, then bring the smaller ones back.\nSteps:\n\nMove \\(n-1\\) disks from source → auxiliary\nMove the largest disk from source → target\nMove \\(n-1\\) disks from auxiliary → target\n\nThis recursive structure repeats until the smallest disk moves directly.\n\n\nExample Step by Step\nFor \\(n = 3\\), pegs: A (source), B (auxiliary), C (target)\n\n\n\nStep\nMove\n\n\n\n\n1\nA → C\n\n\n2\nA → B\n\n\n3\nC → B\n\n\n4\nA → C\n\n\n5\nB → A\n\n\n6\nB → C\n\n\n7\nA → C\n\n\n\nTotal moves: \\(2^3 - 1 = 7\\)\n\n\nTiny Code (Python)\ndef hanoi(n, source, target, aux):\n    if n == 1:\n        print(f\"{source} → {target}\")\n        return\n    hanoi(n - 1, source, aux, target)\n    print(f\"{source} → {target}\")\n    hanoi(n - 1, aux, target, source)\n\nhanoi(3, 'A', 'C', 'B')\n\n\nWhy It Matters\n\nClassic recursive pattern: break → solve → combine\nDemonstrates exponential growth (\\(2^n - 1\\) moves)\nTrains recursive reasoning and stack visualization\nAppears in algorithm analysis, recursion trees, and combinatorics\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be the number of moves for \\(n\\) disks. We must move \\(n-1\\) disks twice and one largest disk once:\n\\[\nT(n) = 2T(n-1) + 1, \\quad T(1) = 1\n\\]\nSolving the recurrence:\n\\[\nT(n) = 2^n - 1\n\\]\nEach recursive step preserves rules and reduces the problem size, ensuring correctness by structural induction.\n\n\nTry It Yourself\n\nTrace \\(n = 2\\) and \\(n = 3\\) by hand.\nCount recursive calls.\nModify code to record moves in a list.\nExtend to display peg states after each move.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected Moves\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n7\n\n\n4\n15\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nMoves\n\\(O(2^n)\\)\n\\(O(n)\\) (recursion stack)\n\n\n\nThe Tower of Hanoi turns recursion into art, every move guided by symmetry, every step revealing how simplicity builds complexity one disk at a time.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-2.-measuring-time-and-space",
    "href": "books/en-us/list-1.html#section-2.-measuring-time-and-space",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 2. Measuring time and space",
    "text": "Section 2. Measuring time and space\n\n11 Counting Operations\nCounting operations is the first step toward understanding time complexity. It’s the art of translating code into math by measuring how many basic steps an algorithm performs, helping us predict performance before running it.\n\nWhat Problem Are We Solving?\nWe want to estimate how long an algorithm takes, not by clock time, but by how many fundamental operations it executes. Instead of relying on hardware speed, we count abstract steps, comparisons, assignments, additions, each treated as one unit of work.\nThis turns algorithms into analyzable formulas.\n\n\nHow It Works (Plain Language)\n\nIdentify the unit step (like one comparison or addition).\nBreak the algorithm into lines or loops.\nCount repetitions for each operation.\nSum all counts to get a total step function \\(T(n)\\).\nSimplify to dominant terms for asymptotic analysis.\n\nWe’re not measuring seconds, we’re measuring structure.\n\n\nExample Step by Step\nCount operations for:\nsum = 0\nfor i in range(1, n + 1):\n    sum += i\nBreakdown:\n\n\n\nLine\nOperation\nCount\n\n\n\n\n1\nInitialization\n1\n\n\n2\nLoop comparison\n\\(n + 1\\)\n\n\n3\nAddition + assignment\n\\(n\\)\n\n\n\nTotal: \\[\nT(n) = 1 + (n + 1) + n = 2n + 2\n\\]\nAsymptotically: \\[\nT(n) = O(n)\n\\]\n\n\nTiny Code (Python)\ndef count_sum_ops(n):\n    ops = 0\n    ops += 1  # init sum\n    for i in range(1, n + 1):\n        ops += 1  # loop check\n        ops += 1  # sum += i\n    ops += 1  # final loop check\n    return ops\nTest: count_sum_ops(5) → 13\n\n\nWhy It Matters\n\nBuilds intuition for algorithm growth\nReveals hidden costs (nested loops, recursion)\nFoundation for Big-O and runtime proofs\nLanguage-agnostic: works for any pseudocode\n\n\n\nA Gentle Proof (Why It Works)\nEvery program can be modeled as a finite sequence of operations parameterized by input size \\(n\\). If \\(f(n)\\) counts these operations exactly, then for large \\(n\\), growth rate \\(\\Theta(f(n))\\) matches actual performance up to constant factors. Counting operations therefore predicts asymptotic runtime behavior.\n\n\nTry It Yourself\n\nCount operations in a nested loop:\nfor i in range(n):\n    for j in range(n):\n        x += 1\nDerive \\(T(n) = n^2 + 2n + 1\\).\nSimplify to \\(O(n^2)\\).\nCompare iterative vs recursive counting.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nStep Function\nBig-O\n\n\n\n\nLinear Loop\n\\(2n + 2\\)\n\\(O(n)\\)\n\n\nNested Loop\n\\(n^2 + 2n + 1\\)\n\\(O(n^2)\\)\n\n\nConstant Work\n\\(c\\)\n\\(O(1)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nCounting Steps\n\\(O(1)\\) (analysis)\n\\(O(1)\\)\n\n\n\nCounting operations transforms code into mathematics, a microscope for understanding how loops, branches, and recursion scale with input size.\n\n\n\n12 Loop Analysis\nLoop analysis is the key to unlocking how algorithms grow, it tells us how many times a loop runs and, therefore, how many operations are performed. Every time you see a loop, you’re looking at a formula in disguise.\n\nWhat Problem Are We Solving?\nWe want to determine how many iterations a loop executes as a function of input size \\(n\\). This helps us estimate total runtime before measuring it empirically.\nWhether a loop is linear, nested, logarithmic, or mixed, understanding its iteration count reveals the algorithm’s true complexity.\n\n\nHow It Works (Plain Language)\n\nIdentify the loop variable (like i in for i in range(...)).\nFind its update rule, additive (i += 1) or multiplicative (i *= 2).\nSolve for how many times the condition holds true.\nMultiply by inner loop work if nested.\nSum all contributions from independent loops.\n\nThis transforms loops into algebraic expressions you can reason about.\n\n\nExample Step by Step\nExample 1: Linear Loop\nfor i in range(1, n + 1):\n    work()\n\\(i\\) runs from \\(1\\) to \\(n\\), incrementing by \\(1\\). Iterations: \\(n\\) Work: \\(O(n)\\)\nExample 2: Logarithmic Loop\ni = 1\nwhile i &lt;= n:\n    work()\n    i *= 2\n\\(i\\) doubles each step: \\(1, 2, 4, 8, \\dots, n\\) Iterations: \\(\\log_2 n + 1\\) Work: \\(O(\\log n)\\)\nExample 3: Nested Loop\nfor i in range(n):\n    for j in range(n):\n        work()\nOuter loop: \\(n\\) Inner loop: \\(n\\) Total work: \\(n \\times n = n^2\\)\n\n\nTiny Code (Python)\ndef linear_loop(n):\n    count = 0\n    for i in range(n):\n        count += 1\n    return count  # n\n\ndef log_loop(n):\n    count = 0\n    i = 1\n    while i &lt;= n:\n        count += 1\n        i *= 2\n    return count  # ≈ log2(n)\n\n\nWhy It Matters\n\nReveals complexity hidden inside loops\nCore tool for deriving \\(O(n)\\), \\(O(\\log n)\\), and \\(O(n^2)\\)\nMakes asymptotic behavior predictable and measurable\nWorks for for-loops, while-loops, and nested structures\n\n\n\nA Gentle Proof (Why It Works)\nEach loop iteration corresponds to a true condition in its guard. If the loop variable \\(i\\) evolves monotonically (by addition or multiplication), the total number of iterations is the smallest \\(k\\) satisfying the exit condition.\nFor additive updates: \\[\ni_0 + k \\cdot \\Delta \\ge n \\implies k = \\frac{n - i_0}{\\Delta}\n\\]\nFor multiplicative updates: \\[\ni_0 \\cdot r^k \\ge n \\implies k = \\log_r \\frac{n}{i_0}\n\\]\n\n\nTry It Yourself\n\nAnalyze loop:\ni = n\nwhile i &gt; 0:\n    i //= 2\n→ \\(O(\\log n)\\)\nAnalyze double loop:\nfor i in range(n):\n    for j in range(i):\n        work()\n→ \\(\\frac{n(n-1)}{2} = O(n^2)\\)\nCombine additive + multiplicative loops.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nCode Pattern\nIterations\nComplexity\n\n\n\n\nfor i in range(n)\n\\(n\\)\n\\(O(n)\\)\n\n\nwhile i &lt; n: i *= 2\n\\(\\log_2 n\\)\n\\(O(\\log n)\\)\n\n\nfor i in range(n): for j in range(n)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\nfor i in range(n): for j in range(i)\n\\(\\frac{n(n-1)}{2}\\)\n\\(O(n^2)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLoop Analysis\n\\(O(1)\\) (per loop)\n\\(O(1)\\)\n\n\n\nLoop analysis turns repetition into arithmetic, every iteration becomes a term, every loop a story in the language of growth.\n\n\n\n13 Recurrence Expansion\nRecurrence expansion is how we unfold recursive definitions to see their true cost. Many recursive algorithms (like Merge Sort or Quick Sort) define runtime in terms of smaller copies of themselves. By expanding the recurrence, we reveal the total work step by step.\n\nWhat Problem Are We Solving?\nRecursive algorithms often express their runtime as:\n\\[\nT(n) = a \\cdot T!\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nHere:\n\n\\(a\\) = number of recursive calls\n\\(b\\) = factor by which input size is reduced\n\\(f(n)\\) = work done outside recursion (splitting, merging, etc.)\n\nWe want to estimate \\(T(n)\\) by expanding this relation until the base case.\n\n\nHow It Works (Plain Language)\nThink of recurrence expansion as peeling an onion. Each recursive layer contributes some cost, and we add all layers until the base.\nSteps:\n\nWrite the recurrence.\nExpand one level: replace \\(T(\\cdot)\\) with its formula.\nRepeat until the argument becomes the base case.\nSum the work done at each level.\nSimplify the sum to get asymptotic form.\n\n\n\nExample Step by Step\nTake Merge Sort:\n\\[\nT(n) = 2T!\\left(\\frac{n}{2}\\right) + n\n\\]\nExpand:\n\nLevel 0: \\(T(n) = 2T(n/2) + n\\)\nLevel 1: \\(T(n/2) = 2T(n/4) + n/2\\) → Substitute \\(T(n) = 4T(n/4) + 2n\\)\nLevel 2: \\(T(n) = 8T(n/8) + 3n\\)\n…\nLevel \\(\\log_2 n\\): \\(T(1) = c\\)\n\nSum work across levels:\n\\[\nT(n) = n \\log_2 n + n = O(n \\log n)\n\\]\n\n\nTiny Code (Python)\ndef recurrence_expand(a, b, f, n, base=1):\n    level = 0\n    total = 0\n    size = n\n    while size &gt;= base:\n        cost = (a  level) * f(size)\n        total += cost\n        size //= b\n        level += 1\n    return total\nUse f = lambda x: x for Merge Sort.\n\n\nWhy It Matters\n\nCore tool for analyzing recursive algorithms\nBuilds intuition before applying the Master Theorem\nTurns abstract recurrence into tangible pattern\nHelps visualize total work per recursion level\n\n\n\nA Gentle Proof (Why It Works)\nAt level \\(i\\):\n\nThere are \\(a^i\\) subproblems.\nEach subproblem has size \\(\\frac{n}{b^i}\\).\nWork per level: \\(a^i \\cdot f!\\left(\\frac{n}{b^i}\\right)\\)\n\nTotal cost:\n\\[\nT(n) = \\sum_{i=0}^{\\log_b n} a^i f!\\left(\\frac{n}{b^i}\\right)\n\\]\nDepending on how \\(f(n)\\) compares to \\(n^{\\log_b a}\\), either top, bottom, or middle levels dominate.\n\n\nTry It Yourself\n\nExpand \\(T(n) = 3T(n/2) + n^2\\).\nExpand \\(T(n) = T(n/2) + 1\\).\nVisualize total work per level.\nCheck your result with Master Theorem.\n\n\n\nTest Cases\n\n\n\nRecurrence\nExpansion Result\nComplexity\n\n\n\n\n\\(T(n) = 2T(n/2) + n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(T(n) = T(n/2) + 1\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n) = 4T(n/2) + n\\)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nExpansion\n\\(O(\\log n)\\) levels\n\\(O(\\log n)\\) tree depth\n\n\n\nRecurrence expansion turns recursion into rhythm, each level adding its verse, the sum revealing the melody of the algorithm’s growth.\n\n\n\n14 Amortized Analysis\nAmortized analysis looks beyond the worst case of individual operations to capture the average cost per operation over a long sequence. It tells us when “expensive” actions even out, revealing algorithms that are faster than they first appear.\n\nWhat Problem Are We Solving?\nSome operations occasionally take a long time (like resizing an array), but most are cheap. A naive worst-case analysis exaggerates total cost. Amortized analysis finds the true average cost across a sequence.\nWe’re not averaging across inputs, but across operations in one run.\n\n\nHow It Works (Plain Language)\nSuppose an operation is usually \\(O(1)\\), but sometimes \\(O(n)\\). If that expensive case happens rarely enough, the average per operation is still small.\nThree main methods:\n\nAggregate method, total cost ÷ number of operations\nAccounting method, charge extra for cheap ops, save credit for costly ones\nPotential method, define potential energy (stored work) and track change\n\n\n\nExample Step by Step\nDynamic Array Resizing\nWhen an array is full, double its size and copy elements.\n\n\n\nOperation\nCost\nComment\n\n\n\n\nInsert #1–#1\n1\ninsert directly\n\n\nInsert #2\n2\nresize to 2, copy 1\n\n\nInsert #3\n3\nresize to 4, copy 2\n\n\nInsert #5\n5\nresize to 8, copy 4\n\n\n…\n…\n…\n\n\n\nTotal cost after \\(n\\) inserts ≈ \\(2n\\) Average cost = \\(2n / n = O(1)\\) So each insert is amortized \\(O(1)\\), not \\(O(n)\\).\n\n\nTiny Code (Python)\ndef dynamic_array(n):\n    arr = []\n    capacity = 1\n    cost = 0\n    for i in range(n):\n        if len(arr) == capacity:\n            capacity *= 2\n            cost += len(arr)  # copying cost\n        arr.append(i)\n        cost += 1  # insert cost\n    return cost, cost / n  # total, amortized average\nTry dynamic_array(10) → roughly total cost ≈ 20, average ≈ 2.\n\n\nWhy It Matters\n\nShows average efficiency over sequences\nKey to analyzing stacks, queues, hash tables, and dynamic arrays\nExplains why “occasionally expensive” operations are still efficient overall\nSeparates perception (worst-case) from reality (aggregate behavior)\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(C_i\\) = cost of \\(i\\)th operation, and \\(n\\) = total operations.\nAggregate Method:\n\\[\n\\text{Amortized cost} = \\frac{\\sum_{i=1}^n C_i}{n}\n\\]\nIf \\(\\sum C_i = O(n)\\), each operation’s average = \\(O(1)\\).\nPotential Method:\nDefine potential \\(\\Phi_i\\) representing saved work. Amortized cost = \\(C_i + \\Phi_i - \\Phi_{i-1}\\) Summing over all operations telescopes potential away, leaving total cost bounded by initial + final potential.\n\n\nTry It Yourself\n\nAnalyze amortized cost for stack with occasional full pop.\nUse accounting method to assign “credits” to inserts.\nShow \\(O(1)\\) amortized insert in hash table with resizing.\nCompare amortized vs worst-case time.\n\n\n\nTest Cases\n\n\n\nOperation Type\nWorst Case\nAmortized\n\n\n\n\nArray Insert (Doubling)\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nStack Push\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nQueue Dequeue (2-stack)\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nUnion-Find (Path Compression)\n\\(O(\\log n)\\)\n\\(O(\\alpha(n))\\)\n\n\n\n\n\nComplexity\n\n\n\nAnalysis Type\nFormula\nGoal\n\n\n\n\nAggregate\n\\(\\frac{\\text{Total Cost}}{n}\\)\nSimplicity\n\n\nAccounting\nAssign credits\nIntuition\n\n\nPotential\n\\(\\Delta \\Phi\\)\nFormal rigor\n\n\n\nAmortized analysis reveals the calm beneath chaos — a few storms don’t define the weather, and one \\(O(n)\\) moment doesn’t ruin \\(O(1)\\) harmony.\n\n\n\n15 Space Counting\nSpace counting is the spatial twin of operation counting, instead of measuring time, we measure how much memory an algorithm consumes. Every variable, array, stack frame, or temporary buffer adds to the footprint. Understanding it helps us write programs that fit in memory and scale gracefully.\n\nWhat Problem Are We Solving?\nWe want to estimate the space complexity of an algorithm — how much memory it needs as input size \\(n\\) grows.\nThis includes:\n\nStatic space (fixed variables)\nDynamic space (arrays, recursion, data structures)\nAuxiliary space (extra working memory beyond input)\n\nOur goal: express total memory as a function \\(S(n)\\).\n\n\nHow It Works (Plain Language)\n\nCount primitive variables (constants, counters, pointers). → constant space \\(O(1)\\)\nAdd data structure sizes (arrays, lists, matrices). → often proportional to \\(n\\), \\(n^2\\), etc.\nAdd recursion stack depth, if applicable.\nIgnore constants for asymptotic space, focus on growth.\n\nIn the end, \\[\nS(n) = S_{\\text{static}} + S_{\\text{dynamic}} + S_{\\text{recursive}}\n\\]\n\n\nExample Step by Step\nExample 1: Linear Array\narr = [0] * n\n\n\\(n\\) integers → \\(O(n)\\) space\n\nExample 2: 2D Matrix\nmatrix = [[0] * n for _ in range(n)]\n\n\\(n \\times n\\) elements → \\(O(n^2)\\) space\n\nExample 3: Recursive Factorial\ndef fact(n):\n    if n == 0:\n        return 1\n    return n * fact(n - 1)\n\nDepth = \\(n\\) → Stack = \\(O(n)\\)\nNo extra data structures → \\(S(n) = O(n)\\)\n\n\n\nTiny Code (Python)\ndef space_counter(n):\n    const = 1             # O(1)\n    arr = [0] * n         # O(n)\n    matrix = [[0]*n for _ in range(n)]  # O(n^2)\n    return const + len(arr) + len(matrix)\nThis simple example illustrates additive contributions.\n\n\nWhy It Matters\n\nMemory is a first-class constraint in large systems\nCritical for embedded, streaming, and real-time algorithms\nReveals tradeoffs between time and space\nGuides design of in-place vs out-of-place solutions\n\n\n\nA Gentle Proof (Why It Works)\nEach algorithm manipulates a finite set of data elements. If \\(s_i\\) is the space allocated for structure \\(i\\), total space is:\n\\[\nS(n) = \\sum_i s_i(n)\n\\]\nAsymptotic space is dominated by the largest term, so \\(S(n) = \\Theta(\\max_i s_i(n))\\).\nThis ensures our analysis scales with data growth.\n\n\nTry It Yourself\n\nCount space for Merge Sort (temporary arrays).\nCompare with Quick Sort (in-place).\nAdd recursion cost explicitly.\nAnalyze time–space tradeoff for dynamic programming.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nSpace\nReason\n\n\n\n\nLinear Search\n\\(O(1)\\)\nConstant extra memory\n\n\nMerge Sort\n\\(O(n)\\)\nExtra array for merging\n\n\nQuick Sort\n\\(O(\\log n)\\)\nStack depth\n\n\nDP Table (2D)\n\\(O(n^2)\\)\nFull grid of states\n\n\n\n\n\nComplexity\n\n\n\nComponent\nExample\nCost\n\n\n\n\nVariables\n\\(a, b, c\\)\n\\(O(1)\\)\n\n\nArrays\narr[n]\n\\(O(n)\\)\n\n\nMatrices\nmatrix[n][n]\n\\(O(n^2)\\)\n\n\nRecursion Stack\nDepth \\(n\\)\n\\(O(n)\\)\n\n\n\nSpace counting turns memory into a measurable quantity, every variable a footprint, every structure a surface, every stack frame a layer in the architecture of an algorithm.\n\n\n\n16 Memory Footprint Estimator\nA Memory Footprint Estimator calculates how much memory an algorithm or data structure truly consumes, not just asymptotically, but in real bytes. It bridges the gap between theoretical space complexity and practical implementation.\n\nWhat Problem Are We Solving?\nKnowing an algorithm is \\(O(n)\\) in space isn’t enough when working close to memory limits. We need actual estimates: how many bytes per element, how much total allocation, and what overheads exist.\nA footprint estimator converts theoretical counts into quantitative estimates for real-world scaling.\n\n\nHow It Works (Plain Language)\n\nIdentify data types used: int, float, pointer, struct, etc.\nEstimate size per element (language dependent, e.g. int = 4 bytes).\nMultiply by count to find total memory usage.\nInclude overheads from:\n\nObject headers or metadata\nPadding or alignment\nPointers or references\n\n\nFinal footprint: \\[\n\\text{Memory} = \\sum_i (\\text{count}_i \\times \\text{size}_i) + \\text{overhead}\n\\]\n\n\nExample Step by Step\nSuppose we have a list of \\(n = 1{,}000{,}000\\) integers in Python.\n\n\n\nComponent\nSize (Bytes)\nCount\nTotal\n\n\n\n\nList object\n64\n1\n64\n\n\nPointers\n8\n1,000,000\n8,000,000\n\n\nInteger objects\n28\n1,000,000\n28,000,000\n\n\n\nTotal ≈ 36 MB (plus interpreter overhead).\nIf using a fixed array('i') (C-style ints): \\(4 \\text{ bytes} \\times 10^6 = 4\\) MB, far more memory-efficient.\n\n\nTiny Code (Python)\nimport sys\n\nn = 1_000_000\narr_list = list(range(n))\narr_array = bytearray(n * 4)\n\nprint(sys.getsizeof(arr_list))   # list object\nprint(sys.getsizeof(arr_array))  # raw byte array\nCompare memory cost using sys.getsizeof().\n\n\nWhy It Matters\n\nReveals true memory requirements\nCritical for large datasets, embedded systems, and databases\nExplains performance tradeoffs in languages with object overhead\nSupports system design and capacity planning\n\n\n\nA Gentle Proof (Why It Works)\nEach variable or element consumes a fixed number of bytes depending on type. If \\(n_i\\) elements of type \\(t_i\\) are allocated, total memory is:\n\\[\nM(n) = \\sum_i n_i \\cdot s(t_i)\n\\]\nSince \\(s(t_i)\\) is constant, growth rate follows counts: \\(M(n) = O(\\max_i n_i)\\), matching asymptotic analysis while giving concrete magnitudes.\n\n\nTry It Yourself\n\nEstimate memory for a matrix of \\(1000 \\times 1000\\) floats (8 bytes each).\nCompare Python list of lists vs NumPy array.\nAdd overheads for pointers and headers.\nRepeat for custom struct or class with multiple fields.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nStructure\nFormula\nApprox Memory\n\n\n\n\nList of \\(n\\) ints\n\\(n \\times 28\\) B\n28 MB (1M items)\n\n\nArray of \\(n\\) ints\n\\(n \\times 4\\) B\n4 MB\n\n\nMatrix \\(n \\times n\\) floats\n\\(8n^2\\) B\n8 MB for \\(n=1000\\)\n\n\nHash Table \\(n\\) entries\n\\(O(n)\\)\nDepends on load factor\n\n\n\n\n\nComplexity\n\n\n\nMetric\nGrowth\nUnit\n\n\n\n\nSpace\n\\(O(n)\\)\nBytes\n\n\nOverhead\n\\(O(1)\\)\nMetadata\n\n\n\nA Memory Footprint Estimator turns abstract “\\(O(n)\\) space” into tangible bytes, letting you see how close you are to the edge before your program runs out of room.\n\n\n\n17 Time Complexity Table\nA Time Complexity Table summarizes how different algorithms grow as input size increases, it’s a map from formula to feeling, showing which complexities are fast, which are dangerous, and how they compare in scale.\n\nWhat Problem Are We Solving?\nWe want a quick reference that links mathematical growth rates to practical performance. Knowing that an algorithm is \\(O(n \\log n)\\) is good; understanding what that means for \\(n = 10^6\\) is better.\nThe table helps estimate feasibility: Can this algorithm handle a million inputs? A billion?\n\n\nHow It Works (Plain Language)\n\nList common complexity classes: constant, logarithmic, linear, etc.\nWrite their formulas and interpretations.\nEstimate operations for various \\(n\\).\nHighlight tipping points, where performance becomes infeasible.\n\nThis creates an intuition grid for algorithmic growth.\n\n\nExample Step by Step\nLet \\(n = 10^6\\) (1 million). Estimate operations per complexity class (approximate scale):\n\n\n\n\n\n\n\n\n\nComplexity\nFormula\nOperations (n=10⁶)\nIntuition\n\n\n\n\n\\(O(1)\\)\nconstant\n1\ninstant\n\n\n\\(O(\\log n)\\)\n\\(\\log_2 10^6 \\approx 20\\)\n20\nlightning fast\n\n\n\\(O(n)\\)\n\\(10^6\\)\n1,000,000\nmanageable\n\n\n\\(O(n \\log n)\\)\n\\(10^6 \\cdot 20\\)\n20M\nstill OK\n\n\n\\(O(n^2)\\)\n\\((10^6)^2\\)\n\\(10^{12}\\)\ntoo slow\n\n\n\\(O(2^n)\\)\n\\(2^{20} \\approx 10^6\\)\nimpossible beyond \\(n=30\\)\n\n\n\n\\(O(n!)\\)\nfactorial\n\\(10^6!\\)\nabsurdly huge\n\n\n\nThe table makes complexity feel real.\n\n\nTiny Code (Python)\nimport math\n\ndef ops_estimate(n):\n    return {\n        \"O(1)\": 1,\n        \"O(log n)\": math.log2(n),\n        \"O(n)\": n,\n        \"O(n log n)\": n * math.log2(n),\n        \"O(n^2)\": n2\n    }\n\nprint(ops_estimate(106))\n\n\nWhy It Matters\n\nBuilds numerical intuition for asymptotics\nHelps choose the right algorithm for large \\(n\\)\nExplains why \\(O(n^2)\\) might work for \\(n=1000\\) but not \\(n=10^6\\)\nConnects abstract math to real-world feasibility\n\n\n\nA Gentle Proof (Why It Works)\nEach complexity class describes a function \\(f(n)\\) bounding operations. Comparing \\(f(n)\\) for common \\(n\\) values illustrates relative growth rates. Because asymptotic notation suppresses constants, differences in growth dominate as \\(n\\) grows.\nThus, numerical examples are faithful approximations of asymptotic behavior.\n\n\nTry It Yourself\n\nFill the table for \\(n = 10^3, 10^4, 10^6\\).\nPlot growth curves for each \\(f(n)\\).\nCompare runtime if each operation = 1 microsecond.\nIdentify feasible vs infeasible complexities for your hardware.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(O(1)\\)\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\\(O(n^2)\\)\n\n\n\n\n\\(10^3\\)\n1\n10\n1,000\n1,000,000\n\n\n\\(10^6\\)\n1\n20\n1,000,000\n\\(10^{12}\\)\n\n\n\\(10^9\\)\n1\n30\n1,000,000,000\n\\(10^{18}\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nType\nInsight\n\n\n\n\nTable Generation\n\\(O(1)\\)\nStatic reference\n\n\nEvaluation\n\\(O(1)\\)\nAnalytical\n\n\n\nA Time Complexity Table turns abstract Big-O notation into a living chart, where \\(O(\\log n)\\) feels tiny, \\(O(n^2)\\) feels heavy, and \\(O(2^n)\\) feels impossible.\n\n\n\n18 Space–Time Tradeoff Explorer\nA Space–Time Tradeoff Explorer helps us understand one of the most fundamental balances in algorithm design: using more memory to gain speed, or saving memory at the cost of time. It’s the art of finding equilibrium between storage and computation.\n\nWhat Problem Are We Solving?\nWe often face a choice:\n\nPrecompute and store results for instant access (more space, less time)\nCompute on demand to save memory (less space, more time)\n\nThe goal is to analyze both sides and choose the best fit for the problem’s constraints.\n\n\nHow It Works (Plain Language)\n\nIdentify repeated computations that can be stored.\nEstimate memory cost of storing precomputed data.\nEstimate time saved per query or reuse.\nCompare tradeoffs using total cost models: \\[\n\\text{Total Cost} = \\text{Time Cost} + \\lambda \\cdot \\text{Space Cost}\n\\] where \\(\\lambda\\) reflects system priorities.\nDecide whether caching, tabulation, or recomputation is preferable.\n\nYou’re tuning performance with two dials, one for memory, one for time.\n\n\nExample Step by Step\nExample 1: Fibonacci Numbers\n\nRecursive (no memory): \\(O(2^n)\\) time, \\(O(1)\\) space\nMemoized: \\(O(n)\\) time, \\(O(n)\\) space\nIterative (tabulated): \\(O(n)\\) time, \\(O(1)\\) space (store only last two)\n\nDifferent tradeoffs for the same problem.\nExample 2: Lookup Tables\nSuppose you need \\(\\sin(x)\\) for many \\(x\\) values:\n\nCompute each time → \\(O(n)\\) per query\nStore all results → \\(O(n)\\) memory, \\(O(1)\\) lookup\nHybrid: store sampled points, interpolate → balance\n\n\n\nTiny Code (Python)\ndef fib_naive(n):\n    if n &lt;= 1: return n\n    return fib_naive(n-1) + fib_naive(n-2)\n\ndef fib_memo(n, memo={}):\n    if n in memo: return memo[n]\n    if n &lt;= 1: return n\n    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)\n    return memo[n]\nCompare time vs memory for each version.\n\n\nWhy It Matters\n\nHelps design algorithms under memory limits or real-time constraints\nEssential in databases, graphics, compilers, and AI caching\nConnects theory (asymptotics) to engineering (resources)\nPromotes thinking in trade curves, not absolutes\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) = time, \\(S(n)\\) = space. If we precompute \\(k\\) results, \\[\nT'(n) = T(n) - \\Delta T, \\quad S'(n) = S(n) + \\Delta S\n\\]\nSince \\(\\Delta T\\) and \\(\\Delta S\\) are usually monotonic, minimizing one increases the other. Thus, the optimal configuration lies where \\[\n\\frac{dT}{dS} = -\\lambda\n\\] reflecting the system’s valuation of time vs memory.\n\n\nTry It Yourself\n\nCompare naive vs memoized vs iterative Fibonacci.\nBuild a lookup table for factorials modulo \\(M\\).\nExplore DP tabulation (space-heavy) vs rolling array (space-light).\nEvaluate caching in a recursive tree traversal.\n\n\n\nTest Cases\n\n\n\nProblem\nSpace\nTime\nStrategy\n\n\n\n\nFibonacci\n\\(O(1)\\)\n\\(O(2^n)\\)\nNaive recursion\n\n\nFibonacci\n\\(O(n)\\)\n\\(O(n)\\)\nMemoization\n\n\nFibonacci\n\\(O(1)\\)\n\\(O(n)\\)\nIterative\n\n\nLookup Table\n\\(O(n)\\)\n\\(O(1)\\)\nPrecompute\n\n\nRecompute\n\\(O(1)\\)\n\\(O(n)\\)\nOn-demand\n\n\n\n\n\nComplexity\n\n\n\nOperation\nDimension\nNote\n\n\n\n\nSpace–Time Analysis\n\\(O(1)\\)\nConceptual\n\n\nOptimization\n\\(O(1)\\)\nTradeoff curve\n\n\n\nA Space–Time Tradeoff Explorer turns resource limits into creative levers, helping you choose when to remember, when to recompute, and when to balance both in harmony.\n\n\n\n19 Profiling Algorithm\nProfiling an algorithm means measuring how it actually behaves, how long it runs, how much memory it uses, how often loops iterate, and where time is really spent. It turns theoretical complexity into real performance data.\n\nWhat Problem Are We Solving?\nBig-O tells us how an algorithm scales, but not how it performs in practice. Constant factors, system load, compiler optimizations, and cache effects all matter.\nProfiling answers:\n\nWhere is the time going?\nWhich function dominates?\nAre we bound by CPU, memory, or I/O?\n\nIt’s the microscope for runtime behavior.\n\n\nHow It Works (Plain Language)\n\nInstrument your code, insert timers, counters, or use built-in profilers.\nRun with representative inputs.\nRecord runtime, call counts, and memory allocations.\nAnalyze hotspots, the 10% of code causing 90% of cost.\nOptimize only where it matters.\n\nProfiling doesn’t guess, it measures.\n\n\nExample Step by Step\n\n\nExample 1: Timing a Function\nimport time\n\nstart = time.perf_counter()\nresult = algorithm(n)\nend = time.perf_counter()\n\nprint(\"Elapsed:\", end - start)\nMeasure total runtime for a given input size.\n\n\nExample 2: Line-Level Profiling\nimport cProfile, pstats\n\ncProfile.run('algorithm(1000)', 'stats')\np = pstats.Stats('stats')\np.sort_stats('cumtime').print_stats(10)\nShows the 10 most time-consuming functions.\n\n\nTiny Code (Python)\ndef slow_sum(n):\n    s = 0\n    for i in range(n):\n        for j in range(i):\n            s += j\n    return s\n\nimport cProfile\ncProfile.run('slow_sum(500)')\nOutput lists functions, calls, total time, and cumulative time.\n\n\nWhy It Matters\n\nBridges theory (Big-O) and practice (runtime)\nIdentifies bottlenecks for optimization\nValidates expected scaling across inputs\nPrevents premature optimization, measure first, fix later\n\n\n\nA Gentle Proof (Why It Works)\nEvery algorithm execution is a trace of operations. Profiling samples or counts these operations in real time.\nIf \\(t_i\\) is time spent in component \\(i\\), then total runtime \\(T = \\sum_i t_i\\). Ranking \\(t_i\\) reveals the dominant terms empirically, confirming or refuting theoretical assumptions.\n\n\nTry It Yourself\n\nProfile a recursive function (like Fibonacci).\nCompare iterative vs recursive runtimes.\nPlot \\(n\\) vs runtime to visualize empirical complexity.\nUse memory_profiler to capture space usage.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nAlgorithm\nExpected\nObserved (example)\nNotes\n\n\n\n\nLinear Search\n\\(O(n)\\)\nruntime ∝ \\(n\\)\nscales linearly\n\n\nMerge Sort\n\\(O(n \\log n)\\)\nruntime grows slightly faster than \\(n\\)\nmerge overhead\n\n\nNaive Fibonacci\n\\(O(2^n)\\)\nexplodes at \\(n&gt;30\\)\nconfirms exponential cost\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nProfiling Run\n\\(O(n)\\) (per trial)\n\\(O(1)\\)\n\n\nReport Generation\n\\(O(f)\\) (per function)\n\\(O(f)\\)\n\n\n\nProfiling is where math meets the stopwatch, transforming asymptotic guesses into concrete numbers and revealing the true heartbeat of your algorithm.\n\n\n\n20 Benchmarking Framework\nA Benchmarking Framework provides a structured way to compare algorithms under identical conditions. It measures performance across input sizes, multiple trials, and varying hardware, revealing which implementation truly performs best in practice.\n\nWhat Problem Are We Solving?\nYou’ve got several algorithms solving the same problem — which one is actually faster? Which scales better? Which uses less memory?\nBenchmarking answers these questions with fair, repeatable experiments instead of intuition or isolated timing tests.\n\n\nHow It Works (Plain Language)\n\nDefine test cases (input sizes, data patterns).\nRun all candidate algorithms under the same conditions.\nRepeat trials to reduce noise.\nRecord metrics:\n\nRuntime\nMemory usage\nThroughput or latency\n\nAggregate results and visualize trends.\n\nThink of it as a “tournament” where each algorithm plays by the same rules.\n\n\nExample Step by Step\nSuppose we want to benchmark sorting methods:\n\nInputs: random arrays of sizes \\(10^3\\), \\(10^4\\), \\(10^5\\)\nAlgorithms: bubble_sort, merge_sort, timsort\nMetric: average runtime over 5 runs\nResult: table or plot\n\n\n\n\nSize\nBubble Sort\nMerge Sort\nTimsort\n\n\n\n\n\\(10^3\\)\n0.05s\n0.001s\n0.0008s\n\n\n\\(10^4\\)\n5.4s\n0.02s\n0.012s\n\n\n\\(10^5\\)\n–\n0.25s\n0.15s\n\n\n\nTimsort wins across all sizes, data confirms theory.\n\n\nTiny Code (Python)\nimport timeit\nimport random\n\ndef bench(func, n, trials=5):\n    data = [random.randint(0, n) for _ in range(n)]\n    return min(timeit.repeat(lambda: func(data.copy()), number=1, repeat=trials))\n\ndef bubble_sort(arr):\n    for i in range(len(arr)):\n        for j in range(len(arr)-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n\ndef merge_sort(arr):\n    if len(arr) &lt;= 1: return arr\n    mid = len(arr)//2\n    return merge(merge_sort(arr[:mid]), merge_sort(arr[mid:]))\n\ndef merge(left, right):\n    result = []\n    while left and right:\n        result.append(left.pop(0) if left[0] &lt; right[0] else right.pop(0))\n    return result + left + right\n\nprint(\"Bubble:\", bench(bubble_sort, 1000))\nprint(\"Merge:\", bench(merge_sort, 1000))\n\n\nWhy It Matters\n\nConverts abstract complexity into empirical performance\nSupports evidence-based optimization\nDetects constant factor effects Big-O hides\nEnsures fair comparisons across algorithms\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(t_{i,j}\\) be time of algorithm \\(i\\) on trial \\(j\\). Benchmarking reports \\(\\min\\), \\(\\max\\), or \\(\\text{mean}(t_{i,*})\\).\nBy controlling conditions (hardware, input distribution), we treat \\(t_{i,j}\\) as samples of the same distribution, allowing valid comparisons of \\(E[t_i]\\) (expected runtime). Hence, results reflect true relative performance.\n\n\nTry It Yourself\n\nBenchmark linear vs binary search on sorted arrays.\nTest dynamic array insertion vs linked list insertion.\nRun across input sizes \\(10^3\\), \\(10^4\\), \\(10^5\\).\nPlot results: \\(n\\) (x-axis) vs time (y-axis).\n\n\n\nTest Cases\n\n\n\nComparison\nExpectation\n\n\n\n\nBubble vs Merge\nMerge faster after small \\(n\\)\n\n\nLinear vs Binary Search\nBinary faster for \\(n &gt; 100\\)\n\n\nList vs Dict lookup\nDict \\(O(1)\\) outperforms List \\(O(n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nRun Each Trial\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nAggregate Results\n\\(O(k)\\)\n\\(O(k)\\)\n\n\nTotal Benchmark\n\\(O(nk)\\)\n\\(O(1)\\)\n\n\n\n(\\(k\\) = number of trials)\nA Benchmarking Framework transforms comparison into science, fair tests, real data, and performance truths grounded in experiment, not hunch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-3.-big-o-big-theta-big-omega",
    "href": "books/en-us/list-1.html#section-3.-big-o-big-theta-big-omega",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 3. Big-O, Big-Theta, Big-Omega",
    "text": "Section 3. Big-O, Big-Theta, Big-Omega\n\n21 Growth Rate Comparator\nA Growth Rate Comparator helps us see how functions grow relative to each other, the backbone of asymptotic reasoning. It lets us answer questions like: does \\(n^2\\) outgrow \\(n \\log n\\)? How fast is \\(2^n\\) compared to \\(n!\\)?\n\nWhat Problem Are We Solving?\nWe need a clear way to compare how fast two functions increase as \\(n\\) becomes large. When analyzing algorithms, runtime functions like \\(n\\), \\(n \\log n\\), and \\(n^2\\) all seem similar at small scales, but their growth rates diverge quickly.\nA comparator gives us a mathematical and visual way to rank them.\n\n\nHow It Works (Plain Language)\n\nWrite the two functions \\(f(n)\\) and \\(g(n)\\).\nCompute the ratio \\(\\dfrac{f(n)}{g(n)}\\) as \\(n \\to \\infty\\).\nInterpret the result:\n\nIf \\(\\dfrac{f(n)}{g(n)} \\to 0\\) → \\(f(n) = o(g(n))\\) (grows slower)\nIf \\(\\dfrac{f(n)}{g(n)} \\to c &gt; 0\\) → \\(f(n) = \\Theta(g(n))\\) (same growth)\nIf \\(\\dfrac{f(n)}{g(n)} \\to \\infty\\) → \\(f(n) = \\omega(g(n))\\) (grows faster)\n\n\nThis ratio test tells us which function dominates for large \\(n\\).\n\n\nExample Step by Step\nExample 1: Compare \\(n \\log n\\) vs \\(n^2\\)\n\\[\n\\frac{n \\log n}{n^2} = \\frac{\\log n}{n}\n\\]\nAs \\(n \\to \\infty\\), \\(\\frac{\\log n}{n} \\to 0\\) → \\(n \\log n = o(n^2)\\)\nExample 2: Compare \\(2^n\\) vs \\(n!\\)\n\\[\n\\frac{2^n}{n!} \\to 0\n\\]\nsince \\(n!\\) grows faster than \\(2^n\\). → \\(2^n = o(n!)\\)\n\n\nTiny Code (Python)\nimport math\n\ndef compare_growth(f, g, ns):\n    for n in ns:\n        ratio = f(n)/g(n)\n        print(f\"n={n:6}, ratio={ratio:.6e}\")\n\ncompare_growth(lambda n: n*math.log2(n),\n               lambda n: n2,\n               [10, 100, 1000, 10000])\nOutput shows ratio shrinking → confirms slower growth.\n\n\nWhy It Matters\n\nBuilds intuition for asymptotic dominance\nEssential for Big-O, Big-Theta, Big-Omega proofs\nClarifies why some algorithms scale better\nTranslates math into visual and numerical comparisons\n\n\n\nA Gentle Proof (Why It Works)\nBy definition of asymptotic notation:\nIf \\(\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\\), then for any \\(\\varepsilon &gt; 0\\), \\(f(n) &lt; \\varepsilon g(n)\\) for large \\(n\\).\nThus, \\(f(n)\\) grows slower than \\(g(n)\\).\nThis formal limit test underlies Big-O reasoning.\n\n\nTry It Yourself\n\nCompare \\(n^3\\) vs \\(2^n\\)\nCompare \\(\\sqrt{n}\\) vs \\(\\log n\\)\nCompare \\(n!\\) vs \\(n^n\\)\nPlot both functions and see where one overtakes the other\n\n\n\nTest Cases\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nResult\nRelation\n\n\n\n\n\\(\\log n\\)\n\\(\\sqrt{n}\\)\n\\(0\\)\n\\(o(\\sqrt{n})\\)\n\n\n\\(n\\)\n\\(n \\log n\\)\n\\(0\\)\n\\(o(n \\log n)\\)\n\n\n\\(n^2\\)\n\\(2^n\\)\n\\(0\\)\n\\(o(2^n)\\)\n\n\n\\(2^n\\)\n\\(n!\\)\n\\(0\\)\n\\(o(n!)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nComparison\n\\(O(1)\\) per pair\n\\(O(1)\\)\n\n\n\nA Growth Rate Comparator turns asymptotic theory into a conversation, showing, with numbers and limits, who really grows faster as \\(n\\) climbs toward infinity.\n\n\n\n22 Dominant Term Extractor\nA Dominant Term Extractor simplifies complexity expressions by identifying which term matters most as \\(n\\) grows large. It’s how we turn messy runtime formulas into clean Big-O notation, by keeping only what truly drives growth.\n\nWhat Problem Are We Solving?\nAlgorithms often produce composite cost formulas like \\[\nT(n) = 3n^2 + 10n + 25\n\\] Not all terms grow equally. The dominant term determines long-run behavior, so we want to isolate it and discard the rest.\nThis step bridges detailed operation counting and asymptotic notation.\n\n\nHow It Works (Plain Language)\n\nWrite the runtime function \\(T(n)\\) (from counting steps).\nList all terms by their growth type (\\(n^3\\), \\(n^2\\), \\(n\\), \\(\\log n\\), constants).\nFind the fastest-growing term as \\(n \\to \\infty\\).\nDrop coefficients and lower-order terms.\nThe result is the Big-O class.\n\nThink of it as zooming out on a curve, smaller waves vanish at infinity.\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = 5n^3 + 2n^2 + 7n + 12\n\\]\nFor large \\(n\\), \\(n^3\\) dominates.\nSo: \\[\nT(n) = O(n^3)\n\\]\nExample 2: \\[\nT(n) = n^2 + n\\log n + 10n\n\\]\nCompare term by term: \\[\nn^2 &gt; n \\log n &gt; n\n\\]\nSo dominant term is \\(n^2\\). \\(\\Rightarrow T(n) = O(n^2)\\)\n\n\nTiny Code (Python)\ndef dominant_term(terms):\n    growth_order = {'1': 0, 'logn': 1, 'n': 2, 'nlogn': 3, 'n^2': 4, 'n^3': 5, '2^n': 6}\n    return max(terms, key=lambda t: growth_order[t])\n\nprint(dominant_term(['n^2', 'nlogn', 'n']))  # n^2\nYou can extend this with symbolic simplification using SymPy.\n\n\nWhy It Matters\n\nSimplifies detailed formulas into clean asymptotics\nFocuses attention on scaling behavior, not constants\nMakes performance comparison straightforward\nCore step in deriving Big-O from raw step counts\n\n\n\nA Gentle Proof (Why It Works)\nLet \\[\nT(n) = a_k n^k + a_{k-1} n^{k-1} + \\dots + a_0\n\\]\nAs \\(n \\to \\infty\\), \\[\n\\frac{a_{k-1} n^{k-1}}{a_k n^k} = \\frac{a_{k-1}}{a_k n} \\to 0\n\\]\nAll lower-order terms vanish relative to the largest exponent. So \\(T(n) = \\Theta(n^k)\\).\nThis generalizes beyond polynomials to any family of functions with strict growth ordering.\n\n\nTry It Yourself\n\nSimplify \\(T(n) = 4n \\log n + 10n + 100\\).\nSimplify \\(T(n) = 2n^3 + 50n^2 + 1000\\).\nSimplify \\(T(n) = 5n + 10\\log n + 100\\).\nVerify using ratio test: \\(\\frac{\\text{lower term}}{\\text{dominant term}} \\to 0\\).\n\n\n\nTest Cases\n\n\n\nExpression\nDominant Term\nBig-O\n\n\n\n\n\\(3n^2 + 4n + 10\\)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\n\\(5n + 8\\log n + 7\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(n \\log n + 100n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(4n^3 + n^2 + 2n\\)\n\\(n^3\\)\n\\(O(n^3)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nExtraction\n\\(O(k)\\)\n\\(O(1)\\)\n\n\n\n(\\(k\\) = number of terms)\nA Dominant Term Extractor is like a spotlight, it shines on the one term that decides the pace, letting you see the true asymptotic character of your algorithm.\n\n\n\n23 Limit-Based Complexity Test\nThe Limit-Based Complexity Test is a precise way to compare how fast two functions grow by using limits. It’s a mathematical tool that turns intuition (“this one feels faster”) into proof (“this one is faster”).\n\nWhat Problem Are We Solving?\nWhen analyzing algorithms, we often ask: Does \\(f(n)\\) grow slower, equal, or faster than \\(g(n)\\)? Instead of guessing, we use limits to determine the exact relationship and classify them using Big-O, \\(\\Theta\\), or \\(\\Omega\\).\nThis method gives a formal and reliable comparison of growth rates.\n\n\nHow It Works (Plain Language)\n\nStart with two positive functions \\(f(n)\\) and \\(g(n)\\).\nCompute the ratio: \\[\nL = \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)}\n\\]\nInterpret the limit:\n\nIf \\(L = 0\\), then \\(f(n) = o(g(n))\\) → \\(f\\) grows slower.\nIf \\(0 &lt; L &lt; \\infty\\), then \\(f(n) = \\Theta(g(n))\\) → same growth rate.\nIf \\(L = \\infty\\), then \\(f(n) = \\omega(g(n))\\) → \\(f\\) grows faster.\n\n\nThe ratio tells us how one function “scales” relative to another.\n\n\nExample Step by Step\nExample 1:\nCompare \\(f(n) = n \\log n\\) and \\(g(n) = n^2\\).\n\\[\n\\frac{f(n)}{g(n)} = \\frac{n \\log n}{n^2} = \\frac{\\log n}{n}\n\\]\nAs \\(n \\to \\infty\\), \\(\\frac{\\log n}{n} \\to 0\\). So \\(n \\log n = o(n^2)\\) → grows slower.\nExample 2:\nCompare \\(f(n) = 3n^2 + 4n\\) and \\(g(n) = n^2\\).\n\\[\n\\frac{f(n)}{g(n)} = \\frac{3n^2 + 4n}{n^2} = 3 + \\frac{4}{n}\n\\]\nAs \\(n \\to \\infty\\), \\(\\frac{4}{n} \\to 0\\). So \\(\\lim = 3\\), constant and positive. Therefore, \\(f(n) = \\Theta(g(n))\\).\n\n\nTiny Code (Python)\nimport sympy as sp\n\nn = sp.symbols('n', positive=True)\nf = n * sp.log(n)\ng = n2\nL = sp.limit(f/g, n, sp.oo)\nprint(\"Limit:\", L)\nOutputs 0, confirming \\(n \\log n = o(n^2)\\).\n\n\nWhy It Matters\n\nProvides formal proof of asymptotic relationships\nEliminates guesswork in comparing growth rates\nCore step in Big-O proofs and recurrence analysis\nHelps verify if approximations are valid\n\n\n\nA Gentle Proof (Why It Works)\nThe definition of asymptotic comparison uses limits:\nIf \\(\\displaystyle \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\\), then for any \\(\\varepsilon &gt; 0\\), \\(\\exists N\\) such that \\(\\forall n &gt; N\\), \\(f(n) \\le \\varepsilon g(n)\\).\nThis satisfies the formal condition for \\(f(n) = o(g(n))\\). Similarly, constant or infinite limits define \\(\\Theta\\) and \\(\\omega\\).\n\n\nTry It Yourself\n\nCompare \\(n^3\\) vs \\(2^n\\).\nCompare \\(\\sqrt{n}\\) vs \\(\\log n\\).\nCompare \\(n!\\) vs \\(n^n\\).\nCheck ratio for \\(n^2 + n\\) vs \\(n^2\\).\n\n\n\nTest Cases\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nLimit\nRelationship\n\n\n\n\n\\(n\\)\n\\(n \\log n\\)\n0\n\\(o(g(n))\\)\n\n\n\\(n^2 + n\\)\n\\(n^2\\)\n1\n\\(\\Theta(g(n))\\)\n\n\n\\(2^n\\)\n\\(n^3\\)\n\\(\\infty\\)\n\\(\\omega(g(n))\\)\n\n\n\\(\\log n\\)\n\\(\\sqrt{n}\\)\n0\n\\(o(g(n))\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLimit Evaluation\n\\(O(1)\\) symbolic\n\\(O(1)\\)\n\n\n\nThe Limit-Based Complexity Test is your mathematical magnifying glass, a clean, rigorous way to compare algorithmic growth and turn asymptotic intuition into certainty.\n\n\n\n24 Summation Simplifier\nA Summation Simplifier converts loops and recursive cost expressions into closed-form formulas using algebra and known summation rules. It bridges the gap between raw iteration counts and Big-O notation.\n\nWhat Problem Are We Solving?\nWhen analyzing loops, we often get total work expressed as a sum:\n\\[\nT(n) = \\sum_{i=1}^{n} i \\quad \\text{or} \\quad T(n) = \\sum_{i=1}^{n} \\log i\n\\]\nBut Big-O requires us to simplify these sums into familiar functions of \\(n\\). Summation simplification transforms iteration patterns into asymptotic form.\n\n\nHow It Works (Plain Language)\n\nWrite down the summation from your loop or recurrence.\nApply standard formulas or approximations:\n\n\\(\\sum_{i=1}^{n} 1 = n\\)\n\\(\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}\\)\n\\(\\sum_{i=1}^{n} i^2 = \\frac{n(n+1)(2n+1)}{6}\\)\n\\(\\sum_{i=1}^{n} \\log i = O(n \\log n)\\)\n\nDrop constants and lower-order terms.\nReturn simplified function \\(f(n)\\) → then apply Big-O.\n\nIt’s like algebraic compression for iteration counts.\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = \\sum_{i=1}^{n} i\n\\] Use formula: \\[\nT(n) = \\frac{n(n+1)}{2}\n\\] Simplify: \\[\nT(n) = O(n^2)\n\\]\nExample 2: \\[\nT(n) = \\sum_{i=1}^{n} \\log i\n\\] Approximate by integral: \\[\n\\int_1^n \\log x , dx = n \\log n - n + 1\n\\] So \\(T(n) = O(n \\log n)\\)\nExample 3: \\[\nT(n) = \\sum_{i=1}^{n} \\frac{1}{i}\n\\] ≈ \\(\\log n\\) (Harmonic series)\n\n\nTiny Code (Python)\nimport sympy as sp\n\ni, n = sp.symbols('i n', positive=True)\nexpr = sp.summation(i, (i, 1, n))\nprint(sp.simplify(expr))  # n*(n+1)/2\nOr use sp.summation(sp.log(i), (i,1,n)) for logarithmic sums.\n\n\nWhy It Matters\n\nConverts nested loops into analyzable formulas\nCore tool in time complexity derivation\nHelps visualize how cumulative work builds up\nConnects discrete steps with continuous approximations\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(f(i)\\) is positive and increasing, then by the integral test:\n\\[\n\\int_1^n f(x),dx \\le \\sum_{i=1}^n f(i) \\le f(n) + \\int_1^n f(x),dx\n\\]\nSo for asymptotic purposes, \\(\\sum f(i)\\) and \\(\\int f(x)\\) grow at the same rate.\nThis equivalence justifies approximations like \\(\\sum \\log i = O(n \\log n)\\).\n\n\nTry It Yourself\n\nSimplify \\(\\sum_{i=1}^n i^3\\).\nSimplify \\(\\sum_{i=1}^n \\sqrt{i}\\).\nSimplify \\(\\sum_{i=1}^n \\frac{1}{i^2}\\).\nApproximate \\(\\sum_{i=1}^{n/2} i\\) using integrals.\n\n\n\nTest Cases\n\n\n\nSummation\nFormula\nBig-O\n\n\n\n\n\\(\\sum 1\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(\\sum i\\)\n\\(\\frac{n(n+1)}{2}\\)\n\\(O(n^2)\\)\n\n\n\\(\\sum i^2\\)\n\\(\\frac{n(n+1)(2n+1)}{6}\\)\n\\(O(n^3)\\)\n\n\n\\(\\sum \\log i\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(\\sum \\frac{1}{i}\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSimplification\n\\(O(1)\\) (formula lookup)\n\\(O(1)\\)\n\n\n\nA Summation Simplifier turns looping arithmetic into elegant formulas, the difference between counting steps and seeing the shape of growth.\n\n\n\n25 Recurrence Tree Method\nThe Recurrence Tree Method is a visual technique for solving divide-and-conquer recurrences. It expands the recursive formula into a tree of subproblems, sums the work done at each level, and reveals the total cost.\n\nWhat Problem Are We Solving?\nMany recursive algorithms (like Merge Sort or Quick Sort) define their running time as \\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\] where:\n\n\\(a\\) = number of subproblems,\n\\(b\\) = size reduction factor,\n\\(f(n)\\) = non-recursive work per call.\n\nThe recurrence tree lets us see the full cost by summing over levels instead of applying a closed-form theorem immediately.\n\n\nHow It Works (Plain Language)\n\nDraw the recursion tree\n\nRoot: problem of size \\(n\\), cost \\(f(n)\\).\nEach node: subproblem of size \\(\\frac{n}{b}\\) with cost \\(f(\\frac{n}{b})\\).\n\nExpand levels until base case (\\(n=1\\)).\nSum work per level:\n\nLevel \\(i\\) has \\(a^i\\) nodes, each size \\(\\frac{n}{b^i}\\).\nTotal work at level \\(i\\): \\[\nW_i = a^i \\cdot f!\\left(\\frac{n}{b^i}\\right)\n\\]\n\nAdd all levels: \\[\nT(n) = \\sum_{i=0}^{\\log_b n} W_i\n\\]\nIdentify the dominant level (top, middle, or bottom).\nSimplify to Big-O form.\n\n\n\nExample Step by Step\nTake Merge Sort:\n\\[\nT(n) = 2T!\\left(\\frac{n}{2}\\right) + n\n\\]\nLevel 0: \\(1 \\times n = n\\) Level 1: \\(2 \\times \\frac{n}{2} = n\\) Level 2: \\(4 \\times \\frac{n}{4} = n\\) ⋯ Depth: \\(\\log_2 n\\) levels\nTotal work: \\[\nT(n) = n \\log_2 n + n = O(n \\log n)\n\\]\nEvery level costs \\(n\\), total = \\(n \\times \\log n\\).\n\n\nTiny Code (Python)\nimport math\n\ndef recurrence_tree(a, b, f, n):\n    total = 0\n    level = 0\n    while n &gt;= 1:\n        work = (alevel) * f(n/(blevel))\n        total += work\n        level += 1\n        n /= b\n    return total\nUse f = lambda x: x for \\(f(n) = n\\).\n\n\nWhy It Matters\n\nMakes recurrence structure visible and intuitive\nExplains why Master Theorem results hold\nHighlights dominant levels (top-heavy vs bottom-heavy)\nGreat teaching and reasoning tool for recursive cost breakdown\n\n\n\nA Gentle Proof (Why It Works)\nEach recursive call contributes \\(f(n)\\) work plus child subcalls. Because each level’s subproblems have equal size, total cost is additive:\n\\[\nT(n) = \\sum_{i=0}^{\\log_b n} a^i f!\\left(\\frac{n}{b^i}\\right)\n\\]\nDominant level dictates asymptotic order:\n\nTop-heavy: \\(f(n)\\) dominates → \\(O(f(n))\\)\nBalanced: all levels equal → \\(O(f(n) \\log n)\\)\nBottom-heavy: leaves dominate → \\(O(n^{\\log_b a})\\)\n\nThis reasoning leads directly to the Master Theorem.\n\n\nTry It Yourself\n\nBuild tree for \\(T(n) = 3T(n/2) + n\\).\nSum each level’s work.\nCompare with Master Theorem result.\nTry \\(T(n) = T(n/2) + 1\\) (logarithmic tree).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nRecurrence\nLevel Work\nLevels\nTotal\nBig-O\n\n\n\n\n\\(2T(n/2)+n\\)\n\\(n\\)\n\\(\\log n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(T(n/2)+1\\)\n\\(1\\)\n\\(\\log n\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\\(4T(n/2)+n\\)\n\\(a^i = 4^i\\), work = \\(n \\cdot 2^i\\)\nbottom dominates\n\\(O(n^2)\\)\n\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nTree Construction\n\\(O(\\log n)\\) levels\n\\(O(\\log n)\\)\n\n\n\nThe Recurrence Tree Method turns abstract formulas into living diagrams, showing each layer’s effort, revealing which level truly drives the algorithm’s cost.\n\n\n\n26 Master Theorem Evaluator\nThe Master Theorem Evaluator gives a quick, formula-based way to solve divide-and-conquer recurrences of the form \\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\] It tells you the asymptotic behavior of \\(T(n)\\) without full expansion or summation, a shortcut born from the recurrence tree.\n\nWhat Problem Are We Solving?\nWe want to find the Big-O complexity of divide-and-conquer algorithms quickly. Manually expanding recursions (via recurrence trees) works, but is tedious. The Master Theorem classifies solutions by comparing the recursive work (\\(a , T(n/b)\\)) and non-recursive work (\\(f(n)\\)).\n\n\nHow It Works (Plain Language)\nGiven \\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\]\n\n\\(a\\) = number of subproblems\n\\(b\\) = shrink factor\n\\(f(n)\\) = work done outside recursion\n\nCompute critical exponent: \\[\nn^{\\log_b a}\n\\]\nCompare \\(f(n)\\) to \\(n^{\\log_b a}\\):\n\nCase 1 (Top-heavy): If \\(f(n) = O(n^{\\log_b a - \\varepsilon})\\), \\[T(n) = \\Theta(n^{\\log_b a})\\] Recursive part dominates.\nCase 2 (Balanced): If \\(f(n) = \\Theta(n^{\\log_b a} \\log^k n)\\), \\[T(n) = \\Theta(n^{\\log_b a} \\log^{k+1} n)\\] Both contribute equally.\nCase 3 (Bottom-heavy): If \\(f(n) = \\Omega(n^{\\log_b a + \\varepsilon})\\) and regularity condition holds: \\[a f(n/b) \\le c f(n)\\] for some \\(c&lt;1\\), then \\[T(n) = \\Theta(f(n))\\] Non-recursive part dominates.\n\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = 2T(n/2) + n\n\\]\n\n\\(a = 2\\), \\(b = 2\\), \\(f(n) = n\\)\n\\(n^{\\log_2 2} = n\\) So \\(f(n) = \\Theta(n^{\\log_2 2})\\) → Case 2\n\n\\[\nT(n) = \\Theta(n \\log n)\n\\]\nExample 2: \\[\nT(n) = 4T(n/2) + n\n\\]\n\n\\(a = 4\\), \\(b = 2\\) → \\(n^{\\log_2 4} = n^2\\)\n\\(f(n) = n = O(n^{2 - \\varepsilon})\\) → Case 1\n\n\\[\nT(n) = \\Theta(n^2)\n\\]\nExample 3: \\[\nT(n) = T(n/2) + n\n\\]\n\n\\(a=1\\), \\(b=2\\), \\(n^{\\log_2 1}=1\\)\n\\(f(n)=n = \\Omega(n^{0+\\varepsilon})\\) → Case 3\n\n\\[\nT(n) = \\Theta(n)\n\\]\n\n\nTiny Code (Python)\nimport math\n\ndef master_theorem(a, b, f_exp):\n    critical = math.log(a, b)\n    if f_exp &lt; critical:\n        return f\"O(n^{critical:.2f})\"\n    elif f_exp == critical:\n        return f\"O(n^{critical:.2f} log n)\"\n    else:\n        return f\"O(n^{f_exp})\"\nFor \\(T(n) = 2T(n/2) + n\\), call master_theorem(2,2,1) → O(n log n)\n\n\nWhy It Matters\n\nSolves recurrences in seconds\nFoundation for analyzing divide-and-conquer algorithms\nValidates intuition from recurrence trees\nUsed widely in sorting, searching, matrix multiplication, FFT\n\n\n\nA Gentle Proof (Why It Works)\nEach recursion level costs: \\[a^i , f!\\left(\\frac{n}{b^i}\\right)\\]\nTotal cost: \\[T(n) = \\sum_{i=0}^{\\log_b n} a^i f!\\left(\\frac{n}{b^i}\\right)\\]\nThe relative growth of \\(f(n)\\) to \\(n^{\\log_b a}\\) determines which level dominates, top, middle, or bottom, yielding the three canonical cases.\n\n\nTry It Yourself\n\n\\(T(n) = 3T(n/2) + n\\)\n\\(T(n) = 2T(n/2) + n^2\\)\n\\(T(n) = 8T(n/2) + n^3\\)\nIdentify \\(a, b, f(n)\\) and apply theorem.\n\n\n\nTest Cases\n\n\n\nRecurrence\nCase\nResult\n\n\n\n\n\\(2T(n/2)+n\\)\n2\n\\(O(n \\log n)\\)\n\n\n\\(4T(n/2)+n\\)\n1\n\\(O(n^2)\\)\n\n\n\\(T(n/2)+n\\)\n3\n\\(O(n)\\)\n\n\n\\(3T(n/3)+n\\log n\\)\n2\n\\(O(n\\log^2 n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nEvaluation\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Master Theorem Evaluator is your formulaic compass, it points instantly to the asymptotic truth hidden in recursive equations, no tree-drawing required.\n\n\n\n27 Big-Theta Proof Builder\nA Big-Theta Proof Builder helps you formally prove that a function grows at the same rate as another. It’s the precise way to show that \\(f(n)\\) and \\(g(n)\\) are asymptotically equivalent, growing neither faster nor slower beyond constant factors.\n\nWhat Problem Are We Solving?\nWe often say an algorithm is \\(T(n) = \\Theta(n \\log n)\\), but how do we prove it? A Big-Theta proof uses inequalities to pin \\(T(n)\\) between two scaled versions of a simpler function \\(g(n)\\), confirming tight asymptotic bounds.\nThis transforms intuition into rigorous evidence.\n\n\nHow It Works (Plain Language)\nWe say \\[\nf(n) = \\Theta(g(n))\n\\] if there exist constants \\(c_1, c_2 &gt; 0\\) and \\(n_0\\) such that for all \\(n \\ge n_0\\): \\[\nc_1 g(n) \\le f(n) \\le c_2 g(n)\n\\]\nSo \\(f(n)\\) is sandwiched between two constant multiples of \\(g(n)\\).\nSteps:\n\nIdentify \\(f(n)\\) and candidate \\(g(n)\\).\nFind constants \\(c_1\\), \\(c_2\\), and threshold \\(n_0\\).\nVerify inequality for all \\(n \\ge n_0\\).\nConclude \\(f(n) = \\Theta(g(n))\\).\n\n\n\nExample Step by Step\nExample 1: \\[\nf(n) = 3n^2 + 10n + 5\n\\] Candidate: \\(g(n) = n^2\\)\nFor large \\(n\\), \\(10n + 5\\) is small compared to \\(3n^2\\).\nWe can show: \\[\n3n^2 \\le 3n^2 + 10n + 5 \\le 4n^2, \\quad \\text{for } n \\ge 10\n\\]\nThus, \\(f(n) = \\Theta(n^2)\\) with \\(c_1 = 3\\), \\(c_2 = 4\\), \\(n_0 = 10\\).\nExample 2: \\[\nf(n) = n \\log n + 100n\n\\] Candidate: \\(g(n) = n \\log n\\)\nFor \\(n \\ge 2\\), \\(\\log n \\ge 1\\), so \\(100n \\le 100n \\log n\\). Hence, \\[\nn \\log n \\le f(n) \\le 101n \\log n\n\\] → \\(f(n) = \\Theta(n \\log n)\\)\n\n\nTiny Code (Python)\ndef big_theta_proof(f, g, n0, c1, c2):\n    for n in range(n0, n0 + 5):\n        if not (c1*g(n) &lt;= f(n) &lt;= c2*g(n)):\n            return False\n    return True\n\nf = lambda n: 3*n2 + 10*n + 5\ng = lambda n: n2\nprint(big_theta_proof(f, g, 10, 3, 4))  # True\n\n\nWhy It Matters\n\nConverts informal claims (“it’s \\(n^2\\)-ish”) into formal proofs\nBuilds rigor in asymptotic reasoning\nEssential for algorithm analysis, recurrence proofs, and coursework\nReinforces understanding of constants and thresholds\n\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\[\nf(n) = \\Theta(g(n)) \\iff \\exists c_1, c_2, n_0 : c_1 g(n) \\le f(n) \\le c_2 g(n)\n\\] This mirrors how Big-O and Big-Omega combine:\n\n\\(f(n) = O(g(n))\\) gives upper bound,\n\\(f(n) = \\Omega(g(n))\\) gives lower bound. Together, they form a tight bound, hence \\(\\Theta\\).\n\n\n\nTry It Yourself\n\nProve \\(5n^3 + n^2 + 100 = \\Theta(n^3)\\).\nProve \\(4n + 10 = \\Theta(n)\\).\nShow \\(n \\log n + 100n = \\Theta(n \\log n)\\).\nFail a proof: \\(n^2 + 3n = \\Theta(n)\\) (not true).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\n\\(c_1, c_2, n_0\\)\nResult\n\n\n\n\n\\(3n^2 + 10n + 5\\)\n\\(n^2\\)\n\\(3,4,10\\)\n\\(\\Theta(n^2)\\)\n\n\n\\(n \\log n + 100n\\)\n\\(n \\log n\\)\n\\(1,101,2\\)\n\\(\\Theta(n \\log n)\\)\n\n\n\\(10n + 50\\)\n\\(n\\)\n\\(10,11,5\\)\n\\(\\Theta(n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nVerification\n\\(O(1)\\) (symbolic)\n\\(O(1)\\)\n\n\n\nThe Big-Theta Proof Builder is your asymptotic courtroom, you bring evidence, constants, and inequalities, and the proof delivers a verdict: \\(\\Theta(g(n))\\), beyond reasonable doubt.\n\n\n\n28 Big-Omega Case Finder\nA Big-Omega Case Finder helps you identify lower bounds on an algorithm’s growth, the guaranteed minimum cost, even in the best-case scenario. It’s the mirror image of Big-O, showing what an algorithm must at least do.\n\nWhat Problem Are We Solving?\nBig-O gives us an upper bound (“it won’t be slower than this”), but sometimes we need to know the floor, a complexity it can never beat.\nBig-Omega helps us state:\n\nThe fastest possible asymptotic behavior, or\nThe minimal cost inherent to the problem itself.\n\nThis is key when analyzing best-case performance or complexity limits (like comparison sorting’s \\(\\Omega(n \\log n)\\) lower bound).\n\n\nHow It Works (Plain Language)\nWe say \\[\nf(n) = \\Omega(g(n))\n\\] if \\(\\exists c &gt; 0, n_0\\) such that \\[\nf(n) \\ge c \\cdot g(n) \\quad \\text{for all } n \\ge n_0\n\\]\nSteps:\n\nIdentify candidate lower-bound function \\(g(n)\\).\nShow \\(f(n)\\) eventually stays above a constant multiple of \\(g(n)\\).\nFind constants \\(c\\) and \\(n_0\\).\nConclude \\(f(n) = \\Omega(g(n))\\).\n\n\n\nExample Step by Step\nExample 1: \\[\nf(n) = 3n^2 + 5n + 10\n\\] Candidate: \\(g(n) = n^2\\)\nFor \\(n \\ge 1\\), \\[\nf(n) \\ge 3n^2 \\ge 3 \\cdot n^2\n\\]\nSo \\(f(n) = \\Omega(n^2)\\) with \\(c = 3\\), \\(n_0 = 1\\).\nExample 2: \\[\nf(n) = n \\log n + 100n\n\\] Candidate: \\(g(n) = n\\)\nSince \\(\\log n \\ge 1\\) for \\(n \\ge 2\\), \\[\nf(n) = n \\log n + 100n \\ge n + 100n = 101n\n\\] → \\(f(n) = \\Omega(n)\\) with \\(c = 101\\), \\(n_0 = 2\\)\n\n\nTiny Code (Python)\ndef big_omega_proof(f, g, n0, c):\n    for n in range(n0, n0 + 5):\n        if f(n) &lt; c * g(n):\n            return False\n    return True\n\nf = lambda n: 3*n2 + 5*n + 10\ng = lambda n: n2\nprint(big_omega_proof(f, g, 1, 3))  # True\n\n\nWhy It Matters\n\nDefines best-case performance\nProvides theoretical lower limits (impossible to beat)\nComplements Big-O (upper bound) and Theta (tight bound)\nKey in proving problem hardness or optimality\n\n\n\nA Gentle Proof (Why It Works)\nIf \\[\n\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = L &gt; 0,\n\\] then for any \\(c \\le L\\), \\(f(n) \\ge c \\cdot g(n)\\) for large \\(n\\). Thus \\(f(n) = \\Omega(g(n))\\). This mirrors the formal definition of \\(\\Omega\\) and follows directly from asymptotic ratio reasoning.\n\n\nTry It Yourself\n\nShow \\(4n^3 + n^2 = \\Omega(n^3)\\)\nShow \\(n \\log n + n = \\Omega(n)\\)\nShow \\(2^n + n^5 = \\Omega(2^n)\\)\nCompare with their Big-O forms for contrast.\n\n\n\nTest Cases\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nConstants\nResult\n\n\n\n\n\\(3n^2 + 10n\\)\n\\(n^2\\)\n\\(c=3\\), \\(n_0=1\\)\n\\(\\Omega(n^2)\\)\n\n\n\\(n \\log n + 100n\\)\n\\(n\\)\n\\(c=101\\), \\(n_0=2\\)\n\\(\\Omega(n)\\)\n\n\n\\(n^3 + n^2\\)\n\\(n^3\\)\n\\(c=1\\), \\(n_0=1\\)\n\\(\\Omega(n^3)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nVerification\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Big-Omega Case Finder shows the floor beneath the curve, ensuring every algorithm stands on a solid lower bound, no matter how fast it tries to run.\n\n\n\n29 Empirical Complexity Estimator\nAn Empirical Complexity Estimator bridges theory and experiment, it measures actual runtimes for various input sizes and fits them to known growth models like \\(O(n)\\), \\(O(n \\log n)\\), or \\(O(n^2)\\). It’s how we discover complexity when the math is unclear or the code is complex.\n\nWhat Problem Are We Solving?\nSometimes the exact formula for \\(T(n)\\) is too messy, or the implementation details are opaque. We can still estimate complexity empirically by observing how runtime changes as \\(n\\) grows.\nThis approach is especially useful for:\n\nBlack-box code (unknown implementation)\nExperimental validation of asymptotic claims\nComparing real-world scaling with theoretical predictions\n\n\n\nHow It Works (Plain Language)\n\nChoose representative input sizes \\(n_1, n_2, \\dots, n_k\\).\nMeasure runtime \\(T(n_i)\\) for each size.\nNormalize or compare ratios:\n\n\\(T(2n)/T(n) \\approx 2\\) → \\(O(n)\\)\n\\(T(2n)/T(n) \\approx 4\\) → \\(O(n^2)\\)\n\\(T(2n)/T(n) \\approx \\log 2\\) → \\(O(\\log n)\\)\n\nFit data to candidate models using regression or ratio tests.\nVisualize trends (e.g., log–log plot) to identify slope = exponent.\n\n\n\nExample Step by Step\nSuppose we test input sizes: \\(n = 1000, 2000, 4000, 8000\\)\n\n\n\n\\(n\\)\n\\(T(n)\\) (ms)\nRatio \\(T(2n)/T(n)\\)\n\n\n\n\n1000\n5\n–\n\n\n2000\n10\n2.0\n\n\n4000\n20\n2.0\n\n\n8000\n40\n2.0\n\n\n\nRatio \\(\\approx 2\\) → linear growth → \\(T(n) = O(n)\\)\nNow suppose:\n\n\n\n\\(n\\)\n\\(T(n)\\)\nRatio\n\n\n\n\n1000\n5\n–\n\n\n2000\n20\n4\n\n\n4000\n80\n4\n\n\n8000\n320\n4\n\n\n\nRatio \\(\\approx 4\\) → quadratic growth → \\(O(n^2)\\)\n\n\nTiny Code (Python)\nimport time, math\n\ndef empirical_estimate(f, ns):\n    times = []\n    for n in ns:\n        start = time.perf_counter()\n        f(n)\n        end = time.perf_counter()\n        times.append(end - start)\n    for i in range(1, len(ns)):\n        ratio = times[i] / times[i-1]\n        print(f\"n={ns[i]:6}, ratio={ratio:.2f}\")\nTest with different algorithms to see scaling.\n\n\nWhy It Matters\n\nConverts runtime data into Big-O form\nDetects bottlenecks or unexpected scaling\nUseful when theoretical analysis is hard\nHelps validate optimizations or refactors\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(T(n) \\approx c \\cdot f(n)\\), then the ratio test \\[\n\\frac{T(kn)}{T(n)} \\approx \\frac{f(kn)}{f(n)}\n\\] reveals the exponent \\(p\\) if \\(f(n) = n^p\\): \\[\n\\frac{f(kn)}{f(n)} = k^p \\implies p = \\log_k \\frac{T(kn)}{T(n)}\n\\]\nRepeated over multiple \\(n\\), this converges to the true growth exponent.\n\n\nTry It Yourself\n\nMeasure runtime of sorting for increasing \\(n\\).\nEstimate \\(p\\) using ratio test.\nPlot \\(\\log n\\) vs \\(\\log T(n)\\), slope ≈ exponent.\nCompare \\(p\\) to theoretical value.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nObserved Ratio\nEstimated Complexity\n\n\n\n\nBubble Sort\n4\n\\(O(n^2)\\)\n\n\nMerge Sort\n2.2\n\\(O(n \\log n)\\)\n\n\nLinear Search\n2\n\\(O(n)\\)\n\n\nBinary Search\n1.1\n\\(O(\\log n)\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nMeasurement\n\\(O(k \\cdot T(n))\\)\n\\(O(k)\\)\n\n\nEstimation\n\\(O(k)\\)\n\\(O(1)\\)\n\n\n\n(\\(k\\) = number of sample points)\nAn Empirical Complexity Estimator transforms stopwatches into science, turning performance data into curves, curves into equations, and equations into Big-O intuition.\n\n\n\n30 Complexity Class Identifier\nA Complexity Class Identifier helps you categorize problems and algorithms into broad complexity classes like constant, logarithmic, linear, quadratic, exponential, or polynomial time. It’s a way to understand where your algorithm lives in the vast map of computational growth.\n\nWhat Problem Are We Solving?\nWhen analyzing an algorithm, we often want to know how big its time cost gets as input grows. Instead of exact formulas, we classify algorithms into families based on their asymptotic growth.\nThis tells us what is feasible (polynomial) and what is explosive (exponential), guiding both design choices and theoretical limits.\n\n\nHow It Works (Plain Language)\nWe map the growth rate of \\(T(n)\\) to a known complexity class:\n\n\n\n\n\n\n\n\nClass\nExample\nDescription\n\n\n\n\n\\(O(1)\\)\nHash lookup\nConstant time, no scaling\n\n\n\\(O(\\log n)\\)\nBinary search\nSublinear, halves each step\n\n\n\\(O(n)\\)\nLinear scan\nWork grows with input size\n\n\n\\(O(n \\log n)\\)\nMerge sort\nNear-linear with log factor\n\n\n\\(O(n^2)\\)\nNested loops\nQuadratic growth\n\n\n\\(O(n^3)\\)\nMatrix multiplication\nCubic growth\n\n\n\\(O(2^n)\\)\nBacktracking\nExponential explosion\n\n\n\\(O(n!)\\)\nBrute-force permutations\nFactorial blowup\n\n\n\nSteps to Identify:\n\nAnalyze loops and recursion structure.\nCount dominant operations.\nMatch pattern to table above.\nVerify with recurrence or ratio test.\nAssign class: constant → logarithmic → polynomial → exponential.\n\n\n\nExample Step by Step\nExample 1: Single loop:\nfor i in range(n):\n    work()\n→ \\(O(n)\\) → Linear\nExample 2: Nested loops:\nfor i in range(n):\n    for j in range(n):\n        work()\n→ \\(O(n^2)\\) → Quadratic\nExample 3: Divide and conquer: \\[\nT(n) = 2T(n/2) + n\n\\] → \\(O(n \\log n)\\) → Log-linear\nExample 4: Brute force subsets: \\[\n2^n \\text{ possibilities}\n\\] → \\(O(2^n)\\) → Exponential\n\n\nTiny Code (Python)\ndef classify_complexity(code_structure):\n    if \"nested n\" in code_structure:\n        return \"O(n^2)\"\n    if \"divide and conquer\" in code_structure:\n        return \"O(n log n)\"\n    if \"constant\" in code_structure:\n        return \"O(1)\"\n    return \"O(n)\"\nYou can extend this to pattern-match pseudocode shapes.\n\n\nWhy It Matters\n\nGives instant intuition about scalability\nGuides design trade-offs (speed vs. simplicity)\nConnects practical code to theoretical limits\nHelps compare algorithms solving the same problem\n\n\n\nA Gentle Proof (Why It Works)\nIf an algorithm performs \\(f(n)\\) fundamental operations for input size \\(n\\), and \\(f(n)\\) is asymptotically similar to a known class \\(g(n)\\): \\[\nf(n) = \\Theta(g(n))\n\\] then it belongs to the same class. Classes form equivalence groups under \\(\\Theta\\) notation, simplifying infinite functions into a finite taxonomy.\n\n\nTry It Yourself\nClassify each:\n\n\\(T(n) = 5n + 10\\)\n\\(T(n) = n \\log n + 100\\)\n\\(T(n) = n^3 + 4n^2\\)\n\\(T(n) = 2^n\\)\n\nIdentify their Big-O class and interpret feasibility.\n\n\nTest Cases\n\n\n\n\\(T(n)\\)\nClass\nDescription\n\n\n\n\n\\(7n + 3\\)\n\\(O(n)\\)\nLinear\n\n\n\\(3n^2 + 10n\\)\n\\(O(n^2)\\)\nQuadratic\n\n\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\nLog-linear\n\n\n\\(2^n\\)\n\\(O(2^n)\\)\nExponential\n\n\n\\(100\\)\n\\(O(1)\\)\nConstant\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nClassification\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Complexity Class Identifier is your map of the algorithmic universe, helping you locate where your code stands, from calm constant time to the roaring infinity of factorial growth.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-4.-algorithm-paradigms",
    "href": "books/en-us/list-1.html#section-4.-algorithm-paradigms",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 4. Algorithm Paradigms",
    "text": "Section 4. Algorithm Paradigms\n\n31 Greedy Coin Example\nThe Greedy Coin Example introduces the greedy algorithm paradigm, solving problems by always taking the best immediate option, hoping it leads to a globally optimal solution. In coin change, we repeatedly pick the largest denomination not exceeding the remaining amount.\n\nWhat Problem Are We Solving?\nWe want to make change for a target amount using the fewest coins possible. A greedy algorithm always chooses the locally optimal coin, the largest denomination ≤ remaining total, and repeats until the target is reached.\nThis method works for canonical coin systems (like U.S. currency) but fails for some arbitrary denominations.\n\n\nHow It Works (Plain Language)\n\nSort available coin denominations in descending order.\nFor each coin:\n\nTake as many as possible without exceeding the total.\nSubtract their value from the remaining amount.\n\nContinue with smaller coins until the remainder is 0.\n\nGreedy assumes: local optimum → global optimum.\n\n\nExample Step by Step\nLet coins = {25, 10, 5, 1}, target = 63\n\n\n\nStep\nCoin\nCount\nRemaining\n\n\n\n\n1\n25\n2\n13\n\n\n2\n10\n1\n3\n\n\n3\n5\n0\n3\n\n\n4\n1\n3\n0\n\n\n\nTotal = 2×25 + 1×10 + 3×1 = 63 Coins used = 6\nGreedy solution = optimal (U.S. system is canonical).\nCounterexample:\nCoins = {4, 3, 1}, target = 6\n\nGreedy: 4 + 1 + 1 = 3 coins\nOptimal: 3 + 3 = 2 coins\n\nSo greedy may fail for non-canonical systems.\n\n\nTiny Code (Python)\ndef greedy_change(coins, amount):\n    coins.sort(reverse=True)\n    result = []\n    for coin in coins:\n        while amount &gt;= coin:\n            amount -= coin\n            result.append(coin)\n    return result\n\nprint(greedy_change([25,10,5,1], 63))  # [25, 25, 10, 1, 1, 1]\n\n\nWhy It Matters\n\nDemonstrates local decision-making\nFast and simple: \\(O(n)\\) over denominations\nFoundation for greedy design in spanning trees, scheduling, compression\nHighlights where greedy works and where it fails\n\n\n\nA Gentle Proof (Why It Works)\nFor canonical systems, greedy satisfies the optimal substructure and greedy-choice property:\n\nGreedy-choice property: Locally best → part of a global optimum.\nOptimal substructure: Remaining subproblem has optimal greedy solution.\n\nInductively, greedy yields minimal coin count.\n\n\nTry It Yourself\n\nTry greedy change with {25, 10, 5, 1} for 68.\nTry {9, 6, 1} for 11, compare with brute force.\nIdentify when greedy fails, test {4, 3, 1}.\nExtend algorithm to return both coins and count.\n\n\n\nTest Cases\n\n\n\nCoins\nAmount\nResult\nOptimal?\n\n\n\n\n{25,10,5,1}\n63\n[25,25,10,1,1,1]\n✅\n\n\n{9,6,1}\n11\n[9,1,1]\n✅\n\n\n{4,3,1}\n6\n[4,1,1]\n❌ (3+3 better)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nSorting\n\\(O(k \\log k)\\)\n\\(O(1)\\)\n\n\nSelection\n\\(O(k)\\)\n\\(O(k)\\)\n\n\n\n(\\(k\\) = number of denominations)\nThe Greedy Coin Example is the first mirror of the greedy philosophy, simple, intuitive, and fast, a lens into problems where choosing best now means best overall.\n\n\n\n32 Greedy Template Simulator\nThe Greedy Template Simulator shows how every greedy algorithm follows the same pattern, repeatedly choosing the best local option, updating the state, and moving toward the goal. It’s a reusable mental and coding framework for designing greedy solutions.\n\nWhat Problem Are We Solving?\nMany optimization problems can be solved by making local choices without revisiting earlier decisions. Instead of searching all paths (like backtracking) or building tables (like DP), greedy algorithms follow a deterministic path of best-next choices.\nWe want a general template to simulate this structure, useful for scheduling, coin change, and spanning tree problems.\n\n\nHow It Works (Plain Language)\n\nInitialize the problem state (remaining value, capacity, etc.).\nWhile goal not reached:\n\nEvaluate all local choices.\nPick the best immediate option (by some criterion).\nUpdate the state accordingly.\n\nEnd when no more valid moves exist.\n\nGreedy depends on a selection rule (which local choice is best) and a feasibility check (is the choice valid?).\n\n\nExample Step by Step\nProblem: Job Scheduling by Deadline (Maximize Profit)\n\n\n\nJob\nDeadline\nProfit\n\n\n\n\nA\n2\n60\n\n\nB\n1\n100\n\n\nC\n3\n20\n\n\nD\n2\n40\n\n\n\nSteps:\n\nSort jobs by profit (desc): B(100), A(60), D(40), C(20)\nTake each job if slot ≤ deadline available\nFill slots:\n\nDay 1: B\nDay 2: A\nDay 3: C → Total Profit = 180\n\n\nGreedy rule: “Pick highest profit first if deadline allows.”\n\n\nTiny Code (Python)\ndef greedy_template(items, is_valid, select_best, update_state):\n    state = initialize(items)\n    while not goal_reached(state):\n        best = select_best(items, state)\n        if is_valid(best, state):\n            update_state(best, state)\n        else:\n            break\n    return state\nConcrete greedy solutions just plug in:\n\nselect_best: define local criterion\nis_valid: define feasibility condition\nupdate_state: modify problem state\n\n\n\nWhy It Matters\n\nReveals shared skeleton behind all greedy algorithms\nSimplifies learning, “different bodies, same bones”\nEncourages reusable code via template-based design\nHelps debug logic: if it fails, test greedy-choice property\n\n\n\nA Gentle Proof (Why It Works)\nIf a problem has:\n\nGreedy-choice property: local best is part of global best\nOptimal substructure: subproblem solutions are optimal\n\nThen any algorithm following this template produces a global optimum. Formally proved via induction on input size.\n\n\nTry It Yourself\n\nImplement template for:\n\nCoin change\nFractional knapsack\nInterval scheduling\n\nCompare with brute-force or DP to confirm optimality.\nIdentify when greedy fails (e.g., non-canonical coin sets).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nProblem\nLocal Rule\nWorks?\nNote\n\n\n\n\nFractional Knapsack\nMax value/weight\n✅\nContinuous\n\n\nInterval Scheduling\nEarliest finish\n✅\nNon-overlapping\n\n\nCoin Change (25,10,5,1)\nLargest coin ≤ remaining\n✅\nCanonical only\n\n\nJob Scheduling\nHighest profit first\n✅\nSorted by profit\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nSelection\n\\(O(n \\log n)\\) (sort)\n\\(O(n)\\)\n\n\nIteration\n\\(O(n)\\)\n\\(O(1)\\)\n\n\n\nThe Greedy Template Simulator is the skeleton key of greedy design, once you learn its shape, every greedy algorithm looks like a familiar face.\n\n\n\n33 Divide & Conquer Skeleton\nThe Divide & Conquer Skeleton captures the universal structure of algorithms that solve big problems by splitting them into smaller, independent pieces, solving each recursively, then combining their results. It’s the framework behind mergesort, quicksort, binary search, and more.\n\nWhat Problem Are We Solving?\nSome problems are too large or complex to handle at once. Divide & Conquer (D&C) solves them by splitting into smaller subproblems of the same type, solving recursively, and combining the results into a whole.\nWe want a reusable template that reveals this recursive rhythm.\n\n\nHow It Works (Plain Language)\nEvery D&C algorithm follows this triplet:\n\nDivide: Break the problem into smaller subproblems.\nConquer: Solve each subproblem (often recursively).\nCombine: Merge or assemble partial solutions.\n\nThis recursion continues until a base case (small enough to solve directly).\nGeneral Recurrence: \\[\nT(n) = aT!\\left(\\frac{n}{b}\\right) + f(n)\n\\]\n\n\\(a\\): number of subproblems\n\\(b\\): factor by which size is reduced\n\\(f(n)\\): cost to divide/combine\n\n\n\nExample Step by Step\nExample: Merge Sort\n\nDivide: Split array into two halves\nConquer: Recursively sort each half\nCombine: Merge two sorted halves into one\n\nFor \\(n = 8\\):\n\nLevel 0: size 8\nLevel 1: size 4 + 4\nLevel 2: size 2 + 2 + 2 + 2\nLevel 3: size 1 (base case)\n\nEach level costs \\(O(n)\\) → total \\(O(n \\log n)\\).\n\n\nTiny Code (Python)\ndef divide_and_conquer(problem, base_case, divide, combine):\n    if base_case(problem):\n        return solve_directly(problem)\n    subproblems = divide(problem)\n    solutions = [divide_and_conquer(p, base_case, divide, combine) for p in subproblems]\n    return combine(solutions)\nPlug in custom divide, combine, and base-case logic for different problems.\n\n\nWhy It Matters\n\nModels recursive structure of many core algorithms\nReveals asymptotic pattern via recurrence\nEnables parallelization (subproblems solved independently)\nBalances simplicity (small subproblems) with power (reduction)\n\n\n\nA Gentle Proof (Why It Works)\nIf each recursive level divides the work evenly and recombines in finite time, then total cost is sum of all level costs: \\[\nT(n) = \\sum_{i=0}^{\\log_b n} a^i \\cdot f!\\left(\\frac{n}{b^i}\\right)\n\\] Master Theorem or tree expansion shows convergence to \\(O(n^{\\log_b a})\\) or \\(O(n \\log n)\\), depending on \\(f(n)\\).\nCorrectness follows by induction: each subproblem solved optimally ⇒ combined result optimal.\n\n\nTry It Yourself\n\nWrite a D&C template for:\n\nBinary Search\nMerge Sort\nKaratsuba Multiplication\n\nIdentify \\(a\\), \\(b\\), \\(f(n)\\) for each.\nSolve their recurrences with Master Theorem.\n\n\n\nTest Cases\n\n\n\nAlgorithm\n\\(a\\)\n\\(b\\)\n\\(f(n)\\)\nComplexity\n\n\n\n\nBinary Search\n1\n2\n1\n\\(O(\\log n)\\)\n\n\nMerge Sort\n2\n2\n\\(n\\)\n\\(O(n \\log n)\\)\n\n\nQuick Sort\n2\n2\n\\(n\\) (expected)\n\\(O(n \\log n)\\)\n\n\nKaratsuba\n3\n2\n\\(n\\)\n\\(O(n^{\\log_2 3})\\)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nRecursive calls\n\\(O(n)\\) to \\(O(n \\log n)\\)\n\\(O(\\log n)\\) (stack)\n\n\nCombine\n\\(O(f(n))\\)\ndepends on merging\n\n\n\nThe Divide & Conquer Skeleton is the heartbeat of recursion, a rhythm of divide, solve, combine, pulsing through the core of algorithmic design.\n\n\n\n34 Backtracking Maze Solver\nThe Backtracking Maze Solver illustrates the backtracking paradigm, exploring all possible paths through a search space, stepping forward when valid, and undoing moves when a dead end is reached. It’s the classic model for recursive search and constraint satisfaction.\n\nWhat Problem Are We Solving?\nWe want to find a path from start to goal in a maze or search space filled with constraints. Brute force would try every path blindly; backtracking improves on this by pruning paths as soon as they become invalid.\nThis approach powers solvers for mazes, Sudoku, N-Queens, and combinatorial search problems.\n\n\nHow It Works (Plain Language)\n\nStart at the initial position.\nTry a move (north, south, east, west).\nIf move is valid, mark position and recurse from there.\nIf stuck, backtrack: undo last move and try a new one.\nStop when goal is reached or all paths are explored.\n\nThe algorithm is depth-first in nature, it explores one branch fully before returning.\n\n\nExample Step by Step\nMaze (Grid Example)\nS . . #\n# . # .\n. . . G\n\nStart at S (0,0), Goal at G (2,3)\nMove right, down, or around obstacles (#)\nMark visited cells\nWhen trapped, step back and try another path\n\nPath Found: S → (0,1) → (1,1) → (2,1) → (2,2) → G\n\n\nTiny Code (Python)\ndef solve_maze(maze, x, y, goal):\n    if (x, y) == goal:\n        return True\n    if not valid_move(maze, x, y):\n        return False\n    maze[x][y] = 'V'  # Mark visited\n    for dx, dy in [(0,1), (1,0), (0,-1), (-1,0)]:\n        if solve_maze(maze, x+dx, y+dy, goal):\n            return True\n    maze[x][y] = '.'  # Backtrack\n    return False\nThe recursion explores all paths, marking and unmarking as it goes.\n\n\nWhy It Matters\n\nDemonstrates search with undoing\nFoundational for DFS, constraint satisfaction, puzzle solving\nIllustrates state exploration and recursive pruning\nFramework for N-Queens, Sudoku, graph coloring\n\n\n\nA Gentle Proof (Why It Works)\nBy exploring all valid moves recursively:\n\nEvery feasible path is eventually checked.\nInfeasible branches terminate early due to validity checks.\nBacktracking guarantees all combinations are explored once.\n\nThus, completeness is ensured, and if a path exists, it will be found.\n\n\nTry It Yourself\n\nDraw a 4×4 maze with one solution.\nRun backtracking manually, marking path and undoing wrong turns.\nModify rules (e.g., diagonal moves allowed).\nCompare runtime with BFS (which finds shortest path).\n\n\n\nTest Cases\n\n\n\nMaze\nSolution Found\nNotes\n\n\n\n\nOpen grid\nYes\nPath straight to goal\n\n\nMaze with block\nYes\nBacks up and reroutes\n\n\nNo path\nNo\nExhausts all options\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nSearch\nO(4ⁿ) worst-case\nO(n) recursion stack\n\n\n\n(n = number of cells)\nPruning and constraints reduce practical cost.\nThe Backtracking Maze Solver is a journey of trial and error, a guided wanderer exploring paths, retreating gracefully, and finding solutions hidden in the labyrinth.\n\n\n\n35 Karatsuba Multiplication\nThe Karatsuba Multiplication algorithm is a divide-and-conquer technique that multiplies two large numbers faster than the classical grade-school method. It reduces the multiplication count from 4 to 3 per recursive step, improving complexity from O(n²) to approximately O(n¹·⁵⁸⁵).\n\nWhat Problem Are We Solving?\nWhen multiplying large numbers (or polynomials), the standard approach performs every pairwise digit multiplication, O(n²) work for n-digit numbers. Karatsuba observed that some of this work is redundant. By reusing partial results cleverly, we can cut down the number of multiplications and gain speed.\nThis is the foundation of many fast arithmetic algorithms and symbolic computation libraries.\n\n\nHow It Works (Plain Language)\nGiven two n-digit numbers:\n\\[\nx = 10^{m} \\cdot a + b \\\ny = 10^{m} \\cdot c + d\n\\]\nwhere ( a, b, c, d ) are roughly n/2-digit halves of x and y.\n\nCompute three products:\n\n( p_1 = a c )\n( p_2 = b d )\n( p_3 = (a + b)(c + d) )\n\nCombine results using: \\[\nx \\cdot y = 10^{2m} \\cdot p_1 + 10^{m} \\cdot (p_3 - p_1 - p_2) + p_2\n\\]\n\nThis reduces recursive multiplications from 4 to 3.\n\n\nExample Step by Step\nMultiply 12 × 34.\nSplit:\n\na = 1, b = 2\nc = 3, d = 4\n\nCompute:\n\n( p_1 = 1 = 3 )\n( p_2 = 2 = 8 )\n( p_3 = (1 + 2)(3 + 4) = 3 = 21 )\n\nCombine: \\[\n(10^{2}) \\cdot 3 + 10 \\cdot (21 - 3 - 8) + 8 = 300 + 100 + 8 = 408\n\\]\nSo 12 × 34 = 408 (correct).\n\n\nTiny Code (Python)\ndef karatsuba(x, y):\n    if x &lt; 10 or y &lt; 10:\n        return x * y\n    n = max(len(str(x)), len(str(y)))\n    m = n // 2\n    a, b = divmod(x, 10m)\n    c, d = divmod(y, 10m)\n    p1 = karatsuba(a, c)\n    p2 = karatsuba(b, d)\n    p3 = karatsuba(a + b, c + d)\n    return p1 * 10(2*m) + (p3 - p1 - p2) * 10m + p2\n\n\nWhy It Matters\n\nFirst sub-quadratic multiplication algorithm\nBasis for advanced methods (Toom–Cook, FFT-based)\nApplies to integers, polynomials, big-number arithmetic\nShowcases power of divide and conquer\n\n\n\nA Gentle Proof (Why It Works)\nThe product expansion is:\n\\[\n(a \\cdot 10^m + b)(c \\cdot 10^m + d) = a c \\cdot 10^{2m} + (a d + b c)10^m + b d\n\\]\nObserve: \\[\n(a + b)(c + d) = ac + ad + bc + bd\n\\]\nThus: \\[\nad + bc = (a + b)(c + d) - ac - bd\n\\]\nKaratsuba leverages this identity to compute ( ad + bc ) without a separate multiplication.\nRecurrence: \\[\nT(n) = 3T(n/2) + O(n)\n\\] Solution: \\(T(n) = O(n^{\\log_2 3}) \\approx O(n^{1.585})\\)\n\n\nTry It Yourself\n\nMultiply 1234 × 5678 using Karatsuba steps.\nCompare with grade-school multiplication count.\nVisualize recursive calls as a tree.\nDerive recurrence and verify complexity.\n\n\n\nTest Cases\n\n\n\nx\ny\nResult\nMethod\n\n\n\n\n12\n34\n408\nWorks\n\n\n123\n456\n56088\nWorks\n\n\n9999\n9999\n99980001\nWorks\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nMultiplication\nO(n¹·⁵⁸⁵)\nO(n)\n\n\nBase case\nO(1)\nO(1)\n\n\n\nKaratsuba Multiplication reveals the magic of algebraic rearrangement, using one clever identity to turn brute-force arithmetic into an elegant, faster divide-and-conquer dance.\n\n\n\n36 DP State Diagram Example\nThe DP State Diagram Example introduces the idea of representing dynamic programming (DP) problems as graphs of states connected by transitions. It’s a visual and structural way to reason about overlapping subproblems, dependencies, and recurrence relations.\n\nWhat Problem Are We Solving?\nDynamic programming problems often involve a set of subproblems that depend on one another. Without a clear mental model, it’s easy to lose track of which states rely on which others.\nA state diagram helps us:\n\nVisualize states as nodes\nShow transitions as directed edges\nUnderstand dependency order for iteration or recursion\n\nThis builds intuition for state definition, transition logic, and evaluation order.\n\n\nHow It Works (Plain Language)\n\nDefine the state, what parameters represent a subproblem (e.g., index, capacity, sum).\nDraw each state as a node.\nAdd edges to show transitions between states.\nAssign recurrence along edges: \\[\ndp[\\text{state}] = \\text{combine}(dp[\\text{previous states}])\n\\]\nSolve by topological order (bottom-up) or memoized recursion (top-down).\n\n\n\nExample Step by Step\nExample: Fibonacci Sequence\n\\[\nF(n) = F(n-1) + F(n-2)\n\\]\nState diagram:\nF(5)\n↙   ↘\nF(4) F(3)\n↙↘   ↙↘\nF(3)F(2)F(2)F(1)\nEach node = state F(k) Edges = dependencies on F(k-1) and F(k-2)\nObservation: Many states repeat, shared subproblems suggest memoization or bottom-up DP.\nAnother Example: 0/1 Knapsack\nState: dp[i][w] = max value using first i items, capacity w. Transitions:\n\nInclude item i: dp[i-1][w-weight[i]] + value[i]\nExclude item i: dp[i-1][w]\n\nDiagram: a grid of states, each cell connected from previous row and shifted left.\n\n\nTiny Code (Python)\ndef fib_dp(n):\n    dp = [0, 1]\n    for i in range(2, n + 1):\n        dp.append(dp[i-1] + dp[i-2])\n    return dp[n]\nEach entry dp[i] represents a state, filled based on prior dependencies.\n\n\nWhy It Matters\n\nMakes DP visual and tangible\nClarifies dependency direction (acyclic structure)\nEnsures correct order of computation\nServes as blueprint for bottom-up or memoized implementation\n\n\n\nA Gentle Proof (Why It Works)\nIf a problem’s structure can be represented as a DAG of states, and:\n\nEvery state’s value depends only on earlier states\nBase states are known\n\nThen by evaluating nodes in topological order, we guarantee correctness, each subproblem is solved after its dependencies.\nThis matches mathematical induction over recurrence depth.\n\n\nTry It Yourself\n\nDraw state diagram for Fibonacci.\nDraw grid for 0/1 Knapsack (rows = items, cols = capacity).\nVisualize transitions for Coin Change (ways to make sum).\nTrace evaluation order bottom-up.\n\n\n\nTest Cases\n\n\n\nProblem\nState\nTransition\nDiagram Shape\n\n\n\n\nFibonacci\ndp[i]\ndp[i-1]+dp[i-2]\nChain\n\n\nKnapsack\ndp[i][w]\nmax(include, exclude)\nGrid\n\n\nCoin Change\ndp[i][s]\ndp[i-1][s]+dp[i][s-coin]\nLattice\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nDiagram construction\nO(n²) (visual)\nO(n²)\n\n\nDP evaluation\nO(n·m) typical\nO(n·m)\n\n\n\nThe DP State Diagram turns abstract recurrences into maps of reasoning, every arrow a dependency, every node a solved step, guiding you from base cases to the final solution.\n\n\n\n37 DP Table Visualization\nThe DP Table Visualization is a way to make dynamic programming tangible, turning states and transitions into a clear table you can fill, row by row or column by column. Each cell represents a subproblem, and the process of filling it shows the algorithm’s structure.\n\nWhat Problem Are We Solving?\nDynamic programming can feel abstract when written as recurrences. A table transforms that abstraction into something concrete:\n\nRows and columns correspond to subproblem parameters\nCell values show computed solutions\nFilling order reveals dependencies\n\nThis approach is especially powerful for tabulation (bottom-up DP).\n\n\nHow It Works (Plain Language)\n\nDefine your DP state (e.g., dp[i][j] = best value up to item i and capacity j).\nInitialize base cases (first row/column).\nIterate through the table in dependency order.\nApply recurrence at each cell: \\[\ndp[i][j] = \\text{combine}(dp[i-1][j], dp[i-1][j-w_i] + v_i)\n\\]\nFinal cell gives the answer (often bottom-right).\n\n\n\nExample Step by Step\nExample: 0/1 Knapsack\nItems:\n\n\n\nItem\nWeight\nValue\n\n\n\n\n1\n1\n15\n\n\n2\n3\n20\n\n\n3\n4\n30\n\n\n\nCapacity = 4\nState: dp[i][w] = max value with first i items, capacity w.\nRecurrence: \\[\ndp[i][w] = \\max(dp[i-1][w], dp[i-1][w - w_i] + v_i)\n\\]\nDP Table:\n\n\n\n\\(i / w\\)\n0\n1\n2\n3\n4\n\n\n\n\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n15\n15\n15\n15\n\n\n2\n0\n15\n15\n20\n35\n\n\n3\n0\n15\n15\n20\n35\n\n\n\nFinal answer: 35 (items 1 and 2)\n\n\nTiny Code (Python)\ndef knapsack(weights, values, W):\n    n = len(weights)\n    dp = [[0]*(W+1) for _ in range(n+1)]\n    for i in range(1, n+1):\n        for w in range(W+1):\n            if weights[i-1] &lt;= w:\n                dp[i][w] = max(dp[i-1][w],\n                               dp[i-1][w-weights[i-1]] + values[i-1])\n            else:\n                dp[i][w] = dp[i-1][w]\n    return dp\nEach dp[i][w] is one table cell, filled in increasing order of i and w.\n\n\nWhy It Matters\n\nTurns recurrence into geometry\nMakes dependencies visible and traceable\nClarifies filling order (row-wise, diagonal, etc.)\nServes as debugging tool and teaching aid\n\n\n\nA Gentle Proof (Why It Works)\nThe table order ensures every subproblem is solved after its dependencies. By induction:\n\nBase row/column initialized correctly\nEach cell built from valid earlier states\nFinal cell accumulates optimal solution\n\nThis is equivalent to a topological sort on the DP dependency graph.\n\n\nTry It Yourself\n\nDraw the DP table for Coin Change (number of ways).\nFill row by row.\nTrace dependencies with arrows.\nMark the path that contributes to the final answer.\n\n\n\nTest Cases\n\n\n\nProblem\nState\nFill Order\nOutput\n\n\n\n\nKnapsack\ndp[i][w]\nRow-wise\nMax value\n\n\nLCS\ndp[i][j]\nDiagonal\nLCS length\n\n\nEdit Distance\ndp[i][j]\nRow/col\nMin ops\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nFilling table\nO(n·m)\nO(n·m)\n\n\nTraceback (optional)\nO(n+m)\nO(1)\n\n\n\nThe DP Table Visualization is the grid view of recursion, a landscape of subproblems, each solved once, all leading toward the final cell that encodes the complete solution.\n\n\n\n38 Recursive Subproblem Tree Demo\nThe Recursive Subproblem Tree Demo shows how a dynamic programming problem expands into a tree of subproblems. It visualizes recursion structure, repeated calls, and where memoization or tabulation can save redundant work.\n\nWhat Problem Are We Solving?\nWhen writing a recursive solution, the same subproblems are often solved multiple times. Without visualizing this, we may not realize how much overlap occurs.\nBy drawing the recursion as a subproblem tree, we can:\n\nIdentify repeated nodes (duplicate work)\nUnderstand recursion depth\nDecide between memoization (top-down) or tabulation (bottom-up)\n\n\n\nHow It Works (Plain Language)\n\nStart from the root: the full problem (e.g., F(n)).\nExpand recursively into smaller subproblems (children).\nContinue until base cases (leaves).\nObserve repeated nodes (same subproblem appearing multiple times).\nReplace repeated computations with a lookup in a table.\n\nThe resulting structure is a tree that becomes a DAG after memoization.\n\n\nExample Step by Step\nExample: Fibonacci (Naive Recursive)\n\\[\nF(n) = F(n-1) + F(n-2)\n\\]\nFor \\(n = 5\\):\n        F(5)\n       /    \\\n    F(4)    F(3)\n   /   \\    /   \\\n F(3) F(2) F(2) F(1)\n / \\\nF(2) F(1)\nRepeated nodes: F(3), F(2) Memoization would store these results and reuse them.\nWith Memoization (Tree Collapsed):\n      F(5)\n     /   \\\n   F(4)  F(3)\nEach node computed once, repeated calls replaced by cache lookups.\n\n\nTiny Code (Python)\ndef fib(n, memo=None):\n    if memo is None:\n        memo = {}\n    if n in memo:\n        return memo[n]\n    if n &lt;= 1:\n        return n\n    memo[n] = fib(n-1, memo) + fib(n-2, memo)\n    return memo[n]\nThe memo dictionary turns the recursion tree into a DAG.\n\n\nWhy It Matters\n\nExposes hidden redundancy in recursive algorithms\nMotivates memoization (cache results)\nShows connection between recursion and iteration\nVisual tool for understanding time complexity\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be the recursion tree size.\nNaive recursion for Fibonacci: \\[\nT(n) = T(n-1) + T(n-2) + 1\n\\] ≈ \\(O(2^n)\\) calls\nWith memoization, each subproblem computed once: \\[\nT(n) = O(n)\n\\]\nProof by induction:\n\nBase case \\(n=1\\): trivial\nInductive step: if all smaller values memoized, reuse ensures constant-time lookups per state\n\n\n\nTry It Yourself\n\nDraw the recursion tree for Fibonacci(6).\nCount repeated nodes.\nAdd a memo table and redraw as DAG.\nApply same technique to factorial or grid path problems.\n\n\n\nTest Cases\n\n\n\nFunction\nNaive Calls\nMemoized Calls\nTime\n\n\n\n\nfib(5)\n15\n6\nO(2ⁿ) → O(n)\n\n\nfib(10)\n177\n11\nO(2ⁿ) → O(n)\n\n\nfib(20)\n21891\n21\nO(2ⁿ) → O(n)\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nNaive recursion\nO(2ⁿ)\nO(n)\n\n\nWith memoization\nO(n)\nO(n)\n\n\n\nThe Recursive Subproblem Tree Demo turns hidden recursion into a picture, every branch a computation, every repeated node a chance to save time, and every cache entry a step toward efficiency.\n\n\n\n39 Greedy Choice Visualization\nThe Greedy Choice Visualization helps you see how greedy algorithms make decisions step by step, choosing the locally optimal option at each point and committing to it. By tracing choices visually, you can verify whether the greedy strategy truly leads to a global optimum.\n\nWhat Problem Are We Solving?\nA greedy algorithm always chooses the best immediate option. But not every problem supports this approach, some require backtracking or DP. To know when greediness works, we need to see the chain of choices and their effects.\nA greedy choice diagram reveals:\n\nWhat each local decision looks like\nHow each choice affects remaining subproblems\nWhether local optima accumulate into a global optimum\n\n\n\nHow It Works (Plain Language)\n\nStart with the full problem (e.g., a set of intervals, coins, or items).\nSort or prioritize by a greedy criterion (e.g., largest value, earliest finish).\nPick the best option currently available.\nEliminate incompatible elements (conflicts, overlaps).\nRepeat until no valid choices remain.\nVisualize each step as a growing path or sequence.\n\nThe resulting picture shows a selection frontier, how choices narrow possibilities.\n\n\nExample Step by Step\nExample 1: Interval Scheduling\nGoal: select max non-overlapping intervals.\n\n\n\nInterval\nStart\nEnd\n\n\n\n\nA\n1\n4\n\n\nB\n3\n5\n\n\nC\n0\n6\n\n\nD\n5\n7\n\n\nE\n8\n9\n\n\n\nGreedy Rule: Choose earliest finish time.\nSteps:\n\nSort by finish → A(1–4), B(3–5), C(0–6), D(5–7), E(8–9)\nPick A → remove overlaps (B, C)\nNext pick D (5–7)\nNext pick E (8–9)\n\nVisualization:\nTimeline: 0---1---3---4---5---7---8---9\n           [A]     [D]      [E]\nTotal = 3 intervals → optimal.\nExample 2: Fractional Knapsack\n\n\n\nItem\nValue\nWeight\nRatio\n\n\n\n\n1\n60\n10\n6\n\n\n2\n100\n20\n5\n\n\n3\n120\n30\n4\n\n\n\nGreedy Rule: Max value/weight ratio Visualization: pick items in decreasing ratio order → 1, 2, part of 3.\n\n\nTiny Code (Python)\ndef greedy_choice(items, key):\n    items = sorted(items, key=key, reverse=True)\n    chosen = []\n    for it in items:\n        if valid(it, chosen):\n            chosen.append(it)\n    return chosen\nBy logging or plotting at each iteration, you can visualize how the solution grows.\n\n\nWhy It Matters\n\nShows local vs global tradeoffs visually\nConfirms greedy-choice property (local best = globally best)\nHelps diagnose greedy failures (where path deviates from optimum)\nStrengthens understanding of problem structure\n\n\n\nA Gentle Proof (Why It Works)\nA greedy algorithm works if:\n\nGreedy-choice property: local best can lead to global best.\nOptimal substructure: optimal solution of whole contains optimal solutions of parts.\n\nVisualization helps verify these conditions, if each chosen step leaves a smaller problem that is still optimally solvable by the same rule, the algorithm is correct.\n\n\nTry It Yourself\n\nDraw intervals and apply earliest-finish greedy rule.\nVisualize coin selections for greedy coin change.\nTry a counterexample where greedy fails (e.g., coin set {4,3,1}).\nPlot selection order to see divergence from optimum.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nProblem\nGreedy Rule\nWorks?\nNote\n\n\n\n\nInterval Scheduling\nEarliest finish\nYes\nOptimal\n\n\nFractional Knapsack\nMax ratio\nYes\nContinuous fractions\n\n\nCoin Change (25,10,5,1)\nLargest coin ≤ remaining\nYes\nCanonical\n\n\nCoin Change (4,3,1)\nLargest coin ≤ remaining\nNo\nNot canonical\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\n\n\n\n\nSort elements\nO(n log n)\nO(n)\n\n\nSelection loop\nO(n)\nO(1)\n\n\n\nThe Greedy Choice Visualization transforms abstract decision logic into a picture, a timeline or path that shows exactly how local choices unfold into (or away from) the global goal.\n\n\n\n40 Amortized Merge Demo\nThe Amortized Merge Demo illustrates how expensive operations can appear cheap when averaged over a long sequence. By analyzing total cost across all steps, we reveal why some algorithms with occasional heavy work still run efficiently overall.\n\nWhat Problem Are We Solving?\nSome data structures or algorithms perform occasional costly operations (like merging arrays, resizing tables, or rebuilding heaps). If we only look at worst-case time per step, they seem inefficient, but amortized analysis shows that, over many operations, the average cost per operation stays low.\nThis method explains why dynamic arrays, union-find, and incremental merges remain efficient.\n\n\nHow It Works (Plain Language)\n\nPerform a sequence of operations ( O_1, O_2, , O_n ).\nSome are cheap (constant time), some are expensive (linear or log).\nCompute total cost over all ( n ) operations.\nDivide total by ( n ) → amortized cost per operation.\n\nAmortized analysis tells us: \\[\n\\text{Amortized cost} = \\frac{\\text{Total cost over sequence}}{n}\n\\]\nEven if a few operations are expensive, their cost is “spread out” across many cheap ones.\n\n\nExample Step by Step\nExample: Dynamic Array Doubling\nSuppose we double the array each time it’s full.\n\n\n\n\n\n\n\n\n\n\nOperation\nCapacity\nActual Cost\nTotal Elements\nCumulative Cost\n\n\n\n\nInsert 1–1\n1\n1\n1\n1\n\n\nInsert 2–2\n2\n2\n2\n3\n\n\nInsert 3–4\n4\n3\n3\n6\n\n\nInsert 4–4\n4\n1\n4\n7\n\n\nInsert 5–8\n8\n5\n5\n12\n\n\nInsert 6–8\n8\n1\n6\n13\n\n\nInsert 7–8\n8\n1\n7\n14\n\n\nInsert 8–8\n8\n1\n8\n15\n\n\nInsert 9–16\n16\n9\n9\n24\n\n\n\nTotal cost (for 9 inserts) = 24 Amortized cost = 24 / 9 ≈ 2.67 ≈ O(1)\nSo although some inserts cost O(n), the average cost per insert = O(1).\nExample: Amortized Merge in Union-Find\nWhen combining sets, always attach the smaller tree to the larger one. Each element’s depth increases at most O(log n) times → total cost O(n log n).\n\n\nTiny Code (Python)\ndef dynamic_array_append(arr, x, capacity):\n    if len(arr) == capacity:\n        capacity *= 2  # double size\n        arr.extend([None]*(capacity - len(arr)))  # copy cost = len(arr)\n    arr[len(arr)//2] = x\n    return arr, capacity\nThis simulates doubling capacity, where copy cost = current array size.\n\n\nWhy It Matters\n\nExplains hidden efficiency behind resizing structures\nShows why occasional spikes don’t ruin performance\nFoundation for analyzing stacks, queues, hash tables\nBuilds intuition for amortized O(1) operations\n\n\n\nA Gentle Proof (Why It Works)\nConsider dynamic array resizing:\n\nEvery element gets moved at most once per doubling.\nOver n insertions, total copies ≤ 2n.\n\nThus, \\[\n\\text{Total cost} = O(n) \\implies \\text{Amortized cost} = O(1)\n\\]\nThis uses the aggregate method of amortized analysis:\n\\[\n\\text{Amortized cost per operation} =\n\\frac{\\text{total work}}{\\text{\\# operations}}\n\\]\n\n\nTry It Yourself\n\nSimulate 10 inserts into a doubling array.\nTrack total copy cost.\nPlot actual vs amortized cost.\nRepeat with tripling growth factor, compare average cost.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation Type\nCost Model\nAmortized Cost\nNotes\n\n\n\n\nArray Doubling\nCopy + Insert\nO(1)\nSpread cost\n\n\nUnion-Find Merge\nAttach smaller to larger\nO(α(n))\nα = inverse Ackermann\n\n\nStack Push\nResize occasionally\nO(1)\nAverage constant\n\n\nQueue Enqueue\nCircular buffer\nO(1)\nRotational reuse\n\n\n\n\n\nComplexity\n\n\n\nStep\nWorst Case\nAmortized\nSpace\n\n\n\n\nSingle Insert\nO(n)\nO(1)\nO(n)\n\n\nn Inserts\nO(n)\nO(n)\nO(n)\n\n\n\nThe Amortized Merge Demo reveals the calm beneath algorithmic chaos, even when some steps are costly, the long-run rhythm stays smooth, predictable, and efficient.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-5.-recurrence-relations",
    "href": "books/en-us/list-1.html#section-5.-recurrence-relations",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 5. Recurrence Relations",
    "text": "Section 5. Recurrence Relations\n\n41 Linear Recurrence Solver\nA Linear Recurrence Solver finds closed-form or iterative solutions for sequences defined in terms of previous values. It transforms recursive definitions like \\(T(n) = aT(n-1) + b\\) into explicit formulas, helping us understand algorithmic growth.\n\nWhat Problem Are We Solving?\nMany algorithms, especially recursive ones, define running time through a recurrence relation, for example:\n\\[\nT(n) = a , T(n-1) + b\n\\]\nTo reason about complexity or compute exact values, we want to solve the recurrence, converting it from a self-referential definition into a direct expression in \\(n\\).\nThis solver provides a methodical way to do that.\n\n\nHow It Works (Plain Language)\nA linear recurrence has the general form:\n\\[\nT(n) = a_1T(n-1) + a_2T(n-2) + \\cdots + a_kT(n-k) + f(n)\n\\]\n\nIdentify coefficients (\\(a_1, a_2, \\ldots\\)).\nWrite the characteristic equation for the homogeneous part.\nSolve for roots (\\(r_1, r_2, \\ldots\\)).\nForm the homogeneous solution using those roots.\nAdd a particular solution if \\(f(n)\\) is non-zero.\nApply initial conditions to fix constants.\n\n\n\nExample Step by Step\nExample 1: \\[\nT(n) = 2T(n-1) + 3, \\quad T(0) = 1\n\\]\n\nHomogeneous part: \\(T(n) - 2T(n-1) = 0\\) → Characteristic root: \\(r = 2\\) → Homogeneous solution: \\(T_h(n) = C \\cdot 2^n\\)\nParticular solution: constant \\(p\\) Plug in: \\(p = 2p + 3 \\implies p = -3\\)\nGeneral solution: \\[\nT(n) = C \\cdot 2^n - 3\n\\]\nApply \\(T(0)=1\\): \\(1 = C - 3 \\implies C = 4\\)\n\n✅ Final: \\[\nT(n) = 4 \\cdot 2^n - 3\n\\]\nExample 2 (Fibonacci):\n\\[\nF(n) = F(n-1) + F(n-2), \\quad F(0)=0, F(1)=1\n\\]\nCharacteristic equation: \\[\nr^2 - r - 1 = 0\n\\]\nRoots: \\[\nr_1 = \\frac{1+\\sqrt{5}}{2}, \\quad r_2 = \\frac{1-\\sqrt{5}}{2}\n\\]\nGeneral solution: \\[\nF(n) = A r_1^n + B r_2^n\n\\]\nSolving constants yields Binet’s Formula: \\[\nF(n) = \\frac{1}{\\sqrt{5}}\\left[\\left(\\frac{1+\\sqrt{5}}{2}\\right)^n - \\left(\\frac{1-\\sqrt{5}}{2}\\right)^n\\right]\n\\]\n\n\nTiny Code (Python)\ndef linear_recurrence(a, b, n, t0):\n    T = [t0]\n    for i in range(1, n + 1):\n        T.append(a * T[i - 1] + b)\n    return T\nThis simulates a simple first-order recurrence like \\(T(n) = aT(n-1) + b\\).\n\n\nWhy It Matters\n\nConverts recursive definitions into explicit formulas\nHelps analyze time complexity for recursive algorithms\nBridges math and algorithm design\nUsed in DP transitions, counting problems, and algorithm analysis\n\n\n\nA Gentle Proof (Why It Works)\nUnroll \\(T(n) = aT(n-1) + b\\):\n\\[\nT(n) = a^nT(0) + b(a^{n-1} + a^{n-2} + \\cdots + 1)\n\\]\nSum is geometric: \\[\nT(n) = a^nT(0) + b \\frac{a^n - 1}{a - 1}\n\\]\nHence the closed form is: \\[\nT(n) = a^nT(0) + \\frac{b(a^n - 1)}{a - 1}\n\\]\nThis matches the method of characteristic equations for constant coefficients.\n\n\nTry It Yourself\n\nSolve \\(T(n) = 3T(n-1) + 2, , T(0)=1\\)\nSolve \\(T(n) = 2T(n-1) - T(n-2)\\)\nCompare numeric results with iterative simulation\nDraw recursion tree to confirm growth trend\n\n\n\nTest Cases\n\n\n\nRecurrence\nInitial\nSolution\nGrowth\n\n\n\n\n\\(T(n)=2T(n-1)+3\\)\n\\(T(0)=1\\)\n\\(4 \\cdot 2^n - 3\\)\n\\(O(2^n)\\)\n\n\n\\(T(n)=T(n-1)+1\\)\n\\(T(0)=0\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(T(n)=3T(n-1)\\)\n\\(T(0)=1\\)\n\\(3^n\\)\n\\(O(3^n)\\)\n\n\n\n\n\nComplexity\n\n\n\nMethod\nTime\nSpace\n\n\n\n\nRecursive (unrolled)\nO(n)\nO(n)\n\n\nClosed-form\nO(1)\nO(1)\n\n\n\nA Linear Recurrence Solver turns repeated dependence into explicit growth, revealing the hidden pattern behind each recursive step.\n\n\n\n42 Master Theorem\nThe Master Theorem provides a direct method to analyze divide-and-conquer recurrences, allowing you to find asymptotic bounds without expanding or guessing. It is a cornerstone tool for understanding recursive algorithms such as merge sort, binary search, and Strassen’s multiplication.\n\nWhat Problem Are We Solving?\nMany recursive algorithms can be expressed as:\n\\[\nT(n) = a , T!\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nwhere:\n\n\\(a\\): number of subproblems\n\\(b\\): shrink factor (problem size per subproblem)\n\\(f(n)\\): additional work outside recursion (combine, partition, etc.)\n\nWe want to find an asymptotic expression for \\(T(n)\\) by comparing recursive cost (\\(n^{\\log_b a}\\)) with non-recursive cost (\\(f(n)\\)).\n\n\nHow It Works (Plain Language)\n\nWrite the recurrence in standard form: \\[\nT(n) = a , T(n/b) + f(n)\n\\]\nCompute the critical exponent \\(\\log_b a\\).\nCompare \\(f(n)\\) with \\(n^{\\log_b a}\\):\n\nIf \\(f(n)\\) is smaller, recursion dominates.\nIf they are equal, both contribute equally.\nIf \\(f(n)\\) is larger, the outside work dominates.\n\n\nThe theorem gives three standard cases depending on which term grows faster.\n\n\n\nThe Three Cases\nCase 1 (Recursive Work Dominates):\nIf \\[\nf(n) = O(n^{\\log_b a - \\varepsilon})\n\\] for some \\(\\varepsilon &gt; 0\\), then \\[\nT(n) = \\Theta(n^{\\log_b a})\n\\]\nCase 2 (Balanced Work):\nIf \\[\nf(n) = \\Theta(n^{\\log_b a})\n\\] then \\[\nT(n) = \\Theta(n^{\\log_b a} \\log n)\n\\]\nCase 3 (Non-Recursive Work Dominates):\nIf \\[\nf(n) = \\Omega(n^{\\log_b a + \\varepsilon})\n\\] and \\[\na , f(n/b) \\le c , f(n)\n\\] for some constant \\(c &lt; 1\\), then \\[\nT(n) = \\Theta(f(n))\n\\]\n\nExample Step by Step\nExample 1: Merge Sort\n\\[\nT(n) = 2T(n/2) + O(n)\n\\]\n\n\\(a = 2\\), \\(b = 2\\), so \\(\\log_b a = 1\\)\n\\(f(n) = O(n)\\)\n\\(f(n) = \\Theta(n^{\\log_b a})\\) → Case 2\n\nResult: \\[\nT(n) = \\Theta(n \\log n)\n\\]\nExample 2: Binary Search\n\\[\nT(n) = T(n/2) + O(1)\n\\]\n\n\\(a = 1\\), \\(b = 2\\), so \\(\\log_b a = 0\\)\n\\(f(n) = O(1) = \\Theta(n^0)\\) → Case 2\n\nResult: \\[\nT(n) = \\Theta(\\log n)\n\\]\nExample 3: Strassen’s Matrix Multiplication\n\\[\nT(n) = 7T(n/2) + O(n^2)\n\\]\n\n\\(a = 7\\), \\(b = 2\\), so \\(\\log_2 7 \\approx 2.81\\)\n\\(f(n) = O(n^2) = O(n^{2.81 - \\varepsilon})\\) → Case 1\n\nResult: \\[\nT(n) = \\Theta(n^{\\log_2 7})\n\\]\n\n\nTiny Code (Python)\nimport math\n\ndef master_theorem(a, b, f_exp):\n    log_term = math.log(a, b)\n    if f_exp &lt; log_term:\n        return f\"Theta(n^{round(log_term, 2)})\"\n    elif abs(f_exp - log_term) &lt; 1e-9:\n        return f\"Theta(n^{round(log_term, 2)} * log n)\"\n    else:\n        return f\"Theta(n^{f_exp})\"\nThis helper approximates the result by comparing exponents.\n\n\nWhy It Matters\n\nConverts recursive definitions into asymptotic forms\nAvoids repeated substitution or tree expansion\nApplies to most divide-and-conquer algorithms\nClarifies when combining work dominates or not\n\n\n\nA Gentle Proof (Why It Works)\nExpand the recurrence:\n\\[\nT(n) = aT(n/b) + f(n)\n\\]\nAfter \\(k\\) levels:\n\\[\nT(n) = a^k T(n/b^k) + \\sum_{i=0}^{k-1} a^i f(n/b^i)\n\\]\nRecursion depth: \\(k = \\log_b n\\)\nNow compare total cost per level to \\(n^{\\log_b a}\\):\n\nIf \\(f(n)\\) grows slower, top levels dominate → Case 1\nIf equal, all levels contribute → Case 2\nIf faster, bottom level dominates → Case 3\n\nThe asymptotic result depends on which component dominates the sum.\n\n\nTry It Yourself\n\nSolve \\(T(n) = 3T(n/2) + n\\)\nSolve \\(T(n) = 4T(n/2) + n^2\\)\nSketch recursion trees and check which term dominates\n\n\n\nTest Cases\n\n\n\nRecurrence\nCase\nSolution\n\n\n\n\n\\(T(n)=2T(n/2)+n\\)\nCase 2\n\\(\\Theta(n \\log n)\\)\n\n\n\\(T(n)=T(n/2)+1\\)\nCase 2\n\\(\\Theta(\\log n)\\)\n\n\n\\(T(n)=7T(n/2)+n^2\\)\nCase 1\n\\(\\Theta(n^{\\log_2 7})\\)\n\n\n\\(T(n)=2T(n/2)+n^2\\)\nCase 3\n\\(\\Theta(n^2)\\)\n\n\n\n\n\nComplexity Summary\n\n\n\n\n\n\n\n\nComponent\nExpression\nInterpretation\n\n\n\n\nRecursive work\n\\(n^{\\log_b a}\\)\nWork across recursive calls\n\n\nCombine work\n\\(f(n)\\)\nWork per level\n\n\nTotal cost\n\\(\\max(n^{\\log_b a}, f(n))\\)\nDominant term decides growth\n\n\n\nThe Master Theorem serves as a blueprint for analyzing recursive algorithms, once the recurrence is in standard form, its complexity follows by simple comparison.\n\n\n\n43 Substitution Method\nThe Substitution Method is a systematic way to prove the asymptotic bound of a recurrence by guessing a solution and then proving it by induction. It’s one of the most flexible techniques for verifying time complexity.\n\nWhat Problem Are We Solving?\nMany algorithms are defined recursively, for example:\n\\[\nT(n) = 2T(n/2) + n\n\\]\nWe often want to show that \\(T(n) = O(n \\log n)\\) or \\(T(n) = \\Theta(n^2)\\). But before we can apply a theorem, we must confirm that our guess fits.\nThe substitution method provides a proof framework:\n\nGuess the asymptotic bound.\nProve it by induction.\nAdjust constants if necessary.\n\n\n\nHow It Works (Plain Language)\n\nMake a guess for \\(T(n)\\), typically inspired by known patterns. For example, for \\(T(n) = 2T(n/2) + n\\), guess \\(T(n) = O(n \\log n)\\).\nWrite the inductive hypothesis: Assume \\(T(k) \\le c , k \\log k\\) for all \\(k &lt; n\\).\nSubstitute into the recurrence: Replace recursive terms with the hypothesis.\nSimplify and verify: Show the inequality holds for \\(n\\), adjusting constants if needed.\nConclude that the guess is valid.\n\n\n\nExample Step by Step\nExample 1:\n\\[\nT(n) = 2T(n/2) + n\n\\]\nGoal: Show \\(T(n) = O(n \\log n)\\)\n\nHypothesis: \\(T(k) \\le c , k \\log k\\) for all \\(k &lt; n\\)\nSubstitute: \\(T(n) \\le 2[c(n/2)\\log(n/2)] + n\\)\nSimplify: \\(= c n \\log(n/2) + n\\) \\(= c n (\\log n - 1) + n\\) \\(= c n \\log n - c n + n\\)\nAdjust constant: If \\(c \\ge 1\\), then \\(-cn + n \\le 0\\), so \\(T(n) \\le c n \\log n\\)\n\n✅ Therefore, \\(T(n) = O(n \\log n)\\).\nExample 2:\n\\[\nT(n) = 3T(n/2) + n\n\\]\nGuess: \\(T(n) = O(n^{\\log_2 3})\\)\n\nHypothesis: \\(T(k) \\le c , k^{\\log_2 3}\\)\nSubstitute: \\(T(n) \\le 3c(n/2)^{\\log_2 3} + n = 3c \\cdot n^{\\log_2 3} / 3 + n = c n^{\\log_2 3} + n\\)\nDominant term: \\(n^{\\log_2 3}\\) ✅ \\(T(n) = O(n^{\\log_2 3})\\)\n\n\n\nTiny Code (Python)\ndef substitution_check(a, b, f_exp, guess_exp):\n    from math import log\n    lhs = a * (1 / b)  guess_exp\n    rhs = 1\n    if lhs &lt; 1:\n        return f\"Guess n^{guess_exp} holds (Case 1)\"\n    elif abs(lhs - 1) &lt; 1e-9:\n        return f\"Guess n^{guess_exp} log n (Case 2)\"\n    else:\n        return f\"Guess n^{guess_exp} fails (try larger exponent)\"\nHelps verify whether a guessed exponent fits the recurrence.\n\n\nWhy It Matters\n\nBuilds proof-based understanding of complexity\nConfirms asymptotic bounds from intuition or Master Theorem\nWorks even when Master Theorem fails (irregular forms)\nReinforces connection between recursion and growth rate\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n) = aT(n/b) + f(n)\\) Guess \\(T(n) = O(n^{\\log_b a})\\).\nInductive step: \\[\nT(n) = aT(n/b) + f(n) \\le a(c(n/b)^{\\log_b a}) + f(n)\n\\] \\[\n= c n^{\\log_b a} + f(n)\n\\]\nIf \\(f(n)\\) grows slower, \\(T(n)\\) remains \\(O(n^{\\log_b a})\\) by choosing \\(c\\) large enough.\n\n\nTry It Yourself\n\nProve \\(T(n) = 2T(n/2) + n^2 = O(n^2)\\)\nProve \\(T(n) = T(n-1) + 1 = O(n)\\)\nAdjust constants to make the induction hold\n\n\n\nTest Cases\n\n\n\nRecurrence\nGuess\nResult\n\n\n\n\n\\(T(n)=2T(n/2)+n\\)\n\\(O(n\\log n)\\)\nCorrect\n\n\n\\(T(n)=T(n-1)+1\\)\n\\(O(n)\\)\nCorrect\n\n\n\\(T(n)=3T(n/2)+n\\)\n\\(O(n^{\\log_2 3})\\)\nCorrect\n\n\n\n\n\nComplexity Summary\n\n\n\nMethod\nEffort\nWhen to Use\n\n\n\n\nMaster Theorem\nQuick\nStandard divide-and-conquer\n\n\nSubstitution\nModerate\nCustom or irregular recurrences\n\n\nIteration\nDetailed\nStep-by-step expansion\n\n\n\nThe Substitution Method blends intuition with rigor, you make a good guess, and algebra does the rest.\n\n\n\n44 Iteration Method\nThe Iteration Method (also called the Recursion Expansion Method) solves recurrences by repeatedly substituting the recursive term until the pattern becomes clear. It is a constructive way to derive closed-form or asymptotic solutions.\n\nWhat Problem Are We Solving?\nRecursive algorithms often define their running time in terms of smaller instances:\n\\[\nT(n) = a , T(n/b) + f(n)\n\\]\nInstead of guessing or applying a theorem, the iteration method unfolds the recurrence layer by layer, showing exactly how cost accumulates across recursion levels.\nThis method is especially helpful when \\(f(n)\\) follows a recognizable pattern, like linear, quadratic, or logarithmic functions.\n\n\nHow It Works (Plain Language)\n\nWrite down the recurrence:\n\\[\nT(n) = a , T(n/b) + f(n)\n\\]\nExpand one level at a time:\n\\[\nT(n) = a[a , T(n/b^2) + f(n/b)] + f(n)\n\\]\n\\[\n= a^2 T(n/b^2) + a f(n/b) + f(n)\n\\]\nContinue expanding \\(k\\) levels until the subproblem size becomes 1:\n\\[\nT(n) = a^k T(n/b^k) + \\sum_{i=0}^{k-1} a^i f(n/b^i)\n\\]\nWhen \\(n/b^k = 1\\), we have \\(k = \\log_b n\\).\nSubstitute \\(k = \\log_b n\\) to find the closed form or asymptotic bound.\n\n\n\nExample Step by Step\nExample 1: Merge Sort\n\\[\nT(n) = 2T(n/2) + n\n\\]\nStep 1: Expand\n[ ]\nStep 2: Base Case\nWhen \\(n/2^k = 1 \\implies k = \\log_2 n\\)\nSo: \\[\nT(n) = n \\cdot T(1) + n \\log_2 n = O(n \\log n)\n\\]\n✅ \\(T(n) = \\Theta(n \\log n)\\)\nExample 2: Binary Search\n\\[\nT(n) = T(n/2) + 1\n\\]\nExpand:\n[ ]\n✅ \\(T(n) = O(\\log n)\\)\nExample 3: Linear Recurrence\n\\[\nT(n) = T(n-1) + 1\n\\]\nExpand:\n\\[\nT(n) = T(n-1) + 1 = T(n-2) + 2 = \\cdots = T(1) + (n-1)\n\\]\n✅ \\(T(n) = O(n)\\)\n\n\nTiny Code (Python)\ndef iterate_recurrence(a, b, f, n):\n    total = 0\n    level = 0\n    while n &gt; 1:\n        total += (a  level) * f(n / (b  level))\n        n /= b\n        level += 1\n    return total\nThis illustrates the summation process level by level.\n\n\nWhy It Matters\n\nMakes recursion visually transparent\nWorks for irregular \\(f(n)\\) (when Master Theorem doesn’t apply)\nDerives exact sums, not just asymptotic bounds\nBuilds intuition for recursion trees and logarithmic depth\n\n\n\nA Gentle Proof (Why It Works)\nEach level \\(i\\) of the recursion contributes:\n\\[\na^i \\cdot f(n/b^i)\n\\]\nTotal number of levels: \\[\n\\log_b n\n\\]\nSo total cost: \\[\nT(n) = \\sum_{i=0}^{\\log_b n - 1} a^i f(n/b^i)\n\\]\nThis sum can be approximated or bounded using standard summation techniques, depending on \\(f(n)\\)’s growth rate.\n\n\nTry It Yourself\n\nSolve \\(T(n) = 3T(n/2) + n^2\\)\nSolve \\(T(n) = 2T(n/2) + n \\log n\\)\nSolve \\(T(n) = T(n/2) + n/2\\)\nCompare with Master Theorem results\n\n\n\nTest Cases\n\n\n\nRecurrence\nSolution\nGrowth\n\n\n\n\n\\(T(n)=2T(n/2)+n\\)\n\\(n \\log n\\)\n\\(O(n \\log n)\\)\n\n\n\\(T(n)=T(n/2)+1\\)\n\\(\\log n\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n)=T(n-1)+1\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(T(n)=3T(n/2)+n^2\\)\n\\(n^2\\)\n\\(O(n^2)\\)\n\n\n\n\n\nComplexity Summary\n\n\n\nStep\nTime\nSpace\n\n\n\n\nExpansion\n\\(O(\\log n)\\) levels\nStack depth \\(O(\\log n)\\)\n\n\nSummation\nDepends on \\(f(n)\\)\nOften geometric or arithmetic\n\n\n\nThe Iteration Method unpacks recursion into layers of work, turning a recurrence into a concrete sum, and a sum into a clear complexity bound.\n\n\n\n45 Generating Function Method\nThe Generating Function Method transforms a recurrence relation into an algebraic equation by encoding the sequence into a power series. Once transformed, algebraic manipulation yields a closed-form expression or asymptotic approximation.\n\nWhat Problem Are We Solving?\nA recurrence defines a sequence \\(T(n)\\) recursively:\n\\[\nT(n) = a_1 T(n-1) + a_2 T(n-2) + \\cdots + f(n)\n\\]\nWe want to find a closed-form formula instead of computing step by step. By representing \\(T(n)\\) as coefficients in a power series, we can use algebraic tools to solve recurrences cleanly, especially linear recurrences with constant coefficients.\n\n\nHow It Works (Plain Language)\n\nDefine the generating function Let \\[\nG(x) = \\sum_{n=0}^{\\infty} T(n) x^n\n\\]\nMultiply the recurrence by \\(x^n\\) and sum over all \\(n\\).\nUse properties of sums (shifting indices, factoring constants) to rewrite in terms of \\(G(x)\\).\nSolve the algebraic equation for \\(G(x)\\).\nExpand \\(G(x)\\) back into a series (using partial fractions or known expansions).\nExtract \\(T(n)\\) as the coefficient of \\(x^n\\).\n\n\n\nExample Step by Step\nExample 1: Fibonacci Sequence\n\\[\nT(n) = T(n-1) + T(n-2), \\quad T(0) = 0, ; T(1) = 1\n\\]\nStep 1: Define generating function\n\\[\nG(x) = \\sum_{n=0}^{\\infty} T(n) x^n\n\\]\nStep 2: Multiply recurrence by \\(x^n\\) and sum over \\(n \\ge 2\\):\n\\[\n\\sum_{n=2}^{\\infty} T(n) x^n = \\sum_{n=2}^{\\infty} T(n-1)x^n + \\sum_{n=2}^{\\infty} T(n-2)x^n\n\\]\nStep 3: Rewrite using shifts:\n\\[\nG(x) - T(0) - T(1)x = x(G(x) - T(0)) + x^2 G(x)\n\\]\nPlug in initial values \\(T(0)=0, T(1)=1\\):\n\\[\nG(x) - x = xG(x) + x^2 G(x)\n\\]\nStep 4: Solve for \\(G(x)\\):\n\\[\nG(x)(1 - x - x^2) = x\n\\]\nSo:\n\\[\nG(x) = \\frac{x}{1 - x - x^2}\n\\]\nStep 5: Expand using partial fractions to get coefficients:\n\\[\nT(n) = \\frac{1}{\\sqrt{5}} \\left[\\left(\\frac{1+\\sqrt{5}}{2}\\right)^n - \\left(\\frac{1-\\sqrt{5}}{2}\\right)^n\\right]\n\\]\n✅ Binet’s Formula derived directly.\nExample 2: \\(T(n) = 2T(n-1) + 3\\), \\(T(0)=1\\)\nLet \\(G(x) = \\sum_{n=0}^{\\infty} T(n)x^n\\)\nMultiply by \\(x^n\\) and sum over \\(n \\ge 1\\):\n\\[\nG(x) - T(0) = 2xG(x) + 3x \\cdot \\frac{1}{1-x}\n\\]\nSimplify:\n\\[\nG(x)(1 - 2x) = 1 + \\frac{3x}{1-x}\n\\]\nSolve and expand using partial fractions → recover closed-form:\n\\[\nT(n) = 4 \\cdot 2^n - 3\n\\]\n\n\nTiny Code (Python)\nfrom sympy import symbols, Function, Eq, rsolve\n\nn = symbols('n', integer=True)\nT = Function('T')\nrecurrence = Eq(T(n), 2*T(n-1) + 3)\nsolution = rsolve(recurrence, T(n), {T(0): 1})\nprint(solution)  # 4*2n - 3\nUse sympy.rsolve to compute closed forms symbolically.\n\n\nWhy It Matters\n\nConverts recurrence relations into algebraic equations\nReveals exact closed forms, not just asymptotics\nWorks for non-homogeneous and constant-coefficient recurrences\nBridges combinatorics, discrete math, and algorithm analysis\n\n\n\nA Gentle Proof (Why It Works)\nGiven a linear recurrence:\n\\[\nT(n) - a_1T(n-1) - \\cdots - a_kT(n-k) = f(n)\n\\]\nMultiply by \\(x^n\\) and sum from \\(n=k\\) to \\(\\infty\\):\n\\[\n\\sum_{n=k}^{\\infty} T(n)x^n = a_1x \\sum_{n=k}^{\\infty} T(n-1)x^{n-1} + \\cdots + f(x)\n\\]\nUsing index shifts, each term can be written in terms of \\(G(x)\\), leading to:\n\\[\nP(x)G(x) = Q(x)\n\\]\nwhere \\(P(x)\\) and \\(Q(x)\\) are polynomials. Solving for \\(G(x)\\) gives the sequence structure.\n\n\nTry It Yourself\n\nSolve \\(T(n)=3T(n-1)-2T(n-2)\\) with \\(T(0)=2, T(1)=3\\).\nFind \\(T(n)\\) if \\(T(n)=T(n-1)+1\\), \\(T(0)=0\\).\nCompare your generating function with unrolled expansion.\n\n\n\nTest Cases\n\n\n\nRecurrence\nClosed Form\nGrowth\n\n\n\n\n\\(T(n)=2T(n-1)+3\\)\n\\(4 \\cdot 2^n - 3\\)\n\\(O(2^n)\\)\n\n\n\\(T(n)=T(n-1)+1\\)\n\\(n\\)\n\\(O(n)\\)\n\n\n\\(T(n)=T(n-1)+T(n-2)\\)\n\\(\\text{Binet's Formula}\\)\n\\(O(\\phi^n)\\)\n\n\n\n\n\nComplexity Summary\n\n\n\nStep\nType\nComplexity\n\n\n\n\nTransformation\nAlgebraic\nO(k) terms\n\n\nSolution\nSymbolic (via roots)\nO(k^3)\n\n\nEvaluation\nClosed form\nO(1)\n\n\n\nThe Generating Function Method turns recurrences into algebra, summations become equations, and equations yield exact formulas.\n\n\n\n46 Matrix Exponentiation\nThe Matrix Exponentiation Method transforms linear recurrences into matrix form, allowing efficient computation of terms in \\(O(\\log n)\\) time using fast exponentiation. It’s ideal for sequences like Fibonacci, Tribonacci, and many dynamic programming transitions.\n\nWhat Problem Are We Solving?\nMany recurrences follow a linear relation among previous terms, such as:\n\\[\nT(n) = a_1 T(n-1) + a_2 T(n-2) + \\cdots + a_k T(n-k)\n\\]\nNaively computing \\(T(n)\\) takes \\(O(n)\\) steps. By encoding this recurrence in a matrix, we can compute \\(T(n)\\) efficiently via exponentiation, reducing runtime to \\(O(k^3 \\log n)\\).\n\n\nHow It Works (Plain Language)\n\nExpress the recurrence as a matrix multiplication.\nConstruct the transition matrix \\(M\\) that moves the state from \\(n-1\\) to \\(n\\).\nCompute \\(M^n\\) using fast exponentiation (divide and conquer).\nMultiply \\(M^n\\) by the initial vector to obtain \\(T(n)\\).\n\nThis approach generalizes well to any linear homogeneous recurrence with constant coefficients.\n\n\nExample Step by Step\nExample 1: Fibonacci Sequence\n\\[\nF(n) = F(n-1) + F(n-2)\n\\]\nDefine state vector:\n\\[\n\\begin{bmatrix}\nF(n) \\\\\nF(n-1)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nF(n-1) \\\\\nF(n-2)\n\\end{bmatrix}\n\\]\nSo:\n\\[\nM =\n\\begin{bmatrix}\n1 & 1 [6pt]\n1 & 0\n\\end{bmatrix}\n\\]\nTherefore:\n\\[\n\\begin{bmatrix}\nF(n) \\ F(n-1)\n\\end{bmatrix}\n= M^{n-1}\n\\begin{bmatrix}\nF(1) \\ F(0)\n\\end{bmatrix}\n\\]\nGiven \\(F(1)=1, F(0)=0\\), \\[\nF(n) = (M^{n-1})_{0,0}\n\\]\nExample 2: Second-Order Recurrence\n\\[\nT(n) = 2T(n-1) + 3T(n-2)\n\\]\nMatrix form:\n\\[\n\\begin{bmatrix}\nT(n) \\\\[4pt]\nT(n-1)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 3 \\\\[4pt]\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nT(n-1) \\\\[4pt]\nT(n-2)\n\\end{bmatrix}\n\\]\nSo:\n\\[\n\\vec{T}(n) = M^{n-2} \\vec{T}(2)\n\\]\n\n\nTiny Code (Python)\ndef mat_mult(A, B):\n    return [[sum(A[i][k] * B[k][j] for k in range(len(A)))\n             for j in range(len(B[0]))] for i in range(len(A))]\n\ndef mat_pow(M, n):\n    if n == 1:\n        return M\n    if n % 2 == 0:\n        half = mat_pow(M, n // 2)\n        return mat_mult(half, half)\n    else:\n        return mat_mult(M, mat_pow(M, n - 1))\n\ndef fib_matrix(n):\n    if n == 0:\n        return 0\n    M = [[1, 1], [1, 0]]\n    Mn = mat_pow(M, n - 1)\n    return Mn[0][0]\nfib_matrix(n) computes \\(F(n)\\) in \\(O(\\log n)\\).\n\n\nWhy It Matters\n\nConverts recursive computation into linear algebra\nEnables \\(O(\\log n)\\) computation for \\(T(n)\\)\nGeneralizes to higher-order recurrences\nCommon in DP transitions, Fibonacci-like sequences, and combinatorial counting\n\n\n\nA Gentle Proof (Why It Works)\nThe recurrence:\n\\[\nT(n) = a_1T(n-1) + a_2T(n-2) + \\cdots + a_kT(n-k)\n\\]\ncan be expressed as:\n\\[\n\\vec{T}(n) = M \\cdot \\vec{T}(n-1)\n\\]\nwhere \\(M\\) is the companion matrix:\n\\[\nM =\n\\begin{bmatrix}\na_1 & a_2 & \\cdots & a_k \\\\\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & & \\ddots & 0\n\\end{bmatrix}\n\\]\nRepeatedly multiplying gives:\n\\[\n\\vec{T}(n) = M^{n-k} \\vec{T}(k)\n\\]\nHence, \\(T(n)\\) is computed by raising \\(M\\) to a power, exponential recursion becomes logarithmic multiplication.\n\n\nTry It Yourself\n\nWrite matrix form for \\(T(n)=3T(n-1)-2T(n-2)\\)\nCompute \\(T(10)\\) with \\(T(0)=2\\), \\(T(1)=3\\)\nImplement matrix exponentiation for \\(3\\times3\\) matrices (Tribonacci)\nCompare with iterative solution runtime\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nRecurrence\nMatrix\n\\(T(n)\\) / Symbol\nComplexity\n\n\n\n\n\\(F(n)=F(n-1)+F(n-2)\\)\n\\(\\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}\\)\n\\(F(n)\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n)=2T(n-1)+3T(n-2)\\)\n\\(\\begin{bmatrix} 2 & 3 \\\\ 1 & 0 \\end{bmatrix}\\)\n\\(T(n)\\)\n\\(O(\\log n)\\)\n\n\n\\(T(n)=T(n-1)+T(n-2)+T(n-3)\\)\n\\(\\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\)\nTribonacci\n\\(O(\\log n)\\)\n\n\n\n\n\nComplexity Summary\n\n\n\nStep\nTime\nSpace\n\n\n\n\nMatrix exponentiation\n\\(O(k^3 \\log n)\\)\n\\(O(k^2)\\)\n\n\nIterative recurrence\n\\(O(n)\\)\n\\(O(k)\\)\n\n\n\nMatrix Exponentiation turns recurrence solving into matrix powering, a bridge between recursion and linear algebra, giving exponential speed-up with mathematical elegance.\n\n\n\n47 Recurrence to DP Table\nThe Recurrence to DP Table method converts a recursive relation into an iterative table-based approach, removing redundant computation and improving efficiency from exponential to polynomial time. It’s a cornerstone of Dynamic Programming.\n\nWhat Problem Are We Solving?\nRecursive formulas often recompute overlapping subproblems. For example:\n\\[\nT(n) = T(n-1) + T(n-2)\n\\]\nA naive recursive call tree grows exponentially because it recomputes \\(T(k)\\) many times. By converting this recurrence into a DP table, we compute each subproblem once and store results, achieving linear or polynomial time.\n\n\nHow It Works (Plain Language)\n\nIdentify the recurrence and base cases.\nCreate a table (array or matrix) to store subproblem results.\nIteratively fill the table using the recurrence formula.\nRead off the final answer from the last cell.\n\nThis technique is called tabulation, a bottom-up form of dynamic programming.\n\n\nExample Step by Step\nExample 1: Fibonacci Numbers\nRecursive formula:\n\\[\nF(n) = F(n-1) + F(n-2), \\quad F(0)=0, , F(1)=1\n\\]\nDP version:\n\n\n\nn\n0\n1\n2\n3\n4\n5\n\n\n\n\nF(n)\n0\n1\n1\n2\n3\n5\n\n\n\nAlgorithm:\n\nInitialize base cases: F[0]=0, F[1]=1\nLoop from 2 to n: F[i] = F[i-1] + F[i-2]\nReturn F[n]\n\nExample 2: Coin Change (Count Ways)\nRecurrence: \\[\n\\text{ways}(n, c) = \\text{ways}(n, c-1) + \\text{ways}(n-\\text{coin}[c], c)\n\\]\nConvert to 2D DP table indexed by (n, c).\nExample 3: Grid Paths\nRecurrence: \\[\nP(i,j) = P(i-1,j) + P(i,j-1)\n\\]\nDP table:\n\n\n\ni\n0\n1\n2\n\n\n\n\n0\n1\n1\n1\n\n\n1\n1\n2\n3\n\n\n2\n1\n3\n6\n\n\n\nEach cell = sum of top and left.\n\n\nTiny Code (Python)\ndef fib_dp(n):\n    if n == 0:\n        return 0\n    dp = [0] * (n + 1)\n    dp[1] = 1\n    for i in range(2, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n    return dp[n]\n\n\nWhy It Matters\n\nConverts exponential recursion to polynomial iteration\nAvoids repeated subproblem computations\nEnables space and time optimization\nForms the foundation of bottom-up dynamic programming\n\n\n\nA Gentle Proof (Why It Works)\nGiven recurrence:\n\\[\nT(n) = a_1 T(n-1) + a_2 T(n-2) + \\cdots + a_k T(n-k)\n\\]\nEach term depends only on previously computed subproblems. So by filling the table in increasing order, we ensure all dependencies are ready.\nBy induction, if base cases are correct, each computed cell is correct.\n\n\nTry It Yourself\n\nConvert \\(F(n)=F(n-1)+F(n-2)\\) to a 1D DP array\nBuild a 2D table for grid paths \\(P(i,j)=P(i-1,j)+P(i,j-1)\\)\nWrite a DP table for factorial \\(n! = n \\times (n-1)!\\)\nOptimize space (keep only last k terms)\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nRecurrence\nExpected\n\n\n\n\n\\(F(5)\\)\n\\(F(n)=F(n-1)+F(n-2)\\)\n5\n\n\nGrid(2,2)\n\\(P(i,j)=P(i-1,j)+P(i,j-1)\\)\n6\n\n\n\\(n=3, coins=[1,2]\\)\n\\(\\text{ways}(n,c)=\\text{ways}(n,c-1)+\\text{ways}(n-\\text{coin}[c],c)\\)\n2\n\n\n\n\n\nComplexity Summary\n\n\n\nMethod\nTime\nSpace\n\n\n\n\nRecursive\n\\(O(2^n)\\)\n\\(O(n)\\)\n\n\nDP Table\n\\(O(n)\\)\n\\(O(n)\\)\n\n\nSpace-Optimized DP\n\\(O(n)\\)\n\\(O(1)\\)\n\n\n\nTransforming a recurrence into a DP table captures the essence of dynamic programming, structure, reuse, and clarity over brute repetition.\n\n\n\n48 Divide & Combine Template\nThe Divide & Combine Template is a structural guide for solving problems by breaking them into smaller, similar subproblems, solving each independently, and combining their results. It’s the core skeleton behind divide-and-conquer algorithms like Merge Sort, Quick Sort, and Karatsuba Multiplication.\n\nWhat Problem Are We Solving?\nMany complex problems can be decomposed into smaller copies of themselves. Instead of solving the full instance at once, we divide it into subproblems, solve each recursively, and combine their results.\nThis approach reduces complexity, promotes parallelism, and yields recurrence relations like:\n\\[\nT(n) = aT\\left(\\frac{n}{b}\\right) + f(n)\n\\]\n\n\nHow It Works (Plain Language)\n\nDivide: Split the problem into \\(a\\) subproblems, each of size \\(\\frac{n}{b}\\).\nConquer: Recursively solve the subproblems.\nCombine: Merge their results into a full solution.\nBase Case: Stop dividing when the subproblem becomes trivially small.\n\nThis recursive structure underpins most efficient algorithms for sorting, searching, and multiplication.\n\n\nExample Step by Step\nExample 1: Merge Sort\n\nDivide: Split array into two halves\nConquer: Recursively sort each half\nCombine: Merge two sorted halves\n\nRecurrence: \\[\nT(n) = 2T\\left(\\frac{n}{2}\\right) + O(n)\n\\]\nExample 2: Karatsuba Multiplication\n\nDivide numbers into halves\nConquer with 3 recursive multiplications\nCombine using linear combinations\n\nRecurrence: \\[\nT(n) = 3T\\left(\\frac{n}{2}\\right) + O(n)\n\\]\nExample 3: Binary Search\n\nDivide the array by midpoint\nConquer on one half\nCombine trivially (return result)\n\nRecurrence: \\[\nT(n) = T\\left(\\frac{n}{2}\\right) + O(1)\n\\]\n\n\n\nGeneric Template (Pseudocode)\ndef divide_and_combine(problem):\n    if is_small(problem):\n        return solve_directly(problem)\n    subproblems = divide(problem)\n    results = [divide_and_combine(p) for p in subproblems]\n    return combine(results)\nThis general template can adapt to many problem domains, arrays, trees, graphs, geometry, and algebra.\n\nWhy It Matters\n\nClarifies recursion structure and base case reasoning\nEnables asymptotic analysis via recurrence\nLays foundation for parallel and cache-efficient algorithms\nPromotes clean decomposition and reusability\n\n\n\nA Gentle Proof (Why It Works)\nIf a problem can be decomposed into independent subproblems whose results can be merged deterministically, recursive decomposition is valid. By induction:\n\nBase case: small input solved directly.\nInductive step: if each subproblem is solved correctly, and the combine step correctly merges, the final solution is correct.\n\nThus correctness follows from structural decomposition.\n\n\nTry It Yourself\n\nImplement divide-and-conquer sum over an array.\nWrite recursive structure for Maximum Subarray (Kadane’s divide form).\nExpress recurrence \\(T(n)=2T(n/2)+n\\) and solve via the Master Theorem.\nModify template for parallel processing (e.g., thread pool).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nProblem\nDivide\nCombine\nComplexity\n\n\n\n\nMerge Sort\nHalve array\nMerge sorted halves\n\\(O(n \\log n)\\)\n\n\nBinary Search\nHalve search space\nReturn result\n\\(O(\\log n)\\)\n\n\nKaratsuba\nSplit numbers\nCombine linear parts\n\\(O(n^{1.585})\\)\n\n\nClosest Pair (2D)\nSplit points\nMerge cross-boundary pairs\n\\(O(n \\log n)\\)\n\n\n\n\n\nComplexity Summary\nGiven: \\[\nT(n) = aT\\left(\\frac{n}{b}\\right) + f(n)\n\\]\nBy the Master Theorem:\n\nIf \\(f(n) = O(n^{\\log_b a - \\epsilon})\\), then \\(T(n) = \\Theta(n^{\\log_b a})\\)\nIf \\(f(n) = \\Theta(n^{\\log_b a})\\), then \\(T(n) = \\Theta(n^{\\log_b a} \\log n)\\)\nIf \\(f(n) = \\Omega(n^{\\log_b a + \\epsilon})\\), then \\(T(n) = \\Theta(f(n))\\)\n\nThe Divide & Combine Template provides the blueprint for recursive problem solving, simple, elegant, and foundational across all algorithmic domains.\n\n\n\n49 Memoized Recursive Solver\nA Memoized Recursive Solver transforms a plain recursive solution into an efficient one by caching intermediate results. It’s the top-down version of dynamic programming, retaining recursion’s clarity while avoiding redundant work.\n\nWhat Problem Are We Solving?\nRecursive algorithms often recompute the same subproblems multiple times. Example:\n\\[\nF(n) = F(n-1) + F(n-2)\n\\]\nA naive recursive call tree repeats \\(F(3)\\), \\(F(2)\\), etc., exponentially many times. By memoizing (storing) results after the first computation, we reuse them in \\(O(1)\\) time later.\n\n\nHow It Works (Plain Language)\n\nDefine the recurrence clearly.\nAdd a cache (dictionary or array) to store computed results.\nBefore each recursive call, check the cache.\nIf present, return cached value.\nOtherwise, compute, store, and return.\n\nThis approach preserves recursive elegance while matching iterative DP performance.\n\n\nExample Step by Step\nExample 1: Fibonacci Numbers\nNaive recursion: \\[\nF(n) = F(n-1) + F(n-2)\n\\]\nMemoized version:\n\n\n\nn\nF(n)\nCached?\n\n\n\n\n0\n0\nBase\n\n\n1\n1\nBase\n\n\n2\n1\nComputed\n\n\n3\n2\nComputed\n\n\n4\n3\nCached lookups\n\n\n\nTime drops from \\(O(2^n)\\) to \\(O(n)\\).\nExample 2: Binomial Coefficient\nRecurrence: \\[\nC(n, k) = C(n-1, k-1) + C(n-1, k)\n\\]\nWithout memoization: exponential With memoization: \\(O(nk)\\)\nExample 3: Coin Change\n\\[\n\\text{ways}(n) = \\text{ways}(n-\\text{coin}) + \\text{ways}(n, \\text{next})\n\\]\nMemoize by \\((n, \\text{index})\\) to avoid recomputing states.\n\n\nTiny Code (Python)\ndef fib_memo(n, memo={}):\n    if n in memo:\n        return memo[n]\n    if n &lt;= 1:\n        return n\n    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)\n    return memo[n]\nOr explicitly pass cache:\ndef fib_memo(n):\n    memo = {}\n    def helper(k):\n        if k in memo:\n            return memo[k]\n        if k &lt;= 1:\n            return k\n        memo[k] = helper(k-1) + helper(k-2)\n        return memo[k]\n    return helper(n)\n\n\nWhy It Matters\n\nRetains intuitive recursive structure\nCuts time complexity drastically\nNatural stepping stone to tabulation (bottom-up DP)\nEnables solving overlapping subproblem recurrences efficiently\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(S\\) be the set of all distinct subproblems. Without memoization, each is recomputed exponentially many times. With memoization, each \\(s \\in S\\) is computed exactly once. Thus, total time = \\(O(|S|)\\).\n\n\nTry It Yourself\n\nAdd memoization to naive Fibonacci.\nMemoize binomial coefficients \\(C(n,k)\\).\nApply memoization to knapsack recursion.\nCount total recursive calls with and without memoization.\n\n\n\nTest Cases\n\n\n\nProblem\nNaive Time\nMemoized Time\n\n\n\n\nFibonacci(40)\n\\(O(2^{40})\\)\n\\(O(40)\\)\n\n\nBinomial(20,10)\n\\(O(2^{20})\\)\n\\(O(200)\\)\n\n\nCoin Change(100)\n\\(O(2^n)\\)\n\\(O(n \\cdot k)\\)\n\n\n\n\n\nComplexity Summary\n\n\n\nMethod\nTime\nSpace\n\n\n\n\nRecursive\nExponential\n\\(O(n)\\) stack\n\n\nMemoized\nPolynomial (distinct subproblems)\nCache + stack\n\n\n\nMemoization blends clarity and efficiency, recursion that remembers. It turns naive exponential algorithms into elegant linear or polynomial solutions with a single insight: never solve the same problem twice.\n\n\n\n50 Characteristic Polynomial Solver\nThe Characteristic Polynomial Solver is a powerful algebraic technique for solving linear homogeneous recurrence relations with constant coefficients. It expresses the recurrence in terms of polynomial roots, giving closed-form solutions.\n\nWhat Problem Are We Solving?\nWhen faced with recurrences like:\n\\[\nT(n) = a_1 T(n-1) + a_2 T(n-2) + \\cdots + a_k T(n-k)\n\\]\nwe want a closed-form expression for \\(T(n)\\) instead of step-by-step computation. The characteristic polynomial captures the recurrence’s structure, its roots determine the general form of the solution.\n\n\nHow It Works (Plain Language)\n\nWrite the recurrence in standard form: \\[\nT(n) - a_1 T(n-1) - a_2 T(n-2) - \\cdots - a_k T(n-k) = 0\n\\]\nReplace \\(T(n-i)\\) with \\(r^{n-i}\\) to form a polynomial equation: \\[\nr^k - a_1 r^{k-1} - a_2 r^{k-2} - \\cdots - a_k = 0\n\\]\nSolve for roots \\(r_1, r_2, \\ldots, r_k\\).\nThe general solution is: \\[\nT(n) = c_1 r_1^n + c_2 r_2^n + \\cdots + c_k r_k^n\n\\]\nUse initial conditions to solve for constants \\(c_i\\).\n\nIf there are repeated roots, multiply by \\(n^p\\) for multiplicity \\(p\\).\n\n\nExample Step by Step\nExample 1: Fibonacci\nRecurrence: \\[\nF(n) = F(n-1) + F(n-2)\n\\]\nCharacteristic polynomial: \\[\nr^2 - r - 1 = 0\n\\]\nRoots: \\[\nr_1 = \\frac{1+\\sqrt{5}}{2}, \\quad r_2 = \\frac{1-\\sqrt{5}}{2}\n\\]\nGeneral solution: \\[\nF(n) = c_1 r_1^n + c_2 r_2^n\n\\]\nUsing \\(F(0)=0\\), \\(F(1)=1\\): \\[\nc_1 = \\frac{1}{\\sqrt{5}}, \\quad c_2 = -\\frac{1}{\\sqrt{5}}\n\\]\nSo: \\[\nF(n) = \\frac{1}{\\sqrt{5}}\\left(\\left(\\frac{1+\\sqrt{5}}{2}\\right)^n - \\left(\\frac{1-\\sqrt{5}}{2}\\right)^n\\right)\n\\]\nThis is Binet’s Formula.\nExample 2: \\(T(n) = 3T(n-1) - 2T(n-2)\\)\nCharacteristic polynomial: \\[\nr^2 - 3r + 2 = 0 \\implies (r-1)(r-2)=0\n\\]\nRoots: \\(r_1=1, , r_2=2\\)\nSolution: \\[\nT(n) = c_1(1)^n + c_2(2)^n = c_1 + c_2 2^n\n\\]\nUse base cases to find \\(c_1, c_2\\).\nExample 3: Repeated Roots\n\\[\nT(n) = 2T(n-1) - T(n-2)\n\\]\nCharacteristic: \\[\nr^2 - 2r + 1 = 0 \\implies (r-1)^2 = 0\n\\]\nSolution: \\[\nT(n) = (c_1 + c_2 n) \\cdot 1^n = c_1 + c_2 n\n\\]\n\n\nTiny Code (Python)\nimport sympy as sp\n\ndef solve_recurrence(coeffs, initials):\n    n = len(coeffs)\n    r = sp.symbols('r')\n    poly = rn - sum(coeffs[i]*r(n-i-1) for i in range(n))\n    roots = sp.roots(poly, r)\n    r_syms = list(roots.keys())\n    c = sp.symbols(' '.join([f'c{i+1}' for i in range(n)]))\n    Tn = sum(c[i]*r_syms[i]sp.symbols('n') for i in range(n))\n    equations = []\n    for i, val in enumerate(initials):\n        equations.append(Tn.subs(sp.symbols('n'), i) - val)\n    sol = sp.solve(equations, c)\n    return Tn.subs(sol)\nCall solve_recurrence([1, 1], [0, 1]) → Binet’s formula.\n\n\nWhy It Matters\n\nGives closed-form solutions for linear recurrences\nEliminates need for iteration or recursion\nConnects algorithm analysis to algebra and eigenvalues\nUsed in runtime analysis, combinatorics, and discrete modeling\n\n\n\nA Gentle Proof (Why It Works)\nSuppose recurrence: \\[\nT(n) = a_1 T(n-1) + \\cdots + a_k T(n-k)\n\\]\nAssume \\(T(n) = r^n\\):\n\\[\nr^n = a_1 r^{n-1} + \\cdots + a_k r^{n-k}\n\\]\nDivide by \\(r^{n-k}\\):\n\\[\nr^k = a_1 r^{k-1} + \\cdots + a_k\n\\]\nSolve polynomial for roots. Each root corresponds to an independent solution. By linearity, the sum of independent solutions is also a solution.\n\n\nTry It Yourself\n\nSolve \\(T(n)=2T(n-1)+T(n-2)\\) with \\(T(0)=0, T(1)=1\\).\nSolve \\(T(n)=T(n-1)+2T(n-2)\\) with \\(T(0)=2, T(1)=3\\).\nSolve with repeated root \\(r=1\\).\nVerify results numerically for \\(n=0\\ldots5\\).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nRecurrence\nPolynomial\nRoots\nClosed Form\n\n\n\n\n\\(F(n)=F(n-1)+F(n-2)\\)\n\\(r^2-r-1=0\\)\n\\(\\frac{1\\pm\\sqrt{5}}{2}\\)\nBinet\n\n\n\\(T(n)=3T(n-1)-2T(n-2)\\)\n\\(r^2-3r+2=0\\)\n1, 2\n\\(c_1+c_2 2^n\\)\n\n\n\\(T(n)=2T(n-1)-T(n-2)\\)\n\\((r-1)^2=0\\)\n1 (double)\n\\(c_1+c_2 n\\)\n\n\n\n\n\nComplexity Summary\n\n\n\nStep\nTime\nSpace\n\n\n\n\nSolve polynomial\n\\(O(k^3)\\)\n\\(O(k)\\)\n\n\nEvaluate closed form\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\nThe Characteristic Polynomial Solver is the algebraic heart of recurrence solving, turning repeated patterns into exact formulas through the power of roots and symmetry.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-6.-searching-basics",
    "href": "books/en-us/list-1.html#section-6.-searching-basics",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 6. Searching basics",
    "text": "Section 6. Searching basics\n\n51 Search Space Visualizer\nA Search Space Visualizer is a conceptual tool to map and understand the entire landscape of possibilities an algorithm explores. By modeling the search process as a tree or graph, you gain intuition about completeness, optimality, and complexity before diving into code.\n\nWhat Problem Are We Solving?\nWhen tackling problems like optimization, constraint satisfaction, or pathfinding, the solution isn’t immediate, we must explore a space of possibilities. Understanding how large that space is, how it grows, and how it can be pruned is crucial for algorithmic design.\nVisualizing the search space helps answer questions like:\n\nHow many states are reachable?\nHow deep or wide is the search?\nWhat’s the branching factor?\nWhere does the goal lie?\n\n\n\nHow It Works (Plain Language)\n\nModel states as nodes. Each represents a partial or complete solution.\nModel transitions as edges. Each move or decision takes you to a new state.\nDefine start and goal nodes. Typically, the root (start) expands toward one or more goals.\nTrace the exploration. Breadth-first explores level by level; depth-first dives deep.\nLabel nodes with cost or heuristic values if applicable (for A*, branch-and-bound, etc.).\n\nThis structure reveals not just correctness but also efficiency and complexity.\n\n\nExample Step by Step\nExample 1: Binary Search Tree Traversal\nFor array [1, 2, 3, 4, 5, 6, 7] and target = 6:\nSearch space (comparisons):\n        4\n       / \\\n      2   6\n     / \\ / \\\n    1  3 5  7\nPath explored: 4 → 6 (found)\nSearch space depth: \\(\\log_2 7 \\approx 3\\)\nExample 2: 8-Queens Problem\nEach level represents placing a queen in a new row. Branching factor shrinks as constraints reduce possibilities.\nVisualization shows 8! total paths, but pruning cuts most.\nExample 3: Maze Solver\nStates = grid cells; edges = possible moves.\nVisualization helps you see BFS’s wavefront vs DFS’s depth-first path.\n\n\nTiny Code (Python)\nfrom collections import deque\n\ndef visualize_bfs(graph, start):\n    visited = set()\n    queue = deque([(start, [start])])\n    while queue:\n        node, path = queue.popleft()\n        print(f\"Visiting: {node}, Path: {path}\")\n        visited.add(node)\n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                queue.append((neighbor, path + [neighbor]))\nUse on a small adjacency list to see BFS layers unfold.\n\n\nWhy It Matters\n\nBuilds intuition about algorithm behavior\nShows breadth vs depth tradeoffs\nReveals redundant paths and pruning opportunities\nUseful for teaching, debugging, and complexity estimation\n\n\n\nA Gentle Proof (Why It Works)\nLet each state \\(s \\in S\\) be connected by transitions \\(E\\). Search algorithms define an ordering of node expansion (DFS, BFS, heuristic-based). Visualizing \\(S\\) as a graph preserves:\n\nCompleteness: BFS explores all finite paths\nOptimality: with uniform cost, shortest path = first found\nComplexity: proportional to nodes generated (often \\(O(b^d)\\))\n\n\n\nTry It Yourself\n\nDraw search tree for binary search on 7 elements.\nVisualize DFS vs BFS on a maze.\nBuild search space for placing 4 queens on a \\(4\\times4\\) board.\nCompare path counts with and without pruning.\n\n\n\nTest Cases\n\n\n\nProblem\nSearch Space Size\nVisualization Insight\n\n\n\n\nBinary Search\n\\(\\log_2 n\\)\nNarrow, balanced\n\n\n8-Queens\n\\(8!\\)\nHeavy pruning needed\n\n\nMaze (10x10)\n\\(100\\) nodes\nBFS = wave, DFS = path\n\n\nSudoku\n\\(9^{81}\\)\nPrune with constraints\n\n\n\n\n\nComplexity Summary\n\n\n\nAlgorithm\nNodes Explored\nMemory\nVisualization\n\n\n\n\nBFS\n\\(O(b^d)\\)\n\\(O(b^d)\\)\nTree layers\n\n\nDFS\n\\(O(bd)\\)\n\\(O(d)\\)\nDeep path\n\n\nA*\n\\(O(b^d)\\)\n\\(O(b^d)\\)\nCost-guided frontier\n\n\n\nA Search Space Visualizer turns abstract computation into geometry, making invisible exploration visible, and helping you reason about complexity before coding.\n\n\n\n52 Decision Tree Depth Estimator\nA Decision Tree Depth Estimator helps you reason about how many questions, comparisons, or branching choices an algorithm must make in the worst, best, or average case. It models decision-making as a tree, where each node is a test and each leaf is an outcome.\n\nWhat Problem Are We Solving?\nAny algorithm that proceeds by comparisons or conditional branches (like sorting, searching, or classification) can be represented as a decision tree. Analyzing its depth gives insight into:\n\nWorst-case time complexity (longest path)\nBest-case time complexity (shortest path)\nAverage-case complexity (weighted path length)\n\nBy studying depth, we understand the minimum information needed to solve the problem.\n\n\nHow It Works (Plain Language)\n\nRepresent each comparison or condition as a branching node.\nFollow each branch based on true/false or less/greater outcomes.\nEach leaf represents a solved instance (e.g. sorted array, found key).\nThe depth = number of decisions on a path.\nMaximum depth → worst-case cost.\n\nThis model abstracts away details and focuses purely on information flow.\n\n\nExample Step by Step\nExample 1: Binary Search\n\nEach comparison halves the search space.\nDecision tree has depth \\(\\log_2 n\\).\nMinimum comparisons in worst case: \\(\\lceil \\log_2 n \\rceil\\).\n\nTree for \\(n=8\\) elements:\n          [mid=4]\n         /       \\\n     [mid=2]     [mid=6]\n     /   \\       /   \\\n   [1]   [3]   [5]   [7]\nDepth: \\(3 = \\log_2 8\\)\nExample 2: Comparison Sort\nEach leaf represents a possible ordering. A valid sorting tree must distinguish all \\(n!\\) orderings.\nSo:\n\\[\n2^h \\ge n! \\implies h \\ge \\log_2(n!)\n\\]\nThus, any comparison sort has lower bound: \\[\n\\Omega(n \\log n)\n\\]\nExample 3: Decision-Making Algorithm\nIf solving a yes/no classification with \\(b\\) possible outcomes, minimum number of comparisons required = \\(\\lceil \\log_2 b \\rceil\\).\n\n\nTiny Code (Python)\nimport math\n\ndef decision_tree_depth(outcomes):\n    # Minimum comparisons to distinguish outcomes\n    return math.ceil(math.log2(outcomes))\n\nprint(decision_tree_depth(8))  # 3\nprint(decision_tree_depth(120))  # ~7 (for 5!)\n\n\nWhy It Matters\n\nReveals theoretical limits (no sort faster than \\(O(n \\log n)\\) by comparison)\nModels decision complexity in search and optimization\nBridges information theory and algorithm design\nHelps compare branching strategies\n\n\n\nA Gentle Proof (Why It Works)\nEach comparison splits the search space in two. To distinguish \\(N\\) possible outcomes, need at least \\(h\\) comparisons such that: \\[\n2^h \\ge N\n\\]\nThus: \\[\nh \\ge \\lceil \\log_2 N \\rceil\n\\]\nFor sorting: \\[\nN = n! \\implies h \\ge \\log_2 (n!) = \\Omega(n \\log n)\n\\]\nThis bound holds independent of implementation, it’s a lower bound on information required.\n\n\nTry It Yourself\n\nBuild decision tree for 3-element sorting.\nCount comparisons for binary search on \\(n=16\\).\nEstimate lower bound for 4-element comparison sort.\nVisualize tree for classification with 8 classes.\n\n\n\nTest Cases\n\n\n\nProblem\nOutcomes\nDepth Bound\n\n\n\n\nBinary Search (n=8)\n8\n3\n\n\nSort 3 elements\n\\(3! = 6\\)\n\\(\\ge 3\\)\n\n\nSort 5 elements\n\\(5! = 120\\)\n\\(\\ge 7\\)\n\n\nClassify 8 outcomes\n8\n3\n\n\n\n\n\nComplexity Summary\n\n\n\n\n\n\n\n\n\nAlgorithm\nSearch Space\nDepth\nMeaning\n\n\n\n\nBinary Search\n\\(n\\)\n\\(\\log_2 n\\)\nWorst-case comparisons\n\n\nComparison Sort\n\\(n!\\)\n\\(\\log_2 n!\\)\nInfo-theoretic limit\n\n\nClassifier\n\\(b\\)\n\\(\\log_2 b\\)\nMin tests for \\(b\\) classes\n\n\n\nA Decision Tree Depth Estimator helps uncover the invisible “question complexity” behind every algorithm, how many decisions must be made, no matter how clever your code is.\n\n\n\n53 Comparison Counter\nA Comparison Counter measures how many times an algorithm compares elements or conditions, a direct way to understand its time complexity, efficiency, and practical performance. Counting comparisons gives insight into what really drives runtime, especially in comparison-based algorithms.\n\nWhat Problem Are We Solving?\nMany algorithms, sorting, searching, selection, optimization, revolve around comparisons. Every if, &lt;, or == is a decision that costs time.\nBy counting comparisons, we can:\n\nEstimate exact step counts for small inputs\nVerify asymptotic bounds (\\(O(n^2)\\), \\(O(n \\log n)\\), etc.)\nCompare different algorithms empirically\nIdentify hot spots in implementation\n\nThis turns performance from a vague idea into measurable data.\n\n\nHow It Works (Plain Language)\n\nInstrument the algorithm: wrap every comparison in a counter.\nIncrement the counter each time a comparison occurs.\nRun the algorithm with sample inputs.\nObserve patterns as input size grows.\nFit results to complexity functions (\\(n\\), \\(n \\log n\\), \\(n^2\\), etc.).\n\nThis gives both empirical evidence and analytic insight.\n\n\nExample Step by Step\nExample 1: Linear Search\nSearch through an array of size \\(n\\). Each comparison checks one element.\n\n\n\nCase\nComparisons\n\n\n\n\nBest\n1\n\n\nWorst\nn\n\n\nAverage\n\\(\\frac{n+1}{2}\\)\n\n\n\nSo: \\[\nT(n) = O(n)\n\\]\nExample 2: Binary Search\nEach step halves the search space.\n\n\n\nCase\nComparisons\n\n\n\n\nBest\n1\n\n\nWorst\n\\(\\lceil \\log_2 n \\rceil\\)\n\n\nAverage\n\\(\\approx \\log_2 n - 1\\)\n\n\n\nSo: \\[\nT(n) = O(\\log n)\n\\]\nExample 3: Bubble Sort\nFor array of length \\(n\\), each pass compares adjacent elements.\n\n\n\nPass\nComparisons\n\n\n\n\n1\n\\(n-1\\)\n\n\n2\n\\(n-2\\)\n\n\n…\n…\n\n\nn-1\n1\n\n\n\nTotal: \\[\nC(n) = (n-1)+(n-2)+\\cdots+1 = \\frac{n(n-1)}{2}\n\\]\nSo: \\[\nT(n) = O(n^2)\n\\]\n\n\nTiny Code (Python)\nclass Counter:\n    def __init__(self):\n        self.count = 0\n    def compare(self, a, b, op):\n        self.count += 1\n        if op == '&lt;': return a &lt; b\n        if op == '&gt;': return a &gt; b\n        if op == '==': return a == b\n\n# Example: Bubble Sort\ndef bubble_sort(arr):\n    c = Counter()\n    n = len(arr)\n    for i in range(n):\n        for j in range(n - i - 1):\n            if c.compare(arr[j], arr[j + 1], '&gt;'):\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr, c.count\nRun on small arrays to record exact comparison counts.\n\n\nWhy It Matters\n\nConverts abstract complexity into measurable data\nReveals hidden constants and practical performance\nUseful for algorithm profiling and pedagogy\nHelps confirm theoretical analysis\n\n\n\nA Gentle Proof (Why It Works)\nEach comparison corresponds to one node in the algorithm’s decision tree. The number of comparisons = number of nodes visited. Counting comparisons thus measures path length, which correlates to runtime for comparison-based algorithms.\nBy summing over all paths, we recover the exact cost function \\(C(n)\\).\n\n\nTry It Yourself\n\nCount comparisons in bubble sort vs insertion sort for \\(n=5\\).\nMeasure binary search comparisons for \\(n=16\\).\nCompare selection sort and merge sort.\nFit measured values to theoretical \\(O(n^2)\\) or \\(O(n \\log n)\\).\n\n\n\nTest Cases\n\n\n\nAlgorithm\nInput Size\nComparisons\nPattern\n\n\n\n\nLinear Search\n10\n10\n\\(O(n)\\)\n\n\nBinary Search\n16\n4\n\\(O(\\log n)\\)\n\n\nBubble Sort\n5\n10\n\\(\\frac{n(n-1)}{2}\\)\n\n\nMerge Sort\n8\n17\n\\(\\approx n \\log n\\)\n\n\n\n\n\nComplexity Summary\n\n\n\nAlgorithm\nBest Case\nWorst Case\nAverage Case\n\n\n\n\nLinear Search\n1\nn\n\\(\\frac{n+1}{2}\\)\n\n\nBinary Search\n1\n\\(\\log_2 n\\)\n\\(\\log_2 n - 1\\)\n\n\nBubble Sort\n\\(n-1\\)\n\\(\\frac{n(n-1)}{2}\\)\n\\(\\frac{n(n-1)}{2}\\)\n\n\n\nA Comparison Counter brings complexity theory to life, every if becomes a data point, and every loop reveals its true cost.\n\n\n\n54 Early Termination Heuristic\nAn Early Termination Heuristic is a strategy to stop an algorithm before full completion when the desired result is already guaranteed or further work won’t change the outcome. It’s a simple yet powerful optimization that saves time in best and average cases.\n\nWhat Problem Are We Solving?\nMany algorithms perform redundant work after the solution is effectively found or when additional steps no longer improve results. By detecting these conditions early, we can cut off unnecessary computation, reducing runtime without affecting correctness.\nKey question: “Can we stop now without changing the answer?”\n\n\nHow It Works (Plain Language)\n\nIdentify a stopping condition beyond the usual loop limit.\nCheck at each step if the result is already determined.\nExit early when the condition is satisfied.\nReturn partial result if it’s guaranteed to be final.\n\nThis optimization is common in search, sorting, simulation, and iterative convergence algorithms.\n\n\nExample Step by Step\nExample 1: Bubble Sort\nNormally runs \\(n-1\\) passes, even if array sorted early. Add a flag to track swaps; if none occur, terminate.\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        swapped = False\n        for j in range(n - i - 1):\n            if arr[j] &gt; arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True\n        if not swapped:\n            break  # early termination\n    return arr\nBest case: already sorted → 1 pass only \\[\nT(n) = O(n)\n\\] Worst case: reversed → still \\(O(n^2)\\)\nExample 2: Linear Search\nSearching for key \\(k\\) in array A:\n\nStop when found (don’t scan full array).\nAverage case improves from \\(O(n)\\) to \\(\\frac{n}{2}\\) comparisons.\n\nExample 3: Convergence Algorithms\nIn iterative solvers:\n\nStop when error &lt; ε (tolerance threshold).\nAvoids unnecessary extra iterations.\n\nExample 4: Constraint Search\nIn backtracking or branch-and-bound:\n\nStop exploring when solution cannot improve current best.\nReduces search space dramatically.\n\n\n\nWhy It Matters\n\nImproves average-case performance\nReduces energy and time in real-world systems\nMaintains correctness (never stops too early)\nEnables graceful degradation for approximate algorithms\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(f(i)\\) represent progress measure after \\(i\\) iterations. If \\(f(i)\\) satisfies a stopping invariant \\(P\\), then continuing further does not alter the final answer. Thus: \\[\n\\exists i &lt; n ;|; P(f(i)) = \\text{True} \\implies T(n) = i\n\\] reducing total operations from \\(n\\) to \\(i\\) in favorable cases.\n\n\nTry It Yourself\n\nAdd early stop to selection sort (when prefix sorted).\nApply tolerance check to Newton’s method.\nImplement linear search with immediate exit.\nCompare runtime with and without early termination.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nCondition\nBest Case\nWorst Case\n\n\n\n\n\n\nBubble Sort\nNo swaps in pass\n\\(O(n)\\)\n\\(O(n^2)\\)\n\n\n\n\nLinear Search\nFound early\n\\(O(1)\\)\n\\(O(n)\\)\n\n\n\n\nNewton’s Method\n$\nx_{i+1}-x_i\n&lt;$\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nDFS\nGoal found early\n\\(O(d)\\)\n\\(O(b^d)\\)\n\n\n\n\n\n\n\nComplexity Summary\n\n\n\nCase\nDescription\nTime\n\n\n\n\nBest\nEarly stop triggered\nReduced from \\(n\\) to \\(k\\)\n\n\nAverage\nDepends on data order\nOften sublinear\n\n\nWorst\nCondition never met\nSame as original\n\n\n\nAn Early Termination Heuristic adds a simple yet profound optimization, teaching algorithms when to quit, not just how to run.\n\n\n\n55 Sentinel Technique\nThe Sentinel Technique is a simple but elegant optimization that eliminates redundant boundary checks in loops by placing a special marker (the sentinel) at the end of a data structure. It’s a subtle trick that makes code faster, cleaner, and safer.\n\nWhat Problem Are We Solving?\nIn many algorithms, especially search and scanning loops, we repeatedly check for two things:\n\nWhether the element matches a target\nWhether we’ve reached the end of the structure\n\nThis double condition costs extra comparisons every iteration. By adding a sentinel value, we can guarantee termination and remove one check.\n\n\nHow It Works (Plain Language)\n\nAppend a sentinel value (e.g. target or infinity) to the end of the array.\nLoop until match found, without checking bounds.\nStop automatically when you hit the sentinel.\nCheck afterward if the match was real or sentinel-triggered.\n\nThis replaces:\nwhile i &lt; n and A[i] != key:\n    i += 1\nwith a simpler loop:\nA[n] = key\nwhile A[i] != key:\n    i += 1\nNo more bound check inside the loop.\n\n\nExample Step by Step\nExample 1: Linear Search with Sentinel\nWithout sentinel:\ndef linear_search(A, key):\n    for i in range(len(A)):\n        if A[i] == key:\n            return i\n    return -1\nEvery step checks both conditions.\nWith sentinel:\ndef linear_search_sentinel(A, key):\n    n = len(A)\n    A.append(key)  # add sentinel\n    i = 0\n    while A[i] != key:\n        i += 1\n    return i if i &lt; n else -1\n\nOnly one condition inside loop\nWorks for both found and not-found cases\n\nCost Reduction: from 2n+1 comparisons to n+1\nExample 2: Merging Sorted Lists\nAdd infinity sentinel at the end of each list:\n\nPrevents repeated end-of-array checks\nSimplifies inner loop logic\n\nE.g. in Merge Sort, use sentinel values to avoid if i &lt; n checks.\nExample 3: String Parsing\nAppend '\\0' (null terminator) so loops can stop automatically on sentinel. Used widely in C strings.\n\n\nWhy It Matters\n\nRemoves redundant checks\nSimplifies loop logic\nImproves efficiency and readability\nCommon in systems programming, parsing, searching\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n\\) be array length. Normally, each iteration does:\n\n1 comparison with bound\n1 comparison with key\n\nSo total \\(\\approx 2n+1\\) comparisons.\nWith sentinel:\n\n1 comparison per element\n1 final check after loop\n\nSo total \\(\\approx n+1\\)\nImprovement factor ≈ 2× speedup for long lists.\n\n\nTry It Yourself\n\nImplement sentinel linear search and count comparisons.\nAdd infinity sentinel in merge routine.\nWrite a parser that stops on sentinel '\\0'.\nCompare runtime vs standard implementation.\n\n\n\nTest Cases\n\n\n\nInput\nKey\nOutput\nComparisons\n\n\n\n\n[1,2,3,4], 3\n3\n2\n3\n\n\n[1,2,3,4], 5\n-1\n4\n5\n\n\n[] , 1\n-1\n0\n1\n\n\n\n\n\nComplexity Summary\n\n\n\n\n\n\n\n\n\nCase\nTime\nSpace\nNotes\n\n\n\n\nBest\n\\(O(1)\\)\n\\(O(1)\\)\nFound immediately\n\n\nWorst\n\\(O(n)\\)\n\\(O(1)\\)\nFound at end / not found\n\n\nImprovement\n~2× fewer comparisons\n+1 sentinel\nAlways safe\n\n\n\nThe Sentinel Technique is a quiet masterpiece of algorithmic design, proving that sometimes, one tiny marker can make a big difference.\n\n\n\n56 Binary Predicate Tester\nA Binary Predicate Tester is a simple yet fundamental tool for checking whether a condition involving two operands holds true, a building block for comparisons, ordering, filtering, and search logic across algorithms. It clarifies logic and promotes reuse by abstracting condition checks.\n\nWhat Problem Are We Solving?\nEvery algorithm depends on decisions, “Is this element smaller?”, “Are these two equal?”, “Does this satisfy the constraint?”. These yes/no questions are binary predicates: functions that return either True or False.\nBy formalizing them as reusable testers, we gain:\n\nClarity, separate logic from control flow\nReusability, pass as arguments to algorithms\nFlexibility, easily switch from &lt; to &gt; or ==\n\nThis underlies sorting, searching, and functional-style algorithms.\n\n\nHow It Works (Plain Language)\n\nDefine a predicate function that takes two arguments.\nReturns True if condition satisfied, False otherwise.\nUse the predicate inside loops, filters, or algorithmic decisions.\nSwap out predicates to change algorithm behavior dynamically.\n\nPredicates serve as the comparison layer, they don’t control flow, but inform it.\n\n\nExample Step by Step\nExample 1: Sorting by Predicate\nDefine different predicates:\ndef less(a, b): return a &lt; b\ndef greater(a, b): return a &gt; b\ndef equal(a, b): return a == b\nPass to sorting routine:\ndef compare_sort(arr, predicate):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if predicate(arr[j + 1], arr[j]):\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\nNow you can sort ascending or descending just by changing the predicate.\nExample 2: Binary Search Condition\nBinary search relies on predicate is_less(mid_value, key) to decide direction:\ndef is_less(a, b): return a &lt; b\nSo the decision step becomes:\nif is_less(arr[mid], key):\n    left = mid + 1\nelse:\n    right = mid - 1\nThis makes the comparison logic explicit, not buried inside control.\nExample 3: Filtering or Matching\ndef between(a, b): return a &lt; b\nfiltered = [x for x in data if between(x, 10)]\nEasily swap predicates for greater-than or equality checks.\n\n\nWhy It Matters\n\nEncapsulates decision logic cleanly\nEnables higher-order algorithms (pass functions as arguments)\nSimplifies testing and customization\nCore to generic programming and templates (C++, Python key functions)\n\n\n\nA Gentle Proof (Why It Works)\nPredicates abstract the notion of ordering or relation. If a predicate satisfies:\n\nReflexivity (\\(P(x,x)=\\text{False}\\) or True, as defined)\nAntisymmetry (\\(P(a,b) \\Rightarrow \\neg P(b,a)\\))\nTransitivity (\\(P(a,b)\\wedge P(b,c) \\Rightarrow P(a,c)\\))\n\nthen it defines a strict weak ordering, sufficient for sorting and searching algorithms.\nThus, correctness of algorithms depends on predicate consistency.\n\n\nTry It Yourself\n\nWrite predicates for &lt;, &gt;, ==, and divisible(a,b).\nUse them in a selection algorithm.\nTest sorting ascending and descending using same code.\nVerify predicate correctness (antisymmetry, transitivity).\n\n\n\nTest Cases\n\n\n\nPredicate\na\nb\nResult\nMeaning\n\n\n\n\nless\n3\n5\nTrue\n3 &lt; 5\n\n\ngreater\n7\n2\nTrue\n7 &gt; 2\n\n\nequal\n4\n4\nTrue\n4 == 4\n\n\ndivisible\n6\n3\nTrue\n6 % 3 == 0\n\n\n\n\n\nComplexity Summary\n\n\n\n\n\n\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nPredicate call\n\\(O(1)\\)\n\\(O(1)\\)\nConstant per check\n\n\nAlgorithm using predicate\nDepends on structure\n,\ne.g. sort: \\(O(n^2)\\)\n\n\n\nA Binary Predicate Tester turns hidden conditions into visible design, clarifying logic, encouraging reuse, and laying the foundation for generic algorithms that think in relationships, not instructions.\n\n\n\n57 Range Test Function\nA Range Test Function checks whether a given value lies within specified bounds, a universal operation in algorithms that handle intervals, array indices, numeric domains, or search constraints. It’s small but powerful, providing correctness and safety across countless applications.\n\nWhat Problem Are We Solving?\nMany algorithms operate on ranges, whether scanning arrays, iterating loops, searching intervals, or enforcing constraints. Repeatedly checking if low &lt;= x &lt;= high can clutter code and lead to subtle off-by-one errors.\nBy defining a reusable range test, we make such checks:\n\nCentralized (one definition, consistent semantics)\nReadable (intent clear at call site)\nSafe (avoid inconsistent inequalities)\n\n\n\nHow It Works (Plain Language)\n\nEncapsulate the boundary logic into a single function.\nInput: a value x and bounds (low, high).\nReturn: True if x satisfies range condition, else False.\nCan handle open, closed, or half-open intervals.\n\nVariants:\n\nClosed: [low, high] → low ≤ x ≤ high\nHalf-open: [low, high) → low ≤ x &lt; high\nOpen: (low, high) → low &lt; x &lt; high\n\n\n\nExample Step by Step\nExample 1: Array Index Bounds\nPrevent out-of-bounds access:\ndef in_bounds(i, n):\n    return 0 &lt;= i &lt; n\n\nif in_bounds(idx, len(arr)):\n    value = arr[idx]\nNo more manual range logic.\nExample 2: Range Filtering\nFilter values inside range [a, b]:\ndef in_range(x, low, high):\n    return low &lt;= x &lt;= high\n\ndata = [1, 3, 5, 7, 9]\nfiltered = [x for x in data if in_range(x, 3, 7)]\n# → [3, 5, 7]\nExample 3: Constraint Checking\nUsed in search or optimization algorithms:\nif not in_range(candidate, min_val, max_val):\n    continue  # skip invalid candidate\nKeeps loops clean and avoids boundary bugs.\nExample 4: Geometry / Interval Problems\nCheck interval overlap:\ndef overlap(a1, a2, b1, b2):\n    return in_range(a1, b1, b2) or in_range(b1, a1, a2)\n\n\nWhy It Matters\n\nPrevents off-by-one errors\nImproves code clarity and consistency\nEssential in loop guards, search boundaries, and validity checks\nEnables parameter validation and defensive programming\n\n\n\nA Gentle Proof (Why It Works)\nRange test expresses a logical conjunction: \\[\nP(x) = (x \\ge \\text{low}) \\land (x \\le \\text{high})\n\\] For closed intervals, the predicate is reflexive and transitive within the set \\([\\text{low}, \\text{high}]\\). By encoding this predicate as a function, correctness follows from elementary properties of inequalities.\nHalf-open variants preserve well-defined iteration bounds (important for array indices).\n\n\nTry It Yourself\n\nImplement in_open_range(x, low, high) for \\((low, high)\\).\nWrite in_half_open_range(i, 0, n) for loops.\nUse range test in binary search termination condition.\nCheck index validity in matrix traversal.\n\n\n\nTest Cases\n\n\n\nInput\nRange\nType\nResult\n\n\n\n\n5\n[1, 10]\nClosed\nTrue\n\n\n10\n[1, 10)\nHalf-open\nFalse\n\n\n0\n(0, 5)\nOpen\nFalse\n\n\n3\n[0, 3]\nClosed\nTrue\n\n\n\n\n\nComplexity Summary\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nRange check\n\\(O(1)\\)\n\\(O(1)\\)\nConstant-time comparison\n\n\nUsed per loop\n\\(O(n)\\)\n\\(O(1)\\)\nLinear overall\n\n\n\nA Range Test Function is a tiny guardrail with big impact, protecting correctness at every boundary and making algorithms easier to reason about.\n\n\n\n58 Search Invariant Checker\nA Search Invariant Checker ensures that key conditions (invariants) hold throughout a search algorithm’s execution. By maintaining these invariants, we guarantee correctness, prevent subtle bugs, and provide a foundation for proofs and reasoning.\n\nWhat Problem Are We Solving?\nWhen performing iterative searches (like binary search or interpolation search), we maintain certain truths that must always hold, such as:\n\nThe target, if it exists, is always within the current bounds.\nThe search interval shrinks every step.\nIndices remain valid and ordered.\n\nLosing these invariants can lead to infinite loops, incorrect results, or index errors. By explicitly checking invariants, we make correctness visible and testable.\n\n\nHow It Works (Plain Language)\n\nDefine invariants, conditions that must stay true during every iteration.\nAfter each update step, verify these conditions.\nIf an invariant fails, assert or log an error.\nUse invariants both for debugging and proofs.\n\nCommon search invariants:\n\n$ $\n$ $\nInterval size decreases: $ ( - ) $ shrinks each step\n\n\n\nExample Step by Step\nExample: Binary Search Invariants\nGoal: Maintain correct search window in \\([\\text{low}, \\text{high}]\\).\n\nInitialization: $ = 0 $, $ = n - 1 $\nInvariant 1: $ $\nInvariant 2: $ $\nStep: Compute mid, narrow range\nCheck: Each iteration, assert these invariants\n\n\n\nTiny Code (Python)\ndef binary_search(arr, target):\n    low, high = 0, len(arr) - 1\n    while low &lt;= high:\n        assert 0 &lt;= low &lt;= high &lt; len(arr), \"Invariant broken!\"\n        mid = (low + high) // 2\n\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\nIf the invariant fails, we catch logic errors early.\n\n\nWhy It Matters\n\nProof of correctness: Each iteration preserves truth\nDebugging aid: Detect logic flaws immediately\nSafety guarantee: Prevent invalid access or infinite loops\nDocumentation: Clarifies algorithm intent\n\n\n\nA Gentle Proof (Why It Works)\nSuppose invariant \\(P\\) holds before iteration. The update step transforms state \\((\\text{low}, \\text{high})\\) to \\((\\text{low}', \\text{high}')\\).\nWe prove:\n\nBase Case: \\(P\\) holds before first iteration (initialization)\nInductive Step: If \\(P\\) holds before iteration, and update rules maintain \\(P\\), then \\(P\\) holds afterward\n\nHence, by induction, \\(P\\) always holds. This ensures algorithm correctness.\n\n\nTry It Yourself\n\nAdd invariants to ternary search\nProve binary search correctness using invariant preservation\nTest boundary cases (empty array, one element)\nVisualize shrinking interval and check invariant truth at each step\n\n\n\nTest Cases\n\n\n\nInput Array\nTarget\nInvariants Hold\nResult\n\n\n\n\n[1, 3, 5, 7, 9]\n5\nYes\nIndex 2\n\n\n[2, 4, 6]\n3\nYes\nNot found\n\n\n[1]\n1\nYes\nIndex 0\n\n\n[]\n10\nYes\nNot found\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nCheck invariant\n\\(O(1)\\)\n\\(O(1)\\)\nConstant-time check\n\n\nTotal search\n\\(O(\\log n)\\)\n\\(O(1)\\)\nPreserves correctness\n\n\n\nThe Search Invariant Checker turns implicit assumptions into explicit guarantees, making your search algorithms not only fast but provably correct.\n\n\n\n59 Probe Counter\nA Probe Counter tracks how many probes or lookup attempts a search algorithm performs. It’s a diagnostic tool to understand efficiency and compare performance between different search strategies or data structures.\n\nWhat Problem Are We Solving?\nIn searching (especially in hash tables, linear probing, or open addressing), performance depends not just on complexity but on how many probes are required to find or miss an element.\nBy counting probes, we:\n\nReveal the cost of each search\nCompare performance under different load factors\nDiagnose clustering or inefficient probing patterns\n\n\n\nHow It Works (Plain Language)\n\nInitialize a counter probes = 0.\nEach time the algorithm checks a position or node, increment probes.\nWhen the search ends, record or return the probe count.\nUse statistics (mean, max, variance) to measure performance.\n\n\n\nExample Step by Step\nExample: Linear Probing in a Hash Table\n\nCompute hash: \\(h = \\text{key} \\bmod m\\)\nStart at \\(h\\), check slot\nIf collision, move to next slot\nIncrement probes each time\nStop when slot is empty or key is found\n\nIf the table is nearly full, probe count increases, revealing efficiency loss.\n\n\nTiny Code (Python)\ndef linear_probe_search(table, key):\n    m = len(table)\n    h = key % m\n    probes = 0\n    i = 0\n\n    while table[(h + i) % m] is not None:\n        probes += 1\n        if table[(h + i) % m] == key:\n            return (h + i) % m, probes\n        i += 1\n        if i == m:\n            break  # table full\n    return None, probes\nExample run:\ntable = [10, 21, 32, None, None]\nindex, probes = linear_probe_search(table, 21)\n# probes = 1\n\n\nWhy It Matters\n\nPerformance insight: Understand search cost beyond asymptotics\nClustering detection: Reveal poor distribution or collisions\nLoad factor tuning: Find thresholds before degradation\nAlgorithm comparison: Evaluate quadratic vs linear probing\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(L\\) be the load factor (fraction of table filled). Expected probes for a successful search in linear probing:\n\\[\nE[P_{\\text{success}}] = \\frac{1}{2}\\left(1 + \\frac{1}{1 - L}\\right)\n\\]\nExpected probes for unsuccessful search:\n\\[\nE[P_{\\text{fail}}] = \\frac{1}{2}\\left(1 + \\frac{1}{(1 - L)^2}\\right)\n\\]\nAs \\(L \\to 1\\), probe counts grow rapidly, performance decays.\n\n\nTry It Yourself\n\nCreate a hash table with linear probing\nInsert keys at different load factors\nMeasure probe counts for hits and misses\nCompare linear vs quadratic probing\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nTable (size 7)\nKey\nLoad Factor\nExpected Probes\nNotes\n\n\n\n\n[10, 21, 32, None…]\n21\n0.4\n1\nDirect hit\n\n\n[10, 21, 32, 43, 54]\n43\n0.7\n3\nClustered region\n\n\n[10, 21, 32, 43, 54]\n99\n0.7\n5\nMiss after probing\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Expected)\nTime (Worst)\nSpace\n\n\n\n\nProbe count\n\\(O(1)\\) per step\n\\(O(n)\\)\n\\(O(1)\\)\n\n\nTotal search\n\\(O(1)\\) average\n\\(O(n)\\)\n\\(O(1)\\)\n\n\n\nBy counting probes, we move from theory to measured understanding, a simple metric that reveals the hidden costs behind collisions, load factors, and search efficiency.\n\n\n\n60 Cost Curve Plotter\nA Cost Curve Plotter visualizes how an algorithm’s running cost grows as the input size increases. It turns abstract complexity into a tangible curve, helping you compare theoretical and empirical performance side by side.\n\nWhat Problem Are We Solving?\nBig-O notation tells us how cost scales, but not how much or where performance starts to break down. A cost curve lets you:\n\nSee real growth vs theoretical models\nIdentify crossover points between algorithms\nDetect anomalies or overhead\nBuild intuition about efficiency and scaling\n\n\n\nHow It Works (Plain Language)\n\nChoose an algorithm and a range of input sizes.\nFor each \\(n\\), run the algorithm and record:\n\nTime cost (runtime)\nSpace cost (memory usage)\nOperation count\n\nPlot \\((n, \\text{cost}(n))\\) points\nOverlay theoretical curves (\\(O(n)\\), \\(O(n \\log n)\\), \\(O(n^2)\\)) for comparison\n\nThis creates a visual map of performance over scale.\n\n\nExample Step by Step\nLet’s measure sorting cost for different input sizes:\n\n\n\nn\nTime (ms)\n\n\n\n\n100\n0.3\n\n\n500\n2.5\n\n\n1000\n5.2\n\n\n2000\n11.3\n\n\n4000\n23.7\n\n\n\nPlot these points. The curve shape suggests \\(O(n \\log n)\\) behavior.\n\n\nTiny Code (Python + Matplotlib)\nimport time, random, matplotlib.pyplot as plt\n\ndef measure_cost(algorithm, sizes):\n    results = []\n    for n in sizes:\n        arr = [random.randint(0, 100000) for _ in range(n)]\n        start = time.time()\n        algorithm(arr)\n        end = time.time()\n        results.append((n, end - start))\n    return results\n\ndef plot_cost_curve(results):\n    xs, ys = zip(*results)\n    plt.plot(xs, ys, marker='o')\n    plt.xlabel(\"Input size (n)\")\n    plt.ylabel(\"Time (seconds)\")\n    plt.title(\"Algorithm Cost Curve\")\n    plt.grid(True)\n    plt.show()\n\n\nWhy It Matters\n\nBrings Big-O to life\nVisual debugging, detect unexpected spikes\nCompare algorithms empirically\nTune thresholds, know when to switch strategies\n\n\n\nA Gentle Proof (Why It Works)\nIf theoretical cost is \\(f(n)\\) and empirical cost is \\(g(n)\\), then we expect:\n\\[\n\\lim_{n \\to \\infty} \\frac{g(n)}{f(n)} = c\n\\]\nwhere \\(c\\) is a constant scaling factor.\nThe plotted curve visually approximates \\(g(n)\\); comparing its shape to \\(f(n)\\) reveals whether the complexity class matches expectations.\n\n\nTry It Yourself\n\nCompare bubble sort vs merge sort vs quicksort.\nOverlay \\(n\\), \\(n \\log n\\), and \\(n^2\\) reference curves.\nExperiment with different data distributions (sorted, reversed).\nPlot both time and memory cost curves.\n\n\n\nTest Cases\n\n\n\nAlgorithm\nInput Size\nTime (ms)\nShape\nMatch\n\n\n\n\nBubble Sort\n1000\n80\nQuadratic\n\\(O(n^2)\\)\n\n\nMerge Sort\n1000\n5\nLinearithmic\n\\(O(n \\log n)\\)\n\n\nQuick Sort\n1000\n3\nLinearithmic\n\\(O(n \\log n)\\)\n\n\n\n\n\nComplexity\n\n\n\nAspect\nCost\nNotes\n\n\n\n\nMeasurement\n\\(O(k \\cdot T(n))\\)\n\\(k\\) sample sizes measured\n\n\nPlotting\n\\(O(k)\\)\nDraw curve from \\(k\\) points\n\n\nSpace\n\\(O(k)\\)\nStore measurement data\n\n\n\nThe Cost Curve Plotter turns theory into shape, a simple graph that makes scaling behavior and trade-offs instantly clear.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-7.-sorting-basics",
    "href": "books/en-us/list-1.html#section-7.-sorting-basics",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 7. Sorting basics",
    "text": "Section 7. Sorting basics\n\n61 Swap Counter\nA Swap Counter tracks the number of element swaps performed during a sorting process. It helps us understand how much rearrangement an algorithm performs and serves as a diagnostic for efficiency, stability, and input sensitivity.\n\nWhat Problem Are We Solving?\nMany sorting algorithms (like Bubble Sort, Selection Sort, or Quick Sort) rearrange elements through swaps. Counting swaps shows how “active” the algorithm is:\n\nBubble Sort → high swap count\nInsertion Sort → fewer swaps on nearly sorted input\nSelection Sort → fixed number of swaps\n\nBy tracking swaps, we compare algorithms on data movement cost, not just comparisons.\n\n\nHow It Works (Plain Language)\n\nInitialize a swap_count = 0.\nEach time two elements exchange positions, increment the counter.\nAt the end, report swap_count to measure rearrangement effort.\nUse results to compare sorting strategies or analyze input patterns.\n\n\n\nExample Step by Step\nExample: Bubble Sort on [3, 2, 1]\n\nCompare 3 and 2 → swap → count = 1 → [2, 3, 1]\nCompare 3 and 1 → swap → count = 2 → [2, 1, 3]\nCompare 2 and 1 → swap → count = 3 → [1, 2, 3]\n\nTotal swaps: 3\nIf input is [1, 2, 3], no swaps occur, cost reflects sortedness.\n\n\nTiny Code (Python)\ndef bubble_sort_with_swaps(arr):\n    n = len(arr)\n    swaps = 0\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] &gt; arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swaps += 1\n    return arr, swaps\nExample:\narr, swaps = bubble_sort_with_swaps([3, 2, 1])\n# swaps = 3\n\n\nWhy It Matters\n\nQuantifies data movement cost\nMeasures input disorder (zero swaps → already sorted)\nCompares algorithms on swap efficiency\nReveals adaptive behavior in real data\n\n\n\nA Gentle Proof (Why It Works)\nEvery swap reduces the inversion count by one. An inversion is a pair \\((i, j)\\) such that \\(i &lt; j\\) and \\(a_i &gt; a_j\\).\nIf initial inversion count = \\(I\\), and each swap fixes one inversion:\n\\[\n\\text{Total Swaps} = I_{\\text{initial}}\n\\]\nThus, swap count directly equals disorder measure, a meaningful cost metric.\n\n\nTry It Yourself\n\nCount swaps for Bubble Sort, Insertion Sort, and Selection Sort.\nRun on sorted, reversed, and random lists.\nCompare counts, which adapts best to nearly sorted data?\nPlot swap count vs input size.\n\n\n\nTest Cases\n\n\n\nInput\nAlgorithm\nSwaps\nObservation\n\n\n\n\n[3, 2, 1]\nBubble Sort\n3\nFull reversal\n\n\n[1, 2, 3]\nBubble Sort\n0\nAlready sorted\n\n\n[2, 3, 1]\nInsertion Sort\n2\nMoves minimal elements\n\n\n[3, 1, 2]\nSelection Sort\n2\nSwaps once per position\n\n\n\n\n\nComplexity\n\n\n\nMetric\nCost\nNotes\n\n\n\n\nTime (Tracking)\n\\(O(1)\\)\nIncrement counter per swap\n\n\nTotal Swaps\n\\(O(n^2)\\)\nWorst case for Bubble Sort\n\n\nSpace\n\\(O(1)\\)\nConstant extra memory\n\n\n\nA Swap Counter offers a clear window into sorting dynamics, revealing how “hard” the algorithm works and how far the input is from order.\n\n\n\n62 Inversion Counter\nAn Inversion Counter measures how far a sequence is from being sorted by counting all pairs that are out of order. It’s a numerical measure of disorder, zero for a sorted list, maximum for a fully reversed one.\n\nWhat Problem Are We Solving?\nSorting algorithms fix inversions. Each inversion is a pair \\((i, j)\\) such that \\(i &lt; j\\) and \\(a_i &gt; a_j\\). Counting inversions gives us:\n\nA quantitative measure of unsortedness\nA way to analyze algorithm progress\nInsight into best-case vs worst-case behavior\n\nThis metric is also used in Kendall tau distance, ranking comparisons, and adaptive sorting research.\n\n\nHow It Works (Plain Language)\n\nTake an array \\(A = [a_1, a_2, \\ldots, a_n]\\).\nFor each pair \\((i, j)\\) where \\(i &lt; j\\), check if \\(a_i &gt; a_j\\).\nIncrement count for each inversion found.\nA sorted array has \\(0\\) inversions; a reversed one has \\(\\frac{n(n-1)}{2}\\).\n\n\n\nExample Step by Step\nArray: [3, 1, 2]\n\n(3, 1): inversion\n(3, 2): inversion\n(1, 2): no inversion\n\nTotal inversions: 2\nA perfect diagnostic: small count → nearly sorted.\n\n\nTiny Code (Brute Force)\ndef count_inversions_bruteforce(arr):\n    count = 0\n    n = len(arr)\n    for i in range(n):\n        for j in range(i + 1, n):\n            if arr[i] &gt; arr[j]:\n                count += 1\n    return count\nOutput: count_inversions_bruteforce([3, 1, 2]) → 2\n\n\nOptimized Approach (Merge Sort)\nCounting inversions can be done in \\(O(n \\log n)\\) by modifying merge sort.\ndef count_inversions_merge(arr):\n    def merge_count(left, right):\n        i = j = inv = 0\n        merged = []\n        while i &lt; len(left) and j &lt; len(right):\n            if left[i] &lt;= right[j]:\n                merged.append(left[i])\n                i += 1\n            else:\n                merged.append(right[j])\n                inv += len(left) - i\n                j += 1\n        merged += left[i:]\n        merged += right[j:]\n        return merged, inv\n\n    def sort_count(sub):\n        if len(sub) &lt;= 1:\n            return sub, 0\n        mid = len(sub) // 2\n        left, invL = sort_count(sub[:mid])\n        right, invR = sort_count(sub[mid:])\n        merged, invM = merge_count(left, right)\n        return merged, invL + invR + invM\n\n    _, total = sort_count(arr)\n    return total\nResult: \\(O(n \\log n)\\) instead of \\(O(n^2)\\).\n\n\nWhy It Matters\n\nQuantifies disorder precisely\nUsed in sorting network analysis\nPredicts best-case improvements for adaptive sorts\nConnects to ranking correlation metrics\n\n\n\nA Gentle Proof (Why It Works)\nEvery swap in a stable sort fixes exactly one inversion. If we let \\(I\\) denote total inversions:\n\\[\nI_{\\text{sorted}} = 0, \\quad I_{\\text{reverse}} = \\frac{n(n-1)}{2}\n\\]\nHence, inversion count measures distance to sorted order, a lower bound on swaps needed by any comparison sort.\n\n\nTry It Yourself\n\nCount inversions for sorted, reversed, and random arrays.\nPlot inversion count vs swap count.\nTest merge sort counter vs brute force counter.\nMeasure how inversion count affects adaptive algorithms.\n\n\n\nTest Cases\n\n\n\nInput\nInversions\nInterpretation\n\n\n\n\n[1, 2, 3]\n0\nAlready sorted\n\n\n[3, 2, 1]\n3\nFully reversed\n\n\n[2, 3, 1]\n2\nTwo pairs out of order\n\n\n[1, 3, 2]\n1\nSlightly unsorted\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nMethod\nTime\nSpace\nNotes\n\n\n\n\nBrute Force\n\\(O(n^2)\\)\n\\(O(1)\\)\nSimple but slow\n\n\nMerge Sort Based\n\\(O(n \\log n)\\)\n\\(O(n)\\)\nEfficient for large arrays\n\n\n\nAn Inversion Counter transforms “how sorted is this list?” into a precise number, perfect for analysis, comparison, and designing smarter sorting algorithms.\n\n\n\n63 Stability Checker\nA Stability Checker verifies whether a sorting algorithm preserves the relative order of equal elements. Stability is essential when sorting complex records with multiple keys, ensuring secondary attributes remain in order after sorting by a primary one.\n\nWhat Problem Are We Solving?\nWhen sorting, sometimes values tie, they’re equal under the primary key. A stable sort keeps these tied elements in their original order. For example, sorting students by grade while preserving the order of names entered earlier.\nWithout stability, sorting by multiple keys becomes error-prone, and chained sorts may lose meaning.\n\n\nHow It Works (Plain Language)\n\nLabel each element with its original position.\nPerform the sort.\nAfter sorting, for all pairs with equal keys, check if the original indices remain in ascending order.\nIf yes, the algorithm is stable. Otherwise, it’s not.\n\n\n\nExample Step by Step\nArray with labels: [(A, 3), (B, 1), (C, 3)] Sort by value → [ (B, 1), (A, 3), (C, 3) ]\nCheck ties:\n\nElements with value 3: A before C, and A’s original index &lt; C’s original index → stable.\n\nIf result was [ (B, 1), (C, 3), (A, 3) ], order of equals reversed → unstable.\n\n\nTiny Code (Python)\ndef is_stable_sort(original, sorted_arr, key=lambda x: x):\n    positions = {}\n    for idx, val in enumerate(original):\n        positions.setdefault(key(val), []).append(idx)\n    \n    last_seen = {}\n    for val in sorted_arr:\n        k = key(val)\n        pos = positions[k].pop(0)\n        if k in last_seen and last_seen[k] &gt; pos:\n            return False\n        last_seen[k] = pos\n    return True\nUsage:\ndata = [('A', 3), ('B', 1), ('C', 3)]\nsorted_data = sorted(data, key=lambda x: x[1])\nis_stable_sort(data, sorted_data, key=lambda x: x[1])  # True\n\n\nWhy It Matters\n\nPreserves secondary order: essential for multi-key sorts\nChaining safety: sort by multiple fields step-by-step\nPredictable results: avoids random reorder of equals\nCommon property: Merge Sort, Insertion Sort stable; Quick Sort not (by default)\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(a_i\\) and \\(a_j\\) be elements with equal keys \\(k\\). If \\(i &lt; j\\) in the input and positions of \\(a_i\\) and \\(a_j\\) after sorting are \\(p_i\\) and \\(p_j\\), then the algorithm is stable if and only if:\n\\[\ni &lt; j \\implies p_i &lt; p_j \\text{ whenever } k_i = k_j\n\\]\nChecking this property across all tied keys confirms stability.\n\n\nTry It Yourself\n\nCompare stable sort (Merge Sort) vs unstable sort (Selection Sort).\nSort list of tuples by one key, check tie preservation.\nChain sorts (first by last name, then by first name).\nRun checker to confirm final stability.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nInput\nSorted Result\nStable?\nExplanation\n\n\n\n\n[(A,3),(B,1),(C,3)]\n[(B,1),(A,3),(C,3)]\nYes\nA before C preserved\n\n\n[(A,3),(B,1),(C,3)]\n[(B,1),(C,3),(A,3)]\nNo\nA and C order reversed\n\n\n[(1,10),(2,10),(3,10)]\n[(1,10),(2,10),(3,10)]\nYes\nAll tied, all preserved\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nChecking\n\\(O(n)\\)\n\\(O(n)\\)\nOne pass over sorted array\n\n\nSorting\nDepends\n,\nChecker independent of sort\n\n\n\nThe Stability Checker ensures your sorts respect order among equals, a small step that safeguards multi-key sorting correctness and interpretability.\n\n\n\n64 Comparison Network Visualizer\nA Comparison Network Visualizer shows how fixed sequences of comparisons sort elements, revealing the structure of sorting networks. These diagrams help us see how parallel sorting works, step by step, independent of input data.\n\nWhat Problem Are We Solving?\nSorting networks are data-oblivious, their comparison sequence is fixed, not driven by data. To understand or design them, we need a clear visual of which elements compare and when. The visualizer turns an abstract sequence of comparisons into a layered network diagram.\nThis is key for:\n\nAnalyzing parallel sorting\nDesigning hardware-based sorters\nStudying bitonic or odd-even merges\n\n\n\nHow It Works (Plain Language)\n\nRepresent each element as a horizontal wire.\nDraw a vertical comparator line connecting the two wires being compared.\nGroup comparators into layers that can run in parallel.\nThe network executes layer by layer, swapping elements if out of order.\n\nResult: a visual map of sorting logic.\n\n\nExample Step by Step\nSorting 4 elements with Bitonic Sort network:\nLayer 1: Compare (0,1), (2,3)\nLayer 2: Compare (0,2), (1,3)\nLayer 3: Compare (1,2)\nVisual:\n0 ──●────┐─────●───\n1 ──●─┐──┼──●──┼───\n2 ───┼─●──●─┘──●───\n3 ───┼────●────┘───\nEach dot pair = comparator. The structure is static, independent of values.\n\n\nTiny Code (Python)\ndef visualize_network(n, layers):\n    wires = [['─'] * (len(layers) + 1) for _ in range(n)]\n\n    for layer_idx, layer in enumerate(layers):\n        for (i, j) in layer:\n            wires[i][layer_idx] = '●'\n            wires[j][layer_idx] = '●'\n    for i in range(n):\n        print(f\"{i}: \" + \"─\".join(wires[i]))\n\nlayers = [[(0,1), (2,3)], [(0,2), (1,3)], [(1,2)]]\nvisualize_network(4, layers)\nThis prints a symbolic visualization of comparator layers.\n\n\nWhy It Matters\n\nReveals parallelism in sorting logic\nHelps debug data-oblivious algorithms\nUseful for hardware and GPU design\nFoundation for Bitonic, Odd-Even Merge, and Batcher networks\n\n\n\nA Gentle Proof (Why It Works)\nA sorting network guarantees correctness if it sorts all binary sequences of length \\(n\\).\nBy the Zero-One Principle:\n\nIf a comparison network correctly sorts all sequences of 0s and 1s, it correctly sorts all sequences of arbitrary numbers.\n\nSo visualizing comparators ensures completeness and layer correctness.\n\n\nTry It Yourself\n\nDraw a 4-input bitonic sorting network.\nVisualize how comparators “flow” through layers.\nCheck how many layers can run in parallel.\nTest sorting 0/1 sequences manually through the network.\n\n\n\nTest Cases\n\n\n\nInputs\nNetwork Type\nLayers\nSorted Output\n\n\n\n\n[3,1,4,2]\nBitonic Sort\n3\n[1,2,3,4]\n\n\n[1,0,1,0]\nOdd-Even Merge\n3\n[0,0,1,1]\n\n\n\n\n\nComplexity\n\n\n\nMetric\nValue\nNotes\n\n\n\n\nComparators\n\\(O(n \\log^2 n)\\)\nBatcher’s network complexity\n\n\nDepth\n\\(O(\\log^2 n)\\)\nLayers executed in parallel\n\n\nSpace\n\\(O(n)\\)\nOne wire per input\n\n\n\nA Comparison Network Visualizer makes parallel sorting tangible, every comparator and layer visible, transforming abstract hardware logic into a clear, educational blueprint.\n\n\n\n65 Adaptive Sort Detector\nAn Adaptive Sort Detector measures how “sorted” an input sequence already is and predicts whether an algorithm can take advantage of it. It’s a diagnostic tool that estimates presortedness and guides the choice of an adaptive sorting algorithm.\n\nWhat Problem Are We Solving?\nNot all inputs are random, many are partially sorted. Some algorithms (like Insertion Sort or Timsort) perform much faster on nearly sorted data. We need a way to detect sortedness before choosing the right strategy.\nAn adaptive detector quantifies how close an input is to sorted order.\n\n\nHow It Works (Plain Language)\n\nDefine a measure of disorder (e.g., number of inversions, runs, or local misplacements).\nTraverse the array, counting indicators of unsortedness.\nReturn a metric (e.g., 0 = fully sorted, 1 = fully reversed).\nUse this score to decide whether to apply:\n\nSimple insertion-like sort (for nearly sorted data)\nGeneral-purpose sort (for random data)\n\n\n\n\nExample Step by Step\nArray: [1, 2, 4, 3, 5, 6]\n\nCompare adjacent pairs:\n\n1 ≤ 2 (ok)\n2 ≤ 4 (ok)\n4 &gt; 3 (disorder)\n3 ≤ 5 (ok)\n5 ≤ 6 (ok)\n\nCount = 1 local inversion\n\nSortedness score: \\[\ns = 1 - \\frac{\\text{disorder}}{n-1} = 1 - \\frac{1}{5} = 0.8\n\\]\n80% sorted, good candidate for adaptive sort.\n\n\nTiny Code (Python)\ndef adaptive_sort_detector(arr):\n    disorder = 0\n    for i in range(len(arr) - 1):\n        if arr[i] &gt; arr[i + 1]:\n            disorder += 1\n    return 1 - disorder / max(1, len(arr) - 1)\n\narr = [1, 2, 4, 3, 5, 6]\nscore = adaptive_sort_detector(arr)\n# score = 0.8\nYou can use this score to select algorithms dynamically.\n\n\nWhy It Matters\n\nDetects near-sorted input efficiently\nEnables algorithm selection at runtime\nSaves time on real-world data (logs, streams, merges)\nCore idea behind Timsort’s run detection\n\n\n\nA Gentle Proof (Why It Works)\nIf an algorithm’s time complexity depends on disorder \\(d\\), e.g. \\(O(n + d)\\), and \\(d = O(1)\\) for nearly sorted arrays, then the adaptive algorithm approaches linear time.\nThe detector approximates \\(d\\), helping us decide when \\(O(n + d)\\) beats \\(O(n \\log n)\\).\n\n\nTry It Yourself\n\nTest arrays with 0, 10%, 50%, and 100% disorder.\nCompare runtime of Insertion Sort vs Merge Sort.\nUse inversion counting for more precise detection.\nIntegrate detector into a hybrid sorting routine.\n\n\n\nTest Cases\n\n\n\nInput\nDisorder\nScore\nRecommendation\n\n\n\n\n[1,2,3,4,5]\n0\n1.0\nInsertion Sort\n\n\n[1,3,2,4,5]\n1\n0.8\nAdaptive Sort\n\n\n[3,2,1]\n2\n0.0\nMerge / Quick Sort\n\n\n[2,1,3,5,4]\n2\n0.6\nAdaptive Sort\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nDisorder check\n\\(O(n)\\)\n\\(O(1)\\)\nSingle scan\n\n\nSorting (chosen)\nAdaptive\n,\nDepends on algorithm selected\n\n\n\nThe Adaptive Sort Detector bridges theory and pragmatism, quantifying how ordered your data is and guiding smarter algorithm choices for real-world performance.\n\n\n\n66 Sorting Invariant Checker\nA Sorting Invariant Checker verifies that key ordering conditions hold throughout a sorting algorithm’s execution. It’s used to reason about correctness step by step, ensuring that each iteration preserves progress toward a fully sorted array.\n\nWhat Problem Are We Solving?\nWhen debugging or proving correctness of sorting algorithms, we need to ensure that certain invariants (conditions that must always hold) remain true. If any invariant breaks, the algorithm may produce incorrect output, even if it “looks” right at a glance.\nA sorting invariant formalizes what “partial progress” means. Examples:\n\n“All elements before index i are in sorted order.”\n“All elements beyond pivot are greater or equal to it.”\n“Heap property holds at every node.”\n\n\n\nHow It Works (Plain Language)\n\nDefine one or more invariants that describe correctness.\nAfter each iteration or recursion step, check that these invariants still hold.\nIf any fail, stop and debug, the algorithm logic is wrong.\nOnce sorting finishes, the global invariant (sorted array) must hold.\n\nThis approach is key for formal verification and debuggable code.\n\n\nExample Step by Step\nInsertion Sort invariant:\n\nBefore processing element i, the subarray arr[:i] is sorted.\n\n\nInitially i = 1: subarray [arr[0]] is sorted.\nAfter inserting arr[1], subarray [arr[0:2]] is sorted.\nBy induction, full array sorted at end.\n\nCheck after every insertion: assert arr[:i] == sorted(arr[:i])\n\n\nTiny Code (Python)\ndef insertion_sort_with_invariant(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= 0 and arr[j] &gt; key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n        # Check invariant\n        assert arr[:i+1] == sorted(arr[:i+1]), f\"Invariant broken at i={i}\"\n    return arr\nIf invariant fails, an assertion error reveals the exact iteration.\n\n\nWhy It Matters\n\nBuilds correctness proofs via induction\nEarly bug detection, pinpoints iteration errors\nClarifies algorithm intent\nTeaches structured reasoning about program logic\n\nUsed in:\n\nFormal proofs (loop invariants)\nAlgorithm verification\nEducation and analysis\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(P(i)\\) denote the invariant “prefix of length \\(i\\) is sorted.”\n\nBase case: \\(P(1)\\) holds trivially.\nInductive step: If \\(P(i)\\) holds, inserting next element keeps \\(P(i+1)\\) true.\n\nBy induction, \\(P(n)\\) holds, full array is sorted.\nThus, the invariant framework guarantees correctness if each step preserves truth.\n\n\nTry It Yourself\n\nAdd invariants to Selection Sort (“min element placed at index i”).\nAdd heap property invariant to Heap Sort.\nRun assertions in test suite.\nUse try/except to log rather than stop when invariants fail.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nAlgorithm\nInvariant\nHolds?\nNotes\n\n\n\n\nInsertion Sort\nPrefix sorted at each step\nYes\nClassic inductive invariant\n\n\nSelection Sort\nMin placed at position i\nYes\nVerified iteratively\n\n\nQuick Sort\nPivot partitions left ≤ pivot ≤ right\nYes\nMust hold after partition\n\n\nBubble Sort\nLargest element bubbles to correct position\nYes\nAfter each full pass\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nCheck Type\nTime\nSpace\nNotes\n\n\n\n\nAssertion\n\\(O(k)\\)\n\\(O(1)\\)\nFor prefix length \\(k\\)\n\n\nTotal cost\n\\(O(n^2)\\) worst\nFor nested invariant checks\n\n\n\n\nA Sorting Invariant Checker transforms correctness from intuition into logic, enforcing order, proving validity, and illuminating the structure of sorting algorithms one invariant at a time.\n\n\n\n67 Distribution Histogram Sort Demo\nA Distribution Histogram Sort Demo visualizes how elements spread across buckets or bins during distribution-based sorting. It helps learners see why and how counting, radix, or bucket sort achieve linear-time behavior by organizing values before final ordering.\n\nWhat Problem Are We Solving?\nDistribution-based sorts (Counting, Bucket, Radix) don’t rely on pairwise comparisons. Instead, they classify elements into bins based on keys or digits. Understanding these algorithms requires visualizing how data is distributed across categories, a histogram captures that process.\nThe demo shows:\n\nHow counts are collected\nHow prefix sums turn counts into positions\nHow items are rebuilt in sorted order\n\n\n\nHow It Works (Plain Language)\n\nInitialize buckets, one for each key or range.\nTraverse input and increment count in the right bucket.\nVisualize the resulting histogram of frequencies.\n(Optional) Apply prefix sums to show cumulative positions.\nReconstruct output by reading bins in order.\n\nThis visualization connects counting logic to the final sorted array.\n\n\nExample Step by Step\nExample: Counting sort on [2, 1, 2, 0, 1]\n\n\n\nValue\nCount\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n2\n\n\n\nPrefix sums → [1, 3, 5] Rebuild array → [0, 1, 1, 2, 2]\nThe histogram clearly shows where each group of values will end up.\n\n\nTiny Code (Python)\nimport matplotlib.pyplot as plt\n\ndef histogram_sort_demo(arr, max_value):\n    counts = [0] * (max_value + 1)\n    for x in arr:\n        counts[x] += 1\n    \n    plt.bar(range(len(counts)), counts)\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Distribution Histogram for Counting Sort\")\n    plt.show()\n    \n    # Optional reconstruction\n    sorted_arr = []\n    for val, freq in enumerate(counts):\n        sorted_arr.extend([val] * freq)\n    return sorted_arr\nExample:\nhistogram_sort_demo([2, 1, 2, 0, 1], 2)\n\n\nWhy It Matters\n\nMakes non-comparison sorting intuitive\nShows data frequency patterns\nBridges between counting and position assignment\nHelps explain \\(O(n + k)\\) complexity visually\n\n\n\nA Gentle Proof (Why It Works)\nEach value’s frequency \\(f_i\\) determines exactly how many times it appears. By prefix-summing counts:\n\\[\np_i = \\sum_{j &lt; i} f_j\n\\]\nwe assign unique output positions for each value, ensuring stable, correct ordering in linear time.\nThus, sorting becomes position mapping, not comparison.\n\n\nTry It Yourself\n\nPlot histograms for random, sorted, and uniform arrays.\nCompare bucket sizes in Bucket Sort vs digit positions in Radix Sort.\nAdd prefix-sum labels to histogram bars.\nAnimate step-by-step rebuild of output.\n\n\n\nTest Cases\n\n\n\nInput\nMax\nHistogram\nSorted Output\n\n\n\n\n[2,1,2,0,1]\n2\n[1,2,2]\n[0,1,1,2,2]\n\n\n[3,3,3,3]\n3\n[0,0,0,4]\n[3,3,3,3]\n\n\n[0,1,2,3]\n3\n[1,1,1,1]\n[0,1,2,3]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nCounting\n\\(O(n)\\)\n\\(O(k)\\)\n\\(k\\) = number of buckets\n\n\nPrefix summation\n\\(O(k)\\)\n\\(O(k)\\)\nSingle pass over counts\n\n\nReconstruction\n\\(O(n + k)\\)\n\\(O(n + k)\\)\nBuild sorted array\n\n\n\nThe Distribution Histogram Sort Demo transforms abstract counting logic into a concrete visual, showing how frequency shapes order and making linear-time sorting crystal clear.\n\n\n\n68 Key Extraction Function\nA Key Extraction Function isolates the specific feature or attribute from a data element that determines its position in sorting. It’s a foundational tool for flexible, reusable sorting logic, enabling algorithms to handle complex records, tuples, or custom objects.\n\nWhat Problem Are We Solving?\nSorting real-world data often involves structured elements, tuples, objects, or dictionaries, not just numbers. We rarely sort entire elements directly; instead, we sort by a key:\n\nName alphabetically\nAge numerically\nDate chronologically\n\nA key extractor defines how to view each item for comparison, decoupling data from ordering.\n\n\nHow It Works (Plain Language)\n\nDefine a key function: key(x) → extracts sortable attribute.\nApply key function during comparisons.\nAlgorithm sorts based on these extracted values.\nThe original elements remain intact, only their order changes.\n\n\n\nExample Step by Step\nSuppose you have:\nstudents = [\n    (\"Alice\", 22, 3.8),\n    (\"Bob\", 20, 3.5),\n    (\"Clara\", 21, 3.9)\n]\nTo sort by age, use key=lambda x: x[1]. To sort by GPA (descending), use key=lambda x: -x[2].\nResults:\n\nBy age → [(\"Bob\", 20, 3.5), (\"Clara\", 21, 3.9), (\"Alice\", 22, 3.8)]\nBy GPA → [(\"Clara\", 21, 3.9), (\"Alice\", 22, 3.8), (\"Bob\", 20, 3.5)]\n\n\n\nTiny Code (Python)\ndef sort_by_key(data, key):\n    return sorted(data, key=key)\n\nstudents = [(\"Alice\", 22, 3.8), (\"Bob\", 20, 3.5), (\"Clara\", 21, 3.9)]\n\n# Sort by age\nresult = sort_by_key(students, key=lambda x: x[1])\n# Sort by GPA descending\nresult2 = sort_by_key(students, key=lambda x: -x[2])\nThis abstraction allows clean, reusable sorting.\n\n\nWhy It Matters\n\nSeparates logic: comparison mechanism vs data structure\nReusability: one algorithm, many orderings\nComposability: multi-level sorting by chaining keys\nStability synergy: stable sorts + key extraction = multi-key sorting\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(f(x)\\) be the key extractor. We sort based on \\(f(x)\\), not \\(x\\). If the comparator satisfies:\n\\[\nf(x_i) \\le f(x_j) \\implies x_i \\text{ precedes } x_j\n\\]\nthen the resulting order respects the intended attribute. Because \\(f\\) is deterministic, sort correctness follows directly from comparator correctness.\n\n\nTry It Yourself\n\nSort strings by length: key=len\nSort dictionary list by field: key=lambda d: d['score']\nCompose keys: key=lambda x: (x.grade, x.name)\nCombine with stability to simulate SQL “ORDER BY”\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nKey\nResult\n\n\n\n\n[(“A”,3),(“B”,1),(“C”,2)]\nlambda x:x[1]\n[(“B”,1),(“C”,2),(“A”,3)]\n\n\n[“cat”,“a”,“bird”]\nlen\n[“a”,“cat”,“bird”]\n\n\n[{“x”:5},{“x”:2},{“x”:4}]\nlambda d:d[\"x\"]\n[{“x”:2},{“x”:4},{“x”:5}]\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\nSpace\nNotes\n\n\n\n\nKey extraction\n\\(O(n)\\)\n\\(O(1)\\)\nOne call per element\n\n\nSorting\n\\(O(n \\log n)\\)\n\\(O(n)\\)\nDepends on algorithm used\n\n\nComposition\n\\(O(k \\cdot n)\\)\n\\(O(1)\\)\nFor multi-key chaining\n\n\n\nThe Key Extraction Function is the bridge between raw data and custom order, empowering algorithms to sort not just numbers, but meaning.\n\n\n\n69 Partially Ordered Set Builder\nA Partially Ordered Set (Poset) Builder constructs a visual and logical model of relationships that define partial orderings among elements, where some items can be compared, and others cannot. It’s a conceptual tool for understanding sorting constraints, dependency graphs, and precedence structures.\n\nWhat Problem Are We Solving?\nNot all collections have a total order. Sometimes only partial comparisons make sense, such as:\n\nTask dependencies (A before B, C independent)\nVersion control merges\nTopological ordering in DAGs\n\nA poset captures these relationships:\n\nReflexive: every element ≤ itself\nAntisymmetric: if A ≤ B and B ≤ A, then A = B\nTransitive: if A ≤ B and B ≤ C, then A ≤ C\n\nBuilding a poset helps us visualize constraints before attempting to sort or schedule.\n\n\nHow It Works (Plain Language)\n\nDefine a relation (≤) among elements.\nBuild a graph where an edge A → B means “A ≤ B.”\nEnsure reflexivity, antisymmetry, and transitivity.\nVisualize the result as a Hasse diagram (omit redundant edges).\nUse this structure to find linear extensions (valid sorted orders).\n\n\n\nExample Step by Step\nExample: Suppose we have tasks with dependencies:\nA ≤ B, A ≤ C, B ≤ D, C ≤ D\nConstruct the poset:\n\nNodes: A, B, C, D\nEdges: A→B, A→C, B→D, C→D\n\nHasse diagram:\n   D\n  / \\\n B   C\n  \\ /\n   A\nPossible total orders (linear extensions):\n\nA, B, C, D\nA, C, B, D\n\n\n\nTiny Code (Python)\nfrom collections import defaultdict\n\ndef build_poset(relations):\n    graph = defaultdict(list)\n    for a, b in relations:\n        graph[a].append(b)\n    return graph\n\nrelations = [('A', 'B'), ('A', 'C'), ('B', 'D'), ('C', 'D')]\nposet = build_poset(relations)\nfor k, v in poset.items():\n    print(f\"{k} → {v}\")\nOutput:\nA → ['B', 'C']\nB → ['D']\nC → ['D']\nYou can extend this to visualize with tools like networkx.\n\n\nWhy It Matters\n\nModels dependencies and precedence\nFoundation of topological sorting\nExplains why total order isn’t always possible\nClarifies constraint satisfaction in scheduling\n\nUsed in:\n\nBuild systems (make, DAGs)\nTask planning\nCompiler dependency analysis\n\n\n\nA Gentle Proof (Why It Works)\nA poset \\((P, \\le)\\) satisfies three axioms:\n\nReflexivity: \\(\\forall x, x \\le x\\)\nAntisymmetry: \\((x \\le y \\land y \\le x) \\implies x = y\\)\nTransitivity: \\((x \\le y \\land y \\le z) \\implies x \\le z\\)\n\nThese properties ensure consistent structure. Sorting a poset means finding a linear extension consistent with all \\(\\le\\) relations, which a topological sort guarantees for DAGs.\n\n\nTry It Yourself\n\nDefine tasks with prerequisites.\nDraw a Hasse diagram.\nPerform topological sort to list valid total orders.\nAdd extra relation, check if antisymmetry breaks.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nRelations\nPoset Edges\nLinear Orders\n\n\n\n\nA ≤ B, A ≤ C, B ≤ D, C ≤ D\nA→B, A→C, B→D, C→D\n[A,B,C,D], [A,C,B,D]\n\n\nA ≤ B, B ≤ C, A ≤ C\nA→B, B→C, A→C\n[A,B,C]\n\n\nA ≤ B, B ≤ A (invalid)\n,\nViolates antisymmetry\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nBuild relation graph\n\\(O(E)\\)\n\\(O(V)\\)\n\\(E\\) = number of relations\n\n\nCheck antisymmetry\n\\(O(E)\\)\n\\(O(V)\\)\nDetect cycles or bidirectional\n\n\nTopological sort\n\\(O(V + E)\\)\n\\(O(V)\\)\nFor linear extensions\n\n\n\nThe Partially Ordered Set Builder turns abstract ordering constraints into structured insight, showing not just what comes first, but what can coexist.\n\n\n\n70 Complexity Comparator\nA Complexity Comparator helps us understand how different algorithms scale by comparing their time or space complexity functions directly. It’s a tool for intuition: how does \\(O(n)\\) stack up against \\(O(n \\log n)\\) or \\(O(2^n)\\) as \\(n\\) grows large?\n\nWhat Problem Are We Solving?\nWhen faced with multiple algorithms solving the same problem, we must decide which is more efficient for large inputs. Rather than guess, we compare growth rates of their complexity functions.\nExample: Is \\(O(n^2)\\) slower than \\(O(n \\log n)\\)? For small \\(n\\), maybe not. But as \\(n \\to \\infty\\), \\(n^2\\) grows faster, so the \\(O(n \\log n)\\) algorithm is asymptotically better.\n\n\nHow It Works (Plain Language)\n\nDefine the two functions \\(f(n)\\) and \\(g(n)\\) representing their costs.\nCompute the ratio \\(\\frac{f(n)}{g(n)}\\) as \\(n \\to \\infty\\).\nInterpret the limit:\n\nIf \\(\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\\), then \\(f(n) = o(g(n))\\) (grows slower).\nIf limit is \\(\\infty\\), then \\(f(n) = \\omega(g(n))\\) (grows faster).\nIf limit is constant, then \\(f(n) = \\Theta(g(n))\\) (same growth).\n\nVisualize using plots or tables for small \\(n\\) to understand crossover points.\n\n\n\nExample Step by Step\nCompare \\(f(n) = n \\log n\\) and \\(g(n) = n^2\\):\n\nCompute ratio: \\(\\frac{f(n)}{g(n)} = \\frac{n \\log n}{n^2} = \\frac{\\log n}{n}\\).\nAs \\(n \\to \\infty\\), \\(\\frac{\\log n}{n} \\to 0\\). Therefore, \\(f(n) = o(g(n))\\).\n\nInterpretation: \\(O(n \\log n)\\) grows slower than \\(O(n^2)\\), so it’s more scalable.\n\n\nTiny Code (Python)\nimport math\n\ndef compare_growth(f, g, n_values):\n    for n in n_values:\n        print(f\"n={n:6d} f(n)={f(n):10.2f} g(n)={g(n):10.2f} ratio={f(n)/g(n):10.6f}\")\n\ncompare_growth(lambda n: n * math.log2(n),\n               lambda n: n2,\n               [2, 4, 8, 16, 32, 64, 128])\nOutput shows how \\(\\frac{f(n)}{g(n)}\\) decreases with \\(n\\).\n\n\nWhy It Matters\n\nMakes asymptotic comparison visual and numeric\nReveals crossover points for real-world input sizes\nHelps choose between multiple implementations\nDeepens intuition about scaling laws\n\n\n\nA Gentle Proof (Why It Works)\nWe rely on limit comparison:\nIf \\(\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = c\\):\n\nIf \\(0 &lt; c &lt; \\infty\\), then \\(f(n) = \\Theta(g(n))\\)\nIf \\(c = 0\\), then \\(f(n) = o(g(n))\\)\nIf \\(c = \\infty\\), then \\(f(n) = \\omega(g(n))\\)\n\nThis follows from formal definitions of asymptotic notation, ensuring consistency across comparisons.\n\n\nTry It Yourself\n\nCompare \\(O(n^2)\\) vs \\(O(n^3)\\)\nCompare \\(O(n \\log n)\\) vs \\(O(n^{1.5})\\)\nCompare \\(O(2^n)\\) vs \\(O(n!)\\)\nPlot their growth using Python or Excel\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\\(f(n)\\)\n\\(g(n)\\)\nRatio as \\(n \\to \\infty\\)\nRelationship\n\n\n\n\n\\(n\\)\n\\(n \\log n\\)\n\\(0\\)\n\\(n = o(n \\log n)\\)\n\n\n\\(n \\log n\\)\n\\(n^2\\)\n\\(0\\)\n\\(n \\log n = o(n^2)\\)\n\n\n\\(n^2\\)\n\\(n^2\\)\n\\(1\\)\n\\(\\Theta\\)\n\n\n\\(2^n\\)\n\\(n^3\\)\n\\(\\infty\\)\n\\(2^n = \\omega(n^3)\\)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nFunction ratio\n\\(O(1)\\)\n\\(O(1)\\)\nConstant-time comparison\n\n\nEmpirical table\n\\(O(k)\\)\n\\(O(k)\\)\nFor \\(k\\) sampled points\n\n\nPlot visualization\n\\(O(k)\\)\n\\(O(k)\\)\nHelps understand crossover\n\n\n\nThe Complexity Comparator is your lens for asymptotic insight, showing not just which algorithm is faster, but why it scales better.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-8.-data-structure-overview",
    "href": "books/en-us/list-1.html#section-8.-data-structure-overview",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 8. Data Structure Overview",
    "text": "Section 8. Data Structure Overview\n\n71 Stack Simulation\nA Stack Simulation lets us watch the push and pop operations unfold step by step, revealing the LIFO (Last In, First Out) nature of this simple yet powerful data structure.\n\nWhat Problem Are We Solving?\nStacks are everywhere: in recursion, expression evaluation, backtracking, and function calls. But for beginners, their dynamic behavior can feel abstract. A simulation makes it concrete, every push adds a layer, every pop removes one.\nGoal: Understand how and when elements enter and leave the stack, and why order matters.\n\n\nHow It Works (Plain Language)\n\nStart with an empty stack.\nPush(x): Add element x to the top.\nPop(): Remove the top element.\nPeek() (optional): Look at the top without removing it.\nThe most recently pushed element is always the first removed.\n\nThink of a stack of plates: you can only take from the top.\n\n\nExample Step by Step\nOperations:\nPush(10)\nPush(20)\nPush(30)\nPop()\nPush(40)\nStack evolution:\n\n\n\nStep\nOperation\nStack State (Top → Bottom)\n\n\n\n\n1\nPush(10)\n10\n\n\n2\nPush(20)\n20, 10\n\n\n3\nPush(30)\n30, 20, 10\n\n\n4\nPop()\n20, 10\n\n\n5\nPush(40)\n40, 20, 10\n\n\n\n\n\nTiny Code (Python)\nclass Stack:\n    def __init__(self):\n        self.data = []\n\n    def push(self, x):\n        self.data.append(x)\n        print(f\"Pushed {x}: {self.data[::-1]}\")\n\n    def pop(self):\n        if self.data:\n            x = self.data.pop()\n            print(f\"Popped {x}: {self.data[::-1]}\")\n            return x\n\n# Demo\ns = Stack()\ns.push(10)\ns.push(20)\ns.push(30)\ns.pop()\ns.push(40)\nEach action prints the current state, simulating stack behavior.\n\n\nWhy It Matters\n\nModels function calls and recursion\nEssential for undo operations and backtracking\nUnderpins expression parsing and evaluation\nBuilds intuition for control flow and memory frames\n\n\n\nA Gentle Proof (Why It Works)\nA stack enforces LIFO ordering: If you push elements in order \\(a_1, a_2, \\ldots, a_n\\), you must pop them in reverse: \\(a_n, \\ldots, a_2, a_1\\).\nFormally, each push increases size by 1, each pop decreases it by 1, ensuring \\(|S| = \\text{pushes} - \\text{pops}\\) and order reverses naturally.\n\n\nTry It Yourself\n\nSimulate postfix expression evaluation (3 4 + 5 *)\nTrace recursive function calls (factorial or Fibonacci)\nImplement browser backtracking with a stack\nPush strings and pop them to reverse order\n\n\n\nTest Cases\n\n\n\nOperation Sequence\nFinal Stack (Top → Bottom)\n\n\n\n\nPush(1), Push(2), Pop()\n1\n\n\nPush(‘A’), Push(‘B’), Push(‘C’)\nC, B, A\n\n\nPush(5), Pop(), Pop()\n(empty)\n\n\nPush(7), Push(9), Push(11), Pop()\n9, 7\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNote\n\n\n\n\nPush(x)\nO(1)\nO(n)\nAppend to list\n\n\nPop()\nO(1)\nO(n)\nRemove last item\n\n\nPeek()\nO(1)\nO(n)\nAccess last item\n\n\n\nA Stack Simulation makes abstract order tangible, every push and pop tells a story of control, memory, and flow.\n\n\n\n72 Queue Simulation\nA Queue Simulation shows how elements move through a first-in, first-out structure, perfect for modeling waiting lines, job scheduling, or data streams.\n\nWhat Problem Are We Solving?\nQueues capture fairness and order. They’re essential in task scheduling, buffering, and resource management, but their behavior can seem opaque without visualization.\nSimulating operations reveals how enqueue and dequeue actions shape the system over time.\nGoal: Understand FIFO (First-In, First-Out) order and how it ensures fairness in processing.\n\n\nHow It Works (Plain Language)\n\nStart with an empty queue.\nEnqueue(x): Add element x to the rear.\nDequeue(): Remove the front element.\nPeek() (optional): See the next item to be processed.\n\nLike a line at a ticket counter, first person in is first to leave.\n\n\nExample Step by Step\nOperations:\nEnqueue(10)\nEnqueue(20)\nEnqueue(30)\nDequeue()\nEnqueue(40)\nQueue evolution:\n\n\n\nStep\nOperation\nQueue State (Front → Rear)\n\n\n\n\n1\nEnqueue(10)\n10\n\n\n2\nEnqueue(20)\n10, 20\n\n\n3\nEnqueue(30)\n10, 20, 30\n\n\n4\nDequeue()\n20, 30\n\n\n5\nEnqueue(40)\n20, 30, 40\n\n\n\n\n\nTiny Code (Python)\nfrom collections import deque\n\nclass Queue:\n    def __init__(self):\n        self.data = deque()\n\n    def enqueue(self, x):\n        self.data.append(x)\n        print(f\"Enqueued {x}: {list(self.data)}\")\n\n    def dequeue(self):\n        if self.data:\n            x = self.data.popleft()\n            print(f\"Dequeued {x}: {list(self.data)}\")\n            return x\n\n# Demo\nq = Queue()\nq.enqueue(10)\nq.enqueue(20)\nq.enqueue(30)\nq.dequeue()\nq.enqueue(40)\nEach step prints the queue’s current state, helping you trace order evolution.\n\n\nWhy It Matters\n\nModels real-world waiting lines\nUsed in schedulers, network buffers, and BFS traversals\nEnsures fair access to limited resources\nBuilds intuition for stream processing\n\n\n\nA Gentle Proof (Why It Works)\nA queue preserves arrival order. If elements arrive in order \\(a_1, a_2, \\ldots, a_n\\), they exit in the same order, \\(a_1, a_2, \\ldots, a_n\\).\nEach enqueue appends to the rear, each dequeue removes from the front. Thus, insertion and removal sequences match, enforcing FIFO.\n\n\nTry It Yourself\n\nSimulate a print queue, jobs enter and complete in order.\nImplement BFS on a small graph using a queue.\nModel ticket line arrivals and departures.\nTrack packet flow through a network buffer.\n\n\n\nTest Cases\n\n\n\n\n\n\n\nOperation Sequence\nFinal Queue (Front → Rear)\n\n\n\n\nEnqueue(1), Enqueue(2), Dequeue()\n2\n\n\nEnqueue(‘A’), Enqueue(‘B’), Enqueue(‘C’)\nA, B, C\n\n\nEnqueue(5), Dequeue(), Dequeue()\n(empty)\n\n\nEnqueue(7), Enqueue(9), Enqueue(11), Dequeue()\n9, 11\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNote\n\n\n\n\nEnqueue(x)\nO(1)\nO(n)\nAppend to rear\n\n\nDequeue()\nO(1)\nO(n)\nRemove from front\n\n\nPeek()\nO(1)\nO(n)\nAccess front item\n\n\n\nA Queue Simulation clarifies the rhythm of fairness, each arrival patiently waits its turn, no one cutting in line.\n\n\n\n73 Linked List Builder\nA Linked List Builder shows how elements connect through pointers, the foundation for dynamic memory structures where data grows or shrinks on demand.\n\nWhat Problem Are We Solving?\nArrays have fixed size and require contiguous memory. Linked lists solve this by linking scattered nodes dynamically, one pointer at a time.\nBy simulating node creation and linkage, we build intuition for pointer manipulation and traversal, essential for mastering lists, stacks, queues, and graphs.\nGoal: Understand how nodes link together and how to maintain references during insertion or deletion.\n\n\nHow It Works (Plain Language)\nA singly linked list is a sequence of nodes, each holding:\n\nA value\nA pointer to the next node\n\nBasic operations:\n\nCreate node(value) → allocate new node.\nInsert after → link new node between existing ones.\nDelete → redirect pointers to skip a node.\nTraverse → follow next pointers until None.\n\nLike a chain, each link knows only the next one.\n\n\nExample Step by Step\nBuild a list:\nInsert(10)\nInsert(20)\nInsert(30)\nProcess:\n\nCreate node(10): head → 10 → None\nCreate node(20): head → 10 → 20 → None\nCreate node(30): head → 10 → 20 → 30 → None\n\nTraversal from head prints: 10 → 20 → 30 → None\n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def insert(self, value):\n        new_node = Node(value)\n        if not self.head:\n            self.head = new_node\n        else:\n            cur = self.head\n            while cur.next:\n                cur = cur.next\n            cur.next = new_node\n        self.display()\n\n    def display(self):\n        cur = self.head\n        elems = []\n        while cur:\n            elems.append(str(cur.value))\n            cur = cur.next\n        print(\" → \".join(elems) + \" → None\")\n\n# Demo\nll = LinkedList()\nll.insert(10)\nll.insert(20)\nll.insert(30)\n\n\nWhy It Matters\n\nEnables dynamic memory allocation\nNo need for contiguous storage\nPowers stacks, queues, hash chains, adjacency lists\nBuilds foundation for advanced pointer-based structures\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n\\) be the number of nodes. Each node has exactly one outgoing pointer (to next) or None. Traversing once visits every node exactly once.\nTherefore, insertion or traversal takes \\(O(n)\\) time, and storage is \\(O(n)\\) (one node per element).\n\n\nTry It Yourself\n\nInsert values {5, 15, 25, 35}\nDelete the second node and reconnect links\nReverse the list manually by reassigning pointers\nVisualize how each next changes during reversal\n\n\n\nTest Cases\n\n\n\nOperation Sequence\nExpected Output\n\n\n\n\nInsert(10), Insert(20)\n10 → 20 → None\n\n\nInsert(5), Insert(15), Insert(25)\n5 → 15 → 25 → None\n\n\nEmpty List\nNone\n\n\nSingle Node\n42 → None\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNote\n\n\n\n\nInsert End\nO(n)\nO(n)\nTraverse to tail\n\n\nDelete Node\nO(n)\nO(n)\nFind predecessor\n\n\nSearch\nO(n)\nO(n)\nSequential traversal\n\n\nTraverse\nO(n)\nO(n)\nVisit each node once\n\n\n\nA Linked List Builder is your first dance with pointers, where structure emerges from simple connections, and memory becomes fluid, flexible, and free.\n\n\n\n74 Array Index Visualizer\nAn Array Index Visualizer helps you see how arrays organize data in contiguous memory and how indexing gives \\(O(1)\\) access to any element.\n\nWhat Problem Are We Solving?\nArrays are the simplest data structure, but beginners often struggle to grasp how indexing truly works under the hood. By visualizing index positions and memory offsets, you can see why arrays allow direct access yet require fixed size and contiguous space.\nGoal: Understand the relationship between index, address, and element access.\n\n\nHow It Works (Plain Language)\nAn array stores \\(n\\) elements consecutively in memory. If the base address is \\(A_0\\), and each element takes \\(s\\) bytes, then:\n\\[ A_i = A_0 + i \\times s \\]\nSo accessing index \\(i\\) is constant-time:\n\nCompute address\nJump directly there\nRetrieve value\n\nThis visualization ties logical indices (0, 1, 2, …) to physical locations.\n\n\nExample Step by Step\nSuppose we have an integer array:\narr = [10, 20, 30, 40]\nBase address: 1000, element size: 4 bytes\n\n\n\nIndex\nAddress\nValue\n\n\n\n\n0\n1000\n10\n\n\n1\n1004\n20\n\n\n2\n1008\n30\n\n\n3\n1012\n40\n\n\n\nAccess arr[2]:\n\nCompute \\(A_0 + 2 \\times 4 = 1008\\)\nRetrieve 30\n\n\n\nTiny Code (Python)\ndef visualize_array(arr, base=1000, size=4):\n    print(f\"{'Index':&lt;8}{'Address':&lt;10}{'Value':&lt;8}\")\n    for i, val in enumerate(arr):\n        address = base + i * size\n        print(f\"{i:&lt;8}{address:&lt;10}{val:&lt;8}\")\n\narr = [10, 20, 30, 40]\nvisualize_array(arr)\nOutput:\nIndex   Address   Value\n0       1000      10\n1       1004      20\n2       1008      30\n3       1012      40\n\n\nWhy It Matters\n\nInstant access via address computation\nContiguity ensures cache locality\nFixed size and type consistency\nCore of higher-level structures (strings, matrices, tensors)\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(A_0\\) be the base address. Each element occupies \\(s\\) bytes. To access element \\(i\\):\n\\[ A_i = A_0 + i \\times s \\]\nThis is a simple arithmetic operation, so access is \\(O(1)\\), independent of \\(n\\).\n\n\nTry It Yourself\n\nVisualize array [5, 10, 15, 20, 25] with base 5000 and size 8.\nAccess arr[4] manually using formula.\nCompare array vs. linked list access time.\nModify size and re-run visualization.\n\n\n\nTest Cases\n\n\n\nArray\nBase\nSize\nAccess\nExpected Address\nValue\n\n\n\n\n[10, 20, 30]\n1000\n4\narr[1]\n1004\n20\n\n\n[7, 14, 21, 28]\n500\n2\narr[3]\n506\n28\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNote\n\n\n\n\nAccess\nO(1)\nO(n)\nDirect via formula\n\n\nUpdate\nO(1)\nO(n)\nSingle write\n\n\nTraverse\nO(n)\nO(n)\nVisit all\n\n\nInsert/Delete\nO(n)\nO(n)\nRequires shifting\n\n\n\nAn Array Index Visualizer reveals how logic meets hardware, every index a direct pointer, every element a predictable step from the base.\n\n\n\n75 Hash Function Mapper\nA Hash Function Mapper shows how keys are transformed into array indices, turning arbitrary data into fast-access positions.\n\nWhat Problem Are We Solving?\nWe often need to store and retrieve data by key (like “Alice” or “user123”), not by numeric index. But arrays only understand numbers. A hash function bridges this gap, mapping keys into integer indices so we can use array-like speed for key-based lookup.\nGoal: Understand how keys become indices and how hash collisions occur.\n\n\nHow It Works (Plain Language)\nA hash function takes a key and computes an index:\n\\[ \\text{index} = h(\\text{key}) \\bmod m \\]\nwhere:\n\n\\(h(\\text{key})\\) is a numeric hash value,\n\\(m\\) is the table size.\n\nFor example:\nkey = \"cat\"\nh(key) = 493728\nm = 10\nindex = 493728 % 10 = 8\nNow \"cat\" is mapped to slot 8.\nIf another key maps to the same index, a collision occurs, handled by chaining or probing.\n\n\nExample Step by Step\nSuppose a table of size 5.\nKeys: \"red\", \"blue\", \"green\"\n\n\n\nKey\nHash Value\nIndex (% 5)\n\n\n\n\nred\n432\n2\n\n\nblue\n107\n2 (collision)\n\n\ngreen\n205\n0\n\n\n\nWe see \"red\" and \"blue\" collide at index 2.\n\n\nTiny Code (Python)\ndef simple_hash(key):\n    return sum(ord(c) for c in key)\n\ndef map_keys(keys, size=5):\n    table = [[] for _ in range(size)]\n    for k in keys:\n        idx = simple_hash(k) % size\n        table[idx].append(k)\n        print(f\"Key: {k:6} -&gt; Index: {idx}\")\n    return table\n\nkeys = [\"red\", \"blue\", \"green\"]\ntable = map_keys(keys)\nOutput:\nKey: red    -&gt; Index: 2\nKey: blue   -&gt; Index: 2\nKey: green  -&gt; Index: 0\n\n\nWhy It Matters\n\nEnables constant-time average lookup and insertion\nForms the backbone of hash tables, dictionaries, caches\nShows tradeoffs between hash quality and collision handling\n\n\n\nA Gentle Proof (Why It Works)\nIf a hash function distributes keys uniformly, expected number of keys per slot is \\(\\frac{n}{m}\\).\nThus, expected lookup time:\n\\[ E[T] = O(1 + \\frac{n}{m}) \\]\nFor well-chosen \\(m\\) and good \\(h\\), \\(E[T] \\approx O(1)\\).\n\n\nTry It Yourself\n\nMap [\"cat\", \"dog\", \"bat\", \"rat\"] to a table of size 7.\nObserve collisions and try a larger table.\nReplace sum(ord(c)) with a polynomial hash: \\[ h(\\text{key}) = \\sum c_i \\times 31^i \\]\nCompare distribution quality.\n\n\n\nTest Cases\n\n\n\nKeys\nTable Size\nResult (Indices)\n\n\n\n\n[“a”, “b”, “c”]\n3\n1, 2, 0\n\n\n[“hi”, “ih”]\n5\ncollision (same sum)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Expected)\nSpace\nNote\n\n\n\n\nInsert\nO(1)\nO(n)\nAverage, good hash\n\n\nSearch\nO(1)\nO(n)\nWith uniform hashing\n\n\nDelete\nO(1)\nO(n)\nSame cost as lookup\n\n\n\nA Hash Function Mapper makes hashing tangible, you watch strings become slots, collisions emerge, and order dissolve into probability and math.\n\n\n\n76 Binary Tree Builder\nA Binary Tree Builder illustrates how hierarchical data structures are constructed by linking nodes with left and right children.\n\nWhat Problem Are We Solving?\nLinear structures like arrays and lists can’t efficiently represent hierarchical relationships. When you need ordering, searching, and hierarchical grouping, a binary tree provides the foundation.\nGoal: Understand how nodes are connected to form a tree and how recursive structure emerges naturally.\n\n\nHow It Works (Plain Language)\nA binary tree is made of nodes. Each node has:\n\na value\na left child\na right child\n\nTo build a tree:\n\nStart with a root node\nRecursively insert new nodes:\n\nIf value &lt; current → go left\nElse → go right\n\nRepeat until you find a null link\n\nThis produces a Binary Search Tree (BST), maintaining order property.\n\n\nExample Step by Step\nInsert values: [10, 5, 15, 3, 7, 12, 18]\nProcess:\n10\n├── 5\n│   ├── 3\n│   └── 7\n└── 15\n    ├── 12\n    └── 18\nTraversal orders:\n\nInorder: 3, 5, 7, 10, 12, 15, 18\nPreorder: 10, 5, 3, 7, 15, 12, 18\nPostorder: 3, 7, 5, 12, 18, 15, 10\n\n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\nclass BST:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, value):\n        self.root = self._insert(self.root, value)\n\n    def _insert(self, node, value):\n        if node is None:\n            return Node(value)\n        if value &lt; node.value:\n            node.left = self._insert(node.left, value)\n        else:\n            node.right = self._insert(node.right, value)\n        return node\n\n    def inorder(self, node):\n        if node:\n            self.inorder(node.left)\n            print(node.value, end=\" \")\n            self.inorder(node.right)\n\n# Demo\ntree = BST()\nfor val in [10, 5, 15, 3, 7, 12, 18]:\n    tree.insert(val)\ntree.inorder(tree.root)\nOutput: 3 5 7 10 12 15 18\n\n\nWhy It Matters\n\nCore structure for search trees, heaps, and expression trees\nForms basis for balanced trees (AVL, Red-Black)\nEnables divide-and-conquer recursion naturally\n\n\n\nA Gentle Proof (Why It Works)\nA binary search tree maintains the invariant:\n\\[ \\forall \\text{node},\\ v:\n\\begin{cases}\nv_{\\text{left}} &lt; v_{\\text{root}} &lt; v_{\\text{right}}\n\\end{cases} \\]\nInsertion preserves this by recursive placement. Each insertion follows a single path of height \\(h\\), so time is \\(O(h)\\). For balanced trees, \\(h = O(\\log n)\\).\n\n\nTry It Yourself\n\nInsert [8, 3, 10, 1, 6, 14, 4, 7, 13].\nDraw the tree structure.\nPerform inorder traversal (should print sorted order).\nCompare with unbalanced insertion order.\n\n\n\nTest Cases\n\n\n\nInput Sequence\nInorder Traversal\n\n\n\n\n[10, 5, 15, 3, 7]\n3, 5, 7, 10, 15\n\n\n[2, 1, 3]\n1, 2, 3\n\n\n[5]\n5\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Avg)\nTime (Worst)\nSpace\n\n\n\n\nInsert\nO(log n)\nO(n)\nO(n)\n\n\nSearch\nO(log n)\nO(n)\nO(1)\n\n\nDelete\nO(log n)\nO(n)\nO(1)\n\n\nTraverse\nO(n)\nO(n)\nO(n)\n\n\n\nA Binary Tree Builder reveals order within hierarchy, each node a decision, each branch a story of lesser and greater.\n\n\n\n77 Heap Structure Demo\nA Heap Structure Demo helps you visualize how binary heaps organize data to always keep the smallest or largest element at the top, enabling fast priority access.\n\nWhat Problem Are We Solving?\nWe often need a structure that quickly retrieves the minimum or maximum element, like in priority queues or scheduling. Sorting every time is wasteful. A heap maintains partial order so the root is always extreme, and rearrangement happens locally.\nGoal: Understand how insertion and removal maintain the heap property.\n\n\nHow It Works (Plain Language)\nA binary heap is a complete binary tree stored as an array. Each node satisfies:\n\nMin-heap: parent ≤ children\nMax-heap: parent ≥ children\n\nInsertion and deletion are handled with sift up and sift down operations.\n\n\nInsert (Heapify Up)\n\nAdd new element at the end\nCompare with parent\nSwap if violates heap property\nRepeat until heap property holds\n\n\n\nRemove Root (Heapify Down)\n\nReplace root with last element\nCompare with children\nSwap with smaller (min-heap) or larger (max-heap) child\nRepeat until property restored\n\n\n\nExample Step by Step (Min-Heap)\nInsert [10, 4, 15, 2]\n\n[10]\n[10, 4] → swap(4, 10) → [4, 10]\n[4, 10, 15] (no swap)\n[4, 10, 15, 2] → swap(2, 10) → swap(2, 4) → [2, 4, 15, 10]\n\nFinal heap (array): [2, 4, 15, 10] Tree view:\n    2\n   / \\\n  4  15\n /\n10\n\n\nTiny Code (Python)\nimport heapq\n\ndef heap_demo():\n    heap = []\n    for x in [10, 4, 15, 2]:\n        heapq.heappush(heap, x)\n        print(\"Insert\", x, \"→\", heap)\n    while heap:\n        print(\"Pop:\", heapq.heappop(heap), \"→\", heap)\n\nheap_demo()\nOutput:\nInsert 10 → [10]\nInsert 4 → [4, 10]\nInsert 15 → [4, 10, 15]\nInsert 2 → [2, 4, 15, 10]\nPop: 2 → [4, 10, 15]\nPop: 4 → [10, 15]\nPop: 10 → [15]\nPop: 15 → []\n\n\nWhy It Matters\n\nEnables priority queues (task schedulers, Dijkstra)\nSupports O(1) access to min/max\nKeeps O(log n) insertion/removal cost\nBasis for Heapsort\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(h = \\lfloor \\log_2 n \\rfloor\\) be heap height. Each insert and delete moves along one path of height \\(h\\). Thus:\n\\[ T_{\\text{insert}} = T_{\\text{delete}} = O(\\log n) \\] \\[ T_{\\text{find-min}} = O(1) \\]\n\n\nTry It Yourself\n\nInsert [7, 2, 9, 1, 5] into a min-heap\nTrace swaps on paper\nRemove min repeatedly and record order (should be sorted ascending)\nRepeat for max-heap version\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput (Heap)\n\n\n\n\nInsert\n[5, 3, 8]\n[3, 5, 8]\n\n\nPop\n[3, 5, 8]\nPop 3 → [5, 8]\n\n\nInsert\n[10, 2, 4]\n[2, 10, 4]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNote\n\n\n\n\nInsert\nO(log n)\nO(n)\nPercolate up\n\n\nDelete\nO(log n)\nO(n)\nPercolate down\n\n\nFind Min/Max\nO(1)\nO(1)\nRoot access\n\n\nBuild Heap\nO(n)\nO(n)\nBottom-up heapify\n\n\n\nA Heap Structure Demo shows order through shape, every parent above its children, every insertion a climb toward balance.\n\n\n\n78 Union-Find Concept\nA Union-Find Concept (also called Disjoint Set Union, DSU) demonstrates how to efficiently manage dynamic grouping, deciding whether elements belong to the same set and merging sets when needed.\n\nWhat Problem Are We Solving?\nIn many problems, we need to track connected components, e.g. in graphs, social networks, or Kruskal’s MST. We want to answer two operations efficiently:\n\nFind(x): which group is x in?\nUnion(x, y): merge the groups of x and y\n\nNaive approaches (like scanning arrays) cost too much. Union-Find structures solve this in almost constant time using parent pointers and path compression.\n\n\nHow It Works (Plain Language)\nEach element points to a parent. The root is the representative of its set. If two elements share the same root, they’re in the same group.\nOperations:\n\nFind(x): Follow parent pointers until reaching a root (node where parent[x] == x) Use path compression to flatten paths for next time\nUnion(x, y): Find roots of x and y If different, attach one root to the other (merge sets) Optionally, use union by rank/size to keep tree shallow\n\n\n\nExample Step by Step\nStart with {1}, {2}, {3}, {4}\nPerform:\nUnion(1, 2) → {1,2}, {3}, {4}\nUnion(3, 4) → {1,2}, {3,4}\nUnion(2, 3) → {1,2,3,4}\nAll now connected under one root.\nIf Find(4) → returns 1 (root of its set)\n\n\nTiny Code (Python)\nclass UnionFind:\n    def __init__(self, n):\n        self.parent = [i for i in range(n)]\n        self.rank = [0] * n\n\n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])  # Path compression\n        return self.parent[x]\n\n    def union(self, x, y):\n        rx, ry = self.find(x), self.find(y)\n        if rx == ry:\n            return\n        if self.rank[rx] &lt; self.rank[ry]:\n            self.parent[rx] = ry\n        elif self.rank[rx] &gt; self.rank[ry]:\n            self.parent[ry] = rx\n        else:\n            self.parent[ry] = rx\n            self.rank[rx] += 1\n\n# Demo\nuf = UnionFind(5)\nuf.union(0, 1)\nuf.union(2, 3)\nuf.union(1, 2)\nprint([uf.find(i) for i in range(5)])\nOutput: [0, 0, 0, 0, 4]\n\n\nWhy It Matters\n\nFoundation for Kruskal’s Minimum Spanning Tree\nDetects cycles in undirected graphs\nEfficient for connectivity queries in dynamic graphs\nUsed in percolation, image segmentation, clustering\n\n\n\nA Gentle Proof (Why It Works)\nEach operation has amortized cost given by the inverse Ackermann function \\(\\alpha(n)\\), practically constant.\n\\[ T_{\\text{find}}(n), T_{\\text{union}}(n) = O(\\alpha(n)) \\]\nBecause path compression ensures every node points closer to root each time, flattening structure to near-constant depth.\n\n\nTry It Yourself\n\nStart with {0}, {1}, {2}, {3}, {4}\nApply: Union(0,1), Union(2,3), Union(1,2)\nQuery Find(3) → should match root of 0\nPrint parent array after each operation\n\n\n\nTest Cases\n\n\n\nOperation Sequence\nResulting Sets\n\n\n\n\nUnion(1, 2), Union(3, 4)\n{1,2}, {3,4}, {0}\n\n\nUnion(2, 3)\n{0}, {1,2,3,4}\n\n\nFind(4)\nRoot = 1 (or 0)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nAmortized Time\nSpace\nNotes\n\n\n\n\nFind\n\\(O(\\alpha(n))\\)\n\\(O(n)\\)\nPath compression\n\n\nUnion\n\\(O(\\alpha(n))\\)\n\\(O(n)\\)\nWith rank heuristic\n\n\nConnected(x, y)\n\\(O(\\alpha(n))\\)\n\\(O(1)\\)\nVia root comparison\n\n\n\nA Union-Find Concept turns disjoint sets into a living network, connections formed and flattened, unity discovered through structure.\n\n\n\n79 Graph Representation Demo\nA Graph Representation Demo reveals how graphs can be encoded in data structures, showing the tradeoffs between adjacency lists, matrices, and edge lists.\n\nWhat Problem Are We Solving?\nGraphs describe relationships, roads between cities, links between websites, friendships in a network. But before we can run algorithms (like BFS, Dijkstra, or DFS), we need a representation that matches the graph’s density, size, and operations.\nGoal: Understand how different representations encode edges and how to choose the right one.\n\n\nHow It Works (Plain Language)\nA graph is defined as: \\[ G = (V, E) \\] where:\n\n\\(V\\) = set of vertices\n\\(E\\) = set of edges (pairs of vertices)\n\nWe can represent \\(G\\) in three main ways:\n\nAdjacency Matrix\n\n2D array of size \\(|V| \\times |V|\\)\nEntry \\((i, j) = 1\\) if edge \\((i, j)\\) exists, else 0\n\nAdjacency List\n\nFor each vertex, a list of its neighbors\nCompact for sparse graphs\n\nEdge List\n\nSimple list of all edges\nEasy to iterate, hard for quick lookup\n\n\n\n\nExample Step by Step\nConsider an undirected graph:\nVertices: {A, B, C, D}\nEdges: {(A, B), (A, C), (B, D)}\nAdjacency Matrix\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n0\n1\n1\n0\n\n\nB\n1\n0\n0\n1\n\n\nC\n1\n0\n0\n0\n\n\nD\n0\n1\n0\n0\n\n\n\nAdjacency List\nA: [B, C]\nB: [A, D]\nC: [A]\nD: [B]\nEdge List\n[(A, B), (A, C), (B, D)]\n\n\nTiny Code (Python)\nfrom collections import defaultdict\n\n# Adjacency List\ngraph = defaultdict(list)\nedges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"D\")]\n\nfor u, v in edges:\n    graph[u].append(v)\n    graph[v].append(u)  # undirected\n\nprint(\"Adjacency List:\")\nfor node, neighbors in graph.items():\n    print(f\"{node}: {neighbors}\")\n\n# Adjacency Matrix\nvertices = [\"A\", \"B\", \"C\", \"D\"]\nn = len(vertices)\nmatrix = [[0]*n for _ in range(n)]\nindex = {v: i for i, v in enumerate(vertices)}\n\nfor u, v in edges:\n    i, j = index[u], index[v]\n    matrix[i][j] = matrix[j][i] = 1\n\nprint(\"\\nAdjacency Matrix:\")\nfor row in matrix:\n    print(row)\nOutput:\nAdjacency List:\nA: ['B', 'C']\nB: ['A', 'D']\nC: ['A']\nD: ['B']\n\nAdjacency Matrix:\n[0, 1, 1, 0]\n[1, 0, 0, 1]\n[1, 0, 0, 0]\n[0, 1, 0, 0]\n\n\nWhy It Matters\n\nAdjacency matrix → fast lookup (\\(O(1)\\)), high space (\\(O(V^2)\\))\nAdjacency list → efficient for sparse graphs (\\(O(V+E)\\))\nEdge list → simple to iterate, ideal for algorithms like Kruskal\n\nChoosing wisely impacts performance of every algorithm on the graph.\n\n\nA Gentle Proof (Why It Works)\nLet \\(V\\) be number of vertices, \\(E\\) edges.\n\n\n\nRepresentation\nStorage\nEdge Check\nIteration\n\n\n\n\nAdjacency Matrix\n\\(O(V^2)\\)\n\\(O(1)\\)\n\\(O(V^2)\\)\n\n\nAdjacency List\n\\(O(V + E)\\)\n\\(O(\\deg(v))\\)\n\\(O(V + E)\\)\n\n\nEdge List\n\\(O(E)\\)\n\\(O(E)\\)\n\\(O(E)\\)\n\n\n\nSparse graphs (\\(E \\ll V^2\\)) → adjacency list preferred. Dense graphs (\\(E \\approx V^2\\)) → adjacency matrix is fine.\n\n\nTry It Yourself\n\nDraw a graph with 5 nodes, 6 edges\nWrite all three representations\nCompute storage cost\nPick best format for BFS vs Kruskal’s MST\n\n\n\nTest Cases\n\n\n\nGraph Type\nRepresentation\nBenefit\n\n\n\n\nSparse\nList\nSpace efficient\n\n\nDense\nMatrix\nConstant lookup\n\n\nWeighted\nEdge List\nEasy sorting\n\n\n\n\n\nComplexity\n\n\n\nOperation\nMatrix\nList\nEdge List\n\n\n\n\nSpace\n\\(O(V^2)\\)\n\\(O(V+E)\\)\n\\(O(E)\\)\n\n\nAdd Edge\n\\(O(1)\\)\n\\(O(1)\\)\n\\(O(1)\\)\n\n\nCheck Edge\n\\(O(1)\\)\n\\(O(\\deg(v))\\)\n\\(O(E)\\)\n\n\nIterate\n\\(O(V^2)\\)\n\\(O(V+E)\\)\n\\(O(E)\\)\n\n\n\nA Graph Representation Demo shows the blueprint of connection, the same network, three different lenses: matrix, list, or edge table.\n\n\n\n80 Trie Structure Visualizer\nA Trie Structure Visualizer helps you see how strings and prefixes are stored efficiently, one character per edge, building shared paths for common prefixes.\n\nWhat Problem Are We Solving?\nWhen you need to store and search many strings, especially by prefix, linear scans or hash tables aren’t ideal. We want something that makes prefix queries fast and memory use efficient through shared structure.\nA trie (prefix tree) does exactly that, storing strings as paths, reusing common prefixes.\nGoal: Understand how each character extends a path and how search and insert work along edges.\n\n\nHow It Works (Plain Language)\nA trie starts with an empty root node. Each edge represents a character. Each node may have multiple children, one for each possible next character.\nTo insert a word:\n\nStart at root\nFor each character:\n\nIf it doesn’t exist, create a new child\nMove to that child\n\nMark last node as “end of word”\n\nTo search:\n\nStart at root\nFollow edges by each character\nIf path exists and end is marked, word found\n\n\n\nExample Step by Step\nInsert cat, car, dog\n(root)\n ├── c\n │    └── a\n │         ├── t*\n │         └── r*\n └── d\n      └── o\n           └── g*\nAsterisk * marks word end. Common prefix ca is shared.\nSearch \"car\":\n\nc ✓\na ✓\nr ✓\nEnd marked → found\n\nSearch \"cap\":\n\nc ✓\na ✓\np ✗ → not found\n\n\n\nTiny Code (Python)\nclass TrieNode:\n    def __init__(self):\n        self.children = {}\n        self.is_end = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        node = self.root\n        for ch in word:\n            if ch not in node.children:\n                node.children[ch] = TrieNode()\n            node = node.children[ch]\n        node.is_end = True\n\n    def search(self, word):\n        node = self.root\n        for ch in word:\n            if ch not in node.children:\n                return False\n            node = node.children[ch]\n        return node.is_end\n\n# Demo\ntrie = Trie()\nfor w in [\"cat\", \"car\", \"dog\"]:\n    trie.insert(w)\nprint(trie.search(\"car\"))  # True\nprint(trie.search(\"cap\"))  # False\n\n\nWhy It Matters\n\nEnables prefix search, autocomplete, dictionary lookup\nAvoids recomputing prefixes\nEfficient for string-intensive applications\nFoundation for compressed tries, DAWGs, and suffix trees\n\n\n\nA Gentle Proof (Why It Works)\nEach character in word \\(w\\) follows one path in trie. Insert cost = \\(O(|w|)\\), Search cost = \\(O(|w|)\\).\nFor \\(n\\) words of average length \\(L\\), total nodes ≤ \\(O(nL)\\).\nPrefix query cost = \\(O(p)\\), where \\(p\\) = prefix length.\n\n\nTry It Yourself\n\nInsert [\"cat\", \"cap\", \"can\", \"dog\"]\nDraw tree paths\nQuery prefixes \"ca\" and \"do\"\nCount total nodes created\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\n\n\n\n\nInsert\n“cat”, “car”\nShared path “ca”\n\n\nSearch\n“car”\nTrue\n\n\nSearch\n“cap”\nFalse\n\n\nPrefix\n“ca”\nExists\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNote\n\n\n\n\nInsert\nO(L)\nO(L)\nL = length of word\n\n\nSearch\nO(L)\nO(1)\nFollow path\n\n\nPrefix Query\nO(p)\nO(1)\nShared traversal\n\n\n\nA Trie Structure Visualizer shows structure born from language, every word a path, every prefix a meeting point, every branch a shared memory.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-9.-graphs-and-trees-overview",
    "href": "books/en-us/list-1.html#section-9.-graphs-and-trees-overview",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 9. Graphs and Trees overview",
    "text": "Section 9. Graphs and Trees overview\n\n81 Graph Model Constructor\nA Graph Model Constructor is how we formally build graphs, sets of vertices connected by edges, to represent relationships, networks, or structures in the world.\n\nWhat Problem Are We Solving?\nWe often face problems where elements are connected, roads between cities, friendships in a network, dependencies in a project. To reason about these, we need a way to model entities (vertices) and connections (edges).\nThe Graph Model Constructor provides the blueprint for turning real-world relationships into graph data structures we can analyze.\n\n\nHow It Works (Plain Language)\nA graph is defined as:\n\\[\nG = (V, E)\n\\]\nwhere\n\n\\(V\\) = set of vertices (nodes)\n\\(E\\) = set of edges (connections) between vertices\n\nEach edge can be:\n\nUndirected: \\((u, v)\\) means \\(u\\) and \\(v\\) are connected both ways\nDirected: \\((u, v)\\) means a one-way connection from \\(u\\) to \\(v\\)\n\nYou can build graphs in multiple ways:\n\nEdge List – list of pairs \\((u, v)\\)\nAdjacency List – dictionary of node → neighbor list\nAdjacency Matrix – 2D table of connections (1 = edge, 0 = none)\n\n\n\nExample\nInput relationships\nA connected to B  \nA connected to C  \nB connected to C  \nC connected to D\nVertices\nV = {A, B, C, D}\nEdges\nE = {(A, B), (A, C), (B, C), (C, D)}\nEdge List\n[(A, B), (A, C), (B, C), (C, D)]\nAdjacency List\nA: [B, C]\nB: [A, C]\nC: [A, B, D]\nD: [C]\nAdjacency Matrix\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n0\n1\n1\n0\n\n\nB\n1\n0\n1\n0\n\n\nC\n1\n1\n0\n1\n\n\nD\n0\n0\n1\n0\n\n\n\n\n\nTiny Code (Python)\ndef build_graph(edge_list):\n    graph = {}\n    for u, v in edge_list:\n        graph.setdefault(u, []).append(v)\n        graph.setdefault(v, []).append(u)  # undirected\n    return graph\n\nedges = [(\"A\",\"B\"),(\"A\",\"C\"),(\"B\",\"C\"),(\"C\",\"D\")]\ngraph = build_graph(edges)\nfor node, neighbors in graph.items():\n    print(node, \":\", neighbors)\nOutput\nA : ['B', 'C']\nB : ['A', 'C']\nC : ['A', 'B', 'D']\nD : ['C']\n\n\nWhy It Matters\n\nGraphs let us model relationships in any domain: roads, social networks, dependencies, knowledge.\nOnce constructed, you can apply graph algorithms, BFS, DFS, shortest paths, spanning trees, connectivity, to solve real problems.\nThe constructor phase defines how efficiently later algorithms run.\n\n\n\nA Gentle Proof (Why It Works)\nGiven \\(n\\) vertices and \\(m\\) edges, we represent each edge \\((u,v)\\) by linking \\(u\\) and \\(v\\). Construction time = \\(O(n + m)\\), since each vertex and edge is processed once.\nAdjacency list size = \\(O(n + m)\\) Adjacency matrix size = \\(O(n^2)\\)\nThus, adjacency lists are more space-efficient for sparse graphs, while matrices offer constant-time edge lookups for dense graphs.\n\n\nTry It Yourself\n\nBuild a graph of 5 cities and their direct flights.\nRepresent it as both edge list and adjacency list.\nCount number of edges and neighbors per vertex.\nDraw the resulting graph on paper.\n\n\n\nTest Cases\n\n\n\nInput\nRepresentation\nKey Property\n\n\n\n\n[(1,2), (2,3)]\nAdjacency List\n3 vertices, 2 edges\n\n\nDirected edges\nAdjacency List\nOne-way links only\n\n\nFully connected 3 nodes\nAdjacency Matrix\nAll 1s except diagonal\n\n\n\n\n\nComplexity\n\n\n\nRepresentation\nSpace\nLookup\nIteration\n\n\n\n\nEdge List\nO(m)\nO(m)\nO(m)\n\n\nAdjacency List\nO(n + m)\nO(deg(v))\nO(m)\n\n\nAdjacency Matrix\nO(n²)\nO(1)\nO(n²)\n\n\n\nA Graph Model Constructor builds the world of connections, from abstract relations to concrete data structures, forming the backbone of every graph algorithm that follows.\n\n\n\n82 Adjacency Matrix Builder\nAn Adjacency Matrix Builder constructs a 2D grid representation of a graph, showing whether pairs of vertices are connected. It’s a simple and powerful way to capture all edges in a compact mathematical form.\n\nWhat Problem Are We Solving?\nWe need a fast, systematic way to test if two vertices are connected. While adjacency lists are space-efficient, adjacency matrices make edge lookup \\(O(1)\\), perfect when connections are dense or frequent checks are needed.\nThe Adjacency Matrix Builder gives us a table-like structure to store edge information clearly.\n\n\nHow It Works (Plain Language)\nAn adjacency matrix is an \\(n \\times n\\) table for a graph with \\(n\\) vertices:\n\\[\nA[i][j] =\n\\begin{cases}\n1, & \\text{if there is an edge from } i \\text{ to } j,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\n\nFor undirected graphs, the matrix is symmetric: \\(A[i][j] = A[j][i]\\)\nFor directed graphs, symmetry may not hold\nFor weighted graphs, store weights instead of 1s\n\n\n\nExample\nVertices: \\(V = {A, B, C, D}\\) Edges: \\({(A,B), (A,C), (B,C), (C,D)}\\)\nAdjacency Matrix (Undirected)\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n0\n1\n1\n0\n\n\nB\n1\n0\n1\n0\n\n\nC\n1\n1\n0\n1\n\n\nD\n0\n0\n1\n0\n\n\n\nTo check if A and C are connected, test \\(A[A][C] = 1\\)\n\n\nTiny Code (Python)\ndef adjacency_matrix(vertices, edges, directed=False):\n    n = len(vertices)\n    index = {v: i for i, v in enumerate(vertices)}\n    A = [[0] * n for _ in range(n)]\n\n    for u, v in edges:\n        i, j = index[u], index[v]\n        A[i][j] = 1\n        if not directed:\n            A[j][i] = 1\n    return A\n\nvertices = [\"A\", \"B\", \"C\", \"D\"]\nedges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"C\"), (\"C\", \"D\")]\nA = adjacency_matrix(vertices, edges)\nfor row in A:\n    print(row)\nOutput\n[0, 1, 1, 0]\n[1, 0, 1, 0]\n[1, 1, 0, 1]\n[0, 0, 1, 0]\n\n\nWhy It Matters\n\nConstant-time check for edge existence\nSimple mathematical representation for graph algorithms and proofs\nFoundation for matrix-based graph algorithms like:\n\nFloyd–Warshall (all-pairs shortest path)\nAdjacency matrix powers (reachability)\nSpectral graph theory (Laplacian, eigenvalues)\n\n\n\n\nA Gentle Proof (Why It Works)\nEach vertex pair \\((u, v)\\) corresponds to one matrix cell \\(A[i][j]\\). We visit each edge once to set two symmetric entries (undirected) or one (directed). Thus:\n\nTime complexity: \\(O(n^2)\\) to initialize, \\(O(m)\\) to fill\nSpace complexity: \\(O(n^2)\\)\n\nThis tradeoff is worth it when \\(m \\approx n^2\\) (dense graphs).\n\n\nTry It Yourself\n\nBuild an adjacency matrix for a directed triangle (A→B, B→C, C→A)\nModify it to add a self-loop on B\nCheck if \\(A[B][B] = 1\\)\nCompare the symmetry of directed vs undirected graphs\n\n\n\nTest Cases\n\n\n\nGraph Type\nEdges\nSymmetry\nValue\n\n\n\n\nUndirected\n(A,B)\nSymmetric\nA[B][A] = 1\n\n\nDirected\n(A,B)\nNot symmetric\nA[B][A] = 0\n\n\nWeighted\n(A,B,w=5)\nValue stored\nA[A][B] = 5\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild Matrix\n\\(O(n^2)\\)\n\\(O(n^2)\\)\n\n\nEdge Check\n\\(O(1)\\)\n-\n\n\nIterate Neighbors\n\\(O(n)\\)\n-\n\n\n\nAn Adjacency Matrix Builder turns a graph into a table, a universal structure for analysis, efficient queries, and algorithmic transformation.\n\n\n\n83 Adjacency List Builder\nAn Adjacency List Builder constructs a flexible representation of a graph, storing each vertex’s neighbors in a list. It’s memory-efficient for sparse graphs and intuitive for traversal-based algorithms.\n\nWhat Problem Are We Solving?\nWe need a way to represent graphs compactly while still supporting quick traversal of connected vertices. When graphs are sparse (few edges compared to \\(n^2\\)), an adjacency matrix wastes space. An adjacency list focuses only on existing edges, making it both lean and intuitive.\n\n\nHow It Works (Plain Language)\nEach vertex keeps a list of all vertices it connects to. In a directed graph, edges point one way; in an undirected graph, each edge appears twice.\nFor a graph with vertices \\(V\\) and edges \\(E\\), the adjacency list is:\n\\[\n\\text{Adj}[u] = {v \\mid (u, v) \\in E}\n\\]\nYou can think of it as a dictionary (or map) where each key is a vertex, and its value is a list of neighbors.\n\n\nExample\nVertices: \\(V = {A, B, C, D}\\) Edges: \\({(A,B), (A,C), (B,C), (C,D)}\\)\nAdjacency List (Undirected)\nA: [B, C]\nB: [A, C]\nC: [A, B, D]\nD: [C]\n\n\nTiny Code (Python)\ndef adjacency_list(vertices, edges, directed=False):\n    adj = {v: [] for v in vertices}\n    for u, v in edges:\n        adj[u].append(v)\n        if not directed:\n            adj[v].append(u)\n    return adj\n\nvertices = [\"A\", \"B\", \"C\", \"D\"]\nedges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"C\"), (\"C\", \"D\")]\n\ngraph = adjacency_list(vertices, edges)\nfor node, nbrs in graph.items():\n    print(f\"{node}: {nbrs}\")\nOutput\nA: ['B', 'C']\nB: ['A', 'C']\nC: ['A', 'B', 'D']\nD: ['C']\n\n\nWhy It Matters\n\nSpace-efficient for sparse graphs (\\(O(n + m)\\))\nNatural fit for DFS, BFS, and pathfinding\nEasy to modify and extend (weighted edges, labels)\nForms the basis for graph traversal algorithms and network models\n\n\n\nA Gentle Proof (Why It Works)\nEach edge is stored exactly once (directed) or twice (undirected). If \\(n\\) is the number of vertices and \\(m\\) is the number of edges:\n\nInitialization: \\(O(n)\\)\nInsertion: \\(O(m)\\)\nTotal Space: \\(O(n + m)\\)\n\nNo wasted space for missing edges, each list grows only with actual neighbors.\n\n\nTry It Yourself\n\nBuild an adjacency list for a directed graph with edges (A→B, A→C, C→A)\nAdd a new vertex E with no edges; confirm it still appears as E: []\nCount how many total neighbors there are, it should match the edge count\n\n\n\nTest Cases\n\n\n\nGraph Type\nInput Edges\nRepresentation\n\n\n\n\nUndirected\n(A,B)\nA: [B], B: [A]\n\n\nDirected\n(A,B)\nA: [B], B: []\n\n\nWeighted\n(A,B,5)\nA: [(B,5)]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild List\n\\(O(n + m)\\)\n\\(O(n + m)\\)\n\n\nCheck Neighbors\n\\(O(\\deg(v))\\)\n-\n\n\nAdd Edge\n\\(O(1)\\)\n-\n\n\nRemove Edge\n\\(O(\\deg(v))\\)\n-\n\n\n\nAn Adjacency List Builder keeps your graph representation clean and scalable, perfect for algorithms that walk, explore, and connect the dots across large networks.\n\n\n\n84 Degree Counter\nA Degree Counter computes how many edges touch each vertex in a graph. For undirected graphs, the degree is the number of neighbors. For directed graphs, we distinguish between in-degree and out-degree.\n\nWhat Problem Are We Solving?\nWe want to know how connected each vertex is. Degree counts help answer structural questions:\n\nIs the graph regular (all vertices same degree)?\nAre there sources (zero in-degree) or sinks (zero out-degree)?\nWhich node is a hub in a network?\n\nThese insights are foundational for traversal, centrality, and optimization.\n\n\nHow It Works (Plain Language)\nFor each edge \\((u, v)\\):\n\nUndirected: increment degree[u] and degree[v]\nDirected: increment out_degree[u] and in_degree[v]\n\nWhen done, every vertex has its connection count.\n\n\nExample\nUndirected graph: \\[\nV = {A, B, C, D}, \\quad E = {(A,B), (A,C), (B,C), (C,D)}\n\\]\n\n\n\nVertex\nDegree\n\n\n\n\nA\n2\n\n\nB\n2\n\n\nC\n3\n\n\nD\n1\n\n\n\nDirected version:\n\nIn-degree(A)=1 (from C), Out-degree(A)=2 (to B,C)\n\n\n\nTiny Code (Python)\ndef degree_counter(vertices, edges, directed=False):\n    if directed:\n        indeg = {v: 0 for v in vertices}\n        outdeg = {v: 0 for v in vertices}\n        for u, v in edges:\n            outdeg[u] += 1\n            indeg[v] += 1\n        return indeg, outdeg\n    else:\n        deg = {v: 0 for v in vertices}\n        for u, v in edges:\n            deg[u] += 1\n            deg[v] += 1\n        return deg\n\nvertices = [\"A\", \"B\", \"C\", \"D\"]\nedges = [(\"A\",\"B\"), (\"A\",\"C\"), (\"B\",\"C\"), (\"C\",\"D\")]\nprint(degree_counter(vertices, edges))\nOutput\n{'A': 2, 'B': 2, 'C': 3, 'D': 1}\n\n\nWhy It Matters\n\nReveals connectivity patterns\nIdentifies isolated nodes\nEnables graph classification (regular, sparse, dense)\nEssential for graph algorithms (topological sort, PageRank, BFS pruning)\n\n\n\nA Gentle Proof (Why It Works)\nIn any undirected graph, the sum of all degrees equals twice the number of edges:\n\\[\n\\sum_{v \\in V} \\deg(v) = 2|E|\n\\]\nIn directed graphs:\n\\[\n\\sum_{v \\in V} \\text{in}(v) = \\sum_{v \\in V} \\text{out}(v) = |E|\n\\]\nThese equalities guarantee correctness, every edge contributes exactly once (or twice if undirected).\n\n\nTry It Yourself\n\nCreate an undirected graph with edges (A,B), (B,C), (C,A)\n\nVerify all vertices have degree 2\n\nAdd an isolated vertex D\n\nCheck that its degree is 0\n\nConvert to directed edges and count in/out separately\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nGraph\nInput Edges\nOutput\n\n\n\n\nUndirected\n(A,B), (A,C)\nA:2, B:1, C:1\n\n\nDirected\n(A,B), (B,C)\nin(A)=0, out(A)=1; in(C)=1, out(C)=0\n\n\nIsolated Node\n(A,B), V={A,B,C}\nC:0\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nCount Degrees\n\\(O(m)\\)\n\\(O(n)\\)\n\n\nLookup Degree\n\\(O(1)\\)\n-\n\n\n\nA Degree Counter exposes the heartbeat of a graph, showing which nodes are busy, which are lonely, and how the network’s structure unfolds.\n\n\n\n85 Path Existence Tester\nA Path Existence Tester checks whether there is a route between two vertices in a graph, whether you can travel from a source to a destination by following edges.\n\nWhat Problem Are We Solving?\nIn many scenarios, navigation, dependency resolution, communication, the essential question is: “Can we get from A to B?”\nThis is not about finding the shortest path, but simply checking if a path exists at all.\nExamples:\n\nIs a file accessible from the root directory?\nCan data flow between two nodes in a network?\nDoes a dependency graph contain a reachable edge?\n\n\n\nHow It Works (Plain Language)\nWe use graph traversal to explore from the source node. If the destination is reached, a path exists.\nSteps:\n\nChoose a traversal (DFS or BFS)\nStart from source node s\nMark visited nodes\nTraverse neighbors recursively (DFS) or level by level (BFS)\nIf destination t is visited, a path exists\n\n\n\nExample\nGraph: \\[\nV = {A, B, C, D}, \\quad E = {(A, B), (B, C), (C, D)}\n\\]\nQuery: Is there a path from A to D?\nTraversal (DFS or BFS):\n\nStart at A → B → C → D\nD is reached → Path exists ✅\n\nQuery: Is there a path from D to A?\n\nStart at D → no outgoing edges → No path ❌\n\n\n\nTiny Code (Python)\nfrom collections import deque\n\ndef path_exists(graph, source, target):\n    visited = set()\n    queue = deque([source])\n\n    while queue:\n        node = queue.popleft()\n        if node == target:\n            return True\n        if node in visited:\n            continue\n        visited.add(node)\n        queue.extend(graph.get(node, []))\n    return False\n\ngraph = {\n    \"A\": [\"B\"],\n    \"B\": [\"C\"],\n    \"C\": [\"D\"],\n    \"D\": []\n}\nprint(path_exists(graph, \"A\", \"D\"))  # True\nprint(path_exists(graph, \"D\", \"A\"))  # False\n\n\nWhy It Matters\n\nCore to graph connectivity\nUsed in cycle detection, topological sorting, and reachability queries\nFoundational in AI search, routing, compilers, and network analysis\n\n\n\nA Gentle Proof (Why It Works)\nLet the graph be \\(G = (V, E)\\) and traversal be BFS or DFS. Every edge \\((u, v)\\) is explored once. If a path exists, traversal will eventually reach all nodes in the connected component of s. Thus, if t lies in that component, it will be discovered.\nTraversal completeness ensures correctness.\n\n\nTry It Yourself\n\nBuild a directed graph \\(A \\to B \\to C\\), and check \\(A \\to C\\) and \\(C \\to A\\).\nAdd an extra edge \\(C \\to A\\).\n\nNow the graph is strongly connected.\nEvery node should reach every other node.\n\nVisualize traversal using a queue or recursion trace.\n\n\n\nTest Cases\n\n\n\nGraph\nSource\nTarget\nResult\n\n\n\n\nA→B→C\nA\nC\nTrue\n\n\nA→B→C\nC\nA\nFalse\n\n\nA↔︎B\nA\nB\nTrue\n\n\nDisconnected\nA\nD\nFalse\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBFS / DFS\n\\(O(n + m)\\)\n\\(O(n)\\)\n\n\n\n\\(n\\) = vertices, \\(m\\) = edges.\nA Path Existence Tester is the simplest yet most powerful diagnostic for graph connectivity, revealing whether two points belong to the same connected world.\n\n\n\n86 Tree Validator\nA Tree Validator checks whether a given graph satisfies the defining properties of a tree: it is connected and acyclic.\n\nWhat Problem Are We Solving?\nWe often encounter structures that look like trees, but we must confirm they truly are. For example:\n\nCan this dependency graph be represented as a tree?\nIs the given parent–child relation a valid hierarchy?\nDoes this undirected graph contain cycles or disconnected parts?\n\nA Tree Validator formalizes that check.\nA tree must satisfy:\n\nConnectivity: every vertex reachable from any other.\nAcyclicity: no cycles exist.\n(Equivalently for undirected graphs) \\[ |E| = |V| - 1 \\]\n\n\n\nHow It Works (Plain Language)\nWe can validate using traversal and counting:\nMethod 1: DFS + Parent Check\n\nStart DFS from any node.\nTrack visited nodes.\nIf a neighbor is visited and not parent, a cycle exists.\nAfter traversal, check all nodes visited (connectedness).\n\nMethod 2: Edge–Vertex Property\n\nCheck if graph has exactly \\(|V| - 1\\) edges.\nRun DFS/BFS to ensure graph is connected.\n\n\n\nExample\nGraph 1: \\[\nV = {A, B, C, D}, \\quad E = {(A, B), (A, C), (B, D)}\n\\]\n\n\\(|V| = 4\\), \\(|E| = 3\\)\nConnected, no cycle → ✅ Tree\n\nGraph 2: \\[\nV = {A, B, C}, \\quad E = {(A, B), (B, C), (C, A)}\n\\]\n\n\\(|V| = 3\\), \\(|E| = 3\\)\nCycle present → ❌ Not a tree\n\n\n\nTiny Code (Python)\ndef is_tree(graph):\n    n = len(graph)\n    visited = set()\n    parent = {}\n\n    def dfs(node, par):\n        visited.add(node)\n        for nbr in graph[node]:\n            if nbr == par:\n                continue\n            if nbr in visited:\n                return False  # cycle detected\n            if not dfs(nbr, node):\n                return False\n        return True\n\n    # Start from first node\n    start = next(iter(graph))\n    if not dfs(start, None):\n        return False\n\n    # Check connectivity\n    return len(visited) == n\nExample:\ngraph = {\n    \"A\": [\"B\", \"C\"],\n    \"B\": [\"A\", \"D\"],\n    \"C\": [\"A\"],\n    \"D\": [\"B\"]\n}\nprint(is_tree(graph))  # True\n\n\nWhy It Matters\nTree validation ensures:\n\nHierarchies are acyclic\nData structures (like ASTs, tries) are well-formed\nNetwork topologies avoid redundant links\nAlgorithms relying on tree properties (DFS order, LCA, spanning tree) are safe\n\n\n\nA Gentle Proof (Why It Works)\nA connected graph without cycles is a tree. Inductive reasoning:\n\nBase: single node, zero edges, trivially a tree.\nInduction: adding one edge that connects a new node preserves acyclicity. If a cycle forms, it violates tree property.\n\nAlso, for undirected graph: \\[\n\\text{Tree} \\iff \\text{Connected} \\land |E| = |V| - 1\n\\]\n\n\nTry It Yourself\n\nDraw a small graph with 4 nodes.\nAdd edges one by one.\n\nAfter each addition, test if graph is still a tree.\n\nIntroduce a cycle and rerun validator.\nRemove an edge and check connectivity failure.\n\n\n\nTest Cases\n\n\n\nGraph\nConnected\nCycle\nTree\n\n\n\n\nA–B–C\n✅\n❌\n✅\n\n\nA–B, B–C, C–A\n✅\n✅\n❌\n\n\nA–B, C\n❌\n❌\n❌\n\n\nSingle Node\n✅\n❌\n✅\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDFS\n\\(O(n + m)\\)\n\\(O(n)\\)\n\n\n\nA Tree Validator ensures structure, order, and simplicity, the quiet geometry behind every hierarchy.\n\n\n\n86 Tree Validator\nA Tree Validator checks whether a given graph satisfies the defining properties of a tree: it is connected and acyclic.\n\nWhat Problem Are We Solving?\nWe often encounter structures that look like trees, but we must confirm they truly are. For example:\n\nCan this dependency graph be represented as a tree?\nIs the given parent–child relation a valid hierarchy?\nDoes this undirected graph contain cycles or disconnected parts?\n\nA Tree Validator formalizes that check.\nA tree must satisfy:\n\nConnectivity: every vertex reachable from any other.\nAcyclicity: no cycles exist.\n(Equivalently for undirected graphs) \\[|E| = |V| - 1\\]\n\n\n\nHow It Works (Plain Language)\nWe can validate using traversal and counting.\nMethod 1: DFS + Parent Check\n\nStart DFS from any node.\nTrack visited nodes.\nIf a neighbor is visited and not parent, a cycle exists.\nAfter traversal, check all nodes visited (connectedness).\n\nMethod 2: Edge–Vertex Property\n\nCheck if graph has exactly \\(|V| - 1\\) edges.\nRun DFS or BFS to ensure graph is connected.\n\n\n\nExample\nGraph 1: \\[\nV = {A, B, C, D}, \\quad E = {(A, B), (A, C), (B, D)}\n\\]\n\n\\(|V| = 4\\), \\(|E| = 3\\)\nConnected, no cycle → Tree\n\nGraph 2: \\[\nV = {A, B, C}, \\quad E = {(A, B), (B, C), (C, A)}\n\\]\n\n\\(|V| = 3\\), \\(|E| = 3\\)\nCycle present → Not a tree\n\n\n\nTiny Code (Python)\ndef is_tree(graph):\n    n = len(graph)\n    visited = set()\n    parent = {}\n\n    def dfs(node, par):\n        visited.add(node)\n        for nbr in graph[node]:\n            if nbr == par:\n                continue\n            if nbr in visited:\n                return False  # cycle detected\n            if not dfs(nbr, node):\n                return False\n        return True\n\n    # Start from first node\n    start = next(iter(graph))\n    if not dfs(start, None):\n        return False\n\n    # Check connectivity\n    return len(visited) == n\nExample:\ngraph = {\n    \"A\": [\"B\", \"C\"],\n    \"B\": [\"A\", \"D\"],\n    \"C\": [\"A\"],\n    \"D\": [\"B\"]\n}\nprint(is_tree(graph))  # True\n\n\nWhy It Matters\nTree validation ensures:\n\nHierarchies are acyclic\nData structures (like ASTs, tries) are well-formed\nNetwork topologies avoid redundant links\nAlgorithms relying on tree properties (DFS order, LCA, spanning tree) are safe\n\n\n\nA Gentle Proof (Why It Works)\nA connected graph without cycles is a tree. Inductive reasoning:\n\nBase: single node, zero edges, trivially a tree.\nInduction: adding one edge that connects a new node preserves acyclicity. If a cycle forms, it violates the tree property.\n\nAlso, for an undirected graph: \\[\n\\text{Tree} \\iff \\text{Connected} \\land |E| = |V| - 1\n\\]\n\n\nTry It Yourself\n\nDraw a small graph with 4 nodes.\nAdd edges one by one.\n\nAfter each addition, test if the graph is still a tree.\n\nIntroduce a cycle and rerun the validator.\nRemove an edge and check for connectivity failure.\n\n\n\nTest Cases\n\n\n\nGraph\nConnected\nCycle\nTree\n\n\n\n\nA–B–C\nYes\nNo\nYes\n\n\nA–B, B–C, C–A\nYes\nYes\nNo\n\n\nA–B, C\nNo\nNo\nNo\n\n\nSingle Node\nYes\nNo\nYes\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDFS\n\\(O(n + m)\\)\n\\(O(n)\\)\n\n\n\nA Tree Validator ensures structure, order, and simplicity, the quiet geometry behind every hierarchy.\n\n\n\n87 Rooted Tree Builder\nA Rooted Tree Builder constructs a tree from a given parent array or edge list, designating one node as the root and connecting all others accordingly.\n\nWhat Problem Are We Solving?\nOften we receive data in flat form—like a list of parent indices, database references, or parent–child pairs—and we need to reconstruct the actual tree structure.\nFor example:\n\nA parent array [ -1, 0, 0, 1, 1, 2 ] represents which node is parent of each.\nIn file systems, each directory knows its parent; we need to rebuild the hierarchy.\n\nThe Rooted Tree Builder formalizes this reconstruction.\n\n\nHow It Works (Plain Language)\nA parent array encodes each node’s parent:\n\nparent[i] = j means node j is the parent of i.\nIf parent[i] = -1, then i is the root.\n\nSteps:\n\nFind the root (the node with parent -1).\nInitialize an adjacency list children for each node.\nFor each node i:\n\nIf parent[i] != -1, append i to children[parent[i]].\n\nOutput the adjacency structure.\n\nThis gives a tree with parent–child relationships.\n\n\nExample\nParent array:\nIndex:  0  1  2  3  4  5\nParent: -1  0  0  1  1  2\nInterpretation:\n\n0 is root.\n1 and 2 are children of 0.\n3 and 4 are children of 1.\n5 is child of 2.\n\nTree:\n0\n├── 1\n│   ├── 3\n│   └── 4\n└── 2\n    └── 5\n\n\nTiny Code (Python)\ndef build_tree(parent):\n    n = len(parent)\n    children = [[] for _ in range(n)]\n    root = None\n\n    for i in range(n):\n        if parent[i] == -1:\n            root = i\n        else:\n            children[parent[i]].append(i)\n\n    return root, children\nExample:\nparent = [-1, 0, 0, 1, 1, 2]\nroot, children = build_tree(parent)\n\nprint(\"Root:\", root)\nfor i, c in enumerate(children):\n    print(f\"{i}: {c}\")\nOutput:\nRoot: 0\n0: [1, 2]\n1: [3, 4]\n2: [5]\n3: []\n4: []\n5: []\n\n\nWhy It Matters\nTree reconstruction is foundational in:\n\nCompilers: abstract syntax tree (AST) reconstruction\nDatabases: reconstructing hierarchical relationships\nOperating systems: file directory trees\nOrganization charts: building hierarchies from parent–child data\n\nIt connects linear storage to hierarchical structure.\n\n\nA Gentle Proof (Why It Works)\nIf the parent array satisfies:\n\nExactly one root: one entry with -1\nAll other nodes have exactly one parent\nThe resulting structure is connected and acyclic\n\nThen the output is a valid rooted tree: \\[\n|E| = |V| - 1, \\text{ and exactly one node has no parent.}\n\\]\nEach child is linked once, forming a tree rooted at the unique node with -1.\n\n\nTry It Yourself\n\nWrite your own parent array (e.g., [ -1, 0, 0, 1, 2 ]).\nConvert it into a tree.\nDraw the hierarchy manually.\nVerify connectivity and acyclicity.\n\n\n\nTest Cases\n\n\n\nParent Array\nRoot\nChildren Structure\n\n\n\n\n[-1, 0, 0, 1, 1, 2]\n0\n0:[1,2], 1:[3,4], 2:[5]\n\n\n[-1, 0, 1, 2]\n0\n0:[1], 1:[2], 2:[3]\n\n\n[-1]\n0\n0:[]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\n\\(O(n)\\)\n\\(O(n)\\)\n\n\n\nThe Rooted Tree Builder bridges the gap between flat data and hierarchical form, turning arrays into living structures.\n\n\n\n88 Traversal Order Visualizer\nA Traversal Order Visualizer shows how different tree traversals (preorder, inorder, postorder, level order) explore nodes, revealing the logic behind recursive and iterative visits.\n\nWhat Problem Are We Solving?\nWhen working with trees, the order of visiting nodes matters. Different traversals serve different goals:\n\nPreorder: process parent before children\nInorder: process left child, then parent, then right child\nPostorder: process children before parent\nLevel order: visit nodes breadth-first\n\nUnderstanding these traversals helps in:\n\nExpression parsing\nFile system navigation\nTree printing and evaluation\n\nA visualizer clarifies when and why each node is visited.\n\n\nHow It Works (Plain Language)\nConsider a binary tree:\n      A\n     / \\\n    B   C\n   / \\\n  D   E\nEach traversal orders nodes differently:\n\n\n\nTraversal\nOrder\n\n\n\n\nPreorder\nA, B, D, E, C\n\n\nInorder\nD, B, E, A, C\n\n\nPostorder\nD, E, B, C, A\n\n\nLevel order\nA, B, C, D, E\n\n\n\nVisualization strategy:\n\nStart at the root.\nUse recursion (depth-first) or queue (breadth-first).\nRecord each visit step.\nOutput sequence in order visited.\n\n\n\nExample Step by Step\nTree:\nA\n├── B\n│   ├── D\n│   └── E\n└── C\nPreorder\n\nVisit A\nVisit B\nVisit D\nVisit E\nVisit C\n\nSequence: A B D E C\nInorder\n\nTraverse left subtree of A (B)\nTraverse left of B (D) → visit D\nVisit B\nTraverse right of B (E) → visit E\nVisit A\nVisit right subtree (C)\n\nSequence: D B E A C\n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef preorder(root):\n    if not root:\n        return []\n    return [root.val] + preorder(root.left) + preorder(root.right)\n\ndef inorder(root):\n    if not root:\n        return []\n    return inorder(root.left) + [root.val] + inorder(root.right)\n\ndef postorder(root):\n    if not root:\n        return []\n    return postorder(root.left) + postorder(root.right) + [root.val]\n\ndef level_order(root):\n    if not root:\n        return []\n    queue = [root]\n    result = []\n    while queue:\n        node = queue.pop(0)\n        result.append(node.val)\n        if node.left:\n            queue.append(node.left)\n        if node.right:\n            queue.append(node.right)\n    return result\n\n\nWhy It Matters\nTraversal order determines:\n\nComputation sequence (evaluation, deletion, printing)\nExpression tree evaluation (postorder)\nSerialization/deserialization (preorder + inorder)\nBreadth-first exploration (level order)\n\nUnderstanding traversal = understanding how algorithms move through structure.\n\n\nA Gentle Proof (Why It Works)\nEach traversal is a systematic walk:\n\nPreorder ensures root-first visitation.\nInorder ensures sorted order in binary search trees.\nPostorder ensures children processed before parent.\nLevel order ensures minimal depth-first layering.\n\nSince each node is visited exactly once, correctness follows from recursion and induction.\n\n\nTry It Yourself\n\nBuild a binary tree with 5 nodes.\nWrite out all four traversals by hand.\nTrace recursive calls step by step.\nObserve how order changes per traversal.\n\n\n\nTest Cases\n\n\n\nTraversal\nExample Tree\nExpected Order\n\n\n\n\nPreorder\nA-B-C\nA B C\n\n\nInorder\nA-B-C\nB A C\n\n\nPostorder\nA-B-C\nB C A\n\n\nLevel order\nA-B-C\nA B C\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDFS (Pre/In/Post)\n\\(O(n)\\)\n\\(O(h)\\) (stack)\n\n\nBFS (Level)\n\\(O(n)\\)\n\\(O(n)\\) (queue)\n\n\n\nThe Traversal Order Visualizer turns abstract definitions into motion, showing how structure guides computation.\n\n\n\n89 Edge Classifier\nAn Edge Classifier determines the type of each edge encountered during a graph traversal, whether it is a tree edge, back edge, forward edge, or cross edge. This classification helps us understand the structure and flow of a directed or undirected graph.\n\nWhat Problem Are We Solving?\nIn graph algorithms, not all edges play the same role. When we traverse using DFS, we can interpret the relationship between vertices based on discovery times.\nEdge classification helps answer questions like:\n\nIs there a cycle? (Look for back edges)\nHow is the graph structured? (Tree vs forward edges)\nIs this DAG (Directed Acyclic Graph)? (No back edges)\nWhat’s the hierarchical relation between nodes?\n\nBy tagging edges, we gain structural insight into traversal behavior.\n\n\nHow It Works (Plain Language)\nDuring DFS, we assign each vertex:\n\nDiscovery time when first visited.\nFinish time when exploration completes.\n\nEach edge \\((u, v)\\) is then classified as:\n\n\n\nType\nCondition\n\n\n\n\nTree edge\n\\(v\\) is first discovered by \\((u, v)\\)\n\n\nBack edge\n\\(v\\) is ancestor of \\(u\\) (cycle indicator)\n\n\nForward edge\n\\(v\\) is descendant of \\(u\\), but already visited\n\n\nCross edge\n\\(v\\) is neither ancestor nor descendant of \\(u\\)\n\n\n\nIn undirected graphs, only tree and back edges occur.\n\n\nExample\nGraph (directed):\n1 → 2 → 3\n↑   ↓\n4 ← 5\nDuring DFS starting at 1:\n\n(1,2): tree edge\n(2,3): tree edge\n(3,4): back edge (cycle 1–2–3–4–1)\n(2,5): tree edge\n(5,4): tree edge\n(4,1): back edge\n\nSo we detect cycles due to back edges.\n\n\nTiny Code (Python)\ndef classify_edges(graph):\n    time = 0\n    discovered = {}\n    finished = {}\n    classification = []\n\n    def dfs(u):\n        nonlocal time\n        time += 1\n        discovered[u] = time\n        for v in graph[u]:\n            if v not in discovered:\n                classification.append(((u, v), \"Tree\"))\n                dfs(v)\n            elif v not in finished:\n                classification.append(((u, v), \"Back\"))\n            elif discovered[u] &lt; discovered[v]:\n                classification.append(((u, v), \"Forward\"))\n            else:\n                classification.append(((u, v), \"Cross\"))\n        time += 1\n        finished[u] = time\n\n    for node in graph:\n        if node not in discovered:\n            dfs(node)\n    return classification\n\n\nWhy It Matters\nEdge classification underpins:\n\nCycle detection (look for back edges)\nTopological sorting (DAGs have no back edges)\nDFS tree structure analysis\nStrongly connected component detection\n\nIt converts traversal into structural insight.\n\n\nA Gentle Proof (Why It Works)\nDFS imposes a temporal order on discovery and finish times. An edge \\((u, v)\\) can only fall into one of the four categories because:\n\\[\n\\text{Each vertex has a distinct discovery and finish time interval.}\n\\]\nBy comparing intervals \\((d[u], f[u])\\) and \\((d[v], f[v])\\), we deduce whether \\(v\\) lies inside, before, or after \\(u\\)’s traversal window.\n\n\nTry It Yourself\n\nDraw a small directed graph.\nAssign discovery/finish times using DFS.\nCompare intervals for each edge.\nLabel each edge as Tree, Back, Forward, or Cross.\nVerify that DAGs have no back edges.\n\n\n\nTest Cases\n\n\n\nEdge\nType\n\n\n\n\n(A, B)\nTree\n\n\n(B, C)\nTree\n\n\n(C, A)\nBack\n\n\n(B, D)\nTree\n\n\n(D, E)\nTree\n\n\n(E, B)\nBack\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDFS Traversal\n\\(O(n + m)\\)\n\\(O(n)\\)\n\n\nClassification\n\\(O(m)\\)\n\\(O(m)\\)\n\n\n\nThe Edge Classifier transforms traversal into topology, making invisible structures like cycles, hierarchies, and cross-links explicit.\n\n\n\n90 Connectivity Checker\nA Connectivity Checker determines whether a graph is connected, that is, whether every vertex can be reached from any other vertex. It’s a fundamental diagnostic tool in graph theory and network analysis.\n\nWhat Problem Are We Solving?\nConnectivity tells us whether the graph forms a single whole or multiple isolated parts.\nWe often ask:\n\nCan all nodes communicate in this network?\nIs this maze solvable from start to end?\nDoes this undirected graph form one component or many?\nFor directed graphs: can we reach every vertex from every other vertex?\n\nThe Connectivity Checker gives a yes/no answer, and can also enumerate connected components.\n\n\nHow It Works (Plain Language)\nUndirected Graph:\n\nPick a starting node.\nPerform DFS or BFS, marking all reachable nodes.\nAfter traversal, if all nodes are marked, the graph is connected.\n\nDirected Graph:\n\nUse two traversals:\n\nRun DFS from any node. If not all nodes are visited, not strongly connected.\nReverse all edges and run DFS again. If still not all nodes are visited, not strongly connected.\n\n\nAlternatively, detect strongly connected components (SCCs) via Kosaraju’s or Tarjan’s algorithm.\n\n\nExample (Undirected)\nGraph 1:\n1, 2, 3\n|       |\n4, 5, 6\nAll nodes reachable → Connected.\nGraph 2:\n1, 2    3, 4\nTwo separate parts → Not connected.\n\n\nExample (Directed)\nGraph:\n1 → 2 → 3\n↑       ↓\n└───────┘\nEvery node reachable from every other → Strongly connected\nGraph:\n1 → 2 → 3\nNo path from 3 → 1 → Not strongly connected\n\n\nTiny Code (Python)\nfrom collections import deque\n\ndef is_connected(graph):\n    n = len(graph)\n    visited = set()\n\n    # BFS from first node\n    start = next(iter(graph))\n    queue = deque([start])\n    while queue:\n        u = queue.popleft()\n        if u in visited:\n            continue\n        visited.add(u)\n        for v in graph[u]:\n            if v not in visited:\n                queue.append(v)\n    \n    return len(visited) == n\nExample:\ngraph = {\n    1: [2, 4],\n    2: [1, 3],\n    3: [2, 6],\n    4: [1, 5],\n    5: [4, 6],\n    6: [3, 5]\n}\nprint(is_connected(graph))  # True\n\n\nWhy It Matters\nConnectivity is central in:\n\nNetwork reliability, ensure all nodes communicate\nGraph algorithms, many assume connected graphs\nClustering, find connected components\nPathfinding, unreachable nodes signal barriers\n\nIt’s often the first diagnostic check before deeper analysis.\n\n\nA Gentle Proof (Why It Works)\nFor undirected graphs, connectivity is equivalence relation:\n\nReflexive: node connects to itself\nSymmetric: if A connects to B, B connects to A\nTransitive: if A connects to B and B connects to C, A connects to C\n\nTherefore, DFS/BFS reachability partitioning defines connected components uniquely.\n\n\nTry It Yourself\n\nDraw a graph with 6 nodes.\nRun BFS or DFS from node 1.\nMark all reachable nodes.\nIf some remain unvisited, you’ve found multiple components.\nFor directed graphs, try reversing edges and retesting.\n\n\n\nTest Cases\n\n\n\nGraph\nType\nResult\n\n\n\n\n1–2–3\nUndirected\nConnected\n\n\n1–2, 3–4\nUndirected\nNot Connected\n\n\n1→2→3, 3→1\nDirected\nStrongly Connected\n\n\n1→2→3\nDirected\nNot Strongly Connected\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDFS/BFS\n\\(O(n + m)\\)\n\\(O(n)\\)\n\n\n\nA Connectivity Checker ensures your graph is a single story, not a collection of isolated tales, a foundation before every journey through the graph.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-1.html#section-10.-algorithm-design-patterns",
    "href": "books/en-us/list-1.html#section-10.-algorithm-design-patterns",
    "title": "Chapter 1. Foundations of Algorithms",
    "section": "Section 10. Algorithm Design Patterns",
    "text": "Section 10. Algorithm Design Patterns\n\n91 Brute Force Pattern\nThe Brute Force Pattern is the simplest and most universal approach to problem-solving: try every possible option, evaluate them all, and pick the best. It trades computational efficiency for conceptual clarity and correctness.\n\nWhat Problem Are We Solving?\nSometimes, before clever optimizations or heuristics, we need a baseline solution, a way to ensure correctness. The brute force approach guarantees finding the right answer by exploring all possible configurations, even if it’s slow.\nCommon use cases:\n\nExhaustive search (e.g., generating all permutations or subsets)\nBaseline testing before implementing heuristics\nProving optimality by comparison\n\n\n\nHow It Works (Plain Language)\nA brute force algorithm generally follows this structure:\n\nEnumerate all candidate solutions.\nEvaluate each candidate for validity or cost.\nSelect the best (or first valid) solution.\n\nThis is conceptually simple, though often expensive in time.\n\n\nExample: Traveling Salesman Problem (TSP)\nGiven \\(n\\) cities and distances between them, find the shortest tour visiting all.\nBrute force solution:\n\nGenerate all \\(n!\\) possible tours.\nCompute the total distance for each.\nReturn the shortest tour.\n\nThis ensures correctness but grows factorially in complexity.\n\n\nTiny Code (Python)\nfrom itertools import permutations\n\ndef tsp_bruteforce(dist):\n    n = len(dist)\n    cities = list(range(n))\n    best = float('inf')\n    best_path = None\n    \n    for perm in permutations(cities[1:]):  # fix city 0 as start\n        path = [0] + list(perm) + [0]\n        cost = sum(dist[path[i]][path[i+1]] for i in range(n))\n        if cost &lt; best:\n            best = cost\n            best_path = path\n    return best, best_path\n\n# Example distance matrix\ndist = [\n    [0, 10, 15, 20],\n    [10, 0, 35, 25],\n    [15, 35, 0, 30],\n    [20, 25, 30, 0]\n]\n\nprint(tsp_bruteforce(dist))  # (80, [0, 1, 3, 2, 0])\n\n\nWhy It Matters\nBrute force is valuable for:\n\nCorrectness: guarantees the right answer.\nBenchmarking: provides a ground truth for optimization.\nSmall inputs: often feasible when \\(n\\) is small.\nTeaching: clarifies the structure of search and evaluation.\n\nIt is the seed from which more refined algorithms (like DP, backtracking, and heuristics) evolve.\n\n\nA Gentle Proof (Why It Works)\nLet \\(S\\) be the finite set of all possible solutions. If the algorithm evaluates every \\(s \\in S\\) and correctly computes its quality, and selects the minimum (or maximum), the chosen \\(s^*\\) is provably optimal: \\[\ns^* = \\arg\\min_{s \\in S} f(s)\n\\] Completeness and correctness are inherent, though efficiency is not.\n\n\nTry It Yourself\n\nEnumerate all subsets of \\({1, 2, 3}\\).\nCheck which subsets sum to 4.\nConfirm all possibilities are considered.\nReflect on the time cost: \\(2^n\\) subsets for \\(n\\) elements.\n\n\n\nTest Cases\n\n\n\nProblem\nInput Size\nFeasible?\nNotes\n\n\n\n\nTSP\nn = 4\n✅\n\\(4! = 24\\) paths\n\n\nTSP\nn = 10\n❌\n\\(10! \\approx 3.6 \\times 10^6\\)\n\n\nSubset Sum\nn = 10\n✅\n\\(2^{10} = 1024\\) subsets\n\n\nSubset Sum\nn = 30\n❌\n\\(2^{30} \\approx 10^9\\) subsets\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nEnumeration\n\\(O(k^n)\\) (varies)\n\\(O(n)\\)\n\n\n\nThe Brute Force Pattern is the blank canvas of algorithmic design: simple, exhaustive, and pure, a way to guarantee truth before seeking elegance.\n\n\n\n92 Greedy Pattern\nThe Greedy Pattern builds a solution step by step, choosing at each stage the locally optimal move, the one that seems best right now, with the hope (and often the proof) that this path leads to a globally optimal result.\n\nWhat Problem Are We Solving?\nGreedy algorithms are used when problems exhibit two key properties:\n\nGreedy-choice property – a global optimum can be reached by choosing local optima.\nOptimal substructure – an optimal solution contains optimal solutions to subproblems.\n\nYou’ll meet greedy reasoning everywhere: scheduling, pathfinding, compression, and resource allocation.\n\n\nHow It Works (Plain Language)\nGreedy thinking is “take the best bite each time.” There’s no looking back, no exploring alternatives, just a sequence of decisive moves.\nGeneral shape:\n\nStart with an empty or initial solution.\nRepeatedly choose the best local move (by some rule).\nStop when no more moves are possible or desired.\n\n\n\nExample: Coin Change (Canonical Coins)\nGiven coins \\({25, 10, 5, 1}\\), make change for 63 cents.\nGreedy approach:\n\nTake largest coin \\(\\le\\) remaining value.\nSubtract and repeat. Result: \\(25 + 25 + 10 + 1 + 1 + 1 = 63\\) (6 coins total)\n\nWorks for canonical systems, not all, a nice teaching point.\n\n\nTiny Code (Python)\ndef greedy_coin_change(coins, amount):\n    result = []\n    for c in sorted(coins, reverse=True):\n        while amount &gt;= c:\n            amount -= c\n            result.append(c)\n    return result\n\nprint(greedy_coin_change([25, 10, 5, 1], 63))\n# [25, 25, 10, 1, 1, 1]\n\n\nWhy It Matters\nThe greedy pattern is a core design paradigm:\n\nSimple and fast – often linear or \\(O(n \\log n)\\).\nProvably optimal when conditions hold.\nIntuitive – builds insight into structure of problems.\nFoundation – many approximation and heuristic algorithms are “greedy at heart.”\n\n\n\nA Gentle Proof (Why It Works)\nFor problems with optimal substructure, we can often prove by induction:\nIf a greedy choice \\(g\\) leaves a subproblem \\(P'\\), and \\[\\text{OPT}(P) = g + \\text{OPT}(P')\\] then solving \\(P'\\) optimally ensures global optimality.\nFor coin change with canonical coins, this holds since choosing a larger coin never prevents an optimal total.\n\n\nTry It Yourself\n\nApply the greedy method to Activity Selection: Sort activities by finishing time, pick earliest finishing one, and skip overlapping.\nCompare against brute force enumeration.\nCheck if the greedy result is optimal, why or why not?\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nProblem\nGreedy Works?\nNote\n\n\n\n\nActivity Selection\n✅\nLocal earliest-finish leads to global max\n\n\nCoin Change (1, 3, 4) for 6\n❌\n3+3 better than 4+1+1\n\n\nHuffman Coding\n✅\nGreedy merging yields optimal tree\n\n\nKruskal’s MST\n✅\nGreedy edge selection builds MST\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSelection\n\\(O(n \\log n)\\) (sorting)\n\\(O(1)\\)\n\n\nStep Choice\n\\(O(n)\\)\n\\(O(1)\\)\n\n\n\nThe Greedy Pattern is the art of decisive reasoning, choosing what seems best now, and trusting the problem’s structure to reward confidence.\n\n\n\n93 Divide and Conquer Pattern\nThe Divide and Conquer Pattern breaks a big problem into smaller, similar subproblems, solves each one (often recursively), and then combines their results into the final answer.\nIt’s the pattern behind merge sort, quicksort, binary search, and fast algorithms across mathematics and computation.\n\nWhat Problem Are We Solving?\nWe use divide and conquer when:\n\nThe problem can be split into smaller subproblems of the same type.\nThose subproblems are independent and easier to solve.\nTheir solutions can be merged efficiently.\n\nIt’s the algorithmic mirror of mathematical induction, reduce, solve, combine.\n\n\nHow It Works (Plain Language)\nThink of divide and conquer as a recursive three-step dance:\n\nDivide – split the problem into smaller parts.\nConquer – solve each part recursively.\nCombine – merge the sub-results into a final answer.\n\nEach recursive call tackles a fraction of the work until reaching a base case.\n\n\nExample: Merge Sort\nSort an array \\(A[1..n]\\).\n\nDivide: split \\(A\\) into two halves.\nConquer: recursively sort each half.\nCombine: merge the two sorted halves.\n\nRecurrence: \\[T(n) = 2T\\left(\\frac{n}{2}\\right) + O(n)\\] Solution: \\[T(n) = O(n \\log n)\\]\n\n\nTiny Code (Python)\ndef merge_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt;= right[j]:\n            result.append(left[i]); i += 1\n        else:\n            result.append(right[j]); j += 1\n    result.extend(left[i:]); result.extend(right[j:])\n    return result\n\n\nWhy It Matters\nDivide and conquer turns recursion into efficiency. It’s a framework for:\n\nSorting (Merge Sort, Quick Sort)\nSearching (Binary Search)\nMatrix Multiplication (Strassen’s Algorithm)\nFFT (Fast Fourier Transform)\nGeometry (Closest Pair, Convex Hull)\nData Science (Divide-and-Conquer Regression, Decision Trees)\n\nIt captures the principle: solve big problems by shrinking them.\n\n\nA Gentle Proof (Why It Works)\nAssume each subproblem of size \\(\\frac{n}{2}\\) is solved optimally.\nIf we combine \\(k\\) subresults with cost \\(f(n)\\), the total cost follows the recurrence \\[T(n) = aT\\left(\\frac{n}{b}\\right) + f(n)\\]\nUsing the Master Theorem, we compare \\(f(n)\\) with \\(n^{\\log_b a}\\) to find \\(T(n)\\).\nFor merge sort: \\(a = 2, b = 2, f(n) = n\\) ⇒ \\(T(n) = O(n \\log n)\\).\n\n\nTry It Yourself\n\nApply divide and conquer to maximum subarray sum (Kadane’s alternative).\nWrite a binary search with clear divide/conquer steps.\nVisualize recursion tree and total cost at each level.\n\n\n\nTest Cases\n\n\n\nProblem\nDivide\nCombine\nWorks Well?\n\n\n\n\nMerge Sort\nSplit array\nMerge halves\n✅\n\n\nQuick Sort\nPartition array\nConcatenate\n✅ (average)\n\n\nBinary Search\nSplit range\nReturn match\n✅\n\n\nClosest Pair\nDivide plane\nCompare boundary\n✅\n\n\n\n\n\nComplexity\n\n\n\nStep\nCost\n\n\n\n\nDivide\n\\(O(1)\\) or \\(O(n)\\)\n\n\nConquer\n\\(aT(n/b)\\)\n\n\nCombine\n\\(O(n)\\) (typical)\n\n\n\nOverall: \\(O(n \\log n)\\) in many classic cases.\nDivide and conquer is the essence of recursive decomposition, see the whole by mastering the parts.\n\n\n\n94 Dynamic Programming Pattern\nThe Dynamic Programming (DP) Pattern solves complex problems by breaking them into overlapping subproblems, solving each once, and storing results to avoid recomputation.\nIt transforms exponential recursive solutions into efficient polynomial ones through memoization or tabulation.\n\nWhat Problem Are We Solving?\nWhen a problem has:\n\nOverlapping subproblems – the same subtask appears multiple times.\nOptimal substructure – an optimal solution can be built from optimal subsolutions.\n\nNaive recursion repeats work. DP ensures each subproblem is solved once.\n\n\nHow It Works (Plain Language)\nThink of DP as smart recursion:\n\nDefine a state that captures progress.\nDefine a recurrence that relates larger states to smaller ones.\nStore results to reuse later.\n\nTwo main flavors:\n\nTop-down (Memoization) – recursion with caching.\nBottom-up (Tabulation) – fill a table iteratively.\n\n\n\nExample: Fibonacci Numbers\nNaive recursion: \\[F(n) = F(n-1) + F(n-2)\\] This recomputes many values.\nDP solution:\n\nBase: \\(F(0)=0, F(1)=1\\)\nBuild up table: \\[F[i] = F[i-1] + F[i-2]\\]\n\nResult: \\(O(n)\\) time, \\(O(n)\\) space (or \\(O(1)\\) optimized).\n\n\nTiny Code (Python)\ndef fib(n):\n    dp = [0, 1] + [0]*(n-1)\n    for i in range(2, n+1):\n        dp[i] = dp[i-1] + dp[i-2]\n    return dp[n]\nOr memoized recursion:\nfrom functools import lru_cache\n\n@lru_cache(None)\ndef fib(n):\n    if n &lt; 2:\n        return n\n    return fib(n-1) + fib(n-2)\n\n\nWhy It Matters\nDP is the core of algorithmic problem solving:\n\nOptimization: shortest paths, knapsack, edit distance\nCounting: number of ways to climb stairs, partitions\nSequence analysis: LIS, LCS\nResource allocation: scheduling, investment problems\n\nIt’s how we bring structure to recursion.\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be the cost to solve all distinct subproblems. Since each is solved once and combined in constant time: \\[T(n) = O(\\text{number of states}) \\times O(\\text{transition cost})\\]\nFor Fibonacci:\n\nStates = \\(n\\)\nTransition cost = \\(O(1)\\) ⇒ \\(T(n) = O(n)\\)\n\nMemoization ensures every subproblem is visited at most once.\n\n\nTry It Yourself\n\nWrite DP for coin change (ways to form a sum).\nTrace longest common subsequence (LCS) table.\nCompare top-down vs bottom-up performance.\n\n\n\nTest Cases\n\n\n\nProblem\nState\nTransition\nTime\n\n\n\n\nFibonacci\n\\(n\\)\n\\(dp[n]=dp[n-1]+dp[n-2]\\)\n\\(O(n)\\)\n\n\nKnapsack\n\\((i,w)\\)\n\\(\\max(\\text{take}, \\text{skip})\\)\n\\(O(nW)\\)\n\n\nEdit Distance\n\\((i,j)\\)\nCompare chars\n\\(O(nm)\\)\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\nType\nTime\nSpace\n\n\n\n\nTop-down Memoization\n\\(O(\\text{\\#states})\\)\n\\(O(\\text{\\#states})\\)\n\n\nBottom-up Tabulation\n\\(O(\\text{\\#states})\\)\n\\(O(\\text{\\#states})\\)\n\n\n\nDynamic Programming is divide and conquer with memory, think recursively, compute once, reuse forever.\n\n\n\n95 Backtracking Pattern\nThe Backtracking Pattern explores all possible solutions by building them step by step and abandoning a path as soon as it becomes invalid.\nIt’s a systematic search strategy for problems where we need to generate combinations, permutations, or subsets, and prune impossible or suboptimal branches early.\n\nWhat Problem Are We Solving?\nWe face problems where:\n\nThe solution space is large, but structured.\nWe can detect invalid partial solutions early.\n\nExamples:\n\nN-Queens (place queens safely)\nSudoku (fill grid with constraints)\nSubset Sum (choose elements summing to target)\n\nBrute force explores everything blindly. Backtracking cuts off dead ends as soon as they appear.\n\n\nHow It Works (Plain Language)\nImagine exploring a maze:\n\nTake a step (make a choice).\nIf it leads to a valid partial solution, continue.\nIf it fails, backtrack, undo and try another path.\n\nEach level of recursion corresponds to a decision point.\n\n\nExample: N-Queens Problem\nWe need to place \\(n\\) queens on an \\(n \\times n\\) board so no two attack each other.\nAt each row, choose a column that is safe. If none works, backtrack to previous row.\n\n\nTiny Code (Python)\ndef solve_n_queens(n):\n    res, board = [], [-1]*n\n\n    def is_safe(row, col):\n        for r in range(row):\n            c = board[r]\n            if c == col or abs(c - col) == abs(r - row):\n                return False\n        return True\n\n    def backtrack(row=0):\n        if row == n:\n            res.append(board[:])\n            return\n        for col in range(n):\n            if is_safe(row, col):\n                board[row] = col\n                backtrack(row + 1)\n                board[row] = -1  # undo\n\n    backtrack()\n    return res\n\n\nWhy It Matters\nBacktracking is a universal solver for:\n\nCombinatorial search: subsets, permutations, partitions\nConstraint satisfaction: Sudoku, graph coloring, N-Queens\nOptimization with pruning (branch and bound builds on it)\n\nIt’s not just brute force, it’s guided exploration.\n\n\nA Gentle Proof (Why It Works)\nLet \\(S\\) be the total number of possible states. Backtracking prunes all invalid paths early, so actual visited nodes \\(\\le S\\).\nIf each state takes \\(O(1)\\) time to check and recurse, total complexity is proportional to the number of valid partial states, often far smaller than full enumeration.\n\n\nTry It Yourself\n\nSolve Subset Sum using backtracking.\nGenerate all permutations of [1,2,3].\nImplement Sudoku Solver (9×9 constraint satisfaction).\n\nTrace calls, each recursive call represents a partial decision.\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nProblem\nDecision\nConstraint\nOutput\n\n\n\n\nN-Queens\nChoose column\nNon-attacking queens\nPlacements\n\n\nSubset Sum\nInclude/Exclude\nSum ≤ target\nValid subsets\n\n\nSudoku\nFill cell\nRow/Col/Subgrid unique\nCompleted grid\n\n\n\n\n\nComplexity\n\n\n\nProblem\nTime\nSpace\n\n\n\n\nN-Queens\n\\(O(n!)\\) worst\n\\(O(n)\\)\n\n\nSubset Sum\n\\(O(2^n)\\)\n\\(O(n)\\)\n\n\nSudoku\nExponential\nGrid size\n\n\n\nBacktracking is the art of searching by undoing, try, test, and retreat until you find a valid path.\n\n\n\n96 Branch and Bound\nThe Branch and Bound pattern is an optimization framework that systematically explores the search space while pruning paths that cannot yield better solutions than the best one found so far.\nIt extends backtracking with bounds that let us skip unpromising branches early.\n\nWhat Problem Are We Solving?\nWe want to solve optimization problems where:\n\nThe search space is combinatorial (e.g., permutations, subsets).\nEach partial solution can be evaluated or bounded.\nWe seek the best solution under some cost function.\n\nExamples:\n\nKnapsack Problem: maximize value under capacity.\nTraveling Salesman Problem (TSP): find shortest tour.\nJob Scheduling: minimize total completion time.\n\nBrute-force search is exponential. Branch and Bound cuts branches that cannot improve the best known answer.\n\n\nHow It Works (Plain Language)\nThink of exploring a tree:\n\nBranch: expand possible choices.\nBound: compute a limit on achievable value from this branch.\nIf bound ≤ best found so far, prune (stop exploring).\nOtherwise, explore deeper.\n\nWe use:\n\nUpper bound: best possible value from this path.\nLower bound: best value found so far.\n\nPrune when upper bound ≤ lower bound.\n\n\nExample: 0/1 Knapsack\nGiven items with weights and values, choose subset with max value ≤ capacity.\nWe recursively include/exclude each item, but prune branches that cannot beat current best (e.g., exceeding weight or potential value too low).\n\n\nTiny Code (Python)\ndef knapsack_branch_bound(items, capacity):\n    best_value = 0\n\n    def bound(i, curr_w, curr_v):\n        # Simple bound: add remaining items greedily\n        if i &gt;= len(items):\n            return curr_v\n        w, v = curr_w, curr_v\n        for j in range(i, len(items)):\n            if w + items[j][0] &lt;= capacity:\n                w += items[j][0]\n                v += items[j][1]\n        return v\n\n    def dfs(i, curr_w, curr_v):\n        nonlocal best_value\n        if curr_w &gt; capacity:\n            return\n        if curr_v &gt; best_value:\n            best_value = curr_v\n        if i == len(items):\n            return\n        if bound(i, curr_w, curr_v) &lt;= best_value:\n            return\n        # Include item\n        dfs(i+1, curr_w + items[i][0], curr_v + items[i][1])\n        # Exclude item\n        dfs(i+1, curr_w, curr_v)\n\n    dfs(0, 0, 0)\n    return best_value\n\n\nWhy It Matters\nBranch and Bound:\n\nGeneralizes backtracking with mathematical pruning.\nTurns exponential search into practical algorithms.\nProvides exact solutions when heuristics might fail.\n\nUsed in:\n\nInteger programming\nRoute optimization\nScheduling and assignment problems\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(U(n)\\) be an upper bound of a subtree. If \\(U(n) \\le V^*\\) (best known value), no solution below can exceed \\(V^*\\).\nBy monotonic bounding, pruning preserves correctness — no optimal solution is ever discarded.\nThe algorithm is complete (explores all promising branches) and optimal (finds global best).\n\n\nTry It Yourself\n\nSolve 0/1 Knapsack with branch and bound.\nImplement TSP with cost matrix and prune by lower bounds.\nCompare nodes explored vs brute-force enumeration.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nItems (w,v)\nCapacity\nBest Value\nBranches Explored\n\n\n\n\n[(2,3),(3,4),(4,5)]\n5\n7\nReduced\n\n\n[(1,1),(2,2),(3,5),(4,6)]\n6\n8\nReduced\n\n\n\n\n\nComplexity\n\n\n\nProblem\nTime (Worst)\nTime (Typical)\nSpace\n\n\n\n\nKnapsack\n\\(O(2^n)\\)\nMuch less (pruning)\n\\(O(n)\\)\n\n\nTSP\n\\(O(n!)\\)\nPruned significantly\n\\(O(n)\\)\n\n\n\nBranch and Bound is search with insight, it trims the impossible and focuses only where the optimum can hide.\n\n\n\n97 Randomized Pattern\nThe Randomized Pattern introduces chance into algorithm design. Instead of following a fixed path, the algorithm makes random choices that, on average, lead to efficient performance or simplicity.\nRandomization can help break symmetry, avoid worst-case traps, and simplify complex logic.\n\nWhat Problem Are We Solving?\nWe want algorithms that:\n\nAvoid pathological worst-case inputs.\nSimplify decisions that are hard deterministically.\nAchieve good expected performance.\n\nCommon examples:\n\nRandomized QuickSort: pivot chosen at random.\nRandomized Search / Sampling: estimate quantities via random trials.\nMonte Carlo and Las Vegas Algorithms: trade accuracy for speed or vice versa.\n\n\n\nHow It Works (Plain Language)\nRandomization can appear in two forms:\n\nLas Vegas Algorithm\n\nAlways produces the correct result.\nRuntime is random (e.g., Randomized QuickSort).\n\nMonte Carlo Algorithm\n\nRuns in fixed time.\nMay have a small probability of error (e.g., primality tests).\n\n\nBy picking random paths or samples, we smooth out bad cases and often simplify logic.\n\n\nExample: Randomized QuickSort\nChoose a pivot randomly to avoid worst-case splits.\nAt each step:\n\nPick random pivot \\(p\\) from array.\nPartition array into smaller (&lt; p) and larger (&gt; p).\nRecursively sort halves.\n\nExpected runtime is \\(O(n \\log n)\\) even if input is adversarial.\n\n\nTiny Code (Python)\nimport random\n\ndef randomized_quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = random.choice(arr)\n    left = [x for x in arr if x &lt; pivot]\n    mid = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return randomized_quicksort(left) + mid + randomized_quicksort(right)\n\n\nWhy It Matters\nRandomized algorithms are:\n\nSimple: randomization replaces complex logic.\nEfficient: often faster in expectation.\nRobust: resistant to adversarial input.\n\nThey appear in:\n\nSorting, searching, and hashing.\nApproximation algorithms.\nCryptography and sampling.\nMachine learning (e.g., SGD, bagging).\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be expected time of Randomized QuickSort: \\[T(n) = n - 1 + \\frac{2}{n} \\sum_{k=0}^{n-1} T(k)\\]\nSolving yields \\(T(n) = O(n \\log n)\\). Random pivot ensures each element has equal probability to split array, making balanced partitions likely on average.\nExpected cost avoids \\(O(n^2)\\) worst-case of fixed-pivot QuickSort.\n\n\nTry It Yourself\n\nImplement Randomized QuickSort, run on sorted input.\nCompare average time to standard QuickSort.\nTry a random primality test (e.g., Miller–Rabin).\nUse random sampling to approximate \\(\\pi\\) via Monte Carlo.\n\n\n\nTest Cases\n\n\n\nInput\nExpected Result\nNotes\n\n\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nRandom pivot avoids worst-case\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nStill fast due to random splits\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nAlgorithm\nExpected Time\nWorst Time\nSpace\n\n\n\n\nRandomized QuickSort\n\\(O(n \\log n)\\)\n\\(O(n^2)\\) (rare)\n\\(O(\\log n)\\)\n\n\nRandomized Search\n\\(O(1)\\) expected\n\\(O(n)\\) worst\n\\(O(1)\\)\n\n\n\nRandomization turns rigid logic into flexible, average-case excellence, a practical ally in uncertain or adversarial worlds.\n\n\n\n98 Approximation Pattern\nThe Approximation Pattern is used when finding the exact solution is too expensive or impossible. Instead of striving for perfection, we design algorithms that produce results close enough to optimal, fast, predictable, and often guaranteed within a factor.\nThis pattern shines in NP-hard problems, where exact methods scale poorly.\n\nWhat Problem Are We Solving?\nSome problems, like Traveling Salesman, Vertex Cover, or Knapsack, have no known polynomial-time exact solutions. We need algorithms that give good-enough answers quickly, especially for large inputs.\nApproximation algorithms ensure:\n\nPredictable performance.\nMeasurable accuracy.\nPolynomial runtime.\n\n\n\nHow It Works (Plain Language)\nAn approximation algorithm outputs a solution within a known ratio of the optimal value:\nIf the optimal cost is \\(\\text{OPT}\\), and our algorithm returns \\(\\text{ALG}\\), then for a minimization problem:\n\\[\\frac{\\text{ALG}}{\\text{OPT}} \\le \\alpha\\]\nwhere \\(\\alpha\\) is the approximation factor (e.g., 2, 1.5, or \\((1 + \\epsilon)\\)).\n\n\nExample: Vertex Cover (2-Approximation)\nProblem: find smallest set of vertices touching all edges.\nAlgorithm:\n\nStart with an empty set \\(C\\).\nWhile edges remain:\n\nPick any uncovered edge \\((u, v)\\).\nAdd both \\(u\\) and \\(v\\) to \\(C\\).\nRemove all edges incident to \\(u\\) or \\(v\\).\n\nReturn \\(C\\).\n\nThis guarantees \\(|C| \\le 2 \\cdot |C^*|\\), where \\(C^*\\) is the optimal vertex cover.\n\n\nTiny Code (Python)\ndef vertex_cover(edges):\n    cover = set()\n    while edges:\n        (u, v) = edges.pop()\n        cover.add(u)\n        cover.add(v)\n        edges = [(x, y) for (x, y) in edges if x not in (u, v) and y not in (u, v)]\n    return cover\n\n\nWhy It Matters\nApproximation algorithms:\n\nProvide provable guarantees.\nScale to large problems.\nOffer predictable trade-offs between time and accuracy.\n\nWidely used in:\n\nCombinatorial optimization.\nScheduling, routing, resource allocation.\nAI planning, clustering, and compression.\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(C^*\\) be optimal cover. Every edge must be covered by \\(C^*\\). We select 2 vertices per edge, so:\n\\[|C| = 2 \\cdot \\text{(number of edges selected)} \\le 2 \\cdot |C^*|\\]\nThus, the approximation factor is 2.\n\n\nTry It Yourself\n\nImplement the 2-approx Vertex Cover algorithm.\nCompare result size with brute-force solution for small graphs.\nExplore \\((1 + \\epsilon)\\)-approximation using greedy selection.\nApply same idea to Set Cover or Knapsack.\n\n\n\nTest Cases\n\n\n\nGraph\nOptimal\nAlgorithm\nRatio\n\n\n\n\nTriangle\n2\n2\n1.0\n\n\nSquare\n2\n4\n2.0\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nAlgorithm\nTime\nSpace\nGuarantee\n\n\n\n\nVertex Cover (Greedy)\n\\(O(E)\\)\n\\(O(V)\\)\n2-Approx\n\n\nKnapsack (FPTAS)\n\\(O(n^3 / \\epsilon)\\)\n\\(O(n^2)\\)\n\\((1+\\epsilon)\\)\n\n\n\nApproximation is the art of being nearly perfect, swiftly, a pragmatic bridge between theory and the real world.\n\n\n\n99 Online Algorithm Pattern\nThe Online Algorithm Pattern is used when input arrives sequentially, and decisions must be made immediately without knowledge of future data. There’s no rewinding or re-optimizing later, you commit as you go.\nThis pattern models real-time decision-making, from caching to task scheduling and resource allocation.\n\nWhat Problem Are We Solving?\nIn many systems, data doesn’t come all at once. You must decide now, not after seeing the full picture.\nTypical scenarios:\n\nCache replacement (decide which page to evict next).\nTask assignment (jobs arrive in real time).\nDynamic routing (packets arrive continuously).\n\nOffline algorithms know everything upfront; online algorithms don’t, yet must perform competitively.\n\n\nHow It Works (Plain Language)\nAn online algorithm processes inputs one by one. Each step:\n\nReceive input item \\(x_t\\) at time \\(t\\).\nMake a decision \\(d_t\\) using current state only.\nCannot change \\(d_t\\) later.\n\nPerformance is measured by the competitive ratio:\n\\[\n\\text{Competitive Ratio} = \\max_{\\text{inputs}} \\frac{\\text{Cost}*{\\text{ALG}}}{\\text{Cost}*{\\text{OPT}}}\n\\]\nIf \\(\\text{ALG}\\)’s cost is at most \\(k\\) times optimal, the algorithm is k-competitive.\n\n\nExample: Paging / Cache Replacement\nYou have cache of size \\(k\\). Sequence of page requests arrives. If requested page is not in cache → page fault → load it (evict one if full).\nAlgorithms:\n\nFIFO (First In First Out): Evict oldest.\nLRU (Least Recently Used): Evict least recently accessed.\nRandom: Evict randomly.\n\nLRU is \\(k\\)-competitive, meaning it performs within factor \\(k\\) of optimal.\n\n\nTiny Code (Python)\ndef lru_cache(pages, capacity):\n    cache = []\n    faults = 0\n    for p in pages:\n        if p not in cache:\n            faults += 1\n            if len(cache) == capacity:\n                cache.pop(0)\n            cache.append(p)\n        else:\n            cache.remove(p)\n            cache.append(p)\n    return faults\n\n\nWhy It Matters\nOnline algorithms:\n\nReflect real-world constraints (no foresight).\nEnable adaptive systems in streaming, caching, and scheduling.\nProvide competitive guarantees even under worst-case input.\n\nUsed in:\n\nOperating systems (page replacement).\nNetworking (packet routing).\nFinance (online pricing, bidding).\nMachine learning (online gradient descent).\n\n\n\nA Gentle Proof (Why It Works)\nFor LRU Cache: Every cache miss means a unique page not seen in last \\(k\\) requests. The optimal offline algorithm (OPT) can avoid some faults but at most \\(k\\) times fewer. Thus:\n\\[\n\\text{Faults(LRU)} \\le k \\cdot \\text{Faults(OPT)}\n\\]\nSo LRU is k-competitive.\n\n\nTry It Yourself\n\nSimulate LRU, FIFO, Random cache on same request sequence.\nCount page faults.\nCompare with offline OPT (Belady’s Algorithm).\nExperiment with \\(k=2,3,4\\).\n\n\n\nTest Cases\n\n\n\nPages\nCache Size\nAlgorithm\nFaults\nRatio (vs OPT)\n\n\n\n\n[1,2,3,1,2,3]\n2\nLRU\n6\n3.0\n\n\n[1,2,3,4,1,2,3,4]\n3\nLRU\n8\n2.7\n\n\n\n\n\nComplexity\n\n\n\nAlgorithm\nTime\nSpace\nCompetitive Ratio\n\n\n\n\nFIFO\n\\(O(nk)\\)\n\\(O(k)\\)\n\\(k\\)\n\n\nLRU\n\\(O(nk)\\)\n\\(O(k)\\)\n\\(k\\)\n\n\nOPT (offline)\n\\(O(nk)\\)\n\\(O(k)\\)\n1\n\n\n\nOnline algorithms embrace uncertainty, they act wisely now, trusting analysis to prove they won’t regret it later.\n\n\n\n100 Hybrid Strategy Pattern\nThe Hybrid Strategy Pattern combines multiple algorithmic paradigms, such as divide and conquer, greedy, and dynamic programming, to balance their strengths and overcome individual weaknesses. Instead of sticking to one design philosophy, hybrid algorithms adapt to the structure of the problem and the size of the input.\n\nWhat Problem Are We Solving?\nNo single paradigm fits all problems. Some inputs are small and benefit from brute force; others require recursive structure; still others need heuristics.\nWe need a meta-strategy that blends paradigms, switching between them based on conditions like:\n\nInput size (e.g., small vs large)\nStructure (e.g., sorted vs unsorted)\nPrecision requirements (e.g., exact vs approximate)\n\nHybrid strategies offer practical performance beyond theoretical asymptotics.\n\n\nHow It Works (Plain Language)\nA hybrid algorithm uses decision logic to pick the best method for each situation.\nCommon patterns:\n\nSmall-case base switch: Use brute force when \\(n\\) is small (e.g., Insertion Sort inside QuickSort).\nStage combination: Use one algorithm for setup, another for refinement (e.g., Greedy for initial solution, DP for optimization).\nConditional strategy: Choose algorithm based on data distribution (e.g., QuickSort vs HeapSort).\n\n\n\nExample: Introsort\nIntrosort starts like QuickSort for average speed, but if recursion depth grows too large (bad pivot splits), it switches to HeapSort to guarantee \\(O(n \\log n)\\) worst-case time.\nSteps:\n\nPartition using QuickSort.\nTrack recursion depth.\nIf depth &gt; threshold (\\(2 \\log n\\)), switch to HeapSort.\n\nThis ensures best of both worlds: average speed + worst-case safety.\n\n\nTiny Code (Python)\ndef introsort(arr, depth_limit):\n    if len(arr) &lt;= 1:\n        return arr\n    if depth_limit == 0:\n        return heapsort(arr)\n    pivot = arr[len(arr)//2]\n    left = [x for x in arr if x &lt; pivot]\n    mid = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return introsort(left, depth_limit - 1) + mid + introsort(right, depth_limit - 1)\n(Uses heapsort when depth limit is reached)\n\n\nWhy It Matters\nHybrid strategies give real-world efficiency, predictable performance, and robust fallback behavior. They mirror how expert developers build systems, not one-size-fits-all, but layered and conditional.\nCommon hybrids:\n\nTimsort = MergeSort + InsertionSort\nIntrosort = QuickSort + HeapSort\nBranch-and-Bound + Greedy = Search with pruning and heuristics\nNeural + Symbolic = Learning + Logical reasoning\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(A_1, A_2, \\ldots, A_k\\) be candidate algorithms with cost functions \\(T_i(n)\\). Hybrid strategy \\(H\\) chooses \\(A_i\\) when condition \\(C_i(n)\\) holds.\nIf decision logic ensures \\[T_H(n) = \\min_i { T_i(n) \\mid C_i(n) }\\] then \\(H\\) performs at least as well as the best applicable algorithm.\nThus \\(T_H(n) = O(\\min_i T_i(n))\\).\n\n\nTry It Yourself\n\nImplement QuickSort + InsertionSort hybrid.\nSet threshold \\(n_0 = 10\\) for switching.\nCompare performance vs pure QuickSort.\nExperiment with different thresholds.\n\n\n\nTest Cases\n\n\n\nInput Size\nAlgorithm\nTime\nNotes\n\n\n\n\n10\nInsertion Sort\nFastest\nSimplicity wins\n\n\n1000\nQuickSort\nOptimal\nLow overhead\n\n\n1e6\nIntrosort\nStable\nNo worst-case blowup\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\n\nComponent\nBest\nAverage\nWorst\nSpace\n\n\n\n\nQuickSort\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\\(O(n^2)\\)\n\\(O(\\log n)\\)\n\n\nHeapSort\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\\(O(1)\\)\n\n\nIntrosort\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\\(O(\\log n)\\)\n\n\n\nA hybrid strategy is not just an algorithmic trick, it’s a mindset: combine precision, adaptability, and pragmatism to build algorithms that thrive in the wild.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 1. Foundations of Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html",
    "href": "books/en-us/list-2.html",
    "title": "Chapter 2. Sorting and searching",
    "section": "",
    "text": "Section 11. Elementary sorting",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-11.-elementary-sorting",
    "href": "books/en-us/list-2.html#section-11.-elementary-sorting",
    "title": "Chapter 2. Sorting and searching",
    "section": "",
    "text": "101 Bubble Sort\nBubble Sort is like washing dishes one by one, you keep moving the biggest plate to the bottom until everything is clean and sorted. It is simple, visual, and perfect for building your sorting intuition before diving into more advanced methods.\n\nWhat Problem Are We Solving?\nWe want to arrange a list of elements in order (ascending or descending) by repeatedly comparing and swapping adjacent items that are out of order.\nFormally: Given an array A[0…n-1], repeat passes until no swaps occur. Each pass bubbles up the largest remaining element to its final position.\n\n\nExample\n\n\n\nStep\nArray State\nDescription\n\n\n\n\n0\n[5, 3, 4, 1]\nInitial array\n\n\n1\n[3, 4, 1, 5]\n5 bubbled to the end\n\n\n2\n[3, 1, 4, 5]\n4 bubbled to position 3\n\n\n3\n[1, 3, 4, 5]\nArray fully sorted\n\n\n\n\n\nHow Does It Work (Plain Language)?\nImagine bubbles rising to the surface, the biggest one reaches the top first. In Bubble Sort, each sweep through the list compares neighboring pairs, swapping them if they are in the wrong order. After each full pass, one more element settles into place.\nWe repeat until a pass finishes with no swaps, meaning the array is sorted.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\nResulting Array\n\n\n\n\n1\nCompare A[0] and A[1]\nSwap if needed\n\n\n2\nCompare A[1] and A[2]\nSwap if needed\n\n\n3\nContinue through A[n-2]\nRepeat comparisons\n\n\n4\nRepeat passes until sorted\nEarly stop if sorted\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid bubble_sort(int a[], int n) {\n    bool swapped;\n    for (int pass = 0; pass &lt; n - 1; pass++) {\n        swapped = false;\n        for (int i = 0; i &lt; n - pass - 1; i++) {\n            if (a[i] &gt; a[i + 1]) {\n                int temp = a[i];\n                a[i] = a[i + 1];\n                a[i + 1] = temp;\n                swapped = true;\n            }\n        }\n        if (!swapped) break; // early exit if already sorted\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    bubble_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef bubble_sort(a):\n    n = len(a)\n    for pass_num in range(n - 1):\n        swapped = False\n        for i in range(n - pass_num - 1):\n            if a[i] &gt; a[i + 1]:\n                a[i], a[i + 1] = a[i + 1], a[i]\n                swapped = True\n        if not swapped:\n            break\n\narr = [5, 3, 4, 1, 2]\nbubble_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nTeaches comparison-based sorting through intuition\nBuilds understanding of adjacent swaps and pass-based progress\nIntroduces stability (equal elements keep their relative order)\nSets the stage for improved versions (Improved Bubble Sort, Cocktail Shaker Sort, Comb Sort)\n\n\n\nA Gentle Proof (Why It Works)\nAfter the first pass, the largest element moves to the last position. After the second pass, the second largest is in position n-2.\nSo after k passes, the last k elements are sorted.\nIf we track comparisons: 1st pass: (n−1) comparisons 2nd pass: (n−2) comparisons … (n−1)th pass: 1 comparison\n\n\n\nPass\nComparisons\nElements Sorted at End\n\n\n\n\n1\nn−1\nLargest element\n\n\n2\nn−2\nNext largest\n\n\n…\n…\n…\n\n\nn−1\n1\nFully sorted array\n\n\n\nTotal comparisons = (n−1) + (n−2) + … + 1 = n(n−1)/2\nSo time = O(n²) in the worst case. If already sorted, early exit makes it O(n).\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [3, 2, 1] step by step\n\n\n2\nCount how many swaps occur\n\n\n3\nAdd a flag to detect early termination\n\n\n4\nCompare with Insertion Sort on the same data\n\n\n5\nModify to sort descending\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nPasses\nSwaps\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n3\n3\n\n\n[1, 2, 3]\n[1, 2, 3]\n1\n0\n\n\n[5, 1, 4, 2, 8]\n[1, 2, 4, 5, 8]\n4\n5\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1) (in-place)\n\n\nStable\nYes\n\n\nAdaptive\nYes (stops early if sorted)\n\n\n\nBubble Sort is your first step into the sorting world, simple enough to code by hand, visual enough to animate, and powerful enough to spark intuition for more advanced sorts.\n\n\n\n102 Improved Bubble Sort\nImproved Bubble Sort builds on the basic version by recognizing that once part of the array is sorted, there’s no need to revisit it. It introduces small optimizations like early termination and tracking the last swap position to reduce unnecessary comparisons.\n\nWhat Problem Are We Solving??\nBasic Bubble Sort keeps scanning the whole array every pass, even when the tail is already sorted. Improved Bubble Sort fixes this by remembering where the last swap happened. Elements beyond that index are already in place, so the next pass can stop earlier.\nThis optimization is especially effective for arrays that are nearly sorted.\n\n\nExample\n\n\n\nStep\nArray State\nLast Swap Index\nRange Checked\n\n\n\n\n0\n[5, 3, 4, 1, 2]\n-\n0 to 4\n\n\n1\n[3, 4, 1, 2, 5]\n3\n0 to 3\n\n\n2\n[3, 1, 2, 4, 5]\n2\n0 to 2\n\n\n3\n[1, 2, 3, 4, 5]\n1\n0 to 1\n\n\n4\n[1, 2, 3, 4, 5]\n0\nStop early\n\n\n\n\n\nHow Does It Work (Plain Language)?\nWe improve efficiency by narrowing each pass to only the unsorted part. We also stop early when no swaps occur, signaling the array is already sorted.\nStep by step:\n\nTrack index of the last swap in each pass\nNext pass ends at that index\nStop when no swaps occur (fully sorted)\n\nThis reduces unnecessary comparisons in nearly sorted arrays.\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid improved_bubble_sort(int a[], int n) {\n    int new_n;\n    while (n &gt; 1) {\n        new_n = 0;\n        for (int i = 1; i &lt; n; i++) {\n            if (a[i - 1] &gt; a[i]) {\n                int temp = a[i - 1];\n                a[i - 1] = a[i];\n                a[i] = temp;\n                new_n = i;\n            }\n        }\n        n = new_n;\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    improved_bubble_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef improved_bubble_sort(a):\n    n = len(a)\n    while n &gt; 1:\n        new_n = 0\n        for i in range(1, n):\n            if a[i - 1] &gt; a[i]:\n                a[i - 1], a[i] = a[i], a[i - 1]\n                new_n = i\n        n = new_n\n\narr = [5, 3, 4, 1, 2]\nimproved_bubble_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nReduces redundant comparisons\nAutomatically adapts to partially sorted data\nStops as soon as the array is sorted\nRetains stability and simplicity\n\n\n\nA Gentle Proof (Why It Works)\nIf the last swap occurs at index k, all elements after k are already in order. Next pass only needs to scan up to k. If no swaps occur (k = 0), the array is sorted.\n\n\n\nPass\nComparisons\nLast Swap\nRange Next Pass\n\n\n\n\n1\nn−1\nk₁\n0..k₁\n\n\n2\nk₁−1\nk₂\n0..k₂\n\n\n…\n…\n…\n…\n\n\n\nIn the best case (already sorted), only one pass occurs: O(n) Worst case remains O(n²)\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [1, 2, 3, 4, 5] and observe early stop\n\n\n2\nSort [5, 4, 3, 2, 1] and track last swap index\n\n\n3\nModify to print last swap index each pass\n\n\n4\nCompare with standard Bubble Sort pass count\n\n\n5\nTry arrays with repeated values to verify stability\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nPasses\nImprovement\n\n\n\n\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n1\nEarly stop\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n4\nFewer checks each pass\n\n\n[2, 1, 3, 4, 5]\n[1, 2, 3, 4, 5]\n1\nDetects sorted tail\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nImproved Bubble Sort shows how a small observation can make a classic algorithm smarter. By tracking the last swap, it skips already-sorted tails and gives a glimpse of how adaptive sorting works in practice.\n\n\n\n103 Cocktail Shaker Sort\nCocktail Shaker Sort, also known as Bidirectional Bubble Sort, improves on Bubble Sort by sorting in both directions during each pass. It moves the largest element to the end and the smallest to the beginning, reducing the number of passes required.\n\nWhat Problem Are We Solving??\nStandard Bubble Sort only bubbles up in one direction, pushing the largest element to the end each pass. If small elements start near the end, they take many passes to reach their position.\nCocktail Shaker Sort fixes this by sweeping back and forth, bubbling both ends at once.\n\n\nExample\n\n\n\nStep\nDirection\nArray State\nDescription\n\n\n\n\n0\n–\n[5, 3, 4, 1, 2]\nInitial array\n\n\n1\nLeft → Right\n[3, 4, 1, 2, 5]\n5 bubbled to end\n\n\n2\nRight → Left\n[1, 3, 4, 2, 5]\n1 bubbled to start\n\n\n3\nLeft → Right\n[1, 3, 2, 4, 5]\n4 bubbled to position 4\n\n\n4\nRight → Left\n[1, 2, 3, 4, 5]\n2 bubbled to position 2\n\n\n\nSorted after 4 directional passes.\n\n\nHow Does It Work (Plain Language)?\nCocktail Shaker Sort is like stirring from both sides of the array. Each forward pass moves the largest unsorted element to the end. Each backward pass moves the smallest unsorted element to the start.\nThe unsorted region shrinks from both ends with each full cycle.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nSweep left to right, bubble largest to end\n\n\n\n2\nSweep right to left, bubble smallest to start\n\n\n\n3\nNarrow bounds, repeat until sorted\n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid cocktail_shaker_sort(int a[], int n) {\n    bool swapped = true;\n    int start = 0, end = n - 1;\n\n    while (swapped) {\n        swapped = false;\n\n        // Forward pass\n        for (int i = start; i &lt; end; i++) {\n            if (a[i] &gt; a[i + 1]) {\n                int temp = a[i];\n                a[i] = a[i + 1];\n                a[i + 1] = temp;\n                swapped = true;\n            }\n        }\n        if (!swapped) break;\n        swapped = false;\n        end--;\n\n        // Backward pass\n        for (int i = end - 1; i &gt;= start; i--) {\n            if (a[i] &gt; a[i + 1]) {\n                int temp = a[i];\n                a[i] = a[i + 1];\n                a[i + 1] = temp;\n                swapped = true;\n            }\n        }\n        start++;\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    cocktail_shaker_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef cocktail_shaker_sort(a):\n    n = len(a)\n    start, end = 0, n - 1\n    swapped = True\n\n    while swapped:\n        swapped = False\n        for i in range(start, end):\n            if a[i] &gt; a[i + 1]:\n                a[i], a[i + 1] = a[i + 1], a[i]\n                swapped = True\n        if not swapped:\n            break\n        swapped = False\n        end -= 1\n        for i in range(end - 1, start - 1, -1):\n            if a[i] &gt; a[i + 1]:\n                a[i], a[i + 1] = a[i + 1], a[i]\n                swapped = True\n        start += 1\n\narr = [5, 3, 4, 1, 2]\ncocktail_shaker_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nSorts in both directions, reducing unnecessary passes\nPerforms better than Bubble Sort on many practical inputs\nStable and easy to visualize\nDemonstrates bidirectional improvement, a foundation for adaptive sorting\n\n\n\nA Gentle Proof (Why It Works)\nEach forward pass moves the maximum element of the unsorted range to the end. Each backward pass moves the minimum element of the unsorted range to the start. Thus, the unsorted range shrinks from both sides, guaranteeing progress each cycle.\n\n\n\nCycle\nForward Pass\nBackward Pass\nSorted Range\n\n\n\n\n1\nLargest to end\nSmallest to start\n[0], [n-1]\n\n\n2\nNext largest\nNext smallest\n[0,1], [n-2,n-1]\n\n\n…\n…\n…\n…\n\n\n\nWorst case still O(n²), best case O(n) if already sorted.\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [5, 3, 4, 1, 2] and track forward/backward passes\n\n\n2\nVisualize the shrinking unsorted range\n\n\n3\nCompare with standard Bubble Sort on reverse array\n\n\n4\nModify code to print array after each pass\n\n\n5\nTest stability with duplicate values\n\n\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nInput\nOutput\nPasses\nNotes\n\n\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n4\nFewer passes than bubble sort\n\n\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n1\nEarly termination\n\n\n[2, 1, 3, 5, 4]\n[1, 2, 3, 4, 5]\n2\nMoves smallest quickly\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nCocktail Shaker Sort takes the simplicity of Bubble Sort and doubles its efficiency for certain inputs. By sorting in both directions, it highlights the power of symmetry and small algorithmic tweaks.\n\n\n\n104 Selection Sort\nSelection Sort is like organizing a deck of cards by repeatedly picking the smallest card and placing it in order. It is simple, predictable, and useful for understanding how selection-based sorting works.\n\nWhat Problem Are We Solving??\nWe want to sort an array by repeatedly selecting the smallest (or largest) element from the unsorted part and swapping it into the correct position.\nSelection Sort separates the array into two parts:\n\nA sorted prefix (built one element at a time)\nAn unsorted suffix (from which we select the next minimum)\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStep\nArray State\nAction\nSorted Part\nUnsorted Part\n\n\n\n\n0\n[5, 3, 4, 1, 2]\nStart\n[]\n[5,3,4,1,2]\n\n\n1\n[1, 3, 4, 5, 2]\nPlace 1 at index 0\n[1]\n[3,4,5,2]\n\n\n2\n[1, 2, 4, 5, 3]\nPlace 2 at index 1\n[1,2]\n[4,5,3]\n\n\n3\n[1, 2, 3, 5, 4]\nPlace 3 at index 2\n[1,2,3]\n[5,4]\n\n\n4\n[1, 2, 3, 4, 5]\nPlace 4 at index 3\n[1,2,3,4]\n[5]\n\n\n5\n[1, 2, 3, 4, 5]\nDone\n[1,2,3,4,5]\n[]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nSelection Sort looks through the unsorted portion, finds the smallest element, and moves it to the front. It does not care about intermediate order until each selection is done.\nEach pass fixes one position permanently.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\nEffect\n\n\n\n\n1\nFind smallest in unsorted part\nMove it to front\n\n\n2\nRepeat for next unsorted index\nGrow sorted prefix\n\n\n3\nStop when entire array sorted\n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid selection_sort(int a[], int n) {\n    for (int i = 0; i &lt; n - 1; i++) {\n        int min_idx = i;\n        for (int j = i + 1; j &lt; n; j++) {\n            if (a[j] &lt; a[min_idx]) {\n                min_idx = j;\n            }\n        }\n        int temp = a[i];\n        a[i] = a[min_idx];\n        a[min_idx] = temp;\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    selection_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef selection_sort(a):\n    n = len(a)\n    for i in range(n - 1):\n        min_idx = i\n        for j in range(i + 1, n):\n            if a[j] &lt; a[min_idx]:\n                min_idx = j\n        a[i], a[min_idx] = a[min_idx], a[i]\n\narr = [5, 3, 4, 1, 2]\nselection_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nSimple, deterministic sorting algorithm\nDemonstrates selection rather than swapping neighbors\nGood for small lists and teaching purposes\nUseful when minimizing number of swaps matters\n\n\n\nA Gentle Proof (Why It Works)\nAt each iteration, the smallest remaining element is placed at its correct position. Once placed, it never moves again.\nThe algorithm performs n−1 selections and at most n−1 swaps. Each selection requires scanning the unsorted part: O(n) comparisons.\n\n\n\nPass\nSearch Range\nComparisons\nSwap\n\n\n\n\n1\nn elements\nn−1\n1\n\n\n2\nn−1 elements\nn−2\n1\n\n\n…\n…\n…\n…\n\n\nn−1\n2 elements\n1\n1\n\n\n\nTotal comparisons = n(n−1)/2 = O(n²)\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nTrace sorting of [5, 3, 4, 1, 2] step by step\n\n\n2\nCount total swaps and comparisons\n\n\n3\nModify to find maximum each pass (descending order)\n\n\n4\nAdd print statements to see progress\n\n\n5\nCompare with Bubble Sort efficiency\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nPasses\nSwaps\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n2\n2\n\n\n[1, 2, 3]\n[1, 2, 3]\n2\n0\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n4\n4\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo (swap may break order)\n\n\nAdaptive\nNo\n\n\n\nSelection Sort is a calm, methodical sorter. It does not adapt, but it does not waste swaps either. It is the simplest demonstration of the idea: find the smallest, place it, repeat.\n\n\n\n105 Double Selection Sort\nDouble Selection Sort is a refined version of Selection Sort. Instead of finding just the smallest element each pass, it finds both the smallest and the largest, placing them at the beginning and end simultaneously. This halves the number of passes needed.\n\nWhat Problem Are We Solving??\nStandard Selection Sort finds one element per pass. Double Selection Sort improves efficiency by selecting two elements per pass, one from each end, reducing total iterations by about half.\nIt is useful when both extremes can be found in a single scan, improving constant factors while keeping overall simplicity.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nStep\nArray State\nMin\nMax\nAction\nSorted Part\n\n\n\n\n0\n[5, 3, 4, 1, 2]\n1\n5\nSwap 1 → front, 5 → back\n[1, …, 5]\n\n\n1\n[1, 3, 4, 2, 5]\n2\n4\nSwap 2 → index 1, 4 → index 3\n[1,2, …,4,5]\n\n\n2\n[1, 2, 3, 4, 5]\n3\n3\nMiddle element sorted\n[1,2,3,4,5]\n\n\n\nSorted in 3 passes instead of 5.\n\n\nHow Does It Work (Plain Language)?\nDouble Selection Sort narrows the unsorted range from both sides. Each pass:\n\nScans the unsorted section once.\nFinds both the smallest and largest elements.\nSwaps them to their correct positions at the front and back.\n\nThen it shrinks the bounds and repeats.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\n\nStep\nAction\nEffect\n\n\n\n\n1\nFind smallest and largest in unsorted\nMove smallest left, largest right\n\n\n2\nShrink unsorted range\nRepeat search\n\n\n3\nStop when range collapses\nArray sorted\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid double_selection_sort(int a[], int n) {\n    int left = 0, right = n - 1;\n\n    while (left &lt; right) {\n        int min_idx = left, max_idx = left;\n\n        for (int i = left; i &lt;= right; i++) {\n            if (a[i] &lt; a[min_idx]) min_idx = i;\n            if (a[i] &gt; a[max_idx]) max_idx = i;\n        }\n\n        // Move smallest to front\n        int temp = a[left];\n        a[left] = a[min_idx];\n        a[min_idx] = temp;\n\n        // If max element was swapped into min_idx\n        if (max_idx == left) max_idx = min_idx;\n\n        // Move largest to back\n        temp = a[right];\n        a[right] = a[max_idx];\n        a[max_idx] = temp;\n\n        left++;\n        right--;\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    double_selection_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef double_selection_sort(a):\n    left, right = 0, len(a) - 1\n    while left &lt; right:\n        min_idx, max_idx = left, left\n        for i in range(left, right + 1):\n            if a[i] &lt; a[min_idx]:\n                min_idx = i\n            if a[i] &gt; a[max_idx]:\n                max_idx = i\n\n        a[left], a[min_idx] = a[min_idx], a[left]\n        if max_idx == left:\n            max_idx = min_idx\n        a[right], a[max_idx] = a[max_idx], a[right]\n\n        left += 1\n        right -= 1\n\narr = [5, 3, 4, 1, 2]\ndouble_selection_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nImproves Selection Sort by reducing passes\nSelects two extremes in one scan\nFewer total swaps and comparisons\nDemonstrates bidirectional selection\n\n\n\nA Gentle Proof (Why It Works)\nEach pass moves two elements to their correct final positions. Thus, after k passes, the first k and last k positions are sorted. The unsorted range shrinks by 2 each pass.\n\n\n\nPass\nRange Checked\nElements Fixed\nRemaining Unsorted\n\n\n\n\n1\n[0..n−1]\n2\nn−2\n\n\n2\n[1..n−2]\n2\nn−4\n\n\n…\n…\n…\n…\n\n\n\nTotal passes = n/2, each O(n) scan ⇒ O(n²) overall.\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [5, 3, 4, 1, 2] manually\n\n\n2\nCount passes and swaps\n\n\n3\nPrint range boundaries each pass\n\n\n4\nCompare to Selection Sort passes\n\n\n5\nModify for descending order\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nPasses\nSwaps\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n2\n2\n\n\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n2\n0\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n3\n6\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nDouble Selection Sort keeps Selection Sort’s simplicity but doubles its reach. By grabbing both ends each pass, it highlights how symmetry can bring efficiency without new data structures.\n\n\n\n106 Insertion Sort\nInsertion Sort builds the sorted array one element at a time, like sorting playing cards in your hand. It takes each new element and inserts it into the correct position among those already sorted.\n\nWhat Problem Are We Solving??\nWe want a simple, stable way to sort elements by inserting each into place within the growing sorted section. This works especially well for small arrays or nearly sorted data.\nInsertion Sort splits the array into two parts:\n\nSorted prefix: elements that are already in order\nUnsorted suffix: remaining elements yet to be inserted\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStep\nArray State\nElement Inserted\nAction\nSorted Prefix\n\n\n\n\n0\n[5, 3, 4, 1, 2]\n-\nStart with first element sorted\n[5]\n\n\n1\n[3, 5, 4, 1, 2]\n3\nInsert 3 before 5\n[3, 5]\n\n\n2\n[3, 4, 5, 1, 2]\n4\nInsert 4 before 5\n[3, 4, 5]\n\n\n3\n[1, 3, 4, 5, 2]\n1\nInsert 1 at front\n[1, 3, 4, 5]\n\n\n4\n[1, 2, 3, 4, 5]\n2\nInsert 2 after 1\n[1, 2, 3, 4, 5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nImagine picking cards one by one and placing each into the correct spot among those already held. Insertion Sort repeats this logic for arrays:\n\nStart with the first element (already sorted)\nTake the next element\nCompare backward through the sorted section\nShift elements to make space and insert it\n\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nTake next unsorted element\n\n\n\n2\nMove through sorted part to find position\n\n\n\n3\nShift larger elements right\n\n\n\n4\nInsert element in correct position\n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid insertion_sort(int a[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = a[i];\n        int j = i - 1;\n        while (j &gt;= 0 && a[j] &gt; key) {\n            a[j + 1] = a[j];\n            j--;\n        }\n        a[j + 1] = key;\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    insertion_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef insertion_sort(a):\n    for i in range(1, len(a)):\n        key = a[i]\n        j = i - 1\n        while j &gt;= 0 and a[j] &gt; key:\n            a[j + 1] = a[j]\n            j -= 1\n        a[j + 1] = key\n\narr = [5, 3, 4, 1, 2]\ninsertion_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nSimple, intuitive, and stable\nWorks well for small or nearly sorted arrays\nCommonly used as a subroutine in advanced algorithms (like Timsort)\nDemonstrates concept of incremental insertion\n\n\n\nA Gentle Proof (Why It Works)\nAt step i, the first i elements are sorted. Inserting element a[i] keeps the prefix sorted. Each insertion shifts elements greater than key to the right, ensuring correct position.\n\n\n\nPass\nSorted Portion\nComparisons (Worst)\nShifts (Worst)\n\n\n\n\n1\n[a₀,a₁]\n1\n1\n\n\n2\n[a₀,a₁,a₂]\n2\n2\n\n\n…\n…\n…\n…\n\n\nn-1\n[a₀…aₙ₋₁]\nn-1\nn-1\n\n\n\nTotal ≈ (n²)/2 operations in the worst case.\nIf already sorted, only one comparison per element → O(n).\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [5, 3, 4, 1, 2] step by step\n\n\n2\nCount shifts and comparisons\n\n\n3\nModify to sort descending\n\n\n4\nCompare runtime with Bubble Sort\n\n\n5\nInsert print statements to trace insertions\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nPasses\nSwaps/Shifts\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n2\n3\n\n\n[1, 2, 3]\n[1, 2, 3]\n2\n0\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n4\n8\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nInsertion Sort captures the logic of careful, incremental organization. It is slow for large random lists, but elegant, stable, and highly efficient when the data is already close to sorted.\n\n\n\n107 Binary Insertion Sort\nBinary Insertion Sort improves on traditional Insertion Sort by using binary search to find the correct insertion point instead of linear scanning. This reduces the number of comparisons from linear to logarithmic per insertion, while keeping the same stable, adaptive behavior.\n\nWhat Problem Are We Solving??\nStandard Insertion Sort searches linearly through the sorted part to find where to insert the new element. If the sorted prefix is long, this costs O(n) comparisons per element.\nBinary Insertion Sort replaces that with binary search, which finds the position in O(log n) time, while still performing O(n) shifts.\nThis makes it a good choice when comparisons are expensive but shifting is cheap.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStep\nSorted Portion\nElement to Insert\nInsertion Index (Binary Search)\nResulting Array\n\n\n\n\n0\n[5]\n3\n0\n[3, 5, 4, 1, 2]\n\n\n1\n[3, 5]\n4\n1\n[3, 4, 5, 1, 2]\n\n\n2\n[3, 4, 5]\n1\n0\n[1, 3, 4, 5, 2]\n\n\n3\n[1, 3, 4, 5]\n2\n1\n[1, 2, 3, 4, 5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nJust like Insertion Sort, we build a sorted prefix one element at a time. But instead of scanning backwards linearly, we use binary search to locate the correct position to insert the next element.\nWe still need to shift larger elements to the right, but we now know exactly where to stop.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\nEffect\n\n\n\n\n1\nPerform binary search in sorted prefix\nFind insertion point\n\n\n2\nShift larger elements right\nCreate space\n\n\n3\nInsert element at found index\nMaintain order\n\n\n4\nRepeat until sorted\nFully sorted array\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search(int a[], int item, int low, int high) {\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        if (item == a[mid]) return mid + 1;\n        else if (item &gt; a[mid]) low = mid + 1;\n        else high = mid - 1;\n    }\n    return low;\n}\n\nvoid binary_insertion_sort(int a[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = a[i];\n        int j = i - 1;\n        int pos = binary_search(a, key, 0, j);\n\n        while (j &gt;= pos) {\n            a[j + 1] = a[j];\n            j--;\n        }\n        a[pos] = key;\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    binary_insertion_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef binary_search(a, item, low, high):\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if item == a[mid]:\n            return mid + 1\n        elif item &gt; a[mid]:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return low\n\ndef binary_insertion_sort(a):\n    for i in range(1, len(a)):\n        key = a[i]\n        pos = binary_search(a, key, 0, i - 1)\n        a[pos + 1 : i + 1] = a[pos : i]\n        a[pos] = key\n\narr = [5, 3, 4, 1, 2]\nbinary_insertion_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nFewer comparisons than standard Insertion Sort\nRetains stability and adaptiveness\nGreat when comparisons dominate runtime (e.g., complex objects)\nDemonstrates combining search and insertion ideas\n\n\n\nA Gentle Proof (Why It Works)\nBinary search always finds the correct index in O(log i) comparisons for the i-th element. Shifting elements still takes O(i) time. So total cost:\n\\[\nT(n) = \\sum_{i=1}^{n-1} (\\log i + i) = O(n^2)\n\\]\nbut with fewer comparisons than standard Insertion Sort.\n\n\n\nStep\nComparisons\nShifts\nTotal Cost\n\n\n\n\n1\nlog₂1 = 0\n1\n1\n\n\n2\nlog₂2 = 1\n2\n3\n\n\n3\nlog₂3 ≈ 2\n3\n5\n\n\n…\n…\n…\n…\n\n\n\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [5, 3, 4, 1, 2] step by step\n\n\n2\nPrint insertion index each pass\n\n\n3\nCompare comparisons vs normal Insertion Sort\n\n\n4\nModify to sort descending\n\n\n5\nTry with already sorted list\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nComparisons\nShifts\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n~3\n3\n\n\n[1, 2, 3]\n[1, 2, 3]\n~2\n0\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n~7\n8\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n log n) comparisons, O(n) shifts\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nBinary Insertion Sort is a thoughtful balance, smarter searches, same simple structure. It reminds us that even small changes (like using binary search) can bring real efficiency when precision matters.\n\n\n\n108 Gnome Sort\nGnome Sort is a simple sorting algorithm that works by swapping adjacent elements, similar to Bubble Sort, but with a twist, it moves backward whenever a swap is made. Imagine a gnome tidying flower pots: each time it finds two out of order, it swaps them and steps back to recheck the previous pair.\n\nWhat Problem Are We Solving??\nWe want a simple, intuitive, and in-place sorting method that uses local swaps to restore order. Gnome Sort is particularly easy to implement and works like an insertion sort with adjacent swaps instead of shifting elements.\nIt’s not the fastest, but it’s charmingly simple, perfect for understanding local correction logic.\n\n\nExample\n\n\n\n\n\n\n\n\n\nStep\nPosition\nArray State\nAction\n\n\n\n\n0\n1\n[5, 3, 4, 1, 2]\nCompare 5 &gt; 3 → Swap, move back\n\n\n1\n0\n[3, 5, 4, 1, 2]\nAt start → move forward\n\n\n2\n1\n[3, 5, 4, 1, 2]\nCompare 5 &gt; 4 → Swap, move back\n\n\n3\n0\n[3, 4, 5, 1, 2]\nAt start → move forward\n\n\n4\n1\n[3, 4, 5, 1, 2]\nCompare 4 &lt; 5 → OK → move forward\n\n\n5\n3\n[3, 4, 5, 1, 2]\nCompare 5 &gt; 1 → Swap, move back\n\n\n6\n2\n[3, 4, 1, 5, 2]\nCompare 4 &gt; 1 → Swap, move back\n\n\n7\n1\n[3, 1, 4, 5, 2]\nCompare 3 &gt; 1 → Swap, move back\n\n\n8\n0\n[1, 3, 4, 5, 2]\nAt start → move forward\n\n\n9\n…\n…\nContinue until sorted [1,2,3,4,5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nThe algorithm “walks” through the list:\n\nIf the current element is greater or equal to the previous one, move forward.\nIf not, swap them and move one step back.\nRepeat until the end is reached.\n\nIf you reach the start of the array, step forward.\nIt’s like Insertion Sort, but instead of shifting, it walks and swaps.\n\n\nStep-by-Step Process\n\n\n\nStep\nCondition\nAction\n\n\n\n\nIf A[i] ≥ A[i−1]\nMove forward (i++)\n\n\n\nIf A[i] &lt; A[i−1]\nSwap, move backward (i−−)\n\n\n\nIf i == 0\nMove forward (i++)\n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid gnome_sort(int a[], int n) {\n    int i = 1;\n    while (i &lt; n) {\n        if (i == 0 || a[i] &gt;= a[i - 1]) {\n            i++;\n        } else {\n            int temp = a[i];\n            a[i] = a[i - 1];\n            a[i - 1] = temp;\n            i--;\n        }\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    gnome_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef gnome_sort(a):\n    i = 1\n    n = len(a)\n    while i &lt; n:\n        if i == 0 or a[i] &gt;= a[i - 1]:\n            i += 1\n        else:\n            a[i], a[i - 1] = a[i - 1], a[i]\n            i -= 1\n\narr = [5, 3, 4, 1, 2]\ngnome_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nDemonstrates sorting through local correction\nVisually intuitive (good for animation)\nRequires no additional memory\nStable and adaptive on partially sorted data\n\n\n\nA Gentle Proof (Why It Works)\nEach time we swap out-of-order elements, we step back to verify order with the previous one. Thus, by the time we move forward again, all prior elements are guaranteed to be sorted.\nGnome Sort effectively performs Insertion Sort via adjacent swaps.\n\n\n\nPass\nSwaps\nMovement\nSorted Portion\n\n\n\n\n1\nFew\nBackward\nExpands gradually\n\n\nn\nMany\nOscillating\nFully sorted\n\n\n\nWorst-case swaps: O(n²) Best-case (already sorted): O(n)\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [5, 3, 4, 1, 2] step by step\n\n\n2\nTrace i pointer movement\n\n\n3\nCompare with Insertion Sort shifts\n\n\n4\nAnimate using console output\n\n\n5\nTry reversed input to see maximum swaps\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nSwaps\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n3\nMany backtracks\n\n\n[1, 2, 3]\n[1, 2, 3]\n0\nAlready sorted\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n8\nModerate swaps\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nGnome Sort is a whimsical algorithm that teaches persistence: every time something’s out of order, step back, fix it, and keep going. It’s inefficient for large data but delightful for learning and visualization.\n\n\n\n109 Odd-Even Sort\nOdd-Even Sort, also known as Brick Sort, is a parallel-friendly variant of Bubble Sort. It alternates between comparing odd-even and even-odd indexed pairs to gradually sort the array. It’s especially useful in parallel processing where pairs can be compared simultaneously.\n\nWhat Problem Are We Solving??\nBubble Sort compares every adjacent pair in one sweep. Odd-Even Sort breaks this into two alternating phases:\n\nOdd phase: compare (1,2), (3,4), (5,6), …\nEven phase: compare (0,1), (2,3), (4,5), …\n\nRepeating these two passes ensures all adjacent pairs eventually become sorted.\nIt’s ideal for parallel systems or hardware implementations since comparisons in each phase are independent.\n\n\nExample\n\n\n\nPhase\nType\nArray State\nAction\n\n\n\n\n0\nInit\n[5, 3, 4, 1, 2]\nStart\n\n\n1\nEven\n[3, 5, 1, 4, 2]\nCompare (0,1), (2,3), (4,5)\n\n\n2\nOdd\n[3, 1, 5, 2, 4]\nCompare (1,2), (3,4)\n\n\n3\nEven\n[1, 3, 2, 4, 5]\nCompare (0,1), (2,3), (4,5)\n\n\n4\nOdd\n[1, 2, 3, 4, 5]\nCompare (1,2), (3,4) → Sorted\n\n\n\n\n\nHow Does It Work (Plain Language)?\nOdd-Even Sort moves elements closer to their correct position with every alternating phase. It works like a traffic system: cars at even intersections move, then cars at odd intersections move. Over time, all cars (elements) line up in order.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nCompare all even-odd pairs\nSwap if out of order\n\n\n2\nCompare all odd-even pairs\nSwap if out of order\n\n\n3\nRepeat until no swaps occur\nSorted array\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid odd_even_sort(int a[], int n) {\n    bool sorted = false;\n    while (!sorted) {\n        sorted = true;\n\n        // Even phase\n        for (int i = 0; i &lt; n - 1; i += 2) {\n            if (a[i] &gt; a[i + 1]) {\n                int temp = a[i];\n                a[i] = a[i + 1];\n                a[i + 1] = temp;\n                sorted = false;\n            }\n        }\n\n        // Odd phase\n        for (int i = 1; i &lt; n - 1; i += 2) {\n            if (a[i] &gt; a[i + 1]) {\n                int temp = a[i];\n                a[i] = a[i + 1];\n                a[i + 1] = temp;\n                sorted = false;\n            }\n        }\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    odd_even_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef odd_even_sort(a):\n    n = len(a)\n    sorted = False\n    while not sorted:\n        sorted = True\n\n        # Even phase\n        for i in range(0, n - 1, 2):\n            if a[i] &gt; a[i + 1]:\n                a[i], a[i + 1] = a[i + 1], a[i]\n                sorted = False\n\n        # Odd phase\n        for i in range(1, n - 1, 2):\n            if a[i] &gt; a[i + 1]:\n                a[i], a[i + 1] = a[i + 1], a[i]\n                sorted = False\n\narr = [5, 3, 4, 1, 2]\nodd_even_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nDemonstrates parallel sorting principles\nConceptually simple, easy to visualize\nCan be implemented on parallel processors (SIMD, GPU)\nStable and in-place\n\n\n\nA Gentle Proof (Why It Works)\nOdd-Even Sort systematically removes all inversions by alternating comparisons:\n\nEven phase fixes pairs (0,1), (2,3), (4,5), …\nOdd phase fixes pairs (1,2), (3,4), (5,6), …\n\nAfter n iterations, every element “bubbles” to its position. It behaves like Bubble Sort but is more structured and phase-based.\n\n\n\nPhase\nComparisons\nIndependent?\nSwaps\n\n\n\n\nEven\n⌊n/2⌋\nYes\nSome\n\n\nOdd\n⌊n/2⌋\nYes\nSome\n\n\n\nTotal complexity remains O(n²), but parallelizable phases reduce wall-clock time.\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nTrace [5, 3, 4, 1, 2] through phases\n\n\n2\nCount number of phases to sort\n\n\n3\nImplement using threads (parallel version)\n\n\n4\nCompare with Bubble Sort\n\n\n5\nAnimate even and odd passes\n\n\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nPhases\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n3\nAlternating passes\n\n\n[1, 2, 3]\n[1, 2, 3]\n1\nEarly stop\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n4\nModerate passes\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nOdd-Even Sort shows that structure matters. By alternating phases, it opens the door to parallel sorting, where independent comparisons can run at once, a neat step toward high-performance sorting.\n\n\n\n110 Stooge Sort\nStooge Sort is one of the most unusual and quirky recursive sorting algorithms. It’s not efficient, but it’s fascinating because it sorts by recursively sorting overlapping sections of the array, a great way to study recursion and algorithmic curiosity.\n\nWhat Problem Are We Solving??\nStooge Sort doesn’t aim for speed. Instead, it provides an example of a recursive divide-and-conquer strategy that’s neither efficient nor conventional. It divides the array into overlapping parts and recursively sorts them, twice on the first two-thirds, once on the last two-thirds.\nThis algorithm is often used for educational purposes to demonstrate how recursion can be applied in non-traditional ways.\n\n\nExample\n\n\n\nStep\nRange\nAction\nArray State\n\n\n\n\n0\n[0..4]\nStart sorting [5,3,4,1,2]\n[5,3,4,1,2]\n\n\n1\n[0..3]\nSort first 2/3 (4 elements)\n[3,4,1,5,2]\n\n\n2\n[1..4]\nSort last 2/3 (4 elements)\n[3,1,4,2,5]\n\n\n3\n[0..3]\nSort first 2/3 again (4 elements)\n[1,3,2,4,5]\n\n\n4\nRepeat\nUntil subarrays shrink to length 1\n[1,2,3,4,5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nIf the first element is larger than the last, swap them. Then recursively:\n\nSort the first two-thirds of the array.\nSort the last two-thirds of the array.\nSort the first two-thirds again.\n\nIt’s like checking the front, then the back, then rechecking the front, until everything settles in order.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\n\n\n\n\n1\nCompare first and last elements; swap if needed\n\n\n2\nRecursively sort first 2/3 of array\n\n\n3\nRecursively sort last 2/3 of array\n\n\n4\nRecursively sort first 2/3 again\n\n\n5\nStop when subarray length ≤ 1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid stooge_sort(int a[], int l, int h) {\n    if (a[l] &gt; a[h]) {\n        int temp = a[l];\n        a[l] = a[h];\n        a[h] = temp;\n    }\n    if (h - l + 1 &gt; 2) {\n        int t = (h - l + 1) / 3;\n        stooge_sort(a, l, h - t);\n        stooge_sort(a, l + t, h);\n        stooge_sort(a, l, h - t);\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    stooge_sort(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef stooge_sort(a, l, h):\n    if a[l] &gt; a[h]:\n        a[l], a[h] = a[h], a[l]\n    if h - l + 1 &gt; 2:\n        t = (h - l + 1) // 3\n        stooge_sort(a, l, h - t)\n        stooge_sort(a, l + t, h)\n        stooge_sort(a, l, h - t)\n\narr = [5, 3, 4, 1, 2]\nstooge_sort(arr, 0, len(arr) - 1)\nprint(arr)\n\n\n\nWhy It Matters\n\nDemonstrates recursive divide-and-conquer logic\nA fun counterexample to “more recursion = more speed”\nUseful in theoretical discussions or algorithmic humor\nHelps build understanding of overlapping subproblems\n\n\n\nA Gentle Proof (Why It Works)\nAt each step, Stooge Sort ensures:\n\nThe smallest element moves toward the front,\nThe largest element moves toward the back,\nThe array converges to sorted order through overlapping recursive calls.\n\nEach recursion operates on 2/3 of the range, repeated 3 times, giving recurrence:\n\\[\nT(n) = 3T(2n/3) + O(1)\n\\]\nSolving it (using Master Theorem):\n\\[\nT(n) = O(n^{\\log_{1.5} 3}) \\approx O(n^{2.7095})\n\\]\nSlower than Bubble Sort (O(n²))!\n\n\n\nStep\nSubarray Length\nRecursive Calls\nWork per Level\n\n\n\n\nn\n3\nO(1)\nConstant\n\n\nn/2\n3×3\nO(3)\nLarger\n\n\n…\n…\n…\n…\n\n\n\n\n\nTry It Yourself\n\n\n\nTask\nDescription\n\n\n\n\n1\nSort [5, 3, 4, 1, 2] manually and trace recursive calls\n\n\n2\nCount total swaps\n\n\n3\nPrint recursion depth\n\n\n4\nCompare with Merge Sort steps\n\n\n5\nMeasure runtime for n=10, 100, 1000\n\n\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nInput\nOutput\nRecursion Depth\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\n3\nWorks recursively\n\n\n[1, 2, 3]\n[1, 2, 3]\n1\nMinimal swaps\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\n6\nMany overlaps\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n².7095)\n\n\nTime (Best)\nO(n².7095)\n\n\nSpace\nO(log n) recursion stack\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nStooge Sort is a delightful oddity, slow, redundant, but undeniably creative. It reminds us that not every recursive idea leads to efficiency, and that algorithm design is as much art as science.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-12.-divide-and-conquer-sorting",
    "href": "books/en-us/list-2.html#section-12.-divide-and-conquer-sorting",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 12. Divide and conquer sorting",
    "text": "Section 12. Divide and conquer sorting\n\n111 Merge Sort\nMerge Sort is one of the most famous divide-and-conquer sorting algorithms. It splits the array into halves, sorts each half recursively, and then merges the two sorted halves into a single sorted array. It guarantees O(n log n) performance, is stable, and serves as the backbone of many modern sorting libraries.\n\nWhat Problem Are We Solving??\nWe want a sorting algorithm that is:\n\nEfficient on large datasets (O(n log n))\nStable (preserves equal element order)\nPredictable (no worst-case degradation)\n\nMerge Sort achieves this by dividing the problem into smaller, easily solved subproblems and combining their results.\nIt’s ideal for:\n\nSorting linked lists\nExternal sorting (on disk)\nStable merges (for multi-key sorting)\n\n\n\nExample\n\n\n\nStep\nAction\nResult\n\n\n\n\n0\nInput\n[5, 3, 4, 1, 2]\n\n\n1\nSplit\n[5, 3, 4] and [1, 2]\n\n\n2\nSplit further\n[5], [3, 4], [1], [2]\n\n\n3\nSort subarrays\n[3, 4, 5], [1, 2]\n\n\n4\nMerge sorted halves\n[1, 2, 3, 4, 5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\n\nDivide: Split the array into two halves.\nConquer: Recursively sort both halves.\nCombine: Merge the sorted halves into a single sorted array.\n\nThink of it as sorting two smaller piles, then interleaving them in order, like merging two stacks of playing cards.\n\n\nStep-by-Step Process\n\nIf the array has 0 or 1 element, it’s already sorted.\nRecursively sort left and right halves.\nUse a helper merge function to combine them.\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid merge(int a[], int left, int mid, int right) {\n    int n1 = mid - left + 1, n2 = right - mid;\n    int L[n1], R[n2];\n\n    for (int i = 0; i &lt; n1; i++) L[i] = a[left + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = a[mid + 1 + j];\n\n    int i = 0, j = 0, k = left;\n    while (i &lt; n1 && j &lt; n2) {\n        if (L[i] &lt;= R[j]) a[k++] = L[i++];\n        else a[k++] = R[j++];\n    }\n    while (i &lt; n1) a[k++] = L[i++];\n    while (j &lt; n2) a[k++] = R[j++];\n}\n\nvoid merge_sort(int a[], int left, int right) {\n    if (left &lt; right) {\n        int mid = left + (right - left) / 2;\n        merge_sort(a, left, mid);\n        merge_sort(a, mid + 1, right);\n        merge(a, left, mid, right);\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    merge_sort(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef merge_sort(a):\n    if len(a) &gt; 1:\n        mid = len(a) // 2\n        left = a[:mid]\n        right = a[mid:]\n\n        merge_sort(left)\n        merge_sort(right)\n\n        i = j = k = 0\n        while i &lt; len(left) and j &lt; len(right):\n            if left[i] &lt;= right[j]:\n                a[k] = left[i]\n                i += 1\n            else:\n                a[k] = right[j]\n                j += 1\n            k += 1\n\n        while i &lt; len(left):\n            a[k] = left[i]\n            i += 1\n            k += 1\n        while j &lt; len(right):\n            a[k] = right[j]\n            j += 1\n            k += 1\n\narr = [5, 3, 4, 1, 2]\nmerge_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nStable: Keeps relative order of equal elements\nDeterministic O(n log n): Always efficient\nParallelizable: Subarrays can be sorted independently\nFoundation: For hybrid algorithms like TimSort and External Merge Sort\n\n\n\nA Gentle Proof (Why It Works)\nMerge Sort divides the array into two halves at each level. There are log₂ n levels of recursion, and each merge takes O(n) time.\nSo total time: \\[\nT(n) = O(n \\log n)\n\\]\nMerging is linear because each element is copied once per level.\n\n\n\nStep\nWork\nSubarrays\nTotal\n\n\n\n\n1\nO(n)\n1\nO(n)\n\n\n2\nO(n/2) × 2\n2\nO(n)\n\n\n3\nO(n/4) × 4\n4\nO(n)\n\n\n…\n…\n…\nO(n log n)\n\n\n\n\n\nTry It Yourself\n\nSplit [5, 3, 4, 1, 2] into halves step by step.\nMerge [3, 5] and [1, 4] manually.\nTrace the recursive calls on paper.\nImplement an iterative bottom-up version.\nModify to sort descending.\nPrint arrays at each merge step.\nCompare the number of comparisons vs. Bubble Sort.\nTry merging two pre-sorted arrays [1,3,5] and [2,4,6].\nSort a list of strings (alphabetically).\nVisualize the recursion tree for n = 8.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nStandard test\n\n\n[1, 2, 3]\n[1, 2, 3]\nAlready sorted\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral case\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nTests stability\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n log n)\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nSpace\nO(n)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\n\nMerge Sort is your first taste of divide and conquer sorting, calm, reliable, and elegant. It divides the problem cleanly, conquers recursively, and merges with precision.\n\n\n\n112 Iterative Merge Sort\nIterative Merge Sort is a non-recursive version of Merge Sort that uses bottom-up merging. Instead of dividing recursively, it starts with subarrays of size 1 and iteratively merges them in pairs, doubling the size each round. This makes it ideal for environments where recursion is expensive or limited.\n\nWhat Problem Are We Solving??\nRecursive Merge Sort requires function calls and stack space. In some systems, recursion might be slow or infeasible (e.g. embedded systems, large arrays). Iterative Merge Sort avoids recursion by sorting iteratively, merging subarrays of increasing size until the entire array is sorted.\nIt’s especially handy for:\n\nIterative environments (no recursion)\nLarge data sets (predictable memory)\nExternal sorting with iterative passes\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nStep\nSubarray Size\nAction\nArray State\n\n\n\n\n0\n1\nEach element is trivially sorted\n[5, 3, 4, 1, 2]\n\n\n1\n1\nMerge pairs of 1-element subarrays\n[3, 5, 1, 4, 2]\n\n\n2\n2\nMerge pairs of 2-element subarrays\n[1, 3, 4, 5, 2]\n\n\n3\n4\nMerge 4-element sorted block with last\n[1, 2, 3, 4, 5]\n\n\n4\nDone\nFully sorted\n[1, 2, 3, 4, 5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nThink of it as sorting small groups first and then merging those into bigger groups, no recursion required.\nProcess:\n\nStart with subarrays of size 1 (already sorted).\nMerge adjacent pairs of subarrays.\nDouble the subarray size and repeat.\nContinue until subarray size ≥ n.\n\n\n\nStep-by-Step Process\n\nOuter loop: size = 1, 2, 4, 8, … until ≥ n\nInner loop: merge every two adjacent blocks of given size\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid merge(int a[], int left, int mid, int right) {\n    int n1 = mid - left + 1, n2 = right - mid;\n    int L[n1], R[n2];\n    for (int i = 0; i &lt; n1; i++) L[i] = a[left + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = a[mid + 1 + j];\n\n    int i = 0, j = 0, k = left;\n    while (i &lt; n1 && j &lt; n2) {\n        if (L[i] &lt;= R[j]) a[k++] = L[i++];\n        else a[k++] = R[j++];\n    }\n    while (i &lt; n1) a[k++] = L[i++];\n    while (j &lt; n2) a[k++] = R[j++];\n}\n\nvoid iterative_merge_sort(int a[], int n) {\n    for (int size = 1; size &lt; n; size *= 2) {\n        for (int left = 0; left &lt; n - 1; left += 2 * size) {\n            int mid = left + size - 1;\n            int right = (left + 2 * size - 1 &lt; n - 1) ? (left + 2 * size - 1) : (n - 1);\n            if (mid &lt; right)\n                merge(a, left, mid, right);\n        }\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    iterative_merge_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef merge(a, left, mid, right):\n    left_arr = a[left:mid+1]\n    right_arr = a[mid+1:right+1]\n    i = j = 0\n    k = left\n    while i &lt; len(left_arr) and j &lt; len(right_arr):\n        if left_arr[i] &lt;= right_arr[j]:\n            a[k] = left_arr[i]\n            i += 1\n        else:\n            a[k] = right_arr[j]\n            j += 1\n        k += 1\n    while i &lt; len(left_arr):\n        a[k] = left_arr[i]\n        i += 1\n        k += 1\n    while j &lt; len(right_arr):\n        a[k] = right_arr[j]\n        j += 1\n        k += 1\n\ndef iterative_merge_sort(a):\n    n = len(a)\n    size = 1\n    while size &lt; n:\n        for left in range(0, n, 2 * size):\n            mid = min(left + size - 1, n - 1)\n            right = min(left + 2 * size - 1, n - 1)\n            if mid &lt; right:\n                merge(a, left, mid, right)\n        size *= 2\n\narr = [5, 3, 4, 1, 2]\niterative_merge_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nEliminates recursion (more predictable memory usage)\nStill guarantees O(n log n) performance\nUseful for iterative, bottom-up, or external sorting\nEasier to parallelize since merge operations are independent\n\n\n\nA Gentle Proof (Why It Works)\nEach iteration doubles the sorted block size. Since each element participates in log₂ n merge levels, and each level costs O(n) work, total cost:\n\\[\nT(n) = O(n \\log n)\n\\]\nLike recursive Merge Sort, each merge step is linear, and merging subarrays is stable.\n\n\n\nIteration\nBlock Size\nMerges\nWork\n\n\n\n\n1\n1\nn/2\nO(n)\n\n\n2\n2\nn/4\nO(n)\n\n\n3\n4\nn/8\nO(n)\n\n\n…\n…\n…\n…\n\n\n\nTotal = O(n log n)\n\n\nTry It Yourself\n\nSort [5, 3, 4, 1, 2] manually using bottom-up passes.\nTrace each pass: subarray size = 1 → 2 → 4.\nPrint intermediate arrays after each pass.\nCompare recursion depth with recursive version.\nImplement a space-efficient version (in-place merge).\nModify to sort descending.\nApply to linked list version.\nTest performance on large array (n = 10⁶).\nVisualize merging passes as a tree.\nImplement on external storage (file-based).\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nSmall test\n\n\n[1, 2, 3]\n[1, 2, 3]\nAlready sorted\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral test\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nTests stability\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n log n)\n\n\nTime (Best)\nO(n log n)\n\n\nSpace\nO(n)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\n\nIterative Merge Sort is the non-recursive twin of classic Merge Sort, efficient, stable, and memory-predictable, making it perfect when stack space is at a premium.\n\n\n\n113 Quick Sort\nQuick Sort is one of the fastest and most widely used sorting algorithms. It works by partitioning an array into two halves around a pivot element and recursively sorting the two parts. With average-case O(n log n) performance and in-place operation, it’s the go-to choice in many libraries and real-world systems.\n\nWhat Problem Are We Solving??\nWe need a sorting algorithm that’s:\n\nEfficient in practice (fast average case)\nIn-place (minimal memory use)\nDivide-and-conquer-based (parallelizable)\n\nQuick Sort partitions the array so that:\n\nAll elements smaller than the pivot go left\nAll elements larger go right\n\nThen it sorts each half recursively.\nIt’s ideal for:\n\nLarge datasets in memory\nSystems where memory allocation is limited\nAverage-case performance optimization\n\n\n\nExample\n\n\n\nStep\nPivot\nAction\nArray State\n\n\n\n\n0\n5\nPartition\n[3, 4, 1, 2, 5]\n\n\n1\n3\nPartition left\n[1, 2, 3, 4, 5]\n\n\n2\nDone\nSorted\n[1, 2, 3, 4, 5]\n\n\n\n\n\nHow Does It Work (Plain Language)?\n\nChoose a pivot element.\nRearrange (partition) array so:\n\nElements smaller than pivot move to the left\nElements larger than pivot move to the right\n\nRecursively apply the same logic to each half.\n\nThink of the pivot as the “divider” that splits the unsorted array into two smaller problems.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\n\n\n\n\n1\nSelect a pivot (e.g., last element)\n\n\n2\nPartition array around pivot\n\n\n3\nRecursively sort left and right subarrays\n\n\n4\nStop when subarray size ≤ 1\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nC (Lomuto Partition Scheme)\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n}\n\nint partition(int a[], int low, int high) {\n    int pivot = a[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (a[j] &lt; pivot) {\n            i++;\n            swap(&a[i], &a[j]);\n        }\n    }\n    swap(&a[i + 1], &a[high]);\n    return i + 1;\n}\n\nvoid quick_sort(int a[], int low, int high) {\n    if (low &lt; high) {\n        int pi = partition(a, low, high);\n        quick_sort(a, low, pi - 1);\n        quick_sort(a, pi + 1, high);\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    quick_sort(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\nPython\ndef partition(a, low, high):\n    pivot = a[high]\n    i = low - 1\n    for j in range(low, high):\n        if a[j] &lt; pivot:\n            i += 1\n            a[i], a[j] = a[j], a[i]\n    a[i + 1], a[high] = a[high], a[i + 1]\n    return i + 1\n\ndef quick_sort(a, low, high):\n    if low &lt; high:\n        pi = partition(a, low, high)\n        quick_sort(a, low, pi - 1)\n        quick_sort(a, pi + 1, high)\n\narr = [5, 3, 4, 1, 2]\nquick_sort(arr, 0, len(arr) - 1)\nprint(arr)\n\n\n\nWhy It Matters\n\nIn-place and fast in most real-world cases\nDivide and conquer: naturally parallelizable\nOften used as the default sorting algorithm in libraries (C, Java, Python)\nIntroduces partitioning, a key algorithmic pattern\n\n\n\nA Gentle Proof (Why It Works)\nEach partition divides the problem into smaller subarrays. Average partition splits are balanced, giving O(log n) depth and O(n) work per level:\n\\[\nT(n) = 2T(n/2) + O(n) = O(n \\log n)\n\\]\nIf the pivot is poor (e.g. smallest or largest), complexity degrades:\n\\[\nT(n) = T(n-1) + O(n) = O(n^2)\n\\]\n\n\n\nCase\nPartition Quality\nComplexity\n\n\n\n\nBest\nPerfect halves\nO(n log n)\n\n\nAverage\nRandom\nO(n log n)\n\n\nWorst\nUnbalanced\nO(n²)\n\n\n\nChoosing pivots wisely (randomization, median-of-three) avoids worst-case splits.\n\n\nTry It Yourself\n\nSort [5, 3, 4, 1, 2] and trace partitions.\nChange pivot selection (first, middle, random).\nCount comparisons and swaps for each case.\nImplement using Hoare Partition scheme.\nModify to sort descending.\nVisualize recursion tree for n = 8.\nCompare runtime with Merge Sort.\nTry sorted input [1, 2, 3, 4, 5] and note behavior.\nAdd a counter to count recursive calls.\nImplement tail recursion optimization.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nBasic\n\n\n[1, 2, 3]\n[1, 2, 3]\nWorst case (sorted input)\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(log n) (recursion)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nQuick Sort is the practical workhorse of sorting, swift, elegant, and widely loved. It teaches how a single smart pivot can bring order to chaos.\n\n\n\n114 Hoare Partition Scheme\nThe Hoare Partition Scheme is an early and elegant version of Quick Sort’s partitioning method, designed by C.A.R. Hoare himself. It’s more efficient than the Lomuto scheme in many cases because it does fewer swaps and uses two pointers moving inward from both ends.\n\nWhat Problem Are We Solving??\nIn Quick Sort, we need a way to divide an array into two parts:\n\nElements less than or equal to the pivot\nElements greater than or equal to the pivot\n\nHoare’s scheme achieves this using two indices that move toward each other, swapping elements that are out of place. It reduces the number of swaps compared to the Lomuto scheme and often performs better on real data.\nIt’s especially useful for:\n\nLarge arrays (fewer writes)\nPerformance-critical systems\nIn-place partitioning without extra space\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nStep\nPivot\nLeft (i)\nRight (j)\nArray State\nAction\n\n\n\n\n0\n5\ni=0\nj=4\n[5, 3, 4, 1, 2]\npivot = 5\n\n\n1\n5\ni→2\nj→4\n[5, 3, 4, 1, 2]\na[j]=2&lt;5 swap(5,2) → [2,3,4,1,5]\n\n\n2\n5\ni→2\nj→3\n[2,3,4,1,5]\na[i]=4&gt;5? no swap\n\n\n3\nstop\n-\n-\n[2,3,4,1,5]\npartition done\n\n\n\nThe pivot ends up near its correct position, but not necessarily in the final index.\n\n\nHow Does It Work (Plain Language)?\nThe algorithm picks a pivot (commonly the first element), then moves two pointers:\n\nLeft pointer (i): moves right, skipping small elements\nRight pointer (j): moves left, skipping large elements\n\nWhen both pointers find misplaced elements, they are swapped. This continues until they cross, at that point, the array is partitioned.\n\n\nStep-by-Step Process\n\nChoose a pivot (e.g. first element).\nSet two indices: i = left - 1, j = right + 1.\nIncrement i until a[i] &gt;= pivot.\nDecrement j until a[j] &lt;= pivot.\nIf i &lt; j, swap a[i] and a[j]. Otherwise, return j (partition index).\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n}\n\nint hoare_partition(int a[], int low, int high) {\n    int pivot = a[low];\n    int i = low - 1;\n    int j = high + 1;\n\n    while (1) {\n        do { i++; } while (a[i] &lt; pivot);\n        do { j--; } while (a[j] &gt; pivot);\n        if (i &gt;= j) return j;\n        swap(&a[i], &a[j]);\n    }\n}\n\nvoid quick_sort_hoare(int a[], int low, int high) {\n    if (low &lt; high) {\n        int p = hoare_partition(a, low, high);\n        quick_sort_hoare(a, low, p);\n        quick_sort_hoare(a, p + 1, high);\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    quick_sort_hoare(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef hoare_partition(a, low, high):\n    pivot = a[low]\n    i = low - 1\n    j = high + 1\n    while True:\n        i += 1\n        while a[i] &lt; pivot:\n            i += 1\n        j -= 1\n        while a[j] &gt; pivot:\n            j -= 1\n        if i &gt;= j:\n            return j\n        a[i], a[j] = a[j], a[i]\n\ndef quick_sort_hoare(a, low, high):\n    if low &lt; high:\n        p = hoare_partition(a, low, high)\n        quick_sort_hoare(a, low, p)\n        quick_sort_hoare(a, p + 1, high)\n\narr = [5, 3, 4, 1, 2]\nquick_sort_hoare(arr, 0, len(arr) - 1)\nprint(arr)\n\n\n\nWhy It Matters\n\nFewer swaps than Lomuto partition\nMore efficient in practice on most datasets\nStill in-place, divide-and-conquer, O(n log n) average\nIntroduces the idea of two-pointer partitioning\n\n\n\nA Gentle Proof (Why It Works)\nThe loop invariants ensure that:\n\nLeft side: all elements ≤ pivot\nRight side: all elements ≥ pivot\ni and j move inward until they cross When they cross, all elements are partitioned correctly.\n\nThe pivot does not end in its final sorted position, but the subarrays can be recursively sorted independently.\n\\[\nT(n) = T(k) + T(n - k - 1) + O(n)\n\\]\nAverage complexity O(n log n); worst-case O(n²) if pivot is poor.\n\n\n\nCase\nPivot\nBehavior\nComplexity\n\n\n\n\nBest\nMedian\nBalanced halves\nO(n log n)\n\n\nAverage\nRandom\nSlight imbalance\nO(n log n)\n\n\nWorst\nMin/Max\nUnbalanced\nO(n²)\n\n\n\n\n\nTry It Yourself\n\nSort [5, 3, 4, 1, 2] step by step using Hoare’s scheme.\nPrint i and j at each iteration.\nCompare with Lomuto’s version on the same array.\nTry different pivot positions (first, last, random).\nMeasure number of swaps vs. Lomuto.\nModify to sort descending.\nVisualize partition boundaries.\nTest on array with duplicates [3,3,2,1,4].\nImplement hybrid pivot selection (median-of-three).\nCompare runtime with Merge Sort.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nSimple\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nWorks with duplicates\n\n\n[1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\nSorted input (worst case)\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(log n) recursion\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nHoare Partition Scheme is elegant and efficient, the original genius of Quick Sort. Its two-pointer dance is graceful and economical, a timeless classic in algorithm design.\n\n\n\n115 Lomuto Partition Scheme\nThe Lomuto Partition Scheme is a simple and widely taught method for partitioning in Quick Sort. It’s easier to understand and implement than Hoare’s scheme, though it often performs slightly more swaps. It always selects a pivot (commonly the last element) and partitions the array in a single forward pass.\n\nWhat Problem Are We Solving??\nWe need a clear and intuitive way to partition an array around a pivot, ensuring all smaller elements go to the left and all larger elements go to the right.\nLomuto’s method uses one scanning pointer and one boundary pointer, making it easy for beginners and ideal for pedagogical purposes or small datasets.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nStep\nPivot\ni (Boundary)\nj (Scan)\nArray State\nAction\n\n\n\n\n0\n2\ni=-1\nj=0\n[5, 3, 4, 1, 2]\npivot = 2\n\n\n1\n2\ni=-1\nj=0\na[0]=5&gt;2 → no swap\n\n\n\n2\n2\ni=-1\nj=1\na[1]=3&gt;2 → no swap\n\n\n\n3\n2\ni=-1\nj=2\na[2]=4&gt;2 → no swap\n\n\n\n4\n2\ni=0\nj=3\na[3]=1&lt;2 → swap(5,1) → [1,3,4,5,2]\n\n\n\n5\n2\ni=0\nj=4\nend of scan; swap pivot with a[i+1]=a[1]\n\n\n\n6\nDone\n-\n-\n[1,2,4,5,3] partitioned\n\n\n\n\nPivot 2 is placed in its final position at index 1. Elements left of 2 are smaller, right are larger.\n\n\nHow Does It Work (Plain Language)?\n\nChoose a pivot (commonly last element).\nInitialize boundary pointer i before start of array.\nIterate through array with pointer j:\n\nIf a[j] &lt; pivot, increment i and swap a[i] with a[j].\n\nAfter loop, swap pivot into position i + 1.\nReturn i + 1 (pivot’s final position).\n\nIt’s like separating a deck of cards, you keep moving smaller cards to the front as you scan.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\n\n\n\n\n1\nChoose pivot (usually last element)\n\n\n2\nMove smaller elements before pivot\n\n\n3\nMove larger elements after pivot\n\n\n4\nPlace pivot in its correct position\n\n\n5\nReturn pivot index for recursive sorting\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n}\n\nint lomuto_partition(int a[], int low, int high) {\n    int pivot = a[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (a[j] &lt; pivot) {\n            i++;\n            swap(&a[i], &a[j]);\n        }\n    }\n    swap(&a[i + 1], &a[high]);\n    return i + 1;\n}\n\nvoid quick_sort_lomuto(int a[], int low, int high) {\n    if (low &lt; high) {\n        int pi = lomuto_partition(a, low, high);\n        quick_sort_lomuto(a, low, pi - 1);\n        quick_sort_lomuto(a, pi + 1, high);\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    quick_sort_lomuto(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef lomuto_partition(a, low, high):\n    pivot = a[high]\n    i = low - 1\n    for j in range(low, high):\n        if a[j] &lt; pivot:\n            i += 1\n            a[i], a[j] = a[j], a[i]\n    a[i + 1], a[high] = a[high], a[i + 1]\n    return i + 1\n\ndef quick_sort_lomuto(a, low, high):\n    if low &lt; high:\n        pi = lomuto_partition(a, low, high)\n        quick_sort_lomuto(a, low, pi - 1)\n        quick_sort_lomuto(a, pi + 1, high)\n\narr = [5, 3, 4, 1, 2]\nquick_sort_lomuto(arr, 0, len(arr) - 1)\nprint(arr)\n\n\n\nWhy It Matters\n\nSimple and easy to implement\nPivot ends in correct final position each step\nUseful for educational demonstration of Quick Sort\nCommon in textbooks and basic Quick Sort examples\n\n\n\nA Gentle Proof (Why It Works)\nInvariant:\n\nElements left of i are smaller than pivot.\nElements between i and j are greater or not yet checked. At the end, swapping pivot with a[i+1] places it in its final position.\n\nTime complexity:\n\\[\nT(n) = T(k) + T(n - k - 1) + O(n)\n\\]\nAverage: O(n log n) Worst (already sorted): O(n²)\n\n\n\nCase\nPartition\nComplexity\n\n\n\n\nBest\nBalanced\nO(n log n)\n\n\nAverage\nRandom\nO(n log n)\n\n\nWorst\nUnbalanced\nO(n²)\n\n\n\n\n\nTry It Yourself\n\nSort [5, 3, 4, 1, 2] using Lomuto step by step.\nTrace i and j positions at each comparison.\nCompare with Hoare partition’s number of swaps.\nTest with sorted input, see worst case.\nRandomize pivot to avoid worst-case.\nModify to sort descending order.\nCount total swaps and comparisons.\nCombine with tail recursion optimization.\nVisualize partition boundary after each pass.\nImplement a hybrid Quick Sort using Lomuto for small arrays.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nSimple\n\n\n[1, 2, 3]\n[1, 2, 3]\nWorst-case\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nHandles duplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(log n) recursion\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nLomuto’s scheme is the friendly teacher of Quick Sort, easy to grasp, simple to code, and perfect for building intuition about partitioning and divide-and-conquer sorting.\n\n\n\n116 Randomized Quick Sort\nRandomized Quick Sort enhances the classic Quick Sort by choosing the pivot randomly. This small tweak eliminates the risk of hitting the worst-case O(n²) behavior on already sorted or adversarial inputs, making it one of the most robust and practical sorting strategies in real-world use.\n\nWhat Problem Are We Solving??\nRegular Quick Sort can degrade badly if the pivot is chosen poorly (for example, always picking the first or last element in a sorted array). Randomized Quick Sort fixes this by selecting a random pivot, ensuring that, on average, partitions are balanced, regardless of input distribution.\nThis makes it ideal for:\n\nUnpredictable or adversarial inputs\nLarge datasets where worst-case avoidance matters\nPerformance-critical systems requiring consistent behavior\n\n\n\nExample\n\n\n\nStep\nAction\nArray State\nPivot\n\n\n\n\n0\nChoose random pivot\n[5, 3, 4, 1, 2]\n4\n\n\n1\nPartition around 4\n[3, 2, 1, 4, 5]\n4 at index 3\n\n\n2\nRecurse on left [3, 2, 1]\n[1, 2, 3]\n2\n\n\n3\nMerge subarrays\n[1, 2, 3, 4, 5]\nDone\n\n\n\nRandomization ensures the pivot is unlikely to create unbalanced partitions.\n\n\nHow Does It Work (Plain Language)?\nIt’s the same Quick Sort, but before partitioning, we randomly pick one element and use it as the pivot. This random pivot is swapped into the last position, and the normal Lomuto or Hoare partitioning continues.\nThis small randomness makes it robust and efficient on average, even for worst-case inputs.\n\n\nStep-by-Step Process\n\nPick a random pivot index between low and high.\nSwap the random pivot with the last element.\nPartition the array (e.g., Lomuto or Hoare).\nRecursively sort left and right partitions.\n\n\n\nTiny Code (Easy Versions)\n\n\nC (Lomuto + Random Pivot)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nvoid swap(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n}\n\nint lomuto_partition(int a[], int low, int high) {\n    int pivot = a[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (a[j] &lt; pivot) {\n            i++;\n            swap(&a[i], &a[j]);\n        }\n    }\n    swap(&a[i + 1], &a[high]);\n    return i + 1;\n}\n\nint randomized_partition(int a[], int low, int high) {\n    int pivotIndex = low + rand() % (high - low + 1);\n    swap(&a[pivotIndex], &a[high]);\n    return lomuto_partition(a, low, high);\n}\n\nvoid randomized_quick_sort(int a[], int low, int high) {\n    if (low &lt; high) {\n        int pi = randomized_partition(a, low, high);\n        randomized_quick_sort(a, low, pi - 1);\n        randomized_quick_sort(a, pi + 1, high);\n    }\n}\n\nint main(void) {\n    srand(time(NULL));\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    randomized_quick_sort(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\nPython\nimport random\n\ndef lomuto_partition(a, low, high):\n    pivot = a[high]\n    i = low - 1\n    for j in range(low, high):\n        if a[j] &lt; pivot:\n            i += 1\n            a[i], a[j] = a[j], a[i]\n    a[i + 1], a[high] = a[high], a[i + 1]\n    return i + 1\n\ndef randomized_partition(a, low, high):\n    pivot_index = random.randint(low, high)\n    a[pivot_index], a[high] = a[high], a[pivot_index]\n    return lomuto_partition(a, low, high)\n\ndef randomized_quick_sort(a, low, high):\n    if low &lt; high:\n        pi = randomized_partition(a, low, high)\n        randomized_quick_sort(a, low, pi - 1)\n        randomized_quick_sort(a, pi + 1, high)\n\narr = [5, 3, 4, 1, 2]\nrandomized_quick_sort(arr, 0, len(arr) - 1)\nprint(arr)\n\n\n\nWhy It Matters\n\nPrevents worst-case O(n²) behavior\nSimple yet highly effective\nEnsures consistent average-case across all inputs\nFoundation for Randomized Select and Randomized Algorithms\n\n\n\nA Gentle Proof (Why It Works)\nRandom pivot selection ensures the expected split size is balanced, independent of input order. Each pivot divides the array such that expected recursion depth is O(log n) and total comparisons O(n log n).\nExpected complexity: \\[\nE[T(n)] = O(n \\log n)\n\\] Worst-case only occurs with extremely low probability (1/n!).\n\n\n\nCase\nPivot Choice\nComplexity\n\n\n\n\nBest\nBalanced\nO(n log n)\n\n\nAverage\nRandom\nO(n log n)\n\n\nWorst\nUnlucky (rare)\nO(n²)\n\n\n\n\n\nTry It Yourself\n\nSort [5, 3, 4, 1, 2] multiple times, note pivot differences.\nPrint pivot each recursive call.\nCompare against deterministic pivot (first, last).\nTest on sorted input [1, 2, 3, 4, 5].\nTest on reverse input [5, 4, 3, 2, 1].\nCount recursive depth across runs.\nModify to use Hoare partition.\nImplement random.choice() version.\nCompare runtime vs. normal Quick Sort.\nSeed RNG to reproduce same run.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nRandom pivot each run\n\n\n[1, 2, 3]\n[1, 2, 3]\nAvoids O(n²)\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nHandles duplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n²) (rare)\n\n\nSpace\nO(log n)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nRandomized Quick Sort shows the power of randomness, a tiny change transforms a fragile algorithm into a reliably fast one, making it one of the most practical sorts in modern computing.\n\n\n\n117 Heap Sort\nHeap Sort is a classic comparison-based, in-place, O(n log n) sorting algorithm built upon the heap data structure. It first turns the array into a max-heap, then repeatedly removes the largest element (the heap root) and places it at the end, shrinking the heap as it goes. It’s efficient and memory-friendly but not stable.\n\nWhat Problem Are We Solving??\nWe want a sorting algorithm that:\n\nHas guaranteed O(n log n) time in all cases\nUses constant extra space (O(1))\nDoesn’t require recursion or extra arrays like Merge Sort\n\nHeap Sort achieves this by using a binary heap to always extract the largest element efficiently, then placing it in its correct position at the end.\nPerfect for:\n\nMemory-constrained systems\nPredictable performance needs\nOffline sorting when data fits in RAM\n\n\n\nExample\n\n\n\nStep\nAction\nArray State\nHeap Size\n\n\n\n\n0\nBuild max-heap\n[5, 3, 4, 1, 2]\n5\n\n\n1\nSwap root with last\n[2, 3, 4, 1, 5]\n4\n\n\n2\nHeapify root\n[4, 3, 2, 1, 5]\n4\n\n\n3\nSwap root with last\n[1, 3, 2, 4, 5]\n3\n\n\n4\nHeapify root\n[3, 1, 2, 4, 5]\n3\n\n\n5\nRepeat until heap shrinks\n[1, 2, 3, 4, 5]\n0\n\n\n\n\n\nHow Does It Work (Plain Language)?\nThink of a heap like a tree stored in an array. The root (index 0) is the largest element. Heap Sort works in two main steps:\n\nBuild a max-heap (arranged so every parent ≥ its children)\nExtract max repeatedly:\n\nSwap root with last element\nReduce heap size\nHeapify root to restore max-heap property\n\n\nAfter each extraction, the sorted part grows at the end of the array.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nBuild a max-heap from the array\n\n\n2\nSwap the first (max) with the last element\n\n\n3\nReduce heap size by one\n\n\n4\nHeapify the root\n\n\n5\nRepeat until heap is empty\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n}\n\nvoid heapify(int a[], int n, int i) {\n    int largest = i;\n    int left = 2 * i + 1;\n    int right = 2 * i + 2;\n\n    if (left &lt; n && a[left] &gt; a[largest]) largest = left;\n    if (right &lt; n && a[right] &gt; a[largest]) largest = right;\n\n    if (largest != i) {\n        swap(&a[i], &a[largest]);\n        heapify(a, n, largest);\n    }\n}\n\nvoid heap_sort(int a[], int n) {\n    // Build max heap\n    for (int i = n / 2 - 1; i &gt;= 0; i--)\n        heapify(a, n, i);\n\n    // Extract elements from heap\n    for (int i = n - 1; i &gt; 0; i--) {\n        swap(&a[0], &a[i]);\n        heapify(a, i, 0);\n    }\n}\n\nint main(void) {\n    int a[] = {5, 3, 4, 1, 2};\n    int n = sizeof(a) / sizeof(a[0]);\n    heap_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef heapify(a, n, i):\n    largest = i\n    left = 2 * i + 1\n    right = 2 * i + 2\n    if left &lt; n and a[left] &gt; a[largest]:\n        largest = left\n    if right &lt; n and a[right] &gt; a[largest]:\n        largest = right\n    if largest != i:\n        a[i], a[largest] = a[largest], a[i]\n        heapify(a, n, largest)\n\ndef heap_sort(a):\n    n = len(a)\n    # Build max heap\n    for i in range(n // 2 - 1, -1, -1):\n        heapify(a, n, i)\n    # Extract max and heapify\n    for i in range(n - 1, 0, -1):\n        a[0], a[i] = a[i], a[0]\n        heapify(a, i, 0)\n\narr = [5, 3, 4, 1, 2]\nheap_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nPredictable O(n log n) in all cases\nIn-place, no extra memory needed\nExcellent when memory is tight or recursion is not preferred\nDemonstrates tree-based sorting logic\n\n\n\nA Gentle Proof (Why It Works)\nBuilding the heap: O(n) Extracting each element: O(log n) Total time:\n\\[\nT(n) = O(n) + n \\times O(\\log n) = O(n \\log n)\n\\]\nEach element is “bubbled down” log n levels at most once.\n\n\n\nPhase\nWork\nTotal\n\n\n\n\nBuild heap\nO(n)\nLinear\n\n\nExtract n elements\nn × O(log n)\nO(n log n)\n\n\n\nNot stable, because swapping can break equal-element order.\n\n\nTry It Yourself\n\nBuild max-heap from [5, 3, 4, 1, 2].\nDraw heap tree for each step.\nTrace heapify calls and swaps.\nImplement min-heap version for descending sort.\nCount comparisons per phase.\nCompare with Merge Sort space usage.\nModify to stop early if already sorted.\nAnimate heap construction.\nTest on reverse array [5,4,3,2,1].\nAdd debug prints showing heap after each step.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 1]\n[1, 2, 3]\nSmall test\n\n\n[1, 2, 3]\n[1, 2, 3]\nAlready sorted\n\n\n[5, 3, 4, 1, 2]\n[1, 2, 3, 4, 5]\nGeneral case\n\n\n[2, 2, 1, 1]\n[1, 1, 2, 2]\nNot stable but correct\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nHeap Sort is the workhorse of guaranteed performance, steady, space-efficient, and built on elegant tree logic. It never surprises you with bad cases, a reliable friend when consistency matters.\n\n\n\n118 3-Way Quick Sort\n3-Way Quick Sort is a refined version of Quick Sort designed to handle arrays with many duplicate elements efficiently. Instead of dividing the array into two parts (less than and greater than pivot), it divides into three regions:\n\n&lt; pivot\n= pivot\n&gt; pivot\n\nThis avoids redundant work when many elements are equal to the pivot, making it especially effective for datasets with low entropy or repeated keys.\n\nWhat Problem Are We Solving??\nStandard Quick Sort can perform unnecessary work when duplicates are present. For example, if all elements are the same, standard Quick Sort still recurses O(n log n) times.\n3-Way Quick Sort fixes this by:\n\nSkipping equal elements during partitioning\nShrinking recursion depth dramatically\n\nIt’s ideal for:\n\nArrays with many duplicates\nSorting strings with common prefixes\nKey-value pairs with repeated keys\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStep\nPivot\nAction\nArray State\nPartitions\n\n\n\n\n0\n3\nStart\n[3, 2, 3, 1, 3]\nl=0, i=0, g=4\n\n\n1\n3\na[i]=3 → equal\n[3, 2, 3, 1, 3]\ni=1\n\n\n2\n3\na[i]=2 &lt; 3 → swap(a[l],a[i])\n[2, 3, 3, 1, 3]\nl=1, i=2\n\n\n3\n3\na[i]=3 → equal\n[2, 3, 3, 1, 3]\ni=3\n\n\n4\n3\na[i]=1 &lt; 3 → swap(a[l],a[i])\n[2, 1, 3, 3, 3]\nl=2, i=4\n\n\n5\n3\na[i]=3 → equal\n[2, 1, 3, 3, 3]\nDone\n\n\n\nNow recursively sort left &lt; pivot region [2,1], skip the middle =3 block, and sort right &gt; pivot (empty).\n\n\nHow Does It Work (Plain Language)?\nWe track three zones using three pointers:\n\nlt (less than region)\ni (current element)\ngt (greater than region)\n\nEach iteration compares a[i] with the pivot:\n\nIf &lt; pivot: swap with lt, expand both regions\nIf &gt; pivot: swap with gt, shrink right region\nIf = pivot: move forward\n\nContinue until i &gt; gt. This single pass partitions array into three regions, no need to revisit equals.\n\n\nStep-by-Step Process\n\n\n\nStep\nCondition\nAction\n\n\n\n\n1\na[i] &lt; pivot\nswap(a[i], a[lt]), i++, lt++\n\n\n2\na[i] &gt; pivot\nswap(a[i], a[gt]), gt–\n\n\n3\na[i] == pivot\ni++\n\n\n4\nStop when i &gt; gt\n\n\n\n\nThen recursively sort [low..lt-1] and [gt+1..high].\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int temp = *a;\n    *a = *b;\n    *b = temp;\n}\n\nvoid quicksort_3way(int a[], int low, int high) {\n    if (low &gt;= high) return;\n\n    int pivot = a[low];\n    int lt = low, i = low, gt = high;\n\n    while (i &lt;= gt) {\n        if (a[i] &lt; pivot) {\n            swap(&a[lt], &a[i]);\n            lt++; i++;\n        } else if (a[i] &gt; pivot) {\n            swap(&a[i], &a[gt]);\n            gt--;\n        } else {\n            i++;\n        }\n    }\n\n    quicksort_3way(a, low, lt - 1);\n    quicksort_3way(a, gt + 1, high);\n}\n\nint main(void) {\n    int a[] = {3, 2, 3, 1, 3};\n    int n = sizeof(a) / sizeof(a[0]);\n    quicksort_3way(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef quicksort_3way(a, low, high):\n    if low &gt;= high:\n        return\n    pivot = a[low]\n    lt, i, gt = low, low, high\n    while i &lt;= gt:\n        if a[i] &lt; pivot:\n            a[lt], a[i] = a[i], a[lt]\n            lt += 1\n            i += 1\n        elif a[i] &gt; pivot:\n            a[i], a[gt] = a[gt], a[i]\n            gt -= 1\n        else:\n            i += 1\n    quicksort_3way(a, low, lt - 1)\n    quicksort_3way(a, gt + 1, high)\n\narr = [3, 2, 3, 1, 3]\nquicksort_3way(arr, 0, len(arr) - 1)\nprint(arr)\n\n\n\nWhy It Matters\n\nEfficient for arrays with duplicates\nReduces unnecessary recursion and comparisons\nUsed in string sorting and key-heavy data\nGeneralizes the idea of “partition” to multi-way splitting\n\n\n\nA Gentle Proof (Why It Works)\nStandard Quick Sort always divides into two regions, even if all elements equal the pivot, leading to O(n²) on identical elements.\n3-Way Quick Sort partitions into three zones:\n\n&lt; pivot (left)\n= pivot (middle)\n&gt; pivot (right)\n\nThe middle zone is skipped from recursion, reducing work dramatically.\nIf all elements are equal → only one pass O(n).\nExpected complexity: \\[\nT(n) = O(n \\log n)\n\\] Worst-case (no duplicates): same as Quick Sort.\n\n\n\nCase\nDuplicates\nComplexity\n\n\n\n\nAll equal\nO(n)\nOne pass only\n\n\nMany\nO(n log n)\nEfficient\n\n\nNone\nO(n log n)\nNormal behavior\n\n\n\n\n\nTry It Yourself\n\nSort [3, 2, 3, 1, 3] step by step.\nPrint regions (lt, i, gt) after each iteration.\nCompare recursion depth with normal Quick Sort.\nTest input [1, 1, 1, 1].\nTest input [5, 4, 3, 2, 1].\nSort [“apple”, “apple”, “banana”, “apple”].\nVisualize partitions on paper.\nModify to count swaps and comparisons.\nImplement descending order.\nApply to random integers with duplicates (e.g. [1,2,2,2,3,3,1]).\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3, 2, 3, 1, 3]\n[1, 2, 3, 3, 3]\nDuplicates\n\n\n[5, 4, 3, 2, 1]\n[1, 2, 3, 4, 5]\nNo duplicates\n\n\n[2, 2, 2, 2]\n[2, 2, 2, 2]\nAll equal (O(n))\n\n\n[1, 3, 1, 3, 1]\n[1, 1, 1, 3, 3]\nClustered duplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n) (all equal)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(log n) recursion\n\n\nStable\nNo\n\n\nAdaptive\nYes (handles duplicates efficiently)\n\n\n\n3-Way Quick Sort shows how a small change, three-way partitioning, can transform Quick Sort into a powerful tool for duplicate-heavy datasets, blending elegance with efficiency.\n\n\n\n119 External Merge Sort\nExternal Merge Sort is a specialized sorting algorithm designed for very large datasets that don’t fit entirely into main memory (RAM). It works by sorting chunks of data in memory, writing them to disk, and then merging those sorted chunks. This makes it a key tool in databases, file systems, and big data processing.\n\nWhat Problem Are We Solving??\nWhen data exceeds RAM capacity, in-memory sorts like Quick Sort or Heap Sort fail, they need random access to all elements. External Merge Sort solves this by processing data in blocks:\n\nSort manageable chunks in memory\nWrite sorted chunks (“runs”) to disk\nMerge runs sequentially using streaming I/O\n\nThis minimizes disk reads/writes, the main bottleneck in large-scale sorting.\nIt’s ideal for:\n\nLarge files (GBs to TBs)\nDatabase query engines\nBatch processing pipelines\n\n\n\nExample\nLet’s say you have 1 GB of data and only 100 MB of RAM.\n\n\n\nStep\nAction\nDescription\n\n\n\n\n1\nSplit\nDivide file into 10 chunks of 100 MB\n\n\n2\nSort\nLoad each chunk in memory, sort, write to disk\n\n\n3\nMerge\nUse k-way merge (e.g. 10-way) to merge sorted runs\n\n\n4\nOutput\nFinal sorted file written sequentially\n\n\n\n\n\nHow Does It Work (Plain Language)?\nThink of it like sorting pages of a giant book:\n\nTake a few pages at a time (fit in memory)\nSort them and place them in order piles\nCombine the piles in order until the whole book is sorted\n\nIt’s a multi-pass algorithm:\n\nPass 1: Create sorted runs\nPass 2+: Merge runs in multiple passes until one remains\n\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nDivide the large file into blocks fitting memory\n\n\n2\nLoad a block, sort it using in-memory sort\n\n\n3\nWrite each sorted block (run) to disk\n\n\n4\nMerge all runs using k-way merging\n\n\n5\nRepeat merges until a single sorted file remains\n\n\n\n\n\nTiny Code (Simplified Simulation)\n\n\nPython (Simulated External Sort)\nimport heapq\nimport tempfile\n\ndef sort_chunk(chunk):\n    chunk.sort()\n    temp = tempfile.TemporaryFile(mode=\"w+t\")\n    temp.writelines(f\"{x}\\n\" for x in chunk)\n    temp.seek(0)\n    return temp\n\ndef merge_files(files, output_file):\n    iters = [map(int, f) for f in files]\n    with open(output_file, \"w\") as out:\n        for num in heapq.merge(*iters):\n            out.write(f\"{num}\\n\")\n\ndef external_merge_sort(input_data, chunk_size=5):\n    chunks = []\n    for i in range(0, len(input_data), chunk_size):\n        chunk = input_data[i:i + chunk_size]\n        chunks.append(sort_chunk(chunk))\n    merge_files(chunks, \"sorted_output.txt\")\n\ndata = [42, 17, 93, 8, 23, 4, 16, 99, 55, 12, 71, 3]\nexternal_merge_sort(data, chunk_size=4)\nThis example simulates external sorting in Python, splitting input into chunks, sorting each, and merging with heapq.merge.\n\n\nWhy It Matters\n\nHandles massive datasets beyond memory limits\nSequential disk I/O (fast and predictable)\nFoundation of database sort-merge joins\nWorks well with distributed systems (MapReduce, Spark)\n\n\n\nA Gentle Proof (Why It Works)\nEach pass performs O(n) work to read and write the entire dataset. If r is the number of runs, and k is merge fan-in (number of runs merged at once):\n\\[\n\\text{Number of passes} = \\lceil \\log_k r \\rceil\n\\]\nTotal cost ≈ \\[\nO(n \\log_k r)\n\\] dominated by I/O operations rather than comparisons.\nFor r = n/M (chunks of memory size M), performance is optimized by choosing k ≈ M.\n\n\n\nPhase\nWork\nCost\n\n\n\n\nCreate Runs\nO(n log M)\nSort chunks\n\n\nMerge Runs\nO(n log_k r)\nMerge passes\n\n\n\n\n\nTry It Yourself\n\nSplit [42, 17, 93, 8, 23, 4, 16, 99, 55, 12, 71, 3] into 4-element chunks.\nSort each chunk individually.\nSimulate merging sorted runs.\nTry merging 2-way vs 4-way, count passes.\nVisualize merging tree (runs combining).\nTest with random large arrays (simulate files).\nModify chunk size and observe performance.\nCompare I/O counts with in-memory sort.\nUse heapq.merge to merge sorted streams.\nExtend to merge files on disk (not just lists).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nInput\nMemory Limit\nOutput\nNotes\n\n\n\n\n[9, 4, 7, 2, 5, 1, 8, 3, 6]\n3 elements\n[1,2,3,4,5,6,7,8,9]\n3-way merge\n\n\n1 GB integers\n100 MB\nSorted file\n10 sorted runs\n\n\n[1,1,1,1,1]\nsmall\n[1,1,1,1,1]\nHandles duplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n log_k (n/M))\n\n\nSpace\nO(M) (memory buffer)\n\n\nI/O Passes\nO(log_k (n/M))\n\n\nStable\nYes\n\n\nAdaptive\nYes (fewer runs if data partially sorted)\n\n\n\nExternal Merge Sort is the unsung hero behind large-scale sorting, when memory ends, it steps in with disk-based precision, keeping order across terabytes with calm efficiency.\n\n\n\n120 Parallel Merge Sort\nParallel Merge Sort takes the familiar divide-and-conquer structure of Merge Sort and spreads the work across multiple threads or processors, achieving faster sorting on multi-core CPUs or distributed systems. It’s an ideal illustration of how parallelism can amplify a classic algorithm without changing its logic.\n\nWhat Problem Are We Solving??\nTraditional Merge Sort runs sequentially, so even though its complexity is O(n log n), it uses only one CPU core. On modern hardware with many cores, that’s a waste.\nParallel Merge Sort tackles this by:\n\nSorting subarrays in parallel\nMerging results concurrently\nUtilizing full CPU or cluster potential\n\nIt’s essential for:\n\nHigh-performance computing\nLarge-scale sorting\nReal-time analytics pipelines\n\n\n\nExample\nSort [5, 3, 4, 1, 2] using 2 threads:\n\n\n\n\n\n\n\n\n\nStep\nAction\nThreads\nResult\n\n\n\n\n1\nSplit array into halves\n2 threads\n[5, 3, 4], [1, 2]\n\n\n2\nSort each half concurrently\nT1: sort [5,3,4], T2: sort [1,2]\n[3,4,5], [1,2]\n\n\n3\nMerge results\n1 thread\n[1,2,3,4,5]\n\n\n\nParallelism reduces total time roughly by 1 / number of threads (with overhead).\n\n\nHow Does It Work (Plain Language)?\nIt’s still divide and conquer, just with teamwork:\n\nSplit array into two halves.\nSort each half in parallel.\nMerge the two sorted halves.\nStop splitting when subarrays are small (then sort sequentially).\n\nEach recursive level can launch new threads until you reach a threshold or maximum depth.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nDivide the array into halves\n\n\n2\nSort both halves concurrently\n\n\n3\nWait for both to finish\n\n\n4\nMerge results sequentially\n\n\n5\nRepeat recursively for subparts\n\n\n\nThis pattern fits well with thread pools, task schedulers, or fork-join frameworks.\n\n\nTiny Code (Easy Versions)\n\n\nC (POSIX Threads Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n\ntypedef struct {\n    int *arr;\n    int left;\n    int right;\n} Args;\n\nvoid merge(int arr[], int l, int m, int r) {\n    int n1 = m - l + 1, n2 = r - m;\n    int L[n1], R[n2];\n    for (int i = 0; i &lt; n1; i++) L[i] = arr[l + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = arr[m + 1 + j];\n\n    int i = 0, j = 0, k = l;\n    while (i &lt; n1 && j &lt; n2) {\n        arr[k++] = (L[i] &lt;= R[j]) ? L[i++] : R[j++];\n    }\n    while (i &lt; n1) arr[k++] = L[i++];\n    while (j &lt; n2) arr[k++] = R[j++];\n}\n\nvoid *parallel_merge_sort(void *arg) {\n    Args *args = (Args *)arg;\n    int l = args-&gt;left, r = args-&gt;right;\n    int *arr = args-&gt;arr;\n\n    if (l &lt; r) {\n        int m = l + (r - l) / 2;\n\n        Args leftArgs = {arr, l, m};\n        Args rightArgs = {arr, m + 1, r};\n        pthread_t leftThread, rightThread;\n\n        pthread_create(&leftThread, NULL, parallel_merge_sort, &leftArgs);\n        pthread_create(&rightThread, NULL, parallel_merge_sort, &rightArgs);\n\n        pthread_join(leftThread, NULL);\n        pthread_join(rightThread, NULL);\n\n        merge(arr, l, m, r);\n    }\n    return NULL;\n}\n\nint main(void) {\n    int arr[] = {5, 3, 4, 1, 2};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    Args args = {arr, 0, n - 1};\n    parallel_merge_sort(&args);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n(Note: This simple version may create too many threads; real implementations limit thread depth.)\n\n\nPython (Using multiprocessing)\nfrom multiprocessing import Pool\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt; right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    result.extend(left[i:])\n    result.extend(right[j:])\n    return result\n\ndef parallel_merge_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    if len(arr) &lt; 1000:  # threshold\n        return sorted(arr)\n    mid = len(arr) // 2\n    with Pool(2) as p:\n        left, right = p.map(parallel_merge_sort, [arr[:mid], arr[mid:]])\n    return merge(left, right)\n\narr = [5, 3, 4, 1, 2]\nprint(parallel_merge_sort(arr))\n\n\nWhy It Matters\n\nExploits multi-core architectures\nSignificantly reduces wall-clock time\nMaintains O(n log n) work\nGreat showcase of parallel divide-and-conquer\n\nUsed in:\n\nHPC (High Performance Computing)\nModern standard libraries (std::execution::par)\nBig data frameworks (Spark, Hadoop)\n\n\n\nA Gentle Proof (Why It Works)\nEach recursive call sorts n/2 elements, but now in parallel. Let P = number of processors.\nWork (total operations): \\[\nT_{work}(n) = O(n \\log n)\n\\] Span (critical path time): \\[\nT_{span}(n) = O(\\log^2 n)\n\\]\nTotal time ≈ \\[\nO\\left(\\frac{n \\log n}{P} + \\log^2 n\\right)\n\\]\nSpeedup ≈ P×, limited by synchronization and merge overhead.\n\n\n\nPhase\nParallelizable\nWork\n\n\n\n\nSort subarrays\nYes\nO(n log n / P)\n\n\nMerge\nPartially\nO(n log P)\n\n\n\n\n\nTry It Yourself\n\nRun with 1, 2, 4, 8 threads, compare speed.\nPrint thread IDs at each recursive call.\nImplement threshold for small subarrays.\nMerge using parallel merging.\nMeasure CPU utilization during sort.\nTest with large random list (10⁶ elements).\nCompare with sequential Merge Sort.\nProfile with timing tools.\nTry OpenMP version in C.\nExtend to distributed nodes (MPI).\n\n\n\nTest Cases\n\n\n\nInput\nThreads\nOutput\nNotes\n\n\n\n\n[3,2,1]\n2\n[1,2,3]\nSimple\n\n\n[5,3,4,1,2]\n2\n[1,2,3,4,5]\nBalanced work\n\n\n1e6 random ints\n8\nsorted\nParallel boost\n\n\n[1,1,1,1]\n4\n[1,1,1,1]\nStable behavior\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nWork\nO(n log n)\n\n\nSpan\nO(log² n)\n\n\nParallel Time\nO(n log n / P + log² n)\n\n\nSpace\nO(n)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\n\nParallel Merge Sort is Merge Sort reborn for the multi-core era, the same elegance, now with teamwork. It’s how classic algorithms learn to scale with hardware.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-13.-counting-and-distribution-sorts",
    "href": "books/en-us/list-2.html#section-13.-counting-and-distribution-sorts",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 13. Counting and distribution sorts",
    "text": "Section 13. Counting and distribution sorts\n\n121 Counting Sort\nCounting Sort is a non-comparison sorting algorithm that sorts integers (or items mapped to integer keys) by counting occurrences of each value. Instead of comparing elements, it directly uses their values as indices in a counting array. It’s fast (O(n + k)), stable, and perfect when the input range is limited and small.\n\nWhat Problem Are We Solving??\nWhen keys are integers within a known range, comparison-based sorts (O(n log n)) are overkill. Counting Sort leverages that limited range to sort in linear time, without any comparisons.\nPerfect for:\n\nSorting grades (0–100)\nSorting digits or ASCII codes (0–255)\nPre-step for Radix Sort or Bucket Sort\nScenarios where key range ≪ n²\n\n\n\nExample\nSort array [4, 2, 2, 8, 3, 3, 1]\n\n\n\n\n\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nFind max = 8\nRange = 0–8\n\n\n2\nInitialize count[9] = [0,0,0,0,0,0,0,0,0]\n\n\n\n3\nCount each number\ncount = [0,1,2,2,1,0,0,0,1]\n\n\n4\nPrefix sum (positions)\ncount = [0,1,3,5,6,6,6,6,7]\n\n\n5\nPlace elements by count\n[1,2,2,3,3,4,8]\n\n\n\nThe count array tracks the position boundaries for each key.\n\n\nHow Does It Work (Plain Language)?\nCounting Sort doesn’t compare elements. It counts how many times each value appears, then uses those counts to reconstruct the sorted list.\nThink of it as filling labeled bins:\n\nOne bin for each number\nDrop each element into its bin\nThen walk through bins in order and empty them\n\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind min and max (determine range k)\n\n\n2\nCreate count array of size k + 1\n\n\n3\nCount occurrences of each value\n\n\n4\nTransform counts into prefix sums (for positions)\n\n\n5\nTraverse input in reverse (for stability), placing elements\n\n\n6\nCopy sorted output back\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nvoid counting_sort(int arr[], int n) {\n    int max = arr[0];\n    for (int i = 1; i &lt; n; i++)\n        if (arr[i] &gt; max) max = arr[i];\n\n    int count[max + 1];\n    memset(count, 0, sizeof(count));\n\n    for (int i = 0; i &lt; n; i++)\n        count[arr[i]]++;\n\n    for (int i = 1; i &lt;= max; i++)\n        count[i] += count[i - 1];\n\n    int output[n];\n    for (int i = n - 1; i &gt;= 0; i--) {\n        output[count[arr[i]] - 1] = arr[i];\n        count[arr[i]]--;\n    }\n\n    for (int i = 0; i &lt; n; i++)\n        arr[i] = output[i];\n}\n\nint main(void) {\n    int arr[] = {4, 2, 2, 8, 3, 3, 1};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    counting_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef counting_sort(arr):\n    max_val = max(arr)\n    count = [0] * (max_val + 1)\n\n    for num in arr:\n        count[num] += 1\n\n    for i in range(1, len(count)):\n        count[i] += count[i - 1]\n\n    output = [0] * len(arr)\n    for num in reversed(arr):\n        count[num] -= 1\n        output[count[num]] = num\n\n    return output\n\narr = [4, 2, 2, 8, 3, 3, 1]\nprint(counting_sort(arr))\n\n\n\nWhy It Matters\n\nLinear time when range is small (O(n + k))\nStable, preserving input order\nFoundation for Radix Sort\nGreat for integer, digit, or bucket sorting\n\n\n\nA Gentle Proof (Why It Works)\nCounting Sort replaces comparison by index-based placement.\nIf n is number of elements and k is key range:\n\nCounting occurrences: O(n)\nPrefix sums: O(k)\nPlacement: O(n)\n\nTotal = O(n + k)\nStable because we traverse input in reverse while placing.\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nCount elements\nO(n)\nScan once\n\n\nPrefix sum\nO(k)\nRange pass\n\n\nPlace elements\nO(n)\nStable write\n\n\n\n\n\nTry It Yourself\n\nSort [4, 2, 2, 8, 3, 3, 1] step by step.\nShow count array after counting.\nConvert count to prefix sums.\nPlace elements in output (reverse scan).\nCompare stable vs unstable version.\nChange input to [9, 9, 1, 2].\nTry sorting [5, 3, 5, 1, 0].\nHandle input with min &gt; 0 (offset counts).\nMeasure runtime vs Bubble Sort.\nUse as subroutine in Radix Sort.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[4,2,2,8,3,3,1]\n[1,2,2,3,3,4,8]\nExample\n\n\n[1,4,1,2,7,5,2]\n[1,1,2,2,4,5,7]\nStable\n\n\n[9,9,9,9]\n[9,9,9,9]\nRepeats\n\n\n[0,1,2,3]\n[0,1,2,3]\nAlready sorted\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n + k)\n\n\nSpace\nO(n + k)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nRange-sensitive\nYes\n\n\n\nCounting Sort is like sorting by bins, no comparisons, no stress, just clean counts and linear time. It’s a powerhouse behind Radix Sort and data bucketing in performance-critical pipelines.\n\n\n\n122 Stable Counting Sort\nStable Counting Sort refines the basic Counting Sort by ensuring equal elements preserve their original order. This property, called stability, is crucial when sorting multi-key data, for example, sorting people by age, then by name. Stable versions are also the building blocks for Radix Sort, where each digit’s sort depends on stability.\n\nWhat Problem Are We Solving??\nBasic Counting Sort can break order among equal elements because it places them in arbitrary order. When sorting records or tuples where order matters (e.g., by secondary key), we need stability, if a and b have equal keys, their order in output must match input.\nStable Counting Sort ensures that:\n\nIf arr[i] and arr[j] have the same key and i &lt; j, then arr[i] appears before arr[j] in the sorted output.\n\nPerfect for:\n\nRadix Sort digits\nMulti-field records (e.g. sort by name, then by score)\nDatabases and stable pipelines\n\n\n\nExample\nSort [4a, 2b, 2a, 8a, 3b, 3a, 1a] (letters mark order)\n\n\n\n\n\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nCount frequencies\ncount = [0,1,2,2,1,0,0,0,1]\n\n\n2\nPrefix sums (positions)\ncount = [0,1,3,5,6,6,6,6,7]\n\n\n3\nTraverse input in reverse\noutput = [1a, 2b, 2a, 3b, 3a, 4a, 8a]\n\n\n\nSee how 2b (index 1) appears before 2a (index 2), stable ordering preserved.\n\n\nHow Does It Work (Plain Language)?\nIt’s Counting Sort with a twist: we fill the output from the end of the input, ensuring last-seen equal items go last. By traversing in reverse, earlier elements are placed later, preserving their original order.\nThis is the core idea behind stable sorting.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nDetermine key range (0..max)\n\n\n2\nCount frequency of each key\n\n\n3\nCompute prefix sums (to determine positions)\n\n\n4\nTraverse input right to left\n\n\n5\nPlace elements in output using count as index\n\n\n6\nDecrement count[key] after placement\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\ntypedef struct {\n    int key;\n    char tag; // to visualize stability\n} Item;\n\nvoid stable_counting_sort(Item arr[], int n) {\n    int max = arr[0].key;\n    for (int i = 1; i &lt; n; i++)\n        if (arr[i].key &gt; max) max = arr[i].key;\n\n    int count[max + 1];\n    memset(count, 0, sizeof(count));\n\n    // Count occurrences\n    for (int i = 0; i &lt; n; i++)\n        count[arr[i].key]++;\n\n    // Prefix sums\n    for (int i = 1; i &lt;= max; i++)\n        count[i] += count[i - 1];\n\n    Item output[n];\n\n    // Traverse input in reverse for stability\n    for (int i = n - 1; i &gt;= 0; i--) {\n        int k = arr[i].key;\n        output[count[k] - 1] = arr[i];\n        count[k]--;\n    }\n\n    for (int i = 0; i &lt; n; i++)\n        arr[i] = output[i];\n}\n\nint main(void) {\n    Item arr[] = {{4,'a'},{2,'b'},{2,'a'},{8,'a'},{3,'b'},{3,'a'},{1,'a'}};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    stable_counting_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"(%d,%c) \", arr[i].key, arr[i].tag);\n    printf(\"\\n\");\n}\n\n\nPython\ndef stable_counting_sort(arr):\n    max_val = max(arr)\n    count = [0] * (max_val + 1)\n\n    for num in arr:\n        count[num] += 1\n\n    for i in range(1, len(count)):\n        count[i] += count[i - 1]\n\n    output = [0] * len(arr)\n    for num in reversed(arr):\n        count[num] -= 1\n        output[count[num]] = num\n\n    return output\n\narr = [4, 2, 2, 8, 3, 3, 1]\nprint(stable_counting_sort(arr))\n\n\n\nWhy It Matters\n\nStable sorting is essential for multi-key operations\nRequired for Radix Sort correctness\nGuarantees consistent behavior for duplicates\nUsed in databases, language sort libraries, pipelines\n\n\n\nA Gentle Proof (Why It Works)\nEach key is assigned a position range via prefix sums. Traversing input from right to left ensures that earlier items occupy smaller indices, preserving order.\nIf a appears before b in input and key(a) = key(b), then count[key(a)] places a before b, stable.\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nCounting\nO(n)\nPass once\n\n\nPrefix sums\nO(k)\nRange pass\n\n\nPlacement\nO(n)\nReverse traversal\n\n\n\nTotal = O(n + k), same as basic Counting Sort, but stable.\n\n\nTry It Yourself\n\nSort [(4,'a'), (2,'b'), (2,'a'), (3,'a')].\nShow count and prefix arrays.\nTraverse input from end, track output.\nCompare with unstable version.\nTry sorting [5,3,5,1,0].\nVisualize stability when equal keys appear.\nModify to handle offset keys (negative values).\nCombine with Radix Sort (LSD).\nProfile runtime vs normal Counting Sort.\nCheck stability by adding tags (letters).\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[4,2,2,8,3,3,1]\n[1,2,2,3,3,4,8]\nSame as basic\n\n\n[(2,‘a’),(2,‘b’)]\n[(2,‘a’),(2,‘b’)]\nStable preserved\n\n\n[1,1,1]\n[1,1,1]\nIdempotent\n\n\n[0,1,2,3]\n[0,1,2,3]\nAlready sorted\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n + k)\n\n\nSpace\nO(n + k)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nRange-sensitive\nYes\n\n\n\nStable Counting Sort is Counting Sort with memory, it not only sorts fast but also remembers your order, making it indispensable for multi-pass algorithms like Radix Sort.\n\n\n\n123 Radix Sort (LSD)\nRadix Sort (Least Significant Digit first) is a non-comparison, stable sorting algorithm that processes integers (or strings) digit by digit, starting from the least significant digit (LSD). By repeatedly applying a stable sort (like Counting Sort) on each digit, it can sort numbers in linear time when digit count is small.\n\nWhat Problem Are We Solving??\nWhen sorting integers or fixed-length keys (like dates, IDs, or strings of digits), traditional comparison-based sorts spend unnecessary effort. Radix Sort (LSD) sidesteps comparisons by leveraging digit-wise order and stability to achieve O(d × (n + k)) performance.\nPerfect for:\n\nSorting numbers, dates, zip codes, or strings\nDatasets with bounded digit length\nApplications where deterministic performance matters\n\n\n\nExample\nSort [170, 45, 75, 90, 802, 24, 2, 66]\n\n\n\n\n\n\n\n\n\nPass\nDigit Place\nInput\nOutput (Stable Sort)\n\n\n\n\n1\nOnes\n[170,45,75,90,802,24,2,66]\n[170,90,802,2,24,45,75,66]\n\n\n2\nTens\n[170,90,802,2,24,45,75,66]\n[802,2,24,45,66,170,75,90]\n\n\n3\nHundreds\n[802,2,24,45,66,170,75,90]\n[2,24,45,66,75,90,170,802]\n\n\n\nFinal sorted output: [2, 24, 45, 66, 75, 90, 170, 802]\nEach pass uses Stable Counting Sort on the current digit.\n\n\nHow Does It Work (Plain Language)?\nThink of sorting by digit positions:\n\nGroup by ones place (units)\nGroup by tens\nGroup by hundreds, etc.\n\nEach pass reorders elements according to the digit at that place, while keeping earlier digit orders intact (thanks to stability).\nIt’s like sorting by last name, then first name, one field at a time, stable each round.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind the maximum number to know the number of digits d\n\n\n2\nFor each digit place (1, 10, 100, …):\n\n\n3\nUse a stable counting sort based on that digit\n\n\n4\nAfter the last pass, the array is fully sorted\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nint get_max(int a[], int n) {\n    int max = a[0];\n    for (int i = 1; i &lt; n; i++)\n        if (a[i] &gt; max) max = a[i];\n    return max;\n}\n\nvoid counting_sort_digit(int a[], int n, int exp) {\n    int output[n];\n    int count[10] = {0};\n\n    for (int i = 0; i &lt; n; i++)\n        count[(a[i] / exp) % 10]++;\n\n    for (int i = 1; i &lt; 10; i++)\n        count[i] += count[i - 1];\n\n    for (int i = n - 1; i &gt;= 0; i--) {\n        int digit = (a[i] / exp) % 10;\n        output[count[digit] - 1] = a[i];\n        count[digit]--;\n    }\n\n    for (int i = 0; i &lt; n; i++)\n        a[i] = output[i];\n}\n\nvoid radix_sort(int a[], int n) {\n    int max = get_max(a, n);\n    for (int exp = 1; max / exp &gt; 0; exp *= 10)\n        counting_sort_digit(a, n, exp);\n}\n\nint main(void) {\n    int a[] = {170, 45, 75, 90, 802, 24, 2, 66};\n    int n = sizeof(a) / sizeof(a[0]);\n    radix_sort(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef counting_sort_digit(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    for num in arr:\n        index = (num // exp) % 10\n        count[index] += 1\n\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    for num in reversed(arr):\n        index = (num // exp) % 10\n        count[index] -= 1\n        output[count[index]] = num\n\n    for i in range(n):\n        arr[i] = output[i]\n\ndef radix_sort(arr):\n    max_val = max(arr)\n    exp = 1\n    while max_val // exp &gt; 0:\n        counting_sort_digit(arr, exp)\n        exp *= 10\n\narr = [170, 45, 75, 90, 802, 24, 2, 66]\nradix_sort(arr)\nprint(arr)\n\n\n\nWhy It Matters\n\nLinear time (O(d × (n + k))) for fixed digits\nStable, retains order for equal keys\nGreat for large numeric datasets\nFoundation for efficient key-based sorting (strings, dates)\n\n\n\nA Gentle Proof (Why It Works)\nAt each digit position:\n\nStable Counting Sort reorders by that digit\nEarlier digits remain ordered (stability)\nAfter all digits, array is fully ordered\n\nIf each digit has range k and d total digits:\n\\[\nT(n) = O(d \\times (n + k))\n\\]\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nPer digit\nO(n + k)\nCounting sort\n\n\nAll digits\nd × O(n + k)\nTotal\n\n\n\nIf d and k are constants → O(n) overall.\n\n\nTry It Yourself\n\nSort [170, 45, 75, 90, 802, 24, 2, 66].\nTrace each pass (ones, tens, hundreds).\nShow count table per digit.\nCompare stable vs unstable sorting.\nAdd zeros: [07, 70, 700].\nTry [3, 1, 2, 10, 11, 21].\nCount digit comparisons.\nModify to handle negative numbers (offset).\nChange base to 16 (hex).\nCompare with Merge Sort performance on large input.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[170,45,75,90,802,24,2,66]\n[2,24,45,66,75,90,170,802]\nClassic\n\n\n[9,8,7,6,5]\n[5,6,7,8,9]\nReversed\n\n\n[10,1,100,1000]\n[1,10,100,1000]\nDifferent lengths\n\n\n[22,22,11,11]\n[11,11,22,22]\nStable\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(d × (n + k))\n\n\nSpace\nO(n + k)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nRange-sensitive\nYes\n\n\n\nRadix Sort (LSD) is the assembly line of sorting, each pass builds upon the last, producing perfectly ordered output from simple stable steps.\n\n\n\n124 Radix Sort (MSD)\nRadix Sort (Most Significant Digit first) is a recursive variant of Radix Sort that begins sorting from the most significant digit (MSD) and works downward. Unlike LSD Radix Sort, which is iterative and stable across all digits, MSD focuses on prefix-based grouping and recursively sorts subgroups. This makes it ideal for variable-length keys such as strings, IP addresses, or long integers.\n\nWhat Problem Are We Solving??\nLSD Radix Sort works best for fixed-length keys, where every element has the same number of digits. But when keys differ in length (e.g., strings “a”, “ab”, “abc”), we need to respect prefix order, “a” should come before “ab”.\nMSD Radix Sort handles this by grouping by prefix digits, then recursively sorting each group.\nPerfect for:\n\nStrings, words, or variable-length keys\nHierarchical data (prefix-sensitive)\nLexicographic ordering (dictionary order)\n\n\n\nExample\nSort: [\"b\", \"ba\", \"abc\", \"ab\", \"ac\"]\n\n\n\nStep\nDigit\nGroups\n\n\n\n\n1\n1st char\na → [“abc”, “ab”, “ac”], b → [“b”, “ba”]\n\n\n2\nGroup “a”\n2nd char → b: [“abc”, “ab”], c: [“ac”]\n\n\n3\nGroup “ab”\n3rd char → c: [“abc”], end: [“ab”]\n\n\n4\nFinal merge\n[“ab”, “abc”, “ac”, “b”, “ba”]\n\n\n\nLexicographic order preserved, even with varying lengths.\n\n\nHow Does It Work (Plain Language)?\nMSD Radix Sort organizes data by prefix trees (tries) conceptually:\n\nPartition elements by their most significant digit (or character)\nRecurse within each group for next digit\nMerge groups in order of digit values\n\nIf LSD is like bucket sorting digits from the back, MSD is tree-like sorting from the top.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind highest digit place (or first character)\n\n\n2\nPartition array into groups by that digit\n\n\n3\nRecursively sort each group by next digit\n\n\n4\nConcatenate groups in order\n\n\n\nFor strings, if one string ends early, it’s considered smaller.\n\n\nTiny Code (Easy Versions)\n\n\nPython (String Example)\ndef msd_radix_sort(arr, pos=0):\n    if len(arr) &lt;= 1:\n        return arr\n\n    # Buckets for ASCII range (0-255) + 1 for end-of-string\n    buckets = [[] for _ in range(257)]\n    for word in arr:\n        index = ord(word[pos]) + 1 if pos &lt; len(word) else 0\n        buckets[index].append(word)\n\n    result = []\n    for bucket in buckets:\n        if bucket:\n            # Only recurse if there's more than one element and not EOS\n            if len(bucket) &gt; 1 and (pos &lt; max(len(w) for w in bucket)):\n                bucket = msd_radix_sort(bucket, pos + 1)\n            result.extend(bucket)\n    return result\n\narr = [\"b\", \"ba\", \"abc\", \"ab\", \"ac\"]\nprint(msd_radix_sort(arr))\nOutput:\n$$'ab', 'abc', 'ac', 'b', 'ba']\n\n\nC (Numeric Example)\n#include &lt;stdio.h&gt;\n\nint get_digit(int num, int exp, int base) {\n    return (num / exp) % base;\n}\n\nvoid msd_radix_sort_rec(int arr[], int n, int exp, int base, int max) {\n    if (exp == 0 || n &lt;= 1) return;\n\n    int buckets[base][n];\n    int count[base];\n    for (int i = 0; i &lt; base; i++) count[i] = 0;\n\n    // Distribute\n    for (int i = 0; i &lt; n; i++) {\n        int d = get_digit(arr[i], exp, base);\n        buckets[d][count[d]++] = arr[i];\n    }\n\n    // Recurse and collect\n    int idx = 0;\n    for (int i = 0; i &lt; base; i++) {\n        if (count[i] &gt; 0) {\n            msd_radix_sort_rec(buckets[i], count[i], exp / base, base, max);\n            for (int j = 0; j &lt; count[i]; j++)\n                arr[idx++] = buckets[i][j];\n        }\n    }\n}\n\nvoid msd_radix_sort(int arr[], int n) {\n    int max = arr[0];\n    for (int i = 1; i &lt; n; i++)\n        if (arr[i] &gt; max) max = arr[i];\n\n    int exp = 1;\n    while (max / exp &gt;= 10) exp *= 10;\n\n    msd_radix_sort_rec(arr, n, exp, 10, max);\n}\n\nint main(void) {\n    int arr[] = {170, 45, 75, 90, 802, 24, 2, 66};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    msd_radix_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nHandles variable-length keys\nNatural for lexicographic ordering\nUsed in string sorting, trie-based systems, suffix array construction\nRecursively partitions, often faster for large diverse keys\n\n\n\nA Gentle Proof (Why It Works)\nEach recursive call partitions the array by digit prefix. Since partitions are disjoint and ordered by digit, concatenating them yields a fully sorted sequence.\nFor n elements, d digits, and base k:\n\\[\nT(n) = O(n + k) \\text{ per level, depth } \\le d\n\\] \\[\n\\Rightarrow O(d \\times (n + k))\n\\]\nStability preserved via ordered grouping.\n\n\n\nPhase\nWork\nDescription\n\n\n\n\nPartition\nO(n)\nPlace items in digit buckets\n\n\nRecurse\nO(d)\nEach level processes subgroups\n\n\nTotal\nO(d(n + k))\nLinear in digits\n\n\n\n\n\nTry It Yourself\n\nSort [\"b\", \"ba\", \"abc\", \"ab\", \"ac\"].\nDraw recursion tree by character.\nCompare order with lexicographic.\nTest [\"dog\", \"cat\", \"apple\", \"apricot\"].\nSort integers [170,45,75,90,802,24,2,66].\nChange base (binary, hex).\nCompare with LSD Radix Sort.\nAdd duplicates and test stability.\nVisualize grouping buckets.\nImplement with trie-like data structure.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[“b”,“ba”,“abc”,“ab”,“ac”]\n[“ab”,“abc”,“ac”,“b”,“ba”]\nVariable length\n\n\n[170,45,75,90,802,24,2,66]\n[2,24,45,66,75,90,170,802]\nNumeric\n\n\n[“a”,“aa”,“aaa”]\n[“a”,“aa”,“aaa”]\nPrefix order\n\n\n[“z”,“y”,“x”]\n[“x”,“y”,“z”]\nReverse input\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(d × (n + k))\n\n\nSpace\nO(n + k)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nSuitable for\nVariable-length keys\n\n\n\nRadix Sort (MSD) is lexicographic sorting by recursion, it builds order from the top down, treating prefixes as leaders and details as followers, much like how dictionaries arrange words.\n\n\n\n125 Bucket Sort\nBucket Sort is a distribution-based sorting algorithm that divides the input into several buckets (bins), sorts each bucket individually (often with Insertion Sort), and then concatenates them. When input data is uniformly distributed, Bucket Sort achieves linear time performance (O(n)).\n\nWhat Problem Are We Solving??\nComparison-based sorts take O(n log n) time in the general case. But if we know that data values are spread evenly across a range, we can exploit this structure to sort faster by grouping similar values together.\nBucket Sort works best when:\n\nInput is real numbers in [0, 1) or any known range\nData is uniformly distributed\nBuckets are balanced, each with few elements\n\nUsed in:\n\nProbability distributions\nHistogram-based sorting\nFloating-point sorting\n\n\n\nExample\nSort [0.78, 0.17, 0.39, 0.26, 0.72, 0.94, 0.21, 0.12, 0.23, 0.68]\n\n\n\n\n\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nCreate 10 buckets for range [0, 1)\n[[] … []]\n\n\n2\nDistribute elements by int(n * value)\n[[0.12,0.17,0.21,0.23,0.26],[0.39],[0.68,0.72,0.78],[0.94]]\n\n\n3\nSort each bucket (Insertion Sort)\nEach bucket sorted individually\n\n\n4\nConcatenate buckets\n[0.12, 0.17, 0.21, 0.23, 0.26, 0.39, 0.68, 0.72, 0.78, 0.94]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nThink of sorting test scores:\n\nYou group scores into bins (0–10, 10–20, 20–30, …)\nSort each bin individually\nMerge bins back together in order\n\nBucket Sort leverages range grouping, local order inside each bucket, global order from bucket sequence.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nCreate empty buckets for each range interval\n\n\n2\nDistribute elements into buckets\n\n\n3\nSort each bucket individually\n\n\n4\nConcatenate buckets sequentially\n\n\n\nIf buckets are evenly filled, each small sort is fast, almost constant time.\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid insertion_sort(float arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        float key = arr[i];\n        int j = i - 1;\n        while (j &gt;= 0 && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nvoid bucket_sort(float arr[], int n) {\n    float buckets[n][n];\n    int count[n];\n    for (int i = 0; i &lt; n; i++) count[i] = 0;\n\n    // Distribute into buckets\n    for (int i = 0; i &lt; n; i++) {\n        int idx = n * arr[i]; // index by range\n        buckets[idx][count[idx]++] = arr[i];\n    }\n\n    // Sort each bucket\n    for (int i = 0; i &lt; n; i++)\n        if (count[i] &gt; 0)\n            insertion_sort(buckets[i], count[i]);\n\n    // Concatenate\n    int k = 0;\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; count[i]; j++)\n            arr[k++] = buckets[i][j];\n}\n\nint main(void) {\n    float arr[] = {0.78,0.17,0.39,0.26,0.72,0.94,0.21,0.12,0.23,0.68};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    bucket_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%.2f \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef bucket_sort(arr):\n    n = len(arr)\n    buckets = [[] for _ in range(n)]\n\n    for num in arr:\n        idx = int(n * num)\n        buckets[idx].append(num)\n\n    for bucket in buckets:\n        bucket.sort()\n\n    result = []\n    for bucket in buckets:\n        result.extend(bucket)\n    return result\n\narr = [0.78,0.17,0.39,0.26,0.72,0.94,0.21,0.12,0.23,0.68]\nprint(bucket_sort(arr))\n\n\n\nWhy It Matters\n\nLinear time for uniformly distributed data\nGreat for floating-point numbers\nIllustrates distribution-based sorting\nFoundation for histogram, flash, and spread sort\n\n\n\nA Gentle Proof (Why It Works)\nIf input elements are independent and uniformly distributed, expected elements per bucket = O(1). Sorting each small bucket takes constant time → total linear time.\nTotal complexity:\n\\[\nT(n) = O(n + \\sum_{i=1}^{n} T_i)\n\\] If each T_i = O(1), \\[\nT(n) = O(n)\n\\]\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nDistribution\nO(n)\nOne pass\n\n\nLocal Sort\nO(n) total\nExpected\n\n\nConcatenate\nO(n)\nCombine\n\n\n\n\n\nTry It Yourself\n\nSort [0.78,0.17,0.39,0.26,0.72,0.94,0.21,0.12,0.23,0.68].\nVisualize buckets as bins.\nChange bucket count to 5, 20, see effect.\nTry non-uniform data [0.99,0.99,0.98].\nReplace insertion sort with counting sort.\nMeasure performance with 10⁶ floats.\nTest [0.1,0.01,0.001], uneven distribution.\nImplement bucket indexing for arbitrary ranges.\nCompare with Quick Sort runtime.\nPlot distribution histogram before sorting.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[0.78,0.17,0.39,0.26]\n[0.17,0.26,0.39,0.78]\nBasic\n\n\n[0.1,0.01,0.001]\n[0.001,0.01,0.1]\nSparse\n\n\n[0.9,0.8,0.7,0.6]\n[0.6,0.7,0.8,0.9]\nReverse\n\n\n[0.1,0.1,0.1]\n[0.1,0.1,0.1]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n)\n\n\nTime (Worst)\nO(n²) (all in one bucket)\n\n\nSpace\nO(n)\n\n\nStable\nYes (if bucket sort is stable)\n\n\nAdaptive\nYes (depends on distribution)\n\n\n\nBucket Sort is like sorting by bins, fast, simple, and beautifully efficient when your data is evenly spread. It’s the go-to choice for continuous values in a fixed range.\n\n\n\n126 Pigeonhole Sort\nPigeonhole Sort is a simple distribution sorting algorithm that places each element directly into its corresponding “pigeonhole” (or bucket) based on its key value. It’s ideal when elements are integers within a small known range, think of it as Counting Sort with explicit placement rather than counting.\n\nWhat Problem Are We Solving??\nWhen data values are integers and close together, we don’t need comparisons, we can map each value to a slot directly. Pigeonhole Sort is particularly useful for dense integer ranges, such as:\n\nSorting integers from 0 to 100\nSorting scores, ranks, or IDs\nSmall ranges with many duplicates\n\nIt trades space for speed, achieving O(n + range) performance.\n\n\nExample\nSort [8, 3, 2, 7, 4, 6, 8]\n\n\n\n\n\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nFind min=2, max=8 → range = 7\nholes[0..6]\n\n\n2\nCreate pigeonholes: [[],[],[],[],[],[],[]]\n\n\n\n3\nPlace each number in its hole\nholes = [[2],[3],[4],[6],[7],[8,8]]\n\n\n4\nConcatenate holes\n[2,3,4,6,7,8,8]\n\n\n\nEach value goes exactly to its mapped hole index.\n\n\nHow Does It Work (Plain Language)?\nIt’s like assigning students to exam rooms based on ID ranges, each slot holds all matching IDs. You fill slots (holes), then read them back in order.\nUnlike Counting Sort, which only counts occurrences, Pigeonhole Sort stores the actual values, preserving duplicates directly.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind minimum and maximum elements\n\n\n2\nCompute range = max - min + 1\n\n\n3\nCreate array of empty pigeonholes of size range\n\n\n4\nFor each element, map to hole arr[i] - min\n\n\n5\nPlace element into that hole (append)\n\n\n6\nRead holes in order and flatten into output\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid pigeonhole_sort(int arr[], int n) {\n    int min = arr[0], max = arr[0];\n    for (int i = 1; i &lt; n; i++) {\n        if (arr[i] &lt; min) min = arr[i];\n        if (arr[i] &gt; max) max = arr[i];\n    }\n\n    int range = max - min + 1;\n    int *holes[range];\n    int counts[range];\n    for (int i = 0; i &lt; range; i++) {\n        holes[i] = malloc(n * sizeof(int));\n        counts[i] = 0;\n    }\n\n    // Place elements into holes\n    for (int i = 0; i &lt; n; i++) {\n        int index = arr[i] - min;\n        holes[index][counts[index]++] = arr[i];\n    }\n\n    // Flatten back\n    int index = 0;\n    for (int i = 0; i &lt; range; i++) {\n        for (int j = 0; j &lt; counts[i]; j++) {\n            arr[index++] = holes[i][j];\n        }\n        free(holes[i]);\n    }\n}\n\nint main(void) {\n    int arr[] = {8, 3, 2, 7, 4, 6, 8};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    pigeonhole_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef pigeonhole_sort(arr):\n    min_val, max_val = min(arr), max(arr)\n    size = max_val - min_val + 1\n    holes = [[] for _ in range(size)]\n\n    for x in arr:\n        holes[x - min_val].append(x)\n\n    sorted_arr = []\n    for hole in holes:\n        sorted_arr.extend(hole)\n    return sorted_arr\n\narr = [8, 3, 2, 7, 4, 6, 8]\nprint(pigeonhole_sort(arr))\n\n\n\nWhy It Matters\n\nSimple mapping for small integer ranges\nLinear time if range ≈ n\nUseful in digit, rank, or ID sorting\nProvides stable grouping with explicit placement\n\n\n\nA Gentle Proof (Why It Works)\nEvery key is mapped uniquely to a hole (offset by min). All duplicates fall into the same hole, preserving multiplicity. Reading holes sequentially yields sorted order.\nIf n = number of elements, k = range of values:\n\\[\nT(n) = O(n + k)\n\\]\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nFind min/max\nO(n)\nSingle pass\n\n\nDistribute to holes\nO(n)\nOne placement each\n\n\nCollect results\nO(n + k)\nFlatten all\n\n\n\n\n\nTry It Yourself\n\nSort [8,3,2,7,4,6,8] by hand.\nShow hole contents after distribution.\nAdd duplicate values, confirm stable grouping.\nTry [1,1,1,1], all in one hole.\nSort negative numbers [0,-1,-2,1] (offset by min).\nIncrease range to see space cost.\nCompare runtime with Counting Sort.\nReplace holes with linked lists.\nImplement in-place version.\nExtend for key-value pairs.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[8,3,2,7,4,6,8]\n[2,3,4,6,7,8,8]\nExample\n\n\n[1,1,1,1]\n[1,1,1,1]\nAll equal\n\n\n[9,8,7,6]\n[6,7,8,9]\nReverse\n\n\n[0,-1,-2,1]\n[-2,-1,0,1]\nNegative offset\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n + k)\n\n\nSpace\nO(n + k)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nRange-sensitive\nYes\n\n\n\nPigeonhole Sort is as direct as sorting gets, one slot per value, one pass in, one pass out. Fast and clean when your data is dense and discrete.\n\n\n\n127 Flash Sort\nFlash Sort is a distribution-based sorting algorithm that combines ideas from bucket sort and insertion sort. It works in two phases:\n\nClassification, distribute elements into classes (buckets) using a linear mapping\nPermutation, rearrange elements in-place using cycles\n\nIt achieves O(n) average time on uniformly distributed data but can degrade to O(n²) in the worst case. Invented by Karl-Dietrich Neubert (1990s), it’s known for being extremely fast in practice on large datasets.\n\nWhat Problem Are We Solving??\nWhen data is numerically distributed over a range, we can approximate where each element should go and move it close to its final position without full comparison sorting.\nFlash Sort is built for:\n\nUniformly distributed numeric data\nLarge arrays\nPerformance-critical applications (e.g., simulations, physics, graphics)\n\nIt leverages approximate indexing and in-place permutation for speed.\n\n\nExample\nSort [9, 3, 1, 7, 4, 6, 2, 8, 5] into m = 5 classes.\n\n\n\n\n\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nFind min = 1, max = 9\nrange = 8\n\n\n2\nMap each value to class k = (m-1)*(a[i]-min)/(max-min)\nClass indices: [4,1,0,3,1,3,0,4,2]\n\n\n3\nCount elements per class, compute cumulative positions\nclass boundaries = [0,2,4,6,8,9]\n\n\n4\nCycle elements into correct class positions\nRearranged approx order\n\n\n5\nApply insertion sort within classes\nSorted list\n\n\n\nFinal: [1,2,3,4,5,6,7,8,9]\n\n\nHow Does It Work (Plain Language)?\nImagine flashing each element to its approximate destination class in one pass, that’s the “flash” phase. Then, fine-tune within each class using a simpler sort (like Insertion Sort).\nIt’s like placing books roughly into shelves, then tidying each shelf.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind min and max\n\n\n2\nChoose number of classes (usually ≈ 0.43 * n)\n\n\n3\nCompute class index for each element\n\n\n4\nCount elements per class and compute prefix sums\n\n\n5\nMove elements to approximate class positions (flash phase)\n\n\n6\nUse Insertion Sort within each class to finish\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid insertion_sort(float arr[], int start, int end) {\n    for (int i = start + 1; i &lt;= end; i++) {\n        float key = arr[i];\n        int j = i - 1;\n        while (j &gt;= start && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nvoid flash_sort(float arr[], int n) {\n    if (n &lt;= 1) return;\n\n    float min = arr[0], max = arr[0];\n    for (int i = 1; i &lt; n; i++) {\n        if (arr[i] &lt; min) min = arr[i];\n        if (arr[i] &gt; max) max = arr[i];\n    }\n    if (max == min) return;\n\n    int m = n * 0.43; // number of classes\n    int L[m];\n    for (int i = 0; i &lt; m; i++) L[i] = 0;\n\n    // Classification count\n    for (int i = 0; i &lt; n; i++) {\n        int k = (int)((m - 1) * (arr[i] - min) / (max - min));\n        L[k]++;\n    }\n\n    // Prefix sums\n    for (int i = 1; i &lt; m; i++) L[i] += L[i - 1];\n\n    // Flash phase\n    int move = 0, j = 0, k = m - 1;\n    while (move &lt; n - 1) {\n        while (j &gt;= L[k]) k = (int)((m - 1) * (arr[j] - min) / (max - min));\n        float flash = arr[j];\n        while (j != L[k]) {\n            k = (int)((m - 1) * (flash - min) / (max - min));\n            float hold = arr[--L[k]];\n            arr[L[k]] = flash;\n            flash = hold;\n            move++;\n        }\n        j++;\n    }\n\n    // Insertion sort finish\n    insertion_sort(arr, 0, n - 1);\n}\n\nint main(void) {\n    float arr[] = {9,3,1,7,4,6,2,8,5};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    flash_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%.0f \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef insertion_sort(arr, start, end):\n    for i in range(start + 1, end + 1):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= start and arr[j] &gt; key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\ndef flash_sort(arr):\n    n = len(arr)\n    if n &lt;= 1:\n        return arr\n    min_val, max_val = min(arr), max(arr)\n    if max_val == min_val:\n        return arr\n    m = int(0.43 * n)\n    L = [0] * m\n\n    for x in arr:\n        k = int((m - 1) * (x - min_val) / (max_val - min_val))\n        L[k] += 1\n\n    for i in range(1, m):\n        L[i] += L[i - 1]\n\n    move, j, k = 0, 0, m - 1\n    while move &lt; n - 1:\n        while j &gt;= L[k]:\n            k = int((m - 1) * (arr[j] - min_val) / (max_val - min_val))\n        flash = arr[j]\n        while j != L[k]:\n            k = int((m - 1) * (flash - min_val) / (max_val - min_val))\n            L[k] -= 1\n            arr[L[k]], flash = flash, arr[L[k]]\n            move += 1\n        j += 1\n    insertion_sort(arr, 0, n - 1)\n    return arr\n\narr = [9,3,1,7,4,6,2,8,5]\nprint(flash_sort(arr))\n\n\n\nWhy It Matters\n\nExtremely fast on uniformly distributed data\nIn-place (O(1) extra space)\nPractical for large arrays\nCombines distribution sorting and insertion finishing\n\n\n\nA Gentle Proof (Why It Works)\nThe classification step maps each element to its expected position region, dramatically reducing disorder. Since each class has a small local range, insertion sort completes quickly.\nExpected complexity (uniform input):\n\\[\nT(n) = O(n) \\text{ (classification) } + O(n) \\text{ (final pass) } = O(n)\n\\]\nWorst-case (skewed distribution): O(n²)\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nClassification\nO(n)\nCompute classes\n\n\nFlash rearrangement\nO(n)\nIn-place moves\n\n\nFinal sort\nO(n)\nLocal sorting\n\n\n\n\n\nTry It Yourself\n\nSort [9,3,1,7,4,6,2,8,5] step by step.\nChange number of classes (0.3n, 0.5n).\nVisualize class mapping for each element.\nCount moves in flash phase.\nCompare with Quick Sort timing on 10⁵ elements.\nTest uniform vs skewed data.\nImplement with different finishing sort.\nTrack cycles formed during flash phase.\nObserve stability (it’s not stable).\nBenchmark against Merge Sort.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[9,3,1,7,4,6,2,8,5]\n[1,2,3,4,5,6,7,8,9]\nExample\n\n\n[5,5,5,5]\n[5,5,5,5]\nEqual elements\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nAlready sorted\n\n\n[9,8,7,6,5]\n[5,6,7,8,9]\nReverse\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nYes (partially)\n\n\n\nFlash Sort is the lightning strike of sorting, it flashes elements to their expected zones in linear time, then finishes with a quick polish. When your data is uniform, it’s one of the fastest practical sorts around.\n\n\n\n128 Postman Sort\nPostman Sort is a stable, multi-key sorting algorithm that works by sorting keys digit by digit or field by field, starting from the least significant field (like LSD Radix Sort) or most significant field (like MSD Radix Sort), depending on the application. It’s often used for compound keys (e.g. postal addresses, dates, strings of fields), hence the name “Postman,” as it sorts data the way a postman organizes mail: by street, then house, then apartment.\n\nWhat Problem Are We Solving??\nWhen sorting complex records by multiple attributes, such as:\n\nSorting people by (city, street, house_number)\nSorting files by (year, month, day)\nSorting products by (category, brand, price)\n\nWe need to sort hierarchically, that’s what Postman Sort excels at. It’s a stable, field-wise sorting approach, built upon Counting or Bucket Sort for each field.\n\n\nExample\nSort tuples (City, Street) by City then Street:\n$$(\"Paris\", \"B\"), (\"London\", \"C\"), (\"Paris\", \"A\"), (\"London\", \"A\")]\n\n\n\n\n\n\n\n\nStep\nSort By\nResult\n\n\n\n\n1\nStreet (LSD)\n[(“London”,“A”),(“Paris”,“A”),(“London”,“C”),(“Paris”,“B”)]\n\n\n2\nCity (MSD)\n[(“London”,“A”),(“London”,“C”),(“Paris”,“A”),(“Paris”,“B”)]\n\n\n\nStable sorting ensures inner order is preserved from previous pass.\n\n\nHow Does It Work (Plain Language)?\nIt’s like organizing mail:\n\nSort by the smallest unit (house number)\nThen sort by street\nThen by city\n\nEach pass refines the previous ordering. If sorting from least to most significant, use LSD order (like Radix). If sorting from most to least, use MSD order (like bucket recursion).\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nIdentify key fields and their order of significance\n\n\n2\nChoose stable sorting method (e.g., Counting Sort)\n\n\n3\nSort by least significant key first\n\n\n4\nRepeat for each key moving to most significant\n\n\n5\nFinal order respects all key hierarchies\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (LSD Approach)\ndef postman_sort(records, key_funcs):\n    # key_funcs = list of functions to extract each field\n    for key_func in reversed(key_funcs):\n        records.sort(key=key_func)\n    return records\n\n# Example: sort by (city, street)\nrecords = [(\"Paris\", \"B\"), (\"London\", \"C\"), (\"Paris\", \"A\"), (\"London\", \"A\")]\nsorted_records = postman_sort(records, [lambda x: x[0], lambda x: x[1]])\nprint(sorted_records)\nOutput:\n$$('London', 'A'), ('London', 'C'), ('Paris', 'A'), ('Paris', 'B')]\n\n\nC (Numeric Fields)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int city;\n    int street;\n} Record;\n\nint cmp_street(const void *a, const void *b) {\n    Record *ra = (Record*)a, *rb = (Record*)b;\n    return ra-&gt;street - rb-&gt;street;\n}\n\nint cmp_city(const void *a, const void *b) {\n    Record *ra = (Record*)a, *rb = (Record*)b;\n    return ra-&gt;city - rb-&gt;city;\n}\n\nvoid postman_sort(Record arr[], int n) {\n    // Sort by street first (LSD)\n    qsort(arr, n, sizeof(Record), cmp_street);\n    // Then by city (MSD)\n    qsort(arr, n, sizeof(Record), cmp_city);\n}\n\nint main(void) {\n    Record arr[] = {{2,3}, {1,2}, {2,1}, {1,1}};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    postman_sort(arr, n);\n    for (int i = 0; i &lt; n; i++)\n        printf(\"(%d,%d) \", arr[i].city, arr[i].street);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nMulti-key sorting (lexicographic order)\nStable, preserves order across passes\nVersatile, works on compound keys, strings, records\nFoundation for:\n\nRadix Sort\nDatabase ORDER BY multi-column\nLexicographic ranking\n\n\n\n\nA Gentle Proof (Why It Works)\nEach stable pass ensures that prior ordering (from less significant fields) remains intact.\nIf we denote each field as \\(f_i\\), sorted stably in order \\(f_k \\to f_1\\):\n\\[\nT(n) = \\sum_{i=1}^k T_i(n)\n\\]\nIf each \\(T_i(n)\\) is \\(O(n)\\), the total is \\(O(kn)\\).\nLexicographic order emerges naturally: \\[\n(a_1, a_2, \\ldots, a_k) &lt; (b_1, b_2, \\ldots, b_k)\n\\iff a_i = b_i \\text{ for } i &lt; j, \\text{ and } a_j &lt; b_j\n\\]\n\n\n\nPhase\nOperation\nStable\nComplexity\n\n\n\n\nLSD Sort\nStart from least significant\nYes\n\\(O(nk)\\)\n\n\nMSD Sort\nStart from most significant\nYes\n\\(O(nk)\\)\n\n\n\n\n\nTry It Yourself\n\nSort [(\"Paris\",\"B\"),(\"London\",\"C\"),(\"Paris\",\"A\"),(\"London\",\"A\")]\nAdd a 3rd field (zip code), sort by zip → street → city\nImplement with lambda keys in Python\nReplace stable sort with Counting Sort per field\nTest with [(2021,12,25),(2020,1,1),(2021,1,1)]\nCompare LSD vs MSD ordering\nSort strings by character groups (first char, second char, etc.)\nVisualize passes and intermediate results\nTest stability with repeated keys\nApply to sorting student records by (grade, class, id)\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[(“Paris”,“B”),(“London”,“C”),(“Paris”,“A”),(“London”,“A”)]\n[(“London”,“A”),(“London”,“C”),(“Paris”,“A”),(“Paris”,“B”)]\nLexicographic\n\n\n[(2021,12,25),(2020,1,1),(2021,1,1)]\n[(2020,1,1),(2021,1,1),(2021,12,25)]\nDate order\n\n\n[(1,2,3),(1,1,3),(1,1,2)]\n[(1,1,2),(1,1,3),(1,2,3)]\nMulti-field\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(k × n)\n\n\nSpace\nO(n)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nKeys\nMulti-field\n\n\n\nPostman Sort delivers order like clockwork, field by field, pass by pass, ensuring every layer of your data finds its proper place, from apartment number to zip code.\n\n\n\n129 Address Calculation Sort\nAddress Calculation Sort (sometimes called Hash Sort or Scatter Sort) is a distribution-based sorting method that uses a hash-like function (called an address function) to compute the final position of each element directly. Instead of comparing pairs, it computes where each element should go, much like direct addressing in hash tables.\n\nWhat Problem Are We Solving??\nComparison sorts need O(n log n) time. If we know the range and distribution of input values, we can instead compute where each element belongs, placing it directly.\nAddress Calculation Sort bridges the gap between sorting and hashing:\n\nIt computes a mapping from value to position.\nIt places each element directly or into small groups.\nIt works best when data distribution is known and uniform.\n\nApplications:\n\nDense numeric datasets\nRanked records\nPre-bucketed ranges\n\n\n\nExample\nSort [3, 1, 4, 0, 2] with range [0..4].\n\n\n\nStep\nElement\nAddress Function \\(f(x) = x\\)\nPlacement\n\n\n\n\n1\n3\nf(3) = 3\nposition 3\n\n\n2\n1\nf(1) = 1\nposition 1\n\n\n3\n4\nf(4) = 4\nposition 4\n\n\n4\n0\nf(0) = 0\nposition 0\n\n\n5\n2\nf(2) = 2\nposition 2\n\n\n\nResult: [0, 1, 2, 3, 4]\nIf multiple elements share the same address, they’re stored in a small linked list or bucket, then sorted locally.\n\n\nHow Does It Work (Plain Language)?\nImagine having labeled mailboxes for every possible key — each element knows exactly which box it belongs in. You just drop each letter into its slot, then read the boxes in order.\nUnlike Counting Sort, which counts occurrences, Address Calculation Sort assigns positions, it can even be in-place if collisions are handled carefully.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nDefine address function \\(f(x)\\) mapping each element to an index\n\n\n2\nInitialize empty output array or buckets\n\n\n3\nFor each element \\(a_i\\): compute index = \\(f(a_i)\\)\n\n\n4\nPlace element at index (handle collisions if needed)\n\n\n5\nCollect or flatten buckets into sorted order\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nC (Simple In-Range Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid address_calculation_sort(int arr[], int n, int min, int max) {\n    int range = max - min + 1;\n    int *output = calloc(range, sizeof(int));\n    int *filled = calloc(range, sizeof(int));\n\n    for (int i = 0; i &lt; n; i++) {\n        int idx = arr[i] - min;\n        output[idx] = arr[i];\n        filled[idx] = 1;\n    }\n\n    int k = 0;\n    for (int i = 0; i &lt; range; i++) {\n        if (filled[i]) arr[k++] = output[i];\n    }\n\n    free(output);\n    free(filled);\n}\n\nint main(void) {\n    int arr[] = {3, 1, 4, 0, 2};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    address_calculation_sort(arr, n, 0, 4);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\nPython\ndef address_calculation_sort(arr, f=None):\n    if not arr:\n        return arr\n    if f is None:\n        f = lambda x: x  # identity mapping\n\n    min_val, max_val = min(arr), max(arr)\n    size = max_val - min_val + 1\n    slots = [[] for _ in range(size)]\n\n    for x in arr:\n        idx = f(x) - min_val\n        slots[idx].append(x)\n\n    # Flatten buckets\n    result = []\n    for bucket in slots:\n        result.extend(sorted(bucket))  # local sort if needed\n    return result\n\narr = [3, 1, 4, 0, 2]\nprint(address_calculation_sort(arr))\nOutput:\n$$0, 1, 2, 3, 4]\n\n\n\nWhy It Matters\n\nDirect computation of sorted positions\nLinear time for predictable distributions\nCombines hashing and sorting\nForms basis of bucket, radix, and flash sort\n\nIt’s a great way to see how functions can replace comparisons.\n\n\nA Gentle Proof (Why It Works)\nIf \\(f(x)\\) maps each key uniquely to its sorted index, the result is already sorted by construction.\nEven with collisions, sorting small local buckets is fast.\nTotal time: \\[\nT(n) = O(n + k)\n\\] where \\(k\\) = number of buckets = range size.\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nCompute addresses\nO(n)\nOne per element\n\n\nBucket insertions\nO(n)\nConstant time each\n\n\nLocal sort\nO(k)\nSmall groups\n\n\nFlatten\nO(n + k)\nRead back\n\n\n\n\n\nTry It Yourself\n\nSort [3, 1, 4, 0, 2] with f(x)=x.\nChange mapping to f(x)=2*x, note gaps.\nAdd duplicates [1,1,2,2,3], use buckets.\nTry floats with f(x)=int(x*10) for [0.1,0.2,0.3].\nSort strings by ASCII sum: f(s)=sum(map(ord,s)).\nCompare with Counting Sort (no explicit storage).\nHandle negative numbers by offsetting min.\nVisualize mapping table.\nTest range gaps (e.g., [10, 20, 30]).\nExperiment with custom hash functions.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,1,4,0,2]\n[0,1,2,3,4]\nPerfect mapping\n\n\n[10,20,30]\n[10,20,30]\nSparse mapping\n\n\n[1,1,2,2]\n[1,1,2,2]\nDuplicates\n\n\n[0.1,0.3,0.2]\n[0.1,0.2,0.3]\nFloat mapping\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n + k)\n\n\nSpace\nO(n + k)\n\n\nStable\nYes (with buckets)\n\n\nAdaptive\nNo\n\n\nRange-sensitive\nYes\n\n\n\nAddress Calculation Sort turns sorting into placement, each value finds its home by formula, not fight. When the range is clear and collisions are rare, it’s lightning-fast and elegantly simple.\n\n\n\n130 Spread Sort\nSpread Sort is a hybrid distribution sort that blends ideas from radix sort, bucket sort, and comparison sorting. It distributes elements into buckets using their most significant bits (MSB) or value ranges, then recursively sorts buckets (like radix/MSD) or switches to a comparison sort (like Quick Sort) when buckets are small.\nIt’s cache-friendly, adaptive, and often faster than Quick Sort on uniformly distributed data. In fact, it’s used in some high-performance libraries such as Boost C++ Spreadsort.\n\nWhat Problem Are We Solving??\nTraditional comparison sorts (like Quick Sort) have \\(O(n \\log n)\\) complexity, while pure radix-based sorts can be inefficient on small or skewed datasets. Spread Sort solves this by adapting dynamically:\n\nDistribute like radix sort when data is wide and random\nCompare like Quick Sort when data is clustered or buckets are small\n\nIt “spreads” data across buckets, then sorts each bucket intelligently.\nPerfect for:\n\nIntegers, floats, and strings\nLarge datasets\nWide value ranges\n\n\n\nExample\nSort [43, 12, 89, 27, 55, 31, 70]\n\n\n\n\n\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nFind min = 12, max = 89\nrange = 77\n\n\n2\nChoose bucket count (e.g. 4)\nbucket size ≈ 19\n\n\n3\nDistribute by bucket index = (val - min)/19\nBuckets: [12], [27,31], [43,55], [70,89]\n\n\n4\nRecursively sort each bucket\n[12], [27,31], [43,55], [70,89]\n\n\n5\nMerge buckets\n[12,27,31,43,55,70,89]\n\n\n\n\n\nHow Does It Work (Plain Language)?\nImagine sorting mail by first letter (distribution), then alphabetizing each pile (comparison). If a pile is still big, spread it again by the next letter. If a pile is small, just sort it directly.\nSpread Sort uses:\n\nDistribution when the data range is wide\nComparison sort when buckets are narrow or small\n\nThis flexibility gives it strong real-world performance.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind min and max values\n\n\n2\nCompute spread = max - min\n\n\n3\nChoose bucket count (based on n or spread)\n\n\n4\nDistribute elements into buckets by value range\n\n\n5\nRecursively apply spread sort to large buckets\n\n\n6\nApply comparison sort to small buckets\n\n\n7\nConcatenate results\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nC (Integer Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint compare(const void *a, const void *b) {\n    return (*(int*)a - *(int*)b);\n}\n\nvoid spread_sort(int arr[], int n) {\n    if (n &lt;= 16) { // threshold for small buckets\n        qsort(arr, n, sizeof(int), compare);\n        return;\n    }\n\n    int min = arr[0], max = arr[0];\n    for (int i = 1; i &lt; n; i++) {\n        if (arr[i] &lt; min) min = arr[i];\n        if (arr[i] &gt; max) max = arr[i];\n    }\n\n    int bucket_count = n / 4 + 1;\n    int range = max - min + 1;\n    int bucket_size = range / bucket_count + 1;\n\n    int *counts = calloc(bucket_count, sizeof(int));\n    int buckets = malloc(bucket_count * sizeof(int*));\n    for (int i = 0; i &lt; bucket_count; i++)\n        buckets[i] = malloc(n * sizeof(int));\n\n    // Distribution\n    for (int i = 0; i &lt; n; i++) {\n        int idx = (arr[i] - min) / bucket_size;\n        buckets[idx][counts[idx]++] = arr[i];\n    }\n\n    // Recursive sort\n    int k = 0;\n    for (int i = 0; i &lt; bucket_count; i++) {\n        if (counts[i] &gt; 0) {\n            spread_sort(buckets[i], counts[i]);\n            for (int j = 0; j &lt; counts[i]; j++)\n                arr[k++] = buckets[i][j];\n        }\n        free(buckets[i]);\n    }\n\n    free(buckets);\n    free(counts);\n}\n\nint main(void) {\n    int arr[] = {43, 12, 89, 27, 55, 31, 70};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    spread_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\nPython\ndef spread_sort(arr, threshold=16):\n    if len(arr) &lt;= threshold:\n        return sorted(arr)\n\n    min_val, max_val = min(arr), max(arr)\n    if min_val == max_val:\n        return arr[:]\n\n    n = len(arr)\n    bucket_count = n // 4 + 1\n    spread = max_val - min_val + 1\n    bucket_size = spread // bucket_count + 1\n\n    buckets = [[] for _ in range(bucket_count)]\n    for x in arr:\n        idx = (x - min_val) // bucket_size\n        buckets[idx].append(x)\n\n    result = []\n    for b in buckets:\n        if len(b) &gt; threshold:\n            result.extend(spread_sort(b, threshold))\n        else:\n            result.extend(sorted(b))\n    return result\n\narr = [43, 12, 89, 27, 55, 31, 70]\nprint(spread_sort(arr))\n\n\n\nWhy It Matters\n\nLinear-time on uniform data\nAdaptive to distribution and size\nCombines bucket and comparison power\nGreat for large numeric datasets\n\nUsed in Boost C++, it’s a real-world performant hybrid.\n\n\nA Gentle Proof (Why It Works)\nSpread Sort’s cost depends on:\n\nDistribution pass: \\(O(n)\\)\n\nRecursion depth: small (logarithmic for uniform data)\n\nLocal sorts: small and fast (often \\(O(1)\\) or \\(O(n \\log n)\\) on small buckets)\n\nFor \\(n\\) elements and \\(b\\) buckets:\n\\[\nT(n) = O(n + \\sum_{i=1}^{b} T_i)\n\\] If average bucket size is constant or small: \\[\nT(n) \\approx O(n)\n\\]\n\n\n\n\n\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nDistribution\n\\(O(n)\\)\nOne pass\n\n\nLocal Sorts\n\\(O(n \\log m)\\)\n\\(m =\\) average bucket size\n\n\nTotal\n\\(O(n)\\) average, \\(O(n \\log n)\\) worst\n\n\n\n\n\n\nTry It Yourself\n\nSort [43,12,89,27,55,31,70].\nTry [5,4,3,2,1] (non-uniform).\nAdjust bucket threshold.\nVisualize recursive buckets.\nMix large and small values (e.g., [1, 10, 1000, 2, 20, 2000]).\nCompare runtime with Quick Sort.\nImplement float version.\nMeasure distribution imbalance.\nTune bucket size formula.\nSort strings by ord(char).\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[43,12,89,27,55,31,70]\n[12,27,31,43,55,70,89]\nUniform\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse\n\n\n[100,10,1,1000]\n[1,10,100,1000]\nWide range\n\n\n[5,5,5]\n[5,5,5]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(n)\n\n\nStable\nYes (if local sort is stable)\n\n\nAdaptive\nYes\n\n\n\nSpread Sort spreads elements like seeds, each finds fertile ground in its range, then blossoms into order through local sorting. It’s the smart hybrid that brings the best of radix and comparison worlds together.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-14.-hybrid-sorts",
    "href": "books/en-us/list-2.html#section-14.-hybrid-sorts",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 14. Hybrid sorts",
    "text": "Section 14. Hybrid sorts\n\n131 IntroSort\nIntroSort (short for Introspective Sort) is a hybrid sorting algorithm that combines the best features of Quick Sort, Heap Sort, and Insertion Sort. It begins with Quick Sort for speed, but if recursion goes too deep (indicating unbalanced partitions), it switches to Heap Sort to guarantee worst-case \\(O(n \\log n)\\) performance. For small partitions, it often uses Insertion Sort for efficiency.\nIt was introduced by David Musser (1997) and is the default sorting algorithm in C++ STL (std::sort), fast, adaptive, and safe.\n\nWhat Problem Are We Solving??\nPure Quick Sort is fast on average but can degrade to \\(O(n^2)\\) in the worst case (for example, sorted input with bad pivots).\nHeap Sort guarantees \\(O(n \\log n)\\) but has worse constants.\nIntroSort combines them, using Quick Sort until danger, then switching to Heap Sort for safety.\nPerfect for:\n\nGeneral-purpose sorting (numeric, string, object)\nPerformance-critical libraries\nMixed data with unknown distribution\n\n\n\nExample\nSort [9, 3, 1, 7, 5, 4, 6, 2, 8]\n\n\n\n\n\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nStart Quick Sort (depth = 0)\npivot = 5\n\n\n2\nPartition → [3,1,2,4] [5] [9,7,6,8]\ndepth = 1\n\n\n3\nRecurse left + right\n\n\n\n4\nIf depth &gt; 2 × log₂(n), switch to Heap Sort\nprevents (O(n^2))\n\n\n5\nUse Insertion Sort on small segments\n\n\n\n\nFinal sorted array: [1,2,3,4,5,6,7,8,9]\n\n\nHow Does It Work (Plain Language)?\nIt’s like a careful driver:\n\nStart fast on the highway (Quick Sort)\nIf road gets tricky (too many turns = recursion depth), switch to 4-wheel drive (Heap Sort)\nFor small parking spots (tiny arrays), use a nimble bike (Insertion Sort)\n\nIntroSort keeps average speed high but avoids worst-case crashes.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart Quick Sort recursively\n\n\n2\nTrack recursion depth\n\n\n3\nIf depth exceeds threshold (2 × log₂ n), switch to Heap Sort\n\n\n4\nUse Insertion Sort for small subarrays (size &lt; threshold)\n\n\n5\nCombine results for final sorted array\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nC (Simplified Implementation)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n\n#define THRESHOLD 16\n\nvoid insertion_sort(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int key = arr[i], j = i - 1;\n        while (j &gt;= 0 && arr[j] &gt; key) arr[j + 1] = arr[j--];\n        arr[j + 1] = key;\n    }\n}\n\nvoid heapify(int arr[], int n, int i) {\n    int largest = i, l = 2*i + 1, r = 2*i + 2;\n    if (l &lt; n && arr[l] &gt; arr[largest]) largest = l;\n    if (r &lt; n && arr[r] &gt; arr[largest]) largest = r;\n    if (largest != i) {\n        int tmp = arr[i]; arr[i] = arr[largest]; arr[largest] = tmp;\n        heapify(arr, n, largest);\n    }\n}\n\nvoid heap_sort(int arr[], int n) {\n    for (int i = n/2 - 1; i &gt;= 0; i--) heapify(arr, n, i);\n    for (int i = n - 1; i &gt;= 0; i--) {\n        int tmp = arr[0]; arr[0] = arr[i]; arr[i] = tmp;\n        heapify(arr, i, 0);\n    }\n}\n\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high], i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt;= pivot) {\n            i++;\n            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;\n        }\n    }\n    int tmp = arr[i+1]; arr[i+1] = arr[high]; arr[high] = tmp;\n    return i + 1;\n}\n\nvoid introsort_rec(int arr[], int low, int high, int depth_limit) {\n    int n = high - low + 1;\n    if (n &lt;= THRESHOLD) {\n        insertion_sort(arr + low, n);\n        return;\n    }\n    if (depth_limit == 0) {\n        heap_sort(arr + low, n);\n        return;\n    }\n    int p = partition(arr, low, high);\n    introsort_rec(arr, low, p - 1, depth_limit - 1);\n    introsort_rec(arr, p + 1, high, depth_limit - 1);\n}\n\nvoid introsort(int arr[], int n) {\n    int depth_limit = 2 * log(n);\n    introsort_rec(arr, 0, n - 1, depth_limit);\n}\n\nint main(void) {\n    int arr[] = {9,3,1,7,5,4,6,2,8};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    introsort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nPython (Conceptual Demo)\nimport math\n\ndef insertion_sort(a):\n    for i in range(1, len(a)):\n        key = a[i]\n        j = i - 1\n        while j &gt;= 0 and a[j] &gt; key:\n            a[j + 1] = a[j]\n            j -= 1\n        a[j + 1] = key\n\ndef heapify(a, n, i):\n    largest = i\n    l, r = 2*i+1, 2*i+2\n    if l &lt; n and a[l] &gt; a[largest]:\n        largest = l\n    if r &lt; n and a[r] &gt; a[largest]:\n        largest = r\n    if largest != i:\n        a[i], a[largest] = a[largest], a[i]\n        heapify(a, n, largest)\n\ndef heap_sort(a):\n    n = len(a)\n    for i in range(n//2-1, -1, -1):\n        heapify(a, n, i)\n    for i in range(n-1, 0, -1):\n        a[0], a[i] = a[i], a[0]\n        heapify(a, i, 0)\n\ndef introsort(a, depth_limit=None):\n    n = len(a)\n    if n &lt;= 16:\n        insertion_sort(a)\n        return a\n    if depth_limit is None:\n        depth_limit = 2 * int(math.log2(n))\n    if depth_limit == 0:\n        heap_sort(a)\n        return a\n\n    pivot = a[-1]\n    left = [x for x in a[:-1] if x &lt;= pivot]\n    right = [x for x in a[:-1] if x &gt; pivot]\n    introsort(left, depth_limit - 1)\n    introsort(right, depth_limit - 1)\n    a[:] = left + [pivot] + right\n    return a\n\narr = [9,3,1,7,5,4,6,2,8]\nprint(introsort(arr))\n\n\nWhy It Matters\n\nDefault in C++ STL, fast and reliable\nGuaranteed worst-case \\(O(n \\log n)\\)\nOptimized for cache and small data\nAdaptive, uses best method for current scenario\n\n\n\nA Gentle Proof (Why It Works)\nQuick Sort dominates until recursion depth = \\(2 \\log_2 n\\). At that point, worst-case risk appears → switch to Heap Sort (safe fallback). Small subarrays are handled by Insertion Sort for low overhead.\nSo overall: \\[\nT(n) = O(n \\log n)\n\\] Always bounded by Heap Sort’s worst case, but often near Quick Sort’s best.\n\n\n\nPhase\nMethod\nComplexity\n\n\n\n\nPartitioning\nQuick Sort\nO(n log n)\n\n\nDeep recursion\nHeap Sort\nO(n log n)\n\n\nSmall arrays\nInsertion Sort\nO(n²) local, negligible\n\n\n\n\n\nTry It Yourself\n\nSort [9,3,1,7,5,4,6,2,8].\nTrack recursion depth, switch to Heap Sort when \\(&gt; 2 \\log_2 n\\).\nReplace threshold = 16 with 8, 32, measure effect.\nTest with already sorted array, confirm Heap fallback.\nCompare timing with Quick Sort and Heap Sort.\nPrint method used at each stage.\nTest large array (10⁵ elements).\nVerify worst-case safety on sorted input.\nTry string sorting with custom comparator.\nImplement generic version using templates or lambdas.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[9,3,1,7,5,4,6,2,8]\n[1,2,3,4,5,6,7,8,9]\nBalanced partitions\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nSorted input (Heap Sort fallback)\n\n\n[5,5,5,5]\n[5,5,5,5]\nEqual elements\n\n\n[9,8,7,6,5,4,3,2,1]\n[1,2,3,4,5,6,7,8,9]\nWorst-case Quick Sort avoided\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(log n)\n\n\nStable\nNo\n\n\nAdaptive\nYes\n\n\n\nIntroSort is the strategist’s algorithm, it starts bold (Quick Sort), defends wisely (Heap Sort), and finishes gracefully (Insertion Sort). A perfect balance of speed, safety, and adaptability.\n\n\n\n132 TimSort\nTimSort is a hybrid sorting algorithm combining Merge Sort and Insertion Sort, designed for real-world data that often contains partially ordered runs. It was invented by Tim Peters in 2002 and is the default sorting algorithm in Python (sorted(), .sort()) and Java (Arrays.sort() for objects).\nTimSort’s superpower is that it detects natural runs in data, sorts them with Insertion Sort if small, and merges them smartly using a stack-based strategy to ensure efficiency.\n\nWhat Problem Are We Solving??\nIn practice, many datasets aren’t random, they already contain sorted segments (like logs, names, timestamps). TimSort exploits this by:\n\nDetecting ascending/descending runs\nSorting small runs via Insertion Sort\nMerging runs using adaptive Merge Sort\n\nThis yields O(n) performance on already-sorted or nearly-sorted data, far better than standard \\(O(n \\log n)\\) sorts in such cases.\nPerfect for:\n\nPartially sorted lists\nReal-world data (time series, strings, logs)\nStable sorting (preserve order of equals)\n\n\n\nExample\nSort [5, 6, 7, 1, 2, 3, 8, 9]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nDetect runs\n[5,6,7], [1,2,3], [8,9]\n\n\n2\nSort small runs if needed\n[5,6,7], [1,2,3], [8,9]\n\n\n3\nMerge runs pairwise\n[1,2,3,5,6,7,8,9]\n\n\n\nTimSort leverages order already present, fewer merges, faster finish.\n\n\nHow Does It Work (Plain Language)?\nThink of TimSort as a smart librarian:\n\nSees which shelves (runs) are already sorted\nTidies up small messy shelves (Insertion Sort)\nMerges shelves together efficiently (Merge Sort)\nUses stack rules to decide merge timing for balance\n\nIt’s adaptive, stable, and real-world optimized.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nScan the array to find runs (ascending or descending)\n\n\n2\nReverse descending runs\n\n\n3\nIf run length &lt; minrun, extend using Insertion Sort\n\n\n4\nPush runs onto stack\n\n\n5\nMerge runs when stack size conditions are violated\n\n\n6\nContinue until one run remains (fully sorted)\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (Simplified Simulation)\ndef insertion_sort(arr, left, right):\n    for i in range(left + 1, right + 1):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= left and arr[j] &gt; key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\ndef merge(arr, left, mid, right):\n    left_part = arr[left:mid+1]\n    right_part = arr[mid+1:right+1]\n    i = j = 0\n    k = left\n    while i &lt; len(left_part) and j &lt; len(right_part):\n        if left_part[i] &lt;= right_part[j]:\n            arr[k] = left_part[i]\n            i += 1\n        else:\n            arr[k] = right_part[j]\n            j += 1\n        k += 1\n    while i &lt; len(left_part):\n        arr[k] = left_part[i]; i += 1; k += 1\n    while j &lt; len(right_part):\n        arr[k] = right_part[j]; j += 1; k += 1\n\ndef timsort(arr):\n    n = len(arr)\n    minrun = 32\n\n    # Sort small runs using insertion sort\n    for start in range(0, n, minrun):\n        end = min(start + minrun - 1, n - 1)\n        insertion_sort(arr, start, end)\n\n    size = minrun\n    while size &lt; n:\n        for left in range(0, n, 2 * size):\n            mid = min(n - 1, left + size - 1)\n            right = min(n - 1, left + 2 * size - 1)\n            if mid &lt; right:\n                merge(arr, left, mid, right)\n        size *= 2\n\narr = [5,6,7,1,2,3,8,9]\ntimsort(arr)\nprint(arr)\nOutput:\n$$1, 2, 3, 5, 6, 7, 8, 9]\n\n\nC (Conceptual Version)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define MINRUN 32\n\nvoid insertion_sort(int arr[], int left, int right) {\n    for (int i = left + 1; i &lt;= right; i++) {\n        int key = arr[i], j = i - 1;\n        while (j &gt;= left && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nvoid merge(int arr[], int l, int m, int r) {\n    int n1 = m - l + 1, n2 = r - m;\n    int *L = malloc(n1 * sizeof(int)), *R = malloc(n2 * sizeof(int));\n    for (int i = 0; i &lt; n1; i++) L[i] = arr[l + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = arr[m + 1 + j];\n    int i = 0, j = 0, k = l;\n    while (i &lt; n1 && j &lt; n2) arr[k++] = (L[i] &lt;= R[j]) ? L[i++] : R[j++];\n    while (i &lt; n1) arr[k++] = L[i++];\n    while (j &lt; n2) arr[k++] = R[j++];\n    free(L); free(R);\n}\n\nvoid timsort(int arr[], int n) {\n    for (int i = 0; i &lt; n; i += MINRUN) {\n        int end = (i + MINRUN - 1 &lt; n) ? (i + MINRUN - 1) : (n - 1);\n        insertion_sort(arr, i, end);\n    }\n    for (int size = MINRUN; size &lt; n; size *= 2) {\n        for (int left = 0; left &lt; n; left += 2 * size) {\n            int mid = (left + size - 1 &lt; n - 1) ? (left + size - 1) : (n - 1);\n            int right = (left + 2 * size - 1 &lt; n - 1) ? (left + 2 * size - 1) : (n - 1);\n            if (mid &lt; right) merge(arr, left, mid, right);\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {5,6,7,1,2,3,8,9};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    timsort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nDefault sort in Python & Java\nStable and adaptive\nO(n) on sorted data\nO(n log n) worst case\nHandles real-world inputs gracefully\n\nIt’s the perfect sort when you don’t know the data shape, it adapts itself.\n\n\nA Gentle Proof (Why It Works)\nIf data already contains sorted runs of average length \\(r\\):\n\nInsertion Sort: \\(O(r^2)\\) per run (tiny)\n\nMerging \\((n / r)\\) runs: \\(O(n \\log (n / r))\\)\n\nOverall: \\[\nT(n) = O(n + n \\log (n/r))\n\\] For \\(r \\approx n\\): \\(O(n)\\)\nFor \\(r = 1\\): \\(O(n \\log n)\\)\n\n\n\nPhase\nWork\nComplexity\n\n\n\n\nRun Detection\n\\(O(n)\\)\nOne pass\n\n\nLocal Sorting\n\\(O(r^2)\\) per run\nTiny runs\n\n\nMerge Phase\n\\(O(n \\log n)\\)\nBalanced merges\n\n\n\n\n\nTry It Yourself\n\nSort [5,6,7,1,2,3,8,9] step by step.\nDetect natural runs manually.\nReverse descending runs before merge.\nAdjust minrun = 16, see difference.\nTest [1,2,3,4,5], should take O(n).\nAdd random noise to partially sorted list.\nTrack stack of runs, when to merge?\nCompare performance with Merge Sort.\nVisualize merge order tree.\nCheck stability with duplicate keys.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[5,6,7,1,2,3,8,9]\n[1,2,3,5,6,7,8,9]\nMixed runs\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nAlready sorted\n\n\n[9,8,7,6]\n[6,7,8,9]\nReverse run\n\n\n[5,5,5,5]\n[5,5,5,5]\nStability test\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(n)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nTimSort is the real-world champion, it watches your data, adapts instantly, and sorts smarter, not harder. It’s the kind of algorithm that doesn’t just run fast, it thinks fast.\n\n\n\n133 Dual-Pivot QuickSort\nDual-Pivot QuickSort is an enhanced variant of QuickSort that uses two pivots instead of one to partition the array into three regions:\n\nElements less than pivot1,\nElements between pivot1 and pivot2,\nElements greater than pivot2.\n\nThis approach often reduces comparisons and improves cache efficiency. It was popularized by Vladimir Yaroslavskiy and became the default sorting algorithm in Java (from Java 7) for primitive types.\n\nWhat Problem Are We Solving??\nStandard QuickSort splits the array into two parts using one pivot. Dual-Pivot QuickSort splits into three, reducing recursion depth and overhead.\nIt’s optimized for:\n\nLarge arrays of primitives (integers, floats)\nRandom and uniform data\nModern CPUs with deep pipelines and caches\n\nIt offers better real-world performance than classic QuickSort, even if asymptotic complexity remains \\(O(n \\log n)\\).\n\n\nExample\nSort [9, 3, 1, 7, 5, 4, 6, 2, 8]\nChoose pivots (\\(p_1 = 3\\), \\(p_2 = 7\\)):\n\n\n\nRegion\nCondition\nElements\n\n\n\n\nLeft\n\\(x &lt; 3\\)\n[1, 2]\n\n\nMiddle\n\\(3 \\le x \\le 7\\)\n[3, 4, 5, 6, 7]\n\n\nRight\n\\(x &gt; 7\\)\n[8, 9]\n\n\n\nRecurse on each region.\nFinal result: [1, 2, 3, 4, 5, 6, 7, 8, 9].\n\n\nHow Does It Work (Plain Language)?\nImagine sorting shoes by size with two markers:\n\nSmall shelf for sizes less than 7\nMiddle shelf for 7–9\nBig shelf for 10+\n\nYou walk through once, placing each shoe in the right group, then sort each shelf individually.\nDual-Pivot QuickSort does exactly that: partition into three zones in one pass, then recurse.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nChoose two pivots (\\(p_1\\), \\(p_2\\)), ensuring \\(p_1 &lt; p_2\\)\n\n\n2\nPartition array into 3 parts: &lt; p₁, p₁..p₂, &gt; p₂\n\n\n3\nRecursively sort each part\n\n\n4\nCombine results in order\n\n\n\nIf \\(p_1 &gt; p_2\\), swap them first.\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid dual_pivot_quicksort(int arr[], int low, int high) {\n    if (low &gt;= high) return;\n\n    if (arr[low] &gt; arr[high]) swap(&arr[low], &arr[high]);\n    int p1 = arr[low], p2 = arr[high];\n\n    int lt = low + 1, gt = high - 1, i = low + 1;\n\n    while (i &lt;= gt) {\n        if (arr[i] &lt; p1) {\n            swap(&arr[i], &arr[lt]);\n            lt++; i++;\n        } else if (arr[i] &gt; p2) {\n            swap(&arr[i], &arr[gt]);\n            gt--;\n        } else {\n            i++;\n        }\n    }\n    lt--; gt++;\n    swap(&arr[low], &arr[lt]);\n    swap(&arr[high], &arr[gt]);\n\n    dual_pivot_quicksort(arr, low, lt - 1);\n    dual_pivot_quicksort(arr, lt + 1, gt - 1);\n    dual_pivot_quicksort(arr, gt + 1, high);\n}\n\nint main(void) {\n    int arr[] = {9,3,1,7,5,4,6,2,8};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    dual_pivot_quicksort(arr, 0, n-1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nPython\ndef dual_pivot_quicksort(arr):\n    def sort(low, high):\n        if low &gt;= high:\n            return\n        if arr[low] &gt; arr[high]:\n            arr[low], arr[high] = arr[high], arr[low]\n        p1, p2 = arr[low], arr[high]\n        lt, gt, i = low + 1, high - 1, low + 1\n        while i &lt;= gt:\n            if arr[i] &lt; p1:\n                arr[i], arr[lt] = arr[lt], arr[i]\n                lt += 1; i += 1\n            elif arr[i] &gt; p2:\n                arr[i], arr[gt] = arr[gt], arr[i]\n                gt -= 1\n            else:\n                i += 1\n        lt -= 1; gt += 1\n        arr[low], arr[lt] = arr[lt], arr[low]\n        arr[high], arr[gt] = arr[gt], arr[high]\n        sort(low, lt - 1)\n        sort(lt + 1, gt - 1)\n        sort(gt + 1, high)\n    sort(0, len(arr) - 1)\n    return arr\n\narr = [9,3,1,7,5,4,6,2,8]\nprint(dual_pivot_quicksort(arr))\n\n\n\nWhy It Matters\n\nDefault in Java for primitives\nFewer comparisons than single-pivot QuickSort\nCache-friendly (less branching)\nStable recursion depth with three partitions\n\n\n\nA Gentle Proof (Why It Works)\nEach partitioning step processes all elements once, \\(O(n)\\). Recursion on three smaller subarrays yields total cost:\n\\[\nT(n) = T(k_1) + T(k_2) + T(k_3) + O(n)\n\\] On average, partitions are balanced → \\(T(n) = O(n \\log n)\\)\n\n\n\nPhase\nOperation\nComplexity\n\n\n\n\nPartitioning\n\\(O(n)\\)\nOne pass\n\n\nRecursion\n3 subarrays\nBalanced depth\n\n\nTotal\n\\(O(n \\log n)\\)\nAverage / Worst\n\n\n\n\n\nTry It Yourself\n\nSort [9,3,1,7,5,4,6,2,8] step by step.\nChoose pivots manually: smallest and largest.\nTrace index movements (lt, gt, i).\nCompare with classic QuickSort partition count.\nUse reversed array, observe stability.\nAdd duplicates [5,5,5,5], see middle zone effect.\nMeasure comparisons vs single pivot.\nTry large input (10⁶) and time it.\nVisualize three partitions recursively.\nImplement tail recursion optimization.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[9,3,1,7,5,4,6,2,8]\n[1,2,3,4,5,6,7,8,9]\nExample\n\n\n[1,2,3,4]\n[1,2,3,4]\nAlready sorted\n\n\n[9,8,7,6,5]\n[5,6,7,8,9]\nReverse\n\n\n[5,5,5,5]\n[5,5,5,5]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(log n)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nDual-Pivot QuickSort slices data with two blades instead of one, making each cut smaller, shallower, and more balanced. A sharper, smarter evolution of a timeless classic.\n\n\n\n134 SmoothSort\nSmoothSort is an adaptive comparison-based sorting algorithm invented by Edsger Dijkstra. It’s similar in spirit to Heap Sort, but smarter, it runs in O(n) time on already sorted data and O(n log n) in the worst case.\nThe key idea is to build a special heap structure (Leonardo heap) that adapts to existing order in the data. When the array is nearly sorted, it finishes quickly. When not, it gracefully falls back to heap-like performance.\n\nWhat Problem Are We Solving??\nHeap Sort always works in \\(O(n \\log n)\\), even when the input is already sorted. SmoothSort improves on this by being adaptive, the more ordered the input, the faster it gets.\nIt’s ideal for:\n\nNearly sorted arrays\nSituations requiring guaranteed upper bounds\nMemory-constrained environments (in-place sort)\n\n\n\nExample\nSort [1, 2, 4, 3, 5]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nBuild initial heap (Leonardo structure)\n[1,2,4,3,5]\n\n\n2\nDetect small disorder (4,3)\nSwap\n\n\n3\nRestore heap property\n[1,2,3,4,5]\n\n\n4\nSorted early, no full rebuild needed\nDone\n\n\n\nResult: finished early since only minor disorder existed.\n\n\nHow Does It Work (Plain Language)?\nImagine a stack of heaps, each representing a Fibonacci-like sequence (Leonardo numbers). You grow this structure as you read the array, maintaining order locally. When disorder appears, you fix only where needed, not everywhere.\nSo SmoothSort is like a gentle gardener: it only trims where weeds grow, not the whole garden.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nRepresent the array as a series of Leonardo heaps\n\n\n2\nAdd elements one by one, updating the heap sequence\n\n\n3\nMaintain heap property with minimal swaps\n\n\n4\nWhen finished, repeatedly extract the maximum (like Heap Sort)\n\n\n5\nDuring extraction, merge smaller heaps as needed\n\n\n\nLeonardo numbers guide heap sizes: ( L(0)=1, L(1)=1, L(n)=L(n-1)+L(n-2)+1 )\n\n\nTiny Code (Easy Versions)\n\n\nPython (Simplified Adaptive Sort)\nThis is a simplified version to show adaptiveness, not a full Leonardo heap implementation.\ndef smoothsort(arr):\n    # Simplified: detect sorted runs, fix only where needed\n    n = len(arr)\n    for i in range(1, n):\n        j = i\n        while j &gt; 0 and arr[j] &lt; arr[j - 1]:\n            arr[j], arr[j - 1] = arr[j - 1], arr[j]\n            j -= 1\n    return arr\n\narr = [1, 2, 4, 3, 5]\nprint(smoothsort(arr))\nOutput:\n$$1, 2, 3, 4, 5]\n(Note: Real SmoothSort uses Leonardo heaps, complex but in-place and efficient.)\n\n\nC (Conceptual Heap Approach)\n#include &lt;stdio.h&gt;\n\nvoid insertion_like_fix(int arr[], int n) {\n    for (int i = 1; i &lt; n; i++) {\n        int j = i;\n        while (j &gt; 0 && arr[j] &lt; arr[j - 1]) {\n            int tmp = arr[j];\n            arr[j] = arr[j - 1];\n            arr[j - 1] = tmp;\n            j--;\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {1,2,4,3,5};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    insertion_like_fix(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nThis mimics SmoothSort’s adaptiveness: fix locally, not globally.\n\n\nWhy It Matters\n\nAdaptive: faster on nearly sorted data\nIn-place: no extra memory\nGuaranteed bound: never worse than \\(O(n \\log n)\\)\nHistorical gem: Dijkstra’s innovation in sorting theory\n\n\n\nA Gentle Proof (Why It Works)\nFor sorted input:\n\nEach insertion requires no swaps → \\(O(n)\\)\n\nFor random input:\n\nHeap restorations per insertion → \\(O(\\log n)\\)\n\nTotal cost: \\(O(n \\log n)\\)\n\n\n\n\nCase\nBehavior\nComplexity\n\n\n\n\nBest (Sorted)\nMinimal swaps\n\\(O(n)\\)\n\n\nAverage\nModerate reheapify\n\\(O(n \\log n)\\)\n\n\nWorst\nFull heap rebuilds\n\\(O(n \\log n)\\)\n\n\n\nSmoothSort adapts between these seamlessly.\n\n\nTry It Yourself\n\nSort [1, 2, 4, 3, 5] step by step.\nTry [1, 2, 3, 4, 5], measure comparisons.\nTry [5, 4, 3, 2, 1], full workload.\nCount swaps in each case.\nCompare to Heap Sort.\nVisualize heap sizes as Leonardo sequence.\nImplement run detection.\nExperiment with large partially sorted arrays.\nTrack adaptive speedup.\nWrite Leonardo heap builder.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[1,2,4,3,5]\n[1,2,3,4,5]\nMinor disorder\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nAlready sorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nFull rebuild\n\n\n[2,1,3,5,4]\n[1,2,3,4,5]\nMixed case\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nYes\n\n\n\nSmoothSort glides gracefully across the array, fixing only what’s broken. It’s sorting that listens to your data, quiet, clever, and precise.\n\n\n\n135 Block Merge Sort\nBlock Merge Sort is a cache-efficient, stable sorting algorithm that merges data using small fixed-size blocks instead of large temporary arrays. It improves on standard Merge Sort by reducing memory usage and enhancing locality of reference, making it a great choice for modern hardware and large datasets.\nIt’s designed to keep data cache-friendly, in-place (or nearly), and stable, making it a practical choice for systems with limited memory bandwidth or tight memory constraints.\n\nWhat Problem Are We Solving??\nClassic Merge Sort is stable and O(n log n), but it needs O(n) extra space. Block Merge Sort solves this by using blocks of fixed size (often √n) as temporary buffers for merging.\nIt aims to:\n\nKeep stability\nUse limited extra memory\nMaximize cache reuse\nMaintain predictable access patterns\n\nIdeal for:\n\nLarge arrays\nExternal memory sorting\nSystems with cache hierarchies\n\n\n\nExample\nSort [8, 3, 5, 1, 6, 2, 7, 4]\n\n\n\n\n\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nDivide into sorted runs (via insertion sort)\n[3,8], [1,5], [2,6], [4,7]\n\n\n2\nMerge adjacent blocks using buffer\n[1,3,5,8], [2,4,6,7]\n\n\n3\nMerge merged blocks with block buffer\n[1,2,3,4,5,6,7,8]\n\n\n\nInstead of full arrays, it uses small block buffers, fewer cache misses, less extra space.\n\n\nHow Does It Work (Plain Language)?\nThink of merging two sorted shelves in a library, but instead of taking all books off, you move a small block at a time, swapping them in place with a small temporary cart.\nYou slide the buffer along the shelves, merge gradually, efficiently, with minimal movement.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nDivide input into runs (sorted subarrays)\n\n\n2\nUse insertion sort or binary insertion to sort each run\n\n\n3\nAllocate small buffer (block)\n\n\n4\nMerge runs pairwise using the block buffer\n\n\n5\nRepeat until one sorted array remains\n\n\n\nBlock merges rely on rotations and buffer swapping to minimize extra space.\n\n\nTiny Code (Easy Versions)\n\n\nPython (Simplified Version)\nThis version simulates block merging using chunks.\ndef insertion_sort(arr, start, end):\n    for i in range(start + 1, end):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= start and arr[j] &gt; key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\ndef merge(arr, left, mid, right):\n    left_part = arr[left:mid]\n    right_part = arr[mid:right]\n    i = j = 0\n    k = left\n    while i &lt; len(left_part) and j &lt; len(right_part):\n        if left_part[i] &lt;= right_part[j]:\n            arr[k] = left_part[i]; i += 1\n        else:\n            arr[k] = right_part[j]; j += 1\n        k += 1\n    while i &lt; len(left_part):\n        arr[k] = left_part[i]; i += 1; k += 1\n    while j &lt; len(right_part):\n        arr[k] = right_part[j]; j += 1; k += 1\n\ndef block_merge_sort(arr, block_size=32):\n    n = len(arr)\n    # Step 1: sort small blocks\n    for start in range(0, n, block_size):\n        end = min(start + block_size, n)\n        insertion_sort(arr, start, end)\n\n    # Step 2: merge adjacent blocks\n    size = block_size\n    while size &lt; n:\n        for left in range(0, n, 2 * size):\n            mid = min(left + size, n)\n            right = min(left + 2 * size, n)\n            merge(arr, left, mid, right)\n        size *= 2\n    return arr\n\narr = [8, 3, 5, 1, 6, 2, 7, 4]\nprint(block_merge_sort(arr))\nOutput:\n$$1, 2, 3, 4, 5, 6, 7, 8]\n\n\nC (Simplified Concept)\n#include &lt;stdio.h&gt;\n\nvoid insertion_sort(int arr[], int left, int right) {\n    for (int i = left + 1; i &lt; right; i++) {\n        int key = arr[i], j = i - 1;\n        while (j &gt;= left && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nvoid merge(int arr[], int left, int mid, int right) {\n    int n1 = mid - left, n2 = right - mid;\n    int L[n1], R[n2];\n    for (int i = 0; i &lt; n1; i++) L[i] = arr[left + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = arr[mid + j];\n\n    int i = 0, j = 0, k = left;\n    while (i &lt; n1 && j &lt; n2) arr[k++] = (L[i] &lt;= R[j]) ? L[i++] : R[j++];\n    while (i &lt; n1) arr[k++] = L[i++];\n    while (j &lt; n2) arr[k++] = R[j++];\n}\n\nvoid block_merge_sort(int arr[], int n, int block_size) {\n    for (int i = 0; i &lt; n; i += block_size) {\n        int end = (i + block_size &lt; n) ? i + block_size : n;\n        insertion_sort(arr, i, end);\n    }\n    for (int size = block_size; size &lt; n; size *= 2) {\n        for (int left = 0; left &lt; n; left += 2 * size) {\n            int mid = (left + size &lt; n) ? left + size : n;\n            int right = (left + 2 * size &lt; n) ? left + 2 * size : n;\n            if (mid &lt; right) merge(arr, left, mid, right);\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {8,3,5,1,6,2,7,4};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    block_merge_sort(arr, n, 2);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nStable like Merge Sort\nIn-place or low-space variant\nCache-efficient due to block locality\nPractical for large arrays or external sorting\n\n\n\nA Gentle Proof (Why It Works)\nEach merge level processes \\(n\\) elements: \\(O(n)\\).\nNumber of levels = \\(\\log_2 (n / b)\\), where \\(b\\) is the block size.\nSo total: \\[\nT(n) = O(n \\log (n / b))\n\\]\nWhen \\(b\\) is large (like \\(\\sqrt{n}\\)), space and time balance nicely.\n\n\n\nPhase\nOperation\nCost\n\n\n\n\nBlock sorting\n\\(\\frac{n}{b} \\times b^2\\)\n\\(O(nb)\\)\n\n\nMerging\n\\(\\log (n / b) \\times n\\)\n\\(O(n \\log n)\\)\n\n\n\nFor typical settings, it’s near \\(O(n \\log n)\\) but cache-optimized.\n\n\nTry It Yourself\n\nSort [8,3,5,1,6,2,7,4] with block size 2.\nIncrease block size to 4, compare steps.\nTrack number of merges.\nCheck stability with duplicates.\nCompare with standard Merge Sort.\nTime on sorted input (adaptive check).\nMeasure cache misses (simulated).\nTry large array (10000+) for memory gain.\nMix ascending and descending runs.\nImplement block buffer rotation manually.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[8,3,5,1,6,2,7,4]\n[1,2,3,4,5,6,7,8]\nClassic\n\n\n[5,5,3,3,1]\n[1,3,3,5,5]\nStable\n\n\n[1,2,3,4]\n[1,2,3,4]\nSorted\n\n\n[9,8,7]\n[7,8,9]\nReverse order\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(b)\n\n\nStable\nYes\n\n\nAdaptive\nPartially\n\n\n\nBlock Merge Sort is the engineer’s Merge Sort, same elegance, less memory, smarter on hardware. It merges not by brute force, but by careful block juggling, balancing speed, space, and stability.\n\n\n\n136 Adaptive Merge Sort\nAdaptive Merge Sort is a stable, comparison-based sorting algorithm that adapts to existing order in the input data. It builds on the idea that real-world datasets are often partially sorted, so by detecting runs (already sorted sequences) and merging them intelligently, it can achieve O(n) time on nearly sorted data while retaining O(n log n) in the worst case.\nIt’s a family of algorithms, including Natural Merge Sort, TimSort, and GrailSort, that all share one key insight: work less when less work is needed.\n\nWhat Problem Are We Solving??\nStandard Merge Sort treats every input the same, even if it’s already sorted. Adaptive Merge Sort improves this by:\n\nDetecting sorted runs (ascending or descending)\nMerging runs instead of single elements\nAchieving linear time on sorted or partially sorted data\n\nThis makes it perfect for:\n\nTime series data\nSorted or semi-sorted logs\nIncrementally updated lists\n\n\n\nExample\nSort [1, 2, 5, 3, 4, 6]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nDetect runs: [1,2,5], [3,4,6]\nFound 2 runs\n\n\n2\nMerge runs\n[1,2,3,4,5,6]\n\n\n3\nDone\nSorted\n\n\n\nOnly one merge needed, input was nearly sorted, so runtime is close to O(n).\n\n\nHow Does It Work (Plain Language)?\nImagine you’re sorting a shelf of books that’s mostly organized — you don’t pull all the books off; you just spot where order breaks, and fix those parts.\nAdaptive Merge Sort does exactly this:\n\nScan once to find sorted parts\nMerge runs using a stable merge procedure\n\nIt’s lazy where it can be, efficient where it must be.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nScan input to find runs (ascending or descending)\n\n\n2\nReverse descending runs\n\n\n3\nPush runs onto a stack\n\n\n4\nMerge runs when stack conditions are violated (size or order)\n\n\n5\nContinue until one sorted run remains\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (Natural Merge Sort)\ndef natural_merge_sort(arr):\n    n = len(arr)\n    runs = []\n    i = 0\n    # Step 1: Detect sorted runs\n    while i &lt; n:\n        start = i\n        i += 1\n        while i &lt; n and arr[i] &gt;= arr[i - 1]:\n            i += 1\n        runs.append(arr[start:i])\n    # Step 2: Merge runs pairwise\n    while len(runs) &gt; 1:\n        new_runs = []\n        for j in range(0, len(runs), 2):\n            if j + 1 &lt; len(runs):\n                new_runs.append(merge(runs[j], runs[j+1]))\n            else:\n                new_runs.append(runs[j])\n        runs = new_runs\n    return runs[0]\n\ndef merge(left, right):\n    i = j = 0\n    result = []\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt;= right[j]:\n            result.append(left[i]); i += 1\n        else:\n            result.append(right[j]); j += 1\n    result.extend(left[i:])\n    result.extend(right[j:])\n    return result\n\narr = [1, 2, 5, 3, 4, 6]\nprint(natural_merge_sort(arr))\nOutput:\n$$1, 2, 3, 4, 5, 6]\n\n\nC (Simplified Conceptual)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid merge(int arr[], int l, int m, int r) {\n    int n1 = m - l + 1, n2 = r - m;\n    int L[n1], R[n2];\n    for (int i = 0; i &lt; n1; i++) L[i] = arr[l + i];\n    for (int j = 0; j &lt; n2; j++) R[j] = arr[m + 1 + j];\n    int i = 0, j = 0, k = l;\n    while (i &lt; n1 && j &lt; n2) arr[k++] = (L[i] &lt;= R[j]) ? L[i++] : R[j++];\n    while (i &lt; n1) arr[k++] = L[i++];\n    while (j &lt; n2) arr[k++] = R[j++];\n}\n\nvoid adaptive_merge_sort(int arr[], int n) {\n    int start = 0;\n    while (start &lt; n - 1) {\n        int mid = start;\n        while (mid &lt; n - 1 && arr[mid] &lt;= arr[mid + 1]) mid++;\n        int end = mid + 1;\n        while (end &lt; n - 1 && arr[end] &lt;= arr[end + 1]) end++;\n        if (end &lt; n) merge(arr, start, mid, end);\n        start = end + 1;\n    }\n}\n\nint main(void) {\n    int arr[] = {1, 2, 5, 3, 4, 6};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    adaptive_merge_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nStable and adaptive\nLinear time on nearly sorted data\nWorks well for real-world sequences\nForms the core idea behind TimSort\nRequires no extra knowledge about data\n\nIt’s the sorting algorithm that notices your data’s effort and rewards it.\n\n\nA Gentle Proof (Why It Works)\nLet average run length = \\(r\\).\nThen number of runs ≈ \\(n / r\\).\nEach merge = \\(O(n)\\) per level.\nDepth = \\(O(\\log (n / r))\\).\nSo total cost: \\[\nT(n) = O(n \\log (n / r))\n\\]\nIf \\(r = n\\) (already sorted): \\(O(n)\\).\nIf \\(r = 1\\): \\(O(n \\log n)\\).\n\n\n\nCase\nRun Length\nComplexity\n\n\n\n\nSorted\n\\(r = n\\)\n\\(O(n)\\)\n\n\nNearly sorted\n\\(r\\) large\n\\(O(n \\log (n / r))\\)\n\n\nRandom\n\\(r\\) small\n\\(O(n \\log n)\\)\n\n\n\n\n\nTry It Yourself\n\nSort [1, 2, 5, 3, 4, 6] step by step.\nTry [1, 2, 3, 4, 5], should detect 1 run.\nReverse a section, see new runs.\nMerge runs manually using table.\nCompare performance to Merge Sort.\nAdd duplicates, check stability.\nUse 10k sorted elements, time the run.\nMix ascending and descending subarrays.\nVisualize run detection.\nImplement descending-run reversal.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[1,2,5,3,4,6]\n[1,2,3,4,5,6]\nTwo runs\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nAlready sorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse (many runs)\n\n\n[2,2,1,1]\n[1,1,2,2]\nStable\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(n)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nAdaptive Merge Sort is like a thoughtful sorter, it looks before it leaps. If your data’s halfway there, it’ll meet it in the middle.\n\n\n\n137 PDQSort (Pattern-Defeating QuickSort)\nPDQSort is a modern, adaptive, in-place sorting algorithm that extends QuickSort with pattern detection, branchless partitioning, and fallback mechanisms to guarantee \\(O(n \\log n)\\) performance even on adversarial inputs.\nInvented by Orson Peters, it’s used in C++’s std::sort() (since C++17) and often outperforms traditional QuickSort and IntroSort in real-world scenarios due to better cache behavior, branch prediction, and adaptive pivoting.\n\nWhat Problem Are We Solving??\nClassic QuickSort performs well on average but can degrade to (O(n^2)) on structured or repetitive data. PDQSort solves this by:\n\nDetecting bad patterns (e.g., sorted input)\nSwitching strategy (to heap sort or insertion sort)\nBranchless partitioning for modern CPUs\nAdaptive pivot selection (median-of-3, Tukey ninther)\n\nIt keeps speed, stability of performance, and cache-friendliness.\nPerfect for:\n\nLarge unsorted datasets\nPartially sorted data\nReal-world data with patterns\n\n\n\nExample\nSort [1, 2, 3, 4, 5] (already sorted)\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nDetect sorted pattern\nPattern found\n\n\n2\nSwitch to Insertion Sort\nEfficient handling\n\n\n3\nOutput sorted\n[1,2,3,4,5]\n\n\n\nInstead of recursive QuickSort calls, PDQSort defeats the pattern by adapting.\n\n\nHow Does It Work (Plain Language)?\nPDQSort is like a smart chef:\n\nIt tastes the input first (checks pattern)\nChooses a recipe (pivot rule, sorting fallback)\nAdjusts to the kitchen conditions (CPU caching, branching)\n\nIt never wastes effort, detecting when recursion or comparisons are unnecessary.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nPick pivot adaptively (median-of-3, ninther)\n\n\n2\nPartition elements into &lt; pivot and &gt; pivot\n\n\n3\nDetect bad patterns (sorted, reversed, many equal elements)\n\n\n4\nApply branchless partitioning to reduce CPU mispredictions\n\n\n5\nRecurse or switch to heap/insertion sort if needed\n\n\n6\nUse tail recursion elimination\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (Conceptual Simplified PDQSort)\ndef pdqsort(arr):\n    def insertion_sort(a, lo, hi):\n        for i in range(lo + 1, hi):\n            key = a[i]\n            j = i - 1\n            while j &gt;= lo and a[j] &gt; key:\n                a[j + 1] = a[j]\n                j -= 1\n            a[j + 1] = key\n\n    def partition(a, lo, hi):\n        pivot = a[(lo + hi) // 2]\n        i, j = lo, hi - 1\n        while True:\n            while a[i] &lt; pivot: i += 1\n            while a[j] &gt; pivot: j -= 1\n            if i &gt;= j: return j\n            a[i], a[j] = a[j], a[i]\n            i += 1; j -= 1\n\n    def _pdqsort(a, lo, hi, depth):\n        n = hi - lo\n        if n &lt;= 16:\n            insertion_sort(a, lo, hi)\n            return\n        if depth == 0:\n            a[lo:hi] = sorted(a[lo:hi])  # heap fallback\n            return\n        mid = partition(a, lo, hi)\n        _pdqsort(a, lo, mid + 1, depth - 1)\n        _pdqsort(a, mid + 1, hi, depth - 1)\n\n    _pdqsort(arr, 0, len(arr), len(arr).bit_length() * 2)\n    return arr\n\narr = [1, 5, 3, 4, 2]\nprint(pdqsort(arr))\nOutput:\n$$1, 2, 3, 4, 5]\n\n\nC (Simplified PDQ-Style)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid insertion_sort(int arr[], int lo, int hi) {\n    for (int i = lo + 1; i &lt; hi; i++) {\n        int key = arr[i], j = i - 1;\n        while (j &gt;= lo && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nint partition(int arr[], int lo, int hi) {\n    int pivot = arr[(lo + hi) / 2];\n    int i = lo, j = hi - 1;\n    while (1) {\n        while (arr[i] &lt; pivot) i++;\n        while (arr[j] &gt; pivot) j--;\n        if (i &gt;= j) return j;\n        int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;\n        i++; j--;\n    }\n}\n\nvoid pdqsort(int arr[], int lo, int hi, int depth) {\n    int n = hi - lo;\n    if (n &lt;= 16) { insertion_sort(arr, lo, hi); return; }\n    if (depth == 0) { qsort(arr + lo, n, sizeof(int), (__compar_fn_t)strcmp); return; }\n    int mid = partition(arr, lo, hi);\n    pdqsort(arr, lo, mid + 1, depth - 1);\n    pdqsort(arr, mid + 1, hi, depth - 1);\n}\n\nint main(void) {\n    int arr[] = {1, 5, 3, 4, 2};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    pdqsort(arr, 0, n, 2 * 32);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nModern default for high-performance sorting\nPattern detection avoids worst cases\nBranchless partitioning → fewer CPU stalls\nHeap fallback ensures \\(O(n \\log n)\\) bound\nAdaptive like TimSort, but in-place\n\nPDQSort combines speed, safety, and hardware awareness.\n\n\nA Gentle Proof (Why It Works)\nPDQSort adds mechanisms to defeat QuickSort’s pitfalls:\n\nBad pattern detection → early fallback\nBalanced pivoting → near-equal splits\nBranchless operations → efficient execution\n\nSo overall: \\[\nT(n) = O(n \\log n)\n\\] Best case (sorted): near O(n), thanks to early detection.\n\n\n\nCase\nBehavior\nComplexity\n\n\n\n\nBest\nSorted or nearly sorted\nO(n)\n\n\nAverage\nRandom\nO(n log n)\n\n\nWorst\nAdversarial\nO(n log n)\n\n\n\n\n\nTry It Yourself\n\nSort [1,2,3,4,5] → detect sorted input.\nTry [5,4,3,2,1] → reversed.\nRandom input of 1000 numbers.\nDuplicate-heavy [5,5,5,5].\nPatterned input [1,3,2,4,3,5,4].\nCompare with QuickSort and HeapSort.\nCount recursion depth.\nBenchmark branchless vs classic partition.\nVisualize fallback triggers.\nMeasure comparisons per element.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[1,5,3,4,2]\n[1,2,3,4,5]\nRandom\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nSorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReversed\n\n\n[5,5,5,5]\n[5,5,5,5]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(log n)\n\n\nStable\nNo\n\n\nAdaptive\nYes\n\n\n\nPDQSort is the ninja of sorting, lightning fast, pattern-aware, and always one step ahead of your data. It doesn’t just sort, it outsmarts the input.\n\n\n\n138 WikiSort\nWikiSort is a stable, in-place merge sort created by Mike Day, designed to combine the stability of Merge Sort with the low memory usage of in-place algorithms. It achieves O(n log n) performance and uses only O(1) extra memory, making it one of the most space-efficient stable sorts available.\nUnlike classic Merge Sort, which allocates a full-size temporary array, WikiSort performs block merges with rotation operations, merging sorted regions directly within the array.\n\nWhat Problem Are We Solving??\nMost stable sorting algorithms (like Merge Sort) need O(n) extra space. WikiSort solves this by performing stable merges in place, using:\n\nBlock rotations instead of large buffers\nAdaptive merging when possible\nSmall local buffers reused efficiently\n\nPerfect for:\n\nMemory-constrained environments\nSorting large arrays in embedded systems\nStable sorts without heavy allocation\n\n\n\nExample\nSort [3, 5, 1, 2, 4]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nDivide into sorted runs\n[3,5], [1,2,4]\n\n\n2\nMerge using block rotations\n[1,2,3,4,5]\n\n\n3\nDone, stable and in-place\nFinal sorted array\n\n\n\nResult: [1,2,3,4,5], sorted, stable, and minimal extra space.\n\n\nHow Does It Work (Plain Language)?\nThink of two sorted shelves of books, instead of moving all books to a table, you rotate sections in place so the shelves merge seamlessly.\nWikiSort keeps a small local buffer (like a tray), uses it to move small chunks, then rotates segments of the array into place. It never needs a second full array.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nSplit array into sorted runs\n\n\n2\nAllocate small buffer (≈ √n elements)\n\n\n3\nMerge adjacent runs using buffer\n\n\n4\nRotate blocks to maintain stability\n\n\n5\nRepeat until one sorted region remains\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (Simplified In-Place Stable Merge)\nThis is a conceptual demonstration of stable in-place merging.\ndef rotate(arr, start, mid, end):\n    arr[start:end] = arr[mid:end] + arr[start:mid]\n\ndef merge_in_place(arr, start, mid, end):\n    left = arr[start:mid]\n    i, j, k = 0, mid, start\n    while i &lt; len(left) and j &lt; end:\n        if left[i] &lt;= arr[j]:\n            arr[k] = left[i]; i += 1\n        else:\n            val = arr[j]\n            rotate(arr, i + start, j, j + 1)\n            arr[k] = val\n            j += 1\n        k += 1\n    while i &lt; len(left):\n        arr[k] = left[i]; i += 1; k += 1\n\ndef wiki_sort(arr):\n    n = len(arr)\n    size = 1\n    while size &lt; n:\n        for start in range(0, n, 2 * size):\n            mid = min(start + size, n)\n            end = min(start + 2 * size, n)\n            if mid &lt; end:\n                merge_in_place(arr, start, mid, end)\n        size *= 2\n    return arr\n\narr = [3,5,1,2,4]\nprint(wiki_sort(arr))\nOutput:\n$$1, 2, 3, 4, 5]\n\n\nC (Simplified Conceptual Version)\n#include &lt;stdio.h&gt;\n\nvoid rotate(int arr[], int start, int mid, int end) {\n    int temp[end - start];\n    int idx = 0;\n    for (int i = mid; i &lt; end; i++) temp[idx++] = arr[i];\n    for (int i = start; i &lt; mid; i++) temp[idx++] = arr[i];\n    for (int i = 0; i &lt; end - start; i++) arr[start + i] = temp[i];\n}\n\nvoid merge_in_place(int arr[], int start, int mid, int end) {\n    int i = start, j = mid;\n    while (i &lt; j && j &lt; end) {\n        if (arr[i] &lt;= arr[j]) {\n            i++;\n        } else {\n            int value = arr[j];\n            rotate(arr, i, j, j + 1);\n            arr[i] = value;\n            i++; j++;\n        }\n    }\n}\n\nvoid wiki_sort(int arr[], int n) {\n    for (int size = 1; size &lt; n; size *= 2) {\n        for (int start = 0; start &lt; n; start += 2 * size) {\n            int mid = (start + size &lt; n) ? start + size : n;\n            int end = (start + 2 * size &lt; n) ? start + 2 * size : n;\n            if (mid &lt; end) merge_in_place(arr, start, mid, end);\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {3,5,1,2,4};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    wiki_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nStable and in-place (O(1) extra space)\nPractical for memory-limited systems\nPredictable performance\nCache-friendly merges\nGreat balance of theory and practice\n\nIt brings the best of Merge Sort (stability) and in-place algorithms (low memory).\n\n\nA Gentle Proof (Why It Works)\nEach merge takes \\(O(n)\\), and there are \\(O(\\log n)\\) levels of merging:\n\\[\nT(n) = O(n \\log n)\n\\]\nExtra memory = small buffer (\\(O(\\sqrt{n})\\)) or even constant space.\n\n\n\nPhase\nOperation\nCost\n\n\n\n\nBlock detection\nScan runs\n\\(O(n)\\)\n\n\nMerging\nRotation-based merge\n\\(O(n \\log n)\\)\n\n\nSpace\nFixed buffer\n\\(O(1)\\)\n\n\n\n\n\nTry It Yourself\n\nSort [3,5,1,2,4] step by step.\nVisualize rotations during merge.\nAdd duplicates [2,2,3,1], verify stability.\nIncrease size to 16, track buffer reuse.\nCompare with Merge Sort memory usage.\nMeasure swaps vs Merge Sort.\nTry [1,2,3,4], minimal rotations.\nReverse [5,4,3,2,1], max work.\nImplement block rotation helper.\nMeasure runtime on sorted input.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,5,1,2,4]\n[1,2,3,4,5]\nBasic\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nWorst case\n\n\n[1,2,3,4]\n[1,2,3,4]\nAlready sorted\n\n\n[2,2,3,1]\n[1,2,2,3]\nStable behavior\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nWikiSort is the minimalist’s Merge Sort, stable, elegant, and almost memory-free. It merges not by copying, but by rotating, smooth, steady, and space-savvy.\n\n\n\n139 GrailSort\nGrailSort (short for “Greedy Adaptive In-place stable Sort”) is a stable, in-place, comparison-based sorting algorithm that merges sorted subarrays using block merging and local buffers.\nCreated by Michał Oryńczak, GrailSort combines the stability and adaptiveness of Merge Sort with in-place operation, needing only a tiny internal buffer (often \\(O(\\sqrt{n})\\)) or even no extra memory in its pure variant.\nIt’s designed for practical stable sorting when memory is tight, achieving \\(O(n \\log n)\\) worst-case time.\n\nWhat Problem Are We Solving??\nTypical stable sorting algorithms (like Merge Sort) require O(n) extra space. GrailSort solves this by:\n\nUsing small local buffers instead of large arrays\nPerforming in-place stable merges\nDetecting and reusing natural runs (adaptive behavior)\n\nPerfect for:\n\nMemory-constrained systems\nEmbedded devices\nLarge stable sorts on limited RAM\n\n\n\nExample\nSort [4, 1, 3, 2, 5]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nDetect short runs\n[4,1], [3,2], [5]\n\n\n2\nSort each run\n[1,4], [2,3], [5]\n\n\n3\nMerge runs using block rotation\n[1,2,3,4,5]\n\n\n4\nStable order preserved\nDone\n\n\n\nAll merges are done in-place, with a small reusable buffer.\n\n\nHow Does It Work (Plain Language)?\nThink of GrailSort like a clever librarian with a tiny desk (the buffer). Instead of taking all books off the shelf, they:\n\nDivide shelves into small sorted groups,\nKeep a few aside as a helper buffer,\nMerge shelves directly on the rack by rotating sections in place.\n\nIt’s stable, in-place, and adaptive, a rare combination.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nDetect and sort small runs (Insertion Sort)\n\n\n2\nChoose small buffer (e.g., √n elements)\n\n\n3\nMerge runs pairwise using buffer and rotation\n\n\n4\nGradually increase block size (1, 2, 4, 8, …)\n\n\n5\nContinue merging until fully sorted\n\n\n\nThe algorithm’s structure mirrors Merge Sort but uses block rotation to avoid copying large chunks.\n\n\nTiny Code (Easy Versions)\n\n\nPython (Simplified Concept)\nBelow is a simplified stable block-merge inspired by GrailSort’s principles.\ndef rotate(arr, start, mid, end):\n    arr[start:end] = arr[mid:end] + arr[start:mid]\n\ndef merge_in_place(arr, left, mid, right):\n    i, j = left, mid\n    while i &lt; j and j &lt; right:\n        if arr[i] &lt;= arr[j]:\n            i += 1\n        else:\n            val = arr[j]\n            rotate(arr, i, j, j + 1)\n            arr[i] = val\n            i += 1\n            j += 1\n\ndef grailsort(arr):\n    n = len(arr)\n    size = 1\n    while size &lt; n:\n        for start in range(0, n, 2 * size):\n            mid = min(start + size, n)\n            end = min(start + 2 * size, n)\n            if mid &lt; end:\n                merge_in_place(arr, start, mid, end)\n        size *= 2\n    return arr\n\narr = [4,1,3,2,5]\nprint(grailsort(arr))\nOutput:\n$$1, 2, 3, 4, 5]\n\n\nC (Simplified Idea)\n#include &lt;stdio.h&gt;\n\nvoid rotate(int arr[], int start, int mid, int end) {\n    int temp[end - start];\n    int idx = 0;\n    for (int i = mid; i &lt; end; i++) temp[idx++] = arr[i];\n    for (int i = start; i &lt; mid; i++) temp[idx++] = arr[i];\n    for (int i = 0; i &lt; end - start; i++) arr[start + i] = temp[i];\n}\n\nvoid merge_in_place(int arr[], int start, int mid, int end) {\n    int i = start, j = mid;\n    while (i &lt; j && j &lt; end) {\n        if (arr[i] &lt;= arr[j]) i++;\n        else {\n            int val = arr[j];\n            rotate(arr, i, j, j + 1);\n            arr[i] = val;\n            i++; j++;\n        }\n    }\n}\n\nvoid grailsort(int arr[], int n) {\n    for (int size = 1; size &lt; n; size *= 2) {\n        for (int start = 0; start &lt; n; start += 2 * size) {\n            int mid = (start + size &lt; n) ? start + size : n;\n            int end = (start + 2 * size &lt; n) ? start + 2 * size : n;\n            if (mid &lt; end) merge_in_place(arr, start, mid, end);\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {4,1,3,2,5};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    grailsort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nStable and in-place (only small buffer)\nAdaptive, faster on partially sorted data\nPredictable performance\nIdeal for limited memory systems\nUsed in practical sorting libraries and research\n\nIt’s the gold standard for stable, low-space sorts.\n\n\nA Gentle Proof (Why It Works)\nEach merge level processes \\(O(n)\\) elements.\nThere are \\(O(\\log n)\\) merge levels.\nThus:\n\\[\nT(n) = O(n \\log n)\n\\]\nA small buffer (\\(O(\\sqrt{n})\\)) is reused, yielding in-place stability.\n\n\n\nPhase\nOperation\nCost\n\n\n\n\nRun sorting\nInsertion Sort\n\\(O(n)\\)\n\n\nBlock merges\nRotation-based\n\\(O(n \\log n)\\)\n\n\nSpace\nLocal buffer\n\\(O(1)\\) or \\(O(\\sqrt{n})\\)\n\n\n\n\n\nTry It Yourself\n\nSort [4,1,3,2,5] step by step.\nTry [1,2,3,4,5], no merges needed.\nCheck duplicates [2,2,1,1], verify stability.\nExperiment with [10,9,8,7,6,5].\nVisualize rotations during merge.\nChange block size, observe performance.\nImplement √n buffer manually.\nCompare with Merge Sort (space).\nMeasure time vs WikiSort.\nTry large array (10k elements).\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[4,1,3,2,5]\n[1,2,3,4,5]\nBasic test\n\n\n[1,2,3,4]\n[1,2,3,4]\nAlready sorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse\n\n\n[2,2,1,1]\n[1,1,2,2]\nStable\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(1) to O(√n)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nGrailSort is the gentle engineer of sorting, steady, stable, and space-wise. It doesn’t rush; it rearranges with precision, merging order from within.\n\n\n\n140 Adaptive Hybrid Sort\nAdaptive Hybrid Sort is a meta-sorting algorithm that dynamically combines multiple sorting strategies, such as QuickSort, Merge Sort, Insertion Sort, and Heap Sort, depending on the data characteristics and runtime patterns it detects.\nIt adapts in real-time, switching between methods based on factors like array size, degree of pre-sortedness, data distribution, and recursion depth. This makes it a universal, practical sorter optimized for diverse workloads.\n\nWhat Problem Are We Solving??\nNo single sorting algorithm is best for all situations:\n\nQuickSort is fast on random data but unstable and bad in worst case.\nMerge Sort is stable but memory-hungry.\nInsertion Sort is great for small or nearly sorted arrays.\nHeap Sort guarantees O(n log n) but has poor locality.\n\nAdaptive Hybrid Sort solves this by blending algorithms:\n\nStart with QuickSort for speed.\nDetect sorted or small regions → switch to Insertion Sort.\nDetect bad pivot patterns → switch to Heap Sort.\nDetect stability needs or patterns → use Merge Sort.\n\nIt’s a unified, self-tuning sorting system.\n\n\nExample\nSort [2, 3, 5, 4, 6, 7, 8, 1]\n\n\n\n\n\n\n\n\nStep\nDetection\nAction\n\n\n\n\n1\nMostly sorted except last few\nSwitch to Insertion Sort\n\n\n2\nSort locally\n[1,2,3,4,5,6,7,8]\n\n\n3\nDone\nAdaptive path chosen automatically\n\n\n\nIf input were random, it would stay with QuickSort. If adversarial, it would pivot to Heap Sort.\n\n\nHow Does It Work (Plain Language)?\nImagine a skilled chef with many tools, knives, mixers, ovens. When slicing carrots (small data), they use a paring knife (Insertion Sort). When breaking a tough root (unsorted array), they grab a heavy cleaver (QuickSort). When something’s too complex, they use machinery (Merge Sort).\nAdaptive Hybrid Sort works the same way, choosing the right tool at the right time.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart with QuickSort (good average case)\n\n\n2\nIf recursion depth too high → switch to Heap Sort\n\n\n3\nIf subarray small (≤ threshold) → use Insertion Sort\n\n\n4\nIf stable sorting required → use Merge Sort\n\n\n5\nIf data partially sorted → use TimSort-like merge\n\n\n6\nCombine results for final sorted output\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (Simplified Hybrid Sort)\ndef insertion_sort(arr, left, right):\n    for i in range(left + 1, right):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= left and arr[j] &gt; key:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\ndef heapify(arr, n, i):\n    largest = i\n    l = 2*i + 1\n    r = 2*i + 2\n    if l &lt; n and arr[l] &gt; arr[largest]:\n        largest = l\n    if r &lt; n and arr[r] &gt; arr[largest]:\n        largest = r\n    if largest != i:\n        arr[i], arr[largest] = arr[largest], arr[i]\n        heapify(arr, n, largest)\n\ndef heap_sort(arr):\n    n = len(arr)\n    for i in range(n//2 - 1, -1, -1):\n        heapify(arr, n, i)\n    for i in range(n - 1, 0, -1):\n        arr[0], arr[i] = arr[i], arr[0]\n        heapify(arr, i, 0)\n\ndef partition(arr, low, high):\n    pivot = arr[(low + high) // 2]\n    i, j = low, high\n    while i &lt;= j:\n        while arr[i] &lt; pivot: i += 1\n        while arr[j] &gt; pivot: j -= 1\n        if i &lt;= j:\n            arr[i], arr[j] = arr[j], arr[i]\n            i += 1; j -= 1\n    return i\n\ndef hybrid_sort(arr, low=0, high=None, depth_limit=None):\n    if high is None:\n        high = len(arr) - 1\n    if depth_limit is None:\n        import math\n        depth_limit = 2 * math.floor(math.log2(len(arr) + 1))\n    size = high - low + 1\n    if size &lt;= 16:\n        insertion_sort(arr, low, high + 1)\n        return\n    if depth_limit == 0:\n        sub = arr[low:high + 1]\n        heap_sort(sub)\n        arr[low:high + 1] = sub\n        return\n    pivot_index = partition(arr, low, high)\n    if low &lt; pivot_index - 1:\n        hybrid_sort(arr, low, pivot_index - 1, depth_limit - 1)\n    if pivot_index &lt; high:\n        hybrid_sort(arr, pivot_index, high, depth_limit - 1)\n\narr = [2, 3, 5, 4, 6, 7, 8, 1]\nhybrid_sort(arr)\nprint(arr)\nOutput:\n$$1, 2, 3, 4, 5, 6, 7, 8]\n\n\nC (Conceptual Hybrid Sort)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nvoid insertion_sort(int arr[], int left, int right) {\n    for (int i = left + 1; i &lt; right; i++) {\n        int key = arr[i], j = i - 1;\n        while (j &gt;= left && arr[j] &gt; key) {\n            arr[j + 1] = arr[j];\n            j--;\n        }\n        arr[j + 1] = key;\n    }\n}\n\nvoid heapify(int arr[], int n, int i) {\n    int largest = i;\n    int l = 2*i + 1, r = 2*i + 2;\n    if (l &lt; n && arr[l] &gt; arr[largest]) largest = l;\n    if (r &lt; n && arr[r] &gt; arr[largest]) largest = r;\n    if (largest != i) {\n        int tmp = arr[i]; arr[i] = arr[largest]; arr[largest] = tmp;\n        heapify(arr, n, largest);\n    }\n}\n\nvoid heap_sort(int arr[], int n) {\n    for (int i = n/2 - 1; i &gt;= 0; i--) heapify(arr, n, i);\n    for (int i = n - 1; i &gt; 0; i--) {\n        int tmp = arr[0]; arr[0] = arr[i]; arr[i] = tmp;\n        heapify(arr, i, 0);\n    }\n}\n\nint partition(int arr[], int low, int high) {\n    int pivot = arr[(low + high) / 2];\n    int i = low, j = high;\n    while (i &lt;= j) {\n        while (arr[i] &lt; pivot) i++;\n        while (arr[j] &gt; pivot) j--;\n        if (i &lt;= j) {\n            int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp;\n            i++; j--;\n        }\n    }\n    return i;\n}\n\nvoid hybrid_sort(int arr[], int low, int high, int depth_limit) {\n    int size = high - low + 1;\n    if (size &lt;= 16) { insertion_sort(arr, low, high + 1); return; }\n    if (depth_limit == 0) { heap_sort(arr + low, size); return; }\n    int p = partition(arr, low, high);\n    if (low &lt; p - 1) hybrid_sort(arr, low, p - 1, depth_limit - 1);\n    if (p &lt; high) hybrid_sort(arr, p, high, depth_limit - 1);\n}\n\nint main(void) {\n    int arr[] = {2,3,5,4,6,7,8,1};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    int depth_limit = 2 * log2(n);\n    hybrid_sort(arr, 0, n - 1, depth_limit);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nAdaptive to input shape\nHybrid = flexibility + safety\nStable runtime across data types\nReal-world robust for mixed data\nBalances speed, memory, stability, and predictability\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be the runtime.\nEach phase is \\(O(n)\\), and recursion depth is \\(O(\\log n)\\).\nAdaptive switching ensures no pathological behavior.\n\\[\nT(n) = O(n \\log n)\n\\]\nBest case (sorted): Insertion Sort runs in \\(O(n)\\).\nWorst case (adversarial): Heap fallback → \\(O(n \\log n)\\).\n\n\n\nCase\nBehavior\nComplexity\n\n\n\n\nSorted\nInsertion Sort\n\\(O(n)\\)\n\n\nRandom\nQuickSort\n\\(O(n \\log n)\\)\n\n\nAdversarial\nHeap Sort\n\\(O(n \\log n)\\)\n\n\n\n\n\nTry It Yourself\n\nSort [2,3,5,4,6,7,8,1].\nTry [1,2,3,4,5,6] → insertion path.\nReverse [9,8,7,6,5] → heap path.\nMix sorted + random halves.\nMeasure recursion depth.\nIncrease threshold to 32.\nAdd duplicates → observe stability.\nCompare with IntroSort, TimSort.\nBenchmark on large random data.\nVisualize switch decisions.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[2,3,5,4,6,7,8,1]\n[1,2,3,4,5,6,7,8]\nRandom\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nSorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse\n\n\n[10,10,9,9,8]\n[8,9,9,10,10]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n log n)\n\n\nSpace\nO(log n)\n\n\nStable\nNo\n\n\nAdaptive\nYes\n\n\n\nAdaptive Hybrid Sort is the chameleon of sorting, it watches the data, reads the room, and chooses the perfect move. Fast when it can be, safe when it must be.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-15.-special-sorts",
    "href": "books/en-us/list-2.html#section-15.-special-sorts",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 15. Special sorts",
    "text": "Section 15. Special sorts\n\n141 Cycle Sort\nCycle Sort is a comparison-based sorting algorithm designed to minimize the number of writes. It’s ideal when writing to memory or storage is expensive (like EEPROM or flash memory), since each element is written exactly once into its final position.\nIt achieves O(n²) comparisons but performs the minimal possible number of writes, making it unique among sorting algorithms.\n\nWhat Problem Are We Solving??\nMost sorting algorithms (like QuickSort or Merge Sort) swap elements multiple times before they reach their final position. If each write is costly (e.g., embedded systems or flash memory), that’s wasteful.\nCycle Sort asks: “How can we put each element directly where it belongs, in one cycle, with as few writes as possible?”\n\n\nExample\nSort [3, 1, 2]\n\n\n\nStep\nElement\nCorrect Position\nAction\n\n\n\n\n1\n3\nIndex 2\nSwap 3 → position 2\n\n\n2\n2\nIndex 1\nSwap 2 → position 1\n\n\n3\n1\nIndex 0\nDone\n\n\nFinal\n[1, 2, 3]\nSorted\n✅ Minimal writes\n\n\n\nEach element cycles into place once.\n\n\nHow Does It Work (Plain Language)?\nThink of it like putting books on a shelf:\n\nYou pick one book (element),\nFigure out where it belongs,\nPut it there, swapping with whatever’s currently there,\nRepeat until every book is in the right spot.\n\nEach cycle ensures every element reaches its final position exactly once.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nLoop through array positions\n\n\n2\nFor each position, count how many elements are smaller\n\n\n3\nThat count = final position\n\n\n4\nIf not already correct, cycle the element to its right place\n\n\n5\nContinue cycling until original element returns to start\n\n\n6\nMove to next position and repeat\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef cycle_sort(arr):\n    n = len(arr)\n    writes = 0\n    for cycle_start in range(n - 1):\n        item = arr[cycle_start]\n        pos = cycle_start\n        for i in range(cycle_start + 1, n):\n            if arr[i] &lt; item:\n                pos += 1\n        if pos == cycle_start:\n            continue\n        while item == arr[pos]:\n            pos += 1\n        arr[pos], item = item, arr[pos]\n        writes += 1\n        while pos != cycle_start:\n            pos = cycle_start\n            for i in range(cycle_start + 1, n):\n                if arr[i] &lt; item:\n                    pos += 1\n            while item == arr[pos]:\n                pos += 1\n            arr[pos], item = item, arr[pos]\n            writes += 1\n    print(\"Total writes:\", writes)\n    return arr\n\narr = [3, 1, 2, 4]\nprint(cycle_sort(arr))\nOutput:\nTotal writes: 3\n$$1, 2, 3, 4]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid cycle_sort(int arr[], int n) {\n    int writes = 0;\n    for (int cycle_start = 0; cycle_start &lt; n - 1; cycle_start++) {\n        int item = arr[cycle_start];\n        int pos = cycle_start;\n\n        for (int i = cycle_start + 1; i &lt; n; i++)\n            if (arr[i] &lt; item)\n                pos++;\n\n        if (pos == cycle_start) continue;\n\n        while (item == arr[pos]) pos++;\n        int temp = arr[pos]; arr[pos] = item; item = temp;\n        writes++;\n\n        while (pos != cycle_start) {\n            pos = cycle_start;\n            for (int i = cycle_start + 1; i &lt; n; i++)\n                if (arr[i] &lt; item)\n                    pos++;\n            while (item == arr[pos]) pos++;\n            temp = arr[pos]; arr[pos] = item; item = temp;\n            writes++;\n        }\n    }\n    printf(\"Total writes: %d\\n\", writes);\n}\n\nint main(void) {\n    int arr[] = {3, 1, 2, 4};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    cycle_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\nTotal writes: 3  \n1 2 3 4\n\n\n\nWhy It Matters\n\nMinimizes writes (useful for flash memory, EEPROMs)\nIn-place\nDeterministic writes = fewer wear cycles\nEducational example of permutation cycles in sorting\n\nNot fast, but frugal, every move counts.\n\n\nA Gentle Proof (Why It Works)\nEach element moves to its correct position once.\nIf array size is \\(n\\), total writes \\(\\le n\\).\nCounting smaller elements ensures correctness: \\[\n\\text{pos}(x) = |{y \\mid y &lt; x}|\n\\]\nEach cycle resolves one permutation cycle of misplaced elements. Thus, algorithm terminates with all items placed exactly once.\n\n\nTry It Yourself\n\nSort [3, 1, 2] step by step.\nCount how many writes you perform.\nTry [4, 3, 2, 1], maximum cycles.\nTry [1, 2, 3, 4], no writes.\nTest duplicates [3, 1, 2, 3].\nImplement a version counting cycles.\nCompare write count with Selection Sort.\nBenchmark on 1000 random elements.\nMeasure wear-leveling benefit for flash.\nVisualize cycles as arrows in permutation graph.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nWrites\nNotes\n\n\n\n\n[3,1,2,4]\n[1,2,3,4]\n3\n3 cycles\n\n\n[1,2,3]\n[1,2,3]\n0\nAlready sorted\n\n\n[4,3,2,1]\n[1,2,3,4]\n4\nMax writes\n\n\n[3,1,2,3]\n[1,2,3,3]\n3\nHandles duplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n²)\n\n\nWrites\n≤ n\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nCycle Sort is the minimalist’s sorter, every write is intentional, every move meaningful. It may not be fast, but it’s precisely efficient.\n\n\n\n142 Comb Sort\nComb Sort is an improved version of Bubble Sort that eliminates small elements (often called “turtles”) faster by comparing elements far apart first, using a shrinking gap strategy.\nIt starts with a large gap (e.g. array length) and reduces it each pass until it reaches 1, where it behaves like a regular Bubble Sort. The result is fewer comparisons and faster convergence.\n\nWhat Problem Are We Solving??\nBubble Sort is simple but slow, mainly because:\n\nIt swaps only adjacent elements.\nSmall elements crawl slowly to the front.\n\nComb Sort fixes this by using a gap to leap over elements, allowing “turtles” to move quickly forward.\nIt’s like sorting with a comb, wide teeth first (large gap), then finer ones (small gap).\n\n\nExample\nSort [8, 4, 1, 3, 7]\n\n\n\nStep\nGap\nPass Result\nNotes\n\n\n\n\n1\n5 / 1.3 ≈ 3\n[3, 4, 1, 8, 7]\nCompare 8↔︎3\n\n\n2\n3 / 1.3 ≈ 2\n[1, 4, 3, 8, 7]\nCompare 3↔︎1\n\n\n3\n2 / 1.3 ≈ 1\n[1, 3, 4, 7, 8]\nBubble finish\n\n\nDone\n,\n[1, 3, 4, 7, 8]\nSorted\n\n\n\nTurtles (1, 3) jump forward earlier, speeding up convergence.\n\n\nHow Does It Work (Plain Language)?\nThink of it like shrinking a jump rope, at first, you make big jumps to cover ground fast, then smaller ones to fine-tune.\nYou start with a gap, compare and swap elements that far apart, shrink the gap each round, and stop when the gap reaches 1 and no swaps happen.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nInitialize gap = n and shrink = 1.3\n\n\n2\nRepeat until gap == 1 and no swaps\n\n\n3\nDivide gap by shrink factor each pass\n\n\n4\nCompare elements at distance gap\n\n\n5\nSwap if out of order\n\n\n6\nContinue until sorted\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef comb_sort(arr):\n    n = len(arr)\n    gap = n\n    shrink = 1.3\n    swapped = True\n\n    while gap &gt; 1 or swapped:\n        gap = int(gap / shrink)\n        if gap &lt; 1:\n            gap = 1\n        swapped = False\n        for i in range(n - gap):\n            if arr[i] &gt; arr[i + gap]:\n                arr[i], arr[i + gap] = arr[i + gap], arr[i]\n                swapped = True\n    return arr\n\narr = [8, 4, 1, 3, 7]\nprint(comb_sort(arr))\nOutput:\n$$1, 3, 4, 7, 8]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid comb_sort(int arr[], int n) {\n    int gap = n;\n    const float shrink = 1.3;\n    int swapped = 1;\n\n    while (gap &gt; 1 || swapped) {\n        gap = (int)(gap / shrink);\n        if (gap &lt; 1) gap = 1;\n        swapped = 0;\n        for (int i = 0; i + gap &lt; n; i++) {\n            if (arr[i] &gt; arr[i + gap]) {\n                int tmp = arr[i];\n                arr[i] = arr[i + gap];\n                arr[i + gap] = tmp;\n                swapped = 1;\n            }\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {8, 4, 1, 3, 7};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    comb_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n1 3 4 7 8\n\n\n\nWhy It Matters\n\nFaster than Bubble Sort\nSimple implementation\nIn-place and adaptive\nEfficient for small datasets or nearly sorted arrays\n\nA stepping stone toward more efficient algorithms like Shell Sort.\n\n\nA Gentle Proof (Why It Works)\nThe shrink factor ensures gap reduction converges to 1 in \\(O(\\log n)\\) steps.\nEach pass fixes distant inversions early, reducing total swaps.\nTotal cost is dominated by local passes when gap = 1 (Bubble Sort).\nHence, average complexity ≈ \\(O(n \\log n)\\) for random data, \\(O(n^2)\\) worst case.\n\n\n\nPhase\nDescription\nCost\n\n\n\n\nLarge gap passes\nMove turtles forward\n\\(O(n \\log n)\\)\n\n\nSmall gap passes\nFinal fine-tuning\n\\(O(n^2)\\) worst\n\n\n\n\n\nTry It Yourself\n\nSort [8, 4, 1, 3, 7] step by step.\nTry [1, 2, 3, 4, 5], minimal passes.\nTry [5, 4, 3, 2, 1], observe gap shrinking.\nChange shrink factor (1.5, 1.2).\nMeasure swaps per iteration.\nCompare with Bubble Sort.\nVisualize movement of smallest element.\nBenchmark large random array.\nTrack gap evolution over time.\nImplement early-stop optimization.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[8,4,1,3,7]\n[1,3,4,7,8]\nBasic test\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nAlready sorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse order\n\n\n[4,1,3,2]\n[1,2,3,4]\nShort array\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n log n)\n\n\nTime (Average)\nO(n log n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nYes\n\n\n\nComb Sort sweeps through data like a comb through tangled hair, wide strokes first, fine ones later, until everything’s smooth and ordered.\n\n\n\n143 Gnome Sort\nGnome Sort is a simple comparison-based sorting algorithm that works like a garden gnome arranging flower pots, it looks at two adjacent elements and swaps them if they’re out of order, then steps backward to check the previous pair again.\nIt’s conceptually similar to Insertion Sort, but implemented with a single loop and no nested structure, making it elegant and intuitive for learners.\n\nWhat Problem Are We Solving??\nInsertion Sort requires nested loops or recursion, which can be tricky to visualize. Gnome Sort offers the same logic using a simple forward–backward walk:\n\nMove forward if elements are ordered.\nMove back and swap if they’re not.\n\nThis creates a human-like sorting routine, step forward, fix, step back, repeat.\n\n\nExample\nSort [5, 3, 4, 2]\n\n\n\nStep\nIndex\nAction\nResult\n\n\n\n\n1\n1\n5 &gt; 3 → swap\n[3, 5, 4, 2]\n\n\n2\n0\nAt start → move forward\n[3, 5, 4, 2]\n\n\n3\n2\n5 &gt; 4 → swap\n[3, 4, 5, 2]\n\n\n4\n1\n3 &lt; 4 → forward\n[3, 4, 5, 2]\n\n\n5\n3\n5 &gt; 2 → swap\n[3, 4, 2, 5]\n\n\n6\n2\n4 &gt; 2 → swap\n[3, 2, 4, 5]\n\n\n7\n1\n3 &gt; 2 → swap\n[2, 3, 4, 5]\n\n\n8\n0\nDone\n[2, 3, 4, 5]\n\n\n\nSorted with local fixes, no explicit nested loops.\n\n\nHow Does It Work (Plain Language)?\nImagine a gnome walking down a row of flower pots:\n\nIf the two pots are in order, step forward.\nIf not, swap them and step back.\nIf you reach the start, just move forward again.\n\nKeep walking until you reach the end, the garden (array) is sorted.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart at index 1\n\n\n2\nCompare arr[i] with arr[i-1]\n\n\n3\nIf arr[i] &gt;= arr[i-1], move forward (i++)\n\n\n4\nElse, swap arr[i] and arr[i-1], move back (i–)\n\n\n5\nIf i == 0, move to i = 1\n\n\n6\nRepeat until i reaches n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef gnome_sort(arr):\n    i = 1\n    n = len(arr)\n    while i &lt; n:\n        if i == 0 or arr[i] &gt;= arr[i - 1]:\n            i += 1\n        else:\n            arr[i], arr[i - 1] = arr[i - 1], arr[i]\n            i -= 1\n    return arr\n\narr = [5, 3, 4, 2]\nprint(gnome_sort(arr))\nOutput:\n$$2, 3, 4, 5]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid gnome_sort(int arr[], int n) {\n    int i = 1;\n    while (i &lt; n) {\n        if (i == 0 || arr[i] &gt;= arr[i - 1]) {\n            i++;\n        } else {\n            int tmp = arr[i];\n            arr[i] = arr[i - 1];\n            arr[i - 1] = tmp;\n            i--;\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {5, 3, 4, 2};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    gnome_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n2 3 4 5\n\n\n\nWhy It Matters\n\nSimple mental model, easy to understand.\nNo nested loops, clean control flow.\nIn-place, no extra space.\nDemonstrates local correction in sorting.\n\nIt’s slower than advanced algorithms but ideal for educational purposes.\n\n\nA Gentle Proof (Why It Works)\nEach swap moves an element closer to its correct position. Whenever a swap happens, the gnome steps back to ensure local order.\nSince every inversion is eventually corrected, the algorithm terminates with a sorted array.\nNumber of swaps proportional to number of inversions → \\(O(n^2)\\).\n\n\n\nCase\nBehavior\nComplexity\n\n\n\n\nSorted\nLinear scan\nO(n)\n\n\nRandom\nFrequent swaps\nO(n²)\n\n\nReverse\nMax swaps\nO(n²)\n\n\n\n\n\nTry It Yourself\n\nSort [5,3,4,2] manually step by step.\nTry [1,2,3,4], minimal steps.\nTry [4,3,2,1], worst case.\nCount number of swaps.\nCompare with Insertion Sort.\nTrack index changes after each swap.\nImplement visual animation (pointer walk).\nTry duplicates [2,1,2,1].\nMeasure time for n = 1000.\nAdd early-exit optimization.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[5,3,4,2]\n[2,3,4,5]\nBasic\n\n\n[1,2,3,4]\n[1,2,3,4]\nAlready sorted\n\n\n[4,3,2,1]\n[1,2,3,4]\nReverse\n\n\n[2,1,2,1]\n[1,1,2,2]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n²)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nGnome Sort is a friendly, step-by-step sorter, it doesn’t rush, just tidies things one pot at a time until the whole row is in order.\n\n\n\n144 Cocktail Sort\nCocktail Sort (also known as Bidirectional Bubble Sort or Shaker Sort) is a simple variation of Bubble Sort that sorts the list in both directions alternately, forward then backward, during each pass.\nThis bidirectional movement helps small elements (“turtles”) bubble up faster from the end, fixing one of Bubble Sort’s main weaknesses.\n\nWhat Problem Are We Solving??\nBubble Sort only moves elements in one direction, large ones float to the end, but small ones crawl slowly to the start.\nCocktail Sort solves this by shaking the list:\n\nForward pass: moves large items right\nBackward pass: moves small items left\n\nThis makes it more efficient on nearly sorted arrays or when both ends need cleaning.\n\n\nExample\nSort [4, 3, 1, 2]\n\n\n\nStep\nDirection\nAction\nResult\n\n\n\n\n1\nForward\nCompare & swap 4↔︎3, 3↔︎1, 4↔︎2\n[3,1,2,4]\n\n\n2\nBackward\nCompare & swap 2↔︎1, 3↔︎1\n[1,3,2,4]\n\n\n3\nForward\nCompare & swap 3↔︎2\n[1,2,3,4]\n\n\nDone\n,\nSorted\n✅\n\n\n\nFewer passes than Bubble Sort.\n\n\nHow Does It Work (Plain Language)?\nThink of a bartender shaking a cocktail shaker back and forth, each shake moves ingredients (elements) closer to the right place from both sides.\nYou traverse the array:\n\nLeft to right: push largest elements to the end\nRight to left: push smallest elements to the start\n\nStop when no swaps occur, the array is sorted.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nInitialize swapped = True\n\n\n2\nWhile swapped:\n\n\n  a. Set swapped = False\n\n\n\n  b. Forward pass (i = start → end): swap if arr[i] &gt; arr[i+1]\n\n\n\n  c. If swapped == False: break (sorted)\n\n\n\n  d. Backward pass (i = end-1 → start): swap if arr[i] &gt; arr[i+1]\n\n\n\n3\nRepeat until sorted\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef cocktail_sort(arr):\n    n = len(arr)\n    swapped = True\n    start = 0\n    end = n - 1\n\n    while swapped:\n        swapped = False\n        # Forward pass\n        for i in range(start, end):\n            if arr[i] &gt; arr[i + 1]:\n                arr[i], arr[i + 1] = arr[i + 1], arr[i]\n                swapped = True\n        if not swapped:\n            break\n        swapped = False\n        end -= 1\n        # Backward pass\n        for i in range(end - 1, start - 1, -1):\n            if arr[i] &gt; arr[i + 1]:\n                arr[i], arr[i + 1] = arr[i + 1], arr[i]\n                swapped = True\n        start += 1\n    return arr\n\narr = [4, 3, 1, 2]\nprint(cocktail_sort(arr))\nOutput:\n$$1, 2, 3, 4]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid cocktail_sort(int arr[], int n) {\n    int start = 0, end = n - 1, swapped = 1;\n    while (swapped) {\n        swapped = 0;\n        // Forward pass\n        for (int i = start; i &lt; end; i++) {\n            if (arr[i] &gt; arr[i + 1]) {\n                int tmp = arr[i];\n                arr[i] = arr[i + 1];\n                arr[i + 1] = tmp;\n                swapped = 1;\n            }\n        }\n        if (!swapped) break;\n        swapped = 0;\n        end--;\n        // Backward pass\n        for (int i = end - 1; i &gt;= start; i--) {\n            if (arr[i] &gt; arr[i + 1]) {\n                int tmp = arr[i];\n                arr[i] = arr[i + 1];\n                arr[i + 1] = tmp;\n                swapped = 1;\n            }\n        }\n        start++;\n    }\n}\n\nint main(void) {\n    int arr[] = {4, 3, 1, 2};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    cocktail_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n1 2 3 4\n\n\n\nWhy It Matters\n\nBidirectional improvement on Bubble Sort\nIn-place and simple\nPerforms well on nearly sorted data\nAdaptive, stops early when sorted\n\nGreat educational bridge to understanding bidirectional scans and adaptive sorting.\n\n\nA Gentle Proof (Why It Works)\nEach forward pass pushes the largest element to the right. Each backward pass pushes the smallest element to the left.\nAfter each full cycle, the sorted region expands from both ends. The algorithm stops when no swaps occur (sorted).\nTotal operations depend on number of inversions: \\[\nO(n^2) \\text{ worst}, \\quad O(n) \\text{ best (sorted input)}\n\\]\n\n\n\nCase\nBehavior\nComplexity\n\n\n\n\nSorted\nOne bidirectional scan\nO(n)\n\n\nRandom\nMany swaps\nO(n²)\n\n\nReverse\nMax passes\nO(n²)\n\n\n\n\n\nTry It Yourself\n\nSort [4,3,1,2] manually step by step.\nTry [1,2,3,4], should stop early.\nTry [5,4,3,2,1], observe shaking effect.\nCount swaps each pass.\nCompare passes with Bubble Sort.\nVisualize forward/backward movement.\nAdd “swap counter” variable.\nTest duplicates [3,1,3,2,1].\nMeasure performance on nearly sorted data.\nModify shrink window size.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[4,3,1,2]\n[1,2,3,4]\nBasic\n\n\n[1,2,3,4]\n[1,2,3,4]\nAlready sorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse\n\n\n[3,1,3,2,1]\n[1,1,2,3,3]\nDuplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(n)\n\n\nTime (Average)\nO(n²)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\n\nCocktail Sort is the sorter that doesn’t just bubble, it shakes the data into order, making sure both ends get attention every round.\n\n\n\n145 Pancake Sort\nPancake Sort is a whimsical and educational sorting algorithm inspired by flipping pancakes on a plate, at each step, you bring the largest unsorted pancake to the top, then flip the stack to move it to its correct position.\nIt’s not practical for large datasets, but it’s a brilliant way to illustrate prefix reversals, maximum selection, and in-place transformations.\n\nWhat Problem Are We Solving??\nStandard selection sort swaps elements pairwise. Pancake Sort instead uses prefix reversals, flipping a whole section of the array, to move the largest item into place.\nIt’s a thought experiment in restricted operations: What if the only thing you could do is flip?\n\n\nExample\nSort [3, 6, 1, 5, 2, 4]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nFind max (6) → index 1, flip first 2\n[6, 3, 1, 5, 2, 4]\n\n\n2\nFlip first 6\n[4, 2, 5, 1, 3, 6]\n\n\n3\nFind max (5) → index 2, flip first 3\n[5, 2, 4, 1, 3, 6]\n\n\n4\nFlip first 5\n[3, 1, 4, 2, 5, 6]\n\n\n5\nRepeat for smaller prefixes\n[1, 2, 3, 4, 5, 6]\n\n\n\nSorted using flips only!\n\n\nHow Does It Work (Plain Language)?\nImagine flipping a stack of pancakes:\n\nFind the biggest one not yet placed.\nFlip the stack to bring it to the top.\nFlip again to move it into its final position.\n\nRepeat, each time excluding the sorted top portion.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFind index of largest unsorted element\n\n\n2\nFlip subarray from start to that index\n\n\n3\nFlip entire unsorted subarray to move it to end\n\n\n4\nReduce unsorted portion by one\n\n\n5\nRepeat until sorted\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef flip(arr, k):\n    arr[:k+1] = reversed(arr[:k+1])\n\ndef pancake_sort(arr):\n    n = len(arr)\n    for curr_size in range(n, 1, -1):\n        max_idx = arr.index(max(arr[:curr_size]))\n        if max_idx != curr_size - 1:\n            flip(arr, max_idx)\n            flip(arr, curr_size - 1)\n    return arr\n\narr = [3, 6, 1, 5, 2, 4]\nprint(pancake_sort(arr))\nOutput:\n$$1, 2, 3, 4, 5, 6]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid flip(int arr[], int k) {\n    int start = 0;\n    while (start &lt; k) {\n        int temp = arr[start];\n        arr[start] = arr[k];\n        arr[k] = temp;\n        start++;\n        k--;\n    }\n}\n\nint find_max(int arr[], int n) {\n    int max_idx = 0;\n    for (int i = 1; i &lt; n; i++)\n        if (arr[i] &gt; arr[max_idx])\n            max_idx = i;\n    return max_idx;\n}\n\nvoid pancake_sort(int arr[], int n) {\n    for (int size = n; size &gt; 1; size--) {\n        int max_idx = find_max(arr, size);\n        if (max_idx != size - 1) {\n            flip(arr, max_idx);\n            flip(arr, size - 1);\n        }\n    }\n}\n\nint main(void) {\n    int arr[] = {3, 6, 1, 5, 2, 4};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    pancake_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n1 2 3 4 5 6\n\n\n\nWhy It Matters\n\nFun demonstration of prefix operations\nIn-place and simple\nShows how restricted operations can still sort\nTheoretical interest, base for pancake networks\nUsed in bioinformatics (genome rearrangements)\n\n\n\nA Gentle Proof (Why It Works)\nEach iteration places the largest remaining element at its final index. Two flips per iteration (worst case). At most ( 2(n - 1) ) flips total.\nCorrectness follows from:\n\nFlipping is a reversal, which preserves order except within flipped segment.\nEach largest element is locked at the end after placement.\n\n\\[\nT(n) = O(n^2)\n\\] because each max() and flip() operation is O(n).\n\n\nTry It Yourself\n\nSort [3,6,1,5,2,4] manually.\nTrace each flip visually.\nTry [1,2,3,4], no flips needed.\nReverse [4,3,2,1], observe maximum flips.\nCount flips per iteration.\nImplement flip visualization.\nReplace max() with manual search.\nPrint intermediate arrays.\nAnalyze flip count for random input.\nChallenge: implement recursive version.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nFlips\nNotes\n\n\n\n\n[3,6,1,5,2,4]\n[1,2,3,4,5,6]\n8\nClassic\n\n\n[1,2,3,4]\n[1,2,3,4]\n0\nAlready sorted\n\n\n[4,3,2,1]\n[1,2,3,4]\n6\nWorst case\n\n\n[2,1,3]\n[1,2,3]\n3\nSmall array\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n²)\n\n\nTime (Average)\nO(n²)\n\n\nTime (Best)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nPancake Sort flips its way to victory, a charming example of ingenuity under constraint. You don’t need fancy tools, just a good spatula and some patience.\n\n\n\n146 Bitonic Sort\nBitonic Sort is a parallel sorting algorithm designed for sorting networks. It works by constructing and merging bitonic sequences, sequences that first increase, then decrease (or vice versa).\nIt’s especially powerful on hardware, GPUs, and parallel processors, where multiple comparisons can happen at once.\n\nWhat Problem Are We Solving??\nMost standard algorithms (QuickSort, MergeSort) are data-dependent, their flow changes depending on comparisons. That’s a problem for hardware or parallel systems.\nBitonic Sort fixes this by having a fixed comparison pattern, perfect for parallel execution.\nIt answers the question:\n\n“How do we sort in parallel using predictable, fixed circuits?”\n\n\n\nExample\nSort [3, 7, 4, 8, 6, 2, 1, 5]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nBuild bitonic sequence ↑↓\n[3,7,4,8, 6,2,1,5]\n\n\n2\nCompare & swap pairs\n[3,2,1,5, 6,7,4,8]\n\n\n3\nMerge halves recursively\n[1,2,3,4, 5,6,7,8]\n\n\n\nEach phase doubles sorted subsequences until the full array is sorted.\n\n\nHow Does It Work (Plain Language)?\nImagine you’re sorting cards but you can only compare pairs at fixed positions, no adaptive decisions.\nYou first build a bitonic sequence (up then down), then merge it into a sorted list using bitonic merge steps, repeatedly compare-and-swap mirrored elements.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nSplit array into halves\n\n\n2\nSort first half ascending, second half descending\n\n\n3\nCombine them into a bitonic sequence\n\n\n4\nRun bitonic merge: recursively compare-swap pairs\n\n\n5\nRepeat until sequence size = 1\n\n\n\nWorks best when n = 2^k (power of two).\n\n\nTiny Code (Easy Versions)\n\nPython\ndef bitonic_merge(arr, low, cnt, direction):\n    if cnt &gt; 1:\n        k = cnt // 2\n        for i in range(low, low + k):\n            if (direction and arr[i] &gt; arr[i + k]) or (not direction and arr[i] &lt; arr[i + k]):\n                arr[i], arr[i + k] = arr[i + k], arr[i]\n        bitonic_merge(arr, low, k, direction)\n        bitonic_merge(arr, low + k, k, direction)\n\ndef bitonic_sort(arr, low=0, cnt=None, direction=True):\n    if cnt is None:\n        cnt = len(arr)\n    if cnt &gt; 1:\n        k = cnt // 2\n        bitonic_sort(arr, low, k, True)\n        bitonic_sort(arr, low + k, k, False)\n        bitonic_merge(arr, low, cnt, direction)\n\narr = [3, 7, 4, 8, 6, 2, 1, 5]\nbitonic_sort(arr)\nprint(arr)\nOutput:\n$$1, 2, 3, 4, 5, 6, 7, 8]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int t = *a; *a = *b; *b = t;\n}\n\nvoid bitonic_merge(int arr[], int low, int cnt, int dir) {\n    if (cnt &gt; 1) {\n        int k = cnt / 2;\n        for (int i = low; i &lt; low + k; i++) {\n            if ((dir && arr[i] &gt; arr[i + k]) || (!dir && arr[i] &lt; arr[i + k]))\n                swap(&arr[i], &arr[i + k]);\n        }\n        bitonic_merge(arr, low, k, dir);\n        bitonic_merge(arr, low + k, k, dir);\n    }\n}\n\nvoid bitonic_sort(int arr[], int low, int cnt, int dir) {\n    if (cnt &gt; 1) {\n        int k = cnt / 2;\n        bitonic_sort(arr, low, k, 1);\n        bitonic_sort(arr, low + k, k, 0);\n        bitonic_merge(arr, low, cnt, dir);\n    }\n}\n\nint main(void) {\n    int arr[] = {3, 7, 4, 8, 6, 2, 1, 5};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    bitonic_sort(arr, 0, n, 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n1 2 3 4 5 6 7 8\n\n\n\nWhy It Matters\n\nParallel-friendly (sorting networks)\nDeterministic structure (no branches)\nPerfect for hardware, GPUs, SIMD\nGood educational model for divide and conquer + merging\n\nIt’s not about runtime on CPUs, it’s about parallel depth.\n\n\nA Gentle Proof (Why It Works)\nBitonic sequence:\nA sequence that increases then decreases is bitonic.\nMerging rule:\nCompare each element with its mirror; recursively merge halves.\nAt each merge stage, the array becomes more sorted.\nRecursion depth = \\(\\log n\\), each level does \\(O(n)\\) work → \\(O(n \\log^2 n)\\).\n\n\n\nStep\nWork\nLevels\nTotal\n\n\n\n\nMerge\n\\(O(n)\\)\n\\(\\log n\\)\n\\(O(n \\log^2 n)\\)\n\n\n\n\n\nTry It Yourself\n\nSort [3,7,4,8,6,2,1,5] manually.\nIdentify bitonic sequences at each stage.\nTry with 4 elements [4,1,3,2].\nChange direction flags (ascending/descending).\nDraw comparison network graph.\nImplement iterative version.\nRun on power-of-two sizes.\nMeasure parallel steps vs QuickSort.\nExperiment with GPU (Numba/CUDA).\nVisualize recursive structure.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,7,4,8,6,2,1,5]\n[1,2,3,4,5,6,7,8]\nStandard\n\n\n[4,1,3,2]\n[1,2,3,4]\nSmall case\n\n\n[5,4,3,2,1,0,9,8]\n[0,1,2,3,4,5,8,9]\nReverse\n\n\n[8,4,2,1,3,6,5,7]\n[1,2,3,4,5,6,7,8]\nRandom\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n log² n)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\nParallel Depth\nO(log² n)\n\n\n\nBitonic Sort shines where parallelism rules, in GPUs, circuits, and sorting networks. Every comparison is planned, every move synchronized, a symphony of order in fixed rhythm.\n\n\n\n147 Odd-Even Merge Sort\nOdd-Even Merge Sort is a parallel sorting algorithm and a sorting network that merges two sorted sequences using a fixed pattern of comparisons between odd and even indexed elements.\nIt was introduced by Ken Batcher, and like Bitonic Sort, it’s designed for parallel hardware or SIMD processors, where predictable comparison patterns matter more than data-dependent branching.\n\nWhat Problem Are We Solving??\nTraditional merge algorithms rely on conditional branching, they decide at runtime which element to pick next. This is problematic in parallel or hardware implementations, where you need fixed, predictable sequences of comparisons.\nOdd-Even Merge Sort solves this by using a static comparison network that merges sorted halves without branching.\nIt’s perfect when:\n\nYou need deterministic behavior\nYou’re building parallel circuits or GPU kernels\n\n\n\nExample\nMerge two sorted halves: [1, 4, 7, 8] and [2, 3, 5, 6]\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nMerge odds [1,7] with [2,5]\n[1,2,5,7]\n\n\n2\nMerge evens [4,8] with [3,6]\n[3,4,6,8]\n\n\n3\nCombine and compare adjacent\n[1,2,3,4,5,6,7,8]\n\n\n\nFixed pattern, no branching, merges completed in parallel.\n\n\nHow Does It Work (Plain Language)?\nImagine two zipper chains, one odd, one even. You weave them together in a fixed, interlocking pattern, comparing and swapping along the way. There’s no guessing, every element knows which neighbor to check.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nSplit array into left and right halves\n\n\n2\nRecursively sort each half\n\n\n3\nUse odd-even merge to combine halves\n\n\n4\nOdd-even merge:\n\n\n  a. Recursively merge odd and even indexed elements\n\n\n\n  b. Compare and swap adjacent pairs\n\n\n\n5\nContinue until array sorted\n\n\n\nWorks best when ( n = 2^k ).\n\n\nTiny Code (Easy Versions)\n\nPython\ndef odd_even_merge(arr, lo, n, direction):\n    if n &gt; 1:\n        m = n // 2\n        odd_even_merge(arr, lo, m, direction)\n        odd_even_merge(arr, lo + m, m, direction)\n        for i in range(lo + m, lo + n - m):\n            if (arr[i] &gt; arr[i + m]) == direction:\n                arr[i], arr[i + m] = arr[i + m], arr[i]\n\ndef odd_even_merge_sort(arr, lo=0, n=None, direction=True):\n    if n is None:\n        n = len(arr)\n    if n &gt; 1:\n        m = n // 2\n        odd_even_merge_sort(arr, lo, m, direction)\n        odd_even_merge_sort(arr, lo + m, m, direction)\n        odd_even_merge(arr, lo, n, direction)\n\narr = [8, 3, 2, 7, 4, 6, 5, 1]\nodd_even_merge_sort(arr)\nprint(arr)\nOutput:\n$$1, 2, 3, 4, 5, 6, 7, 8]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int t = *a; *a = *b; *b = t;\n}\n\nvoid odd_even_merge(int arr[], int lo, int n, int dir) {\n    if (n &gt; 1) {\n        int m = n / 2;\n        odd_even_merge(arr, lo, m, dir);\n        odd_even_merge(arr, lo + m, m, dir);\n        for (int i = lo + m; i &lt; lo + n - m; i++) {\n            if ((arr[i] &gt; arr[i + m]) == dir)\n                swap(&arr[i], &arr[i + m]);\n        }\n    }\n}\n\nvoid odd_even_merge_sort(int arr[], int lo, int n, int dir) {\n    if (n &gt; 1) {\n        int m = n / 2;\n        odd_even_merge_sort(arr, lo, m, dir);\n        odd_even_merge_sort(arr, lo + m, m, dir);\n        odd_even_merge(arr, lo, n, dir);\n    }\n}\n\nint main(void) {\n    int arr[] = {8, 3, 2, 7, 4, 6, 5, 1};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    odd_even_merge_sort(arr, 0, n, 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n1 2 3 4 5 6 7 8\n\n\n\nWhy It Matters\n\nFixed sequence, perfect for parallelism\nNo data-dependent branching\nUsed in hardware sorting networks\nTheoretical foundation for parallel sorting\n\nWhen you need determinism and concurrency, this algorithm shines.\n\n\nA Gentle Proof (Why It Works)\nEach odd-even merge merges two sorted sequences using fixed compare-swap operations. At each stage:\n\nOdd indices are merged separately\nEven indices merged separately\nAdjacent elements compared to restore global order\n\nEach level performs O(n) work, depth = O(log² n) → total complexity: \\[\nT(n) = O(n \\log^2 n)\n\\]\n\n\nTry It Yourself\n\nSort [8,3,2,7,4,6,5,1] step by step.\nTrace odd-index and even-index merges.\nDraw merge network diagram.\nTry smaller [4,3,2,1] for clarity.\nRun on power-of-two lengths.\nMeasure comparisons.\nCompare with Bitonic Sort.\nImplement iterative version.\nVisualize parallel depth.\nExperiment with ascending/descending flags.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[8,3,2,7,4,6,5,1]\n[1,2,3,4,5,6,7,8]\nClassic\n\n\n[4,3,2,1]\n[1,2,3,4]\nSmall\n\n\n[9,7,5,3,1,2,4,6]\n[1,2,3,4,5,6,7,9]\nMixed\n\n\n[5,4,3,2]\n[2,3,4,5]\nReverse half\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n log² n)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\nParallel Depth\nO(log² n)\n\n\n\nOdd-Even Merge Sort weaves order from two halves like clockwork, steady, parallel, and predictable. Every comparison is planned, every merge synchronized, it’s sorting as architecture.\n\n\n\n148 Sleep Sort\nSleep Sort is one of the most playful and unconventional algorithms ever invented, it sorts numbers by leveraging time delays. Each element is “slept” for a duration proportional to its value, and when the sleep ends, it prints the number.\nIn effect, time itself becomes the sorting mechanism.\n\nWhat Problem Are We Solving??\nWhile not practical, Sleep Sort offers a fun demonstration of parallelism and asynchronous timing, showing that even sorting can be expressed through temporal order rather than comparisons.\nIt’s often used as a thought experiment to teach concurrency, timing, and creative thinking about problem-solving.\n\n\nExample\nSort [3, 1, 4, 2]\n\n\n\nStep\nValue\nSleep (seconds)\nPrint order\n\n\n\n\n1\n1\n1s\n1\n\n\n2\n2\n2s\n2\n\n\n3\n3\n3s\n3\n\n\n4\n4\n4s\n4\n\n\n\nOutput (over time): 1 2 3 4\nSorted by time of completion!\n\n\nHow Does It Work (Plain Language)?\nEach number is given a timer equal to its value. All timers start simultaneously, and when a timer finishes, that number is output. Small numbers “wake up” first, so they’re printed earlier, creating a sorted sequence.\nIt’s like a race where each runner’s speed is inversely proportional to its size, smaller ones finish first.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nFor each element x, create a thread or coroutine\n\n\n2\nEach thread sleeps for x units of time\n\n\n3\nWhen sleep completes, print x\n\n\n4\nNumbers appear in sorted order\n\n\n5\nOptionally collect outputs into a list\n\n\n\n\n\nTiny Code (Easy Versions)\n\n\nPython (Using Threads)\nimport threading\nimport time\n\ndef sleeper(x):\n    time.sleep(x * 0.1)  # scale factor for speed\n    print(x, end=' ')\n\ndef sleep_sort(arr):\n    threads = []\n    for x in arr:\n        t = threading.Thread(target=sleeper, args=(x,))\n        t.start()\n        threads.append(t)\n    for t in threads:\n        t.join()\n\narr = [3, 1, 4, 2]\nsleep_sort(arr)\nOutput (timed):\n1 2 3 4\n\n\nC (Using Threads and Sleep)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n#include &lt;unistd.h&gt;\n\nvoid* sleeper(void* arg) {\n    int x = *(int*)arg;\n    usleep(x * 100000); // scaled down\n    printf(\"%d \", x);\n    return NULL;\n}\n\nvoid sleep_sort(int arr[], int n) {\n    pthread_t threads[n];\n    for (int i = 0; i &lt; n; i++)\n        pthread_create(&threads[i], NULL, sleeper, &arr[i]);\n    for (int i = 0; i &lt; n; i++)\n        pthread_join(threads[i], NULL);\n}\n\nint main(void) {\n    int arr[] = {3, 1, 4, 2};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    sleep_sort(arr, n);\n    printf(\"\\n\");\n}\nOutput (timed):\n1 2 3 4\n\n\nWhy It Matters\n\nCreative demonstration of parallelism\nFun teaching tool for concurrency\nVisually intuitive, sorting emerges naturally\nGreat reminder: algorithms ≠ just code, they’re processes\n\nIt’s impractical, but delightfully educational.\n\n\nA Gentle Proof (Why It Works)\nIf all threads start simultaneously and sleep proportionally to their values, then:\n\nSmaller values finish earlier\nNo collisions (if distinct integers)\nOutput sequence = sorted list\n\nFor duplicates, slight offsets may be added to maintain stability.\nLimitations:\n\nRequires positive integers\nDepends on accurate timers\nSensitive to scheduler latency\n\n\n\nTry It Yourself\n\nSort [3,1,4,2], observe timing.\nTry [10,5,1,2], slower but clearer pattern.\nAdd duplicates [2,2,1], test ordering.\nScale sleep time down (x * 0.05).\nRun on multi-core CPU, observe concurrency.\nReplace sleep with await asyncio.sleep(x) for async version.\nCollect results in a list instead of print.\nUse multiprocessing instead of threads.\nVisualize time vs value graph.\nTry fractional delays for floats.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,1,4,2]\n[1,2,3,4]\nClassic example\n\n\n[1,2,3,4]\n[1,2,3,4]\nAlready sorted\n\n\n[4,3,2,1]\n[1,2,3,4]\nReversed\n\n\n[2,2,1]\n[1,2,2]\nHandles duplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Theoretical)\nO(n) real-time (wall-clock)\n\n\nTime (CPU Work)\nO(n) setup\n\n\nSpace\nO(n) threads\n\n\nStable\nYes (with offset)\n\n\nAdaptive\nNo\n\n\n\nSleep Sort is sorting reimagined, not by computation, but by patience. Every number simply waits its turn, no comparisons, no loops, just time.\n\n\n\n149 Bead Sort\nBead Sort, also known as Gravity Sort, is a natural sorting algorithm inspired by how beads slide under gravity on parallel rods. Imagine an abacus turned on its side: heavier piles settle first, automatically sorting themselves.\nIt’s visual, parallel, and analog in spirit, more of a conceptual model than a practical tool, but brilliant for intuition.\n\nWhat Problem Are We Solving??\nSorting algorithms usually rely on comparisons. Bead Sort instead uses physical simulation, items fall until they settle into order.\nThis approach helps visualize distribution sorting and natural computation, where sorting happens through physical laws rather than arithmetic operations.\n\n\nExample\nSort [5, 3, 1, 7, 4]:\n\nRepresent each number as a row of beads.\nDrop beads under gravity.\nCount beads per column from bottom up.\n\n\n\n\n\n\n\n\n\n\nStep\nRepresentation\nAfter Gravity\nOutput\n\n\n\n\nInitial\n5●●●●●3●●●1●7●●●●●●●4●●●●\n,\n,\n\n\nGravity\nColumns fill from bottom\nRows shorten\n,\n\n\nResult\n1●3●●●4●●●●5●●●●●7●●●●●●●\n[1, 3, 4, 5, 7]\n\n\n\n\nThe smallest number rises to top, largest sinks to bottom, sorted.\n\n\nHow Does It Work (Plain Language)?\nEach number is a pile of beads. Beads “fall” downward until no empty space below. Since heavier rows push beads downward faster, larger numbers accumulate at the bottom. When gravity stops, reading row lengths from top to bottom yields sorted order.\nIt’s sorting by simulated gravity, no comparisons at all.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nRepresent each integer by beads on rods (1 bead per unit)\n\n\n2\nLet beads fall to the lowest empty position in each column\n\n\n3\nAfter settling, count beads per row (top-down)\n\n\n4\nThese counts form the sorted list\n\n\n\nWorks only for non-negative integers.\n\n\nTiny Code (Easy Versions)\n\nPython\ndef bead_sort(arr):\n    if any(x &lt; 0 for x in arr):\n        raise ValueError(\"Only non-negative integers allowed\")\n    max_val = max(arr)\n    grid = [[1 if i &lt; x else 0 for i in range(max_val)] for x in arr]\n    for col in range(max_val):\n        beads = sum(row[col] for row in grid)\n        for row in range(len(arr)):\n            grid[row][col] = 1 if row &gt;= len(arr) - beads else 0\n    return [sum(row) for row in grid]\n\narr = [5, 3, 1, 7, 4]\nprint(bead_sort(arr))\nOutput:\n$$1, 3, 4, 5, 7]\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nvoid bead_sort(int *a, int n) {\n    int max = 0;\n    for (int i = 0; i &lt; n; i++) if (a[i] &gt; max) max = a[i];\n    unsigned char beads[n][max];\n    memset(beads, 0, n * max);\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; a[i]; j++)\n            beads[i][j] = 1;\n\n    for (int j = 0; j &lt; max; j++) {\n        int sum = 0;\n        for (int i = 0; i &lt; n; i++) sum += beads[i][j];\n        for (int i = 0; i &lt; n; i++)\n            beads[i][j] = (i &gt;= n - sum) ? 1 : 0;\n    }\n\n    for (int i = 0; i &lt; n; i++) {\n        a[i] = 0;\n        for (int j = 0; j &lt; max; j++)\n            a[i] += beads[i][j];\n    }\n}\n\nint main(void) {\n    int arr[] = {5, 3, 1, 7, 4};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    bead_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput:\n1 3 4 5 7\n\n\n\nWhy It Matters\n\nDemonstrates non-comparison-based sorting\nShows physical analogies for computation\nIdeal for visual and educational purposes\nParallelizable (each column independent)\n\nThough impractical, it inspires biological and physics-inspired algorithm design.\n\n\nA Gentle Proof (Why It Works)\nEach column acts like a gravity channel:\n\nBeads fall to fill lowest positions\nColumns represent magnitudes across numbers\nAfter settling, beads in each row = sorted value\n\nNo two beads can occupy the same slot twice, ensuring correctness. Complexity: \\[\nT(n) = O(S)\n\\] where \\(S = \\sum a_i\\), total bead count.\nEfficient only when numbers are small.\n\n\nTry It Yourself\n\nSort [5,3,1,7,4] by hand using dots.\nDraw rods and let beads fall.\nTry [3,0,2,1], zeros stay top.\nExperiment with duplicates [2,2,3].\nUse grid visualization in Python.\nCompare with Counting Sort.\nExtend for stable ordering.\nAnimate bead falling step by step.\nScale with numbers ≤ 10.\nReflect: what if gravity was sideways?\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[5,3,1,7,4]\n[1,3,4,5,7]\nClassic example\n\n\n[3,0,2,1]\n[0,1,2,3]\nHandles zeros\n\n\n[2,2,3]\n[2,2,3]\nWorks with duplicates\n\n\n[1]\n[1]\nSingle element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(S), where S = sum of elements\n\n\nSpace\nO(S)\n\n\nStable\nNo\n\n\nAdaptive\nNo\n\n\n\nBead Sort shows how even gravity can sort, numbers become beads, and time, motion, and matter do the work. It’s sorting you can see, not just compute.\n\n\n\n150 Bogo Sort\nBogo Sort (also called Permutation Sort or Stupid Sort) is a deliberately absurd algorithm that repeatedly shuffles the array until it becomes sorted.\nIt’s the poster child of inefficiency, often used in classrooms as a comic counterexample, sorting by pure luck.\n\nWhat Problem Are We Solving??\nWe’re not solving a problem so much as demonstrating futility. Bogo Sort asks, “What if we just kept trying random orders until we got lucky?”\nIt’s a great teaching tool for:\n\nUnderstanding algorithmic inefficiency\nAppreciating complexity bounds\nLearning to recognize good vs. bad strategies\n\n\n\nExample\nSort [3, 1, 2]\n\n\n\nAttempt\nShuffle\nSorted?\n\n\n\n\n1\n[3,1,2]\nNo\n\n\n2\n[1,2,3]\nYes ✅\n\n\n\nStop when lucky! (You could get lucky early… or never.)\n\n\nHow Does It Work (Plain Language)?\nThe idea is painfully simple:\n\nCheck if the array is sorted.\nIf not, shuffle it randomly.\nRepeat until sorted.\n\nIt’s sorting by random chance, not logic. Each attempt has a tiny probability of being sorted, but given infinite time, it will finish (eventually).\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nCheck if array is sorted\n\n\n2\nIf sorted, done\n\n\n3\nElse, shuffle randomly\n\n\n4\nGo back to step 1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport random\n\ndef is_sorted(arr):\n    return all(arr[i] &lt;= arr[i+1] for i in range(len(arr)-1))\n\ndef bogo_sort(arr):\n    attempts = 0\n    while not is_sorted(arr):\n        random.shuffle(arr)\n        attempts += 1\n    print(\"Sorted in\", attempts, \"attempts\")\n    return arr\n\narr = [3, 1, 2]\nprint(bogo_sort(arr))\nOutput (random):\nSorted in 7 attempts\n$$1, 2, 3]\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint is_sorted(int arr[], int n) {\n    for (int i = 0; i &lt; n - 1; i++)\n        if (arr[i] &gt; arr[i + 1]) return 0;\n    return 1;\n}\n\nvoid shuffle(int arr[], int n) {\n    for (int i = 0; i &lt; n; i++) {\n        int j = rand() % n;\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n}\n\nvoid bogo_sort(int arr[], int n) {\n    int attempts = 0;\n    while (!is_sorted(arr, n)) {\n        shuffle(arr, n);\n        attempts++;\n    }\n    printf(\"Sorted in %d attempts\\n\", attempts);\n}\n\nint main(void) {\n    srand(time(NULL));\n    int arr[] = {3, 1, 2};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    bogo_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nOutput (random):\nSorted in 12 attempts  \n1 2 3\n\n\n\nWhy It Matters\n\nHumorous cautionary tale, what not to do\nDemonstrates expected runtime analysis\nA good way to visualize randomness\nReinforces need for algorithmic reasoning\n\nIt’s the algorithmic equivalent of throwing dice until sorted, mathematically silly, but conceptually rich.\n\n\nA Gentle Proof (Why It Works)\nWith \\(n\\) elements, there are \\(n!\\) permutations.\nOnly one is sorted.\nProbability of success = \\(\\frac{1}{n!}\\)\nExpected attempts: \\[\nE(n) = n!\n\\]\nEach check takes \\(O(n)\\), so total expected time: \\[\nT(n) = O(n \\times n!)\n\\]\nGuaranteed termination (eventually), since the probability of not sorting forever \\(\\to 0\\).\n\n\nTry It Yourself\n\nRun on [3,1,2] and count attempts.\nTry [1,2,3], instant success.\nTest [4,3,2,1], likely infinite patience required.\nReplace random.shuffle with deterministic shuffle (see fail).\nAdd a timeout.\nVisualize shuffles on screen.\nMeasure average attempts over 100 trials.\nCompare with Bubble Sort.\nTry “Bogobogosort” (recursive Bogo!).\nReflect: what’s the expected runtime for n=5?\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,1,2]\n[1,2,3]\nClassic\n\n\n[1,2,3]\n[1,2,3]\nAlready sorted\n\n\n[2,1]\n[1,2]\nFast\n\n\n[4,3,2,1]\n[1,2,3,4]\nPossibly never terminates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Expected)\nO(n × n!)\n\n\nTime (Best)\nO(n)\n\n\nTime (Worst)\nUnbounded\n\n\nSpace\nO(1)\n\n\nStable\nYes (if shuffle stable)\n\n\nAdaptive\nNo\n\n\n\nBogo Sort is chaos pretending to be order, sorting by faith, not logic. It’s the universe’s reminder that hope isn’t a strategy, not even in algorithms.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-16.-linear-and-binary-search",
    "href": "books/en-us/list-2.html#section-16.-linear-and-binary-search",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 16. Linear and Binary Search",
    "text": "Section 16. Linear and Binary Search\n\n151 Linear Search\nLinear Search (also known as Sequential Search) is the simplest and most intuitive searching algorithm. It scans through each element one by one until it finds the target, or reaches the end of the list.\nIt’s easy to understand, easy to implement, and works on both sorted and unsorted data.\n\nWhat Problem Are We Solving??\nGiven a list and a target value, how can we check if the target is present, and if so, at which index?\nLinear Search solves this by scanning each element in order until a match is found.\nPerfect for:\n\nSmall datasets\nUnsorted arrays\nEarly learning of search principles\n\n\n\nExample\nFind 7 in [3, 5, 7, 2, 9]:\n\n\n\nStep\nIndex\nValue\nMatch?\n\n\n\n\n1\n0\n3\nNo\n\n\n2\n1\n5\nNo\n\n\n3\n2\n7\n✅ Yes\n\n\n4\nStop\n,\nFound at index 2\n\n\n\n\n\nHow Does It Work (Plain Language)?\nIt’s like flipping through pages one by one looking for a word. No skipping, no guessing, just check everything in order.\nIf the list is [a₀, a₁, a₂, ..., aₙ₋₁]:\n\nStart at index 0\nCompare a[i] with the target\nIf equal → found\nElse → move to next\nStop when found or end reached\n\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart from index 0\n\n\n2\nCompare current element with target\n\n\n3\nIf equal, return index\n\n\n4\nElse, increment index\n\n\n5\nRepeat until end\n\n\n6\nIf not found, return -1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef linear_search(arr, target):\n    for i, val in enumerate(arr):\n        if val == target:\n            return i\n    return -1\n\narr = [3, 5, 7, 2, 9]\ntarget = 7\nidx = linear_search(arr, target)\nprint(\"Found at index:\", idx)\nOutput:\nFound at index: 2\n\n\nC\n#include &lt;stdio.h&gt;\n\nint linear_search(int arr[], int n, int target) {\n    for (int i = 0; i &lt; n; i++)\n        if (arr[i] == target)\n            return i;\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {3, 5, 7, 2, 9};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int target = 7;\n    int idx = linear_search(arr, n, target);\n    if (idx != -1)\n        printf(\"Found at index: %d\\n\", idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound at index: 2\n\n\n\nWhy It Matters\n\nWorks on any list, sorted or unsorted\nNo preprocessing needed\nGuaranteed to find (if present)\nGreat introduction to time complexity\nFoundation for better search algorithms\n\n\n\nA Gentle Proof (Why It Works)\nIf the element exists, scanning each element ensures it will eventually be found.\nFor ( n ) elements:\n\nBest case: found at index 0 → O(1)\nWorst case: not found or last → O(n)\nAverage case: half-way → O(n)\n\nBecause there’s no faster way without structure.\n\n\nTry It Yourself\n\nSearch 7 in [3,5,7,2,9]\nSearch 10 (not in list)\nTry [1,2,3,4,5] with target=1 (best case)\nTry target=5 (worst case)\nCount comparisons made\nPrint “Found” or “Not Found”\nTry on unsorted vs sorted arrays\nModify to return all indices of target\nImplement recursive version\nExtend for string search in list of words\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[3,5,7,2,9]\n7\n2\nFound\n\n\n[3,5,7,2,9]\n10\n-1\nNot found\n\n\n[1,2,3]\n1\n0\nBest case\n\n\n[1,2,3]\n3\n2\nWorst case\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(1)\n\n\nTime (Worst)\nO(n)\n\n\nTime (Average)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\n\nLinear Search is the simplest lens into algorithmic thinking, brute force but guaranteed. It’s your first step from guessing to reasoning.\n\n\n\n152 Linear Search (Sentinel)\nSentinel Linear Search is a clever twist on the basic Linear Search. Instead of checking array bounds each time, we place a sentinel (a guard value) at the end of the array equal to the target.\nThis eliminates the need for explicit boundary checks inside the loop, making the search slightly faster and cleaner, especially in low-level languages like C.\n\nWhat Problem Are We Solving??\nIn a standard linear search, each iteration checks both:\n\nIf the current element equals the target\nIf the index is still within bounds\n\nThat second check adds overhead.\nBy placing a sentinel, we can guarantee the loop will always terminate, no bounds check needed.\nThis is useful in tight loops, embedded systems, and performance-critical code.\n\n\nExample\nFind 7 in [3, 5, 7, 2, 9]:\n\nAppend sentinel (duplicate target) → [3, 5, 7, 2, 9, 7]\nScan until 7 is found\nIf index &lt; n, found in array\nIf index == n, only sentinel found → not in array\n\n\n\n\nStep\nIndex\nValue\nMatch?\n\n\n\n\n1\n0\n3\nNo\n\n\n2\n1\n5\nNo\n\n\n3\n2\n7\n✅ Yes\n\n\n4\nStop, index &lt; n\nFound at 2\n\n\n\n\n\n\nHow Does It Work (Plain Language)?\nThink of the sentinel as a stop sign placed beyond the last element. You don’t have to look over your shoulder to check if you’ve gone too far, the sentinel will catch you.\nIt ensures you’ll always hit a match, but then you check whether it was a real match or the sentinel.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nSave last element\n\n\n2\nPlace target at the end (sentinel)\n\n\n3\nLoop until arr[i] == target\n\n\n4\nRestore last element\n\n\n5\nIf index &lt; n → found, else → not found\n\n\n\n\n\nTiny Code (Easy Versions)\n\nC\n#include &lt;stdio.h&gt;\n\nint sentinel_linear_search(int arr[], int n, int target) {\n    int last = arr[n - 1];\n    arr[n - 1] = target; // sentinel\n    int i = 0;\n    while (arr[i] != target)\n        i++;\n    arr[n - 1] = last; // restore\n    if (i &lt; n - 1 || arr[n - 1] == target)\n        return i;\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {3, 5, 7, 2, 9};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int target = 7;\n    int idx = sentinel_linear_search(arr, n, target);\n    if (idx != -1)\n        printf(\"Found at index: %d\\n\", idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound at index: 2\n\n\n\nPython (Simulated)\ndef sentinel_linear_search(arr, target):\n    n = len(arr)\n    last = arr[-1]\n    arr[-1] = target\n    i = 0\n    while arr[i] != target:\n        i += 1\n    arr[-1] = last\n    if i &lt; n - 1 or arr[-1] == target:\n        return i\n    return -1\n\narr = [3, 5, 7, 2, 9]\nprint(sentinel_linear_search(arr, 7))  # Output: 2\n\n\nWhy It Matters\n\nRemoves boundary check overhead\nSlightly faster for large arrays\nClassic example of sentinel optimization\nTeaches loop invariants and guard conditions\n\nThis is how you make a simple algorithm tight and elegant.\n\n\nA Gentle Proof (Why It Works)\nBy placing the target as the last element:\n\nLoop must terminate (guaranteed match)\nOnly after the loop do we check if it was sentinel or real match\n\nNo wasted comparisons. Total comparisons ≤ n + 1 (vs 2n in naive version).\n\\[\nT(n) = O(n)\n\\]\n\n\nTry It Yourself\n\nSearch 7 in [3,5,7,2,9]\nSearch 10 (not in list)\nTrack number of comparisons vs regular linear search\nImplement in Python, Java, C++\nVisualize sentinel placement\nUse array of 1000 random elements, benchmark\nTry replacing last element temporarily\nSearch first element, check best case\nSearch last element, check sentinel restore\nDiscuss when this optimization helps most\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[3,5,7,2,9]\n7\n2\nFound\n\n\n[3,5,7,2,9]\n10\n-1\nNot found\n\n\n[1,2,3]\n1\n0\nBest case\n\n\n[1,2,3]\n3\n2\nSentinel replaced last\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(1)\n\n\nTime (Worst)\nO(n)\n\n\nTime (Average)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\n\nSentinel Linear Search is how you turn simplicity into elegance, one tiny guard makes the whole loop smarter.\n\n\n\n153 Binary Search (Iterative)\nBinary Search (Iterative) is one of the most elegant and efficient searching algorithms for sorted arrays. It repeatedly divides the search interval in half, eliminating half the remaining elements at each step.\nThis version uses a loop, avoiding recursion and keeping memory usage minimal.\n\nWhat Problem Are We Solving??\nWhen working with sorted data, a linear scan is wasteful. If you always know the list is ordered, you can use binary search to find your target in O(log n) time instead of O(n).\n\n\nExample\nFind 7 in [1, 3, 5, 7, 9, 11]:\n\n\n\nStep\nLow\nHigh\nMid\nValue\nCompare\n\n\n\n\n1\n0\n5\n2\n5\n7 &gt; 5 → search right\n\n\n2\n3\n5\n4\n9\n7 &lt; 9 → search left\n\n\n3\n3\n3\n3\n7\n✅ Found\n\n\n\nFound at index 3.\n\n\nHow Does It Work (Plain Language)?\nBinary Search is like guessing a number between 1 and 100:\n\nAlways pick the midpoint.\nIf the number is too low, search the upper half.\nIf it’s too high, search the lower half.\nRepeat until found or interval is empty.\n\nEach guess cuts the space in half, that’s why it’s so fast.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart with low = 0, high = n - 1\n\n\n2\nWhile low ≤ high, find mid = (low + high) // 2\n\n\n3\nIf arr[mid] == target → return index\n\n\n4\nIf arr[mid] &lt; target → search right half (low = mid + 1)\n\n\n5\nElse → search left half (high = mid - 1)\n\n\n6\nIf not found, return -1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef binary_search(arr, target):\n    low, high = 0, len(arr) - 1\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n\narr = [1, 3, 5, 7, 9, 11]\nprint(binary_search(arr, 7))  # Output: 3\nOutput:\n3\n\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search(int arr[], int n, int target) {\n    int low = 0, high = n - 1;\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2; // avoid overflow\n        if (arr[mid] == target)\n            return mid;\n        else if (arr[mid] &lt; target)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {1, 3, 5, 7, 9, 11};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = binary_search(arr, n, 7);\n    if (idx != -1)\n        printf(\"Found at index: %d\\n\", idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound at index: 3\n\n\n\nWhy It Matters\n\nFundamental divide-and-conquer algorithm\nO(log n) time complexity\nUsed everywhere: search engines, databases, compilers\nBuilds intuition for binary decision trees\n\nThis is the first truly efficient search most programmers learn.\n\n\nA Gentle Proof (Why It Works)\nAt each step, the search interval halves.\nAfter \\(k\\) steps, remaining elements = \\(\\frac{n}{2^k}\\).\nStop when \\(\\frac{n}{2^k} = 1\\)\n⟹ \\(k = \\log_2 n\\)\nSo, total comparisons: \\[\nT(n) = O(\\log n)\n\\]\nWorks only on sorted arrays.\n\n\nTry It Yourself\n\nSearch 7 in [1,3,5,7,9,11]\nSearch 2 (not found)\nTrace values of low, high, mid\nTry on even-length array [1,2,3,4,5,6]\nTry on odd-length array [1,2,3,4,5]\nCompare iteration count with linear search\nImplement recursive version\nUse binary search to find insertion point\nAdd counter to measure steps\nExplain why sorting is required\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[1,3,5,7,9,11]\n7\n3\nFound\n\n\n[1,3,5,7,9,11]\n2\n-1\nNot found\n\n\n[1,2,3,4,5]\n1\n0\nFirst element\n\n\n[1,2,3,4,5]\n5\n4\nLast element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(1)\n\n\nTime (Worst)\nO(log n)\n\n\nTime (Average)\nO(log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nBinary Search (Iterative) is the gold standard of efficiency, halving your problem at every step, one decision at a time.\n\n\n\n154 Binary Search (Recursive)\nBinary Search (Recursive) is the classic divide-and-conquer form of binary search. Instead of looping, it calls itself on smaller subarrays, each time halving the search space until the target is found or the interval becomes empty.\nIt’s a perfect demonstration of recursion in action, each call tackles a smaller slice of the problem.\n\nWhat Problem Are We Solving??\nGiven a sorted array, we want to find a target value efficiently. Rather than scanning linearly, we repeatedly split the array in half, focusing only on the half that could contain the target.\nThis version expresses that logic through recursive calls.\n\n\nExample\nFind 7 in [1, 3, 5, 7, 9, 11]\n\n\n\nStep\nLow\nHigh\nMid\nValue\nAction\n\n\n\n\n1\n0\n5\n2\n5\n7 &gt; 5 → search right half\n\n\n2\n3\n5\n4\n9\n7 &lt; 9 → search left half\n\n\n3\n3\n3\n3\n7\n✅ Found\n\n\n\nRecursive calls shrink the interval each time until match is found.\n\n\nHow Does It Work (Plain Language)?\nBinary search says:\n\n“If the middle element isn’t what I want, I can ignore half the data.”\n\nIn recursive form:\n\nCheck the midpoint.\nIf equal → found.\nIf smaller → recurse right.\nIf larger → recurse left.\nBase case: if low &gt; high, element not found.\n\nEach call reduces the search space by half, logarithmic depth recursion.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nCheck base case: if low &gt; high, return -1\n\n\n2\nCompute mid = (low + high) // 2\n\n\n3\nIf arr[mid] == target, return mid\n\n\n4\nIf arr[mid] &gt; target, recurse left half\n\n\n5\nElse, recurse right half\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef binary_search_recursive(arr, target, low, high):\n    if low &gt; high:\n        return -1\n    mid = (low + high) // 2\n    if arr[mid] == target:\n        return mid\n    elif arr[mid] &lt; target:\n        return binary_search_recursive(arr, target, mid + 1, high)\n    else:\n        return binary_search_recursive(arr, target, low, mid - 1)\n\narr = [1, 3, 5, 7, 9, 11]\nprint(binary_search_recursive(arr, 7, 0, len(arr)-1))  # Output: 3\nOutput:\n3\n\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search_recursive(int arr[], int low, int high, int target) {\n    if (low &gt; high)\n        return -1;\n    int mid = low + (high - low) / 2;\n    if (arr[mid] == target)\n        return mid;\n    else if (arr[mid] &lt; target)\n        return binary_search_recursive(arr, mid + 1, high, target);\n    else\n        return binary_search_recursive(arr, low, mid - 1, target);\n}\n\nint main(void) {\n    int arr[] = {1, 3, 5, 7, 9, 11};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = binary_search_recursive(arr, 0, n - 1, 7);\n    if (idx != -1)\n        printf(\"Found at index: %d\\n\", idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound at index: 3\n\n\n\nWhy It Matters\n\nElegant divide-and-conquer demonstration\nShows recursion depth = log₂(n)\nSame complexity as iterative version\nLays foundation for recursive algorithms (merge sort, quicksort)\n\nRecursion mirrors the mathematical idea of halving the interval, clean and intuitive.\n\n\nA Gentle Proof (Why It Works)\nAt each recursive call:\n\nProblem size halves: \\(n \\to n/2 \\to n/4 \\to \\dots\\)\nRecursion stops after \\(\\log_2 n\\) levels\n\nThus total complexity: \\[\nT(n) = T(n/2) + O(1) = O(\\log n)\n\\]\nCorrectness follows from:\n\nSorted input ensures ordering decisions are valid\n\nBase case ensures termination\n\n\n\nTry It Yourself\n\nSearch 7 in [1,3,5,7,9,11]\nSearch 2 (not present)\nAdd print statements to trace recursion\nCompare call count vs iterative version\nTest base case with low &gt; high\nSearch first and last element\nObserve recursion depth for size 8 → 3 calls\nAdd memo of (low, high) per step\nImplement tail-recursive variant\nReflect on stack vs loop tradeoffs\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[1,3,5,7,9,11]\n7\n3\nFound\n\n\n[1,3,5,7,9,11]\n2\n-1\nNot found\n\n\n[1,2,3,4,5]\n1\n0\nFirst element\n\n\n[1,2,3,4,5]\n5\n4\nLast element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Best)\nO(1)\n\n\nTime (Worst)\nO(log n)\n\n\nTime (Average)\nO(log n)\n\n\nSpace\nO(log n) (recursion stack)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nBinary Search (Recursive) is divide-and-conquer at its purest, halving the world each time, until the answer reveals itself.\n\n\n\n155 Binary Search (Lower Bound)\nLower Bound Binary Search is a variant of binary search that finds the first position where a value could be inserted without breaking the sorted order. In other words, it returns the index of the first element greater than or equal to the target.\nIt’s used extensively in search engines, databases, range queries, and C++ STL (std::lower_bound).\n\nWhat Problem Are We Solving??\nIn many cases, you don’t just want to know if an element exists — you want to know where it would go in a sorted structure.\n\nIf the value exists, return its first occurrence.\nIf it doesn’t exist, return the position where it could be inserted to keep the array sorted.\n\nThis is essential for insertion, frequency counting, and range boundaries.\n\n\nExample\nFind lower bound of 7 in [1, 3, 5, 7, 7, 9, 11]:\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nLow\nHigh\nMid\nValue\nCompare\nAction\n\n\n\n\n1\n0\n6\n3\n7\narr[mid] &gt;= 7\nmove high = 3\n\n\n2\n0\n2\n1\n3\narr[mid] &lt; 7\nmove low = 2\n\n\n3\n2\n3\n2\n5\narr[mid] &lt; 7\nmove low = 3\n\n\n4\nlow == high\n,\n,\n,\nStop\n\n\n\n\nResult: Index 3 (first 7)\n\n\nHow Does It Work (Plain Language)?\nLower bound finds the leftmost slot where the target fits. It slides the search window until low == high, with low marking the first candidate ≥ target.\nYou can think of it as:\n\n“How far left can I go while still being ≥ target?”\n\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nSet low = 0, high = n (note: high = n, not n-1)\n\n\n2\nWhile low &lt; high:\n\n\n a. mid = (low + high) // 2\n\n\n\n b. If arr[mid] &lt; target, move low = mid + 1\n\n\n\n c. Else, move high = mid\n\n\n\n3\nWhen loop ends, low is the lower bound index\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef lower_bound(arr, target):\n    low, high = 0, len(arr)\n    while low &lt; high:\n        mid = (low + high) // 2\n        if arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid\n    return low\n\narr = [1, 3, 5, 7, 7, 9, 11]\nprint(lower_bound(arr, 7))  # Output: 3\nOutput:\n3\n\n\nC\n#include &lt;stdio.h&gt;\n\nint lower_bound(int arr[], int n, int target) {\n    int low = 0, high = n;\n    while (low &lt; high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] &lt; target)\n            low = mid + 1;\n        else\n            high = mid;\n    }\n    return low;\n}\n\nint main(void) {\n    int arr[] = {1, 3, 5, 7, 7, 9, 11};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = lower_bound(arr, n, 7);\n    printf(\"Lower bound index: %d\\n\", idx);\n}\nOutput:\nLower bound index: 3\n\n\n\nWhy It Matters\n\nFinds insertion position for sorted arrays\nUseful in binary search trees, maps, intervals\nCore of range queries (like count of elements ≥ x)\nBuilds understanding of boundary binary search\n\nWhen you need more than yes/no, you need where, use lower bound.\n\n\nA Gentle Proof (Why It Works)\nInvariant:\n\nAll indices before low contain elements &lt; target\nAll indices after high contain elements ≥ target\n\nLoop maintains invariant until convergence: \\[\nlow = high = \\text{first index where arr[i] ≥ target}\n\\]\nComplexity: \\[\nT(n) = O(\\log n)\n\\]\n\n\nTry It Yourself\n\narr = [1,3,5,7,7,9], target = 7 → 3\ntarget = 6 → 3 (would insert before first 7)\ntarget = 10 → 6 (end)\ntarget = 0 → 0 (front)\nCount elements ≥ 7: len(arr) - lower_bound(arr, 7)\nCompare with bisect_left in Python\nUse to insert elements while keeping list sorted\nApply to sorted strings [\"apple\", \"banana\", \"cherry\"]\nVisualize range [lower, upper) for duplicates\nBenchmark vs linear scan for large n\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nMeaning\n\n\n\n\n[1,3,5,7,7,9,11]\n7\n3\nFirst 7\n\n\n[1,3,5,7,7,9,11]\n6\n3\nInsert before 7\n\n\n[1,3,5,7,7,9,11]\n12\n7\nEnd position\n\n\n[1,3,5,7,7,9,11]\n0\n0\nFront\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nBinary Search (Lower Bound) is how algorithms learn where things belong, not just if they exist. It’s the precise edge of order.\n\n\n\n156 Binary Search (Upper Bound)\nUpper Bound Binary Search is a close cousin of Lower Bound, designed to find the first index where an element is greater than the target. It’s used to locate the right boundary of equal elements or the insertion point after duplicates.\nIn simpler words:\n\nIt finds “the spot just after the last occurrence” of the target.\n\n\nWhat Problem Are We Solving??\nSometimes you need to know where to insert a value after existing duplicates. For example, in frequency counting or range queries, you might want the end of a block of identical elements.\nUpper bound returns that exact position, the smallest index where \\[\n\\text{arr[i]} &gt; \\text{target}\n\\]\n\n\nExample\nFind upper bound of 7 in [1, 3, 5, 7, 7, 9, 11]:\n\n\n\nStep\nLow\nHigh\nMid\nValue\nCompare\nAction\n\n\n\n\n1\n0\n7\n3\n7\n7 ≤ 7\nmove low = 4\n\n\n2\n4\n7\n5\n9\n9 &gt; 7\nmove high = 5\n\n\n3\n4\n5\n4\n7\n7 ≤ 7\nmove low = 5\n\n\n4\nlow == high\n,\n,\n,\nStop\n\n\n\n\nResult: Index 5 → position after last 7.\n\n\nHow Does It Work (Plain Language)?\nIf Lower Bound finds “the first ≥ target,” then Upper Bound finds “the first &gt; target.”\nTogether, they define equal ranges: \\[\n\\], ) $$ → all elements equal to the target.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nSet low = 0, high = n\n\n\n2\nWhile low &lt; high:\n\n\n a. mid = (low + high) // 2\n\n\n\n b. If arr[mid] &lt;= target, move low = mid + 1\n\n\n\n c. Else, move high = mid\n\n\n\n3\nWhen loop ends, low is the upper bound index\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef upper_bound(arr, target):\n    low, high = 0, len(arr)\n    while low &lt; high:\n        mid = (low + high) // 2\n        if arr[mid] &lt;= target:\n            low = mid + 1\n        else:\n            high = mid\n    return low\n\narr = [1, 3, 5, 7, 7, 9, 11]\nprint(upper_bound(arr, 7))  # Output: 5\nOutput:\n5\n\n\nC\n#include &lt;stdio.h&gt;\n\nint upper_bound(int arr[], int n, int target) {\n    int low = 0, high = n;\n    while (low &lt; high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] &lt;= target)\n            low = mid + 1;\n        else\n            high = mid;\n    }\n    return low;\n}\n\nint main(void) {\n    int arr[] = {1, 3, 5, 7, 7, 9, 11};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = upper_bound(arr, n, 7);\n    printf(\"Upper bound index: %d\\n\", idx);\n}\nOutput:\nUpper bound index: 5\n\n\n\nWhy It Matters\n\nLocates end of duplicate block\nEnables range counting:  count = upper_bound - lower_bound\nUsed in maps, sets, STL containers\nKey component in range queries and interval merging\n\nIt’s the right-hand anchor of sorted intervals.\n\n\nA Gentle Proof (Why It Works)\nInvariant:\n\nAll indices before low contain \\(\\le\\) target\n\nAll indices after high contain \\(&gt;\\) target\n\nWhen low == high, it’s the smallest index where arr[i] &gt; target.\nNumber of steps = \\(\\log_2 n\\)\n→ Complexity: \\[\nT(n) = O(\\log n)\n\\]\n\n\nTry It Yourself\n\narr = [1,3,5,7,7,9], target=7 → 5\ntarget=6 → 3 (insert after 5)\ntarget=10 → 6 (end)\ntarget=0 → 0 (front)\nCount elements equal to 7 → upper - lower\nCompare with bisect_right in Python\nUse to insert new element after duplicates\nCombine with lower bound to find range\nVisualize [lower, upper) range\nApply to floating point sorted list\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nMeaning\n\n\n\n\n[1,3,5,7,7,9,11]\n7\n5\nAfter last 7\n\n\n[1,3,5,7,7,9,11]\n6\n3\nInsert after 5\n\n\n[1,3,5,7,7,9,11]\n12\n7\nEnd position\n\n\n[1,3,5,7,7,9,11]\n0\n0\nFront\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nBinary Search (Upper Bound) is how you find where “greater than” begins, the right edge of equality, the step beyond the last twin.\n\n\n\n157 Exponential Search\nExponential Search is a hybrid search algorithm that quickly locates the range where a target might lie, then uses binary search inside that range.\nIt’s ideal when searching unbounded or very large sorted arrays, especially when the size is unknown or dynamic (like data streams or infinite arrays).\n\nWhat Problem Are We Solving??\nIn a standard binary search, you need the array size. But what if the size is unknown, or huge?\nExponential Search solves this by:\n\nQuickly finding an interval where the target could exist.\nPerforming binary search within that interval.\n\nThis makes it great for:\n\nInfinite arrays (conceptual)\nStreams\nLinked structures with known order\nLarge sorted data where bounds are costly\n\n\n\nExample\nFind 15 in [1, 2, 4, 8, 16, 32, 64, 128]\n\n\n\n\n\n\n\n\n\n\nStep\nRange\nValue\nCompare\nAction\n\n\n\n\n1\nindex 1\n2\n2 &lt; 15\nexpand\n\n\n2\nindex 2\n4\n4 &lt; 15\nexpand\n\n\n3\nindex 4\n16\n16 ≥ 15\nstop\n\n\n4\nRange = [2, 4]\nBinary Search in [4, 8, 16]\n✅ Found 15 at index 4\n\n\n\n\nWe doubled the bound each time (1, 2, 4, 8…) until we passed the target.\n\n\nHow Does It Work (Plain Language)?\nThink of it like zooming in:\n\nStart small, double your step size until you overshoot the target.\nOnce you’ve bracketed it, zoom in with binary search.\n\nThis avoids scanning linearly when the array could be massive.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart with index 1\n\n\n2\nWhile i &lt; n and arr[i] &lt; target, double i\n\n\n3\nNow target ∈ [i/2, min(i, n-1)]\n\n\n4\nApply binary search in that subrange\n\n\n5\nReturn found index or -1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef binary_search(arr, target, low, high):\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n\ndef exponential_search(arr, target):\n    if arr[0] == target:\n        return 0\n    i = 1\n    n = len(arr)\n    while i &lt; n and arr[i] &lt; target:\n        i *= 2\n    return binary_search(arr, target, i // 2, min(i, n - 1))\n\narr = [1, 2, 4, 8, 16, 32, 64, 128]\nprint(exponential_search(arr, 16))  # Output: 4\nOutput:\n4\n\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search(int arr[], int low, int high, int target) {\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] == target)\n            return mid;\n        else if (arr[mid] &lt; target)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1;\n}\n\nint exponential_search(int arr[], int n, int target) {\n    if (arr[0] == target)\n        return 0;\n    int i = 1;\n    while (i &lt; n && arr[i] &lt; target)\n        i *= 2;\n    int low = i / 2;\n    int high = (i &lt; n) ? i : n - 1;\n    return binary_search(arr, low, high, target);\n}\n\nint main(void) {\n    int arr[] = {1, 2, 4, 8, 16, 32, 64, 128};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = exponential_search(arr, n, 16);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nWorks with unknown or infinite size arrays\nFaster than linear scan for large n\nCombines doubling search and binary search\nUsed in streaming data structures, file systems, unbounded containers\n\nIt’s the searcher’s flashlight, shine brighter until you see your target.\n\n\nA Gentle Proof (Why It Works)\nDoubling creates at most \\(\\log_2 p\\) expansions,\nwhere \\(p\\) is the position of the target.\nThen binary search on a range of size \\(O(p)\\) takes another \\(O(\\log p)\\).\nSo total time: \\[\nT(n) = O(\\log p)\n\\]\nAsymptotically equal to binary search when \\(p \\ll n\\).\n\n\nTry It Yourself\n\nSearch 16 in [1,2,4,8,16,32,64,128]\nSearch 3 → not found\nTrace expansions: 1,2,4,8…\nTry [10,20,30,40,50,60,70] with target=60\nModify doubling factor (try 3x)\nCompare with simple binary search\nImplement recursive exponential search\nUse for unknown-size input stream\nMeasure expansion count\nVisualize ranges on number line\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[1,2,4,8,16,32,64]\n16\n4\nClassic\n\n\n[1,2,4,8,16,32,64]\n3\n-1\nNot found\n\n\n[2,4,6,8]\n2\n0\nFirst element\n\n\n[2,4,6,8]\n10\n-1\nOut of range\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log p), where p = position of target\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nPartially\n\n\nPrerequisite\nSorted array\n\n\n\nExponential Search is how you find your way in the dark, double your reach, then look closely where the light lands.\n\n\n\n158 Jump Search\nJump Search is a simple improvement over Linear Search, designed for sorted arrays. It works by jumping ahead in fixed-size steps instead of checking every element, then performing a linear scan within the block where the target might be.\nIt trades a bit of extra logic for a big win in speed, especially when data is sorted and random access is cheap.\n\nWhat Problem Are We Solving??\nLinear search checks each element one by one, slow for large arrays. Jump Search improves on this by “skipping ahead” in fixed jumps, so it makes fewer comparisons overall.\nIt’s great for:\n\nSorted arrays\nFast random access (like arrays, not linked lists)\nSimple, predictable search steps\n\n\n\nExample\nFind 9 in [1, 3, 5, 7, 9, 11, 13, 15]\nArray size = 8 → Jump size = √8 = 2 or 3\n\n\n\nStep\nJump to Index\nValue\nCompare\nAction\n\n\n\n\n1\n2\n5\n5 &lt; 9\njump forward\n\n\n2\n4\n9\n9 ≥ 9\nstop jump\n\n\n3\nLinear scan from 2\n5, 7, 9\n✅ found 9\n\n\n\n\nFound at index 4.\n\n\nHow Does It Work (Plain Language)?\nImagine you’re reading a sorted list of numbers. Instead of reading one number at a time, you skip ahead every few steps, like jumping stairs. Once you overshoot or match, you walk back linearly within that block.\nJump size ≈ √n gives a good balance between jumps and scans.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nChoose jump size step = √n\n\n\n2\nJump ahead while arr[min(step, n)-1] &lt; target\n\n\n3\nOnce overshoot, do linear search in the previous block\n\n\n4\nIf found, return index, else -1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport math\n\ndef jump_search(arr, target):\n    n = len(arr)\n    step = int(math.sqrt(n))\n    prev = 0\n\n    # Jump ahead\n    while prev &lt; n and arr[min(step, n) - 1] &lt; target:\n        prev = step\n        step += int(math.sqrt(n))\n        if prev &gt;= n:\n            return -1\n\n    # Linear search within block\n    for i in range(prev, min(step, n)):\n        if arr[i] == target:\n            return i\n    return -1\n\narr = [1, 3, 5, 7, 9, 11, 13, 15]\nprint(jump_search(arr, 9))  # Output: 4\nOutput:\n4\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nint jump_search(int arr[], int n, int target) {\n    int step = sqrt(n);\n    int prev = 0;\n\n    while (prev &lt; n && arr[(step &lt; n ? step : n) - 1] &lt; target) {\n        prev = step;\n        step += sqrt(n);\n        if (prev &gt;= n) return -1;\n    }\n\n    for (int i = prev; i &lt; (step &lt; n ? step : n); i++) {\n        if (arr[i] == target) return i;\n    }\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {1, 3, 5, 7, 9, 11, 13, 15};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = jump_search(arr, n, 9);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nFaster than Linear Search for sorted arrays\nSimpler to implement than Binary Search\nGood for systems with non-random access penalties\nBalance of jumps and scans minimizes comparisons\n\nBest of both worlds: quick leaps and gentle steps.\n\n\nA Gentle Proof (Why It Works)\nLet jump size = \\(m\\).\nWe perform at most \\(\\frac{n}{m}\\) jumps and \\(m\\) linear steps.\nTotal cost: \\[\nT(n) = O\\left(\\frac{n}{m} + m\\right)\n\\]\nMinimized when \\(m = \\sqrt{n}\\)\nThus: \\[\nT(n) = O(\\sqrt{n})\n\\]\n\n\nTry It Yourself\n\nSearch 11 in [1,3,5,7,9,11,13,15]\nSearch 2 (not found)\nUse step = 2, 3, 4 → compare jumps\nImplement recursive version\nVisualize jumps on paper\nTry unsorted array (observe failure)\nSearch edge cases: first, last, middle\nUse different step size formula\nCombine with exponential jump\nCompare with binary search for timing\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[1,3,5,7,9,11,13,15]\n9\n4\nFound\n\n\n[1,3,5,7,9,11,13,15]\n2\n-1\nNot found\n\n\n[2,4,6,8,10]\n10\n4\nLast element\n\n\n[2,4,6,8,10]\n2\n0\nFirst element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(√n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nJump Search is like hopscotch on sorted ground, leap smartly, then step carefully once you’re close.\n\n\n\n159 Fibonacci Search\nFibonacci Search is a divide-and-conquer search algorithm that uses Fibonacci numbers to determine probe positions, rather than midpoints like Binary Search. It’s especially efficient for sorted arrays stored in sequential memory, where element access cost grows with distance (for example, on magnetic tape or cache-sensitive systems).\nIt’s a clever twist on binary search, using Fibonacci numbers instead of powers of two.\n\nWhat Problem Are We Solving??\nIn Binary Search, the midpoint splits the array evenly. In Fibonacci Search, we split using Fibonacci ratios, which keeps indices aligned to integer arithmetic, no division needed, and better cache locality in some hardware.\nThis is particularly useful when:\n\nAccess cost depends on distance\nMemory access is sequential or limited\nWe want to avoid division and floating-point math\n\n\n\nExample\nFind 8 in [1, 3, 5, 8, 13, 21, 34]\n\n\n\nStep\nFib(k)\nIndex Checked\nValue\nCompare\nAction\n\n\n\n\n1\n8\n5\n13\n13 &gt; 8\nmove left\n\n\n2\n5\n2\n5\n5 &lt; 8\nmove right\n\n\n3\n3\n3\n8\n8 = 8\n✅ found\n\n\n\nFibonacci numbers: 1, 2, 3, 5, 8, 13, … We reduce the search space using previous Fibonacci values, like Fibonacci decomposition.\n\n\nHow Does It Work (Plain Language)?\nThink of Fibonacci Search as binary search guided by Fibonacci jumps. At each step:\n\nYou compare the element at index offset + Fib(k-2)\nDepending on the result, you move left or right, using smaller Fibonacci numbers to define new intervals.\n\nYou “walk down” the Fibonacci sequence until the range collapses.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nGenerate smallest Fibonacci number ≥ n\n\n\n2\nUse Fib(k-2) as probe index\n\n\n3\nCompare target with arr[offset + Fib(k-2)]\n\n\n4\nIf smaller, move left (reduce by Fib(k-2))\n\n\n5\nIf larger, move right (increase offset, reduce by Fib(k-1))\n\n\n6\nContinue until Fib(k) = 1\n\n\n7\nCheck last element if needed\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef fibonacci_search(arr, target):\n    n = len(arr)\n    fibMMm2 = 0  # (m-2)'th Fibonacci\n    fibMMm1 = 1  # (m-1)'th Fibonacci\n    fibM = fibMMm1 + fibMMm2  # m'th Fibonacci\n\n    # Find smallest Fibonacci &gt;= n\n    while fibM &lt; n:\n        fibMMm2, fibMMm1 = fibMMm1, fibM\n        fibM = fibMMm1 + fibMMm2\n\n    offset = -1\n\n    while fibM &gt; 1:\n        i = min(offset + fibMMm2, n - 1)\n\n        if arr[i] &lt; target:\n            fibM = fibMMm1\n            fibMMm1 = fibMMm2\n            fibMMm2 = fibM - fibMMm1\n            offset = i\n        elif arr[i] &gt; target:\n            fibM = fibMMm2\n            fibMMm1 -= fibMMm2\n            fibMMm2 = fibM - fibMMm1\n        else:\n            return i\n\n    if fibMMm1 and offset + 1 &lt; n and arr[offset + 1] == target:\n        return offset + 1\n\n    return -1\n\narr = [1, 3, 5, 8, 13, 21, 34]\nprint(fibonacci_search(arr, 8))  # Output: 3\nOutput:\n3\n\n\nC\n#include &lt;stdio.h&gt;\n\nint min(int a, int b) { return (a &lt; b) ? a : b; }\n\nint fibonacci_search(int arr[], int n, int target) {\n    int fibMMm2 = 0;\n    int fibMMm1 = 1;\n    int fibM = fibMMm1 + fibMMm2;\n\n    while (fibM &lt; n) {\n        fibMMm2 = fibMMm1;\n        fibMMm1 = fibM;\n        fibM = fibMMm1 + fibMMm2;\n    }\n\n    int offset = -1;\n\n    while (fibM &gt; 1) {\n        int i = min(offset + fibMMm2, n - 1);\n\n        if (arr[i] &lt; target) {\n            fibM = fibMMm1;\n            fibMMm1 = fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n            offset = i;\n        } else if (arr[i] &gt; target) {\n            fibM = fibMMm2;\n            fibMMm1 -= fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n        } else {\n            return i;\n        }\n    }\n\n    if (fibMMm1 && offset + 1 &lt; n && arr[offset + 1] == target)\n        return offset + 1;\n\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {1, 3, 5, 8, 13, 21, 34};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = fibonacci_search(arr, n, 8);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 3\n\n\n\nWhy It Matters\n\nUses only addition and subtraction, no division\nGreat for sequential access memory\nMatches binary search performance (O(log n))\nOffers better locality on some hardware\n\nIt’s the search strategy built from nature’s own numbers.\n\n\nA Gentle Proof (Why It Works)\nEach iteration reduces the search space by a Fibonacci ratio: \\[\nF_k = F_{k-1} + F_{k-2}\n\\]\nHence, search depth ≈ Fibonacci index \\(k \\sim \\log_\\phi n\\),\nwhere \\(\\phi\\) is the golden ratio (\\(\\approx 1.618\\)).\nSo total time: \\[\nT(n) = O(\\log n)\n\\]\n\n\nTry It Yourself\n\nSearch 8 in [1,3,5,8,13,21,34]\nSearch 21\nSearch 2 (not found)\nTrace Fibonacci sequence steps\nCompare probe indices with binary search\nTry with n=10 → Fibonacci 13 ≥ 10\nImplement recursive version\nVisualize probe intervals\nReplace Fibonacci with powers of 2 (binary search)\nExperiment with non-uniform arrays\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[1,3,5,8,13,21,34]\n8\n3\nFound\n\n\n[1,3,5,8,13,21,34]\n2\n-1\nNot found\n\n\n[1,3,5,8,13,21,34]\n34\n6\nLast element\n\n\n[1,3,5,8,13,21,34]\n1\n0\nFirst element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nFibonacci Search is searching with nature’s rhythm, each step shaped by the golden ratio, balancing reach and precision.\n\n\n\n160 Uniform Binary Search\nUniform Binary Search is an optimized form of Binary Search where the probe positions are precomputed. Instead of recalculating the midpoint at each step, it uses a lookup table of offsets to determine where to go next, making it faster in tight loops or hardware-limited systems.\nIt’s all about removing repetitive midpoint arithmetic and branching for consistent, uniform steps.\n\nWhat Problem Are We Solving??\nStandard binary search repeatedly computes:\n\\[\nmid = low + \\frac{high - low}{2}\n\\]\nThis is cheap on modern CPUs but expensive on:\n\nEarly hardware\nEmbedded systems\nTight loops where division or shifting is costly\n\nUniform Binary Search (UBS) replaces these computations with precomputed offsets for each step, giving predictable, uniform jumps.\n\n\nExample\nSearch 25 in [5, 10, 15, 20, 25, 30, 35, 40]\n\n\n\nStep\nOffset\nIndex\nValue\nCompare\nAction\n\n\n\n\n1\n3\n3\n20\n20 &lt; 25\nmove right\n\n\n2\n1\n5\n30\n30 &gt; 25\nmove left\n\n\n3\n0\n4\n25\n25 = 25\n✅ found\n\n\n\nInstead of recalculating midpoints, UBS uses offset table [3, 1, 0].\n\n\nHow Does It Work (Plain Language)?\nIt’s binary search with a preplanned route. At each level:\n\nYou jump by a fixed offset (from a table)\nCompare\nMove left or right, shrinking your window uniformly\n\nNo divisions, no mid calculations, just jump and compare.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nPrecompute offset table based on array size\n\n\n2\nStart at offset[0] from beginning\n\n\n3\nCompare element with target\n\n\n4\nMove left/right using next offset\n\n\n5\nStop when offset = 0 or found\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef uniform_binary_search(arr, target):\n    n = len(arr)\n    # Precompute offsets (powers of 2 less than n)\n    offsets = []\n    k = 1\n    while k &lt; n:\n        offsets.append(k)\n        k *= 2\n    offsets.reverse()\n\n    low = 0\n    idx = offsets[0]\n\n    for offset in offsets:\n        mid = low + offset if low + offset &lt; n else n - 1\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        # else move left (implicitly handled next iteration)\n\n    # Final check\n    if low &lt; n and arr[low] == target:\n        return low\n    return -1\n\narr = [5, 10, 15, 20, 25, 30, 35, 40]\nprint(uniform_binary_search(arr, 25))  # Output: 4\nOutput:\n4\n\n\nC\n#include &lt;stdio.h&gt;\n\nint uniform_binary_search(int arr[], int n, int target) {\n    int k = 1;\n    while (k &lt; n) k &lt;&lt;= 1;\n    k &gt;&gt;= 1;\n\n    int low = 0;\n    while (k &gt; 0) {\n        int mid = low + k - 1;\n        if (mid &gt;= n) mid = n - 1;\n\n        if (arr[mid] == target)\n            return mid;\n        else if (arr[mid] &lt; target)\n            low = mid + 1;\n\n        k &gt;&gt;= 1; // next smaller offset\n    }\n\n    return (low &lt; n && arr[low] == target) ? low : -1;\n}\n\nint main(void) {\n    int arr[] = {5, 10, 15, 20, 25, 30, 35, 40};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = uniform_binary_search(arr, n, 25);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nAvoids recomputing midpoints\nIdeal for hardware, firmware, microcontrollers\nEnsures consistent runtime path\nFewer instructions → faster in tight loops\n\nUniformity = predictability = performance.\n\n\nA Gentle Proof (Why It Works)\nPrecomputed offsets correspond to halving the search space. At each step, offset = floor(remaining_size / 2). After log₂(n) steps, we narrow down to a single element.\nTotal steps = ⌈log₂ n⌉ Each step = constant cost (no recomputation)\nSo: \\[\nT(n) = O(\\log n)\n\\]\n\n\nTry It Yourself\n\nSearch 25 in [5,10,15,20,25,30,35,40]\nSearch 35\nSearch 6 → not found\nPrecompute offset table for n=8\nCompare steps with binary search\nImplement for n=16\nUse for microcontroller table lookup\nVisualize jumps on paper\nMeasure comparisons\nImplement recursive version\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[5,10,15,20,25,30,35,40]\n25\n4\nFound\n\n\n[5,10,15,20,25,30,35,40]\n5\n0\nFirst element\n\n\n[5,10,15,20,25,30,35,40]\n50\n-1\nNot found\n\n\n[5,10,15,20,25,30,35,40]\n40\n7\nLast element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\nPrerequisite\nSorted array\n\n\n\nUniform Binary Search is binary search with a map, no guesswork, no recalculations, just smooth, evenly spaced jumps to the answer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-17.-interpolation-and-exponential-search",
    "href": "books/en-us/list-2.html#section-17.-interpolation-and-exponential-search",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 17. Interpolation and exponential search",
    "text": "Section 17. Interpolation and exponential search\n\n161 Interpolation Search\nInterpolation Search is a search algorithm for sorted arrays with uniformly distributed values. Unlike Binary Search, which always probes the middle, Interpolation Search estimates the position of the target using value interpolation, like finding where a number lies on a number line.\nIf Binary Search is “divide by index,” Interpolation Search is “divide by value.”\n\nWhat Problem Are We Solving??\nBinary Search assumes no relationship between index and value. But if the array values are uniformly spaced, we can do better by guessing where the target should be, not just the middle.\nIt’s ideal for:\n\nUniformly distributed sorted data\nNumeric keys (IDs, prices, timestamps)\nLarge arrays where value-based positioning matters\n\n\n\nExample\nFind 70 in [10, 20, 30, 40, 50, 60, 70, 80, 90]\nEstimate position:\n\\[\npos = low + \\frac{(target - arr[low]) \\times (high - low)}{arr[high] - arr[low]}\n\\]\n\n\n\nStep\nLow\nHigh\nPos\nValue\nCompare\nAction\n\n\n\n\n1\n0\n8\n7\n80\n80 &gt; 70\nmove left\n\n\n2\n0\n6\n6\n70\n70 = 70\n✅ found\n\n\n\n\n\nHow Does It Work (Plain Language)?\nImagine the array as a number line. If your target is closer to the high end, start searching closer to the right. You interpolate, estimate the target’s index based on its value proportion between min and max.\nSo rather than halving blindly, you jump to the likely spot.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nInitialize low = 0, high = n - 1\n\n\n2\nWhile low &lt;= high and target is in range:\n\n\n  Estimate position using interpolation formula\n\n\n\n3\nCompare arr[pos] with target\n\n\n4\nIf equal → found\n\n\n5\nIf smaller → move low = pos + 1\n\n\n6\nIf larger → move high = pos - 1\n\n\n7\nRepeat until found or low &gt; high\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef interpolation_search(arr, target):\n    low, high = 0, len(arr) - 1\n\n    while low &lt;= high and arr[low] &lt;= target &lt;= arr[high]:\n        if arr[high] == arr[low]:\n            if arr[low] == target:\n                return low\n            break\n\n        pos = low + (target - arr[low]) * (high - low) // (arr[high] - arr[low])\n\n        if arr[pos] == target:\n            return pos\n        elif arr[pos] &lt; target:\n            low = pos + 1\n        else:\n            high = pos - 1\n\n    return -1\n\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90]\nprint(interpolation_search(arr, 70))  # Output: 6\nOutput:\n6\n\n\nC\n#include &lt;stdio.h&gt;\n\nint interpolation_search(int arr[], int n, int target) {\n    int low = 0, high = n - 1;\n\n    while (low &lt;= high && target &gt;= arr[low] && target &lt;= arr[high]) {\n        if (arr[high] == arr[low]) {\n            if (arr[low] == target) return low;\n            else break;\n        }\n\n        int pos = low + (double)(high - low) * (target - arr[low]) / (arr[high] - arr[low]);\n\n        if (arr[pos] == target)\n            return pos;\n        if (arr[pos] &lt; target)\n            low = pos + 1;\n        else\n            high = pos - 1;\n    }\n\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {10, 20, 30, 40, 50, 60, 70, 80, 90};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = interpolation_search(arr, n, 70);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 6\n\n\n\nWhy It Matters\n\nFaster than Binary Search for uniformly distributed data\nIdeal for dense key spaces (like hash slots, ID ranges)\nCan achieve O(log log n) average time\nPreserves sorted order search logic\n\nIt’s the value-aware cousin of Binary Search.\n\n\nA Gentle Proof (Why It Works)\nIf data is uniformly distributed, each probe halves value range logarithmically. Expected probes: \\[\nT(n) = O(\\log \\log n)\n\\] Worst case (non-uniform): \\[\nT(n) = O(n)\n\\]\nSo it outperforms binary search only under uniform value distribution.\n\n\nTry It Yourself\n\nSearch 70 in [10,20,30,40,50,60,70,80,90]\nSearch 25 (not found)\nTry arr = [2,4,8,16,32,64], notice uneven distribution\nPlot estimated positions\nCompare steps with Binary Search\nUse floating-point vs integer division\nImplement recursive version\nSearch 10 (first element)\nSearch 90 (last element)\nMeasure iterations on uniform vs non-uniform data\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[10,20,30,40,50,60,70,80,90]\n70\n6\nFound\n\n\n[10,20,30,40,50,60,70,80,90]\n25\n-1\nNot found\n\n\n[10,20,30,40,50,60,70,80,90]\n10\n0\nFirst\n\n\n[10,20,30,40,50,60,70,80,90]\n90\n8\nLast\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (avg)\nO(log log n)\n\n\nTime (worst)\nO(n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\nPrerequisite\nSorted & uniform array\n\n\n\nInterpolation Search is like a treasure map that scales by value, it doesn’t just guess the middle, it guesses where the gold really lies.\n\n\n\n162 Recursive Interpolation Search\nRecursive Interpolation Search is the recursive variant of the classic Interpolation Search. Instead of looping, it calls itself on smaller subranges, estimating the likely position using the same value-based interpolation formula.\nIt’s a natural way to express the algorithm for learners who think recursively, same logic, cleaner flow.\n\nWhat Problem Are We Solving??\nWe’re taking the iterative interpolation search and expressing it recursively, to highlight the divide-and-conquer nature. The recursive form is often more intuitive and mathematically aligned with its interpolation logic.\nYou’ll use this when:\n\nTeaching or visualizing recursive logic\nWriting clean, declarative search code\nPracticing recursion-to-iteration transitions\n\n\n\nExample\nFind 50 in [10, 20, 30, 40, 50, 60, 70, 80, 90]\nStep 1: low = 0, high = 8 pos = 0 + (50 - 10) × (8 - 0) / (90 - 10) = 4 arr[4] = 50 → ✅ found\nRecursion stops immediately, one call, one success.\n\n\nHow Does It Work (Plain Language)?\nInstead of looping, each recursive call zooms in to the subrange where the target might be. Each call computes an estimated index pos proportional to the target’s distance between arr[low] and arr[high].\nIf value is higher → recurse right If lower → recurse left If equal → return index\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nBase case: if low &gt; high or target out of range → not found\n\n\n2\nCompute position estimate using interpolation formula\n\n\n3\nIf arr[pos] == target → return pos\n\n\n4\nIf arr[pos] &lt; target → recurse on right half\n\n\n5\nElse recurse on left half\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef interpolation_search_recursive(arr, low, high, target):\n    if low &gt; high or target &lt; arr[low] or target &gt; arr[high]:\n        return -1\n\n    if arr[high] == arr[low]:\n        return low if arr[low] == target else -1\n\n    pos = low + (target - arr[low]) * (high - low) // (arr[high] - arr[low])\n\n    if arr[pos] == target:\n        return pos\n    elif arr[pos] &lt; target:\n        return interpolation_search_recursive(arr, pos + 1, high, target)\n    else:\n        return interpolation_search_recursive(arr, low, pos - 1, target)\n\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90]\nprint(interpolation_search_recursive(arr, 0, len(arr) - 1, 50))  # Output: 4\nOutput:\n4\n\n\nC\n#include &lt;stdio.h&gt;\n\nint interpolation_search_recursive(int arr[], int low, int high, int target) {\n    if (low &gt; high || target &lt; arr[low] || target &gt; arr[high])\n        return -1;\n\n    if (arr[high] == arr[low])\n        return (arr[low] == target) ? low : -1;\n\n    int pos = low + (double)(high - low) * (target - arr[low]) / (arr[high] - arr[low]);\n\n    if (arr[pos] == target)\n        return pos;\n    else if (arr[pos] &lt; target)\n        return interpolation_search_recursive(arr, pos + 1, high, target);\n    else\n        return interpolation_search_recursive(arr, low, pos - 1, target);\n}\n\nint main(void) {\n    int arr[] = {10, 20, 30, 40, 50, 60, 70, 80, 90};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = interpolation_search_recursive(arr, 0, n - 1, 50);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nShows the recursive nature of interpolation-based estimation\nClean, mathematical form for teaching and analysis\nDemonstrates recursion depth proportional to search cost\nEasier to reason about with divide-by-value intuition\n\nThink of it as binary search’s artistic cousin, elegant and value-aware.\n\n\nA Gentle Proof (Why It Works)\nIn uniform distributions, each recursive step shrinks the search space multiplicatively, not just by half.\nIf data is uniform: \\[\nT(n) = T(n / f) + O(1)\n\\Rightarrow T(n) = O(\\log \\log n)\n\\]\nIf not uniform: \\[\nT(n) = O(n)\n\\]\nRecursion depth = number of interpolation refinements ≈ log log n.\n\n\nTry It Yourself\n\nSearch 70 in [10,20,30,40,50,60,70,80,90]\nSearch 25 (not found)\nTrace recursive calls manually\nAdd print statements to watch low/high shrink\nCompare depth with binary search\nTry [2,4,8,16,32,64] (non-uniform)\nAdd guard for arr[low] == arr[high]\nImplement tail recursion optimization\nTest base cases (first, last, single element)\nTime comparison: iterative vs recursive\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[10,20,30,40,50,60,70,80,90]\n50\n4\nFound\n\n\n[10,20,30,40,50,60,70,80,90]\n25\n-1\nNot found\n\n\n[10,20,30,40,50,60,70,80,90]\n10\n0\nFirst\n\n\n[10,20,30,40,50,60,70,80,90]\n90\n8\nLast\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (avg)\nO(log log n)\n\n\nTime (worst)\nO(n)\n\n\nSpace\nO(log log n) (recursion)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\nPrerequisite\nSorted & uniform array\n\n\n\nRecursive Interpolation Search is like zooming in with value-based intuition, each step a mathematical guess, each call a sharper focus.\n\n\n\n163 Exponential Search\nExponential Search combines range expansion with binary search. It’s perfect when the array size is unknown or infinite, it first finds the range that might contain the target by doubling the index, then uses binary search within that range.\nIt’s the “zoom out, then zoom in” strategy for searching sorted data.\n\nWhat Problem Are We Solving??\nIf you don’t know the length of your sorted array, you can’t directly apply binary search. You need to first bound the search space, so you know where to look.\nExponential Search does exactly that:\n\nIt doubles the index (1, 2, 4, 8, 16…) until it overshoots.\nThen it performs binary search in that bracket.\n\nUse it for:\n\nInfinite or dynamically sized sorted arrays\nStreams\nUnknown-length files or data structures\n\n\n\nExample\nFind 19 in [2, 4, 8, 16, 19, 23, 42, 64, 128]\n\n\n\nStep\nRange\nValue\nCompare\nAction\n\n\n\n\n1\n1\n4\n4 &lt; 19\ndouble index\n\n\n2\n2\n8\n8 &lt; 19\ndouble index\n\n\n3\n4\n19\n19 = 19\n✅ found\n\n\n\nIf it had overshot (say, target 23), we’d binary search between 4 and 8.\n\n\nHow Does It Work (Plain Language)?\nThink of searching an endless bookshelf. You take steps of size 1, 2, 4, 8… until you pass the book number you want. Now you know which shelf section it’s on, then you check precisely using binary search.\nFast to expand, precise to finish.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart at index 1\n\n\n2\nWhile i &lt; n and arr[i] &lt; target, double i\n\n\n3\nWhen overshoot → binary search between i/2 and min(i, n-1)\n\n\n4\nReturn index if found, else -1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef binary_search(arr, low, high, target):\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n\ndef exponential_search(arr, target):\n    n = len(arr)\n    if n == 0:\n        return -1\n    if arr[0] == target:\n        return 0\n    i = 1\n    while i &lt; n and arr[i] &lt;= target:\n        i *= 2\n    return binary_search(arr, i // 2, min(i, n - 1), target)\n\narr = [2, 4, 8, 16, 19, 23, 42, 64, 128]\nprint(exponential_search(arr, 19))  # Output: 4\nOutput:\n4\n\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search(int arr[], int low, int high, int target) {\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] == target)\n            return mid;\n        else if (arr[mid] &lt; target)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1;\n}\n\nint exponential_search(int arr[], int n, int target) {\n    if (n == 0) return -1;\n    if (arr[0] == target) return 0;\n\n    int i = 1;\n    while (i &lt; n && arr[i] &lt;= target)\n        i *= 2;\n\n    int low = i / 2;\n    int high = (i &lt; n) ? i : n - 1;\n    return binary_search(arr, low, high, target);\n}\n\nint main(void) {\n    int arr[] = {2, 4, 8, 16, 19, 23, 42, 64, 128};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = exponential_search(arr, n, 19);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nHandles unknown size arrays\nLogarithmic search after expansion\nFewer comparisons for small targets\nCommon in unbounded search, streams, linked memory\n\nIt’s the search that grows as needed, like zooming your scope until the target appears.\n\n\nA Gentle Proof (Why It Works)\nEach iteration reduces the search space by a Fibonacci ratio: \\[\nF_k = F_{k-1} + F_{k-2}\n\\]\nHence, search depth ≈ Fibonacci index \\(k \\sim \\log_\\phi n\\),\nwhere \\(\\phi\\) is the golden ratio (\\(\\approx 1.618\\)).\nSo total time: \\[\nT(n) = O(\\log n)\n\\]\n\n\nTry It Yourself\n\nSearch 19 in [2,4,8,16,19,23,42,64,128]\nSearch 42\nSearch 1 (not found)\nTrace expansion: 1, 2, 4, 8…\nCompare expansion steps vs binary search calls\nTry on huge array, small target\nTry on dynamic-size list (simulate infinite)\nImplement recursive version\nMeasure expansions vs comparisons\nCombine with galloping search in TimSort\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[2,4,8,16,19,23,42,64,128]\n19\n4\nFound\n\n\n[2,4,8,16,19,23,42,64,128]\n23\n5\nFound\n\n\n[2,4,8,16,19,23,42,64,128]\n1\n-1\nNot found\n\n\n[2,4,8,16,19,23,42,64,128]\n128\n8\nLast element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log p)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\nPrerequisite\nSorted array\n\n\n\nExponential Search is your wayfinder, it reaches out in powers of two, then narrows in precisely where the target hides.\n\n\n\n164 Doubling Search\nDoubling Search (also called Unbounded Search) is the general pattern behind Exponential Search. It’s used when the data size or range is unknown, and you need to quickly discover a search interval that contains the target before performing a precise search (like binary search) inside that interval.\nThink of it as “search by doubling until you find your bracket.”\n\nWhat Problem Are We Solving??\nIn many real-world scenarios, you don’t know the array’s length or the bounds of your search domain. You can’t jump straight into binary search, you need an upper bound first.\nDoubling Search gives you a strategy to find that bound quickly, in logarithmic time, by doubling your index or step size until you overshoot the target.\nPerfect for:\n\nInfinite or streaming sequences\nFunctions or implicit arrays (not stored in memory)\nSearch domains defined by value, not length\n\n\n\nExample\nFind 23 in [2, 4, 8, 16, 19, 23, 42, 64, 128]\n\n\n\nStep\nRange\nValue\nCompare\nAction\n\n\n\n\n1\ni = 1\n4\n4 &lt; 23\ndouble i\n\n\n2\ni = 2\n8\n8 &lt; 23\ndouble i\n\n\n3\ni = 4\n16\n16 &lt; 23\ndouble i\n\n\n4\ni = 8\n128\n128 &gt; 23\nstop\n\n\n\nRange found: [4, 8] Now run binary search within [16, 19, 23, 42, 64, 128] ✅ Found at index 5\n\n\nHow Does It Work (Plain Language)?\nStart small, test the first few steps. Every time you don’t find your target and the value is still less, double your index (1, 2, 4, 8, 16…). Once you pass your target, you’ve found your interval, then you search precisely.\nIt’s like walking in the dark and doubling your stride each time until you see the light, then stepping back carefully.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart at index 1\n\n\n2\nWhile arr[i] &lt; target, set i = 2 × i\n\n\n3\nWhen overshoot, define range [i/2, min(i, n-1)]\n\n\n4\nApply binary search within range\n\n\n5\nReturn index or -1 if not found\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef doubling_search(arr, target):\n    n = len(arr)\n    if n == 0:\n        return -1\n    if arr[0] == target:\n        return 0\n\n    i = 1\n    while i &lt; n and arr[i] &lt; target:\n        i *= 2\n\n    low = i // 2\n    high = min(i, n - 1)\n\n    # binary search\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n\n    return -1\n\narr = [2, 4, 8, 16, 19, 23, 42, 64, 128]\nprint(doubling_search(arr, 23))  # Output: 5\nOutput:\n5\n\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search(int arr[], int low, int high, int target) {\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] == target)\n            return mid;\n        else if (arr[mid] &lt; target)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1;\n}\n\nint doubling_search(int arr[], int n, int target) {\n    if (n == 0) return -1;\n    if (arr[0] == target) return 0;\n\n    int i = 1;\n    while (i &lt; n && arr[i] &lt; target)\n        i *= 2;\n\n    int low = i / 2;\n    int high = (i &lt; n) ? i : n - 1;\n\n    return binary_search(arr, low, high, target);\n}\n\nint main(void) {\n    int arr[] = {2, 4, 8, 16, 19, 23, 42, 64, 128};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = doubling_search(arr, n, 23);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 5\n\n\n\nWhy It Matters\n\nAllows binary search when size is unknown\nOnly O(log p) probes, where p is target index\nNatural strategy for streaming, infinite, or lazy structures\nUsed in exponential, galloping, and tim-sort merges\n\nIt’s the blueprint for all “expand then search” algorithms.\n\n\nA Gentle Proof (Why It Works)\nEach doubling step multiplies range by 2, so number of expansions ≈ log₂(p) Then binary search inside a range of size ≤ p\nTotal time: \\[\nT(n) = O(\\log p)\n\\]\nStill logarithmic in target position, not array size, efficient even for unbounded data.\n\n\nTry It Yourself\n\nSearch 23 in [2,4,8,16,19,23,42,64,128]\nSearch 42 (more jumps)\nSearch 1 (before first element)\nSearch 128 (last element)\nTry doubling factor 3 instead of 2\nCompare expansion steps with exponential search\nImplement recursive version\nVisualize on a number line\nSimulate unknown-length array with a safe check\nMeasure steps for small vs large targets\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[2,4,8,16,19,23,42,64,128]\n23\n5\nFound\n\n\n[2,4,8,16,19,23,42,64,128]\n4\n1\nEarly\n\n\n[2,4,8,16,19,23,42,64,128]\n1\n-1\nNot found\n\n\n[2,4,8,16,19,23,42,64,128]\n128\n8\nLast element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log p)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\nPrerequisite\nSorted array\n\n\n\nDoubling Search is how you explore the unknown, double your reach, find your bounds, then pinpoint your goal.\n\n\n\n165 Galloping Search\nGalloping Search (also called Exponential Gallop or Search by Doubling) is a hybrid search technique that quickly finds a range by galloping forward exponentially, then switches to binary search within that range.\nIt’s heavily used inside TimSort’s merge step, where it helps merge sorted runs efficiently by skipping over large stretches that don’t need detailed comparison.\n\nWhat Problem Are We Solving??\nWhen merging two sorted arrays (or searching in a sorted list), repeatedly comparing one by one is wasteful if elements differ by a large margin. Galloping Search “jumps ahead” quickly to find the region of interest, then finishes with a binary search to locate the exact boundary.\nUsed in:\n\nTimSort merges\nRun merging in hybrid sorting\nSearching in sorted blocks\nOptimizing comparison-heavy algorithms\n\n\n\nExample\nFind 25 in [5, 10, 15, 20, 25, 30, 35, 40, 45]\n\n\n\nStep\nIndex\nValue\nCompare\nAction\n\n\n\n\n1\n1\n10\n10 &lt; 25\ngallop (×2)\n\n\n2\n2\n15\n15 &lt; 25\ngallop\n\n\n3\n4\n25\n25 = 25\n✅ found\n\n\n\nIf we overshoot (say, search for 22), we’d gallop past it (index 4), then perform binary search between the previous bound and current one.\n\n\nHow Does It Work (Plain Language)?\nYou start by leaping, checking 1, 2, 4, 8 steps away, until you pass the target or reach the end. Once you overshoot, you gallop back (binary search) in the last valid interval.\nIt’s like sprinting ahead to find the neighborhood, then walking house-to-house once you know the block.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nStart at start index\n\n\n2\nGallop forward by powers of 2 (1, 2, 4, 8, ...) until arr[start + k] &gt;= target\n\n\n3\nDefine range [start + k/2, min(start + k, n-1)]\n\n\n4\nApply binary search in that range\n\n\n5\nReturn index if found\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef binary_search(arr, low, high, target):\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n\ndef galloping_search(arr, start, target):\n    n = len(arr)\n    if start &gt;= n:\n        return -1\n    if arr[start] == target:\n        return start\n\n    k = 1\n    while start + k &lt; n and arr[start + k] &lt; target:\n        k *= 2\n\n    low = start + k // 2\n    high = min(start + k, n - 1)\n    return binary_search(arr, low, high, target)\n\narr = [5, 10, 15, 20, 25, 30, 35, 40, 45]\nprint(galloping_search(arr, 0, 25))  # Output: 4\nOutput:\n4\n\n\nC\n#include &lt;stdio.h&gt;\n\nint binary_search(int arr[], int low, int high, int target) {\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] == target)\n            return mid;\n        else if (arr[mid] &lt; target)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1;\n}\n\nint galloping_search(int arr[], int n, int start, int target) {\n    if (start &gt;= n) return -1;\n    if (arr[start] == target) return start;\n\n    int k = 1;\n    while (start + k &lt; n && arr[start + k] &lt; target)\n        k *= 2;\n\n    int low = start + k / 2;\n    int high = (start + k &lt; n) ? start + k : n - 1;\n\n    return binary_search(arr, low, high, target);\n}\n\nint main(void) {\n    int arr[] = {5, 10, 15, 20, 25, 30, 35, 40, 45};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = galloping_search(arr, n, 0, 25);\n    printf(\"Found at index: %d\\n\", idx);\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nAccelerates merging in TimSort\nMinimizes comparisons when merging large sorted runs\nGreat for adaptive sorting and range scans\nBalances speed (gallop) and precision (binary)\n\nIt’s a dynamic balance, gallop when far, tiptoe when close.\n\n\nA Gentle Proof (Why It Works)\nGalloping phase:\nTakes \\(O(\\log p)\\) steps to reach the target vicinity (where \\(p\\) is the distance from the start).\nBinary phase:\nAnother \\(O(\\log p)\\) for local search.\nTotal: \\[\nT(n) = O(\\log p)\n\\]\nFaster than linear merging when runs differ greatly in length.\n\n\nTry It Yourself\n\nSearch 25 in [5,10,15,20,25,30,35,40,45]\nSearch 40 (larger gallop)\nSearch 3 (before first)\nTry start=2\nPrint gallop steps\nCompare with pure binary search\nImplement recursive gallop\nUse for merging two sorted arrays\nCount comparisons per search\nTest on long list with early/late targets\n\n\n\nTest Cases\n\n\n\nInput\nTarget\nOutput\nNotes\n\n\n\n\n[5,10,15,20,25,30,35,40,45]\n25\n4\nFound\n\n\n[5,10,15,20,25,30,35,40,45]\n40\n7\nLarger gallop\n\n\n[5,10,15,20,25,30,35,40,45]\n3\n-1\nNot found\n\n\n[5,10,15,20,25,30,35,40,45]\n5\n0\nFirst element\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log p)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\nPrerequisite\nSorted array\n\n\n\nGalloping Search is how TimSort runs ahead with confidence, sprint through big gaps, slow down only when precision counts.\n\n\n\n166 Unbounded Binary Search\nUnbounded Binary Search is a technique for finding a value in a sorted but unbounded (or infinite) sequence. You don’t know where the data ends, or even how large it is, but you know it’s sorted. So first, you find a search boundary, then perform a binary search within that discovered range.\nIt’s a direct application of doubling search followed by binary search, especially suited for monotonic functions or streams.\n\nWhat Problem Are We Solving??\nIf you’re working with data that:\n\nHas no fixed size,\nComes as a stream,\nOr is represented as a monotonic function (like f(x) increasing with x),\n\nthen you can’t apply binary search immediately, because you don’t know high.\nSo you first need to find bounds [low, high] that contain the target, by expanding exponentially, and only then use binary search.\n\n\nExample\nSuppose you’re searching for 20 in a monotonic function:\nf(x) = 2x + 2\nYou want the smallest x such that f(x) ≥ 20.\nStep 1: Find bounds. Check x = 1 → f(1) = 4 Check x = 2 → f(2) = 6 Check x = 4 → f(4) = 10 Check x = 8 → f(8) = 18 Check x = 16 → f(16) = 34 (overshoot!)\nNow you know target lies between x = 8 and x = 16.\nStep 2: Perform binary search in [8, 16]. ✅ Found f(9) = 20\n\n\nHow Does It Work (Plain Language)?\nYou start with a small step and double your reach each time until you go beyond the target. Once you’ve overshot, you know the search interval, and you can binary search inside it for precision.\nIt’s the go-to strategy when the array (or domain) has no clear end.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nInitialize low = 0, high = 1\n\n\n2\nWhile f(high) &lt; target, set low = high, high *= 2\n\n\n3\nOnce f(high) &gt;= target, binary search between [low, high]\n\n\n4\nReturn index (or value)\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef unbounded_binary_search(f, target):\n    low, high = 0, 1\n    # Step 1: find bounds\n    while f(high) &lt; target:\n        low = high\n        high *= 2\n\n    # Step 2: binary search in [low, high]\n    while low &lt;= high:\n        mid = (low + high) // 2\n        val = f(mid)\n        if val == target:\n            return mid\n        elif val &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n\n# Example: f(x) = 2x + 2\nf = lambda x: 2 * x + 2\nprint(unbounded_binary_search(f, 20))  # Output: 9\nOutput:\n9\n\n\nC\n#include &lt;stdio.h&gt;\n\nint f(int x) {\n    return 2 * x + 2;\n}\n\nint binary_search(int (*f)(int), int low, int high, int target) {\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        int val = f(mid);\n        if (val == target)\n            return mid;\n        else if (val &lt; target)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1;\n}\n\nint unbounded_binary_search(int (*f)(int), int target) {\n    int low = 0, high = 1;\n    while (f(high) &lt; target) {\n        low = high;\n        high *= 2;\n    }\n    return binary_search(f, low, high, target);\n}\n\nint main(void) {\n    int idx = unbounded_binary_search(f, 20);\n    printf(\"Found at x = %d\\n\", idx);\n}\nOutput:\nFound at x = 9\n\n\n\nWhy It Matters\n\nHandles infinite sequences or unbounded domains\nPerfect for monotonic functions (e.g. f(x) increasing)\nKey in searching without array length\nUsed in root finding, binary lifting, streaming\n\nIt’s the “expand, then refine” pattern, the explorer’s search.\n\n\nA Gentle Proof (Why It Works)\nExpanding step: doubles index each time → \\(O(\\log p)\\), where p is the position of the target.\nBinary search step: searches within a range of size ≤ p → another \\(O(\\log p)\\).\nTotal complexity: \\[\nT(n) = O(\\log p)\n\\]\nIndependent of total domain size.\n\n\nTry It Yourself\n\nf(x) = 2x + 2, target = 20 → 9\nf(x) = x², target = 64 → 8\nSearch for non-existing value (e.g. 7 in even series)\nModify f(x) to be exponential\nPrint bounds before binary search\nTry with negative domain (guard with if)\nApply to sorted infinite list (simulate with f)\nUse float function and tolerance check\nCompare with linear probing\nUse to find smallest x where f(x) ≥ target\n\n\n\nTest Cases\n\n\n\nFunction\nTarget\nOutput\nNotes\n\n\n\n\n2x+2\n20\n9\nf(9)=20\n\n\nx²\n64\n8\nf(8)=64\n\n\n2x+2\n7\n-1\nNot found\n\n\n3x+1\n31\n10\nf(10)=31\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log p)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\nAdaptive\nYes\n\n\nPrerequisite\nMonotonic function or sorted data\n\n\n\nUnbounded Binary Search is how you search the infinite, double your bounds, then zoom in on the truth.\n\n\n\n167 Root-Finding Bisection\nRoot-Finding Bisection is a numerical search algorithm for locating the point where a continuous function crosses zero. It repeatedly halves an interval where the sign of the function changes, guaranteeing that a root exists within that range.\nIt’s the simplest, most reliable method for solving equations like f(x) = 0 when you only know that a solution exists somewhere between a and b.\n\nWhat Problem Are We Solving??\nWhen you have a function \\(f(x)\\) and want to solve \\(f(x) = 0\\),\nbut you can’t solve it algebraically, you can use the Bisection Method.\nIf \\(f(a)\\) and \\(f(b)\\) have opposite signs, then by the Intermediate Value Theorem,\nthere is at least one root between \\(a\\) and \\(b\\).\nExample:\nFind the root of \\(f(x) = x^3 - x - 2\\) in \\([1, 2]\\).\n\n\\(f(1) = -2\\)\n\n\\(f(2) = 4\\)\nSigns differ → there’s a root in \\([1, 2]\\).\n\n\n\nHow Does It Work (Plain Language)?\nYou start with an interval \\([a, b]\\) where \\(f(a)\\) and \\(f(b)\\) have opposite signs.\nThen, at each step:\n\nFind midpoint \\(m = \\frac{a + b}{2}\\)\n\nEvaluate \\(f(m)\\)\n\nReplace either a or b so that signs at the ends still differ\n\nRepeat until the interval is small enough\n\nYou’re squeezing the interval tighter and tighter until it hugs the root.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n\n\n\n\n1\nChoose a, b with f(a) and f(b) of opposite signs\n\n\n\n\n\n\n2\nCompute midpoint m = (a + b) / 2\n\n\n\n\n\n\n3\nEvaluate f(m)\n\n\n\n\n\n\n4\nIf f(m) has same sign as f(a), set a = m, else b = m\n\n\n\n\n\n\n5\nRepeat until | f(m) |or| b - a | &lt; tolerance\n\n\n\n\n\n\n6\nReturn m as root\n\n\n\n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef f(x):\n    return x3 - x - 2\n\ndef bisection(f, a, b, tol=1e-6):\n    if f(a) * f(b) &gt;= 0:\n        raise ValueError(\"No sign change: root not guaranteed\")\n    while (b - a) / 2 &gt; tol:\n        m = (a + b) / 2\n        if f(m) == 0:\n            return m\n        elif f(a) * f(m) &lt; 0:\n            b = m\n        else:\n            a = m\n    return (a + b) / 2\n\nroot = bisection(f, 1, 2)\nprint(\"Approx root:\", root)\nOutput:\nApprox root: 1.52138\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) {\n    return x*x*x - x - 2;\n}\n\ndouble bisection(double (*f)(double), double a, double b, double tol) {\n    if (f(a) * f(b) &gt;= 0) {\n        printf(\"No sign change. Root not guaranteed.\\n\");\n        return NAN;\n    }\n    double m;\n    while ((b - a) / 2 &gt; tol) {\n        m = (a + b) / 2;\n        double fm = f(m);\n        if (fm == 0)\n            return m;\n        if (f(a) * fm &lt; 0)\n            b = m;\n        else\n            a = m;\n    }\n    return (a + b) / 2;\n}\n\nint main(void) {\n    double root = bisection(f, 1, 2, 1e-6);\n    printf(\"Approx root: %.6f\\n\", root);\n}\nOutput:\nApprox root: 1.521380\n\n\n\nWhy It Matters\n\nGuaranteed convergence if ( f ) is continuous and signs differ\nSimple and robust\nWorks for nonlinear equations\nGreat starting point before more advanced methods (Newton, Secant)\nUsed in physics, finance, engineering for precise solving\n\n\n\nA Gentle Proof (Why It Works)\nBy the Intermediate Value Theorem:\nIf \\(f(a) \\cdot f(b) &lt; 0\\), then there exists \\(c \\in [a, b]\\) such that \\(f(c) = 0\\).\nEach iteration halves the interval size, so after \\(k\\) steps:\n\\[\nb_k - a_k = \\frac{b_0 - a_0}{2^k}\n\\]\nTo achieve tolerance \\(\\varepsilon\\):\n\\[\nk \\approx \\log_2\\left(\\frac{b_0 - a_0}{\\varepsilon}\\right)\n\\]\nThus, convergence is linear, but guaranteed.\n\n\nTry It Yourself\n\n\\(f(x) = x^2 - 2\\), interval \\([1, 2]\\) → \\(\\sqrt{2}\\)\n\n\\(f(x) = \\cos x - x\\), interval \\([0, 1]\\)\n\n\\(f(x) = x^3 - 7\\), interval \\([1, 3]\\)\n\nTry tighter tolerances (1e-3, 1e-9)\nCount how many iterations needed\nPrint each midpoint\nCompare with Newton’s method\nPlot convergence curve\nModify to return both root and iterations\nTest on function with multiple roots\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\nRoot (Approx)\nNotes\n\n\n\n\n\\(x^2 - 2\\)\n\\([1, 2]\\)\n1.4142\n\\(\\sqrt{2}\\)\n\n\n\\(x^3 - x - 2\\)\n\\([1, 2]\\)\n1.5214\nClassic example\n\n\n\\(\\cos x - x\\)\n\\([0, 1]\\)\n0.7391\nFixed point root\n\n\n\\(x^3 - 7\\)\n\\([1, 3]\\)\n1.913\nCube root of 7\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log((b−a)/tol))\n\n\nSpace\nO(1)\n\n\nConvergence\nLinear\n\n\nStability\nHigh\n\n\nRequires continuity\nYes\n\n\n\nBisection Method is your steady compass in numerical analysis, it never fails when a root is truly there.\n\n\n\n168 Golden Section Search\nGolden Section Search is a clever optimization algorithm for finding the minimum (or maximum) of a unimodal function on a closed interval ([a, b])—without derivatives. It’s a close cousin of binary search, but instead of splitting in half, it uses the golden ratio to minimize function evaluations.\n\nWhat Problem Are We Solving??\nYou want to find the x that minimizes f(x) on an interval ([a, b]), but you can’t or don’t want to take derivatives (maybe f is noisy or discontinuous).\nIf f(x) is unimodal (has a single peak or trough), then the Golden Section Search gives you a guaranteed narrowing path to the optimum.\n\n\nExample\nFind the minimum of \\[\nf(x) = (x-2)^2 + 1\n\\] on ([0, 5])\nSince (f(x)) is quadratic, its minimum is at (x = 2). The algorithm will zoom in around (2) by evaluating at golden-ratio points.\n\n\nHow Does It Work (Plain Language)?\nImagine slicing your search interval using the golden ratio (( )). By placing test points at those ratios, you can reuse past evaluations and eliminate one side of the interval every step.\nEach iteration shrinks the search range while keeping the ratio intact — like a mathematically perfect zoom.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCondition\nResult / Note\n\n\n\n\n1\nChoose initial interval \\([a, b]\\)\n\n\n\n\n2\nCompute golden ratio \\(\\phi = \\frac{\\sqrt{5} - 1}{2} \\approx 0.618\\)\n\n\n\n\n3\nSet \\(c = b - \\phi(b - a)\\), \\(d = a + \\phi(b - a)\\)\n\n\n\n\n4\nEvaluate \\(f(c)\\) and \\(f(d)\\)\n\n\n\n\n5\nIf \\(f(c) &lt; f(d)\\), new interval = \\([a, d]\\)\n\n\n\n\n6\nElse, new interval = \\([c, b]\\)\n\n\n\n\n7\nRepeat until \\(b - a &lt; \\text{tolerance}\\)\n\n\n\n\n8\nReturn midpoint as best estimate\n\n\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport math\n\ndef f(x):\n    return (x - 2)2 + 1\n\ndef golden_section_search(f, a, b, tol=1e-5):\n    phi = (math.sqrt(5) - 1) / 2\n    c = b - phi * (b - a)\n    d = a + phi * (b - a)\n    fc, fd = f(c), f(d)\n    \n    while abs(b - a) &gt; tol:\n        if fc &lt; fd:\n            b, d, fd = d, c, fc\n            c = b - phi * (b - a)\n            fc = f(c)\n        else:\n            a, c, fc = c, d, fd\n            d = a + phi * (b - a)\n            fd = f(d)\n    return (b + a) / 2\n\nroot = golden_section_search(f, 0, 5)\nprint(\"Minimum near x =\", root)\nOutput:\nMinimum near x = 2.0000\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) {\n    return (x - 2)*(x - 2) + 1;\n}\n\ndouble golden_section_search(double (*f)(double), double a, double b, double tol) {\n    double phi = (sqrt(5.0) - 1) / 2;\n    double c = b - phi * (b - a);\n    double d = a + phi * (b - a);\n    double fc = f(c), fd = f(d);\n\n    while (fabs(b - a) &gt; tol) {\n        if (fc &lt; fd) {\n            b = d;\n            d = c;\n            fd = fc;\n            c = b - phi * (b - a);\n            fc = f(c);\n        } else {\n            a = c;\n            c = d;\n            fc = fd;\n            d = a + phi * (b - a);\n            fd = f(d);\n        }\n    }\n    return (b + a) / 2;\n}\n\nint main(void) {\n    double x = golden_section_search(f, 0, 5, 1e-5);\n    printf(\"Minimum near x = %.5f\\n\", x);\n}\nOutput:\nMinimum near x = 2.00000\n\n\n\nWhy It Matters\n\nNo derivatives required\nFewer evaluations than simple binary search\nGuaranteed convergence for unimodal functions\nUsed in numerical optimization, tuning, engineering design, hyperparameter search\nThe golden ratio ensures optimal reuse of computed points\n\n\n\nA Gentle Proof (Why It Works)\nAt each step, the interval length is multiplied by \\(\\phi \\approx 0.618\\).\nSo after \\(k\\) steps: \\[\nL_k = (b_0 - a_0) \\times \\phi^k\n\\]\nTo reach tolerance \\(\\varepsilon\\): \\[\nk = \\frac{\\log(\\varepsilon / (b_0 - a_0))}{\\log(\\phi)}\n\\]\nThus, convergence is linear but efficient, and each iteration needs only one new evaluation.\n\n\nTry It Yourself\n\n\\(f(x) = (x - 3)^2\\), interval \\([0, 6]\\)\n\n\\(f(x) = x^4 - 10x^2 + 9\\), interval \\([-5, 5]\\)\n\n\\(f(x) = |x - 1|\\), interval \\([-2, 4]\\)\n\nTry changing tolerance to 1e-3, 1e-9\nTrack number of iterations\nPlot search intervals\nSwitch to maximizing (compare signs)\nTest non-unimodal function (observe failure)\nModify to return f(x*) as well\nCompare with ternary search\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\nMinimum \\(x\\)\n\\(f(x)\\)\n\n\n\n\n\\((x - 2)^2 + 1\\)\n\\([0, 5]\\)\n2.0000\n1.0000\n\n\n\\((x - 3)^2\\)\n\\([0, 6]\\)\n3.0000\n0.0000\n\n\n\\(x^2\\)\n\\([-3, 2]\\)\n0.0000\n0.0000\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log((b−a)/tol))\n\n\nSpace\nO(1)\n\n\nEvaluations per step\n1 new point\n\n\nConvergence\nLinear\n\n\nRequires unimodality\nYes\n\n\n\nGolden Section Search is optimization’s quiet craftsman, balancing precision and simplicity with the elegance of φ.\n\n\n\n169 Fibonacci Search (Optimum)\nFibonacci Search is a divide-and-conquer algorithm that uses the Fibonacci sequence to determine probe positions when searching for a target in a sorted array. It’s similar to binary search but uses Fibonacci numbers instead of halving intervals, which makes it ideal for sequential access systems (like tapes or large memory arrays).\nIt shines where comparison count matters or when random access is expensive.\n\nWhat Problem Are We Solving??\nYou want to search for an element in a sorted array efficiently, but instead of halving the interval (as in binary search), you want to use Fibonacci numbers to decide where to look — minimizing comparisons and taking advantage of arithmetic-friendly jumps.\n\n\nExample\nLet’s search for x = 55 in:\narr = [10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100]\n\nFind smallest Fibonacci number ≥ length (11) → F(7) = 13\nUse Fibonacci offsets to decide index jumps.\nCheck mid using fibM2 = 5 (F(5)=5 → index 4): arr[4] = 45 &lt; 55\nMove window and repeat until found or narrowed.\n\n\n\nHow Does It Work (Plain Language)?\nInstead of splitting in half like binary search, it splits using ratios of Fibonacci numbers, keeping subarray sizes close to golden ratio.\nThis approach reduces comparisons and works especially well when array size is known and access cost is linear or limited.\nThink of it as a mathematically balanced jump search guided by Fibonacci spacing.\n\n\nStep-by-Step Process\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nCompute smallest Fibonacci number Fm ≥ n\n\n\n2\nSet offsets: Fm1 = Fm-1, Fm2 = Fm-2\n\n\n3\nCompare target with arr[offset + Fm2]\n\n\n4\nIf smaller, search left subarray (shift Fm1, Fm2)\n\n\n5\nIf larger, search right subarray (update offset, shift Fm1, Fm2)\n\n\n6\nIf equal, return index\n\n\n7\nContinue until Fm1 = 1\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef fibonacci_search(arr, x):\n    n = len(arr)\n    \n    # Initialize fibonacci numbers\n    fibMMm2 = 0  # F(m-2)\n    fibMMm1 = 1  # F(m-1)\n    fibM = fibMMm2 + fibMMm1  # F(m)\n\n    # Find smallest Fibonacci &gt;= n\n    while fibM &lt; n:\n        fibMMm2 = fibMMm1\n        fibMMm1 = fibM\n        fibM = fibMMm2 + fibMMm1\n\n    offset = -1\n\n    while fibM &gt; 1:\n        i = min(offset + fibMMm2, n - 1)\n\n        if arr[i] &lt; x:\n            fibM = fibMMm1\n            fibMMm1 = fibMMm2\n            fibMMm2 = fibM - fibMMm1\n            offset = i\n        elif arr[i] &gt; x:\n            fibM = fibMMm2\n            fibMMm1 -= fibMMm2\n            fibMMm2 = fibM - fibMMm1\n        else:\n            return i\n\n    if fibMMm1 and offset + 1 &lt; n and arr[offset + 1] == x:\n        return offset + 1\n\n    return -1\n\narr = [10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100]\nprint(\"Found at index:\", fibonacci_search(arr, 55))  # Output: -1\nprint(\"Found at index:\", fibonacci_search(arr, 85))  # Output: 8\nOutput:\nFound at index: -1\nFound at index: 8\n\n\nC\n#include &lt;stdio.h&gt;\n\nint min(int a, int b) { return (a &lt; b) ? a : b; }\n\nint fibonacci_search(int arr[], int n, int x) {\n    int fibMMm2 = 0;   // F(m-2)\n    int fibMMm1 = 1;   // F(m-1)\n    int fibM = fibMMm2 + fibMMm1;  // F(m)\n\n    while (fibM &lt; n) {\n        fibMMm2 = fibMMm1;\n        fibMMm1 = fibM;\n        fibM = fibMMm2 + fibMMm1;\n    }\n\n    int offset = -1;\n\n    while (fibM &gt; 1) {\n        int i = min(offset + fibMMm2, n - 1);\n\n        if (arr[i] &lt; x) {\n            fibM = fibMMm1;\n            fibMMm1 = fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n            offset = i;\n        } else if (arr[i] &gt; x) {\n            fibM = fibMMm2;\n            fibMMm1 -= fibMMm2;\n            fibMMm2 = fibM - fibMMm1;\n        } else {\n            return i;\n        }\n    }\n\n    if (fibMMm1 && offset + 1 &lt; n && arr[offset + 1] == x)\n        return offset + 1;\n\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    int x = 85;\n    int idx = fibonacci_search(arr, n, x);\n    if (idx != -1)\n        printf(\"Found at index: %d\\n\", idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound at index: 8\n\n\n\nWhy It Matters\n\nUses Fibonacci numbers to reduce comparisons\nEfficient for sorted arrays with sequential access\nAvoids division (only addition/subtraction)\nInspired by golden ratio search (optimal probing)\nExcellent teaching tool for divide-and-conquer logic\n\n\n\nA Gentle Proof (Why It Works)\nThe Fibonacci split maintains nearly golden ratio balance.\nAt each step, one subproblem has size ≈ \\(\\frac{1}{\\phi}\\) of the previous.\nSo total steps ≈ number of Fibonacci numbers \\(\\le n\\),\nwhich grows as \\(O(\\log_\\phi n) \\approx O(\\log n)\\).\nThus, the time complexity is the same as binary search,\nbut with fewer comparisons and more efficient arithmetic.\n\n\nTry It Yourself\n\nSearch 45 in [10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100]\nSearch 100 (last element)\nSearch 10 (first element)\nSearch value not in array\nCount comparisons made\nCompare with binary search\nTry on length = Fibonacci number (e.g. 13)\nVisualize index jumps\nModify to print intervals\nApply to sorted strings (lex order)\n\n\n\nTest Cases\n\n\n\nArray\nTarget\nOutput\nNotes\n\n\n\n\n[10,22,35,40,45,50,80,82,85,90,100]\n85\n8\nFound\n\n\n[10,22,35,40,45]\n22\n1\nFound\n\n\n[1,2,3,5,8,13,21]\n21\n6\nFound\n\n\n[2,4,6,8,10]\n7\n-1\nNot found\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(log n)\n\n\nSpace\nO(1)\n\n\nComparisons\n≤ log₍φ₎(n)\n\n\nAccess type\nSequential-friendly\n\n\nRequires sorted input\nYes\n\n\n\nFibonacci Search, a golden search for discrete worlds, where each step follows nature’s rhythm.\n\n\n\n170 Jump + Binary Hybrid\nJump + Binary Hybrid Search blends the best of two worlds, Jump Search for fast skipping and Binary Search for precise refinement. It’s perfect when your data is sorted, and you want a balance between linear jumps and logarithmic probing within small subranges.\n\nWhat Problem Are We Solving??\nBinary search is powerful but needs random access (you can jump anywhere). Jump search works well for sequential data (like linked blocks or caches) but may overshoot.\nThis hybrid combines them:\n\nJump ahead in fixed steps to find the block.\nOnce you know the target range, switch to binary search inside it.\n\nIt’s a practical approach for sorted datasets with limited random access (like disk blocks or database pages).\n\n\nExample\nSearch for 45 in [10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100].\nStep 1: Jump by block size √n = 3\n\nCheck arr[2] = 35 → 35 &lt; 45\nCheck arr[5] = 50 → 50 &gt; 45\n\nNow we know target is in [35, 40, 45, 50) → indices [3..5)\nStep 2: Binary search within block [3..5)\n\nMid = 4 → arr[4] = 45 ✅ Found!\n\n\n\nHow Does It Work (Plain Language)?\n\nChoose block size \\(m = \\sqrt{n}\\).\n\nJump ahead by \\(m\\) until you pass or reach the target.\n\nOnce in the block, switch to binary search inside.\n\nJumping narrows the search zone quickly, Binary search finishes it cleanly, fewer comparisons than either alone.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nCompute block size \\(m = \\lfloor \\sqrt{n} \\rfloor\\)\n\n\n2\nJump by \\(m\\) until arr[j] ≥ target or end\n\n\n3\nDetermine block \\([j - m, j)\\)\n\n\n4\nRun binary search inside that block\n\n\n5\nReturn index or -1 if not found\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport math\n\ndef jump_binary_search(arr, x):\n    n = len(arr)\n    step = int(math.sqrt(n))\n    prev = 0\n\n    # Jump phase\n    while prev &lt; n and arr[min(step, n) - 1] &lt; x:\n        prev = step\n        step += int(math.sqrt(n))\n        if prev &gt;= n:\n            return -1\n\n    # Binary search in block\n    low, high = prev, min(step, n) - 1\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if arr[mid] == x:\n            return mid\n        elif arr[mid] &lt; x:\n            low = mid + 1\n        else:\n            high = mid - 1\n\n    return -1\n\narr = [10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100]\nprint(\"Found at index:\", jump_binary_search(arr, 45))\nOutput:\nFound at index: 4\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nint jump_binary_search(int arr[], int n, int x) {\n    int step = (int)sqrt(n);\n    int prev = 0;\n\n    // Jump phase\n    while (prev &lt; n && arr[(step &lt; n ? step : n) - 1] &lt; x) {\n        prev = step;\n        step += (int)sqrt(n);\n        if (prev &gt;= n)\n            return -1;\n    }\n\n    // Binary search in block\n    int low = prev;\n    int high = (step &lt; n ? step : n) - 1;\n\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] == x)\n            return mid;\n        else if (arr[mid] &lt; x)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {10, 22, 35, 40, 45, 50, 80, 82, 85, 90, 100};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int idx = jump_binary_search(arr, n, 45);\n    if (idx != -1)\n        printf(\"Found at index: %d\\n\", idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound at index: 4\n\n\n\nWhy It Matters\n\nCombines fast skipping (jump) and efficient narrowing (binary)\nWorks well on sorted lists with slow access\nReduces comparisons vs pure jump or linear\nGreat for block-based storage and database indexing\nDemonstrates hybrid thinking in algorithm design\n\n\n\nA Gentle Proof (Why It Works)\nJump phase: \\(O(\\sqrt{n})\\) steps\nBinary phase: \\(O(\\log \\sqrt{n}) = O(\\log n)\\)\nTotal: \\[\nT(n) = O(\\sqrt{n}) + O(\\log n)\n\\]\nFor large \\(n\\), dominated by \\(O(\\sqrt{n})\\), but faster in practice.\n\n\nTry It Yourself\n\nSearch 80 in [10,22,35,40,45,50,80,82,85,90,100]\nTry 10 (first element)\nTry 100 (last element)\nTry value not in array\nCompare comparisons with binary search\nChange block size (try 2√n or n/4)\nPrint jumps and block\nRun on array of length 100\nCombine with exponential block discovery\nExtend for descending arrays\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nArray\nTarget\nOutput\nNotes\n\n\n\n\n[10,22,35,40,45,50,80,82,85,90,100]\n45\n4\nFound\n\n\n[10,22,35,40,45]\n10\n0\nFound at start\n\n\n[10,22,35,40,45]\n100\n-1\nNot found\n\n\n[1,3,5,7,9,11,13]\n7\n3\nFound in middle\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(√n + log n)\n\n\nSpace\nO(1)\n\n\nRequires sorted\nYes\n\n\nStable\nYes\n\n\nAdaptive\nNo\n\n\n\nJump + Binary Hybrid, leap with purpose, then zero in. It’s how explorers search with both speed and focus.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-18.-selection-algorithms",
    "href": "books/en-us/list-2.html#section-18.-selection-algorithms",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 18. Selection Algorithms",
    "text": "Section 18. Selection Algorithms\n\n171 Quickselect\nQuickselect is a selection algorithm to find the k-th smallest element in an unsorted array — faster on average than sorting the entire array.\nIt’s based on the same partitioning idea as Quicksort, but only recurses into the side that contains the desired element.\nAverage time complexity: O(n) Worst-case (rare): O(n²)\n\nWhat Problem Are We Solving??\nSuppose you have an unsorted list and you want:\n\nThe median,\nThe k-th smallest, or\nThe k-th largest element,\n\nYou don’t need to fully sort, you just need one order statistic.\nQuickselect solves this by partitioning the array and narrowing focus to the relevant half only.\n\n\nExample\nFind 4th smallest element in: [7, 2, 1, 6, 8, 5, 3, 4]\n\nChoose pivot (e.g. 4).\nPartition → [2, 1, 3] [4] [7, 6, 8, 5]\nPivot position = 3 (0-based)\nk = 4 → pivot index 3 matches → 4 is 4th smallest ✅\n\nNo need to sort the rest!\n\n\nHow Does It Work (Plain Language)?\nQuickselect picks a pivot, partitions the list into less-than and greater-than parts, and decides which side to recurse into based on the pivot’s index vs. target k.\nIt’s a divide and conquer search on positions, not order.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nPick pivot (random or last element)\n\n\n2\nPartition array around pivot\n\n\n3\nGet pivot index p\n\n\n4\nIf p == k, return element\n\n\n5\nIf p &gt; k, search left\n\n\n6\nIf p &lt; k, search right (adjust k)\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport random\n\ndef partition(arr, low, high):\n    pivot = arr[high]\n    i = low\n    for j in range(low, high):\n        if arr[j] &lt; pivot:\n            arr[i], arr[j] = arr[j], arr[i]\n            i += 1\n    arr[i], arr[high] = arr[high], arr[i]\n    return i\n\ndef quickselect(arr, k):\n    low, high = 0, len(arr) - 1\n    while low &lt;= high:\n        pivot_index = partition(arr, low, high)\n        if pivot_index == k:\n            return arr[pivot_index]\n        elif pivot_index &gt; k:\n            high = pivot_index - 1\n        else:\n            low = pivot_index + 1\n\narr = [7, 2, 1, 6, 8, 5, 3, 4]\nk = 3  # 0-based index: 4th smallest\nprint(\"4th smallest:\", quickselect(arr, k))\nOutput:\n4th smallest: 4\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int t = *a;\n    *a = *b;\n    *b = t;\n}\n\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            swap(&arr[i], &arr[j]);\n            i++;\n        }\n    }\n    swap(&arr[i], &arr[high]);\n    return i;\n}\n\nint quickselect(int arr[], int low, int high, int k) {\n    if (low &lt;= high) {\n        int pi = partition(arr, low, high);\n        if (pi == k)\n            return arr[pi];\n        else if (pi &gt; k)\n            return quickselect(arr, low, pi - 1, k);\n        else\n            return quickselect(arr, pi + 1, high, k);\n    }\n    return -1;\n}\n\nint main(void) {\n    int arr[] = {7, 2, 1, 6, 8, 5, 3, 4};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    int k = 3;  // 4th smallest (0-based)\n    printf(\"4th smallest: %d\\n\", quickselect(arr, 0, n - 1, k));\n}\nOutput:\n4th smallest: 4\n\n\n\nWhy It Matters\n\nFind median in linear time (expected)\nAvoid sorting when you only need one element\nBasis for algorithms like Median of Medians, BFPRT\nCommon in order statistics, quantiles, top-k problems\nUsed in libraries (e.g. nth_element in C++)\n\n\n\nA Gentle Proof (Why It Works)\nEach partition reduces problem size by eliminating one side. Average split ≈ half → O(n) expected comparisons. Worst case (bad pivot) → O(n²), but with randomized pivot, very unlikely.\nExpected time: \\[\nT(n) = T(n/2) + O(n) \\Rightarrow O(n)\n\\]\n\n\nTry It Yourself\n\nFind 1st smallest (min)\nFind last (max)\nFind median (k = n/2)\nAdd random pivoting\nCount comparisons per iteration\nModify for k-th largest (n-k)\nCompare runtime with full sort\nVisualize partition steps\nTest on repeated elements\nCombine with deterministic pivot\n\n\n\nTest Cases\n\n\n\nArray\nk (0-based)\nOutput\nNotes\n\n\n\n\n[7,2,1,6,8,5,3,4]\n3\n4\n4th smallest\n\n\n[3,1,2]\n1\n2\nmedian\n\n\n[10,80,30,90,40,50,70]\n4\n70\nmiddle element\n\n\n[5,5,5,5]\n2\n5\nduplicates\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Average)\nO(n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nIn-place\nYes\n\n\nRandomized\nRecommended\n\n\n\nQuickselect, the surgical strike of sorting: find exactly what you need, and ignore the rest.\n\n\n\n172 Median of Medians\nMedian of Medians is a deterministic selection algorithm that guarantees O(n) worst-case time for finding the k-th smallest element. It improves on Quickselect, which can degrade to (O(n^2)) in unlucky cases, by carefully choosing a good pivot every time.\nIt’s a cornerstone of theoretical computer science, balancing speed and worst-case safety.\n\nWhat Problem Are We Solving??\nIn Quickselect, a bad pivot can lead to unbalanced partitions (like always picking smallest/largest). Median of Medians fixes this by ensuring the pivot is “good enough” — always splitting the array so that each side has at least a constant fraction of elements.\nGoal: Find the k-th smallest element deterministically in O(n), no randomness, no risk.\n\n\nExample\nFind 5th smallest in [12, 3, 5, 7, 4, 19, 26, 23, 8, 15]\n\nSplit into groups of 5: [12, 3, 5, 7, 4], [19, 26, 23, 8, 15]\nSort each group: [3,4,5,7,12], [8,15,19,23,26]\nTake medians: [5, 19]\nFind median of medians: 19\nPartition array around 19\nRecurse on appropriate side until k-th found\n\nPivot 19 ensures balanced split, leading to linear runtime.\n\n\nHow Does It Work (Plain Language)?\n\nBreak array into groups of 5\nSort each small group (cheap)\nCollect all medians of groups\nRecursively find median of those medians → good pivot\nPartition around pivot\nRecurse into half that contains the k-th element\n\nEach level discards a constant fraction → (O(n)) total work.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nDivide array into chunks of 5\n\n\n2\nSort each chunk\n\n\n3\nExtract medians\n\n\n4\nFind median of medians recursively\n\n\n5\nPartition using this pivot\n\n\n6\nRecurse into correct side based on k\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef partition(arr, pivot):\n    less = [x for x in arr if x &lt; pivot]\n    equal = [x for x in arr if x == pivot]\n    greater = [x for x in arr if x &gt; pivot]\n    return less, equal, greater\n\ndef select(arr, k):\n    if len(arr) &lt;= 5:\n        return sorted(arr)[k]\n\n    # Step 1: group in chunks of 5\n    chunks = [arr[i:i+5] for i in range(0, len(arr), 5)]\n\n    # Step 2: find medians\n    medians = [sorted(chunk)[len(chunk)//2] for chunk in chunks]\n\n    # Step 3: pivot = median of medians\n    pivot = select(medians, len(medians)//2)\n\n    # Step 4: partition\n    less, equal, greater = partition(arr, pivot)\n\n    # Step 5: recurse\n    if k &lt; len(less):\n        return select(less, k)\n    elif k &lt; len(less) + len(equal):\n        return pivot\n    else:\n        return select(greater, k - len(less) - len(equal))\n\narr = [12, 3, 5, 7, 4, 19, 26, 23, 8, 15]\nk = 4  # 0-based: 5th smallest\nprint(\"5th smallest:\", select(arr, k))\nOutput:\n5th smallest: 8\n\n\n\nC (Simplified Version)\n(Pseudocode-like clarity for readability)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint cmp(const void* a, const void* b) {\n    return (*(int*)a - *(int*)b);\n}\n\nint median_of_medians(int arr[], int n, int k);\n\nint select_group_median(int arr[], int n) {\n    qsort(arr, n, sizeof(int), cmp);\n    return arr[n/2];\n}\n\nint median_of_medians(int arr[], int n, int k) {\n    if (n &lt;= 5) {\n        qsort(arr, n, sizeof(int), cmp);\n        return arr[k];\n    }\n\n    int groups = (n + 4) / 5;\n    int medians[groups];\n    for (int i = 0; i &lt; groups; i++) {\n        int size = (i*5 + 5 &lt;= n) ? 5 : n - i*5;\n        medians[i] = select_group_median(arr + i*5, size);\n    }\n\n    int pivot = median_of_medians(medians, groups, groups/2);\n\n    // Partition\n    int less[n], greater[n], l = 0, g = 0, equal = 0;\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] &lt; pivot) less[l++] = arr[i];\n        else if (arr[i] &gt; pivot) greater[g++] = arr[i];\n        else equal++;\n    }\n\n    if (k &lt; l)\n        return median_of_medians(less, l, k);\n    else if (k &lt; l + equal)\n        return pivot;\n    else\n        return median_of_medians(greater, g, k - l - equal);\n}\n\nint main(void) {\n    int arr[] = {12, 3, 5, 7, 4, 19, 26, 23, 8, 15};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    printf(\"5th smallest: %d\\n\", median_of_medians(arr, n, 4));\n}\nOutput:\n5th smallest: 8\n\n\nWhy It Matters\n\nGuaranteed O(n) even in worst case\nNo bad pivots → stable performance\nBasis for BFPRT algorithm\nUsed in theoretical guarantees for real systems\nKey for deterministic selection, safe quantile computations\n\n\n\nA Gentle Proof (Why It Works)\nEach pivot ensures at least 30% of elements are discarded each recursion (proof via grouping).\nRecurrence: \\[\nT(n) = T(n/5) + T(7n/10) + O(n) \\Rightarrow O(n)\n\\]\nThus, always linear time, even worst case.\n\n\nTry It Yourself\n\nFind median of [5, 2, 1, 3, 4]\nTest with duplicates\nCompare with Quickselect runtime\nCount recursive calls\nChange group size to 3 or 7\nVisualize grouping steps\nPrint pivot each round\nApply to large random list\nBenchmark vs sorting\nImplement as pivot strategy for Quickselect\n\n\n\nTest Cases\n\n\n\nArray\nk\nOutput\nNotes\n\n\n\n\n[12,3,5,7,4,19,26,23,8,15]\n4\n8\n5th smallest\n\n\n[5,2,1,3,4]\n2\n3\nMedian\n\n\n[7,7,7,7]\n2\n7\nDuplicates\n\n\n[10,9,8,7,6,5]\n0\n5\nMin\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Worst)\nO(n)\n\n\nTime (Avg)\nO(n)\n\n\nSpace\nO(n)\n\n\nStable\nNo\n\n\nDeterministic\nYes\n\n\n\nMedian of Medians, a balanced thinker in the world of selection: slow and steady, but always linear.\n\n\n\n173 Randomized Select\nRandomized Select is a probabilistic version of Quickselect, where the pivot is chosen randomly to avoid worst-case behavior. This small twist makes the algorithm’s expected time O(n), even though the worst case remains (O(n^2)). In practice, it’s fast, simple, and robust, a true workhorse for order statistics.\n\nWhat Problem Are We Solving??\nYou need the k-th smallest element in an unsorted list. Quickselect works well, but choosing the first or last element as pivot can cause bad splits.\nRandomized Select improves this by picking a random pivot, making bad luck rare and performance stable.\n\n\nExample\nFind 4th smallest in [7, 2, 1, 6, 8, 5, 3, 4]\n\nPick random pivot (say 5)\nPartition → [2,1,3,4] [5] [7,6,8]\nPivot index = 4 → 4 &gt; 3, so recurse on left [2,1,3,4]\nPick random pivot again (say 3)\nPartition → [2,1] [3] [4]\nIndex 2 = k=3 → found 4th smallest = 4 ✅\n\n\n\nHow Does It Work (Plain Language)?\nIt’s Quickselect with random pivoting. At each step:\n\nPick a random element as pivot.\nPartition around pivot.\nOnly recurse into one side (where k lies).\n\nThis randomness ensures average-case balance, even on adversarial inputs.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nPick random pivot index\n\n\n2\nPartition array around pivot\n\n\n3\nGet pivot index p\n\n\n4\nIf p == k, return element\n\n\n5\nIf p &gt; k, recurse left\n\n\n6\nIf p &lt; k, recurse right (adjust k)\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport random\n\ndef partition(arr, low, high):\n    pivot = arr[high]\n    i = low\n    for j in range(low, high):\n        if arr[j] &lt; pivot:\n            arr[i], arr[j] = arr[j], arr[i]\n            i += 1\n    arr[i], arr[high] = arr[high], arr[i]\n    return i\n\ndef randomized_select(arr, low, high, k):\n    if low == high:\n        return arr[low]\n\n    pivot_index = random.randint(low, high)\n    arr[pivot_index], arr[high] = arr[high], arr[pivot_index]\n\n    p = partition(arr, low, high)\n\n    if p == k:\n        return arr[p]\n    elif p &gt; k:\n        return randomized_select(arr, low, p - 1, k)\n    else:\n        return randomized_select(arr, p + 1, high, k)\n\narr = [7, 2, 1, 6, 8, 5, 3, 4]\nk = 3  # 4th smallest\nprint(\"4th smallest:\", randomized_select(arr, 0, len(arr)-1, k))\nOutput:\n4th smallest: 4\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nvoid swap(int *a, int *b) {\n    int t = *a; *a = *b; *b = t;\n}\n\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            swap(&arr[i], &arr[j]);\n            i++;\n        }\n    }\n    swap(&arr[i], &arr[high]);\n    return i;\n}\n\nint randomized_select(int arr[], int low, int high, int k) {\n    if (low == high) return arr[low];\n\n    int pivot_index = low + rand() % (high - low + 1);\n    swap(&arr[pivot_index], &arr[high]);\n    int p = partition(arr, low, high);\n\n    if (p == k) return arr[p];\n    else if (p &gt; k) return randomized_select(arr, low, p - 1, k);\n    else return randomized_select(arr, p + 1, high, k);\n}\n\nint main(void) {\n    srand(time(NULL));\n    int arr[] = {7, 2, 1, 6, 8, 5, 3, 4};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    int k = 3;\n    printf(\"4th smallest: %d\\n\", randomized_select(arr, 0, n - 1, k));\n}\nOutput:\n4th smallest: 4\n\n\n\nWhy It Matters\n\nExpected O(n) time, simple, and practical\nAvoids worst-case trap of fixed-pivot Quickselect\nGreat for top-k queries, quantiles, median\nCombines simplicity + randomness = robust performance\nCommonly used in competitive programming and real-world systems\n\n\n\nA Gentle Proof (Why It Works)\nExpected recurrence: \\[\nT(n) = T(\\alpha n) + O(n)\n\\] where \\(\\alpha\\) is random, expected \\(\\approx \\tfrac{1}{2}\\)\n→ \\(T(n) = O(n)\\)\nWorst case still \\(O(n^2)\\), but rare.\nExpected comparisons \\(\\approx 2n\\).\n\n\nTry It Yourself\n\nRun multiple times and observe pivot randomness\nCompare with deterministic Quickselect\nCount recursive calls\nTest with sorted input (robust)\nTest all same elements\nChange k (first, median, last)\nModify to find k-th largest (n-k-1)\nCompare performance with sort()\nLog pivot indices\nMeasure runtime on 10⁶ elements\n\n\n\nTest Cases\n\n\n\nArray\nk\nOutput\nNotes\n\n\n\n\n[7,2,1,6,8,5,3,4]\n3\n4\n4th smallest\n\n\n[10,80,30,90,40,50,70]\n4\n70\nWorks on any order\n\n\n[1,2,3,4,5]\n0\n1\nSorted input safe\n\n\n[5,5,5,5]\n2\n5\nDuplicates fine\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime (Expected)\nO(n)\n\n\nTime (Worst)\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\nRandomized\nYes\n\n\nIn-place\nYes\n\n\n\nRandomized Select, a game of chance that almost always wins: fast, fair, and beautifully simple.\n\n\n\n174 Binary Search on Answer\nBinary Search on Answer (also called Parametric Search) is a powerful optimization trick used when the search space is monotonic—meaning once a condition becomes true, it stays true (or vice versa). Instead of searching a sorted array, we’re searching for the smallest or largest value that satisfies a condition.\n\nWhat Problem Are We Solving??\nSometimes you don’t have an array to search, but you need to minimize or maximize a numeric answer. Examples:\n\nMinimum capacity to transport items in k days\nMinimum maximum distance between routers\nMaximum median satisfying a condition\n\nWe can’t iterate all possibilities efficiently, but we can binary search the answer space.\n\n\nExample\nProblem: Given array [1, 2, 3, 4, 5], split into 2 parts, minimize the largest sum among parts.\nWe can’t directly find it, but if we can check whether a candidate value mid is valid (can split with sum ≤ mid), we can binary search on mid.\n\n\n\nmid\ncanSplit(nums, 2, mid)\nResult\n\n\n\n\n9\nTrue (splits: [1,2,3,4], [5])\nok → move left\n\n\n7\nFalse\nmove right\n\n\n8\nTrue\nok → final answer = 9\n\n\n\n✅ Result = 9\n\n\nHow Does It Work (Plain Language)?\nYou don’t search elements, you search values. You define a function can(mid) that checks if a solution is possible with mid. Then use binary search to narrow down the range until you find the optimal value.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nDefine the range of answers (lo, hi)\n\n\n2\nWhile lo &lt; hi:\n\n\n\n mid = (lo + hi) // 2\n\n\n\n If can(mid): hi = mid\n\n\n\n Else: lo = mid + 1\n\n\n3\nReturn lo as the optimal answer\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef can_split(nums, k, mid):\n    count, current = 1, 0\n    for x in nums:\n        if current + x &gt; mid:\n            count += 1\n            current = x\n        else:\n            current += x\n    return count &lt;= k\n\ndef binary_search_answer(nums, k):\n    lo, hi = max(nums), sum(nums)\n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if can_split(nums, k, mid):\n            hi = mid\n        else:\n            lo = mid + 1\n    return lo\n\nnums = [1, 2, 3, 4, 5]\nk = 2\nprint(\"Minimum largest sum:\", binary_search_answer(nums, k))\nOutput:\nMinimum largest sum: 9\n\n\nC\n#include &lt;stdio.h&gt;\n\nint can_split(int arr[], int n, int k, int mid) {\n    int count = 1, sum = 0;\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] &gt; mid) return 0;\n        if (sum + arr[i] &gt; mid) {\n            count++;\n            sum = arr[i];\n        } else {\n            sum += arr[i];\n        }\n    }\n    return count &lt;= k;\n}\n\nint binary_search_answer(int arr[], int n, int k) {\n    int lo = arr[0], hi = 0;\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] &gt; lo) lo = arr[i];\n        hi += arr[i];\n    }\n    while (lo &lt; hi) {\n        int mid = (lo + hi) / 2;\n        if (can_split(arr, n, k, mid))\n            hi = mid;\n        else\n            lo = mid + 1;\n    }\n    return lo;\n}\n\nint main(void) {\n    int arr[] = {1, 2, 3, 4, 5};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    int k = 2;\n    printf(\"Minimum largest sum: %d\\n\", binary_search_answer(arr, n, k));\n}\nOutput:\nMinimum largest sum: 9\n\n\n\nWhy It Matters\n\nSolves optimization problems without brute force\nTurns decision problems into search problems\nA universal pattern: works for capacity, distance, time, etc.\nCommon in LeetCode, interviews, and competitive programming\n\n\n\nA Gentle Proof (Why It Works)\nIf a function f(x) is monotonic (true after a point or false after a point), binary search can find the threshold. Formally:\nIf f(lo) = false, f(hi) = true, and f(x) is monotonic, then binary search converges to smallest x such that f(x) = true.\n\n\nTry It Yourself\n\nFind smallest capacity to ship packages in d days\nFind smallest max page load per student (book allocation)\nFind largest minimum distance between routers\nFind smallest time to paint all boards\nFind minimum speed to reach on time\nDefine a monotonic function can(x) and apply search\nExperiment with float range and tolerance\nTry max instead of min (reverse condition)\nCount binary search steps for each case\nCompare with brute force\n\n\n\nTest Cases\n\n\n\nProblem\nInput\nOutput\nExplanation\n\n\n\n\nSplit array\n[1,2,3,4,5], k=2\n9\n[1,2,3,4],[5]\n\n\nBook allocation\n[10,20,30,40], k=2\n60\n[10,20,30],[40]\n\n\nRouter placement\n[1,2,8,12], k=3\n5\nPlace at 1,6,12\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n log(max - min))\n\n\nSpace\nO(1)\n\n\nMonotonicity Required\nYes\n\n\nType\nDecision-based binary search\n\n\n\nBinary Search on Answer, when you can’t sort the data, sort the solution space.\n\n\n\n175 Order Statistics Tree\nAn Order Statistics Tree is a special kind of augmented binary search tree (BST) that supports two powerful operations efficiently:\n\nSelect(k): find the k-th smallest element.\nRank(x): find the position (rank) of element x.\n\nIt’s a classic data structure where each node stores subtree size, allowing order-based queries in O(log n) time.\n\nWhat Problem Are We Solving??\nSometimes you don’t just want to search by key, you want to search by order. For example:\n\n“What’s the 5th smallest element?”\n“What rank is 37 in the tree?”\n“How many numbers ≤ 50 are there?”\n\nAn order statistics tree gives you both key-based and rank-based access in one structure.\n\n\nExample\nSuppose you insert [20, 15, 25, 10, 18, 22, 30].\nEach node stores size (the number of nodes in its subtree).\n        20(size=7)\n       /          \\\n  15(3)          25(3)\n  /   \\          /   \\\n10(1) 18(1)   22(1) 30(1)\nSelect(4) → 20 (the 4th smallest) Rank(22) → 6 (22 is the 6th smallest)\n\n\nHow Does It Work (Plain Language)?\nEvery node tracks how many nodes exist in its subtree (left + right + itself). When you traverse:\n\nTo select k-th smallest, compare k with size of left subtree.\nTo find rank of x, accumulate sizes while traversing down.\n\n\n\nSelect(k) Pseudocode\nselect(node, k):\n    left_size = size(node.left)\n    if k == left_size + 1: return node.key\n    if k &lt;= left_size: return select(node.left, k)\n    else: return select(node.right, k - left_size - 1)\n\n\nRank(x) Pseudocode\nrank(node, x):\n    if node == NULL: return 0\n    if x &lt; node.key: return rank(node.left, x)\n    if x == node.key: return size(node.left) + 1\n    else: return size(node.left) + 1 + rank(node.right, x)\n\n\nTiny Code (Easy Versions)\n\nPython\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n        self.size = 1\n\ndef update_size(node):\n    if node:\n        node.size = 1 + (node.left.size if node.left else 0) + (node.right.size if node.right else 0)\n\ndef insert(node, key):\n    if node is None:\n        return Node(key)\n    if key &lt; node.key:\n        node.left = insert(node.left, key)\n    else:\n        node.right = insert(node.right, key)\n    update_size(node)\n    return node\n\ndef select(node, k):\n    left_size = node.left.size if node.left else 0\n    if k == left_size + 1:\n        return node.key\n    elif k &lt;= left_size:\n        return select(node.left, k)\n    else:\n        return select(node.right, k - left_size - 1)\n\ndef rank(node, key):\n    if node is None:\n        return 0\n    if key &lt; node.key:\n        return rank(node.left, key)\n    elif key == node.key:\n        return (node.left.size if node.left else 0) + 1\n    else:\n        left_size = node.left.size if node.left else 0\n        return left_size + 1 + rank(node.right, key)\n\nroot = None\nfor x in [20, 15, 25, 10, 18, 22, 30]:\n    root = insert(root, x)\n\nprint(\"Select(4):\", select(root, 4))  # 20\nprint(\"Rank(22):\", rank(root, 22))    # 6\n\n\n\nC (Conceptual Skeleton)\ntypedef struct Node {\n    int key;\n    int size;\n    struct Node *left, *right;\n} Node;\n\nint size(Node* n) { return n ? n-&gt;size : 0; }\n\nvoid update_size(Node* n) {\n    if (n) n-&gt;size = 1 + size(n-&gt;left) + size(n-&gt;right);\n}\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key; n-&gt;size = 1;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else root-&gt;right = insert(root-&gt;right, key);\n    update_size(root);\n    return root;\n}\n\nint select_k(Node* root, int k) {\n    int left_size = size(root-&gt;left);\n    if (k == left_size + 1) return root-&gt;key;\n    else if (k &lt;= left_size) return select_k(root-&gt;left, k);\n    else return select_k(root-&gt;right, k - left_size - 1);\n}\n\n\nWhy It Matters\n\nUseful in rank-based queries, median finding, and order-statistics problems\nCore to balanced trees (AVL, Red-Black, Treaps) with order augmentation\nEnables dynamic median queries and range counting\n\nYou can imagine it as a self-updating leaderboard, always knowing who’s in position k.\n\n\nA Gentle Proof (Why It Works)\nBecause subtree sizes are updated correctly on insertions/deletions, each traversal can compute ranks or k-th values in O(log n) time (in balanced trees). If balanced (like in an AVL or RB-tree), operations remain logarithmic.\n\n\nTry It Yourself\n\nBuild an Order Statistics Tree for [10,20,30,40,50].\nFind Select(3) and Rank(40).\nInsert new elements and recheck ranks.\nExtend to find median dynamically.\nModify to support deletions.\nCompare with sorting then indexing (O(n log n) vs O(log n)).\nTry building on top of Red-Black Tree.\nUse for running percentiles.\nExplore dynamic segment trees for same queries.\nImplement countLessThan(x) using rank.\n\n\n\nTest Cases\n\n\n\nQuery\nExpected Result\n\n\n\n\nSelect(1)\n10\n\n\nSelect(4)\n20\n\n\nRank(10)\n1\n\n\nRank(22)\n6\n\n\nRank(30)\n7\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nInsert / Delete\nO(log n)\n\n\nSelect(k)\nO(log n)\n\n\nRank(x)\nO(log n)\n\n\nSpace\nO(n)\n\n\n\nAn Order Statistics Tree blends search and ranking, perfect for problems that need to know what and where at the same time.\n\n\n\n176 Tournament Tree Selection\nA Tournament Tree is a binary tree structure that simulates a knockout tournament among elements. Each match compares two elements, and the winner moves up. It’s an elegant way to find minimum, maximum, or even k-th smallest elements with structured comparisons.\n\nWhat Problem Are We Solving??\nFinding the minimum or maximum in a list takes O(n). But if you also want the second smallest, third smallest, or k-th, you’d like to reuse earlier comparisons. A tournament tree keeps track of all matches, so you don’t need to start over.\n\n\nExample\nSuppose we have elements: [4, 7, 2, 9, 5, 1, 8, 6].\n\nPair them up: compare (4,7), (2,9), (5,1), (8,6)\nWinners move up: [4, 2, 1, 6]\nNext round: (4,2), (1,6) → winners [2, 1]\nFinal match: (2,1) → winner 1\n\nThe root of the tree = minimum element (1).\nIf you store the losing element from each match, you can trace back the second smallest, it must have lost directly to 1.\n\n\nHow Does It Work (Plain Language)?\nImagine a sports tournament:\n\nEvery player plays one match.\nThe winner moves on, loser is eliminated.\nThe champion (root) is the smallest element.\nThe second smallest is the best among those who lost to the champion.\n\nEach match is one comparison, so total comparisons = n - 1 for the min. To find second min, check log n losers.\n\n\nSteps\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nBuild a complete binary tree where each leaf is an element.\n\n\n2\nCompare each pair and move winner up.\n\n\n3\nStore “losers” in each node.\n\n\n4\nThe root = min. The second min = min(losers along winner’s path).\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef tournament_min(arr):\n    matches = []\n    tree = [[x] for x in arr]\n    while len(tree) &gt; 1:\n        next_round = []\n        for i in range(0, len(tree), 2):\n            if i + 1 == len(tree):\n                next_round.append(tree[i])\n                continue\n            a, b = tree[i][0], tree[i+1][0]\n            if a &lt; b:\n                next_round.append([a, b])\n            else:\n                next_round.append([b, a])\n        matches = next_round\n        tree = next_round\n    return tree[0][0]\n\ndef find_second_min(arr):\n    # Build tournament, keep track of losers\n    n = len(arr)\n    tree = [[x, []] for x in arr]\n    while len(tree) &gt; 1:\n        next_round = []\n        for i in range(0, len(tree), 2):\n            if i + 1 == len(tree):\n                next_round.append(tree[i])\n                continue\n            a, a_losers = tree[i]\n            b, b_losers = tree[i+1]\n            if a &lt; b:\n                next_round.append([a, a_losers + [b]])\n            else:\n                next_round.append([b, b_losers + [a]])\n        tree = next_round\n    winner, losers = tree[0]\n    return winner, min(losers)\n\narr = [4, 7, 2, 9, 5, 1, 8, 6]\nmin_val, second_min = find_second_min(arr)\nprint(\"Min:\", min_val)\nprint(\"Second Min:\", second_min)\nOutput:\nMin: 1\nSecond Min: 2\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\nint tournament_min(int arr[], int n) {\n    int size = n;\n    while (size &gt; 1) {\n        for (int i = 0; i &lt; size / 2; i++) {\n            arr[i] = (arr[2*i] &lt; arr[2*i + 1]) ? arr[2*i] : arr[2*i + 1];\n        }\n        size = (size + 1) / 2;\n    }\n    return arr[0];\n}\n\nint main(void) {\n    int arr[] = {4, 7, 2, 9, 5, 1, 8, 6};\n    int n = sizeof(arr) / sizeof(arr[0]);\n    printf(\"Minimum: %d\\n\", tournament_min(arr, n));\n}\n\n\n\nWhy It Matters\n\nFinds minimum in O(n), second minimum in O(n + log n) comparisons\nReusable for k-th selection if you store all match info\nForms the backbone of selection networks, parallel sorting, and merge tournaments\n\n\n\nA Gentle Proof (Why It Works)\nEach element except the minimum loses exactly once. The minimum element competes in log₂n matches (height of tree). So second minimum must be the smallest of log₂n losers, requiring log₂n extra comparisons.\nTotal = n - 1 + log₂n comparisons, asymptotically optimal.\n\n\nTry It Yourself\n\nBuild a tournament for [5,3,8,2,9,4].\nFind minimum and second minimum manually.\nModify code to find maximum and second maximum.\nPrint tree rounds to visualize matches.\nExperiment with uneven sizes (non-power-of-2).\nTry to extend it to third smallest (hint: store paths).\nCompare with sorting-based approach.\nUse tournament structure for pairwise elimination problems.\nSimulate sports bracket winner path.\nCount comparisons for each step.\n\n\n\nTest Cases\n\n\n\nInput\nMin\nSecond Min\n\n\n\n\n[4,7,2,9,5,1,8,6]\n1\n2\n\n\n[10,3,6,2]\n2\n3\n\n\n[5,4,3,2,1]\n1\n2\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild Tournament\nO(n)\n\n\nFind Minimum\nO(1)\n\n\nFind Second Minimum\nO(log n)\n\n\nSpace\nO(n)\n\n\n\nA Tournament Tree turns comparisons into matches, where every element plays once, and the champion reveals not just victory, but the story of every defeat.\n\n\n\n177 Heap Select (Min-Heap)\nHeap Select is a simple, powerful technique for finding the k smallest (or largest) elements in a collection using a heap. It’s one of the most practical selection algorithms, trading minimal code for strong efficiency.\n\nWhat Problem Are We Solving??\nYou often don’t need a full sort, just the k smallest or k largest items. Examples:\n\nFind top 10 scores\nGet smallest 5 distances\nMaintain top-k trending topics\n\nA heap (priority queue) makes this easy, keep a running set of size k, pop or push as needed.\n\n\nExample\nFind 3 smallest elements in [7, 2, 9, 1, 5, 4].\n\nCreate max-heap of first k=3 elements → [7, 2, 9] → heap = [9, 2, 7]\nFor each next element:\n\n1 &lt; 9 → pop 9, push 1 → heap = [7, 2, 1]\n5 &lt; 7 → pop 7, push 5 → heap = [5, 2, 1]\n4 &lt; 5 → pop 5, push 4 → heap = [4, 2, 1]\n\n\nResult → [1, 2, 4] (the 3 smallest)\n\n\nHow Does It Work (Plain Language)?\nYou keep a heap of size k:\n\nFor smallest elements → use a max-heap (remove largest if new smaller appears).\nFor largest elements → use a min-heap (remove smallest if new larger appears).\n\nThis keeps only the top-k interesting values at all times.\n\n\nStep-by-Step Process\n\n\n\nStep\nAction\n\n\n\n\n1\nInitialize heap with first k elements\n\n\n2\nConvert to max-heap (if looking for smallest k)\n\n\n3\nFor each remaining element x:\n\n\n\nIf x &lt; heap[0] → replace top\n\n\n4\nResult is heap contents (unsorted)\n\n\n5\nSort heap if needed for final output\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport heapq\n\ndef k_smallest(nums, k):\n    heap = [-x for x in nums[:k]]\n    heapq.heapify(heap)\n    for x in nums[k:]:\n        if -x &gt; heap[0]:\n            heapq.heappop(heap)\n            heapq.heappush(heap, -x)\n    return sorted([-h for h in heap])\n\nnums = [7, 2, 9, 1, 5, 4]\nprint(\"3 smallest:\", k_smallest(nums, 3))\nOutput:\n3 smallest: [1, 2, 4]\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid swap(int *a, int *b) { int t = *a; *a = *b; *b = t; }\n\nvoid heapify(int arr[], int n, int i) {\n    int largest = i, l = 2*i+1, r = 2*i+2;\n    if (l &lt; n && arr[l] &gt; arr[largest]) largest = l;\n    if (r &lt; n && arr[r] &gt; arr[largest]) largest = r;\n    if (largest != i) {\n        swap(&arr[i], &arr[largest]);\n        heapify(arr, n, largest);\n    }\n}\n\nvoid build_heap(int arr[], int n) {\n    for (int i = n/2 - 1; i &gt;= 0; i--) heapify(arr, n, i);\n}\n\nvoid heap_select(int arr[], int n, int k) {\n    int heap[k];\n    for (int i = 0; i &lt; k; i++) heap[i] = arr[i];\n    build_heap(heap, k);\n    for (int i = k; i &lt; n; i++) {\n        if (arr[i] &lt; heap[0]) {\n            heap[0] = arr[i];\n            heapify(heap, k, 0);\n        }\n    }\n    printf(\"%d smallest elements:\\n\", k);\n    for (int i = 0; i &lt; k; i++) printf(\"%d \", heap[i]);\n}\n\nint main(void) {\n    int arr[] = {7, 2, 9, 1, 5, 4};\n    int n = 6;\n    heap_select(arr, n, 3);\n}\nOutput:\n3 smallest elements:\n1 2 4\n\n\n\nWhy It Matters\n\nAvoids full sorting (O(n log n))\nGreat for streaming data, sliding windows, top-k problems\nScales well for large n and small k\nUsed in leaderboards, analytics, data pipelines\n\nIf you only need some order, don’t sort it all.\n\n\nA Gentle Proof (Why It Works)\n\nBuilding heap: O(k)\nFor each new element: compare + heapify = O(log k)\nTotal: O(k + (n-k) log k) ≈ O(n log k)\n\nFor small k, this is much faster than sorting.\n\n\nTry It Yourself\n\nFind 3 largest elements using min-heap\nStream numbers from input, maintain smallest 5\nTrack top 10 scores dynamically\nCompare runtime vs sorted(nums)[:k]\nTry k = 1 (minimum), k = n (full sort)\nModify for objects with custom keys (e.g. score, id)\nHandle duplicates, keep all or unique only\nExperiment with random arrays of size 1e6\nVisualize heap evolution per step\nCombine with binary search to tune thresholds\n\n\n\nTest Cases\n\n\n\nInput\nk\nOutput\n\n\n\n\n[7,2,9,1,5,4]\n3\n[1,2,4]\n\n\n[10,8,6,4,2]\n2\n[2,4]\n\n\n[1,1,1,1]\n2\n[1,1]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild Heap\nO(k)\n\n\nIterate Array\nO((n-k) log k)\n\n\nTotal Time\nO(n log k)\n\n\nSpace\nO(k)\n\n\n\nHeap Select is your practical shortcut, sort only what you need, ignore the rest.\n\n\n\n178 Partial QuickSort\nPartial QuickSort is a twist on classic QuickSort, it stops sorting once it has placed the first k elements (or top k) in their correct positions. It’s perfect when you need top-k smallest/largest elements but don’t need the rest sorted.\nThink of it as QuickSort with early stopping, a hybrid between QuickSort and Quickselect.\n\nWhat Problem Are We Solving??\nSometimes you need only part of the sorted order:\n\n“Get top 10 scores”\n“Find smallest k elements”\n“Sort first half”\n\nFully sorting wastes work. Partial QuickSort skips unnecessary partitions.\n\n\nExample\nArray: [9, 4, 6, 2, 8, 1], k = 3\nWe want smallest 3 elements. QuickSort picks a pivot, partitions array:\nPivot = 6\n→ [4, 2, 1] | 6 | [9, 8]\nNow we know all elements left of pivot (4, 2, 1) are smaller. Since left_size == k, we can stop, [1,2,4] are our smallest 3.\n\n\nHow Does It Work (Plain Language)?\nJust like QuickSort, but after partition:\n\nIf pivot index == k → done.\nIf pivot index &gt; k → recurse only left.\nIf pivot index &lt; k → recurse right partially.\n\nYou never sort beyond what’s needed.\n\n\nStep-by-Step Process\n\n\n\nStep\nDescription\n\n\n\n\n1\nChoose pivot\n\n\n2\nPartition array\n\n\n3\nIf pivot index == k → done\n\n\n4\nIf pivot index &gt; k → recurse left\n\n\n5\nElse recurse right on remaining portion\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef partial_quicksort(arr, low, high, k):\n    if low &lt; high:\n        p = partition(arr, low, high)\n        if p &gt; k:\n            partial_quicksort(arr, low, p - 1, k)\n        elif p &lt; k:\n            partial_quicksort(arr, low, p - 1, k)\n            partial_quicksort(arr, p + 1, high, k)\n\ndef partition(arr, low, high):\n    pivot = arr[high]\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] &lt; pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n\narr = [9, 4, 6, 2, 8, 1]\nk = 3\npartial_quicksort(arr, 0, len(arr) - 1, k - 1)\nprint(\"Smallest 3 elements:\", sorted(arr[:k]))\nOutput:\nSmallest 3 elements: [1, 2, 4]\n\n\nC\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) { int t = *a; *a = *b; *b = t; }\n\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (arr[j] &lt; pivot) {\n            i++;\n            swap(&arr[i], &arr[j]);\n        }\n    }\n    swap(&arr[i + 1], &arr[high]);\n    return i + 1;\n}\n\nvoid partial_quicksort(int arr[], int low, int high, int k) {\n    if (low &lt; high) {\n        int p = partition(arr, low, high);\n        if (p &gt; k)\n            partial_quicksort(arr, low, p - 1, k);\n        else if (p &lt; k)\n            partial_quicksort(arr, p + 1, high, k);\n    }\n}\n\nint main(void) {\n    int arr[] = {9, 4, 6, 2, 8, 1};\n    int n = 6, k = 3;\n    partial_quicksort(arr, 0, n - 1, k - 1);\n    printf(\"Smallest %d elements:\\n\", k);\n    for (int i = 0; i &lt; k; i++) printf(\"%d \", arr[i]);\n}\nOutput:\nSmallest 3 elements:\n1 2 4\n\n\n\nWhy It Matters\n\nEfficient top-k selection when order matters\nAvoids sorting unnecessary portions\nCombines strengths of Quickselect and QuickSort\nWorks in-place (no extra memory)\n\nGreat for partial sorting like “leaderboards”, “top results”, or “bounded priority lists”.\n\n\nA Gentle Proof (Why It Works)\nQuickSort partitions data into two halves; Only parts that could contain the first k elements are explored. Average time complexity becomes O(n) for selection, O(n log k) for partial order.\n\n\nTry It Yourself\n\nFind smallest 5 numbers in [10,9,8,7,6,5,4,3,2,1]\nModify to find largest k instead\nCompare runtime vs full sort()\nVisualize recursion path\nTrack how many elements actually get sorted\nTry random pivot vs median pivot\nTest k = 1 (min) and k = n (full sort)\nMeasure comparisons count\nTry with duplicates\nCombine with heap for hybrid version\n\n\n\nTest Cases\n\n\n\nInput\nk\nOutput\n\n\n\n\n[9,4,6,2,8,1]\n3\n[1,2,4]\n\n\n[5,4,3,2,1]\n2\n[1,2]\n\n\n[7,7,7,7]\n2\n[7,7]\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nAverage Time\nO(n)\n\n\nWorst Time\nO(n²)\n\n\nSpace\nO(1)\n\n\nStable\nNo\n\n\n\nPartial QuickSort, fast, focused, and frugal, because sometimes, you only need a slice of order, not the whole loaf.\n\n\n\n179 BFPRT Algorithm (Median of Medians Selection)\nThe BFPRT algorithm (named after Blum, Floyd, Pratt, Rivest, and Tarjan) is a deterministic linear-time selection algorithm. It finds the k-th smallest element in an unsorted array, guaranteeing O(n) worst-case time, a mathematically elegant and exact alternative to randomized quickselect.\n\nWhat Problem Are We Solving??\nYou want to find the k-th smallest element, like the median, but you don’t want to gamble on random pivots (which could hit worst-case O(n²)). BFPRT chooses pivots so well that it always guarantees O(n).\nThis makes it ideal for systems where deterministic behavior matters, like embedded systems, compilers, and real-time applications.\n\n\nExample\nFind the median (k=5) of [9, 4, 7, 3, 6, 1, 8, 2, 5, 10]\n\nDivide into groups of 5: [9,4,7,3,6], [1,8,2,5,10]\nFind median of each group: → [6, 5]\nFind median of medians: → median of [6,5] is 5.5 ≈ 5\nPartition around pivot 5 → [4,3,1,2,5] | 5 | [9,7,8,6,10]\nPosition of 5 = 5 → done.\n\n✅ The 5th smallest = 5\n\n\nHow Does It Work (Plain Language)?\nIt’s Quickselect with a smarter pivot:\n\nDivide the array into groups of 5.\nFind median of each group.\nRecursively find median of these medians.\nUse that as pivot → partition → recurse on the correct side.\n\nThis ensures the pivot is always good enough to split the array reasonably, keeping recursion balanced.\n\n\nStep-by-Step Summary\n\n\n\nStep\nDescription\n\n\n\n\n1\nSplit array into groups of 5\n\n\n2\nSort each group and take its median\n\n\n3\nRecursively find median of medians\n\n\n4\nPartition around pivot\n\n\n5\nRecurse on side containing k-th smallest\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef partition(arr, pivot):\n    less, equal, greater = [], [], []\n    for x in arr:\n        if x &lt; pivot: less.append(x)\n        elif x &gt; pivot: greater.append(x)\n        else: equal.append(x)\n    return less, equal, greater\n\ndef select(arr, k):\n    if len(arr) &lt;= 5:\n        return sorted(arr)[k]\n    \n    # Step 1: Divide into groups of 5\n    groups = [arr[i:i+5] for i in range(0, len(arr), 5)]\n    \n    # Step 2: Find medians\n    medians = [sorted(g)[len(g)//2] for g in groups]\n    \n    # Step 3: Median of medians as pivot\n    pivot = select(medians, len(medians)//2)\n    \n    # Step 4: Partition\n    less, equal, greater = partition(arr, pivot)\n    \n    # Step 5: Recurse\n    if k &lt; len(less):\n        return select(less, k)\n    elif k &lt; len(less) + len(equal):\n        return pivot\n    else:\n        return select(greater, k - len(less) - len(equal))\n\narr = [9,4,7,3,6,1,8,2,5,10]\nk = 4  # 0-based index → 5th smallest\nprint(\"5th smallest:\", select(arr, k))\nOutput:\n5th smallest: 5\n\n\n\nC (Conceptual Skeleton)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint cmp(const void *a, const void *b) { return (*(int*)a - *(int*)b); }\n\nint median_of_medians(int arr[], int n);\n\nint select_kth(int arr[], int n, int k) {\n    if (n &lt;= 5) {\n        qsort(arr, n, sizeof(int), cmp);\n        return arr[k];\n    }\n\n    int groups = (n + 4) / 5;\n    int *medians = malloc(groups * sizeof(int));\n    for (int i = 0; i &lt; groups; i++) {\n        int start = i * 5;\n        int end = (start + 5 &lt; n) ? start + 5 : n;\n        qsort(arr + start, end - start, sizeof(int), cmp);\n        medians[i] = arr[start + (end - start) / 2];\n    }\n\n    int pivot = median_of_medians(medians, groups);\n    free(medians);\n\n    int *left = malloc(n * sizeof(int));\n    int *right = malloc(n * sizeof(int));\n    int l = 0, r = 0, equal = 0;\n    for (int i = 0; i &lt; n; i++) {\n        if (arr[i] &lt; pivot) left[l++] = arr[i];\n        else if (arr[i] &gt; pivot) right[r++] = arr[i];\n        else equal++;\n    }\n\n    if (k &lt; l) return select_kth(left, l, k);\n    else if (k &lt; l + equal) return pivot;\n    else return select_kth(right, r, k - l - equal);\n}\n\nint median_of_medians(int arr[], int n) {\n    return select_kth(arr, n, n / 2);\n}\n\n\nWhy It Matters\n\nDeterministic O(n), no randomness or bad pivots\nUsed in theoretical CS, worst-case analysis, exact solvers\nFoundation for deterministic selection, median-finding, linear-time sorting bounds\nCore to intro algorithms theory (CLRS Chapter 9)\n\n\n\nA Gentle Proof (Why It Works)\n\nEach group of 5 → median is ≥ 3 elements in group (2 below, 2 above)\nAt least half of medians ≥ pivot → pivot ≥ 30% of elements\nAt least half ≤ pivot → pivot ≤ 70% of elements\nSo pivot always splits array 30–70, guaranteeing T(n) = T(n/5) + T(7n/10) + O(n) = O(n)\n\nNo chance of quadratic blowup.\n\n\nTry It Yourself\n\nFind median of [5,3,2,8,1,9,7,6,4]\nTrace pivot selection tree\nCompare with random quickselect pivots\nMeasure time for n = 1e6\nTry with duplicates\nTry k = 0, k = n-1 (min/max)\nModify group size (e.g. 3 or 7), compare performance\nVerify recursion depth\nUse for percentile queries\nImplement streaming median with repeated selection\n\n\n\nTest Cases\n\n\n\nInput\nk\nOutput\nMeaning\n\n\n\n\n[9,4,7,3,6,1,8,2,5,10]\n4\n5\n5th smallest\n\n\n[1,2,3,4,5]\n2\n3\nmiddle\n\n\n[10,9,8,7,6]\n0\n6\nsmallest\n\n\n\n\n\nComplexity\n\n\n\nAspect\nValue\n\n\n\n\nTime\nO(n) deterministic\n\n\nSpace\nO(n) (can be optimized to O(1))\n\n\nStable\nNo\n\n\nPivot Quality\nGuaranteed 30–70 split\n\n\n\nThe BFPRT Algorithm, proof that with clever pivots and math, even chaos can be conquered in linear time.\n\n\n\n180 Kth Largest Stream\nThe Kth Largest Stream problem focuses on maintaining the kth largest element in a sequence that grows over time, a stream. Instead of sorting everything every time, we can use a min-heap of size k to always keep track of the top k elements efficiently.\nThis is the foundation for real-time leaderboards, streaming analytics, and online ranking systems.\n\nWhat Problem Are We Solving??\nGiven a stream of numbers (arriving one by one), we want to:\n\nAlways know the kth largest element so far.\nUpdate quickly when a new number comes in.\n\nYou don’t know the final list, you process as it flows.\n\n\nExample\nSay k = 3, stream = [4, 5, 8, 2]\n\nStart empty heap\nAdd 4 → heap = [4] → kth largest = 4\nAdd 5 → heap = [4, 5] → kth largest = 4\nAdd 8 → heap = [4, 5, 8] → kth largest = 4\nAdd 2 → ignore (2 &lt; 4) → kth largest = 4\n\nAdd new number 10:\n\n10 &gt; 4 → pop 4, push 10 → heap = [5,8,10]\nkth largest = 5\n\n✅ Each new element processed in O(log k)\n\n\nHow Does It Work (Plain Language)?\nKeep a min-heap of size k:\n\nIt holds the k largest elements seen so far.\nThe smallest among them (heap root) is the kth largest.\nWhen a new value arrives:\n\nIf heap size &lt; k → push it\nElse if value &gt; heap[0] → pop smallest, push new\n\n\n\n\nStep-by-Step Summary\n\n\n\nStep\nAction\n\n\n\n\n1\nInitialize empty min-heap\n\n\n2\nFor each new element x:\n\n\n\nIf heap size &lt; k → push x\n\n\n\nElse if x &gt; heap[0] → pop, push x\n\n\n3\nkth largest = heap[0]\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\nimport heapq\n\nclass KthLargest:\n    def __init__(self, k, nums):\n        self.k = k\n        self.heap = nums\n        heapq.heapify(self.heap)\n        while len(self.heap) &gt; k:\n            heapq.heappop(self.heap)\n\n    def add(self, val):\n        if len(self.heap) &lt; self.k:\n            heapq.heappush(self.heap, val)\n        elif val &gt; self.heap[0]:\n            heapq.heapreplace(self.heap, val)\n        return self.heap[0]\n\n# Example\nstream = KthLargest(3, [4,5,8,2])\nprint(stream.add(3))  # 4\nprint(stream.add(5))  # 5\nprint(stream.add(10)) # 5\nprint(stream.add(9))  # 8\nprint(stream.add(4))  # 8\nOutput:\n4\n5\n5\n8\n8\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid swap(int *a, int *b) { int t = *a; *a = *b; *b = t; }\n\nvoid heapify(int arr[], int n, int i) {\n    int smallest = i, l = 2*i+1, r = 2*i+2;\n    if (l &lt; n && arr[l] &lt; arr[smallest]) smallest = l;\n    if (r &lt; n && arr[r] &lt; arr[smallest]) smallest = r;\n    if (smallest != i) {\n        swap(&arr[i], &arr[smallest]);\n        heapify(arr, n, smallest);\n    }\n}\n\nvoid push_heap(int heap[], int *n, int val) {\n    heap[(*n)++] = val;\n    for (int i = (*n)/2 - 1; i &gt;= 0; i--) heapify(heap, *n, i);\n}\n\nint pop_min(int heap[], int *n) {\n    int root = heap[0];\n    heap[0] = heap[--(*n)];\n    heapify(heap, *n, 0);\n    return root;\n}\n\nint add(int heap[], int *n, int k, int val) {\n    if (*n &lt; k) {\n        push_heap(heap, n, val);\n    } else if (val &gt; heap[0]) {\n        heap[0] = val;\n        heapify(heap, *n, 0);\n    }\n    return heap[0];\n}\n\nint main(void) {\n    int heap[10] = {4,5,8,2};\n    int n = 4, k = 3;\n    for (int i = n/2 - 1; i &gt;= 0; i--) heapify(heap, n, i);\n    while (n &gt; k) pop_min(heap, &n);\n\n    printf(\"%d\\n\", add(heap, &n, k, 3));  // 4\n    printf(\"%d\\n\", add(heap, &n, k, 5));  // 5\n    printf(\"%d\\n\", add(heap, &n, k, 10)); // 5\n    printf(\"%d\\n\", add(heap, &n, k, 9));  // 8\n    printf(\"%d\\n\", add(heap, &n, k, 4));  // 8\n}\n\n\n\nWhy It Matters\n\nReal-time streaming top-k tracking\nConstant-time query (O(1)), fast update (O(log k))\nCore building block for:\n\nLeaderboards\nMonitoring systems\nContinuous analytics\nOnline medians & percentiles\n\n\n\n\nA Gentle Proof (Why It Works)\nMin-heap stores only top k values. Whenever new value &gt; heap[0], it must belong in top k. So invariant holds: heap = top-k largest elements seen so far. kth largest = heap[0].\nEach update → O(log k). Total after n elements → O(n log k).\n\n\nTry It Yourself\n\nInitialize with [4,5,8,2], k=3, stream = [3,5,10,9,4]\nTry decreasing sequence\nTry duplicates\nTest k = 1 (maximum tracker)\nAdd 1000 elements randomly, measure performance\nCompare with full sort each time\nVisualize heap evolution per step\nModify for k smallest\nBuild real-time median tracker using two heaps\nExtend to stream of objects (track by score field)\n\n\n\nTest Cases\n\n\n\nInitial\nk\nStream\nOutput Sequence\n\n\n\n\n[4,5,8,2]\n3\n[3,5,10,9,4]\n[4,5,5,8,8]\n\n\n[10,7,11,5]\n2\n[8,12,4]\n[10,11,11]\n\n\n[1]\n1\n[2,3]\n[2,3]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nAdd\nO(log k)\n\n\nQuery kth Largest\nO(1)\n\n\nSpace\nO(k)\n\n\n\nKth Largest Stream, stay calm in the flow; the heap remembers what matters most.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-19.-range-search-and-nearest-neighbor",
    "href": "books/en-us/list-2.html#section-19.-range-search-and-nearest-neighbor",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 19. Range Search and Nearest Neighbor",
    "text": "Section 19. Range Search and Nearest Neighbor\n\n181 Binary Search Range\nBinary Search Range extends the basic binary search to find not just one occurrence, but the range of positions where a given value appears, specifically, the first (lower bound) and last (upper bound) indices of a target in a sorted array.\nIt’s the backbone for problems that require counting occurrences, range queries, and insertion positions in ordered data.\n\nWhat Problem Are We Solving??\nStandard binary search returns one match. But what if the target appears multiple times, or we want where it should go?\nExamples:\n\nCount occurrences of x in sorted array\nFind the first element ≥ x\nFind the last element ≤ x\n\nWith two binary searches, we can find the full [start, end] range efficiently.\n\n\nExample\nArray: [1, 2, 2, 2, 3, 4, 5], target = 2\n\n\n\nFunction\nResult\n\n\n\n\nLower Bound (≥ 2)\n1\n\n\nUpper Bound (&gt; 2)\n4\n\n\nRange\n[1, 3]\n\n\n\nOccurrences = upper - lower = 3\n\n\nHow Does It Work (Plain Language)?\nWe use binary search twice:\n\nOne to find the first index ≥ target (lower bound)\nOne to find the first index &gt; target (upper bound)\n\nSubtract them to get the count, or slice the range.\n\n\nStep-by-Step Summary\n\n\n\nStep\nDescription\n\n\n\n\n1\nBinary search for first index i where arr[i] &gt;= target\n\n\n2\nBinary search for first index j where arr[j] &gt; target\n\n\n3\nRange = [i, j - 1] if i &lt; j and arr[i] == target\n\n\n4\nCount = j - i\n\n\n\n\n\nTiny Code (Easy Versions)\n\nPython\ndef lower_bound(arr, target):\n    lo, hi = 0, len(arr)\n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if arr[mid] &lt; target:\n            lo = mid + 1\n        else:\n            hi = mid\n    return lo\n\ndef upper_bound(arr, target):\n    lo, hi = 0, len(arr)\n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if arr[mid] &lt;= target:\n            lo = mid + 1\n        else:\n            hi = mid\n    return lo\n\ndef binary_search_range(arr, target):\n    l = lower_bound(arr, target)\n    r = upper_bound(arr, target)\n    if l == r:\n        return (-1, -1)  # Not found\n    return (l, r - 1)\n\narr = [1, 2, 2, 2, 3, 4, 5]\ntarget = 2\nprint(\"Range:\", binary_search_range(arr, target))\nprint(\"Count:\", upper_bound(arr, target) - lower_bound(arr, target))\nOutput:\nRange: (1, 3)\nCount: 3\n\n\nC\n#include &lt;stdio.h&gt;\n\nint lower_bound(int arr[], int n, int target) {\n    int lo = 0, hi = n;\n    while (lo &lt; hi) {\n        int mid = (lo + hi) / 2;\n        if (arr[mid] &lt; target) lo = mid + 1;\n        else hi = mid;\n    }\n    return lo;\n}\n\nint upper_bound(int arr[], int n, int target) {\n    int lo = 0, hi = n;\n    while (lo &lt; hi) {\n        int mid = (lo + hi) / 2;\n        if (arr[mid] &lt;= target) lo = mid + 1;\n        else hi = mid;\n    }\n    return lo;\n}\n\nint main(void) {\n    int arr[] = {1, 2, 2, 2, 3, 4, 5};\n    int n = 7, target = 2;\n    int l = lower_bound(arr, n, target);\n    int r = upper_bound(arr, n, target);\n    if (l == r) printf(\"Not found\\n\");\n    else printf(\"Range: [%d, %d], Count: %d\\n\", l, r - 1, r - l);\n}\nOutput:\nRange: [1, 3], Count: 3\n\n\n\nWhy It Matters\n\nExtends binary search beyond “found or not”\nEssential in frequency counting, range queries, histograms\nPowers data structures like Segment Trees, Fenwick Trees, and Range Indexes\nUsed in competitive programming and database indexing\n\n\n\nA Gentle Proof (Why It Works)\nBecause binary search maintains sorted invariants (lo &lt; hi and mid conditions),\n\nLower bound finds first index where condition flips (&lt; target → ≥ target)\nUpper bound finds first index beyond target\n\nBoth run in O(log n), giving exact range boundaries.\n\n\nTry It Yourself\n\nTest with no occurrences (e.g. [1,3,5], target=2)\nTest with all equal (e.g. [2,2,2,2], target=2)\nTest with first element = target\nTest with last element = target\nTry to count elements ≤ x or &lt; x\nExtend for floating point or custom comparator\nUse on strings or tuples\nCombine with bisect in Python\nCompare iterative vs recursive\nUse as primitive for frequency table\n\n\n\nTest Cases\n\n\n\nArray\nTarget\nRange\nCount\n\n\n\n\n[1,2,2,2,3,4,5]\n2\n[1,3]\n3\n\n\n[1,3,5,7]\n2\n[-1,-1]\n0\n\n\n[2,2,2,2]\n2\n[0,3]\n4\n\n\n[1,2,3,4]\n4\n[3,3]\n1\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nLower Bound\nO(log n)\n\n\nUpper Bound\nO(log n)\n\n\nSpace\nO(1)\n\n\nStable\nYes\n\n\n\nBinary Search Range, when one answer isn’t enough, and precision is everything.\n\n\n\n182 Segment Tree Query\nSegment Tree Query is a powerful data structure technique that allows you to efficiently compute range queries like sum, minimum, maximum, or even custom associative operations over subarrays.\nIt preprocesses the array into a binary tree structure, where each node stores a summary (aggregate) of a segment.\nOnce built, you can answer queries and updates in O(log n) time.\n\nWhat Problem Are We Solving??\nGiven an array, we often want to query over ranges:\n\nSum over [L, R]\nMinimum or Maximum in [L, R]\nGCD, product, XOR, or any associative function\n\nA naive approach would loop each query: O(n) per query. Segment Trees reduce this to O(log n) with a one-time O(n) build.\n\n\nExample\nArray: [2, 4, 5, 7, 8, 9]\n\n\n\nQuery\nResult\n\n\n\n\nSum(1,3)\n4+5+7 = 16\n\n\nMin(2,5)\nmin(5,7,8,9) = 5\n\n\n\n\n\nHow It Works (Intuitive View)\nA Segment Tree is like a binary hierarchy:\n\nThe root covers the full range [0, n-1]\nEach node covers a subrange\nThe leaf nodes are individual elements\nEach internal node stores a merge (sum, min, max…) of its children\n\nTo query a range, you traverse only relevant branches.\n\n\nBuild, Query, Update\n\n\n\nOperation\nDescription\nTime\n\n\n\n\nBuild\nRecursively combine child segments\nO(n)\n\n\nQuery\nTraverse overlapping nodes\nO(log n)\n\n\nUpdate\nRecompute along path\nO(log n)\n\n\n\n\n\nTiny Code (Sum Query Example)\n\nPython\nclass SegmentTree:\n    def __init__(self, arr):\n        self.n = len(arr)\n        self.tree = [0] * (4 * self.n)\n        self._build(arr, 1, 0, self.n - 1)\n\n    def _build(self, arr, node, l, r):\n        if l == r:\n            self.tree[node] = arr[l]\n        else:\n            mid = (l + r) // 2\n            self._build(arr, 2 * node, l, mid)\n            self._build(arr, 2 * node + 1, mid + 1, r)\n            self.tree[node] = self.tree[2 * node] + self.tree[2 * node + 1]\n\n    def query(self, node, l, r, ql, qr):\n        if qr &lt; l or ql &gt; r:  # no overlap\n            return 0\n        if ql &lt;= l and r &lt;= qr:  # total overlap\n            return self.tree[node]\n        mid = (l + r) // 2\n        left = self.query(2 * node, l, mid, ql, qr)\n        right = self.query(2 * node + 1, mid + 1, r, ql, qr)\n        return left + right\n\n# Example\narr = [2, 4, 5, 7, 8, 9]\nst = SegmentTree(arr)\nprint(st.query(1, 0, len(arr) - 1, 1, 3))  # Sum from index 1 to 3\nOutput:\n16\n\n\nC\n#include &lt;stdio.h&gt;\n\n#define MAXN 100\nint tree[4 * MAXN];\nint arr[MAXN];\n\nint build(int node, int l, int r) {\n    if (l == r) return tree[node] = arr[l];\n    int mid = (l + r) / 2;\n    int left = build(2 * node, l, mid);\n    int right = build(2 * node + 1, mid + 1, r);\n    return tree[node] = left + right;\n}\n\nint query(int node, int l, int r, int ql, int qr) {\n    if (qr &lt; l || ql &gt; r) return 0;\n    if (ql &lt;= l && r &lt;= qr) return tree[node];\n    int mid = (l + r) / 2;\n    return query(2 * node, l, mid, ql, qr) + query(2 * node + 1, mid + 1, r, ql, qr);\n}\n\nint main() {\n    int n = 6;\n    int data[] = {2, 4, 5, 7, 8, 9};\n    for (int i = 0; i &lt; n; i++) arr[i] = data[i];\n    build(1, 0, n - 1);\n    printf(\"Sum [1,3] = %d\\n\", query(1, 0, n - 1, 1, 3));\n}\nOutput:\nSum [1,3] = 16\n\n\n\nWhy It Matters\n\nHandles dynamic range queries and updates efficiently\nCore of competitive programming and data analytics\nForms base for Range Minimum Query, 2D queries, and lazy propagation\nUseful in databases, financial systems, and game engines\n\n\n\nIntuition (Associativity Rule)\nSegment Trees only work when the operation is associative:\nmerge(a, merge(b, c)) = merge(merge(a, b), c)\nExamples:\n\nSum, Min, Max, GCD, XOR\nNot Median, Not Mode (non-associative)\n\n\n\nTry It Yourself\n\nImplement for min or max instead of sum\nAdd update() for point changes\nImplement lazy propagation for range updates\nExtend to 2D segment tree\nCompare with Fenwick Tree (BIT)\nTest on non-trivial ranges\nVisualize the tree layout\nBuild iterative segment tree\nHandle custom operations (GCD, XOR)\nBenchmark O(n log n) vs naive O(nq)\n\n\n\nTest Cases\n\n\n\nArray\nQuery\nExpected\n\n\n\n\n[2,4,5,7,8,9]\nSum(1,3)\n16\n\n\n[1,2,3,4]\nSum(0,3)\n10\n\n\n[5,5,5,5]\nSum(1,2)\n10\n\n\n[3,2,1,4]\nMin(1,3)\n1\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild\nO(n)\n\n\nQuery\nO(log n)\n\n\nUpdate\nO(log n)\n\n\nSpace\nO(4n)\n\n\n\nSegment Tree Query, build once, query many, fast forever.\n\n\n\n183 Fenwick Tree Query\nA Fenwick Tree (or Binary Indexed Tree) is a data structure designed for prefix queries and point updates in O(log n) time. It’s a more space-efficient, iterative cousin of the Segment Tree, perfect when operations are cumulative (sum, XOR, etc.) and updates are frequent.\n\nWhat Problem Are We Solving??\nWe want to:\n\nCompute prefix sums efficiently\nSupport updates dynamically\n\nA naive approach takes O(n) per query or update. A Fenwick Tree does both in O(log n).\n\n\nExample\nArray: [2, 4, 5, 7, 8]\n\n\n\nQuery\nResult\n\n\n\n\nPrefixSum(3)\n2 + 4 + 5 + 7 = 18\n\n\nRangeSum(1, 3)\nPrefix(3) - Prefix(0) = 18 - 2 = 16\n\n\n\n\n\nHow It Works (Plain Language)\nA Fenwick Tree stores cumulative information in indexed chunks. Each index covers a range determined by its least significant bit (LSB).\nindex i covers range (i - LSB(i) + 1) ... i\nWe can update or query by moving through indices using bit operations:\n\nUpdate: move forward by adding LSB\nQuery: move backward by subtracting LSB\n\nThis clever bit trick keeps operations O(log n).\n\n\nExample Walkthrough\nFor array [2, 4, 5, 7, 8] (1-based index):\n\n\n\nIndex\nBinary\nLSB\nRange\nValue\n\n\n\n\n1\n001\n1\n[1]\n2\n\n\n2\n010\n2\n[1–2]\n6\n\n\n3\n011\n1\n[3]\n5\n\n\n4\n100\n4\n[1–4]\n18\n\n\n5\n101\n1\n[5]\n8\n\n\n\n\n\nTiny Code (Sum Example)\n\nPython\nclass FenwickTree:\n    def __init__(self, n):\n        self.n = n\n        self.bit = [0] * (n + 1)\n\n    def update(self, i, delta):\n        while i &lt;= self.n:\n            self.bit[i] += delta\n            i += i & -i\n\n    def query(self, i):\n        s = 0\n        while i &gt; 0:\n            s += self.bit[i]\n            i -= i & -i\n        return s\n\n    def range_sum(self, l, r):\n        return self.query(r) - self.query(l - 1)\n\n# Example\narr = [2, 4, 5, 7, 8]\nft = FenwickTree(len(arr))\nfor i, val in enumerate(arr, 1):\n    ft.update(i, val)\n\nprint(ft.range_sum(2, 4))  # 4 + 5 + 7 = 16\nOutput:\n16\n\n\nC\n#include &lt;stdio.h&gt;\n\n#define MAXN 100\nint bit[MAXN + 1], n;\n\nvoid update(int i, int delta) {\n    while (i &lt;= n) {\n        bit[i] += delta;\n        i += i & -i;\n    }\n}\n\nint query(int i) {\n    int s = 0;\n    while (i &gt; 0) {\n        s += bit[i];\n        i -= i & -i;\n    }\n    return s;\n}\n\nint range_sum(int l, int r) {\n    return query(r) - query(l - 1);\n}\n\nint main() {\n    n = 5;\n    int arr[] = {0, 2, 4, 5, 7, 8}; // 1-based\n    for (int i = 1; i &lt;= n; i++) update(i, arr[i]);\n    printf(\"Sum [2,4] = %d\\n\", range_sum(2,4)); // 16\n}\nOutput:\nSum [2,4] = 16\n\n\n\nWhy It Matters\n\nElegant bit manipulation for efficient queries\nSimpler and smaller than Segment Trees\nPerfect for prefix sums, inversions, frequency tables\nExtends to 2D Fenwick Trees for grid-based data\nCore in competitive programming, streaming, finance\n\n\n\nIntuition (Least Significant Bit)\nThe LSB trick (i & -i) finds the rightmost set bit, controlling how far we jump. This ensures logarithmic traversal through relevant nodes.\n\n\nTry It Yourself\n\nImplement a prefix XOR version\nAdd range updates with two trees\nExtend to 2D BIT for matrix sums\nVisualize tree structure for array [1..8]\nCompare speed with naive O(n) approach\nTrack frequency counts for elements\nUse it for inversion counting\nCreate a Fenwick Tree class in C++\nHandle point updates interactively\nPractice bit math: draw index cover ranges\n\n\n\nTest Cases\n\n\n\nArray\nQuery\nExpected\n\n\n\n\n[2,4,5,7,8]\nSum(2,4)\n16\n\n\n[1,2,3,4]\nPrefix(3)\n6\n\n\n[5,5,5,5]\nSum(1,3)\n15\n\n\n[3,1,4,2]\nUpdate(2,+3), Sum(1,2)\n7\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild\nO(n log n)\n\n\nQuery\nO(log n)\n\n\nUpdate\nO(log n)\n\n\nSpace\nO(n)\n\n\n\nA Fenwick Tree turns prefix operations into lightning-fast bit magic, simple, small, and powerful.\n\n\n\n184 Interval Tree Search\nAn Interval Tree is a data structure built to efficiently store intervals (ranges like [l, r]) and query all intervals that overlap with a given interval or point. It’s like a BST with range-awareness, enabling fast queries such as “which tasks overlap with time t?” or “which rectangles overlap this region?”\n\nWhat Problem Are We Solving??\nWe want to efficiently find overlapping intervals. A naive search checks all intervals, O(n) per query. An Interval Tree speeds this up to O(log n + k), where k is the number of overlapping intervals.\n\n\nExample\nStored intervals:\n$$5, 20], [10, 30], [12, 15], [17, 19], [30, 40]\nQuery: [14, 16]\nOverlaps: [10, 30], [12, 15]\n\n\nHow It Works (Plain Language)\n\nBuild a BST using the midpoint or start of intervals as keys.\nEach node stores:\n\ninterval [low, high]\nmax endpoint of its subtree\n\nFor queries:\n\nTraverse tree, skip branches where low &gt; query_high or max &lt; query_low.\nCollect overlapping intervals efficiently.\n\n\nThis pruning makes it logarithmic for most cases.\n\n\nExample Tree (Sorted by low)\n            [10, 30]\n           /        \\\n     [5, 20]       [17, 19]\n                     \\\n                    [30, 40]\nEach node stores max endpoint of its subtree.\n\n\nTiny Code (Query Example)\n\nPython\nclass IntervalNode:\n    def __init__(self, low, high):\n        self.low = low\n        self.high = high\n        self.max = high\n        self.left = None\n        self.right = None\n\ndef insert(root, low, high):\n    if root is None:\n        return IntervalNode(low, high)\n    if low &lt; root.low:\n        root.left = insert(root.left, low, high)\n    else:\n        root.right = insert(root.right, low, high)\n    root.max = max(root.max, high)\n    return root\n\ndef overlap(i1, i2):\n    return i1[0] &lt;= i2[1] and i2[0] &lt;= i1[1]\n\ndef search(root, query):\n    if root is None:\n        return []\n    result = []\n    if overlap((root.low, root.high), query):\n        result.append((root.low, root.high))\n    if root.left and root.left.max &gt;= query[0]:\n        result += search(root.left, query)\n    if root.right and root.low &lt;= query[1]:\n        result += search(root.right, query)\n    return result\n\n# Example\nintervals = [(5,20), (10,30), (12,15), (17,19), (30,40)]\nroot = None\nfor l, h in intervals:\n    root = insert(root, l, h)\n\nprint(search(root, (14,16)))  # [(10,30), (12,15)]\nOutput:\n$$(10, 30), (12, 15)]\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int low, high, max;\n    struct Node *left, *right;\n} Node;\n\nNode* newNode(int low, int high) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;low = low;\n    n-&gt;high = high;\n    n-&gt;max = high;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nint max(int a, int b) { return a &gt; b ? a : b; }\n\nNode* insert(Node* root, int low, int high) {\n    if (!root) return newNode(low, high);\n    if (low &lt; root-&gt;low)\n        root-&gt;left = insert(root-&gt;left, low, high);\n    else\n        root-&gt;right = insert(root-&gt;right, low, high);\n    root-&gt;max = max(root-&gt;max, high);\n    return root;\n}\n\nint overlap(int l1, int h1, int l2, int h2) {\n    return l1 &lt;= h2 && l2 &lt;= h1;\n}\n\nvoid search(Node* root, int ql, int qh) {\n    if (!root) return;\n    if (overlap(root-&gt;low, root-&gt;high, ql, qh))\n        printf(\"[%d, %d] overlaps\\n\", root-&gt;low, root-&gt;high);\n    if (root-&gt;left && root-&gt;left-&gt;max &gt;= ql)\n        search(root-&gt;left, ql, qh);\n    if (root-&gt;right && root-&gt;low &lt;= qh)\n        search(root-&gt;right, ql, qh);\n}\n\nint main() {\n    Node* root = NULL;\n    int intervals[][2] = {{5,20},{10,30},{12,15},{17,19},{30,40}};\n    int n = 5;\n    for (int i = 0; i &lt; n; i++)\n        root = insert(root, intervals[i][0], intervals[i][1]);\n    printf(\"Overlaps with [14,16]:\\n\");\n    search(root, 14, 16);\n}\nOutput:\nOverlaps with [14,16]:\n$$10, 30] overlaps\n$$12, 15] overlaps\n\n\n\nWhy It Matters\n\nEfficient for overlap queries (e.g. events, tasks, ranges)\nUsed in:\n\nScheduling (detecting conflicts)\nComputational geometry\nMemory allocation checks\nGenomic range matching\n\nFoundation for Segment Tree with intervals\n\n\n\n\nKey Intuition\nEach node stores the max endpoint of its subtree. This helps prune non-overlapping branches early.\nThink of it as a “range-aware BST”.\n\nTry It Yourself\n\nBuild tree for intervals: [1,5], [2,6], [7,9], [10,15]\nQuery [4,8], which overlap?\nVisualize pruning path\nExtend to delete intervals\nAdd count of overlapping intervals\nImplement iterative search\nCompare with brute-force O(n) approach\nAdapt for point queries only\nTry dynamic updates\nUse to detect meeting conflicts\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nIntervals\nQuery\nExpected Overlaps\n\n\n\n\n[5,20], [10,30], [12,15], [17,19], [30,40]\n[14,16]\n[10,30], [12,15]\n\n\n[1,3], [5,8], [6,10]\n[7,9]\n[5,8], [6,10]\n\n\n[2,5], [6,8]\n[1,1]\nnone\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild\nO(n log n)\n\n\nQuery\nO(log n + k)\n\n\nSpace\nO(n)\n\n\n\nAn Interval Tree is your go-to for range overlap queries, BST elegance meets interval intelligence.\n\n\n\n185 KD-Tree Search\nA KD-Tree (k-dimensional tree) is a space-partitioning data structure that organizes points in k-dimensional space for efficient nearest neighbor, range, and radius searches. It’s like a binary search tree, but it splits space along alternating dimensions.\n\nWhat Problem Are We Solving??\nWe want to:\n\nFind points near a given location\nQuery points within a region or radius\nDo this faster than checking all points (O(n))\n\nA KD-Tree answers such queries in O(log n) (average), versus O(n) for brute-force.\n\n\nExample\nPoints in 2D:\n(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)\nQuery: Nearest neighbor of (9,2) Result: (8,1)\n\n\nHow It Works (Plain Language)\n\nBuild Tree\n\nChoose a splitting dimension (x, y, …)\nPick median point along that axis\nRecursively build left/right subtrees\n\nSearch\n\nCompare query coordinate along current axis\nRecurse into the nearer subtree\nBacktrack to check the other side if necessary (only if the hypersphere crosses boundary)\n\n\nThis pruning makes nearest-neighbor search efficient.\n\n\nExample (2D Split)\n           (7,2)  [split x]\n          /           \\\n    (5,4) [y]         (9,6) [y]\n    /     \\             /\n (2,3)  (4,7)       (8,1)\n\n\nTiny Code (2D Example)\n\nPython\nfrom math import sqrt\n\nclass Node:\n    def __init__(self, point, axis):\n        self.point = point\n        self.axis = axis\n        self.left = None\n        self.right = None\n\ndef build_kdtree(points, depth=0):\n    if not points:\n        return None\n    k = len(points[0])\n    axis = depth % k\n    points.sort(key=lambda p: p[axis])\n    mid = len(points) // 2\n    node = Node(points[mid], axis)\n    node.left = build_kdtree(points[:mid], depth + 1)\n    node.right = build_kdtree(points[mid+1:], depth + 1)\n    return node\n\ndef distance2(a, b):\n    return sum((x - y)  2 for x, y in zip(a, b))\n\ndef nearest(root, target, best=None):\n    if root is None:\n        return best\n    point = root.point\n    if best is None or distance2(point, target) &lt; distance2(best, target):\n        best = point\n    axis = root.axis\n    next_branch = root.left if target[axis] &lt; point[axis] else root.right\n    best = nearest(next_branch, target, best)\n    if (target[axis] - point[axis])  2 &lt; distance2(best, target):\n        other = root.right if next_branch == root.left else root.left\n        best = nearest(other, target, best)\n    return best\n\npoints = [(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)]\ntree = build_kdtree(points)\nprint(nearest(tree, (9,2)))  # (8,1)\nOutput:\n(8, 1)\n\n\n\nC (2D Simplified)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n\ntypedef struct Node {\n    double point[2];\n    int axis;\n    struct Node *left, *right;\n} Node;\n\nint cmpx(const void* a, const void* b) {\n    double* pa = (double*)a;\n    double* pb = (double*)b;\n    return (pa[0] &gt; pb[0]) - (pa[0] &lt; pb[0]);\n}\n\nint cmpy(const void* a, const void* b) {\n    double* pa = (double*)a;\n    double* pb = (double*)b;\n    return (pa[1] &gt; pb[1]) - (pa[1] &lt; pb[1]);\n}\n\ndouble dist2(double a[2], double b[2]) {\n    return (a[0]-b[0])*(a[0]-b[0]) + (a[1]-b[1])*(a[1]-b[1]);\n}\n\n// Simplified build & search omitted for brevity (tree construction similar to Python)\n\n\nWhy It Matters\n\nEfficient for spatial queries in 2D, 3D, etc.\nUsed in:\n\nMachine Learning (KNN classification)\nGraphics (ray tracing, collision detection)\nRobotics (path planning, SLAM)\nDatabases (multi-dimensional indexing)\n\n\n\n\nIntuition\nA KD-Tree is like playing “binary search” in multiple dimensions. Each split narrows down the search region.\n\n\nTry It Yourself\n\nBuild a KD-Tree for points in 2D\nSearch nearest neighbor of (3,5)\nAdd 3D points, use modulo axis split\nVisualize splits as alternating vertical/horizontal lines\nExtend to k-NN (top-k closest)\nAdd radius query (points within r)\nCompare speed to brute-force\nTrack backtrack count for pruning visualization\nTry non-uniform data\nImplement deletion (bonus)\n\n\n\nTest Cases\n\n\n\nPoints\nQuery\nExpected Nearest\n\n\n\n\n(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)\n(9,2)\n(8,1)\n\n\n(1,1),(3,3),(5,5)\n(4,4)\n(3,3)\n\n\n(0,0),(10,10)\n(7,8)\n(10,10)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild\nO(n log n)\n\n\nNearest Query\nO(log n) average\n\n\nWorst Case\nO(n)\n\n\nSpace\nO(n)\n\n\n\nA KD-Tree slices space along dimensions, your go-to for fast nearest neighbor searches in multidimensional worlds.\n\n\n\n186 R-Tree Query\nAn R-Tree is a hierarchical spatial index built for efficiently querying geometric objects (rectangles, polygons, circles) in 2D or higher dimensions. It’s like a B-Tree for rectangles, grouping nearby objects into bounding boxes and organizing them in a tree for fast spatial lookups.\n\nWhat Problem Are We Solving??\nWe need to query spatial data efficiently:\n\n“Which rectangles overlap this area?”\n“What points fall inside this region?”\n“Which shapes intersect this polygon?”\n\nA naive approach checks every object (O(n)). An R-Tree reduces this to O(log n + k) using bounding-box hierarchy.\n\n\nExample\nRectangles:\nA: [1,1,3,3]\nB: [2,2,5,4]\nC: [4,1,6,3]\nQuery: [2.5,2.5,4,4] Overlaps: A, B\n\n\nHow It Works (Plain Language)\n\nStore rectangles (or bounding boxes) as leaves.\nGroup nearby rectangles into Minimum Bounding Rectangles (MBRs).\nBuild hierarchy so each node’s box covers its children.\nQuery by recursively checking nodes whose boxes overlap the query.\n\nThis spatial grouping allows skipping entire regions quickly.\n\n\nExample Tree\n             [1,1,6,4]\n            /         \\\n     [1,1,3,3]       [4,1,6,4]\n       (A,B)            (C)\nQuery [2.5,2.5,4,4]:\n\nIntersects left node → check A, B\nIntersects right node partially → check C (no overlap)\n\n\n\nTiny Code (2D Rectangles)\n\nPython\ndef overlap(a, b):\n    return not (a[2] &lt; b[0] or a[0] &gt; b[2] or a[3] &lt; b[1] or a[1] &gt; b[3])\n\nclass RTreeNode:\n    def __init__(self, box, children=None, is_leaf=False):\n        self.box = box  # [x1, y1, x2, y2]\n        self.children = children or []\n        self.is_leaf = is_leaf\n\ndef search_rtree(node, query):\n    results = []\n    if not overlap(node.box, query):\n        return results\n    if node.is_leaf:\n        for child in node.children:\n            if overlap(child.box, query):\n                results.append(child.box)\n    else:\n        for child in node.children:\n            results.extend(search_rtree(child, query))\n    return results\n\n# Example\nA = RTreeNode([1,1,3,3], is_leaf=True)\nB = RTreeNode([2,2,5,4], is_leaf=True)\nC = RTreeNode([4,1,6,3], is_leaf=True)\n\nleft = RTreeNode([1,1,5,4], [A,B], is_leaf=True)\nright = RTreeNode([4,1,6,3], [C], is_leaf=True)\nroot = RTreeNode([1,1,6,4], [left, right])\n\nquery = [2.5,2.5,4,4]\nprint(search_rtree(root, query))\nOutput:\n$$[1, 1, 3, 3], [2, 2, 5, 4]]\n\n\n\nC (Simplified Query)\n#include &lt;stdio.h&gt;\n\ntypedef struct Box {\n    float x1, y1, x2, y2;\n} Box;\n\nint overlap(Box a, Box b) {\n    return !(a.x2 &lt; b.x1 || a.x1 &gt; b.x2 || a.y2 &lt; b.y1 || a.y1 &gt; b.y2);\n}\n\n// Example: Manual tree simulation omitted for brevity\n\n\nWhy It Matters\n\nIdeal for geospatial databases, mapping, collision detection, and GIS.\nPowers PostGIS, SQLite R*Tree module, spatial indexes.\nHandles overlaps, containment, and range queries.\n\n\n\nIntuition\nR-Trees work by bounding and grouping. Each node is a “container box”, if it doesn’t overlap the query, skip it entirely. This saves massive time in spatial datasets.\n\n\nTry It Yourself\n\nRepresent 2D rectangles with [x1,y1,x2,y2].\nBuild a 2-level tree (group nearby).\nQuery overlap region.\nExtend to 3D bounding boxes.\nImplement insertion using least expansion rule.\nAdd R*-Tree optimization (reinsert on overflow).\nCompare with QuadTree (grid-based).\nVisualize bounding boxes per level.\nImplement nearest neighbor search.\nTry dataset with 10k rectangles, measure speedup.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nRectangles\nQuery\nExpected Overlaps\n\n\n\n\nA[1,1,3,3], B[2,2,5,4], C[4,1,6,3]\n[2.5,2.5,4,4]\nA, B\n\n\nA[0,0,2,2], B[3,3,4,4]\n[1,1,3,3]\nA\n\n\nA[1,1,5,5], B[6,6,8,8]\n[7,7,9,9]\nB\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild\nO(n log n)\n\n\nQuery\nO(log n + k)\n\n\nSpace\nO(n)\n\n\n\nAn R-Tree is your geometric librarian, organizing space into nested rectangles so you can query complex regions fast and clean.\n\n\n\n187 Range Minimum Query (RMQ), Sparse Table Approach\nA Range Minimum Query (RMQ) answers questions like:\n\n“What’s the smallest element between indices L and R?”\n\nIt’s a core subroutine in many algorithms, from LCA (Lowest Common Ancestor) to scheduling, histograms, and segment analysis. The Sparse Table method precomputes answers so each query is O(1) after O(n log n) preprocessing.\n\nWhat Problem Are We Solving??\nGiven an array arr[0..n-1], we want to answer:\nRMQ(L, R) = min(arr[L], arr[L+1], …, arr[R])\nEfficiently, for multiple static queries (no updates).\nNaive approach: O(R-L) per query Sparse Table: O(1) per query after preprocessing.\n\n\nExample\nArray: [2, 5, 1, 4, 9, 3]\n\n\n\nQuery\nResult\n\n\n\n\nRMQ(1, 3)\nmin(5,1,4) = 1\n\n\nRMQ(2, 5)\nmin(1,4,9,3) = 1\n\n\n\n\n\nHow It Works (Plain Language)\n\nPrecompute answers for all intervals of length 2^k.\nTo answer RMQ(L,R):\n\nLet len = R-L+1\nLet k = floor(log2(len))\nCombine two overlapping intervals of size 2^k:\nRMQ(L,R) = min(st[L][k], st[R - 2^k + 1][k])\n\n\nNo updates, so data stays static and queries stay O(1).\n\n\nSparse Table Example\n\n\n\ni\narr[i]\nst[i][0]\nst[i][1]\nst[i][2]\n\n\n\n\n0\n2\n2\nmin(2,5)=2\nmin(2,1)=1\n\n\n1\n5\n5\nmin(5,1)=1\nmin(5,4)=1\n\n\n2\n1\n1\nmin(1,4)=1\nmin(1,9)=1\n\n\n3\n4\n4\nmin(4,9)=4\nmin(4,3)=3\n\n\n4\n9\n9\nmin(9,3)=3\n,\n\n\n5\n3\n3\n,\n,\n\n\n\n\n\nTiny Code\n\nPython\nimport math\n\ndef build_sparse_table(arr):\n    n = len(arr)\n    K = math.floor(math.log2(n)) + 1\n    st = [[0]*K for _ in range(n)]\n\n    for i in range(n):\n        st[i][0] = arr[i]\n\n    j = 1\n    while (1 &lt;&lt; j) &lt;= n:\n        i = 0\n        while i + (1 &lt;&lt; j) &lt;= n:\n            st[i][j] = min(st[i][j-1], st[i + (1 &lt;&lt; (j-1))][j-1])\n            i += 1\n        j += 1\n    return st\n\ndef query(st, L, R):\n    j = int(math.log2(R - L + 1))\n    return min(st[L][j], st[R - (1 &lt;&lt; j) + 1][j])\n\n# Example\narr = [2, 5, 1, 4, 9, 3]\nst = build_sparse_table(arr)\nprint(query(st, 1, 3))  # 1\nprint(query(st, 2, 5))  # 1\nOutput:\n1\n1\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define MAXN 100\n#define LOG 17\n\nint st[MAXN][LOG];\nint arr[MAXN];\nint n;\n\nvoid build() {\n    for (int i = 0; i &lt; n; i++)\n        st[i][0] = arr[i];\n    for (int j = 1; (1 &lt;&lt; j) &lt;= n; j++) {\n        for (int i = 0; i + (1 &lt;&lt; j) &lt;= n; i++) {\n            st[i][j] = (st[i][j-1] &lt; st[i + (1 &lt;&lt; (j-1))][j-1]) \n                       ? st[i][j-1] \n                       : st[i + (1 &lt;&lt; (j-1))][j-1];\n        }\n    }\n}\n\nint query(int L, int R) {\n    int j = log2(R - L + 1);\n    int left = st[L][j];\n    int right = st[R - (1 &lt;&lt; j) + 1][j];\n    return left &lt; right ? left : right;\n}\n\nint main() {\n    n = 6;\n    int arr_temp[] = {2,5,1,4,9,3};\n    for (int i = 0; i &lt; n; i++) arr[i] = arr_temp[i];\n    build();\n    printf(\"RMQ(1,3) = %d\\n\", query(1,3)); // 1\n    printf(\"RMQ(2,5) = %d\\n\", query(2,5)); // 1\n}\nOutput:\nRMQ(1,3) = 1  \nRMQ(2,5) = 1\n\n\n\nWhy It Matters\n\nInstant queries after precomputation\nCrucial for:\n\nSegment analysis (min, max)\nLCA in trees\nSparse range data\nStatic arrays (no updates)\n\nPerfect when array does not change frequently.\n\n\n\nIntuition\nEach table entry st[i][k] stores the minimum of range [i, i + 2^k - 1]. Queries merge two overlapping intervals that cover [L,R].\n\n\nTry It Yourself\n\nBuild table for [1,3,2,7,9,11,3,5,6]\nQuery RMQ(2,6) and RMQ(4,8)\nModify code to compute Range Max Query\nVisualize overlapping intervals for query\nCompare with Segment Tree version\nAdd precomputed log[] for faster lookup\nHandle 1-based vs 0-based indices carefully\nPractice on random arrays\nCompare preprocessing time with naive\nUse to solve LCA using Euler Tour\n\n\n\nTest Cases\n\n\n\nArray\nQuery\nExpected\n\n\n\n\n[2,5,1,4,9,3]\nRMQ(1,3)\n1\n\n\n[2,5,1,4,9,3]\nRMQ(2,5)\n1\n\n\n[1,2,3,4]\nRMQ(0,3)\n1\n\n\n[7,6,5,4,3]\nRMQ(1,4)\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nPreprocess\nO(n log n)\n\n\nQuery\nO(1)\n\n\nSpace\nO(n log n)\n\n\n\nA Sparse Table turns repeated queries into instant lookups, your go-to tool when arrays are static and speed is king.\n\n\n\n188 Mo’s Algorithm\nMo’s Algorithm is a clever offline technique for answering range queries on static arrays in approximately O((n + q)√n) time. It’s ideal when you have many queries like sum, distinct count, frequency, etc., but no updates. Instead of recomputing each query, Mo’s algorithm reuses results smartly by moving the range endpoints efficiently.\n\nWhat Problem Are We Solving??\nWe want to answer multiple range queries efficiently:\n\nGiven an array arr[0..n-1] and q queries [L, R], compute something like sum, count distinct, etc., for each range.\n\nA naive approach is O(n) per query → O(nq) total. Mo’s algorithm cleverly orders queries to achieve O((n + q)√n) total time.\n\n\nExample\nArray: [1, 2, 1, 3, 4, 2, 3] Queries:\n\n[0, 4] → distinct = 4\n[1, 3] → distinct = 3\n[2, 4] → distinct = 3\n\n\n\nHow It Works (Plain Language)\n\nDivide array into blocks of size √n.\nSort queries by:\n\nBlock of L\nR (within block)\n\nMaintain a sliding window [currL, currR):\n\nMove endpoints left/right step-by-step\nUpdate the answer incrementally\n\nStore results per query index.\n\nThe sorting ensures minimal movement between consecutive queries.\n\n\nExample\nIf √n = 3, queries sorted by block:\nBlock 0: [0,4], [1,3]\nBlock 1: [2,4]\nYou move pointers minimally:\n\nFrom [0,4] → [1,3] → [2,4]\nReusing much of previous computation.\n\n\n\nTiny Code (Distinct Count Example)\n\nPython\nimport math\n\ndef mos_algorithm(arr, queries):\n    n = len(arr)\n    q = len(queries)\n    block_size = int(math.sqrt(n))\n\n    # Sort queries\n    queries = sorted(enumerate(queries), key=lambda x: (x[1][0] // block_size, x[1][1]))\n\n    freq = {}\n    currL, currR = 0, 0\n    curr_ans = 0\n    answers = [0]*q\n\n    def add(x):\n        nonlocal curr_ans\n        freq[x] = freq.get(x, 0) + 1\n        if freq[x] == 1:\n            curr_ans += 1\n\n    def remove(x):\n        nonlocal curr_ans\n        freq[x] -= 1\n        if freq[x] == 0:\n            curr_ans -= 1\n\n    for idx, (L, R) in queries:\n        while currL &gt; L:\n            currL -= 1\n            add(arr[currL])\n        while currR &lt;= R:\n            add(arr[currR])\n            currR += 1\n        while currL &lt; L:\n            remove(arr[currL])\n            currL += 1\n        while currR &gt; R + 1:\n            currR -= 1\n            remove(arr[currR])\n        answers[idx] = curr_ans\n\n    return answers\n\n# Example\narr = [1,2,1,3,4,2,3]\nqueries = [(0,4), (1,3), (2,4)]\nprint(mos_algorithm(arr, queries))  # [4,3,3]\nOutput:\n$$4, 3, 3]\n\n\n\nC (Structure and Idea)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define MAXN 100000\n#define MAXQ 100000\n\ntypedef struct { int L, R, idx; } Query;\n\nint arr[MAXN], ans[MAXQ], freq[1000001];\nint curr_ans = 0, block;\n\nint cmp(const void* a, const void* b) {\n    Query *x = (Query*)a, *y = (Query*)b;\n    if (x-&gt;L / block != y-&gt;L / block) return x-&gt;L / block - y-&gt;L / block;\n    return x-&gt;R - y-&gt;R;\n}\n\nvoid add(int x) { if (++freq[x] == 1) curr_ans++; }\nvoid remove_(int x) { if (--freq[x] == 0) curr_ans--; }\n\nint main() {\n    int n = 7, q = 3;\n    int arr_[] = {1,2,1,3,4,2,3};\n    for (int i = 0; i &lt; n; i++) arr[i] = arr_[i];\n\n    Query queries[] = {{0,4,0},{1,3,1},{2,4,2}};\n    block = sqrt(n);\n    qsort(queries, q, sizeof(Query), cmp);\n\n    int currL = 0, currR = 0;\n    for (int i = 0; i &lt; q; i++) {\n        int L = queries[i].L, R = queries[i].R;\n        while (currL &gt; L) add(arr[--currL]);\n        while (currR &lt;= R) add(arr[currR++]);\n        while (currL &lt; L) remove_(arr[currL++]);\n        while (currR &gt; R+1) remove_(arr[--currR]);\n        ans[queries[i].idx] = curr_ans;\n    }\n\n    for (int i = 0; i &lt; q; i++) printf(\"%d \", ans[i]);\n}\nOutput:\n4 3 3\n\n\nWhy It Matters\n\nConverts many range queries into near-linear total time\nIdeal for:\n\nSum / Count / Frequency queries\nDistinct elements\nGCD, XOR, etc. with associative properties\n\nWorks on static arrays (no updates)\n\n\n\nIntuition\nMo’s Algorithm is like sorting your errands by location. By visiting nearby “blocks” first, you minimize travel time. Here, “travel” = pointer movement.\n\n\nTry It Yourself\n\nRun on [1,2,3,4,5] with queries (0,2),(1,4),(2,4)\nChange to sum instead of distinct count\nVisualize pointer movement\nExperiment with block size variations\nAdd offline query index tracking\nTry sqrt decomposition vs Mo’s\nCount frequency of max element per range\nMix different query types (still offline)\nAdd precomputed sqrt(n) block grouping\nUse in competitive programming problems\n\n\n\nTest Cases\n\n\n\nArray\nQueries\nOutput\n\n\n\n\n[1,2,1,3,4,2,3]\n(0,4),(1,3),(2,4)\n[4,3,3]\n\n\n[1,1,1,1]\n(0,3),(1,2)\n[1,1]\n\n\n[1,2,3,4,5]\n(0,2),(2,4)\n[3,3]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nPre-sort Queries\nO(q log q)\n\n\nProcessing\nO((n + q)√n)\n\n\nSpace\nO(n)\n\n\n\nMo’s Algorithm is your range-query workhorse, smartly ordered, block-based, and blazingly efficient for static datasets.\n\n\n\n189 Sweep Line Range Search\nA Sweep Line Algorithm is a geometric technique that processes events in a sorted order along one dimension, typically the x-axis, to solve range, interval, and overlap problems efficiently. Think of it like dragging a vertical line across a 2D plane and updating active intervals as you go.\n\nWhat Problem Are We Solving??\nWe want to efficiently find:\n\nWhich intervals overlap a point\nWhich rectangles intersect\nHow many shapes cover a region\n\nA brute-force check for all pairs is O(n²). A Sweep Line reduces it to O(n log n) by sorting and processing events incrementally.\n\n\nExample\nRectangles:\nR1: [1, 3], R2: [2, 5], R3: [4, 6]\nEvents (sorted by x):\nx = 1: R1 starts  \nx = 2: R2 starts  \nx = 3: R1 ends  \nx = 4: R3 starts  \nx = 5: R2 ends  \nx = 6: R3 ends\nThe active set changes as the line sweeps → track overlaps dynamically.\n\n\nHow It Works (Plain Language)\n\nConvert objects into events\n\nEach interval/rectangle generates start and end events.\n\nSort all events by coordinate (x or y).\nSweep through events:\n\nOn start, add object to active set.\nOn end, remove object.\nAt each step, query active set for intersections, counts, etc.\n\nUse balanced tree / set for active range maintenance.\n\n\n\nExample Walkthrough\nIntervals: [1,3], [2,5], [4,6] Events:\n(1, start), (2, start), (3, end), (4, start), (5, end), (6, end)\nStep-by-step:\n\nAt 1: add [1,3]\nAt 2: add [2,5], overlap detected (1,3) ∩ (2,5)\nAt 3: remove [1,3]\nAt 4: add [4,6], overlap detected (2,5) ∩ (4,6)\nAt 5: remove [2,5]\nAt 6: remove [4,6]\n\nResult: 2 overlapping pairs\n\n\nTiny Code (Interval Overlaps)\n\nPython\ndef sweep_line_intervals(intervals):\n    events = []\n    for l, r in intervals:\n        events.append((l, 'start'))\n        events.append((r, 'end'))\n    events.sort()\n\n    active = 0\n    overlaps = 0\n    for pos, typ in events:\n        if typ == 'start':\n            overlaps += active  # count overlaps\n            active += 1\n        else:\n            active -= 1\n    return overlaps\n\nintervals = [(1,3), (2,5), (4,6)]\nprint(sweep_line_intervals(intervals))  # 2 overlaps\nOutput:\n2\n\n\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int x;\n    int type; // 1 = start, -1 = end\n} Event;\n\nint cmp(const void* a, const void* b) {\n    Event *e1 = (Event*)a, *e2 = (Event*)b;\n    if (e1-&gt;x == e2-&gt;x) return e1-&gt;type - e2-&gt;type;\n    return e1-&gt;x - e2-&gt;x;\n}\n\nint main() {\n    int intervals[][2] = {{1,3},{2,5},{4,6}};\n    int n = 3;\n    Event events[2*n];\n    for (int i = 0; i &lt; n; i++) {\n        events[2*i] = (Event){intervals[i][0], 1};\n        events[2*i+1] = (Event){intervals[i][1], -1};\n    }\n    qsort(events, 2*n, sizeof(Event), cmp);\n\n    int active = 0, overlaps = 0;\n    for (int i = 0; i &lt; 2*n; i++) {\n        if (events[i].type == 1) {\n            overlaps += active;\n            active++;\n        } else {\n            active--;\n        }\n    }\n    printf(\"Total overlaps: %d\\n\", overlaps);\n}\nOutput:\nTotal overlaps: 2\n\n\n\nWhy It Matters\n\nUniversal pattern in computational geometry:\n\nInterval intersection counting\nRectangle overlap\nSegment union length\nPlane sweep algorithms (Voronoi, Convex Hull)\n\nOptimizes from O(n²) to O(n log n)\n\nUsed in:\n\nGIS (geographic data)\nScheduling (conflict detection)\nEvent simulation\n\n\n\nIntuition\nImagine sliding a vertical line across your data: You only “see” the intervals currently active. No need to look back, everything behind is already resolved.\n\n\nTry It Yourself\n\nCount overlaps in [1,4],[2,3],[5,6]\nModify to compute max active intervals (peak concurrency)\nExtend to rectangle intersections (sweep + segment tree)\nTrack total covered length\nCombine with priority queue for dynamic ranges\nVisualize on a timeline (scheduling conflicts)\nApply to meeting room allocation\nExtend to 2D sweep (events sorted by x, active y-ranges)\nCount overlaps per interval\nCompare runtime to brute force\n\n\n\nTest Cases\n\n\n\nIntervals\nOverlaps\n\n\n\n\n[1,3],[2,5],[4,6]\n2\n\n\n[1,2],[3,4]\n0\n\n\n[1,4],[2,3],[3,5]\n3\n\n\n[1,5],[2,4],[3,6]\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nSort Events\nO(n log n)\n\n\nSweep\nO(n)\n\n\nTotal\nO(n log n)\n\n\nSpace\nO(n)\n\n\n\nA Sweep Line algorithm is your moving scanner, it sweeps across time or space, managing active elements, and revealing hidden overlaps in elegant, ordered fashion.\n\n\n\n190 Ball Tree Nearest Neighbor\nA Ball Tree is a hierarchical spatial data structure built from nested hyperspheres (“balls”). It organizes points into clusters based on distance, enabling efficient nearest neighbor and range queries in high-dimensional or non-Euclidean spaces.\nWhile KD-Trees split by axis, Ball Trees split by distance, making them robust when dimensions increase or when the distance metric isn’t axis-aligned.\n\nWhat Problem Are We Solving??\nWe want to efficiently:\n\nFind nearest neighbors for a query point\nPerform radius searches (all points within distance r)\nHandle high-dimensional data where KD-Trees degrade\n\nNaive search is O(n) per query. A Ball Tree improves to roughly O(log n) average.\n\n\nExample\nPoints in 2D:\n(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)\nQuery: (9,2) → Nearest: (8,1)\nInstead of splitting by x/y axis, the Ball Tree groups nearby points by distance from a center point (centroid or median).\n\n\nHow It Works (Plain Language)\n\nBuild Tree recursively:\n\nChoose a pivot (center) (often centroid or median).\nCompute radius (max distance to any point in cluster).\nPartition points into two subsets (inner vs outer ball).\nRecursively build sub-balls.\n\nQuery:\n\nStart at root ball.\nCheck if child balls could contain closer points.\nPrune branches where distance(center, query) - radius &gt; best_dist.\n\n\nThis yields logarithmic average behavior.\n\n\nExample Tree (Simplified)\nBall(center=(6,4), radius=5)\n├── Ball(center=(3,5), radius=2) → [(2,3),(4,7),(5,4)]\n└── Ball(center=(8,3), radius=3) → [(7,2),(8,1),(9,6)]\nQuery (9,2):\n\nCheck root\nCompare both children\nPrune (3,5) (too far)\nSearch (8,3) cluster → nearest (8,1)\n\n\n\nTiny Code (2D Example)\n\nPython\nfrom math import sqrt\n\nclass BallNode:\n    def __init__(self, points):\n        self.center = tuple(sum(x)/len(x) for x in zip(*points))\n        self.radius = max(sqrt(sum((p[i]-self.center[i])2 for i in range(len(p)))) for p in points)\n        self.points = points if len(points) &lt;= 2 else None\n        self.left = None\n        self.right = None\n        if len(points) &gt; 2:\n            points.sort(key=lambda p: sqrt(sum((p[i]-self.center[i])2 for i in range(len(p)))))\n            mid = len(points)//2\n            self.left = BallNode(points[:mid])\n            self.right = BallNode(points[mid:])\n\ndef dist(a, b):\n    return sqrt(sum((x - y)2 for x, y in zip(a, b)))\n\ndef nearest(node, target, best=None):\n    if node is None:\n        return best\n    if node.points is not None:\n        for p in node.points:\n            if best is None or dist(p, target) &lt; dist(best, target):\n                best = p\n        return best\n    d_center = dist(node.center, target)\n    candidates = []\n    if d_center - node.radius &lt;= dist(best, target) if best else True:\n        candidates.append(node.left)\n        candidates.append(node.right)\n    for child in candidates:\n        best = nearest(child, target, best)\n    return best\n\npoints = [(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)]\ntree = BallNode(points)\nprint(nearest(tree, (9,2)))  # (8,1)\nOutput:\n(8, 1)\n\n\n\nC (Structure Idea)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    double cx, cy, radius;\n    struct Node *left, *right;\n    double (*points)[2];\n    int count;\n} Node;\n\n// Building logic: compute centroid, radius, split by distance.\n// Query logic: prune if (dist(center, query) - radius) &gt; best.\nBall Trees are rarely implemented manually in C due to complexity; usually built into libraries (like scikit-learn).\n\n\nWhy It Matters\n\nWorks well for non-axis-aligned data\nHandles high-dimensional and non-Euclidean metrics (e.g. cosine)\nUsed in:\n\nKNN search\nClustering (Ball Tree K-Means)\nSpatial databases\nMachine learning (scikit-learn NearestNeighbors)\n\n\n\n\nIntuition\nImagine wrapping your dataset in nested bubbles. Each bubble encloses a cluster. To find the nearest neighbor, you only open bubbles that could possibly contain closer points.\n\n\nTry It Yourself\n\nBuild a Ball Tree for (1,1),(2,2),(3,3),(10,10)\nQuery (2.5,2.5)\nVisualize nested “balls”\nChange to 3D points\nUse Manhattan distance instead of Euclidean\nCompare performance vs KD-Tree\nTrack pruned branches count\nImplement k-nearest neighbors\nAdd radius search (within distance r)\nHandle duplicate points\n\n\n\nTest Cases\n\n\n\nPoints\nQuery\nNearest\n\n\n\n\n(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)\n(9,2)\n(8,1)\n\n\n(1,1),(2,2),(3,3)\n(2.5,2.5)\n(3,3)\n\n\n(0,0),(10,10)\n(7,8)\n(10,10)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nComplexity\n\n\n\n\nBuild\nO(n log n)\n\n\nQuery (avg)\nO(log n)\n\n\nQuery (worst)\nO(n)\n\n\nSpace\nO(n)\n\n\n\nA Ball Tree is your spherical guide in high-dimensional space, splitting by proximity, not axes, to find neighbors fast and flexibly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-2.html#section-20.-search-optimizations-and-variants",
    "href": "books/en-us/list-2.html#section-20.-search-optimizations-and-variants",
    "title": "Chapter 2. Sorting and searching",
    "section": "Section 20. Search Optimizations and variants",
    "text": "Section 20. Search Optimizations and variants\n\n191 Binary Search with Tolerance\nA Binary Search with Tolerance is a variant of binary search designed for real numbers or floating-point values where exact matches may not exist due to rounding or measurement error.\nInstead of checking for strict equality (==), we accept a range of closeness, defined by a tolerance ε (epsilon).\n\nWhat Problem Are We Solving??\nFor integers, equality is exact. For floating-point numbers, tiny rounding errors make equality unreliable:\nif (arr[mid] == x) // ❌ unreliable for floats\nWe instead test:\nif (fabs(arr[mid] - x) &lt; epsilon) // ✅ tolerance-based match\nThis approach is essential for:\n\nScientific computation\nNumerical analysis\nApproximations\nRoot-finding\nSimulation and measurement data\n\n\n\n\nExample\nGiven sorted real values:\n$$0.1, 0.2, 0.3000000001, 0.4, 0.5]\nSearching for 0.3 with epsilon = 1e-6:\n\nfabs(0.3000000001 - 0.3) &lt; 1e-6 → found!\n\n\nTiny Code\n\n\nC Implementation\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nint binary_search_tolerance(double arr[], int n, double target, double eps) {\n    int low = 0, high = n - 1;\n    while (low &lt;= high) {\n        int mid = (low + high) / 2;\n        double diff = arr[mid] - target;\n        if (fabs(diff) &lt; eps)\n            return mid;  // found within tolerance\n        else if (diff &lt; 0)\n            low = mid + 1;\n        else\n            high = mid - 1;\n    }\n    return -1; // not found\n}\n\nint main() {\n    double arr[] = {0.1, 0.2, 0.3000000001, 0.4, 0.5};\n    int n = 5;\n    double x = 0.3;\n    int idx = binary_search_tolerance(arr, n, x, 1e-6);\n    if (idx != -1)\n        printf(\"Found %.6f at index %d\\n\", x, idx);\n    else\n        printf(\"Not found\\n\");\n}\nOutput:\nFound 0.300000 at index 2\n\n\nPython Implementation\ndef binary_search_tolerance(arr, x, eps=1e-6):\n    lo, hi = 0, len(arr) - 1\n    while lo &lt;= hi:\n        mid = (lo + hi) // 2\n        if abs(arr[mid] - x) &lt; eps:\n            return mid\n        elif arr[mid] &lt; x:\n            lo = mid + 1\n        else:\n            hi = mid - 1\n    return -1\n\narr = [0.1, 0.2, 0.3000000001, 0.4, 0.5]\nprint(binary_search_tolerance(arr, 0.3))  # 2\n\n\nWhy It Matters\n\nAvoids false negatives when comparing floats\nHandles round-off errors gracefully\nUseful in:\n\nRoot-finding\nFloating-point datasets\nPhysics simulations\nNumerical optimization\n\n\n\n\nIntuition\nBinary search assumes exact comparison. With floating-point numbers, “equal” often means “close enough.” ε defines your acceptable margin of error.\nThink of it as:\n\n“If the difference is less than ε, consider it found.”\n\n\n\nTry It Yourself\n\nUse an array [0.1, 0.2, 0.3, 0.4, 0.5] Search for 0.3000001 with ε = 1e-5\nReduce ε → observe when search fails\nTry negative numbers or decimals\nCompare with integer binary search\nExperiment with non-uniform spacing\nModify to find nearest value if not within ε\nVisualize tolerance as a small band around target\nApply in root finding (f(x) ≈ 0)\nAdjust ε dynamically based on scale\nMeasure precision loss with large floats\n\n\n\nTest Cases\n\n\n\nArray\nTarget\nEpsilon\nExpected\n\n\n\n\n[0.1, 0.2, 0.3000000001]\n0.3\n1e-6\nFound\n\n\n[1.0, 2.0, 3.0]\n2.000001\n1e-5\nFound\n\n\n[1.0, 2.0, 3.0]\n2.1\n1e-5\nNot found\n\n\n\n\n\nComplexity\n\n\n\nStep\nComplexity\n\n\n\n\nSearch\nO(log n)\n\n\nSpace\nO(1)\n\n\n\nA Binary Search with Tolerance is a small but essential upgrade when dealing with real numbers—because in floating-point land, “close enough” is often the truth.\n\n\n\n192 Ternary Search\nA Ternary Search is an algorithm for finding the maximum or minimum of a unimodal function—a function that increases up to a point and then decreases (or vice versa). It is a divide-and-conquer method similar to binary search but splits the range into three parts each time.\n\nWhat Problem Are We Solving??\nYou have a function f(x) defined on an interval [l, r], and you know it has one peak (or valley). You want to find the x where f(x) is maximized (or minimized).\nUnlike binary search (which searches for equality), ternary search searches for extremes.\n\n\n\nExample (Unimodal Function)\nLet\nf(x) = - (x - 2)^2 + 4\nThis is a parabola with a maximum at x = 2.\nTernary search gradually narrows the interval around the maximum:\n\nDivide [l, r] into three parts\nEvaluate f(m1) and f(m2)\nKeep the side that contains the peak\nRepeat until small enough\n\n\nHow It Works (Step-by-Step)\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\n1\nPick two midpoints: m1 = l + (r - l) / 3, m2 = r - (r - l) / 3\n\n\n2\nCompare f(m1) and f(m2)\n\n\n3\nIf f(m1) &lt; f(m2) → maximum is in [m1, r]\n\n\n4\nElse → maximum is in [l, m2]\n\n\n5\nRepeat until r - l &lt; ε\n\n\n\n\n\nTiny Code\n\n\nC Implementation (Maximization)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) {\n    return -pow(x - 2, 2) + 4; // peak at x=2\n}\n\ndouble ternary_search(double l, double r, double eps) {\n    while (r - l &gt; eps) {\n        double m1 = l + (r - l) / 3;\n        double m2 = r - (r - l) / 3;\n        if (f(m1) &lt; f(m2))\n            l = m1;\n        else\n            r = m2;\n    }\n    return (l + r) / 2; // approx peak\n}\n\nint main() {\n    double l = 0, r = 4;\n    double res = ternary_search(l, r, 1e-6);\n    printf(\"Approx max at x = %.6f, f(x) = %.6f\\n\", res, f(res));\n}\nOutput:\nApprox max at x = 2.000000, f(x) = 4.000000\n\n\nPython Implementation\ndef f(x):\n    return -(x - 2)2 + 4  # Peak at x=2\n\ndef ternary_search(l, r, eps=1e-6):\n    while r - l &gt; eps:\n        m1 = l + (r - l) / 3\n        m2 = r - (r - l) / 3\n        if f(m1) &lt; f(m2):\n            l = m1\n        else:\n            r = m2\n    return (l + r) / 2\n\nres = ternary_search(0, 4)\nprint(f\"Max at x = {res:.6f}, f(x) = {f(res):.6f}\")\n\n\nWhy It Matters\n\nFinds extrema (max/min) in continuous functions\nNo derivative required (unlike calculus-based optimization)\nWorks when:\n\nf(x) is unimodal\nDomain is continuous\nYou can evaluate f(x) cheaply\n\n\nUsed in:\n\nMathematical optimization\nMachine learning hyperparameter tuning\nGeometry problems (e.g., closest distance)\nPhysics simulations\n\n\n\nTry It Yourself\n\nTry f(x) = (x - 5)^2 + 1 (minimization)\nUse interval [0, 10] and eps = 1e-6\nChange eps to 1e-3 → observe faster but rougher result\nApply to distance between two moving points\nCompare with binary search on derivative\nPlot f(x) to visualize narrowing intervals\nSwitch condition to find minimum\nTest with f(x) = sin(x) on [0, π]\nUse integer search version for discrete arrays\nCombine with golden section search for efficiency\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\nExpected\nType\n\n\n\n\nf(x) = -(x-2)^2 + 4\n[0, 4]\nx ≈ 2\nMaximum\n\n\nf(x) = (x-5)^2 + 1\n[0, 10]\nx ≈ 5\nMinimum\n\n\nf(x) = sin(x)\n[0, 3.14]\nx ≈ 1.57\nMaximum\n\n\n\n\n\nComplexity\n\n\n\nMetric\nValue\n\n\n\n\nTime\nO(log((r - l)/ε))\n\n\nSpace\nO(1)\n\n\n\nA Ternary Search slices the search space into thirds—zooming in on the peak or valley with mathematical precision, no derivatives required.\n\n\n\n193 Hash-Based Search\nA Hash-Based Search uses a hash function to map keys directly to indices in a table, giving constant-time expected lookups. Instead of scanning or comparing elements, it jumps straight to the bucket where the data should be.\n\nWhat Problem Are We Solving??\nWhen searching in a large dataset, linear search is too slow (O(n)) and binary search needs sorted data (O(log n)). Hash-based search lets you find, insert, or delete an item in O(1) average time, regardless of ordering.\nIt’s the foundation of hash tables, hash maps, and dictionaries.\n\n\n\nExample (Simple Lookup)\nSuppose you want to store and search for names quickly:\n$$\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\nA hash function maps each name to an index:\nhash(\"Alice\") → 2\nhash(\"Bob\")   → 5\nhash(\"Carol\") → 1\nYou place each name in its corresponding slot. Searching becomes instant:\nhash(\"Bob\") = 5 → found!\n\nHow It Works (Plain Language)\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\n1\nCompute hash(key) to get an index\n\n\n2\nLook up the bucket at that index\n\n\n3\nIf multiple items hash to same bucket (collision), handle with a strategy (chaining, open addressing)\n\n\n4\nCompare keys if necessary\n\n\n5\nReturn result\n\n\n\n\n\nTiny Code\n\n\nC Implementation (with Linear Probing)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define SIZE 10\n\ntypedef struct {\n    char key[20];\n    int value;\n    int used;\n} Entry;\n\nEntry table[SIZE];\n\nint hash(char *key) {\n    int h = 0;\n    for (int i = 0; key[i]; i++)\n        h = (h * 31 + key[i]) % SIZE;\n    return h;\n}\n\nvoid insert(char *key, int value) {\n    int h = hash(key);\n    while (table[h].used) {\n        if (strcmp(table[h].key, key) == 0) break;\n        h = (h + 1) % SIZE;\n    }\n    strcpy(table[h].key, key);\n    table[h].value = value;\n    table[h].used = 1;\n}\n\nint search(char *key) {\n    int h = hash(key), start = h;\n    while (table[h].used) {\n        if (strcmp(table[h].key, key) == 0)\n            return table[h].value;\n        h = (h + 1) % SIZE;\n        if (h == start) break;\n    }\n    return -1; // not found\n}\n\nint main() {\n    insert(\"Alice\", 10);\n    insert(\"Bob\", 20);\n    printf(\"Value for Bob: %d\\n\", search(\"Bob\"));\n}\nOutput:\nValue for Bob: 20\n\n\nPython Implementation (Built-in Dict)\npeople = {\"Alice\": 10, \"Bob\": 20, \"Carol\": 30}\n\nprint(\"Bob\" in people)       # True\nprint(people[\"Carol\"])       # 30\nPython’s dict uses an optimized open addressing hash table.\n\n\nWhy It Matters\n\nO(1) average lookup, insertion, and deletion\nNo need for sorting\nCore to symbol tables, caches, dictionaries, compilers, and databases\nCan scale with resizing (rehashing)\n\n\n\nTry It Yourself\n\nImplement hash table with chaining using linked lists\nReplace linear probing with quadratic probing\nMeasure lookup time vs. linear search on same dataset\nInsert keys with collisions, ensure correctness\nCreate custom hash for integers: h(x) = x % m\nObserve performance as table fills up (load factor &gt; 0.7)\nImplement delete operation carefully\nResize table when load factor high\nTry a poor hash function (like h=1) and measure slowdown\nCompare with Python’s built-in dict\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nExpected Output\n\n\n\n\nInsert\n(“Alice”, 10)\nStored\n\n\nInsert\n(“Bob”, 20)\nStored\n\n\nSearch\n“Alice”\n10\n\n\nSearch\n“Eve”\nNot found\n\n\nDelete\n“Bob”\nRemoved\n\n\nSearch\n“Bob”\nNot found\n\n\n\n\n\nComplexity\n\n\n\nMetric\nAverage\nWorst Case\n\n\n\n\nSearch\nO(1)\nO(n)\n\n\nInsert\nO(1)\nO(n)\n\n\nSpace\nO(n)\nO(n)\n\n\n\nA Hash-Based Search is like a magic index, it jumps straight to the data you need, turning search into instant lookup.\n\n\n\n194 Bloom Filter Lookup\nA Bloom Filter is a probabilistic data structure that tells you if an element is definitely not in a set or possibly is. It’s super fast and memory efficient, but allows false positives (never false negatives).\n\nWhat Problem Are We Solving??\nWhen working with huge datasets (like URLs, cache keys, or IDs), you may not want to store every element just to check membership. Bloom Filters give you a fast O(1) check:\n\n“Is this element in my set?” → Maybe\n“Is it definitely not in my set?” → Yes\n\nThey’re widely used in databases, network systems, and search engines (e.g., to skip disk lookups).\n\n\n\nExample (Cache Lookup)\nYou have a cache of 1 million items. Before hitting the database, you want to know:\n\nShould I even bother checking cache for this key?\n\nUse a Bloom Filter to quickly tell if the key could be in cache. If filter says “no,” skip lookup entirely.\n\nHow It Works (Plain Language)\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\n1\nCreate a bit array of size m (all zeros)\n\n\n2\nChoose k independent hash functions\n\n\n3\nTo insert an element: compute k hashes, set corresponding bits to 1\n\n\n4\nTo query an element: compute k hashes, check if all bits are 1\n\n\n5\nIf any bit = 0 → definitely not in set\n\n\n6\nIf all bits = 1 → possibly in set (false positive possible)\n\n\n\n\n\nTiny Code\n\n\nPython (Simple Bloom Filter)\nfrom hashlib import sha256\n\nclass BloomFilter:\n    def __init__(self, size=1000, hash_count=3):\n        self.size = size\n        self.hash_count = hash_count\n        self.bits = [0] * size\n\n    def _hashes(self, item):\n        for i in range(self.hash_count):\n            h = int(sha256((item + str(i)).encode()).hexdigest(), 16)\n            yield h % self.size\n\n    def add(self, item):\n        for h in self._hashes(item):\n            self.bits[h] = 1\n\n    def contains(self, item):\n        return all(self.bits[h] for h in self._hashes(item))\n\n# Example usage\nbf = BloomFilter()\nbf.add(\"Alice\")\nprint(bf.contains(\"Alice\"))  # True (probably)\nprint(bf.contains(\"Bob\"))    # False (definitely not)\nOutput:\nTrue\nFalse\n\n\nWhy It Matters\n\nMemory efficient for large sets\nNo false negatives, if it says “no,” you can trust it\nUsed in caches, databases, distributed systems\nReduces I/O by skipping non-existent entries\n\n\n\nTry It Yourself\n\nBuild a Bloom Filter with 1000 bits and 3 hash functions\nInsert 100 random elements\nQuery 10 existing and 10 non-existing elements\nMeasure false positive rate\nExperiment with different m and k values\nIntegrate into a simple cache simulation\nImplement double hashing to reduce hash cost\nCompare memory use vs. Python set\nAdd support for merging filters (bitwise OR)\nTry a Counting Bloom Filter (to support deletions)\n\n\n\nTest Cases\n\n\n\nInput\nExpected Output\n\n\n\n\nadd(“Alice”)\nBits set\n\n\ncontains(“Alice”)\nTrue (maybe)\n\n\ncontains(“Bob”)\nFalse (definitely not)\n\n\nadd(“Bob”)\nBits updated\n\n\ncontains(“Bob”)\nTrue (maybe)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(k)\nO(m)\n\n\nLookup\nO(k)\nO(m)\n\n\n\nk: number of hash functions m: size of bit array\nA Bloom Filter is like a polite doorman, it’ll never wrongly turn you away, but it might let in a stranger once in a while.\n\n\n\n195 Cuckoo Hash Search\nA Cuckoo Hash Table is a clever hash-based structure that guarantees O(1) lookup while avoiding long probe chains. It uses two hash functions and relocates existing keys when a collision occurs, just like a cuckoo bird kicking eggs out of a nest.\n\nWhat Problem Are We Solving??\nTraditional hash tables can degrade to O(n) when collisions pile up. Cuckoo hashing ensures constant-time lookups by guaranteeing every key has at most two possible positions. If both are taken, it kicks out an existing key and re-inserts it elsewhere.\nIt’s widely used in network routers, high-performance caches, and hash-based indexes.\n\n\n\nExample (Small Table)\nSuppose you have 2 hash functions:\nh1(x) = x % 3  \nh2(x) = (x / 3) % 3\nInsert keys: 5, 8, 11\n\n\n\nKey\nh1\nh2\n\n\n\n\n5\n2\n1\n\n\n8\n2\n2\n\n\n11\n2\n1\n\n\n\nWhen inserting 11, slot 2 is full, so we kick out 5 to its alternate position. This continues until every key finds a home.\n\nHow It Works (Plain Language)\n\n\n\nStep\nAction\n\n\n\n\n1\nCompute two hash indices: h1(key), h2(key)\n\n\n2\nTry placing the key in h1 slot\n\n\n3\nIf occupied, evict existing key to its alternate slot\n\n\n4\nRepeat up to a threshold (to prevent infinite loops)\n\n\n5\nIf full cycle detected → rehash with new functions\n\n\n\nEach key lives in either position h1(key) or h2(key).\n\n\nTiny Code\n\n\nC Implementation (Simplified)\n#include &lt;stdio.h&gt;\n\n#define SIZE 7\n\nint table1[SIZE], table2[SIZE];\n\nint h1(int key) { return key % SIZE; }\nint h2(int key) { return (key / SIZE) % SIZE; }\n\nvoid insert(int key, int depth) {\n    if (depth &gt; SIZE) return; // avoid infinite loop\n    int pos1 = h1(key);\n    if (table1[pos1] == 0) {\n        table1[pos1] = key;\n        return;\n    }\n    int displaced = table1[pos1];\n    table1[pos1] = key;\n    int pos2 = h2(displaced);\n    if (table2[pos2] == 0)\n        table2[pos2] = displaced;\n    else\n        insert(displaced, depth + 1);\n}\n\nint search(int key) {\n    return table1[h1(key)] == key || table2[h2(key)] == key;\n}\n\nint main() {\n    insert(10, 0);\n    insert(20, 0);\n    insert(30, 0);\n    printf(\"Search 20: %s\\n\", search(20) ? \"Found\" : \"Not Found\");\n}\nOutput:\nSearch 20: Found\n\n\nPython Implementation\nSIZE = 7\ntable1 = [None] * SIZE\ntable2 = [None] * SIZE\n\ndef h1(x): return x % SIZE\ndef h2(x): return (x // SIZE) % SIZE\n\ndef insert(key, depth=0):\n    if depth &gt; SIZE:\n        return False  # cycle detected\n    pos1 = h1(key)\n    if table1[pos1] is None:\n        table1[pos1] = key\n        return True\n    key, table1[pos1] = table1[pos1], key\n    pos2 = h2(key)\n    if table2[pos2] is None:\n        table2[pos2] = key\n        return True\n    return insert(key, depth + 1)\n\ndef search(key):\n    return table1[h1(key)] == key or table2[h2(key)] == key\n\ninsert(10); insert(20); insert(30)\nprint(\"Search 20:\", search(20))\n\n\nWhy It Matters\n\nO(1) worst-case lookup (always two probes)\nEliminates long collision chains\nGreat for high-performance systems\nDeterministic position = easy debugging\n\n\n\nTry It Yourself\n\nInsert numbers 1–10 and trace movements\nAdd detection for cycles → trigger rehash\nCompare average probe count vs. linear probing\nImplement delete(key) (mark slot empty)\nTry resizing table dynamically\nUse different hash functions for variety\nTrack load factor before rehashing\nStore (key, value) pairs\nBenchmark against chaining\nVisualize movement paths during insertion\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nExpected\n\n\n\n\nInsert\n5\nPlaced\n\n\nInsert\n8\nKicked 5, placed both\n\n\nSearch\n5\nFound\n\n\nSearch\n11\nNot Found\n\n\nDelete\n8\nRemoved\n\n\nSearch\n8\nNot Found\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\nO(1)\nO(n)\n\n\nInsert\nO(1) average\nO(n)\n\n\nDelete\nO(1)\nO(n)\n\n\n\nCuckoo hashing is like musical chairs for keys, when one can’t sit, it makes another stand up and move, but everyone eventually finds a seat.\n\n\n\n196 Robin Hood Hashing\nRobin Hood Hashing is an open addressing strategy that balances fairness in hash table lookups. When two keys collide, the one that’s traveled farther from its “home” index gets to stay, stealing the slot like Robin Hood, who took from the rich and gave to the poor.\n\nWhat Problem Are We Solving??\nStandard linear probing can cause clusters, long runs of occupied slots that slow searches. Robin Hood Hashing reduces variance in probe lengths, so every key has roughly equal access time. This leads to more predictable performance, even at high load factors.\n\n\n\nExample (Collision Handling)\nSuppose hash(x) = x % 10. Insert [10, 20, 30, 21].\n\n\n\nKey\nHash\nPosition\nProbe Distance\n\n\n\n\n10\n0\n0\n0\n\n\n20\n0\n1\n1\n\n\n30\n0\n2\n2\n\n\n21\n1\n2 (collision)\n1\n\n\n\nAt position 2, 21 meets 30 (whose distance = 2). Since 21’s distance (1) is less, it keeps probing. Robin Hood rule: if newcomer has greater or equal distance → swap. This keeps distribution even.\n\nHow It Works (Plain Language)\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\n1\nCompute hash = key % table_size\n\n\n2\nIf slot empty → place item\n\n\n3\nElse, compare probe distance with occupant’s\n\n\n4\nIf new item’s distance ≥ current’s → swap and continue probing displaced item\n\n\n5\nRepeat until inserted\n\n\n\n\n\nTiny Code\n\n\nC Implementation (Simplified)\n#include &lt;stdio.h&gt;\n\n#define SIZE 10\n\ntypedef struct {\n    int key;\n    int used;\n    int distance;\n} Entry;\n\nEntry table[SIZE];\n\nint hash(int key) { return key % SIZE; }\n\nvoid insert(int key) {\n    int index = hash(key);\n    int dist = 0;\n    while (table[index].used) {\n        if (table[index].distance &lt; dist) {\n            int temp_key = table[index].key;\n            int temp_dist = table[index].distance;\n            table[index].key = key;\n            table[index].distance = dist;\n            key = temp_key;\n            dist = temp_dist;\n        }\n        index = (index + 1) % SIZE;\n        dist++;\n    }\n    table[index].key = key;\n    table[index].distance = dist;\n    table[index].used = 1;\n}\n\nint search(int key) {\n    int index = hash(key);\n    int dist = 0;\n    while (table[index].used) {\n        if (table[index].key == key) return 1;\n        if (table[index].distance &lt; dist) return 0;\n        index = (index + 1) % SIZE;\n        dist++;\n    }\n    return 0;\n}\n\nint main() {\n    insert(10);\n    insert(20);\n    insert(30);\n    printf(\"Search 20: %s\\n\", search(20) ? \"Found\" : \"Not Found\");\n}\nOutput:\nSearch 20: Found\n\n\nPython Implementation\nSIZE = 10\ntable = [None] * SIZE\ndistances = [0] * SIZE\n\ndef h(k): return k % SIZE\n\ndef insert(key):\n    index = h(key)\n    dist = 0\n    while table[index] is not None:\n        if distances[index] &lt; dist:\n            table[index], key = key, table[index]\n            distances[index], dist = dist, distances[index]\n        index = (index + 1) % SIZE\n        dist += 1\n    table[index], distances[index] = key, dist\n\ndef search(key):\n    index = h(key)\n    dist = 0\n    while table[index] is not None:\n        if table[index] == key:\n            return True\n        if distances[index] &lt; dist:\n            return False\n        index = (index + 1) % SIZE\n        dist += 1\n    return False\n\ninsert(10); insert(20); insert(30)\nprint(\"Search 20:\", search(20))\n\n\nWhy It Matters\n\nPredictable lookup times, probe lengths nearly uniform\nOutperforms linear probing at high load\nFewer long clusters, more stable table behavior\nGreat for performance-critical systems\n\n\n\nTry It Yourself\n\nInsert [10, 20, 30, 21] and trace swaps\nMeasure probe length variance vs. linear probing\nImplement delete() with tombstone handling\nAdd resizing when load factor &gt; 0.8\nCompare performance with quadratic probing\nVisualize table after 20 inserts\nExperiment with different table sizes\nCreate histogram of probe lengths\nAdd key-value pair storage\nBenchmark search time at 70%, 80%, 90% load\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nExpected\n\n\n\n\nInsert\n10, 20, 30, 21\nEvenly distributed\n\n\nSearch\n20\nFound\n\n\nSearch\n40\nNot Found\n\n\nDelete\n10\nRemoved\n\n\nInsert\n50\nPlaced at optimal position\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\nO(1) avg\nO(n)\n\n\nInsert\nO(1) avg\nO(n)\n\n\nDelete\nO(1) avg\nO(n)\n\n\n\nRobin Hood Hashing keeps everyone honest, no key hoards fast access, and none are left wandering too far.\n\n\n\n197 Jump Consistent Hashing\nJump Consistent Hashing is a lightweight, fast, and deterministic way to assign keys to buckets (servers, shards, or partitions) that minimizes remapping when the number of buckets changes.\nIt’s designed for load balancing in distributed systems, like database shards or cache clusters.\n\nWhat Problem Are We Solving??\nWhen scaling systems horizontally, you often need to assign keys (like user IDs) to buckets (like servers). Naive methods (e.g. key % N) cause massive remapping when N changes. Jump Consistent Hashing avoids that, only a small fraction of keys move when a bucket is added or removed.\nThis ensures stability and predictable redistribution, ideal for distributed caches (Memcached, Redis) and databases (Bigtable, Ceph).\n\n\n\nExample (Adding Buckets)\nSuppose we have keys 1 to 6 and 3 buckets:\n\n\n\nKey\nBucket (3)\n\n\n\n\n1\n2\n\n\n2\n0\n\n\n3\n2\n\n\n4\n1\n\n\n5\n0\n\n\n6\n2\n\n\n\nWhen we add a new bucket (4 total), only a few keys change buckets, most stay where they are. That’s the magic of consistency.\n\nHow It Works (Plain Language)\n\n\n\nStep\nAction\n\n\n\n\n1\nTreat key as a 64-bit integer\n\n\n2\nInitialize b = -1 and j = 0\n\n\n3\nWhile j &lt; num_buckets, update:\n\n\n\nb = j, key = key * 2862933555777941757ULL + 1\n\n\n\nj = floor((b + 1) * (1LL &lt;&lt; 31) / ((key &gt;&gt; 33) + 1))\n\n\n4\nReturn b as bucket index\n\n\n\nIt uses integer arithmetic only, no tables, no storage, just math.\n\n\nTiny Code\n\n\nC Implementation\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n\nint jump_consistent_hash(uint64_t key, int num_buckets) {\n    int64_t b = -1, j = 0;\n    while (j &lt; num_buckets) {\n        b = j;\n        key = key * 2862933555777941757ULL + 1;\n        j = (b + 1) * ((double)(1LL &lt;&lt; 31) / ((key &gt;&gt; 33) + 1));\n    }\n    return (int)b;\n}\n\nint main() {\n    for (uint64_t k = 1; k &lt;= 6; k++)\n        printf(\"Key %llu → Bucket %d\\n\", k, jump_consistent_hash(k, 3));\n}\nOutput:\nKey 1 → Bucket 2  \nKey 2 → Bucket 0  \nKey 3 → Bucket 2  \nKey 4 → Bucket 1  \nKey 5 → Bucket 0  \nKey 6 → Bucket 2\n\n\nPython Implementation\ndef jump_hash(key, num_buckets):\n    b, j = -1, 0\n    while j &lt; num_buckets:\n        b = j\n        key = key * 2862933555777941757 + 1\n        j = int((b + 1) * (1 &lt;&lt; 31) / ((key &gt;&gt; 33) + 1))\n    return b\n\nfor k in range(1, 7):\n    print(f\"Key {k} → Bucket {jump_hash(k, 3)}\")\n\n\nWhy It Matters\n\nStable distribution: minimal remapping on resize\nO(1) time, O(1) space\nWorks great for sharding, load balancing, partitioning\nNo need for external storage or ring structures (unlike consistent hashing rings)\n\n\n\nTry It Yourself\n\nAssign 10 keys across 3 buckets\nAdd a 4th bucket and see which keys move\nCompare with key % N approach\nTry very large bucket counts (10k+)\nBenchmark speed, notice it’s almost constant\nIntegrate with a distributed cache simulation\nTest uniformity (distribution of keys)\nAdd random seeds for per-service variation\nVisualize redistribution pattern\nCompare with Rendezvous Hashing\n\n\n\nTest Cases\n\n\n\nInput\nBuckets\nOutput\n\n\n\n\nKey=1, N=3\n3\n2\n\n\nKey=2, N=3\n3\n0\n\n\nKey=3, N=3\n3\n2\n\n\nKey=1, N=4\n4\n3 or 2 (depends on math)\n\n\n\nOnly a fraction of keys remap when N changes.\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nLookup\nO(1)\nO(1)\n\n\nInsert\nO(1)\nO(1)\n\n\n\nJump Consistent Hashing is like a steady hand, even as your system grows, it keeps most keys right where they belong.\n\n\n\n198 Prefix Search in Trie\nA Trie (prefix tree) is a specialized tree structure that stores strings by their prefixes, enabling fast prefix-based lookups, ideal for autocomplete, dictionaries, and word search engines.\nWith a trie, searching for “app” instantly finds “apple”, “apply”, “appetite”, etc.\n\nWhat Problem Are We Solving??\nTraditional data structures like arrays or hash tables can’t efficiently answer questions like:\n\n“List all words starting with pre”\n“Does any word start with tri?”\n\nA trie organizes data by prefix paths, making such queries fast and natural, often O(k) where k is the prefix length.\n\n\n\nExample (Words: app, apple, bat)\nThe trie looks like this:\n(root)\n ├─ a\n │  └─ p\n │     └─ p *\n │        └─ l\n │           └─ e *\n └─ b\n    └─ a\n       └─ t *\nStars (*) mark word endings. Search “app” → found; list all completions → “app”, “apple”.\n\nHow It Works (Plain Language)\n\n\n\nStep\nAction\n\n\n\n\n1\nEach node represents a character\n\n\n2\nA path from root to node represents a prefix\n\n\n3\nWhen inserting, follow characters and create nodes as needed\n\n\n4\nMark end of word when reaching last character\n\n\n5\nTo search prefix: traverse nodes character by character\n\n\n6\nIf all exist → prefix found; else → not found\n\n\n\n\n\nTiny Code\n\n\nC Implementation (Simplified)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define ALPHABET 26\n\ntypedef struct TrieNode {\n    struct TrieNode *child[ALPHABET];\n    bool isEnd;\n} TrieNode;\n\nTrieNode* newNode() {\n    TrieNode* node = calloc(1, sizeof(TrieNode));\n    node-&gt;isEnd = false;\n    return node;\n}\n\nvoid insert(TrieNode *root, const char *word) {\n    for (int i = 0; word[i]; i++) {\n        int idx = word[i] - 'a';\n        if (!root-&gt;child[idx]) root-&gt;child[idx] = newNode();\n        root = root-&gt;child[idx];\n    }\n    root-&gt;isEnd = true;\n}\n\nbool startsWith(TrieNode *root, const char *prefix) {\n    for (int i = 0; prefix[i]; i++) {\n        int idx = prefix[i] - 'a';\n        if (!root-&gt;child[idx]) return false;\n        root = root-&gt;child[idx];\n    }\n    return true;\n}\n\nint main() {\n    TrieNode *root = newNode();\n    insert(root, \"app\");\n    insert(root, \"apple\");\n    printf(\"Starts with 'ap': %s\\n\", startsWith(root, \"ap\") ? \"Yes\" : \"No\");\n}\nOutput:\nStarts with 'ap': Yes\n\n\nPython Implementation\nclass TrieNode:\n    def __init__(self):\n        self.children = {}\n        self.is_end = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        node = self.root\n        for ch in word:\n            if ch not in node.children:\n                node.children[ch] = TrieNode()\n            node = node.children[ch]\n        node.is_end = True\n\n    def starts_with(self, prefix):\n        node = self.root\n        for ch in prefix:\n            if ch not in node.children:\n                return False\n            node = node.children[ch]\n        return True\n\ntrie = Trie()\ntrie.insert(\"apple\")\ntrie.insert(\"app\")\nprint(trie.starts_with(\"ap\"))  # True\n\n\nWhy It Matters\n\nO(k) prefix lookup (k = prefix length)\nPerfect for autocomplete, spell checkers, search engines\nStores shared prefixes efficiently\nCan be extended for frequency, weights, or wildcard matching\n\n\n\nTry It Yourself\n\nInsert [“app”, “apple”, “apply”, “apt”]\nSearch prefix “ap” → expect True\nSearch prefix “ba” → expect False\nAdd function to list all words starting with a prefix\nImplement delete(word)\nAdd frequency count at each node\nSupport uppercase letters\nStore end-of-word markers clearly\nCompare memory use vs hash table\nExtend to autocomplete feature returning top-N completions\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\n\n\n\n\nInsert\n“app”\nStored\n\n\nInsert\n“apple”\nStored\n\n\nstartsWith\n“ap”\nTrue\n\n\nstartsWith\n“ba”\nFalse\n\n\nstartsWith\n“app”\nTrue\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(k)\nO(k)\n\n\nSearch Prefix\nO(k)\nO(1)\n\n\nSpace (n words)\nO(sum of all characters)\n,\n\n\n\nA Trie turns your data into a map of words, every branch is a path toward meaning, every prefix a shortcut to discovery.\n\n\n\n199 Pattern Search in Suffix Array\nA Suffix Array is a sorted list of all suffixes of a string. It enables fast substring searches, perfect for pattern matching in text editors, DNA analysis, and search engines.\nBy combining it with binary search, you can find whether a pattern appears in a string in O(m log n) time.\n\nWhat Problem Are We Solving??\nGiven a large text T (like \"banana\") and a pattern P (like \"ana\"), we want to quickly check if P exists in T. Naive search takes O(nm) (compare every position). A suffix array lets us search more efficiently by working on a pre-sorted list of suffixes.\n\n\n\nExample (Text = “banana”)\nList all suffixes and sort them:\n\n\n\nIndex\nSuffix\n\n\n\n\n0\nbanana\n\n\n1\nanana\n\n\n2\nnana\n\n\n3\nana\n\n\n4\nna\n\n\n5\na\n\n\n\nSorted suffixes:\n\n\n\nSA Index\nSuffix\nOriginal Position\n\n\n\n\n5\na\n5\n\n\n3\nana\n3\n\n\n1\nanana\n1\n\n\n0\nbanana\n0\n\n\n4\nna\n4\n\n\n2\nnana\n2\n\n\n\nNow search \"ana\" using binary search over these sorted suffixes.\n\nHow It Works (Plain Language)\n\n\n\nStep\nAction\n\n\n\n\n1\nBuild suffix array = all suffixes sorted lexicographically\n\n\n2\nUse binary search to find lower/upper bounds for pattern\n\n\n3\nCompare only m characters per comparison\n\n\n4\nIf found → pattern occurs at suffix index\n\n\n5\nOtherwise → not in text\n\n\n\n\n\nTiny Code\n\n\nC Implementation (Simplified)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\nint cmp(const void *a, const void *b, void *txt) {\n    int i = *(int*)a, j = *(int*)b;\n    return strcmp((char*)txt + i, (char*)txt + j);\n}\n\nvoid build_suffix_array(char *txt, int n, int sa[]) {\n    for (int i = 0; i &lt; n; i++) sa[i] = i;\n    qsort_r(sa, n, sizeof(int), cmp, txt);\n}\n\nint binary_search_suffix(char *txt, int sa[], int n, char *pat) {\n    int l = 0, r = n - 1;\n    while (l &lt;= r) {\n        int mid = (l + r) / 2;\n        int res = strncmp(pat, txt + sa[mid], strlen(pat));\n        if (res == 0) return sa[mid];\n        if (res &lt; 0) r = mid - 1;\n        else l = mid + 1;\n    }\n    return -1;\n}\n\nint main() {\n    char txt[] = \"banana\";\n    int n = strlen(txt), sa[n];\n    build_suffix_array(txt, n, sa);\n    char pat[] = \"ana\";\n    int pos = binary_search_suffix(txt, sa, n, pat);\n    if (pos &gt;= 0) printf(\"Pattern found at %d\\n\", pos);\n    else printf(\"Pattern not found\\n\");\n}\nOutput:\nPattern found at 1\n\n\nPython Implementation\ndef build_suffix_array(s):\n    return sorted(range(len(s)), key=lambda i: s[i:])\n\ndef search(s, sa, pat):\n    l, r = 0, len(sa) - 1\n    while l &lt;= r:\n        mid = (l + r) // 2\n        suffix = s[sa[mid]:]\n        if suffix.startswith(pat):\n            return sa[mid]\n        if suffix &lt; pat:\n            l = mid + 1\n        else:\n            r = mid - 1\n    return -1\n\ns = \"banana\"\nsa = build_suffix_array(s)\nprint(\"Suffix Array:\", sa)\nprint(\"Search 'ana':\", search(s, sa, \"ana\"))\nOutput:\nSuffix Array: [5, 3, 1, 0, 4, 2]\nSearch 'ana': 1\n\n\nWhy It Matters\n\nSubstring search in O(m log n)\nSpace-efficient alternative to suffix trees\nGreat for full-text search, DNA sequencing, plagiarism detection\nCan be extended with LCP array for longest common prefix queries\n\n\n\nTry It Yourself\n\nBuild suffix array for \"banana\"\nSearch \"na\", \"ban\", \"apple\"\nPrint all suffixes for visualization\nAdd LCP array for faster repeated queries\nCompare with KMP algorithm speed\nUse binary search manually to trace comparisons\nExtend to count occurrences of pattern\nTry longer text (e.g. “mississippi”)\nImplement in-place quicksort for SA\nBenchmark vs. naive substring search\n\n\n\nTest Cases\n\n\n\nText\nPattern\nExpected Result\n\n\n\n\n“banana”\n“ana”\nFound at 1\n\n\n“banana”\n“ban”\nFound at 0\n\n\n“banana”\n“na”\nFound at 2 or 4\n\n\n“banana”\n“cat”\nNot Found\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild SA\nO(n log n)\nO(n)\n\n\nSearch\nO(m log n)\nO(1)\n\n\n\nA Suffix Array is like a library index, once sorted, every search becomes as simple as flipping to the right page.\n\n\n\n200 Search in Infinite Array\nA Search in Infinite Array (or unbounded array) is a technique for finding an element in a sorted list of unknown length. You can’t directly use binary search because you don’t know n, so you must first find a search bound, then perform binary search within it.\nThis idea is crucial for systems where data is streamed or dynamically sized, like logs, unrolled lists, or file scans.\n\nWhat Problem Are We Solving??\nIf you’re given an array-like interface (like get(i)), but no size n, how do you find target efficiently? You can’t do linear search, it could be infinite. The trick: exponential search, grow bounds exponentially until you pass the target, then apply binary search in that window.\n\n\n\nExample\nGiven sorted sequence:\n$$3, 5, 9, 12, 17, 23, 31, 45, 67, 88, 100, ...]\nSearch for 31:\n\nStart with low = 0, high = 1\nWhile arr[high] &lt; 31, double high\nhigh = 1 → 2 → 4 → 8\nNow arr[8] = 67 &gt; 31, so search range is [4, 8]\nPerform binary search in [4, 8]\nFound at index 6\n\n\nHow It Works (Plain Language)\n\n\n\nStep\nAction\n\n\n\n\n1\nStart with low = 0, high = 1\n\n\n2\nWhile arr[high] &lt; target, set low = high, high *= 2\n\n\n3\nNow the target lies between low and high\n\n\n4\nPerform standard binary search in [low, high]\n\n\n5\nReturn index if found, else -1\n\n\n\n\n\nTiny Code\n\n\nC Implementation (Simulated Infinite Array)\n#include &lt;stdio.h&gt;\n\nint get(int arr[], int size, int i) {\n    if (i &gt;= size) return 1e9; // simulate infinity\n    return arr[i];\n}\n\nint binary_search(int arr[], int low, int high, int target, int size) {\n    while (low &lt;= high) {\n        int mid = low + (high - low) / 2;\n        int val = get(arr, size, mid);\n        if (val == target) return mid;\n        if (val &lt; target) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1;\n}\n\nint search_infinite(int arr[], int size, int target) {\n    int low = 0, high = 1;\n    while (get(arr, size, high) &lt; target) {\n        low = high;\n        high *= 2;\n    }\n    return binary_search(arr, low, high, target, size);\n}\n\nint main() {\n    int arr[] = {3, 5, 9, 12, 17, 23, 31, 45, 67, 88, 100};\n    int size = sizeof(arr)/sizeof(arr[0]);\n    int idx = search_infinite(arr, size, 31);\n    printf(\"Found at index %d\\n\", idx);\n}\nOutput:\nFound at index 6\n\n\nPython Implementation\ndef get(arr, i):\n    return arr[i] if i &lt; len(arr) else float('inf')\n\ndef binary_search(arr, low, high, target):\n    while low &lt;= high:\n        mid = (low + high) // 2\n        val = get(arr, mid)\n        if val == target:\n            return mid\n        if val &lt; target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n\ndef search_infinite(arr, target):\n    low, high = 0, 1\n    while get(arr, high) &lt; target:\n        low = high\n        high *= 2\n    return binary_search(arr, low, high, target)\n\narr = [3, 5, 9, 12, 17, 23, 31, 45, 67, 88, 100]\nprint(\"Found at index:\", search_infinite(arr, 31))\nOutput:\nFound at index: 6\n\n\nWhy It Matters\n\nWorks with streams, linked storage, APIs, or infinite generators\nAvoids full traversal, logarithmic growth\nCombines exploration (finding bounds) with binary search (exact match)\nIdeal for search engines, log readers, cloud data paging\n\n\n\nTry It Yourself\n\nSearch [1, 3, 5, 9, 12, 20] for 9\nSearch [2, 4, 8, 16, 32, 64] for 33 (not found)\nCount number of get() calls, compare to linear search\nTry target smaller than first element\nHandle edge cases: empty array, target &gt; max\nSimulate infinite stream with get()\nReplace doubling with high = low + step for adaptive growth\nVisualize search window expansion\nGeneralize to descending arrays\nCompare performance vs naive scan\n\n\n\nTest Cases\n\n\n\nInput Array\nTarget\nOutput\n\n\n\n\n[3, 5, 9, 12, 17, 23, 31, 45]\n31\n6\n\n\n[3, 5, 9, 12, 17, 23, 31, 45]\n4\n-1\n\n\n[1, 2, 4, 8, 16]\n8\n3\n\n\n[10, 20, 30]\n40\n-1\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\nO(log p)\nO(1)\n\n\n\np = position of target in array\nSearching an infinite array feels like navigating a foggy road, first find your headlights (bounds), then drive straight to the target.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 2. Sorting and searching</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html",
    "href": "books/en-us/list-3.html",
    "title": "Chapter 3. Data Structure in Action",
    "section": "",
    "text": "Section 21. Arrays, linked lists, stacks, queues",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-21.-arrays-linked-lists-stacks-queues",
    "href": "books/en-us/list-3.html#section-21.-arrays-linked-lists-stacks-queues",
    "title": "Chapter 3. Data Structure in Action",
    "section": "",
    "text": "201 Dynamic Array Resize\nDynamic array resizing is how we keep push operations fast while growing storage as needed. The idea is simple: when the array is full, allocate a bigger one, copy the elements, and keep going. If we double the capacity on each resize, the average cost per push stays constant.\n\nWhat Problem Are We Solving?\nA fixed array has a fixed size. Real programs do not always know n in advance. We want an array that supports push at end with near constant time, grows automatically, and keeps elements in contiguous memory for cache friendliness.\nGoal: Provide push, pop, get, set on a resizable array with amortized O(1) time for push.\n\n\nHow Does It Work (Plain Language)?\nKeep two numbers: size and capacity.\n\nStart with a small capacity (for example, 1 or 8).\nOn push, if size &lt; capacity, write the element and increase size.\nIf size == capacity, allocate new storage with capacity doubled, copy old elements, release the old block, then push.\nOptionally, if many pops reduce size far below capacity, shrink by halving to avoid wasted space.\n\nWhy doubling? Doubling keeps the number of costly resizes small. Most pushes are cheap writes. Only occasionally do we pay for copying.\nExample Steps (Growth Simulation)\n\n\n\n\n\n\n\n\n\n\n\nStep\nSize Before\nCapacity Before\nAction\nSize After\nCapacity After\n\n\n\n\n1\n0\n1\npush(1)\n1\n1\n\n\n2\n1\n1\nfull → resize to 2, copy 1 element, push(2)\n2\n2\n\n\n3\n2\n2\nfull → resize to 4, copy 2 elements, push(3)\n3\n4\n\n\n4\n3\n4\npush(4)\n4\n4\n\n\n5\n4\n4\nfull → resize to 8, copy 4 elements, push(5)\n5\n8\n\n\n\nNotice how capacity doubles occasionally while most pushes cost O(1).\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\ntypedef struct {\n    size_t size;\n    size_t capacity;\n    int *data;\n} DynArray;\n\nint da_init(DynArray *a, size_t init_cap) {\n    if (init_cap == 0) init_cap = 1;\n    a-&gt;data = (int *)malloc(init_cap * sizeof(int));\n    if (!a-&gt;data) return 0;\n    a-&gt;size = 0;\n    a-&gt;capacity = init_cap;\n    return 1;\n}\n\nint da_resize(DynArray *a, size_t new_cap) {\n    int *p = (int *)realloc(a-&gt;data, new_cap * sizeof(int));\n    if (!p) return 0;\n    a-&gt;data = p;\n    a-&gt;capacity = new_cap;\n    return 1;\n}\n\nint da_push(DynArray *a, int x) {\n    if (a-&gt;size == a-&gt;capacity) {\n        size_t new_cap = a-&gt;capacity * 2;\n        if (!da_resize(a, new_cap)) return 0;\n    }\n    a-&gt;data[a-&gt;size++] = x;\n    return 1;\n}\n\nint da_pop(DynArray *a, int *out) {\n    if (a-&gt;size == 0) return 0;\n    *out = a-&gt;data[--a-&gt;size];\n    if (a-&gt;capacity &gt; 1 && a-&gt;size &lt;= a-&gt;capacity / 4) {\n        size_t new_cap = a-&gt;capacity / 2;\n        if (new_cap == 0) new_cap = 1;\n        da_resize(a, new_cap);\n    }\n    return 1;\n}\n\nint main(void) {\n    DynArray a;\n    if (!da_init(&a, 1)) return 1;\n    for (int i = 0; i &lt; 10; ++i) da_push(&a, i);\n    for (size_t i = 0; i &lt; a.size; ++i) printf(\"%d \", a.data[i]);\n    printf(\"\\nsize=%zu cap=%zu\\n\", a.size, a.capacity);\n    free(a.data);\n    return 0;\n}\nPython\nclass DynArray:\n    def __init__(self, init_cap=1):\n        self._cap = max(1, init_cap)\n        self._n = 0\n        self._data = [None] * self._cap\n\n    def _resize(self, new_cap):\n        new = [None] * new_cap\n        for i in range(self._n):\n            new[i] = self._data[i]\n        self._data = new\n        self._cap = new_cap\n\n    def push(self, x):\n        if self._n == self._cap:\n            self._resize(self._cap * 2)\n        self._data[self._n] = x\n        self._n += 1\n\n    def pop(self):\n        if self._n == 0:\n            raise IndexError(\"pop from empty DynArray\")\n        self._n -= 1\n        x = self._data[self._n]\n        self._data[self._n] = None\n        if self._cap &gt; 1 and self._n &lt;= self._cap // 4:\n            self._resize(max(1, self._cap // 2))\n        return x\n\n    def __getitem__(self, i):\n        if not 0 &lt;= i &lt; self._n:\n            raise IndexError(\"index out of range\")\n        return self._data[i]\n\n\nWhy It Matters\n\nAmortized O(1) push for dynamic growth\nContiguous memory for cache performance\nSimple API that underlies vectors, array lists, and many scripting language arrays\nBalanced memory usage via optional shrinking\n\n\n\nA Gentle Proof (Why It Works)\nAccounting view for doubling Charge each push a constant credit (e.g. 3 units).\n\nA normal push costs 1 unit and stores 2 credits with the element.\nWhen resizing from capacity k → 2k, copying k elements costs k units, paid by saved credits.\nThe new element pays its own 1 unit.\n\nThus amortized cost per push is O(1).\nGrowth factor choice Any factor &gt; 1 gives amortized O(1).\n\nDoubling → fewer resizes, more memory slack\n1.5x → less slack, more frequent copies\nToo small (&lt;1.2) → breaks amortized bound\n\n\n\nTry It Yourself\n\nSimulate pushes 1..32 for factors 2.0 and 1.5. Count resizes.\nAdd reserve(n) to preallocate capacity.\nImplement shrinking when size &lt;= capacity / 4.\nReplace int with a struct, measure copy cost.\nUse potential method: Φ = 2·size − capacity.\n\n\n\nTest Cases\n\n\n\nOperation Sequence\nCapacity Trace (Factor 2)\nNotes\n\n\n\n\npush 1..8\n1 → 2 → 4 → 8\nresize at 1, 2, 4\n\n\npush 9\n8 → 16\nresize before write\n\n\npush 10..16\n16\nno resize\n\n\npop 16..9\n16\nshrinking optional\n\n\npop 8\n16 → 8\nshrink at 25% load\n\n\n\nEdge Cases\n\nPush on full array triggers one resize before write\nPop on empty array should error or return false\nreserve(n) larger than current capacity must preserve data\nShrink never reduces capacity below size\n\n\n\nComplexity\n\nTime:\n\npush amortized O(1)\npush worst case O(n) on resize\npop amortized O(1)\nget/set O(1)\n\nSpace: O(n), capacity ≤ constant × size\n\nDynamic array resizing turns a rigid array into a flexible container. Grow when needed, copy occasionally, and enjoy constant-time pushes on average.\n\n\n\n202 Circular Array Implementation\nA circular array (or ring buffer) stores elements in a fixed-size array while allowing wrap-around indexing. It is perfect for implementing queues and buffers where old data is overwritten or processed in a first-in-first-out manner.\n\nWhat Problem Are We Solving?\nA regular array wastes space if we only move front and rear pointers forward. A circular array solves this by wrapping indices around when they reach the end, so all slots can be reused without shifting elements.\nGoal: Efficiently support enqueue, dequeue, peek, and size in O(1) time using a fixed-size buffer with wrap-around indexing.\n\n\nHow Does It Work (Plain Language)?\nMaintain:\n\nfront: index of the first element\nrear: index of the next free slot\ncount: number of elements in the buffer\ncapacity: maximum number of elements\n\nWrap-around indexing uses modulo arithmetic:\nnext_index = (current_index + 1) % capacity\nWhen adding or removing, always increment front or rear with this rule.\nExample Steps (Wrap-around Simulation)\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nFront\nRear\nCount\nArray State\nNote\n\n\n\n\n1\nenqueue(10)\n0\n1\n1\n[10, , , _]\nrear advanced\n\n\n2\nenqueue(20)\n0\n2\n2\n[10, 20, , ]\n\n\n\n3\nenqueue(30)\n0\n3\n3\n[10, 20, 30, _]\n\n\n\n4\ndequeue()\n1\n3\n2\n[, 20, 30, ]\nfront advanced\n\n\n5\nenqueue(40)\n1\n0\n3\n[40, 20, 30, _]\nwrap-around\n\n\n6\nenqueue(50)\n1\n1\n4\n[40, 20, 30, 50]\nfull queue\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int front;\n    int rear;\n    int count;\n    int capacity;\n} CircularQueue;\n\nint cq_init(CircularQueue *q, int capacity) {\n    q-&gt;data = malloc(capacity * sizeof(int));\n    if (!q-&gt;data) return 0;\n    q-&gt;capacity = capacity;\n    q-&gt;front = 0;\n    q-&gt;rear = 0;\n    q-&gt;count = 0;\n    return 1;\n}\n\nint cq_enqueue(CircularQueue *q, int x) {\n    if (q-&gt;count == q-&gt;capacity) return 0; // full\n    q-&gt;data[q-&gt;rear] = x;\n    q-&gt;rear = (q-&gt;rear + 1) % q-&gt;capacity;\n    q-&gt;count++;\n    return 1;\n}\n\nint cq_dequeue(CircularQueue *q, int *out) {\n    if (q-&gt;count == 0) return 0; // empty\n    *out = q-&gt;data[q-&gt;front];\n    q-&gt;front = (q-&gt;front + 1) % q-&gt;capacity;\n    q-&gt;count--;\n    return 1;\n}\n\nint main(void) {\n    CircularQueue q;\n    cq_init(&q, 4);\n    cq_enqueue(&q, 10);\n    cq_enqueue(&q, 20);\n    cq_enqueue(&q, 30);\n    int val;\n    cq_dequeue(&q, &val);\n    cq_enqueue(&q, 40);\n    cq_enqueue(&q, 50); // should fail if capacity=4\n    printf(\"Front value: %d\\n\", q.data[q.front]);\n    free(q.data);\n    return 0;\n}\nPython\nclass CircularQueue:\n    def __init__(self, capacity):\n        self._cap = capacity\n        self._data = [None] * capacity\n        self._front = 0\n        self._rear = 0\n        self._count = 0\n\n    def enqueue(self, x):\n        if self._count == self._cap:\n            raise OverflowError(\"queue full\")\n        self._data[self._rear] = x\n        self._rear = (self._rear + 1) % self._cap\n        self._count += 1\n\n    def dequeue(self):\n        if self._count == 0:\n            raise IndexError(\"queue empty\")\n        x = self._data[self._front]\n        self._front = (self._front + 1) % self._cap\n        self._count -= 1\n        return x\n\n    def peek(self):\n        if self._count == 0:\n            raise IndexError(\"queue empty\")\n        return self._data[self._front]\n\n\nWhy It Matters\n\nEnables constant time queue operations\nNo element shifting needed\nEfficient use of fixed memory\nBackbone for circular buffers, task schedulers, streaming pipelines, and audio/video buffers\n\n\n\nA Gentle Proof (Why It Works)\nModulo indexing ensures positions loop back when reaching array end. If capacity is n, then all indices stay in [0, n-1]. No overflow occurs since front, rear always wrap around. The condition count == capacity detects full queue, and count == 0 detects empty.\nThus each operation touches only one element and updates O(1) variables.\n\n\nTry It Yourself\n\nImplement a circular buffer with overwrite (old data replaced).\nAdd is_full() and is_empty() helpers.\nSimulate producer-consumer with one enqueue per producer tick and one dequeue per consumer tick.\nExtend to store structs instead of integers.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nOperation Sequence\nFront\nRear\nCount\nArray State\nNotes\n\n\n\n\nenqueue(10)\n0\n1\n1\n[10, , , _]\nnormal push\n\n\nenqueue(20)\n0\n2\n2\n[10, 20, , ]\n\n\n\nenqueue(30)\n0\n3\n3\n[10, 20, 30, _]\n\n\n\ndequeue()\n1\n3\n2\n[, 20, 30, ]\n\n\n\nenqueue(40)\n1\n0\n3\n[40, 20, 30, _]\nwrap-around\n\n\nenqueue(50)\n1\n1\n4\n[40, 20, 30, 50]\nfull\n\n\n\nEdge Cases\n\nEnqueue when full → error or overwrite\nDequeue when empty → error\nModulo indexing avoids overflow\nWorks with any capacity ≥ 1\n\n\n\nComplexity\n\nTime: O(1) for enqueue, dequeue, peek\nSpace: O(n) fixed buffer\n\nCircular arrays provide elegant constant-time queues with wrap-around indexing, a small trick that powers big systems.\n\n\n\n203 Singly Linked List Insert/Delete\nA singly linked list is a chain of nodes, each pointing to the next. It grows and shrinks dynamically without preallocating memory. Operations like insert and delete rely on pointer adjustments rather than shifting elements.\n\nWhat Problem Are We Solving?\nStatic arrays are fixed-size and require shifting elements for insertions or deletions in the middle. Singly linked lists solve this by connecting elements through pointers, allowing efficient O(1) insertion and deletion (given a node reference).\nGoal: Support insert, delete, and traversal on a structure that can grow dynamically without reallocating or shifting elements.\n\n\nHow Does It Work (Plain Language)?\nEach node stores two fields:\n\ndata (the value)\nnext (pointer to the next node)\n\nThe list has a head pointer to the first node. Insertion and deletion work by updating the next pointers.\nExample Steps (Insertion at Position)\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nNode Updated\nBefore\nAfter\nNotes\n\n\n\n\n1\ncreate list\n-\nempty\nhead = NULL\nlist starts empty\n\n\n2\ninsert(10) at head\nnew node\nNULL\n[10]\nhead → 10\n\n\n3\ninsert(20) at head\nnew node\n[10]\n[20 → 10]\nhead updated\n\n\n4\ninsert(30) after 20\nnew node\n[20 → 10]\n[20 → 30 → 10]\npointer rerouted\n\n\n5\ndelete(30)\nnode 20\n[20 → 30 → 10]\n[20 → 10]\nbypass removed node\n\n\n\nKey idea: To insert, point the new node’s next to the target’s next, then link the target’s next to the new node. To delete, bypass the target node by linking previous node’s next to the one after target.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int data;\n    struct Node *next;\n} Node;\n\nNode* insert_head(Node* head, int value) {\n    Node* new_node = malloc(sizeof(Node));\n    new_node-&gt;data = value;\n    new_node-&gt;next = head;\n    return new_node; // new head\n}\n\nNode* insert_after(Node* node, int value) {\n    if (!node) return NULL;\n    Node* new_node = malloc(sizeof(Node));\n    new_node-&gt;data = value;\n    new_node-&gt;next = node-&gt;next;\n    node-&gt;next = new_node;\n    return new_node;\n}\n\nNode* delete_value(Node* head, int value) {\n    if (!head) return NULL;\n    if (head-&gt;data == value) {\n        Node* tmp = head-&gt;next;\n        free(head);\n        return tmp;\n    }\n    Node* prev = head;\n    Node* cur = head-&gt;next;\n    while (cur) {\n        if (cur-&gt;data == value) {\n            prev-&gt;next = cur-&gt;next;\n            free(cur);\n            break;\n        }\n        prev = cur;\n        cur = cur-&gt;next;\n    }\n    return head;\n}\n\nvoid print_list(Node* head) {\n    for (Node* p = head; p; p = p-&gt;next)\n        printf(\"%d -&gt; \", p-&gt;data);\n    printf(\"NULL\\n\");\n}\n\nint main(void) {\n    Node* head = NULL;\n    head = insert_head(head, 10);\n    head = insert_head(head, 20);\n    insert_after(head, 30);\n    print_list(head);\n    head = delete_value(head, 30);\n    print_list(head);\n    return 0;\n}\nPython\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def insert_head(self, data):\n        node = Node(data)\n        node.next = self.head\n        self.head = node\n\n    def insert_after(self, prev, data):\n        if prev is None:\n            return\n        node = Node(data)\n        node.next = prev.next\n        prev.next = node\n\n    def delete_value(self, value):\n        cur = self.head\n        prev = None\n        while cur:\n            if cur.data == value:\n                if prev:\n                    prev.next = cur.next\n                else:\n                    self.head = cur.next\n                return\n            prev = cur\n            cur = cur.next\n\n    def print_list(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" -&gt; \")\n            cur = cur.next\n        print(\"NULL\")\n\n\nWhy It Matters\n\nProvides dynamic growth with no preallocation\nEnables O(1) insertion and deletion at head or given position\nUseful in stacks, queues, hash table buckets, and adjacency lists\nFoundation for more complex data structures (trees, graphs)\n\n\n\nA Gentle Proof (Why It Works)\nEach operation only changes a constant number of pointers.\n\nInsert: 2 assignments → O(1)\nDelete: find node O(n), then 1 reassignment Because pointers are updated locally, the rest of the list remains valid.\n\n\n\nTry It Yourself\n\nImplement insert_tail for O(n) tail insertion.\nWrite reverse() to flip the list in place.\nAdd size() to count nodes.\nExperiment with deleting from head and middle.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation\nInput\nResult\nNotes\n\n\n\n\ninsert_head(10)\n[]\n[10]\ncreates head\n\n\ninsert_head(20)\n[10]\n[20 → 10]\nnew head\n\n\ninsert_after(head, 30)\n[20 → 10]\n[20 → 30 → 10]\npointer reroute\n\n\ndelete_value(30)\n[20 → 30 → 10]\n[20 → 10]\nnode removed\n\n\ndelete_value(40)\n[20 → 10]\n[20 → 10]\nno change\n\n\n\nEdge Cases\n\nDelete from empty list → no effect\nInsert after NULL → no effect\nDelete head node → update head pointer\n\n\n\nComplexity\n\nTime: insert O(1), delete O(n) (if searching by value), traversal O(n)\nSpace: O(n) for n nodes\n\nSingly linked lists teach pointer manipulation, where structure grows one node at a time, and every link matters.\n\n\n\n204 Doubly Linked List Insert/Delete\nA doubly linked list extends the singly linked list by adding backward links. Each node points to both its previous and next neighbor, making insertions and deletions easier in both directions.\n\nWhat Problem Are We Solving?\nSingly linked lists cannot traverse backward and deleting a node requires access to its predecessor. Doubly linked lists fix this by storing two pointers per node, allowing constant-time insertions and deletions at any position when you have a node reference.\nGoal: Support bidirectional traversal and efficient local insert/delete operations with O(1) pointer updates.\n\n\nHow Does It Work (Plain Language)?\nEach node stores:\n\ndata: the value\nprev: pointer to the previous node\nnext: pointer to the next node\n\nThe list tracks two ends:\n\nhead points to the first node\ntail points to the last node\n\nExample Steps (Insertion and Deletion)\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nTarget\nBefore\nAfter\nNote\n\n\n\n\n1\ncreate list\n-\nempty\n[10]\nhead=tail=10\n\n\n2\ninsert_front(20)\nhead\n[10]\n[20 ⇄ 10]\nhead updated\n\n\n3\ninsert_back(30)\ntail\n[20 ⇄ 10]\n[20 ⇄ 10 ⇄ 30]\ntail updated\n\n\n4\ndelete(10)\nmiddle\n[20 ⇄ 10 ⇄ 30]\n[20 ⇄ 30]\npointers bypass 10\n\n\n\nEach operation only touches nearby nodes, no need to shift elements.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int data;\n    struct Node* prev;\n    struct Node* next;\n} Node;\n\nNode* insert_front(Node* head, int value) {\n    Node* node = malloc(sizeof(Node));\n    node-&gt;data = value;\n    node-&gt;prev = NULL;\n    node-&gt;next = head;\n    if (head) head-&gt;prev = node;\n    return node;\n}\n\nNode* insert_back(Node* head, int value) {\n    Node* node = malloc(sizeof(Node));\n    node-&gt;data = value;\n    node-&gt;next = NULL;\n    if (!head) {\n        node-&gt;prev = NULL;\n        return node;\n    }\n    Node* tail = head;\n    while (tail-&gt;next) tail = tail-&gt;next;\n    tail-&gt;next = node;\n    node-&gt;prev = tail;\n    return head;\n}\n\nNode* delete_value(Node* head, int value) {\n    Node* cur = head;\n    while (cur && cur-&gt;data != value) cur = cur-&gt;next;\n    if (!cur) return head; // not found\n    if (cur-&gt;prev) cur-&gt;prev-&gt;next = cur-&gt;next;\n    else head = cur-&gt;next; // deleting head\n    if (cur-&gt;next) cur-&gt;next-&gt;prev = cur-&gt;prev;\n    free(cur);\n    return head;\n}\n\nvoid print_forward(Node* head) {\n    for (Node* p = head; p; p = p-&gt;next) printf(\"%d ⇄ \", p-&gt;data);\n    printf(\"NULL\\n\");\n}\nPython\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\nclass DoublyLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def insert_front(self, data):\n        node = Node(data)\n        node.next = self.head\n        if self.head:\n            self.head.prev = node\n        self.head = node\n\n    def insert_back(self, data):\n        node = Node(data)\n        if not self.head:\n            self.head = node\n            return\n        cur = self.head\n        while cur.next:\n            cur = cur.next\n        cur.next = node\n        node.prev = cur\n\n    def delete_value(self, value):\n        cur = self.head\n        while cur:\n            if cur.data == value:\n                if cur.prev:\n                    cur.prev.next = cur.next\n                else:\n                    self.head = cur.next\n                if cur.next:\n                    cur.next.prev = cur.prev\n                return\n            cur = cur.next\n\n    def print_forward(self):\n        cur = self.head\n        while cur:\n            print(cur.data, end=\" ⇄ \")\n            cur = cur.next\n        print(\"NULL\")\n\n\nWhy It Matters\n\nBidirectional traversal (forward/backward)\nO(1) insertion and deletion when node is known\nFoundation for deque, LRU cache, and text editor buffers\nEnables clean list reversal and splice operations\n\n\n\nA Gentle Proof (Why It Works)\nEach node update modifies at most four pointers:\n\nInsert: new node’s next, prev + neighbor links\nDelete: predecessor’s next, successor’s prev Since each step is constant work, operations are O(1) given node reference.\n\nTraversal still costs O(n), but local edits are efficient.\n\n\nTry It Yourself\n\nImplement reverse() by swapping prev and next pointers.\nAdd tail pointer to allow O(1) insert_back.\nSupport bidirectional iteration.\nImplement pop_front() and pop_back().\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\nNotes\n\n\n\n\ninsert_front(10)\n[]\n[10]\nhead=tail=10\n\n\ninsert_front(20)\n[10]\n[20 ⇄ 10]\nnew head\n\n\ninsert_back(30)\n[20 ⇄ 10]\n[20 ⇄ 10 ⇄ 30]\nnew tail\n\n\ndelete_value(10)\n[20 ⇄ 10 ⇄ 30]\n[20 ⇄ 30]\nmiddle removed\n\n\ndelete_value(20)\n[20 ⇄ 30]\n[30]\nhead removed\n\n\n\nEdge Cases\n\nDeleting from empty list → no effect\nInserting on empty list sets both head and tail\nProperly maintain both directions after each update\n\n\n\nComplexity\n\nTime:\n\nInsert/Delete (given node): O(1)\nSearch: O(n)\nTraverse: O(n)\n\nSpace: O(n) (2 pointers per node)\n\nDoubly linked lists bring symmetry, move forward, move back, and edit in constant time.\n\n\n\n205 Stack Push/Pop\nA stack is a simple but powerful data structure that follows the LIFO (Last In, First Out) rule. The most recently added element is the first to be removed, like stacking plates where you can only take from the top.\n\nWhat Problem Are We Solving?\nWe often need to reverse order or track nested operations, such as function calls, parentheses, undo/redo, and expression evaluation. A stack gives us a clean way to manage this behavior with push (add) and pop (remove).\nGoal: Support push, pop, peek, and is_empty in O(1) time, maintaining LIFO order.\n\n\nHow Does It Work (Plain Language)?\nThink of a vertical pile.\n\nPush(x): Place x on top.\nPop(): Remove the top element.\nPeek(): View top without removing.\n\nImplementation can use either an array (fixed or dynamic) or a linked list.\nExample Steps (Array Stack Simulation)\n\n\n\nStep\nOperation\nStack (Top → Bottom)\nNote\n\n\n\n\n1\npush(10)\n[10]\nfirst element\n\n\n2\npush(20)\n[20, 10]\ntop is 20\n\n\n3\npush(30)\n[30, 20, 10]\n\n\n\n4\npop()\n[20, 10]\n30 removed\n\n\n5\npeek()\ntop = 20\ntop unchanged\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Array-Based Stack)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int top;\n    int capacity;\n} Stack;\n\nint stack_init(Stack *s, int cap) {\n    s-&gt;data = malloc(cap * sizeof(int));\n    if (!s-&gt;data) return 0;\n    s-&gt;capacity = cap;\n    s-&gt;top = -1;\n    return 1;\n}\n\nint stack_push(Stack *s, int x) {\n    if (s-&gt;top + 1 == s-&gt;capacity) return 0; // full\n    s-&gt;data[++s-&gt;top] = x;\n    return 1;\n}\n\nint stack_pop(Stack *s, int *out) {\n    if (s-&gt;top == -1) return 0; // empty\n    *out = s-&gt;data[s-&gt;top--];\n    return 1;\n}\n\nint stack_peek(Stack *s, int *out) {\n    if (s-&gt;top == -1) return 0;\n    *out = s-&gt;data[s-&gt;top];\n    return 1;\n}\n\nint main(void) {\n    Stack s;\n    stack_init(&s, 5);\n    stack_push(&s, 10);\n    stack_push(&s, 20);\n    int val;\n    stack_pop(&s, &val);\n    printf(\"Popped: %d\\n\", val);\n    stack_peek(&s, &val);\n    printf(\"Top: %d\\n\", val);\n    free(s.data);\n    return 0;\n}\nPython (List as Stack)\nclass Stack:\n    def __init__(self):\n        self._data = []\n\n    def push(self, x):\n        self._data.append(x)\n\n    def pop(self):\n        if not self._data:\n            raise IndexError(\"pop from empty stack\")\n        return self._data.pop()\n\n    def peek(self):\n        if not self._data:\n            raise IndexError(\"peek from empty stack\")\n        return self._data[-1]\n\n    def is_empty(self):\n        return len(self._data) == 0\n\n# Example\ns = Stack()\ns.push(10)\ns.push(20)\nprint(s.pop())  # 20\nprint(s.peek()) # 10\n\n\nWhy It Matters\n\nCore to recursion, parsing, and backtracking\nUnderpins function call stacks in programming languages\nNatural structure for undo, reverse, and balanced parentheses\nSimplicity with wide applications in algorithms\n\n\n\nA Gentle Proof (Why It Works)\nEach operation touches only the top element or index.\n\npush: increment top, write value\npop: read top, decrement top All O(1) time. The LIFO property ensures correct reverse order for function calls and nested scopes.\n\n\n\nTry It Yourself\n\nImplement stack with linked list nodes.\nExtend capacity dynamically when full.\nUse stack to check balanced parentheses in a string.\nReverse a list using stack operations.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\nNotes\n\n\n\n\npush(10)\n[]\n[10]\ntop = 10\n\n\npush(20)\n[10]\n[20, 10]\ntop = 20\n\n\npop()\n[20, 10]\nreturns 20, stack [10]\n\n\n\npeek()\n[10]\nreturns 10\n\n\n\npop()\n[10]\nreturns 10, stack []\nempty after pop\n\n\n\nEdge Cases\n\nPop from empty → error or return false\nPeek from empty → error\nOverflow if array full (unless dynamic)\n\n\n\nComplexity\n\nTime: push O(1), pop O(1), peek O(1)\nSpace: O(n) for n elements\n\nStacks embody disciplined memory, last in, first out, a minimalist model of control flow and history.\n\n\n\n206 Queue Enqueue/Dequeue\nA queue is the mirror twin of a stack. It follows FIFO (First In, First Out), the first element added is the first one removed, like people waiting in line.\n\nWhat Problem Are We Solving?\nWe need a structure where items are processed in arrival order: scheduling tasks, buffering data, breadth-first search, or managing print jobs. A queue lets us enqueue at the back and dequeue from the front, simple and fair.\nGoal: Support enqueue, dequeue, peek, and is_empty in O(1) time using a circular layout or linked list.\n\n\nHow Does It Work (Plain Language)?\nA queue has two ends:\n\nfront → where items are removed\nrear → where items are added\n\nOperations:\n\nenqueue(x) adds at rear\ndequeue() removes from front\npeek() looks at the front item\n\nIf implemented with a circular array, wrap indices using modulo arithmetic.\nExample Steps (FIFO Simulation)\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nQueue (Front → Rear)\nFront\nRear\nNote\n\n\n\n\n1\nenqueue(10)\n[10]\n0\n1\nfirst element\n\n\n2\nenqueue(20)\n[10, 20]\n0\n2\nrear advanced\n\n\n3\nenqueue(30)\n[10, 20, 30]\n0\n3\n\n\n\n4\ndequeue()\n[20, 30]\n1\n3\n10 removed\n\n\n5\nenqueue(40)\n[20, 30, 40]\n1\n0\nwrap-around\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Circular Array Queue)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int front;\n    int rear;\n    int count;\n    int capacity;\n} Queue;\n\nint queue_init(Queue *q, int cap) {\n    q-&gt;data = malloc(cap * sizeof(int));\n    if (!q-&gt;data) return 0;\n    q-&gt;capacity = cap;\n    q-&gt;front = 0;\n    q-&gt;rear = 0;\n    q-&gt;count = 0;\n    return 1;\n}\n\nint enqueue(Queue *q, int x) {\n    if (q-&gt;count == q-&gt;capacity) return 0; // full\n    q-&gt;data[q-&gt;rear] = x;\n    q-&gt;rear = (q-&gt;rear + 1) % q-&gt;capacity;\n    q-&gt;count++;\n    return 1;\n}\n\nint dequeue(Queue *q, int *out) {\n    if (q-&gt;count == 0) return 0; // empty\n    *out = q-&gt;data[q-&gt;front];\n    q-&gt;front = (q-&gt;front + 1) % q-&gt;capacity;\n    q-&gt;count--;\n    return 1;\n}\n\nint queue_peek(Queue *q, int *out) {\n    if (q-&gt;count == 0) return 0;\n    *out = q-&gt;data[q-&gt;front];\n    return 1;\n}\n\nint main(void) {\n    Queue q;\n    queue_init(&q, 4);\n    enqueue(&q, 10);\n    enqueue(&q, 20);\n    enqueue(&q, 30);\n    int val;\n    dequeue(&q, &val);\n    printf(\"Dequeued: %d\\n\", val);\n    enqueue(&q, 40);\n    enqueue(&q, 50); // will fail if capacity = 4\n    queue_peek(&q, &val);\n    printf(\"Front: %d\\n\", val);\n    free(q.data);\n    return 0;\n}\nPython (List as Queue using deque)\nfrom collections import deque\n\nclass Queue:\n    def __init__(self):\n        self._data = deque()\n\n    def enqueue(self, x):\n        self._data.append(x)\n\n    def dequeue(self):\n        if not self._data:\n            raise IndexError(\"dequeue from empty queue\")\n        return self._data.popleft()\n\n    def peek(self):\n        if not self._data:\n            raise IndexError(\"peek from empty queue\")\n        return self._data[0]\n\n    def is_empty(self):\n        return len(self._data) == 0\n\n# Example\nq = Queue()\nq.enqueue(10)\nq.enqueue(20)\nprint(q.dequeue())  # 10\nprint(q.peek())     # 20\n\n\nWhy It Matters\n\nEnforces fairness (first come, first served)\nFoundation for BFS, schedulers, buffers, pipelines\nEasy to implement and reason about\nNatural counterpart to stack\n\n\n\nA Gentle Proof (Why It Works)\nEach operation updates only front or rear index, not entire array. Circular indexing ensures constant-time wrap-around:\nrear = (rear + 1) % capacity\nfront = (front + 1) % capacity\nAll operations touch O(1) data and fields, so runtime stays O(1).\n\n\nTry It Yourself\n\nImplement a linked-list-based queue.\nAdd is_full() and is_empty() checks.\nWrite a queue-based BFS on a simple graph.\nCompare linear vs circular queue behavior.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nOperation\nQueue (Front → Rear)\nFront\nRear\nCount\nNotes\n\n\n\n\nenqueue(10)\n[10]\n0\n1\n1\n\n\n\nenqueue(20)\n[10, 20]\n0\n2\n2\n\n\n\nenqueue(30)\n[10, 20, 30]\n0\n3\n3\n\n\n\ndequeue()\n[20, 30]\n1\n3\n2\nremoves 10\n\n\nenqueue(40)\n[20, 30, 40]\n1\n0\n3\nwrap-around\n\n\npeek()\nfront=20\n1\n0\n3\ncheck front\n\n\n\nEdge Cases\n\nDequeue from empty → error\nEnqueue to full → overflow\nWorks seamlessly with wrap-around\n\n\n\nComplexity\n\nTime: enqueue O(1), dequeue O(1), peek O(1)\nSpace: O(n) for fixed buffer or dynamic growth\n\nQueues bring fairness to data, what goes in first comes out first, steady and predictable.\n\n\n\n207 Deque Implementation\nA deque (double-ended queue) is a flexible container that allows adding and removing elements from both ends, a blend of stack and queue behavior.\n\nWhat Problem Are We Solving?\nStacks restrict you to one end, queues to two fixed roles. Sometimes we need both: insert at front or back, pop from either side. Deques power sliding window algorithms, palindrome checks, undo-redo systems, and task schedulers.\nGoal: Support push_front, push_back, pop_front, pop_back, and peek_front/back in O(1) time.\n\n\nHow Does It Work (Plain Language)?\nDeques can be built using:\n\nA circular array (using wrap-around indexing)\nA doubly linked list (bidirectional pointers)\n\nOperations:\n\npush_front(x) → insert before front\npush_back(x) → insert after rear\npop_front() → remove front element\npop_back() → remove rear element\n\nExample Steps (Circular Array Simulation)\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nFront\nRear\nDeque (Front → Rear)\nNote\n\n\n\n\n1\npush_back(10)\n0\n1\n[10]\nfirst item\n\n\n2\npush_back(20)\n0\n2\n[10, 20]\nrear grows\n\n\n3\npush_front(5)\n3\n2\n[5, 10, 20]\nwrap-around front\n\n\n4\npop_back()\n3\n1\n[5, 10]\n20 removed\n\n\n5\npop_front()\n0\n1\n[10]\n5 removed\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Circular Array Deque)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int front;\n    int rear;\n    int count;\n    int capacity;\n} Deque;\n\nint dq_init(Deque *d, int cap) {\n    d-&gt;data = malloc(cap * sizeof(int));\n    if (!d-&gt;data) return 0;\n    d-&gt;capacity = cap;\n    d-&gt;front = 0;\n    d-&gt;rear = 0;\n    d-&gt;count = 0;\n    return 1;\n}\n\nint dq_push_front(Deque *d, int x) {\n    if (d-&gt;count == d-&gt;capacity) return 0; // full\n    d-&gt;front = (d-&gt;front - 1 + d-&gt;capacity) % d-&gt;capacity;\n    d-&gt;data[d-&gt;front] = x;\n    d-&gt;count++;\n    return 1;\n}\n\nint dq_push_back(Deque *d, int x) {\n    if (d-&gt;count == d-&gt;capacity) return 0;\n    d-&gt;data[d-&gt;rear] = x;\n    d-&gt;rear = (d-&gt;rear + 1) % d-&gt;capacity;\n    d-&gt;count++;\n    return 1;\n}\n\nint dq_pop_front(Deque *d, int *out) {\n    if (d-&gt;count == 0) return 0;\n    *out = d-&gt;data[d-&gt;front];\n    d-&gt;front = (d-&gt;front + 1) % d-&gt;capacity;\n    d-&gt;count--;\n    return 1;\n}\n\nint dq_pop_back(Deque *d, int *out) {\n    if (d-&gt;count == 0) return 0;\n    d-&gt;rear = (d-&gt;rear - 1 + d-&gt;capacity) % d-&gt;capacity;\n    *out = d-&gt;data[d-&gt;rear];\n    d-&gt;count--;\n    return 1;\n}\n\nint main(void) {\n    Deque d;\n    dq_init(&d, 4);\n    dq_push_back(&d, 10);\n    dq_push_back(&d, 20);\n    dq_push_front(&d, 5);\n    int val;\n    dq_pop_back(&d, &val);\n    printf(\"Popped back: %d\\n\", val);\n    dq_pop_front(&d, &val);\n    printf(\"Popped front: %d\\n\", val);\n    free(d.data);\n    return 0;\n}\nPython (Deque using collections)\nfrom collections import deque\n\nd = deque()\nd.append(10)       # push_back\nd.appendleft(5)    # push_front\nd.append(20)\nprint(d.pop())     # pop_back -&gt; 20\nprint(d.popleft()) # pop_front -&gt; 5\nprint(d)           # deque([10])\n\n\nWhy It Matters\n\nGeneralizes both stack and queue\nCore tool for sliding window maximum, palindrome checks, BFS with state, and task buffers\nDoubly-ended flexibility with constant-time operations\nIdeal for systems needing symmetric access\n\n\n\nA Gentle Proof (Why It Works)\nCircular indexing ensures wrap-around in O(1). Each operation moves one index and changes one value:\n\npush → set value + adjust index\npop → adjust index + read value\n\nNo shifting or reallocation needed, so all operations remain O(1).\n\n\nTry It Yourself\n\nImplement deque with a doubly linked list.\nAdd peek_front() and peek_back().\nSimulate a sliding window maximum algorithm.\nCompare performance of deque vs list for queue-like tasks.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nOperation\nFront\nRear\nCount\nDeque (Front → Rear)\nNotes\n\n\n\n\npush_back(10)\n0\n1\n1\n[10]\ninit\n\n\npush_back(20)\n0\n2\n2\n[10, 20]\n\n\n\npush_front(5)\n3\n2\n3\n[5, 10, 20]\nwrap front\n\n\npop_back()\n3\n1\n2\n[5, 10]\n20 removed\n\n\npop_front()\n0\n1\n1\n[10]\n5 removed\n\n\n\nEdge Cases\n\nPush on full deque → error or resize\nPop on empty deque → error\nWrap-around correctness is critical\n\n\n\nComplexity\n\nTime: push/pop front/back O(1)\nSpace: O(n)\n\nDeques are the agile queues, you can act from either side, fast and fair.\n\n\n\n208 Circular Queue\nA circular queue is a queue optimized for fixed-size buffers where indices wrap around automatically. It’s widely used in real-time systems, network packet buffers, and streaming pipelines to reuse space efficiently.\n\nWhat Problem Are We Solving?\nA linear queue wastes space after several dequeues, since front indices move forward. A circular queue solves this by wrapping the indices, making every slot reusable.\nGoal: Implement a queue with fixed capacity where enqueue and dequeue both take O(1) time and space is used cyclically.\n\n\nHow Does It Work (Plain Language)?\nA circular queue keeps track of:\n\nfront: index of the first element\nrear: index of the next position to insert\ncount: number of elements\n\nUse modulo arithmetic for wrap-around:\nnext_index = (current_index + 1) % capacity\nKey Conditions\n\nFull when count == capacity\nEmpty when count == 0\n\nExample Steps (Wrap-around Simulation)\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nFront\nRear\nCount\nQueue State\nNote\n\n\n\n\n1\nenqueue(10)\n0\n1\n1\n[10, , , _]\n\n\n\n2\nenqueue(20)\n0\n2\n2\n[10, 20, , ]\n\n\n\n3\nenqueue(30)\n0\n3\n3\n[10, 20, 30, _]\n\n\n\n4\ndequeue()\n1\n3\n2\n[, 20, 30, ]\nfront advanced\n\n\n5\nenqueue(40)\n1\n0\n3\n[40, 20, 30, _]\nwrap-around\n\n\n6\nenqueue(50)\n1\n1\n4\n[40, 20, 30, 50]\nfull\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int front;\n    int rear;\n    int count;\n    int capacity;\n} CircularQueue;\n\nint cq_init(CircularQueue *q, int cap) {\n    q-&gt;data = malloc(cap * sizeof(int));\n    if (!q-&gt;data) return 0;\n    q-&gt;capacity = cap;\n    q-&gt;front = 0;\n    q-&gt;rear = 0;\n    q-&gt;count = 0;\n    return 1;\n}\n\nint cq_enqueue(CircularQueue *q, int x) {\n    if (q-&gt;count == q-&gt;capacity) return 0; // full\n    q-&gt;data[q-&gt;rear] = x;\n    q-&gt;rear = (q-&gt;rear + 1) % q-&gt;capacity;\n    q-&gt;count++;\n    return 1;\n}\n\nint cq_dequeue(CircularQueue *q, int *out) {\n    if (q-&gt;count == 0) return 0; // empty\n    *out = q-&gt;data[q-&gt;front];\n    q-&gt;front = (q-&gt;front + 1) % q-&gt;capacity;\n    q-&gt;count--;\n    return 1;\n}\n\nint cq_peek(CircularQueue *q, int *out) {\n    if (q-&gt;count == 0) return 0;\n    *out = q-&gt;data[q-&gt;front];\n    return 1;\n}\n\nint main(void) {\n    CircularQueue q;\n    cq_init(&q, 4);\n    cq_enqueue(&q, 10);\n    cq_enqueue(&q, 20);\n    cq_enqueue(&q, 30);\n    int val;\n    cq_dequeue(&q, &val);\n    printf(\"Dequeued: %d\\n\", val);\n    cq_enqueue(&q, 40);\n    cq_enqueue(&q, 50); // should fail if full\n    cq_peek(&q, &val);\n    printf(\"Front: %d\\n\", val);\n    free(q.data);\n    return 0;\n}\nPython\nclass CircularQueue:\n    def __init__(self, capacity):\n        self._cap = capacity\n        self._data = [None] * capacity\n        self._front = 0\n        self._rear = 0\n        self._count = 0\n\n    def enqueue(self, x):\n        if self._count == self._cap:\n            raise OverflowError(\"Queue full\")\n        self._data[self._rear] = x\n        self._rear = (self._rear + 1) % self._cap\n        self._count += 1\n\n    def dequeue(self):\n        if self._count == 0:\n            raise IndexError(\"Queue empty\")\n        x = self._data[self._front]\n        self._front = (self._front + 1) % self._cap\n        self._count -= 1\n        return x\n\n    def peek(self):\n        if self._count == 0:\n            raise IndexError(\"Queue empty\")\n        return self._data[self._front]\n\n# Example\nq = CircularQueue(4)\nq.enqueue(10)\nq.enqueue(20)\nq.enqueue(30)\nprint(q.dequeue())  # 10\nq.enqueue(40)\nprint(q.peek())     # 20\n\n\nWhy It Matters\n\nEfficient space reuse, no wasted slots\nPredictable memory usage for real-time systems\nBackbone of buffering systems (audio, network, streaming)\nFast O(1) operations, no shifting elements\n\n\n\nA Gentle Proof (Why It Works)\nSince both front and rear wrap using modulo, all operations remain in range [0, capacity-1]. Each operation modifies a fixed number of variables. Thus:\n\nenqueue: 1 write + 2 updates\ndequeue: 1 read + 2 updates No shifting, no resizing → all O(1) time.\n\n\n\nTry It Yourself\n\nAdd is_full() and is_empty() helpers.\nImplement an overwrite mode where new enqueues overwrite oldest data.\nVisualize index movement for multiple wraps.\nUse a circular queue to simulate a producer-consumer buffer.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nOperation\nFront\nRear\nCount\nQueue (Front → Rear)\nNotes\n\n\n\n\nenqueue(10)\n0\n1\n1\n[10, , , _]\nfirst element\n\n\nenqueue(20)\n0\n2\n2\n[10, 20, , ]\n\n\n\nenqueue(30)\n0\n3\n3\n[10, 20, 30, _]\n\n\n\ndequeue()\n1\n3\n2\n[, 20, 30, ]\n10 removed\n\n\nenqueue(40)\n1\n0\n3\n[40, 20, 30, _]\nwrap-around\n\n\nenqueue(50)\n1\n1\n4\n[40, 20, 30, 50]\nfull queue\n\n\n\nEdge Cases\n\nEnqueue when full → reject or overwrite\nDequeue when empty → error\nWrap-around indexing must handle 0 correctly\n\n\n\nComplexity\n\nTime: enqueue O(1), dequeue O(1), peek O(1)\nSpace: O(n) fixed buffer\n\nCircular queues are the heartbeat of real-time data flows, steady, cyclic, and never wasting a byte.\n\n\n\n209 Stack via Queue\nA stack via queue is a playful twist, implementing LIFO behavior using FIFO tools. It shows how one structure can simulate another by combining basic operations cleverly.\n\nWhat Problem Are We Solving?\nSometimes we’re limited to queue operations (enqueue, dequeue) but still want a stack’s last-in-first-out order. We can simulate push and pop using one or two queues.\nGoal: Build a stack that supports push, pop, and peek in O(1) or O(n) time (depending on strategy) using only queue operations.\n\n\nHow Does It Work (Plain Language)?\nTwo main strategies:\n\nPush costly: rotate elements after every push so front is always top.\nPop costly: enqueue normally, but rotate during pop.\n\nWe’ll show push costly version, simpler conceptually.\nIdea: Each push enqueues new item, then rotates all older elements behind it, so that last pushed is always at the front (ready to pop).\nExample Steps (Push Costly)\n\n\n\nStep\nOperation\nQueue (Front → Rear)\nNote\n\n\n\n\n1\npush(10)\n[10]\nonly one element\n\n\n2\npush(20)\n[20, 10]\nrotated so 20 is front\n\n\n3\npush(30)\n[30, 20, 10]\nrotated again\n\n\n4\npop()\n[20, 10]\n30 removed\n\n\n5\npush(40)\n[40, 20, 10]\nrotation maintains order\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Using Two Queues)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int front, rear, count, capacity;\n} Queue;\n\nint q_init(Queue *q, int cap) {\n    q-&gt;data = malloc(cap * sizeof(int));\n    if (!q-&gt;data) return 0;\n    q-&gt;front = 0;\n    q-&gt;rear = 0;\n    q-&gt;count = 0;\n    q-&gt;capacity = cap;\n    return 1;\n}\n\nint q_enqueue(Queue *q, int x) {\n    if (q-&gt;count == q-&gt;capacity) return 0;\n    q-&gt;data[q-&gt;rear] = x;\n    q-&gt;rear = (q-&gt;rear + 1) % q-&gt;capacity;\n    q-&gt;count++;\n    return 1;\n}\n\nint q_dequeue(Queue *q, int *out) {\n    if (q-&gt;count == 0) return 0;\n    *out = q-&gt;data[q-&gt;front];\n    q-&gt;front = (q-&gt;front + 1) % q-&gt;capacity;\n    q-&gt;count--;\n    return 1;\n}\n\ntypedef struct {\n    Queue q1, q2;\n} StackViaQueue;\n\nint svq_init(StackViaQueue *s, int cap) {\n    return q_init(&s-&gt;q1, cap) && q_init(&s-&gt;q2, cap);\n}\n\nint svq_push(StackViaQueue *s, int x) {\n    q_enqueue(&s-&gt;q2, x);\n    int val;\n    while (s-&gt;q1.count) {\n        q_dequeue(&s-&gt;q1, &val);\n        q_enqueue(&s-&gt;q2, val);\n    }\n    // swap q1 and q2\n    Queue tmp = s-&gt;q1;\n    s-&gt;q1 = s-&gt;q2;\n    s-&gt;q2 = tmp;\n    return 1;\n}\n\nint svq_pop(StackViaQueue *s, int *out) {\n    return q_dequeue(&s-&gt;q1, out);\n}\n\nint svq_peek(StackViaQueue *s, int *out) {\n    if (s-&gt;q1.count == 0) return 0;\n    *out = s-&gt;q1.data[s-&gt;q1.front];\n    return 1;\n}\n\nint main(void) {\n    StackViaQueue s;\n    svq_init(&s, 10);\n    svq_push(&s, 10);\n    svq_push(&s, 20);\n    svq_push(&s, 30);\n    int val;\n    svq_pop(&s, &val);\n    printf(\"Popped: %d\\n\", val);\n    svq_peek(&s, &val);\n    printf(\"Top: %d\\n\", val);\n    free(s.q1.data);\n    free(s.q2.data);\n    return 0;\n}\nPython\nfrom collections import deque\n\nclass StackViaQueue:\n    def __init__(self):\n        self.q = deque()\n\n    def push(self, x):\n        n = len(self.q)\n        self.q.append(x)\n        # rotate all older elements\n        for _ in range(n):\n            self.q.append(self.q.popleft())\n\n    def pop(self):\n        if not self.q:\n            raise IndexError(\"pop from empty stack\")\n        return self.q.popleft()\n\n    def peek(self):\n        if not self.q:\n            raise IndexError(\"peek from empty stack\")\n        return self.q[0]\n\n# Example\ns = StackViaQueue()\ns.push(10)\ns.push(20)\ns.push(30)\nprint(s.pop())  # 30\nprint(s.peek()) # 20\n\n\nWhy It Matters\n\nDemonstrates duality of data structures (stack built on queue)\nReinforces operation trade-offs (costly push vs costly pop)\nGreat teaching example for algorithmic simulation\nBuilds insight into complexity and resource usage\n\n\n\nA Gentle Proof (Why It Works)\nIn push costly approach:\n\nEach push rotates all previous elements behind the new one → new element becomes front.\nPop simply dequeues from front → correct LIFO order.\n\nSo order is maintained: newest always exits first.\n\n\nTry It Yourself\n\nImplement pop costly variant (push O(1), pop O(n)).\nAdd is_empty() helper.\nCompare total number of operations for n pushes and pops.\nExtend to generic types with structs or templates.\n\n\n\nTest Cases\n\n\n\nOperation\nQueue (Front → Rear)\nNotes\n\n\n\n\npush(10)\n[10]\nsingle element\n\n\npush(20)\n[20, 10]\nrotated\n\n\npush(30)\n[30, 20, 10]\nrotated\n\n\npop()\n[20, 10]\nreturns 30\n\n\npeek()\n[20, 10]\nreturns 20\n\n\n\nEdge Cases\n\nPop/peek from empty → error\nCapacity reached → reject push\nWorks even with single queue (rotation after push)\n\n\n\nComplexity\n\nPush: O(n)\nPop/Peek: O(1)\nSpace: O(n)\n\nStack via queue proves constraints breed creativity, same data, different dance.\n\n\n\n210 Queue via Stack\nA queue via stack flips the story: build FIFO behavior using LIFO tools. It’s a classic exercise in algorithmic inversion, showing how fundamental operations can emulate each other with clever order manipulation.\n\nWhat Problem Are We Solving?\nSuppose you only have stacks (with push, pop, peek) but need queue behavior (with enqueue, dequeue). We want to process items in arrival order, first in, first out, even though stacks operate last in, first out.\nGoal: Implement enqueue, dequeue, and peek for a queue using only stack operations.\n\n\nHow Does It Work (Plain Language)?\nTwo stacks are enough:\n\ninbox: where we push new items (enqueue)\noutbox: where we pop old items (dequeue)\n\nWhen dequeuing, if outbox is empty, we move all items from inbox to outbox, reversing their order so oldest items are on top.\nThis reversal step restores FIFO behavior.\nExample Steps (Two-Stack Method)\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\ninbox (Top → Bottom)\noutbox (Top → Bottom)\nNote\n\n\n\n\n1\nenqueue(10)\n[10]\n[]\n\n\n\n2\nenqueue(20)\n[20, 10]\n[]\n\n\n\n3\nenqueue(30)\n[30, 20, 10]\n[]\n\n\n\n4\ndequeue()\n[]\n[10, 20, 30]\ntransfer + pop(10)\n\n\n5\nenqueue(40)\n[40]\n[20, 30]\nmixed state\n\n\n6\ndequeue()\n[40]\n[30]\npop(20) from outbox\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Using Two Stacks)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct {\n    int *data;\n    int top;\n    int capacity;\n} Stack;\n\nint stack_init(Stack *s, int cap) {\n    s-&gt;data = malloc(cap * sizeof(int));\n    if (!s-&gt;data) return 0;\n    s-&gt;top = -1;\n    s-&gt;capacity = cap;\n    return 1;\n}\n\nint stack_push(Stack *s, int x) {\n    if (s-&gt;top + 1 == s-&gt;capacity) return 0;\n    s-&gt;data[++s-&gt;top] = x;\n    return 1;\n}\n\nint stack_pop(Stack *s, int *out) {\n    if (s-&gt;top == -1) return 0;\n    *out = s-&gt;data[s-&gt;top--];\n    return 1;\n}\n\nint stack_peek(Stack *s, int *out) {\n    if (s-&gt;top == -1) return 0;\n    *out = s-&gt;data[s-&gt;top];\n    return 1;\n}\n\nint stack_empty(Stack *s) {\n    return s-&gt;top == -1;\n}\n\ntypedef struct {\n    Stack in, out;\n} QueueViaStack;\n\nint qvs_init(QueueViaStack *q, int cap) {\n    return stack_init(&q-&gt;in, cap) && stack_init(&q-&gt;out, cap);\n}\n\nint qvs_enqueue(QueueViaStack *q, int x) {\n    return stack_push(&q-&gt;in, x);\n}\n\nint qvs_shift(QueueViaStack *q) {\n    int val;\n    while (!stack_empty(&q-&gt;in)) {\n        stack_pop(&q-&gt;in, &val);\n        stack_push(&q-&gt;out, val);\n    }\n    return 1;\n}\n\nint qvs_dequeue(QueueViaStack *q, int *out) {\n    if (stack_empty(&q-&gt;out)) qvs_shift(q);\n    return stack_pop(&q-&gt;out, out);\n}\n\nint qvs_peek(QueueViaStack *q, int *out) {\n    if (stack_empty(&q-&gt;out)) qvs_shift(q);\n    return stack_peek(&q-&gt;out, out);\n}\n\nint main(void) {\n    QueueViaStack q;\n    qvs_init(&q, 10);\n    qvs_enqueue(&q, 10);\n    qvs_enqueue(&q, 20);\n    qvs_enqueue(&q, 30);\n    int val;\n    qvs_dequeue(&q, &val);\n    printf(\"Dequeued: %d\\n\", val);\n    qvs_enqueue(&q, 40);\n    qvs_peek(&q, &val);\n    printf(\"Front: %d\\n\", val);\n    free(q.in.data);\n    free(q.out.data);\n    return 0;\n}\nPython (Two Stack Queue)\nclass QueueViaStack:\n    def __init__(self):\n        self.inbox = []\n        self.outbox = []\n\n    def enqueue(self, x):\n        self.inbox.append(x)\n\n    def dequeue(self):\n        if not self.outbox:\n            while self.inbox:\n                self.outbox.append(self.inbox.pop())\n        if not self.outbox:\n            raise IndexError(\"dequeue from empty queue\")\n        return self.outbox.pop()\n\n    def peek(self):\n        if not self.outbox:\n            while self.inbox:\n                self.outbox.append(self.inbox.pop())\n        if not self.outbox:\n            raise IndexError(\"peek from empty queue\")\n        return self.outbox[-1]\n\n# Example\nq = QueueViaStack()\nq.enqueue(10)\nq.enqueue(20)\nq.enqueue(30)\nprint(q.dequeue())  # 10\nq.enqueue(40)\nprint(q.peek())     # 20\n\n\nWhy It Matters\n\nDemonstrates queue emulation with stack operations\nCore teaching example for data structure duality\nHelps in designing abstract interfaces under constraints\nUnderpins some streaming and buffering systems\n\n\n\nA Gentle Proof (Why It Works)\nEach transfer (inbox → outbox) reverses order once, restoring FIFO sequence.\n\nEnqueue pushes to inbox (LIFO)\nDequeue pops from outbox (LIFO of reversed) So overall effect = FIFO\n\nTransfers happen only when outbox is empty, so amortized cost per operation is O(1).\n\n\nTry It Yourself\n\nImplement a single-stack recursive version.\nAdd is_empty() helper.\nMeasure amortized vs worst-case complexity.\nExtend to generic data type.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nOperation\ninbox (Top→Bottom)\noutbox (Top→Bottom)\nResult\nNotes\n\n\n\n\nenqueue(10)\n[10]\n[]\n\n\n\n\nenqueue(20)\n[20, 10]\n[]\n\n\n\n\nenqueue(30)\n[30, 20, 10]\n[]\n\n\n\n\ndequeue()\n[]\n[10, 20, 30]\nreturns 10\ntransfer + pop\n\n\nenqueue(40)\n[40]\n[20, 30]\n\n\n\n\ndequeue()\n[40]\n[30]\nreturns 20\n\n\n\n\nEdge Cases\n\nDequeue from empty queue → error\nMultiple dequeues trigger one transfer\nOutbox reused efficiently\n\n\n\nComplexity\n\nTime: amortized O(1) per operation (worst-case O(n) on transfer)\nSpace: O(n)\n\nQueue via stack shows symmetry, turn LIFO into FIFO with one clever reversal.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-22.-hash-tables-and-variants",
    "href": "books/en-us/list-3.html#section-22.-hash-tables-and-variants",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 22. Hash Tables and Variants",
    "text": "Section 22. Hash Tables and Variants\n\n211 Hash Table Insertion\nA hash table stores key-value pairs for lightning-fast lookups. It uses a hash function to map keys to array indices, letting us access data in near-constant time.\n\nWhat Problem Are We Solving?\nWe need a data structure that can insert, search, and delete by key efficiently, without scanning every element. Arrays give random access by index; hash tables extend that power to arbitrary keys.\nGoal: Map each key to a slot via a hash function and resolve any collisions gracefully.\n\n\nHow Does It Work (Plain Language)?\nA hash table uses a hash function to convert a key into an index:\nindex = hash(key) % capacity\nWhen inserting a new (key, value):\n\nCompute hash index.\nIf slot is empty → place pair there.\nIf occupied → handle collision (chaining or open addressing).\n\nWe’ll use separate chaining (linked list per slot) as the simplest method.\nExample Steps (Separate Chaining)\n\n\n\nStep\nKey\nHash(key)\nIndex\nAction\n\n\n\n\n1\n“apple”\n42\n2\nInsert (“apple”, 10)\n\n\n2\n“banana”\n15\n3\nInsert (“banana”, 20)\n\n\n3\n“pear”\n18\n2\nCollision → chain in index 2\n\n\n4\n“peach”\n21\n1\nInsert new pair\n\n\n\nTable after insertions:\n\n\n\nIndex\nChain\n\n\n\n\n0\n-\n\n\n1\n(“peach”, 40)\n\n\n2\n(“apple”, 10) → (“pear”, 30)\n\n\n3\n(“banana”, 20)\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Separate Chaining Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\n#define TABLE_SIZE 5\n\ntypedef struct Node {\n    char *key;\n    int value;\n    struct Node *next;\n} Node;\n\ntypedef struct {\n    Node *buckets[TABLE_SIZE];\n} HashTable;\n\nunsigned int hash(const char *key) {\n    unsigned int h = 0;\n    while (*key) h = h * 31 + *key++;\n    return h % TABLE_SIZE;\n}\n\nHashTable* ht_create() {\n    HashTable *ht = malloc(sizeof(HashTable));\n    for (int i = 0; i &lt; TABLE_SIZE; i++) ht-&gt;buckets[i] = NULL;\n    return ht;\n}\n\nvoid ht_insert(HashTable *ht, const char *key, int value) {\n    unsigned int idx = hash(key);\n    Node *node = ht-&gt;buckets[idx];\n    while (node) {\n        if (strcmp(node-&gt;key, key) == 0) { node-&gt;value = value; return; }\n        node = node-&gt;next;\n    }\n    Node *new_node = malloc(sizeof(Node));\n    new_node-&gt;key = strdup(key);\n    new_node-&gt;value = value;\n    new_node-&gt;next = ht-&gt;buckets[idx];\n    ht-&gt;buckets[idx] = new_node;\n}\n\nint ht_search(HashTable *ht, const char *key, int *out) {\n    unsigned int idx = hash(key);\n    Node *node = ht-&gt;buckets[idx];\n    while (node) {\n        if (strcmp(node-&gt;key, key) == 0) { *out = node-&gt;value; return 1; }\n        node = node-&gt;next;\n    }\n    return 0;\n}\n\nint main(void) {\n    HashTable *ht = ht_create();\n    ht_insert(ht, \"apple\", 10);\n    ht_insert(ht, \"pear\", 30);\n    ht_insert(ht, \"banana\", 20);\n    int val;\n    if (ht_search(ht, \"pear\", &val))\n        printf(\"pear: %d\\n\", val);\n    return 0;\n}\nPython (Dictionary Simulation)\nclass HashTable:\n    def __init__(self, size=5):\n        self.size = size\n        self.table = [[] for _ in range(size)]\n\n    def _hash(self, key):\n        return hash(key) % self.size\n\n    def insert(self, key, value):\n        idx = self._hash(key)\n        for pair in self.table[idx]:\n            if pair[0] == key:\n                pair[1] = value\n                return\n        self.table[idx].append([key, value])\n\n    def search(self, key):\n        idx = self._hash(key)\n        for k, v in self.table[idx]:\n            if k == key:\n                return v\n        return None\n\n# Example\nht = HashTable()\nht.insert(\"apple\", 10)\nht.insert(\"pear\", 30)\nht.insert(\"banana\", 20)\nprint(ht.search(\"pear\"))  # 30\n\n\nWhy It Matters\n\nProvides average O(1) access, insert, delete\nBackbone of symbol tables, maps, sets, and dictionaries\nUsed in caches, compilers, and indexing systems\nIntroduces key idea: hash function + collision handling\n\n\n\nA Gentle Proof (Why It Works)\nLet table size = m, number of keys = n. If hash spreads keys uniformly, expected chain length = α = n/m (load factor).\n\nAverage lookup: O(1 + α)\nKeep α ≤ 1 → near constant time If collisions are minimized, operations stay fast.\n\n\n\nTry It Yourself\n\nImplement update to modify existing key’s value.\nAdd delete(key) to remove entries from chain.\nExperiment with different hash functions (e.g. djb2, FNV-1a).\nMeasure time vs load factor.\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nValue\nResult\nNotes\n\n\n\n\ninsert\n“apple”\n10\nsuccess\nnew key\n\n\ninsert\n“banana”\n20\nsuccess\nnew key\n\n\ninsert\n“apple”\n15\nupdate\nkey exists\n\n\nsearch\n“banana”\n\n20\nfound\n\n\nsearch\n“grape”\n\nNone\nnot found\n\n\n\nEdge Cases\n\nInsert duplicate → update value\nSearch non-existent → return None\nTable full (open addressing) → needs rehash\n\n\n\nComplexity\n\nTime: average O(1), worst O(n) (all collide)\nSpace: O(n + m) (keys + table slots)\n\nHash table insertion is the art of turning chaos into order, hash, map, resolve, and store.\n\n\n\n212 Linear Probing\nLinear probing is one of the simplest collision resolution strategies in open addressing hash tables. When a collision occurs, it looks for the next empty slot by moving step by step through the table, wrapping around if needed.\n\nWhat Problem Are We Solving?\nWhen two keys hash to the same index, where do we store the new one? Instead of chaining nodes, linear probing searches the next available slot, keeping all data inside the array.\nGoal: Resolve collisions by scanning linearly from the point of conflict until an empty slot is found.\n\n\nHow Does It Work (Plain Language)?\nWhen inserting a key:\n\nCompute index = hash(key) % capacity.\nIf slot empty → place key there.\nIf occupied → move to (index + 1) % capacity.\nRepeat until an empty slot is found or table is full.\n\nLookups and deletions follow the same probe sequence until key is found or empty slot encountered.\nExample Steps (Capacity = 7)\n\n\n\nStep\nKey\nHash(key)\nIndex\nAction\n\n\n\n\n1\n10\n3\n3\nPlace at index 3\n\n\n2\n24\n3\n4\nCollision → move to 4\n\n\n3\n31\n3\n5\nCollision → move to 5\n\n\n4\n17\n3\n6\nCollision → move to 6\n\n\n5\n38\n3\n0\nWrap around → place at 0\n\n\n\nFinal Table\n\n\n\nIndex\nValue\n\n\n\n\n0\n38\n\n\n1\n-\n\n\n2\n-\n\n\n3\n10\n\n\n4\n24\n\n\n5\n31\n\n\n6\n17\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define CAPACITY 7\n#define EMPTY -1\n#define DELETED -2\n\ntypedef struct {\n    int *table;\n} HashTable;\n\nint hash(int key) { return key % CAPACITY; }\n\nHashTable* ht_create() {\n    HashTable *ht = malloc(sizeof(HashTable));\n    ht-&gt;table = malloc(sizeof(int) * CAPACITY);\n    for (int i = 0; i &lt; CAPACITY; i++) ht-&gt;table[i] = EMPTY;\n    return ht;\n}\n\nvoid ht_insert(HashTable *ht, int key) {\n    int idx = hash(key);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        int pos = (idx + i) % CAPACITY;\n        if (ht-&gt;table[pos] == EMPTY || ht-&gt;table[pos] == DELETED) {\n            ht-&gt;table[pos] = key;\n            return;\n        }\n    }\n    printf(\"Table full, cannot insert %d\\n\", key);\n}\n\nint ht_search(HashTable *ht, int key) {\n    int idx = hash(key);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        int pos = (idx + i) % CAPACITY;\n        if (ht-&gt;table[pos] == EMPTY) return -1;\n        if (ht-&gt;table[pos] == key) return pos;\n    }\n    return -1;\n}\n\nvoid ht_delete(HashTable *ht, int key) {\n    int pos = ht_search(ht, key);\n    if (pos != -1) ht-&gt;table[pos] = DELETED;\n}\n\nint main(void) {\n    HashTable *ht = ht_create();\n    ht_insert(ht, 10);\n    ht_insert(ht, 24);\n    ht_insert(ht, 31);\n    ht_insert(ht, 17);\n    ht_insert(ht, 38);\n    for (int i = 0; i &lt; CAPACITY; i++)\n        printf(\"[%d] = %d\\n\", i, ht-&gt;table[i]);\n    return 0;\n}\nPython\nclass LinearProbingHash:\n    def __init__(self, size=7):\n        self.size = size\n        self.table = [None] * size\n\n    def _hash(self, key):\n        return key % self.size\n\n    def insert(self, key):\n        idx = self._hash(key)\n        for i in range(self.size):\n            pos = (idx + i) % self.size\n            if self.table[pos] is None or self.table[pos] == \"DELETED\":\n                self.table[pos] = key\n                return\n        raise OverflowError(\"Hash table full\")\n\n    def search(self, key):\n        idx = self._hash(key)\n        for i in range(self.size):\n            pos = (idx + i) % self.size\n            if self.table[pos] is None:\n                return None\n            if self.table[pos] == key:\n                return pos\n        return None\n\n    def delete(self, key):\n        pos = self.search(key)\n        if pos is not None:\n            self.table[pos] = \"DELETED\"\n\n# Example\nht = LinearProbingHash()\nfor k in [10, 24, 31, 17, 38]:\n    ht.insert(k)\nprint(ht.table)\nprint(\"Search 17 at:\", ht.search(17))\n\n\nWhy It Matters\n\nSimplest form of open addressing\nKeeps all entries inside one array (no extra memory)\nExcellent cache performance\nForms the basis for modern in-place hash maps\n\n\n\nA Gentle Proof (Why It Works)\nEvery key follows the same probe sequence during insert, search, and delete. So if a key is in the table, search will find it; if it’s not, search will hit an empty slot and stop. Uniform hashing ensures average probe length ≈ 1 / (1 - α), where α = n / m is load factor.\n\n\nTry It Yourself\n\nImplement resize() when load factor &gt; 0.7.\nTest insertion order and wrap-around behavior.\nCompare linear probing vs chaining performance.\nVisualize clustering as load increases.\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nResult\nNotes\n\n\n\n\ninsert\n10\nindex 3\nno collision\n\n\ninsert\n24\nindex 4\nmove 1 slot\n\n\ninsert\n31\nindex 5\nmove 2 slots\n\n\ninsert\n17\nindex 6\nmove 3 slots\n\n\ninsert\n38\nindex 0\nwrap-around\n\n\nsearch\n17\nfound at 6\nlinear search\n\n\ndelete\n24\nmark deleted\nslot reusable\n\n\n\nEdge Cases\n\nTable full → insertion fails\nDeleted slots reused\nMust stop on EMPTY, not DELETED\n\n\n\nComplexity\n\nTime:\n\nAverage O(1) if α small\nWorst O(n) if full cluster\n\nSpace: O(n)\n\nLinear probing walks straight lines through collisions, simple, local, and fast when load is low.\n\n\n\n213 Quadratic Probing\nQuadratic probing improves upon linear probing by reducing primary clustering. Instead of stepping through every slot one by one, it jumps in quadratic increments, spreading colliding keys more evenly across the table.\n\nWhat Problem Are We Solving?\nIn linear probing, consecutive occupied slots cause clustering, leading to long probe chains and degraded performance. Quadratic probing breaks up these runs by using nonlinear probe sequences.\nGoal: Resolve collisions by checking indices offset by quadratic values, +1², +2², +3², …, reducing clustering while keeping predictable probe order.\n\n\nHow Does It Work (Plain Language)?\nWhen inserting a key:\n\nCompute index = hash(key) % capacity.\nIf slot empty → insert.\nIf occupied → try (index + 1²) % capacity, (index + 2²) % capacity, etc.\nContinue until an empty slot is found or table is full.\n\nLookups and deletions follow the same probe sequence.\nExample Steps (Capacity = 7)\n\n\n\nStep\nKey\nHash(key)\nProbes (sequence)\nFinal Slot\n\n\n\n\n1\n10\n3\n3\n3\n\n\n2\n24\n3\n3, 4\n4\n\n\n3\n31\n3\n3, 4, 0\n0\n\n\n4\n17\n3\n3, 4, 0, 2\n2\n\n\n\nTable after insertions:\n\n\n\nIndex\nValue\n\n\n\n\n0\n31\n\n\n1\n-\n\n\n2\n17\n\n\n3\n10\n\n\n4\n24\n\n\n5\n-\n\n\n6\n-\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define CAPACITY 7\n#define EMPTY -1\n#define DELETED -2\n\ntypedef struct {\n    int *table;\n} HashTable;\n\nint hash(int key) { return key % CAPACITY; }\n\nHashTable* ht_create() {\n    HashTable *ht = malloc(sizeof(HashTable));\n    ht-&gt;table = malloc(sizeof(int) * CAPACITY);\n    for (int i = 0; i &lt; CAPACITY; i++) ht-&gt;table[i] = EMPTY;\n    return ht;\n}\n\nvoid ht_insert(HashTable *ht, int key) {\n    int idx = hash(key);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        int pos = (idx + i * i) % CAPACITY;\n        if (ht-&gt;table[pos] == EMPTY || ht-&gt;table[pos] == DELETED) {\n            ht-&gt;table[pos] = key;\n            return;\n        }\n    }\n    printf(\"Table full, cannot insert %d\\n\", key);\n}\n\nint ht_search(HashTable *ht, int key) {\n    int idx = hash(key);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        int pos = (idx + i * i) % CAPACITY;\n        if (ht-&gt;table[pos] == EMPTY) return -1;\n        if (ht-&gt;table[pos] == key) return pos;\n    }\n    return -1;\n}\n\nint main(void) {\n    HashTable *ht = ht_create();\n    ht_insert(ht, 10);\n    ht_insert(ht, 24);\n    ht_insert(ht, 31);\n    ht_insert(ht, 17);\n    for (int i = 0; i &lt; CAPACITY; i++)\n        printf(\"[%d] = %d\\n\", i, ht-&gt;table[i]);\n    return 0;\n}\nPython\nclass QuadraticProbingHash:\n    def __init__(self, size=7):\n        self.size = size\n        self.table = [None] * size\n\n    def _hash(self, key):\n        return key % self.size\n\n    def insert(self, key):\n        idx = self._hash(key)\n        for i in range(self.size):\n            pos = (idx + i * i) % self.size\n            if self.table[pos] is None or self.table[pos] == \"DELETED\":\n                self.table[pos] = key\n                return\n        raise OverflowError(\"Hash table full\")\n\n    def search(self, key):\n        idx = self._hash(key)\n        for i in range(self.size):\n            pos = (idx + i * i) % self.size\n            if self.table[pos] is None:\n                return None\n            if self.table[pos] == key:\n                return pos\n        return None\n\n# Example\nht = QuadraticProbingHash()\nfor k in [10, 24, 31, 17]:\n    ht.insert(k)\nprint(ht.table)\nprint(\"Search 24 at:\", ht.search(24))\n\n\nWhy It Matters\n\nReduces primary clustering seen in linear probing\nKeeps keys more evenly distributed\nAvoids extra pointers (everything stays in one array)\nUseful in hash tables where space is tight and locality helps\n\n\n\nA Gentle Proof (Why It Works)\nProbe sequence: \\[\ni = 0, 1, 2, 3, \\ldots\n\\] Index at step i is \\[\n(index + i^2) \\bmod m\n\\] If table size m is prime, this sequence visits up to ⌈m/2⌉ distinct slots before repeating, guaranteeing an empty slot is found if load factor &lt; 0.5.\nThus all operations follow predictable, finite sequences.\n\n\nTry It Yourself\n\nCompare clustering with linear probing under same keys.\nExperiment with different table sizes (prime vs composite).\nImplement deletion markers properly.\nVisualize probe paths with small tables.\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nProbe Sequence\nSlot\nNotes\n\n\n\n\ninsert\n10\n3\n3\nno collision\n\n\ninsert\n24\n3, 4\n4\n1 step\n\n\ninsert\n31\n3, 4, 0\n0\n2 steps\n\n\ninsert\n17\n3, 4, 0, 2\n2\n3 steps\n\n\nsearch\n31\n3 → 4 → 0\nfound\nquadratic path\n\n\n\nEdge Cases\n\nTable full → insertion fails\nRequires table size prime for full coverage\nNeeds load factor &lt; 0.5 to avoid infinite loops\n\n\n\nComplexity\n\nTime: average O(1), worst O(n)\nSpace: O(n)\n\nQuadratic probing trades a straight line for a curve, spreading collisions smoothly across the table.\n\n\n\n214 Double Hashing\nDouble hashing uses two independent hash functions to minimize collisions. When a conflict occurs, it jumps forward by a second hash value, creating probe sequences unique to each key and greatly reducing clustering.\n\nWhat Problem Are We Solving?\nLinear and quadratic probing both suffer from clustering patterns, especially when keys share similar initial indices. Double hashing breaks this pattern by introducing a second hash function that defines each key’s step size.\nGoal: Use two hash functions to determine probe sequence: \\[\n\\text{index}_i = (h_1(key) + i \\cdot h_2(key)) \\bmod m\n\\] This produces independent probe paths and avoids overlap among keys.\n\n\nHow Does It Work (Plain Language)?\nWhen inserting a key:\n\nCompute primary hash: h1 = key % capacity.\nCompute step size: h2 = 1 + (key % (capacity - 1)) (never zero).\nTry h1; if occupied, try (h1 + h2) % m, (h1 + 2*h2) % m, etc.\nRepeat until an empty slot is found.\n\nSame pattern applies for search and delete.\nExample Steps (Capacity = 7)\n\n\n\nStep\nKey\nh₁(key)\nh₂(key)\nProbe Sequence\nFinal Slot\n\n\n\n\n1\n10\n3\n4\n3\n3\n\n\n2\n24\n3\n4\n3 → 0\n0\n\n\n3\n31\n3\n4\n3 → 0 → 4\n4\n\n\n4\n17\n3\n3\n3 → 6\n6\n\n\n\nFinal Table\n\n\n\nIndex\nValue\n\n\n\n\n0\n24\n\n\n1\n-\n\n\n2\n-\n\n\n3\n10\n\n\n4\n31\n\n\n5\n-\n\n\n6\n17\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define CAPACITY 7\n#define EMPTY -1\n#define DELETED -2\n\ntypedef struct {\n    int *table;\n} HashTable;\n\nint h1(int key) { return key % CAPACITY; }\nint h2(int key) { return 1 + (key % (CAPACITY - 1)); }\n\nHashTable* ht_create() {\n    HashTable *ht = malloc(sizeof(HashTable));\n    ht-&gt;table = malloc(sizeof(int) * CAPACITY);\n    for (int i = 0; i &lt; CAPACITY; i++) ht-&gt;table[i] = EMPTY;\n    return ht;\n}\n\nvoid ht_insert(HashTable *ht, int key) {\n    int idx1 = h1(key);\n    int step = h2(key);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        int pos = (idx1 + i * step) % CAPACITY;\n        if (ht-&gt;table[pos] == EMPTY || ht-&gt;table[pos] == DELETED) {\n            ht-&gt;table[pos] = key;\n            return;\n        }\n    }\n    printf(\"Table full, cannot insert %d\\n\", key);\n}\n\nint ht_search(HashTable *ht, int key) {\n    int idx1 = h1(key), step = h2(key);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        int pos = (idx1 + i * step) % CAPACITY;\n        if (ht-&gt;table[pos] == EMPTY) return -1;\n        if (ht-&gt;table[pos] == key) return pos;\n    }\n    return -1;\n}\n\nint main(void) {\n    HashTable *ht = ht_create();\n    ht_insert(ht, 10);\n    ht_insert(ht, 24);\n    ht_insert(ht, 31);\n    ht_insert(ht, 17);\n    for (int i = 0; i &lt; CAPACITY; i++)\n        printf(\"[%d] = %d\\n\", i, ht-&gt;table[i]);\n    return 0;\n}\nPython\nclass DoubleHash:\n    def __init__(self, size=7):\n        self.size = size\n        self.table = [None] * size\n\n    def _h1(self, key):\n        return key % self.size\n\n    def _h2(self, key):\n        return 1 + (key % (self.size - 1))\n\n    def insert(self, key):\n        h1 = self._h1(key)\n        h2 = self._h2(key)\n        for i in range(self.size):\n            pos = (h1 + i * h2) % self.size\n            if self.table[pos] is None or self.table[pos] == \"DELETED\":\n                self.table[pos] = key\n                return\n        raise OverflowError(\"Hash table full\")\n\n    def search(self, key):\n        h1 = self._h1(key)\n        h2 = self._h2(key)\n        for i in range(self.size):\n            pos = (h1 + i * h2) % self.size\n            if self.table[pos] is None:\n                return None\n            if self.table[pos] == key:\n                return pos\n        return None\n\n# Example\nht = DoubleHash()\nfor k in [10, 24, 31, 17]:\n    ht.insert(k)\nprint(ht.table)\nprint(\"Search 24 at:\", ht.search(24))\n\n\nWhy It Matters\n\nMinimizes primary and secondary clustering\nProbe sequences depend on key, not shared among colliding keys\nAchieves uniform distribution when both hash functions are good\nForms basis for high-performance open addressing maps\n\n\n\nA Gentle Proof (Why It Works)\nIf capacity m is prime and h₂(key) never equals 0, then each key generates a unique probe sequence covering all slots: \\[\n\\text{indices} = {h_1, h_1 + h_2, h_1 + 2h_2, \\ldots} \\bmod m\n\\] Thus an empty slot is always reachable, and searches find all candidates.\nExpected probe count ≈ \\(\\frac{1}{1 - \\alpha}\\), same as other open addressing, but with lower clustering.\n\n\nTry It Yourself\n\nExperiment with different h₂ functions (e.g. 7 - key % 7).\nCompare probe lengths with linear and quadratic probing.\nVisualize probe paths for small table sizes.\nTest with composite vs prime capacities.\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nh₁\nh₂\nProbe Sequence\nFinal Slot\n\n\n\n\ninsert\n10\n3\n4\n3\n3\n\n\ninsert\n24\n3\n4\n3, 0\n0\n\n\ninsert\n31\n3\n4\n3, 0, 4\n4\n\n\ninsert\n17\n3\n3\n3, 6\n6\n\n\nsearch\n24\n3, 0\nfound\n\n\n\n\n\nEdge Cases\n\nh₂(key) must be nonzero\nm should be prime for full coverage\nPoor hash choice → incomplete coverage\n\n\n\nComplexity\n\nTime: average O(1), worst O(n)\nSpace: O(n)\n\nDouble hashing turns collisions into a graceful dance, two hash functions weaving paths that rarely cross.\n\n\n\n215 Cuckoo Hashing\nCuckoo hashing takes inspiration from nature: like the cuckoo bird laying eggs in multiple nests, each key has more than one possible home. If a spot is taken, it kicks out the current occupant, which then moves to its alternate home, ensuring fast and predictable lookups.\n\nWhat Problem Are We Solving?\nTraditional open addressing methods (linear, quadratic, double hashing) may degrade under high load factors, causing long probe sequences. Cuckoo hashing guarantees constant-time lookups by giving each key multiple possible positions.\nGoal: Use two hash functions and relocate keys upon collision, maintaining O(1) search and insert time.\n\n\nHow Does It Work (Plain Language)?\nEach key has two candidate slots, determined by two hash functions: \\[\nh_1(k), \\quad h_2(k)\n\\]\nWhen inserting a key:\n\nTry h1(k) → if empty, place it.\nIf occupied → kick out the existing key.\nReinsert the displaced key into its alternate slot.\nRepeat until all keys placed or cycle detected (then rehash).\n\nExample Steps (Capacity = 7)\n\n\n\nStep\nKey\nh₁(key)\nh₂(key)\nAction\n\n\n\n\n1\n10\n3\n5\nSlot 3 empty → place 10\n\n\n2\n24\n3\n4\nSlot 3 occupied → move 10\n\n\n3\n10\n5\n3\nSlot 5 empty → place 10\n\n\n4\n31\n3\n6\nSlot 3 empty → place 31\n\n\n\nFinal Table\n\n\n\nIndex\nValue\n\n\n\n\n0\n-\n\n\n1\n-\n\n\n2\n-\n\n\n3\n31\n\n\n4\n24\n\n\n5\n10\n\n\n6\n-\n\n\n\nEvery key is accessible in O(1) by checking two positions only.\n\n\nTiny Code (Easy Versions)\nC (Two-Table Cuckoo Hashing)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define CAPACITY 7\n#define EMPTY -1\n#define MAX_RELOCATIONS 10\n\ntypedef struct {\n    int table1[CAPACITY];\n    int table2[CAPACITY];\n} CuckooHash;\n\nint h1(int key) { return key % CAPACITY; }\nint h2(int key) { return (key / CAPACITY) % CAPACITY; }\n\nvoid init(CuckooHash *ht) {\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        ht-&gt;table1[i] = EMPTY;\n        ht-&gt;table2[i] = EMPTY;\n    }\n}\n\nint insert(CuckooHash *ht, int key) {\n    int pos, tmp, loop_guard = 0;\n    for (int i = 0; i &lt; MAX_RELOCATIONS; i++) {\n        pos = h1(key);\n        if (ht-&gt;table1[pos] == EMPTY) {\n            ht-&gt;table1[pos] = key;\n            return 1;\n        }\n        // kick out\n        tmp = ht-&gt;table1[pos];\n        ht-&gt;table1[pos] = key;\n        key = tmp;\n        pos = h2(key);\n        if (ht-&gt;table2[pos] == EMPTY) {\n            ht-&gt;table2[pos] = key;\n            return 1;\n        }\n        tmp = ht-&gt;table2[pos];\n        ht-&gt;table2[pos] = key;\n        key = tmp;\n    }\n    printf(\"Cycle detected, rehash needed\\n\");\n    return 0;\n}\n\nint search(CuckooHash *ht, int key) {\n    int pos1 = h1(key);\n    int pos2 = h2(key);\n    if (ht-&gt;table1[pos1] == key || ht-&gt;table2[pos2] == key) return 1;\n    return 0;\n}\n\nint main(void) {\n    CuckooHash ht;\n    init(&ht);\n    insert(&ht, 10);\n    insert(&ht, 24);\n    insert(&ht, 31);\n    for (int i = 0; i &lt; CAPACITY; i++)\n        printf(\"[%d] T1=%d T2=%d\\n\", i, ht.table1[i], ht.table2[i]);\n    return 0;\n}\nPython\nclass CuckooHash:\n    def __init__(self, size=7):\n        self.size = size\n        self.table1 = [None] * size\n        self.table2 = [None] * size\n        self.max_reloc = 10\n\n    def _h1(self, key): return key % self.size\n    def _h2(self, key): return (key // self.size) % self.size\n\n    def insert(self, key):\n        for _ in range(self.max_reloc):\n            idx1 = self._h1(key)\n            if self.table1[idx1] is None:\n                self.table1[idx1] = key\n                return\n            key, self.table1[idx1] = self.table1[idx1], key  # swap\n\n            idx2 = self._h2(key)\n            if self.table2[idx2] is None:\n                self.table2[idx2] = key\n                return\n            key, self.table2[idx2] = self.table2[idx2], key  # swap\n        raise RuntimeError(\"Cycle detected, rehash needed\")\n\n    def search(self, key):\n        return key in self.table1 or key in self.table2\n\n# Example\nht = CuckooHash()\nfor k in [10, 24, 31]:\n    ht.insert(k)\nprint(\"Table1:\", ht.table1)\nprint(\"Table2:\", ht.table2)\nprint(\"Search 24:\", ht.search(24))\n\n\nWhy It Matters\n\nO(1) lookup, always two slots per key\nAvoids clustering entirely\nExcellent for high load factors (up to 0.5–0.9)\nSimple predictable probe path\nGreat choice for hardware tables (e.g. network routing)\n\n\n\nA Gentle Proof (Why It Works)\nEach key has at most two possible homes.\n\nIf both occupied, displacement ensures eventual convergence (or detects a cycle).\nCycle length bounded → rehash needed rarely.\n\nExpected insertion time = O(1) amortized; search always 2 checks only.\n\n\nTry It Yourself\n\nImplement rehash when cycle detected.\nAdd delete(key) and test reinsert.\nVisualize displacement chain on insertions.\nCompare performance with double hashing.\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nh₁\nh₂\nAction\n\n\n\n\ninsert\n10\n3\n5\nplace at 3\n\n\ninsert\n24\n3\n4\ndisplace 10 → move to 5\n\n\ninsert\n31\n3\n6\nplace at 3\n\n\nsearch\n10\n3, 5\nfound\n\n\n\n\nEdge Cases\n\nCycle detected → requires rehash\nBoth tables full → resize\nMust limit relocation attempts\n\n\n\nComplexity\n\nTime:\n\nLookup: O(1)\nInsert: O(1) amortized\n\nSpace: O(2n)\n\nCuckoo hashing keeps order in the nest, every key finds a home, or the table learns to rebuild its world.\n\n\n\n216 Robin Hood Hashing\nRobin Hood hashing is a clever twist on open addressing: when a new key collides, it compares its “distance from home” with the current occupant. If the new key has traveled farther, it steals the slot, redistributing probe distances more evenly and keeping variance low.\n\nWhat Problem Are We Solving?\nIn linear probing, unlucky keys might travel long distances while others sit close to their home. This leads to probe sequence imbalance, long searches for some keys, short for others. Robin Hood hashing “robs” near-home keys to help far-away ones, minimizing the maximum probe distance.\nGoal: Equalize probe distances by swapping keys so that no key is “too far” behind others.\n\n\nHow Does It Work (Plain Language)?\nEach entry remembers its probe distance = number of steps from its original hash slot. When inserting a new key:\n\nCompute index = hash(key) % capacity.\nIf slot empty → insert.\nIf occupied → compare probe distances.\n\nIf newcomer’s distance &gt; occupant’s distance → swap them.\nContinue insertion for the displaced key.\n\n\nExample Steps (Capacity = 7)\n\n\n\n\n\n\n\n\n\n\nStep\nKey\nHash(key)\nProbe Distance\nAction\n\n\n\n\n1\n10\n3\n0\nPlace at 3\n\n\n2\n24\n3\n0\nCollision → move to 4 (dist=1)\n\n\n3\n31\n3\n0\nCollision → dist=0 &lt; 0? no → move → dist=1 &lt; 1? no → dist=2 → place at 5\n\n\n4\n17\n3\n0\nCollision chain → compare and swap if farther\n\n\n\nThis ensures all keys stay near their home index, fairer access for all.\nResult Table:\n\n\n\nIndex\nKey\nDist\n\n\n\n\n3\n10\n0\n\n\n4\n24\n1\n\n\n5\n31\n2\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define CAPACITY 7\n#define EMPTY -1\n\ntypedef struct {\n    int key;\n    int dist; // probe distance\n} Slot;\n\ntypedef struct {\n    Slot *table;\n} HashTable;\n\nint hash(int key) { return key % CAPACITY; }\n\nHashTable* ht_create() {\n    HashTable *ht = malloc(sizeof(HashTable));\n    ht-&gt;table = malloc(sizeof(Slot) * CAPACITY);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        ht-&gt;table[i].key = EMPTY;\n        ht-&gt;table[i].dist = 0;\n    }\n    return ht;\n}\n\nvoid ht_insert(HashTable *ht, int key) {\n    int idx = hash(key);\n    int dist = 0;\n\n    while (1) {\n        if (ht-&gt;table[idx].key == EMPTY) {\n            ht-&gt;table[idx].key = key;\n            ht-&gt;table[idx].dist = dist;\n            return;\n        }\n\n        if (dist &gt; ht-&gt;table[idx].dist) {\n            // swap keys\n            int tmp_key = ht-&gt;table[idx].key;\n            int tmp_dist = ht-&gt;table[idx].dist;\n            ht-&gt;table[idx].key = key;\n            ht-&gt;table[idx].dist = dist;\n            key = tmp_key;\n            dist = tmp_dist;\n        }\n\n        idx = (idx + 1) % CAPACITY;\n        dist++;\n        if (dist &gt;= CAPACITY) {\n            printf(\"Table full\\n\");\n            return;\n        }\n    }\n}\n\nint ht_search(HashTable *ht, int key) {\n    int idx = hash(key);\n    int dist = 0;\n    while (ht-&gt;table[idx].key != EMPTY && dist &lt;= ht-&gt;table[idx].dist) {\n        if (ht-&gt;table[idx].key == key) return idx;\n        idx = (idx + 1) % CAPACITY;\n        dist++;\n    }\n    return -1;\n}\n\nint main(void) {\n    HashTable *ht = ht_create();\n    ht_insert(ht, 10);\n    ht_insert(ht, 24);\n    ht_insert(ht, 31);\n    for (int i = 0; i &lt; CAPACITY; i++) {\n        if (ht-&gt;table[i].key != EMPTY)\n            printf(\"[%d] key=%d dist=%d\\n\", i, ht-&gt;table[i].key, ht-&gt;table[i].dist);\n    }\n    return 0;\n}\nPython\nclass RobinHoodHash:\n    def __init__(self, size=7):\n        self.size = size\n        self.table = [None] * size\n        self.dist = [0] * size\n\n    def _hash(self, key):\n        return key % self.size\n\n    def insert(self, key):\n        idx = self._hash(key)\n        d = 0\n        while True:\n            if self.table[idx] is None:\n                self.table[idx] = key\n                self.dist[idx] = d\n                return\n            # Robin Hood swap if newcomer is farther\n            if d &gt; self.dist[idx]:\n                key, self.table[idx] = self.table[idx], key\n                d, self.dist[idx] = self.dist[idx], d\n            idx = (idx + 1) % self.size\n            d += 1\n            if d &gt;= self.size:\n                raise OverflowError(\"Table full\")\n\n    def search(self, key):\n        idx = self._hash(key)\n        d = 0\n        while self.table[idx] is not None and d &lt;= self.dist[idx]:\n            if self.table[idx] == key:\n                return idx\n            idx = (idx + 1) % self.size\n            d += 1\n        return None\n\n# Example\nht = RobinHoodHash()\nfor k in [10, 24, 31]:\n    ht.insert(k)\nprint(list(zip(range(ht.size), ht.table, ht.dist)))\nprint(\"Search 24:\", ht.search(24))\n\n\nWhy It Matters\n\nBalances access time across all keys\nMinimizes variance of probe lengths\nOutperforms linear probing under high load\nElegant fairness principle, long-traveling keys get priority\n\n\n\nA Gentle Proof (Why It Works)\nBy ensuring all probe distances are roughly equal, worst-case search cost ≈ average search cost. Keys never get “stuck” behind long clusters, and searches terminate early when probe distance exceeds that of existing slot.\nAverage search cost ≈ O(1 + α), but with smaller variance than standard linear probing.\n\n\nTry It Yourself\n\nInsert keys in different orders and compare probe distances.\nImplement deletion (mark deleted and shift neighbors).\nTrack average probe distance as load grows.\nCompare fairness with standard linear probing.\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nHome\nFinal Slot\nDist\nNotes\n\n\n\n\ninsert\n10\n3\n3\n0\nfirst insert\n\n\ninsert\n24\n3\n4\n1\ncollision\n\n\ninsert\n31\n3\n5\n2\nfurther collision\n\n\nsearch\n24\n3→4\nfound\n\n\n\n\n\nEdge Cases\n\nTable full → stop insertion\nMust cap distance to prevent infinite loop\nDeletion requires rebalancing neighbors\n\n\n\nComplexity\n\nTime: average O(1), worst O(n)\nSpace: O(n)\n\nRobin Hood hashing brings justice to collisions, no key left wandering too far from home.\n\n\n\n217 Chained Hash Table\nA chained hash table is the classic solution for handling collisions, instead of squeezing every key into the array, each bucket holds a linked list (or chain) of entries that share the same hash index.\n\nWhat Problem Are We Solving?\nWith open addressing, collisions force you to probe for new slots inside the array. Chaining solves collisions externally, every index points to a small dynamic list, so multiple keys can share the same slot without crowding.\nGoal: Use linked lists (chains) to store colliding keys at the same hash index, keeping insert, search, and delete simple and efficient on average.\n\n\nHow Does It Work (Plain Language)?\nEach array index stores a pointer to a linked list of key-value pairs. When inserting:\n\nCompute index = hash(key) % capacity.\nTraverse chain to check if key exists.\nIf not, append new node to the front (or back).\n\nSearching and deleting follow the same index and chain.\nExample (Capacity = 5)\n\n\n\nStep\nKey\nHash(key)\nIndex\nAction\n\n\n\n\n1\n“cat”\n2\n2\nPlace in chain[2]\n\n\n2\n“dog”\n4\n4\nPlace in chain[4]\n\n\n3\n“bat”\n2\n2\nAppend to chain[2]\n\n\n4\n“ant”\n2\n2\nAppend to chain[2]\n\n\n\nTable Structure\n\n\n\nIndex\nChain\n\n\n\n\n0\n-\n\n\n1\n-\n\n\n2\n“cat” → “bat” → “ant”\n\n\n3\n-\n\n\n4\n“dog”\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\n#define CAPACITY 5\n\ntypedef struct Node {\n    char *key;\n    int value;\n    struct Node *next;\n} Node;\n\ntypedef struct {\n    Node *buckets[CAPACITY];\n} HashTable;\n\nunsigned int hash(const char *key) {\n    unsigned int h = 0;\n    while (*key) h = h * 31 + *key++;\n    return h % CAPACITY;\n}\n\nHashTable* ht_create() {\n    HashTable *ht = malloc(sizeof(HashTable));\n    for (int i = 0; i &lt; CAPACITY; i++) ht-&gt;buckets[i] = NULL;\n    return ht;\n}\n\nvoid ht_insert(HashTable *ht, const char *key, int value) {\n    unsigned int idx = hash(key);\n    Node *node = ht-&gt;buckets[idx];\n    while (node) {\n        if (strcmp(node-&gt;key, key) == 0) { node-&gt;value = value; return; }\n        node = node-&gt;next;\n    }\n    Node *new_node = malloc(sizeof(Node));\n    new_node-&gt;key = strdup(key);\n    new_node-&gt;value = value;\n    new_node-&gt;next = ht-&gt;buckets[idx];\n    ht-&gt;buckets[idx] = new_node;\n}\n\nint ht_search(HashTable *ht, const char *key, int *out) {\n    unsigned int idx = hash(key);\n    Node *node = ht-&gt;buckets[idx];\n    while (node) {\n        if (strcmp(node-&gt;key, key) == 0) { *out = node-&gt;value; return 1; }\n        node = node-&gt;next;\n    }\n    return 0;\n}\n\nvoid ht_delete(HashTable *ht, const char *key) {\n    unsigned int idx = hash(key);\n    Node curr = &ht-&gt;buckets[idx];\n    while (*curr) {\n        if (strcmp((*curr)-&gt;key, key) == 0) {\n            Node *tmp = *curr;\n            *curr = (*curr)-&gt;next;\n            free(tmp-&gt;key);\n            free(tmp);\n            return;\n        }\n        curr = &(*curr)-&gt;next;\n    }\n}\n\nint main(void) {\n    HashTable *ht = ht_create();\n    ht_insert(ht, \"cat\", 1);\n    ht_insert(ht, \"bat\", 2);\n    ht_insert(ht, \"ant\", 3);\n    int val;\n    if (ht_search(ht, \"bat\", &val))\n        printf(\"bat: %d\\n\", val);\n    ht_delete(ht, \"bat\");\n    if (!ht_search(ht, \"bat\", &val))\n        printf(\"bat deleted\\n\");\n    return 0;\n}\nPython\nclass ChainedHash:\n    def __init__(self, size=5):\n        self.size = size\n        self.table = [[] for _ in range(size)]\n\n    def _hash(self, key):\n        return hash(key) % self.size\n\n    def insert(self, key, value):\n        idx = self._hash(key)\n        for pair in self.table[idx]:\n            if pair[0] == key:\n                pair[1] = value\n                return\n        self.table[idx].append([key, value])\n\n    def search(self, key):\n        idx = self._hash(key)\n        for k, v in self.table[idx]:\n            if k == key:\n                return v\n        return None\n\n    def delete(self, key):\n        idx = self._hash(key)\n        self.table[idx] = [p for p in self.table[idx] if p[0] != key]\n\n# Example\nht = ChainedHash()\nht.insert(\"cat\", 1)\nht.insert(\"bat\", 2)\nht.insert(\"ant\", 3)\nprint(ht.table)\nprint(\"Search bat:\", ht.search(\"bat\"))\nht.delete(\"bat\")\nprint(ht.table)\n\n\nWhy It Matters\n\nSimple and reliable collision handling\nLoad factor can exceed 1 (chains absorb overflow)\nDeletion is straightforward (just remove node)\nPerformance stable even at high load (if hash spread is uniform)\n\n\n\nA Gentle Proof (Why It Works)\nExpected chain length = load factor \\(\\alpha = \\frac{n}{m}\\). Each operation traverses a single chain, so average cost = O(1 + α). Uniform hash distribution ensures α remains small → operations ≈ O(1).\n\n\nTry It Yourself\n\nImplement dynamic resizing when average chain length grows.\nCompare prepend vs append strategies.\nMeasure average search steps as table fills.\nReplace linked list with balanced tree (for high α).\n\n\n\nTest Cases\n\n\n\nOperation\nKey\nIndex\nChain Result\nNotes\n\n\n\n\ninsert\n“cat”\n2\n[“cat”]\n\n\n\ninsert\n“bat”\n2\n[“bat”, “cat”]\ncollision\n\n\ninsert\n“ant”\n2\n[“ant”, “bat”, “cat”]\nchain grows\n\n\nsearch\n“bat”\n2\nfound\n\n\n\ndelete\n“bat”\n2\n[“ant”, “cat”]\nremoved\n\n\n\nEdge Cases\n\nMany keys same index → long chains\nPoor hash function → uneven distribution\nNeeds memory for pointers/nodes\n\n\n\nComplexity\n\nTime: average O(1), worst O(n) (all in one chain)\nSpace: O(n + m)\n\nChained hashing turns collisions into conversation, if one bucket’s full, it just lines them up neatly in a list.\n\n\n\n218 Perfect Hashing\nPerfect hashing is the dream scenario for hash tables, no collisions at all. Every key maps to a unique slot, so lookups, inserts, and deletes all take O(1) time worst case, not just on average.\n\nWhat Problem Are We Solving?\nMost hashing strategies (linear probing, chaining, cuckoo) deal with collisions after they happen. Perfect hashing eliminates them entirely by designing a collision-free hash function for a fixed key set.\nGoal: Construct a hash function \\(h(k)\\) such that all keys map to distinct indices.\n\n\nHow Does It Work (Plain Language)?\nIf the set of keys is known in advance (static set), we can carefully choose or build a hash function that gives each key a unique slot.\nTwo main types:\n\nPerfect Hashing, no collisions.\nMinimal Perfect Hashing, no collisions and table size = number of keys.\n\nSimple Example (Keys = {10, 24, 31, 17}, Capacity = 7)\nLet’s find a function: \\[\nh(k) = (a \\cdot k + b) \\bmod 7\n\\] We can search for coefficients a and b that produce unique indices:\n\n\n\nKey\nh(k) = (2k + 1) mod 7\nIndex\n\n\n\n\n10\n(21) % 7 = 0\n0\n\n\n24\n(49) % 7 = 0\n❌ collision\n\n\n31\n(63) % 7 = 0\n❌ collision\n\n\n\nSo we try another pair (a=3, b=2):\n\n\n\nKey\n(3k + 2) mod 7\nIndex\n\n\n\n\n10\n5\n5\n\n\n24\n4\n4\n\n\n31\n4\n❌ collision\n\n\n17\n6\n6\n\n\n\nEventually, we find a mapping with no repeats by adjusting parameters or using two-level construction.\n\n\nTwo-Level Perfect Hashing\nPractical perfect hashing often uses a two-level scheme:\n\nTop level: Hash keys into buckets.\nSecond level: Each bucket gets its own small hash table with its own perfect hash function.\n\nThis ensures zero collisions overall, with total space ≈ O(n).\nProcess:\n\nEach bucket of size b gets a secondary table of size b².\nUse a second hash \\(h_i\\) for that bucket to place all keys uniquely.\n\n\n\nTiny Code (Easy Version)\nPython (Two-Level Static Perfect Hashing)\nimport random\n\nclass PerfectHash:\n    def __init__(self, keys):\n        self.n = len(keys)\n        self.size = self.n\n        self.buckets = [[] for _ in range(self.size)]\n        self.secondary = [None] * self.size\n\n        # First-level hashing\n        a, b = 3, 5  # fixed small hash parameters\n        def h1(k): return (a * k + b) % self.size\n\n        # Distribute keys into buckets\n        for k in keys:\n            self.buckets[h1(k)].append(k)\n\n        # Build second-level tables\n        for i, bucket in enumerate(self.buckets):\n            if not bucket:\n                continue\n            m = len(bucket)  2\n            table = [None] * m\n            found = False\n            while not found:\n                found = True\n                a2, b2 = random.randint(1, m - 1), random.randint(0, m - 1)\n                def h2(k): return (a2 * k + b2) % m\n                table = [None] * m\n                for k in bucket:\n                    pos = h2(k)\n                    if table[pos] is not None:\n                        found = False\n                        break\n                    table[pos] = k\n            self.secondary[i] = (table, a2, b2)\n\n        self.h1 = h1\n\n    def search(self, key):\n        i = self.h1(key)\n        table, a2, b2 = self.secondary[i]\n        m = len(table)\n        pos = (a2 * key + b2) % m\n        return table[pos] == key\n\n# Example\nkeys = [10, 24, 31, 17]\nph = PerfectHash(keys)\nprint([len(b) for b in ph.buckets])\nprint(\"Search 24:\", ph.search(24))\nprint(\"Search 11:\", ph.search(11))\n\n\nWhy It Matters\n\nGuaranteed O(1) worst-case lookup\nNo clustering, no collisions, no chains\nIdeal for static key sets (e.g. reserved keywords in a compiler, routing tables)\nMemory predictable, access blazing fast\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n=|S|\\) be the number of keys.\nWith a truly random hash family into \\(m\\) buckets, the collision probability for two distinct keys is \\(1/m\\).\nChoose \\(m=n^2\\). Then the expected number of collisions is \\[\n\\mathbb{E}[C]=\\binom{n}{2}\\cdot \\frac{1}{m}\n=\\frac{n(n-1)}{2n^2}&lt;\\tfrac12.\n\\] By Markov, \\(\\Pr[C\\ge1]\\le \\mathbb{E}[C]&lt;\\tfrac12\\), so \\(\\Pr[C=0]&gt;\\tfrac12\\).\nTherefore a collision-free hash exists. In practice, try random seeds until one has \\(C=0\\), or use a deterministic construction for perfect hashing.\n\n\nTry It Yourself\n\nGenerate perfect hash for small static set {“if”, “else”, “for”, “while”}.\nBuild minimal perfect hash (table size = n).\nCompare lookup times with standard dict.\nVisualize second-level hash table sizes.\n\n\n\nTest Cases\n\n\n\nOperation\nKeys\nResult\nNotes\n\n\n\n\nbuild\n[10,24,31,17]\nsuccess\neach slot unique\n\n\nsearch\n24\nTrue\nfound\n\n\nsearch\n11\nFalse\nnot in table\n\n\ncollisions\nnone\n\nperfect mapping\n\n\n\nEdge Cases\n\nWorks only for static sets (no dynamic inserts)\nBuilding may require rehash trials\nMemory ↑ with quadratic secondary tables\n\n\n\nComplexity\n\nBuild: O(n²) (search for collision-free mapping)\nLookup: O(1)\nSpace: O(n) to O(n²) depending on method\n\nPerfect hashing is like finding the perfect key for every lock, built once, opens instantly, never collides.\n\n\n\n219 Consistent Hashing\nConsistent hashing is a collision-handling strategy designed for distributed systems rather than single in-memory tables. It ensures that when nodes (servers, caches, or shards) join or leave, only a small fraction of keys need to be remapped, making it the backbone of scalable, fault-tolerant architectures.\n\nWhat Problem Are We Solving?\nIn traditional hashing (e.g. hash(key) % n), when the number of servers n changes, almost every key’s location changes. That’s disastrous for caching, databases, or load balancing.\nConsistent hashing fixes this by mapping both keys and servers into the same hash space, and placing keys near their nearest server clockwise, minimizing reassignments when the system changes.\nGoal: Achieve stable key distribution under dynamic server counts with minimal movement and good balance.\n\n\nHow Does It Work (Plain Language)?\n\nImagine a hash ring, numbers 0 through (2^{m}-1) arranged in a circle.\nEach node (server) and key is hashed to a position on the ring.\nA key is assigned to the next server clockwise from its hash position.\nWhen a node joins/leaves, only keys in its immediate region move.\n\nExample (Capacity = 2¹⁶)\n\n\n\nItem\nHash\nPlaced On Ring\nOwner\n\n\n\n\nNode A\n1000\n•\n\n\n\nNode B\n4000\n•\n\n\n\nNode C\n8000\n•\n\n\n\nKey K₁\n1200\n→ Node B\n\n\n\nKey K₂\n8500\n→ Node A (wrap-around)\n\n\n\n\nIf Node B leaves, only keys in its segment (1000–4000) move, everything else stays put.\n\n\nImproving Load Balance\nTo prevent uneven distribution, each node is represented by multiple virtual nodes (vnodes), each vnode gets its own hash. This smooths the key spread across all nodes.\nExample:\n\nNode A → hashes to 1000, 6000\nNode B → hashes to 3000, 9000\n\nKeys are assigned to closest vnode clockwise.\n\n\nTiny Code (Easy Versions)\nPython (Simple Consistent Hashing with Virtual Nodes)\nimport bisect\nimport hashlib\n\ndef hash_fn(key):\n    return int(hashlib.md5(str(key).encode()).hexdigest(), 16)\n\nclass ConsistentHash:\n    def __init__(self, nodes=None, vnodes=3):\n        self.ring = []\n        self.map = {}\n        self.vnodes = vnodes\n        if nodes:\n            for n in nodes:\n                self.add_node(n)\n\n    def add_node(self, node):\n        for i in range(self.vnodes):\n            h = hash_fn(f\"{node}-{i}\")\n            self.map[h] = node\n            bisect.insort(self.ring, h)\n\n    def remove_node(self, node):\n        for i in range(self.vnodes):\n            h = hash_fn(f\"{node}-{i}\")\n            self.ring.remove(h)\n            del self.map[h]\n\n    def get_node(self, key):\n        if not self.ring:\n            return None\n        h = hash_fn(key)\n        idx = bisect.bisect(self.ring, h) % len(self.ring)\n        return self.map[self.ring[idx]]\n\n# Example\nch = ConsistentHash([\"A\", \"B\", \"C\"], vnodes=2)\nprint(\"Key 42 -&gt;\", ch.get_node(42))\nch.remove_node(\"B\")\nprint(\"After removing B, Key 42 -&gt;\", ch.get_node(42))\n\n\nWhy It Matters\n\nMinimizes key remapping when nodes change (≈ 1/n of keys move)\nEnables elastic scaling for caches, DB shards, and distributed stores\nUsed in systems like Amazon Dynamo, Cassandra, Riak, and memcached clients\nBalances load using virtual nodes\nDecouples hash function from node count\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(N\\) be the number of nodes and \\(K\\) the number of keys.\nEach node is responsible for a fraction \\(\\frac{1}{N}\\) of the ring.\nWhen one node leaves, only its segment’s keys move, about \\(K/N\\).\nSo the expected remapping fraction is \\(\\frac{1}{N}\\).\nAdding virtual nodes (vnodes) increases uniformity.\nWith \\(V\\) vnodes per physical node, the variance of the per-node load fraction scales as \\(\\approx \\frac{1}{V}\\).\n\n\nTry It Yourself\n\nAdd and remove nodes, track how many keys move.\nExperiment with different vnode counts (1, 10, 100).\nVisualize hash ring, mark nodes and key positions.\nSimulate caching: assign 1000 keys, remove one node, count moves.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nOperation\nNode(s)\nKeys\nResult\nNotes\n\n\n\n\nadd_nodes\nA, B, C\n[1..1000]\ndistributed evenly\n\n\n\nremove_node\nB\n[1..1000]\n~1/3 keys moved\nstability check\n\n\nadd_node\nD\n[1..1000]\n~1/4 keys remapped\n\n\n\nlookup\n42\n-&gt; Node C\nconsistent mapping\n\n\n\n\nEdge Cases\n\nEmpty ring → return None\nDuplicate nodes → handle via unique vnode IDs\nWithout vnodes → uneven load\n\n\n\nComplexity\n\nLookup: \\(O(\\log n)\\) (binary search in the ring)\n\nInsert/Delete node: \\(O(v \\log n)\\)\n\nSpace: \\(O(n \\times v)\\)\n\nConsistent hashing keeps order in the storm — servers may come and go, but most keys stay right where they belong.\n\n\n\n220 Dynamic Rehashing\nDynamic rehashing is how a hash table gracefully adapts as data grows or shrinks. Instead of being trapped in a fixed-size array, the table resizes itself, rebuilding its layout so that load stays balanced and lookups remain fast.\n\nWhat Problem Are We Solving?\nWhen a hash table fills up, collisions become frequent, degrading performance to O(n). We need a mechanism to maintain a low load factor (ratio of elements to capacity) by resizing and rehashing automatically.\nGoal: Detect when load factor crosses a threshold, allocate a larger array, and rehash all keys into their new positions efficiently.\n\n\nHow Does It Work (Plain Language)?\n\nMonitor load factor\n\\[\n\\alpha = \\frac{n}{m}\n\\] where \\(n\\) is the number of elements and \\(m\\) is the table size.\nTrigger rehash\n\nIf \\(\\alpha &gt; 0.75\\), expand the table (for example, double the capacity).\n\nIf \\(\\alpha &lt; 0.25\\), shrink it (optional).\n\nRebuild\n\nCreate a new table with the updated capacity.\n\nReinsert each key using the new hash function modulo the new capacity.\n\n\nExample Steps\n\n\n\nStep\nCapacity\nItems\nLoad Factor\nAction\n\n\n\n\n1\n4\n2\n0.5\nok\n\n\n2\n4\n3\n0.75\nok\n\n\n3\n4\n4\n1.0\nresize to 8\n\n\n4\n8\n4\n0.5\nrehashed\n\n\n\nEvery key gets new position because hash(key) % new_capacity changes.\n\n\nIncremental Rehashing\nInstead of rehashing all keys at once (costly spike), incremental rehashing spreads work across operations:\n\nMaintain both old and new tables.\nRehash a few entries per insert/search until old table empty.\n\nThis keeps amortized O(1) performance, even during resize.\n\n\nTiny Code (Easy Versions)\nC (Simple Doubling Rehash)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define INIT_CAP 4\n\ntypedef struct {\n    int *keys;\n    int size;\n    int count;\n} HashTable;\n\nint hash(int key, int size) { return key % size; }\n\nHashTable* ht_create(int size) {\n    HashTable *ht = malloc(sizeof(HashTable));\n    ht-&gt;keys = malloc(sizeof(int) * size);\n    for (int i = 0; i &lt; size; i++) ht-&gt;keys[i] = -1;\n    ht-&gt;size = size;\n    ht-&gt;count = 0;\n    return ht;\n}\n\nvoid ht_resize(HashTable *ht, int new_size) {\n    printf(\"Resizing from %d to %d\\n\", ht-&gt;size, new_size);\n    int *old_keys = ht-&gt;keys;\n    int old_size = ht-&gt;size;\n\n    ht-&gt;keys = malloc(sizeof(int) * new_size);\n    for (int i = 0; i &lt; new_size; i++) ht-&gt;keys[i] = -1;\n\n    ht-&gt;size = new_size;\n    ht-&gt;count = 0;\n    for (int i = 0; i &lt; old_size; i++) {\n        if (old_keys[i] != -1) {\n            int key = old_keys[i];\n            int idx = hash(key, new_size);\n            while (ht-&gt;keys[idx] != -1) idx = (idx + 1) % new_size;\n            ht-&gt;keys[idx] = key;\n            ht-&gt;count++;\n        }\n    }\n    free(old_keys);\n}\n\nvoid ht_insert(HashTable *ht, int key) {\n    float load = (float)ht-&gt;count / ht-&gt;size;\n    if (load &gt; 0.75) ht_resize(ht, ht-&gt;size * 2);\n\n    int idx = hash(key, ht-&gt;size);\n    while (ht-&gt;keys[idx] != -1) idx = (idx + 1) % ht-&gt;size;\n    ht-&gt;keys[idx] = key;\n    ht-&gt;count++;\n}\n\nint main(void) {\n    HashTable *ht = ht_create(INIT_CAP);\n    for (int i = 0; i &lt; 10; i++) ht_insert(ht, i * 3);\n    for (int i = 0; i &lt; ht-&gt;size; i++)\n        printf(\"[%d] = %d\\n\", i, ht-&gt;keys[i]);\n    return 0;\n}\nPython\nclass DynamicHash:\n    def __init__(self, cap=4):\n        self.cap = cap\n        self.size = 0\n        self.table = [None] * cap\n\n    def _hash(self, key):\n        return hash(key) % self.cap\n\n    def _rehash(self, new_cap):\n        old_table = self.table\n        self.table = [None] * new_cap\n        self.cap = new_cap\n        self.size = 0\n        for key in old_table:\n            if key is not None:\n                self.insert(key)\n\n    def insert(self, key):\n        if self.size / self.cap &gt; 0.75:\n            self._rehash(self.cap * 2)\n        idx = self._hash(key)\n        while self.table[idx] is not None:\n            idx = (idx + 1) % self.cap\n        self.table[idx] = key\n        self.size += 1\n\n# Example\nht = DynamicHash()\nfor k in [10, 24, 31, 17, 19, 42, 56, 77]:\n    ht.insert(k)\nprint(ht.table)\n\n\nWhy It Matters\n\nKeeps load factor stable for O(1) operations\nPrevents clustering in open addressing\nSupports unbounded growth\nFoundation for dynamic dictionaries, maps, caches\n\n\n\nA Gentle Proof (Why It Works)\nIf each rehash doubles capacity, total cost of N inserts = O(N). Each element is moved O(1) times (once per doubling), so amortized cost per insert is O(1).\nIncremental rehashing further ensures no single operation is expensive, work spread evenly.\n\n\nTry It Yourself\n\nAdd printouts to observe load factor at each insert.\nImplement shrink when load &lt; 0.25.\nImplement incremental rehash with two tables.\nCompare doubling vs prime-capacity growth.\n\n\n\nTest Cases\n\n\n\nStep\nCapacity\nItems\nLoad Factor\nAction\n\n\n\n\n1\n4\n2\n0.5\nnone\n\n\n2\n4\n3\n0.75\nok\n\n\n3\n4\n4\n1.0\nresize\n\n\n4\n8\n4\n0.5\nrehashed\n\n\n\nEdge Cases\n\nRehash must handle deleted slots correctly\nAvoid resizing too frequently (hysteresis)\nKeep hash function consistent across resizes\n\n\n\nComplexity\n\nAverage Insert/Search/Delete: O(1)\nAmortized Insert: O(1)\nWorst-case Resize: O(n)\n\nDynamic rehashing is the table’s heartbeat, it expands when full, contracts when idle, always keeping operations smooth and steady.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-23.-heaps",
    "href": "books/en-us/list-3.html#section-23.-heaps",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 23. Heaps",
    "text": "Section 23. Heaps\n\n221 Binary Heap Insert\nA binary heap is a complete binary tree stored in an array that maintains the heap property, each parent is smaller (min-heap) or larger (max-heap) than its children. The insert operation keeps the heap ordered by bubbling up the new element until it finds the right place.\n\nWhat Problem Are We Solving?\nWe need a data structure that efficiently gives access to the minimum (or maximum) element, while supporting fast insertions.\nA binary heap offers:\n\nO(1) access to min/max\nO(log n) insertion and deletion\nO(n) build time for initial heap\n\nGoal: Insert a new element while maintaining the heap property and completeness.\n\n\nHow Does It Work (Plain Language)?\nA heap is stored as an array representing a complete binary tree. Each node at index i has:\n\nParent: (i - 1) / 2\nLeft child: 2i + 1\nRight child: 2i + 2\n\nInsertion Steps (Min-Heap)\n\nAppend the new element at the end (bottom level, rightmost).\nCompare it with its parent.\nIf smaller (min-heap) or larger (max-heap), swap.\nRepeat until the heap property is restored.\n\nExample (Min-Heap)\nInsert sequence: [10, 24, 5, 31]\n\n\n\nStep\nArray\nAction\n\n\n\n\nStart\n[ ]\nempty\n\n\nInsert 10\n[10]\nroot only\n\n\nInsert 24\n[10, 24]\n24 &gt; 10, no swap\n\n\nInsert 5\n[10, 24, 5]\n5 &lt; 10 → swap → [5, 24, 10]\n\n\nInsert 31\n[5, 24, 10, 31]\n31 &gt; 24, no swap\n\n\n\nFinal heap: [5, 24, 10, 31]\nTree view:\n        5\n      /   \\\n    24     10\n   /\n 31\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#define MAX 100\n\ntypedef struct {\n    int arr[MAX];\n    int size;\n} MinHeap;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid heap_insert(MinHeap *h, int val) {\n    int i = h-&gt;size++;\n    h-&gt;arr[i] = val;\n\n    // bubble up\n    while (i &gt; 0) {\n        int parent = (i - 1) / 2;\n        if (h-&gt;arr[i] &gt;= h-&gt;arr[parent]) break;\n        swap(&h-&gt;arr[i], &h-&gt;arr[parent]);\n        i = parent;\n    }\n}\n\nvoid heap_print(MinHeap *h) {\n    for (int i = 0; i &lt; h-&gt;size; i++) printf(\"%d \", h-&gt;arr[i]);\n    printf(\"\\n\");\n}\n\nint main(void) {\n    MinHeap h = {.size = 0};\n    int vals[] = {10, 24, 5, 31};\n    for (int i = 0; i &lt; 4; i++) heap_insert(&h, vals[i]);\n    heap_print(&h);\n}\nPython\nclass MinHeap:\n    def __init__(self):\n        self.arr = []\n\n    def _parent(self, i): return (i - 1) // 2\n\n    def insert(self, val):\n        self.arr.append(val)\n        i = len(self.arr) - 1\n        while i &gt; 0:\n            p = self._parent(i)\n            if self.arr[i] &gt;= self.arr[p]:\n                break\n            self.arr[i], self.arr[p] = self.arr[p], self.arr[i]\n            i = p\n\n    def __repr__(self):\n        return str(self.arr)\n\n# Example\nh = MinHeap()\nfor x in [10, 24, 5, 31]:\n    h.insert(x)\nprint(h)\n\n\nWhy It Matters\n\nFundamental for priority queues\nCore of Dijkstra’s shortest path, Prim’s MST, and schedulers\nSupports efficient insert, extract-min/max, and peek\nUsed in heapsort and event-driven simulations\n\n\n\nA Gentle Proof (Why It Works)\nEach insertion bubbles up at most height of heap = \\(\\log_2 n\\). Since heap always remains complete, the structure is balanced, ensuring logarithmic operations.\nHeap property is preserved because every swap ensures parent ≤ child (min-heap).\n\n\nTry It Yourself\n\nInsert elements in descending order → observe bubbling.\nSwitch comparisons for max-heap.\nPrint tree level by level after each insert.\nImplement extract_min() to remove root and restore heap.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\nNotes\n\n\n\n\nInsert\n[10, 24, 5, 31]\n[5, 24, 10, 31]\nmin-heap property\n\n\nInsert\n[3, 2, 1]\n[1, 3, 2]\nswap chain\n\n\nInsert\n[10]\n[10]\nsingle element\n\n\nInsert\n[]\n[x]\nempty start\n\n\n\nEdge Cases\n\nFull array (static heap) → resize needed\nNegative values → handled same\nDuplicate keys → order preserved\n\n\n\nComplexity\n\nInsert: O(log n)\nSearch: O(n) (unsorted beyond heap property)\nSpace: O(n)\n\nBinary heap insertion is the heartbeat of priority queues, each element climbs to its rightful place, one gentle swap at a time.\n\n\n\n222 Binary Heap Delete\nDeleting from a binary heap means removing the root element (the minimum in a min-heap or maximum in a max-heap) while keeping both the heap property and complete tree structure intact. To do this, we swap the root with the last element, remove the last, and bubble down the new root until the heap is valid again.\n\nWhat Problem Are We Solving?\nWe want to remove the highest-priority element quickly (min or max) from a heap, without breaking the structure.\nIn a min-heap, the smallest value always lives at index 0. In a max-heap, the largest value lives at index 0.\nGoal: Efficiently remove the root and restore order in O(log n) time.\n\n\nHow Does It Work (Plain Language)?\n\nSwap root (index 0) with last element.\nRemove last element (now root value is gone).\nHeapify down (bubble down) from the root:\n\nCompare with children.\nSwap with smaller (min-heap) or larger (max-heap) child.\nRepeat until heap property restored.\n\n\nExample (Min-Heap)\nStart: [5, 24, 10, 31] Remove min (5):\n\n\n\n\n\n\n\n\n\nStep\nAction\nArray\n\n\n\n\n\n1\nSwap root (5) with last (31)\n[31, 24, 10, 5]\n\n\n\n2\nRemove last\n[31, 24, 10]\n\n\n\n3\nCompare 31 with children (24, 10) → smallest = 10\nswap\n[10, 24, 31]\n\n\n4\nStop (31 &gt; no child)\n[10, 24, 31]\n\n\n\n\nResult: [10, 24, 31], still a valid min-heap.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#define MAX 100\n\ntypedef struct {\n    int arr[MAX];\n    int size;\n} MinHeap;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid heapify_down(MinHeap *h, int i) {\n    int smallest = i;\n    int left = 2*i + 1;\n    int right = 2*i + 2;\n\n    if (left &lt; h-&gt;size && h-&gt;arr[left] &lt; h-&gt;arr[smallest])\n        smallest = left;\n    if (right &lt; h-&gt;size && h-&gt;arr[right] &lt; h-&gt;arr[smallest])\n        smallest = right;\n\n    if (smallest != i) {\n        swap(&h-&gt;arr[i], &h-&gt;arr[smallest]);\n        heapify_down(h, smallest);\n    }\n}\n\nint heap_delete_min(MinHeap *h) {\n    if (h-&gt;size == 0) return -1;\n    int root = h-&gt;arr[0];\n    h-&gt;arr[0] = h-&gt;arr[h-&gt;size - 1];\n    h-&gt;size--;\n    heapify_down(h, 0);\n    return root;\n}\n\nint main(void) {\n    MinHeap h = {.arr = {5, 24, 10, 31}, .size = 4};\n    int val = heap_delete_min(&h);\n    printf(\"Deleted: %d\\n\", val);\n    for (int i = 0; i &lt; h.size; i++) printf(\"%d \", h.arr[i]);\n    printf(\"\\n\");\n}\nPython\nclass MinHeap:\n    def __init__(self):\n        self.arr = []\n\n    def _parent(self, i): return (i - 1) // 2\n    def _left(self, i): return 2 * i + 1\n    def _right(self, i): return 2 * i + 2\n\n    def insert(self, val):\n        self.arr.append(val)\n        i = len(self.arr) - 1\n        while i &gt; 0 and self.arr[i] &lt; self.arr[self._parent(i)]:\n            p = self._parent(i)\n            self.arr[i], self.arr[p] = self.arr[p], self.arr[i]\n            i = p\n\n    def _heapify_down(self, i):\n        smallest = i\n        left, right = self._left(i), self._right(i)\n        n = len(self.arr)\n        if left &lt; n and self.arr[left] &lt; self.arr[smallest]:\n            smallest = left\n        if right &lt; n and self.arr[right] &lt; self.arr[smallest]:\n            smallest = right\n        if smallest != i:\n            self.arr[i], self.arr[smallest] = self.arr[smallest], self.arr[i]\n            self._heapify_down(smallest)\n\n    def delete_min(self):\n        if not self.arr:\n            return None\n        root = self.arr[0]\n        last = self.arr.pop()\n        if self.arr:\n            self.arr[0] = last\n            self._heapify_down(0)\n        return root\n\n# Example\nh = MinHeap()\nfor x in [5, 24, 10, 31]:\n    h.insert(x)\nprint(\"Before:\", h.arr)\nprint(\"Deleted:\", h.delete_min())\nprint(\"After:\", h.arr)\n\n\nWhy It Matters\n\nKey operation in priority queues\nCore of Dijkstra’s and Prim’s algorithms\nBasis of heap sort (repeated delete-min)\nEnsures efficient extraction of extreme element\n\n\n\nA Gentle Proof (Why It Works)\nEach delete operation:\n\nConstant-time root removal\nLogarithmic heapify-down (height of tree = log n) → Total cost: O(log n)\n\nThe heap property holds because every swap moves a larger (min-heap) or smaller (max-heap) element down to children, ensuring local order at each step.\n\n\nTry It Yourself\n\nDelete repeatedly to sort the array (heap sort).\nTry max-heap delete (reverse comparisons).\nVisualize swaps after each deletion.\nTest on ascending/descending input sequences.\n\n\n\nTest Cases\n\n\n\nInput\nOperation\nOutput\nHeap After\nNotes\n\n\n\n\n[5,24,10,31]\ndelete\n5\n[10,24,31]\nvalid\n\n\n[1,3,2]\ndelete\n1\n[2,3]\nOK\n\n\n[10]\ndelete\n10\n[]\nempty heap\n\n\n[]\ndelete\nNone\n[]\nsafe\n\n\n\nEdge Cases\n\nEmpty heap → return sentinel\nSingle element → clears heap\nDuplicates handled naturally\n\n\n\nComplexity\n\nDelete root: O(log n)\nSpace: O(n)\n\nDeleting from a heap is like removing the top card from a neat stack, replace it, sift it down, and balance restored.\n\n\n\n223 Build Heap (Heapify)\nHeapify (Build Heap) is the process of constructing a valid binary heap from an unsorted array in O(n) time. Instead of inserting elements one by one, we reorganize the array in place so every parent satisfies the heap property.\n\nWhat Problem Are We Solving?\nIf we insert each element individually into an empty heap, total time is O(n log n). But we can do better. By heapifying from the bottom up, we can build the entire heap in O(n), crucial for heapsort and initializing priority queues efficiently.\nGoal: Turn any array into a valid min-heap or max-heap quickly.\n\n\nHow Does It Work (Plain Language)?\nA heap stored in an array represents a complete binary tree:\n\nFor node i, children are 2i + 1 and 2i + 2\n\nTo build the heap:\n\nStart from the last non-leaf node = (n / 2) - 1.\nApply heapify-down (sift down) to ensure the subtree rooted at i satisfies heap property.\nMove upwards to the root, repeating the process.\n\nEach subtree becomes a valid heap, and when done, the whole array is a heap.\nExample (Min-Heap)\nStart: [31, 10, 24, 5, 12, 7]\n\n\n\nStep\ni\nSubtree\nAction\nResult\n\n\n\n\nStart\n-\nfull array\n-\n[31, 10, 24, 5, 12, 7]\n\n\n1\n2\n(24, 7)\n24 &gt; 7 → swap\n[31, 10, 7, 5, 12, 24]\n\n\n2\n1\n(10, 5, 12)\n10 &gt; 5 → swap\n[31, 5, 7, 10, 12, 24]\n\n\n3\n0\n(31, 5, 7)\n31 &gt; 5 → swap\n[5, 31, 7, 10, 12, 24]\n\n\n4\n1\n(31, 10, 12)\n31 &gt; 10 → swap\n[5, 10, 7, 31, 12, 24]\n\n\n\nFinal heap: [5, 10, 7, 31, 12, 24]\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up Build Heap)\n#include &lt;stdio.h&gt;\n\n#define MAX 100\n\ntypedef struct {\n    int arr[MAX];\n    int size;\n} MinHeap;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid heapify_down(MinHeap *h, int i) {\n    int smallest = i;\n    int left = 2 * i + 1;\n    int right = 2 * i + 2;\n\n    if (left &lt; h-&gt;size && h-&gt;arr[left] &lt; h-&gt;arr[smallest])\n        smallest = left;\n    if (right &lt; h-&gt;size && h-&gt;arr[right] &lt; h-&gt;arr[smallest])\n        smallest = right;\n    if (smallest != i) {\n        swap(&h-&gt;arr[i], &h-&gt;arr[smallest]);\n        heapify_down(h, smallest);\n    }\n}\n\nvoid build_heap(MinHeap *h) {\n    for (int i = h-&gt;size / 2 - 1; i &gt;= 0; i--)\n        heapify_down(h, i);\n}\n\nint main(void) {\n    MinHeap h = {.arr = {31, 10, 24, 5, 12, 7}, .size = 6};\n    build_heap(&h);\n    for (int i = 0; i &lt; h.size; i++) printf(\"%d \", h.arr[i]);\n    printf(\"\\n\");\n}\nPython\nclass MinHeap:\n    def __init__(self, arr):\n        self.arr = arr\n        self.size = len(arr)\n        self.build_heap()\n\n    def _left(self, i): return 2 * i + 1\n    def _right(self, i): return 2 * i + 2\n\n    def _heapify_down(self, i):\n        smallest = i\n        l, r = self._left(i), self._right(i)\n        if l &lt; self.size and self.arr[l] &lt; self.arr[smallest]:\n            smallest = l\n        if r &lt; self.size and self.arr[r] &lt; self.arr[smallest]:\n            smallest = r\n        if smallest != i:\n            self.arr[i], self.arr[smallest] = self.arr[smallest], self.arr[i]\n            self._heapify_down(smallest)\n\n    def build_heap(self):\n        for i in range(self.size // 2 - 1, -1, -1):\n            self._heapify_down(i)\n\n# Example\narr = [31, 10, 24, 5, 12, 7]\nh = MinHeap(arr)\nprint(h.arr)\n\n\nWhy It Matters\n\nBuilds a valid heap in O(n) (not O(n log n))\nUsed in heapsort initialization\nEfficient for constructing priority queues from bulk data\nGuarantees balanced tree structure automatically\n\n\n\nA Gentle Proof (Why It Works)\nEach node at depth d takes O(height) = O(log(n/d)) work. There are more nodes at lower depths (less work each), fewer at top (more work each). Sum across levels yields O(n) total time, not O(n log n).\nHence, bottom-up heapify is asymptotically optimal.\n\n\nTry It Yourself\n\nRun with different array sizes, random orders.\nCompare time vs inserting one-by-one.\nFlip comparisons for max-heap.\nVisualize swaps as a tree.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[31,10,24,5,12,7]\n[5,10,7,31,12,24]\nvalid min-heap\n\n\n[3,2,1]\n[1,2,3]\nheapified\n\n\n[10]\n[10]\nsingle\n\n\n[]\n[]\nempty safe\n\n\n\nEdge Cases\n\nAlready heap → no change\nReverse-sorted input → many swaps\nDuplicates handled correctly\n\n\n\nComplexity\n\nTime: O(n)\nSpace: O(1) (in-place)\n\nHeapify is the quiet craftsman, shaping chaos into order with gentle swaps from the ground up.\n\n\n\n224 Heap Sort\nHeap Sort is a classic comparison-based sorting algorithm that uses a binary heap to organize data and extract elements in sorted order. By building a max-heap (or min-heap) and repeatedly removing the root, we achieve a fully sorted array in O(n log n) time, with no extra space.\n\nWhat Problem Are We Solving?\nWe want a fast, in-place sorting algorithm that:\n\nDoesn’t require recursion (like mergesort)\nHas predictable O(n log n) behavior\nAvoids worst-case quadratic time (like quicksort)\n\nGoal: Use the heap’s structure to repeatedly select the next largest (or smallest) element efficiently.\n\n\nHow Does It Work (Plain Language)?\n\nBuild a max-heap from the unsorted array.\nRepeat until heap is empty:\n\nSwap root (max element) with last element.\nReduce heap size by one.\nHeapify-down the new root to restore heap property.\n\n\nThe array becomes sorted in ascending order (for max-heap).\nExample (Ascending Sort)\nStart: [5, 31, 10, 24, 7]\n\n\n\nStep\nAction\nArray\n\n\n\n\n1\nBuild max-heap\n[31, 24, 10, 5, 7]\n\n\n2\nSwap 31 ↔︎ 7, heapify\n[24, 7, 10, 5, 31]\n\n\n3\nSwap 24 ↔︎ 5, heapify\n[10, 7, 5, 24, 31]\n\n\n4\nSwap 10 ↔︎ 5, heapify\n[5, 7, 10, 24, 31]\n\n\n5\nSorted\n[5, 7, 10, 24, 31]\n\n\n\n\n\nTiny Code (Easy Versions)\nC (In-place Heap Sort)\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid heapify(int arr[], int n, int i) {\n    int largest = i;\n    int left = 2*i + 1;\n    int right = 2*i + 2;\n\n    if (left &lt; n && arr[left] &gt; arr[largest])\n        largest = left;\n    if (right &lt; n && arr[right] &gt; arr[largest])\n        largest = right;\n\n    if (largest != i) {\n        swap(&arr[i], &arr[largest]);\n        heapify(arr, n, largest);\n    }\n}\n\nvoid heap_sort(int arr[], int n) {\n    // build max-heap\n    for (int i = n / 2 - 1; i &gt;= 0; i--)\n        heapify(arr, n, i);\n\n    // extract elements\n    for (int i = n - 1; i &gt; 0; i--) {\n        swap(&arr[0], &arr[i]);\n        heapify(arr, i, 0);\n    }\n}\n\nint main(void) {\n    int arr[] = {5, 31, 10, 24, 7};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    heap_sort(arr, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", arr[i]);\n    printf(\"\\n\");\n}\nPython\ndef heapify(arr, n, i):\n    largest = i\n    l, r = 2*i + 1, 2*i + 2\n\n    if l &lt; n and arr[l] &gt; arr[largest]:\n        largest = l\n    if r &lt; n and arr[r] &gt; arr[largest]:\n        largest = r\n\n    if largest != i:\n        arr[i], arr[largest] = arr[largest], arr[i]\n        heapify(arr, n, largest)\n\ndef heap_sort(arr):\n    n = len(arr)\n    # build max-heap\n    for i in range(n//2 - 1, -1, -1):\n        heapify(arr, n, i)\n    # extract\n    for i in range(n - 1, 0, -1):\n        arr[0], arr[i] = arr[i], arr[0]\n        heapify(arr, i, 0)\n\n# Example\narr = [5, 31, 10, 24, 7]\nheap_sort(arr)\nprint(arr)\n\n\nWhy It Matters\n\nGuaranteed O(n log n) runtime (worst-case safe)\nIn-place, no extra arrays\nGreat teaching example of heap structure in action\nUsed in systems with strict space limits\n\n\n\nA Gentle Proof (Why It Works)\n\nBuilding the heap = O(n) (bottom-up heapify).\nEach extraction = O(log n), repeated n times → O(n log n).\nHeap property ensures root always holds largest element.\n\nSo final array is sorted after repeated root extractions.\n\n\nTry It Yourself\n\nSwitch comparisons for min-heap sort (descending).\nPrint array after each swap to see process.\nCompare with quicksort and mergesort.\nTest large random arrays.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[5,31,10,24,7]\n[5,7,10,24,31]\nascending\n\n\n[10,9,8,7]\n[7,8,9,10]\nreverse input\n\n\n[3]\n[3]\nsingle\n\n\n[]\n[]\nempty\n\n\n\nEdge Cases\n\nRepeated values handled fine\nAlready sorted input still O(n log n)\nStable? ❌ (order not preserved)\n\n\n\nComplexity\n\nTime: O(n log n)\nSpace: O(1)\nStable: No\n\nHeap sort is the mountain climber’s algorithm, pulling the biggest to the top, one step at a time, until order reigns from summit to base.\n\n\n\n225 Min Heap Implementation\nA min-heap is a binary tree where each parent is smaller than or equal to its children. Stored compactly in an array, it guarantees O(1) access to the smallest element and O(log n) insertion and deletion, perfect for priority queues and scheduling systems.\n\nWhat Problem Are We Solving?\nWe need a data structure that:\n\nQuickly gives us the minimum element\nSupports efficient insertions and deletions\nMaintains order dynamically\n\nA min-heap achieves this balance, always keeping the smallest element at the root while remaining compact and complete.\n\n\nHow Does It Work (Plain Language)?\nA binary min-heap is stored as an array, where:\n\nparent(i) = (i - 1) / 2\nleft(i) = 2i + 1\nright(i) = 2i + 2\n\nOperations:\n\nInsert(x):\n\nAdd x at the end.\n“Bubble up” while x &lt; parent(x).\n\nExtractMin():\n\nRemove root.\nMove last element to root.\n“Bubble down” while parent &gt; smallest child.\n\nPeek():\n\nReturn arr[0].\n\n\nExample\nStart: [ ] Insert sequence: 10, 24, 5, 31, 7\n\n\n\nStep\nAction\nArray\n\n\n\n\n\n1\ninsert 10\n[10]\n\n\n\n2\ninsert 24\n[10, 24]\n\n\n\n3\ninsert 5\nbubble up → swap with 10\n[5, 24, 10]\n\n\n4\ninsert 31\n[5, 24, 10, 31]\n\n\n\n5\ninsert 7\n[5, 7, 10, 31, 24]\n\n\n\n\nExtractMin:\n\nSwap root with last (5 ↔︎ 24) → [24, 7, 10, 31, 5]\nRemove last → [24, 7, 10, 31]\nBubble down → [7, 24, 10, 31]\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#define MAX 100\n\ntypedef struct {\n    int arr[MAX];\n    int size;\n} MinHeap;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid heapify_up(MinHeap *h, int i) {\n    while (i &gt; 0) {\n        int parent = (i - 1) / 2;\n        if (h-&gt;arr[i] &gt;= h-&gt;arr[parent]) break;\n        swap(&h-&gt;arr[i], &h-&gt;arr[parent]);\n        i = parent;\n    }\n}\n\nvoid heapify_down(MinHeap *h, int i) {\n    int smallest = i;\n    int left = 2*i + 1;\n    int right = 2*i + 2;\n\n    if (left &lt; h-&gt;size && h-&gt;arr[left] &lt; h-&gt;arr[smallest]) smallest = left;\n    if (right &lt; h-&gt;size && h-&gt;arr[right] &lt; h-&gt;arr[smallest]) smallest = right;\n\n    if (smallest != i) {\n        swap(&h-&gt;arr[i], &h-&gt;arr[smallest]);\n        heapify_down(h, smallest);\n    }\n}\n\nvoid insert(MinHeap *h, int val) {\n    h-&gt;arr[h-&gt;size] = val;\n    heapify_up(h, h-&gt;size);\n    h-&gt;size++;\n}\n\nint extract_min(MinHeap *h) {\n    if (h-&gt;size == 0) return -1;\n    int root = h-&gt;arr[0];\n    h-&gt;arr[0] = h-&gt;arr[--h-&gt;size];\n    heapify_down(h, 0);\n    return root;\n}\n\nint peek(MinHeap *h) {\n    return h-&gt;size &gt; 0 ? h-&gt;arr[0] : -1;\n}\n\nint main(void) {\n    MinHeap h = {.size = 0};\n    int vals[] = {10, 24, 5, 31, 7};\n    for (int i = 0; i &lt; 5; i++) insert(&h, vals[i]);\n    printf(\"Min: %d\\n\", peek(&h));\n    printf(\"Extracted: %d\\n\", extract_min(&h));\n    for (int i = 0; i &lt; h.size; i++) printf(\"%d \", h.arr[i]);\n    printf(\"\\n\");\n}\nPython\nclass MinHeap:\n    def __init__(self):\n        self.arr = []\n\n    def _parent(self, i): return (i - 1) // 2\n    def _left(self, i): return 2 * i + 1\n    def _right(self, i): return 2 * i + 2\n\n    def insert(self, val):\n        self.arr.append(val)\n        i = len(self.arr) - 1\n        while i &gt; 0 and self.arr[i] &lt; self.arr[self._parent(i)]:\n            p = self._parent(i)\n            self.arr[i], self.arr[p] = self.arr[p], self.arr[i]\n            i = p\n\n    def extract_min(self):\n        if not self.arr:\n            return None\n        root = self.arr[0]\n        last = self.arr.pop()\n        if self.arr:\n            self.arr[0] = last\n            self._heapify_down(0)\n        return root\n\n    def _heapify_down(self, i):\n        smallest = i\n        l, r = self._left(i), self._right(i)\n        if l &lt; len(self.arr) and self.arr[l] &lt; self.arr[smallest]:\n            smallest = l\n        if r &lt; len(self.arr) and self.arr[r] &lt; self.arr[smallest]:\n            smallest = r\n        if smallest != i:\n            self.arr[i], self.arr[smallest] = self.arr[smallest], self.arr[i]\n            self._heapify_down(smallest)\n\n    def peek(self):\n        return self.arr[0] if self.arr else None\n\n# Example\nh = MinHeap()\nfor x in [10, 24, 5, 31, 7]:\n    h.insert(x)\nprint(\"Heap:\", h.arr)\nprint(\"Min:\", h.peek())\nprint(\"Extracted:\", h.extract_min())\nprint(\"After:\", h.arr)\n\n\nWhy It Matters\n\nBackbone of priority queues, Dijkstra’s, Prim’s, A*\nAlways gives minimum element in O(1)\nCompact array-based structure (no pointers)\nExcellent for dynamic, ordered sets\n\n\n\nA Gentle Proof (Why It Works)\nEach insertion or deletion affects only a single path (height = log n). Since every swap improves local order, heap property restored in O(log n). At any time, parent ≤ children → global min at root.\n\n\nTry It Yourself\n\nImplement a max-heap variant.\nTrack the number of swaps per insert.\nCombine with heap sort by repeated extract_min.\nVisualize heap as a tree diagram.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\nHeap After\nNotes\n\n\n\n\nInsert\n[10,24,5,31,7]\n-\n[5,7,10,31,24]\nvalid\n\n\nPeek\n-\n5\n[5,7,10,31,24]\nsmallest\n\n\nExtractMin\n-\n5\n[7,24,10,31]\nreordered\n\n\nEmpty Extract\n[]\nNone\n[]\nsafe\n\n\n\nEdge Cases\n\nDuplicates → handled fine\nEmpty heap → return sentinel\nNegative numbers → no problem\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n)\nO(1)\n\n\nExtractMin\nO(log n)\nO(1)\n\n\nPeek\nO(1)\nO(1)\n\n\n\nA min-heap is the quiet organizer, always keeping the smallest task at the top, ready when you are.\n\n\n\n226 Max Heap Implementation\nA max-heap is a binary tree where each parent is greater than or equal to its children. Stored compactly in an array, it guarantees O(1) access to the largest element and O(log n) insertion and deletion, making it ideal for scheduling, leaderboards, and priority-based systems.\n\nWhat Problem Are We Solving?\nWe want a data structure that efficiently maintains a dynamic set of elements while allowing us to:\n\nAccess the largest element quickly\nInsert and delete efficiently\nMaintain order automatically\n\nA max-heap does exactly that, it always bubbles the biggest element to the top.\n\n\nHow Does It Work (Plain Language)?\nA binary max-heap is stored as an array, with these relationships:\n\nparent(i) = (i - 1) / 2\nleft(i) = 2i + 1\nright(i) = 2i + 2\n\nOperations:\n\nInsert(x):\n\nAdd x at the end.\n“Bubble up” while x &gt; parent(x).\n\nExtractMax():\n\nRemove root (maximum).\nMove last element to root.\n“Bubble down” while parent &lt; larger child.\n\nPeek():\n\nReturn arr[0] (max element).\n\n\nExample\nStart: [ ] Insert sequence: 10, 24, 5, 31, 7\n\n\n\nStep\nAction\nArray\n\n\n\n\n1\ninsert 10\n[10]\n\n\n2\ninsert 24\nbubble up → [24, 10]\n\n\n3\ninsert 5\n[24, 10, 5]\n\n\n4\ninsert 31\nbubble up → [31, 24, 5, 10]\n\n\n5\ninsert 7\n[31, 24, 5, 10, 7]\n\n\n\nExtractMax:\n\nSwap root with last (31 ↔︎ 7): [7, 24, 5, 10, 31]\nRemove last: [7, 24, 5, 10]\nBubble down: swap 7 ↔︎ 24 → [24, 10, 5, 7]\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#define MAX 100\n\ntypedef struct {\n    int arr[MAX];\n    int size;\n} MaxHeap;\n\nvoid swap(int *a, int *b) {\n    int tmp = *a; *a = *b; *b = tmp;\n}\n\nvoid heapify_up(MaxHeap *h, int i) {\n    while (i &gt; 0) {\n        int parent = (i - 1) / 2;\n        if (h-&gt;arr[i] &lt;= h-&gt;arr[parent]) break;\n        swap(&h-&gt;arr[i], &h-&gt;arr[parent]);\n        i = parent;\n    }\n}\n\nvoid heapify_down(MaxHeap *h, int i) {\n    int largest = i;\n    int left = 2 * i + 1;\n    int right = 2 * i + 2;\n\n    if (left &lt; h-&gt;size && h-&gt;arr[left] &gt; h-&gt;arr[largest]) largest = left;\n    if (right &lt; h-&gt;size && h-&gt;arr[right] &gt; h-&gt;arr[largest]) largest = right;\n\n    if (largest != i) {\n        swap(&h-&gt;arr[i], &h-&gt;arr[largest]);\n        heapify_down(h, largest);\n    }\n}\n\nvoid insert(MaxHeap *h, int val) {\n    h-&gt;arr[h-&gt;size] = val;\n    heapify_up(h, h-&gt;size);\n    h-&gt;size++;\n}\n\nint extract_max(MaxHeap *h) {\n    if (h-&gt;size == 0) return -1;\n    int root = h-&gt;arr[0];\n    h-&gt;arr[0] = h-&gt;arr[--h-&gt;size];\n    heapify_down(h, 0);\n    return root;\n}\n\nint peek(MaxHeap *h) {\n    return h-&gt;size &gt; 0 ? h-&gt;arr[0] : -1;\n}\n\nint main(void) {\n    MaxHeap h = {.size = 0};\n    int vals[] = {10, 24, 5, 31, 7};\n    for (int i = 0; i &lt; 5; i++) insert(&h, vals[i]);\n    printf(\"Max: %d\\n\", peek(&h));\n    printf(\"Extracted: %d\\n\", extract_max(&h));\n    for (int i = 0; i &lt; h.size; i++) printf(\"%d \", h.arr[i]);\n    printf(\"\\n\");\n}\nPython\nclass MaxHeap:\n    def __init__(self):\n        self.arr = []\n\n    def _parent(self, i): return (i - 1) // 2\n    def _left(self, i): return 2 * i + 1\n    def _right(self, i): return 2 * i + 2\n\n    def insert(self, val):\n        self.arr.append(val)\n        i = len(self.arr) - 1\n        while i &gt; 0 and self.arr[i] &gt; self.arr[self._parent(i)]:\n            p = self._parent(i)\n            self.arr[i], self.arr[p] = self.arr[p], self.arr[i]\n            i = p\n\n    def extract_max(self):\n        if not self.arr:\n            return None\n        root = self.arr[0]\n        last = self.arr.pop()\n        if self.arr:\n            self.arr[0] = last\n            self._heapify_down(0)\n        return root\n\n    def _heapify_down(self, i):\n        largest = i\n        l, r = self._left(i), self._right(i)\n        if l &lt; len(self.arr) and self.arr[l] &gt; self.arr[largest]:\n            largest = l\n        if r &lt; len(self.arr) and self.arr[r] &gt; self.arr[largest]:\n            largest = r\n        if largest != i:\n            self.arr[i], self.arr[largest] = self.arr[largest], self.arr[i]\n            self._heapify_down(largest)\n\n    def peek(self):\n        return self.arr[0] if self.arr else None\n\n# Example\nh = MaxHeap()\nfor x in [10, 24, 5, 31, 7]:\n    h.insert(x)\nprint(\"Heap:\", h.arr)\nprint(\"Max:\", h.peek())\nprint(\"Extracted:\", h.extract_max())\nprint(\"After:\", h.arr)\n\n\nWhy It Matters\n\nPriority queues that need fast access to maximum\nScheduling highest-priority tasks\nTracking largest elements dynamically\nUsed in heap sort, selection problems, top-k queries\n\n\n\nA Gentle Proof (Why It Works)\nEach operation adjusts at most height = log n levels. Since all swaps move greater elements upward, heap property (parent ≥ children) is restored in logarithmic time. Thus the root always holds the maximum.\n\n\nTry It Yourself\n\nConvert to min-heap by flipping comparisons.\nUse heap to implement k-largest elements finder.\nTrace swaps after each insert/delete.\nVisualize heap as a tree diagram.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nOperation\nInput\nOutput\nHeap After\nNotes\n\n\n\n\nInsert\n[10,24,5,31,7]\n-\n[31,24,5,10,7]\nmax at root\n\n\nPeek\n-\n31\n[31,24,5,10,7]\nlargest\n\n\nExtractMax\n-\n31\n[24,10,5,7]\nheap fixed\n\n\nEmpty Extract\n[]\nNone\n[]\nsafe\n\n\n\nEdge Cases\n\nDuplicates → handled fine\nEmpty heap → sentinel\nNegative numbers → valid\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n)\nO(1)\n\n\nExtractMax\nO(log n)\nO(1)\n\n\nPeek\nO(1)\nO(1)\n\n\n\nA max-heap is the summit keeper, always crowning the greatest, ensuring the hierarchy remains true from root to leaf.\n\n\n\n227 Fibonacci Heap Insert/Delete\nA Fibonacci heap is a meldable heap optimized for very fast amortized operations. It keeps a collection of heap-ordered trees with minimal structural work on most operations, banking work for occasional consolidations. Classic result: insert, find-min, and decrease-key run in O(1) amortized, while extract-min runs in O(log n) amortized.\n\nWhat Problem Are We Solving?\nWe want ultra fast priority queue ops in algorithms that call decrease-key a lot, like Dijkstra and Prim. Binary and pairing heaps give decrease-key in O(log n) or good constants, but Fibonacci heaps achieve amortized O(1) for insert and decrease-key, improving theoretical bounds.\nGoal: Maintain a set of heap-ordered trees where most updates only touch a few pointers, delaying consolidation until extract-min.\n\n\nHow Does It Work (Plain Language)?\nStructure highlights\n\nA root list of trees, each obeying min-heap order.\nA pointer to the global minimum root.\nEach node stores degree, parent, child, and a mark bit used by decrease-key.\nRoot list is a circular doubly linked list, children lists are similar.\n\nCore ideas\n\nInsert adds a 1 node tree into the root list and updates min if needed.\nDelete is typically implemented as decrease-key(x, -inf) then extract-min.\nExtract-min removes the min root, promotes its children to the root list, then consolidates roots by linking trees of equal degree until all root degrees are unique.\n\n\n\nHow Does Insert Work\nSteps for insert(x)\n\nMake a singleton node x.\nSplice x into the root list.\nUpdate min if x.key &lt; min.key.\n\nExample Steps (Root List View)\n\n\n\nStep\nAction\nRoot List\nMin\n\n\n\n\n1\ninsert 12\n[12]\n12\n\n\n2\ninsert 7\n[12, 7]\n7\n\n\n3\ninsert 25\n[12, 7, 25]\n7\n\n\n4\ninsert 3\n[12, 7, 25, 3]\n3\n\n\n\nAll inserts are O(1) pointer splices.\n\n\nHow Does Delete Work\nTo delete an arbitrary node x, standard approach\n\ndecrease-key(x, -inf) so x becomes the minimum.\nextract-min() to remove it.\n\nDelete inherits the extract-min cost.\n\n\nTiny Code (Educational Skeleton)\nThis is a minimal sketch to illustrate structure and the two requested operations. It omits full decrease-key and cascading cuts to keep focus. In practice, a complete Fibonacci heap also implements decrease-key and extract-min with consolidation arrays.\nPython (didactic skeleton for insert and delete via extract-min path)\nclass FibNode:\n    def __init__(self, key):\n        self.key = key\n        self.degree = 0\n        self.mark = False\n        self.parent = None\n        self.child = None\n        # circular doubly linked list pointers\n        self.left = self\n        self.right = self\n\ndef _splice(a, b):\n    # insert node b to the right of node a in a circular list\n    b.right = a.right\n    b.left = a\n    a.right.left = b\n    a.right = b\n\ndef _remove_from_list(x):\n    x.left.right = x.right\n    x.right.left = x.left\n    x.left = x.right = x\n\nclass FibHeap:\n    def __init__(self):\n        self.min = None\n        self.n = 0\n\n    def insert(self, key):\n        x = FibNode(key)\n        if self.min is None:\n            self.min = x\n        else:\n            _splice(self.min, x)\n            if x.key &lt; self.min.key:\n                self.min = x\n        self.n += 1\n        return x\n\n    def _merge_root_list(self, other_min):\n        if other_min is None:\n            return\n        # concat circular lists: self.min and other_min\n        a = self.min\n        b = other_min\n        a_right = a.right\n        b_left = b.left\n        a.right = b\n        b.left = a\n        a_right.left = b_left\n        b_left.right = a_right\n        if b.key &lt; self.min.key:\n            self.min = b\n\n    def extract_min(self):\n        z = self.min\n        if z is None:\n            return None\n        # promote children to root list\n        if z.child:\n            c = z.child\n            nodes = []\n            cur = c\n            while True:\n                nodes.append(cur)\n                cur = cur.right\n                if cur == c:\n                    break\n            for x in nodes:\n                x.parent = None\n                _remove_from_list(x)\n                _splice(z, x)  # into root list near z\n        # remove z from root list\n        if z.right == z:\n            self.min = None\n        else:\n            nxt = z.right\n            _remove_from_list(z)\n            self.min = nxt\n            self._consolidate()  # real impl would link equal degree trees\n        self.n -= 1\n        return z.key\n\n    def _consolidate(self):\n        # Placeholder: a full implementation uses an array A[0..floor(log_phi n)]\n        # to link roots of equal degree until all degrees unique.\n        pass\n\n    def delete(self, node):\n        # In full Fibonacci heap:\n        # decrease_key(node, -inf), then extract_min\n        # Here we approximate by manual min-bump if node is the min\n        # and ignore cascading cuts for brevity.\n        # For correctness in real use, implement decrease_key and cuts.\n        node.key = float(\"-inf\")\n        if self.min and node.key &lt; self.min.key:\n            self.min = node\n        return self.extract_min()\n\n# Example\nH = FibHeap()\nn1 = H.insert(12)\nn2 = H.insert(7)\nn3 = H.insert(25)\nn4 = H.insert(3)\nprint(\"Extracted min:\", H.extract_min())  # 3\nNote This skeleton shows how insert stitches into the root list and how extract_min would promote children. A production version must implement decrease-key with cascading cuts and _consolidate that links trees of equal degree.\n\n\nWhy It Matters\n\nTheoretical speedups for graph algorithms heavy on decrease-key\nMeld operation can be O(1) by concatenating root lists\nAmortized guarantees backed by potential function analysis\n\n\n\nA Gentle Proof (Why It Works)\nAmortized analysis uses a potential function based on number of trees in the root list and number of marked nodes.\n\ninsert only adds a root and maybe updates min, decreasing or slightly increasing potential, so O(1) amortized.\ndecrease-key cuts a node and possibly cascades parent cuts, but marks bound the total number of cascades over a sequence, giving O(1) amortized.\nextract-min triggers consolidation. The number of distinct degrees is O(log n), so linking costs O(log n) amortized.\n\n\n\nTry It Yourself\n\nComplete _consolidate with an array indexed by degree, linking roots of equal degree until unique.\nImplement decrease_key(x, new_key) with cut and cascading cut rules.\nAdd union(H1, H2) that concatenates root lists and picks the smaller min.\nBenchmark against binary and pairing heaps on workloads with many decrease-key operations.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation\nInput\nExpected\nNotes\n\n\n\n\ninsert\n12, 7, 25, 3\nmin = 3\nsimple root list updates\n\n\nextract_min\nafter above\nreturns 3\nchildren promoted, consolidate\n\n\ndelete(node)\ndelete 7\n7 removed\nvia decrease to minus infinity then extract\n\n\nmeld\nunion of two heaps\nnew min = min(m1, m2)\nO(1) concat\n\n\n\nEdge Cases\n\nExtract from empty heap returns None\nDuplicate keys are fine\nLarge inserts without extract build many small trees until consolidation\n\n\n\nComplexity\n\n\n\nOperation\nAmortized Time\n\n\n\n\ninsert\nO(1)\n\n\nfind-min\nO(1)\n\n\ndecrease-key\nO(1)\n\n\nextract-min\nO(log n)\n\n\ndelete\nO(log n) via decrease then extract\n\n\n\nFibonacci heaps trade strict order for lazy elegance. Most ops are tiny pointer shuffles, and the heavy lifting happens rarely during consolidation.\n\n\n\n228 Pairing Heap Merge\nA pairing heap is a simple, pointer-based, meldable heap that is famously fast in practice. Its secret weapon is the merge operation: link two heap roots by making the larger-key root a child of the smaller-key root. Many other operations reduce to a small number of merges.\n\nWhat Problem Are We Solving?\nWe want a priority queue with a tiny constant factor and very simple code, while keeping theoretical guarantees close to Fibonacci heaps. Pairing heaps offer extremely quick insert, meld, and often decrease-key, with delete-min powered by a lightweight multi-merge.\nGoal: Represent a heap as a tree of nodes and implement merge so that all higher-level operations can be expressed as sequences of merges.\n\n\nHow Does It Work (Plain Language)?\nEach node has: key, first child, and next sibling. The heap is just a pointer to the root. To merge two heaps A and B:\n\nIf one is empty, return the other.\nCompare roots.\nMake the larger-root heap the new child of the smaller-root heap by linking it as the smaller root’s first child.\n\nOther ops via merge\n\nInsert(x): make a 1-node heap and merge(root, x).\nFind-min: the root’s key.\nDelete-min: remove the root, then merge its children in two passes\n\nLeft to right, pairwise merge adjacent siblings.\nRight to left, merge the resulting heaps back into one.\n\nDecrease-key(x, new): cut x from its place, set x.key = new, then merge(root, x).\n\nExample Steps (Merge only)\n\n\n\nStep\nHeap A (root)\nHeap B (root)\nAction\nNew Root\n\n\n\n\n1\n7\n12\nlink 12 under 7\n7\n\n\n2\n7\n3\nlink 7 under 3\n3\n\n\n3\n3\n9\nlink 9 under 3\n3\n\n\n\nResulting root is the minimum of all merged heaps.\n\n\nTiny Code (Easy Versions)\nC (merge, insert, find-min, delete-min two-pass)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key;\n    struct Node *child;\n    struct Node *sibling;\n} Node;\n\nNode* make_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key; n-&gt;child = NULL; n-&gt;sibling = NULL;\n    return n;\n}\n\nNode* merge(Node* a, Node* b) {\n    if (!a) return b;\n    if (!b) return a;\n    if (b-&gt;key &lt; a-&gt;key) { Node* t = a; a = b; b = t; }\n    // make b the first child of a\n    b-&gt;sibling = a-&gt;child;\n    a-&gt;child = b;\n    return a;\n}\n\nNode* insert(Node* root, int key) {\n    return merge(root, make_node(key));\n}\n\nNode* merge_pairs(Node* first) {\n    if (!first || !first-&gt;sibling) return first;\n    Node* a = first;\n    Node* b = first-&gt;sibling;\n    Node* rest = b-&gt;sibling;\n    a-&gt;sibling = b-&gt;sibling = NULL;\n    return merge(merge(a, b), merge_pairs(rest));\n}\n\nNode* delete_min(Node* root, int* out) {\n    if (!root) return NULL;\n    *out = root-&gt;key;\n    Node* new_root = merge_pairs(root-&gt;child);\n    free(root);\n    return new_root;\n}\n\nint main(void) {\n    Node* h = NULL;\n    h = insert(h, 7);\n    h = insert(h, 12);\n    h = insert(h, 3);\n    h = insert(h, 9);\n    int m;\n    h = delete_min(h, &m);\n    printf(\"Deleted min: %d\\n\", m); // 3\n    return 0;\n}\nPython (succinct pairing heap)\nclass Node:\n    __slots__ = (\"key\", \"child\", \"sibling\")\n    def __init__(self, key):\n        self.key = key\n        self.child = None\n        self.sibling = None\n\ndef merge(a, b):\n    if not a: return b\n    if not b: return a\n    if b.key &lt; a.key:\n        a, b = b, a\n    b.sibling = a.child\n    a.child = b\n    return a\n\ndef merge_pairs(first):\n    if not first or not first.sibling:\n        return first\n    a, b, rest = first, first.sibling, first.sibling.sibling\n    a.sibling = b.sibling = None\n    return merge(merge(a, b), merge_pairs(rest))\n\nclass PairingHeap:\n    def __init__(self): self.root = None\n    def find_min(self): return None if not self.root else self.root.key\n    def insert(self, x):\n        self.root = merge(self.root, Node(x))\n    def meld(self, other):\n        self.root = merge(self.root, other.root)\n    def delete_min(self):\n        if not self.root: return None\n        m = self.root.key\n        self.root = merge_pairs(self.root.child)\n        return m\n\n# Example\nh = PairingHeap()\nfor x in [7, 12, 3, 9]:\n    h.insert(x)\nprint(h.find_min())      # 3\nprint(h.delete_min())    # 3\nprint(h.find_min())      # 7\n\n\nWhy It Matters\n\nIncredibly simple code yet high performance in practice\nMeld is constant time pointer work\nExcellent for workloads mixing frequent inserts and decrease-keys\nA strong practical alternative to Fibonacci heaps\n\n\n\nA Gentle Proof (Why It Works)\nMerge correctness\n\nAfter linking, the smaller root remains parent, so heap order holds at the root and along the newly attached subtree.\n\nDelete-min two-pass\n\nPairwise merges reduce the number of trees while keeping roots small.\nThe second right-to-left fold merges larger partial heaps into a single heap.\nAnalyses show delete-min runs in O(log n) amortized; insert and meld in O(1) amortized.\ndecrease-key is conjectured O(1) amortized in practice and near that in theory under common models.\n\n\n\nTry It Yourself\n\nImplement decrease_key(node, new_key): cut the node from its parent and merge(root, node) after lowering its key.\nAdd a handle table to access nodes for fast decrease-key.\nBenchmark against binary and Fibonacci heaps on Dijkstra workloads.\nVisualize delete-min’s two-pass pairing on random trees.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation\nInput\nOutput\nNotes\n\n\n\n\ninsert\n7, 12, 3, 9\nmin = 3\nroot tracks global min\n\n\nmeld\nmeld two heaps\nnew min is min of both\nconstant-time link\n\n\ndelete_min\nafter above\n3\ntwo-pass pairing\n\n\ndelete_min\nnext\n7\nheap restructures\n\n\n\nEdge Cases\n\nMerging with empty heap returns the other heap\nDuplicate keys behave naturally\nSingle-node heap deletes to empty safely\n\n\n\nComplexity\n\n\n\n\n\n\n\n\nOperation\nAmortized Time\nNotes\n\n\n\n\nmeld\nO(1)\ncore primitive\n\n\ninsert\nO(1)\nmerge singleton\n\n\nfind-min\nO(1)\nroot key\n\n\ndelete-min\nO(log n)\ntwo-pass merge\n\n\ndecrease-key\nO(1) practical, near O(1) amortized in models\ncut+merge\n\n\n\nPairing heaps make merging feel effortless: one comparison, a couple of pointers, and you are done.\n\n\n\n229 Binomial Heap Merge\nA binomial heap is a set of binomial trees that act like a binary counter for priority queues. The star move is merge: combining two heaps is like adding two binary numbers. Trees of the same degree collide, you link one under the other, and carry to the next degree.\n\nWhat Problem Are We Solving?\nWe want a priority queue that supports fast meld (union) while keeping simple, provable bounds. Binomial heaps deliver:\n\nmeld in O(log n)\ninsert in O(1) amortized by melding a 1-node heap\nfind-min in O(log n)\ndelete-min in O(log n) via a meld with reversed children\n\nGoal: Represent the heap as a sorted list of binomial trees and merge two heaps by walking these lists, linking equal-degree roots.\n\n\nHow Does It Work (Plain Language)?\nBinomial tree facts\n\nA binomial tree of degree k has 2^k nodes.\nEach degree occurs at most once in a binomial heap.\nRoots are kept in increasing order of degree.\n\nMerging two heaps H1 and H2\n\nMerge the root lists by degree like a sorted list merge.\nWalk the combined list. Whenever two consecutive trees have the same degree, link them: make the larger-key root a child of the smaller-key root, increasing the degree by 1.\nUse a carry idea just like binary addition.\n\nLink(u, v)\n\nPrecondition: degree(u) == degree(v).\nAfter linking, min(u.key, v.key) becomes parent and degree increases by 1.\n\nExample Steps (degrees in parentheses)\nStart\n\nH1 roots: [2(0), 7(1), 12(3)]\nH2 roots: [3(0), 9(2)]\n\n1 Merge lists by degree\n\nCombined: [2(0), 3(0), 7(1), 9(2), 12(3)]\n\n2 Resolve equal degrees\n\nLink 2(0) and 3(0) under min root → 2 becomes parent: 2(1)\nNow list: [2(1), 7(1), 9(2), 12(3)]\nLink 2(1) and 7(1) → 2 becomes parent: 2(2)\nNow list: [2(2), 9(2), 12(3)]\nLink 2(2) and 9(2) → 2 becomes parent: 2(3)\nNow list: [2(3), 12(3)]\nLink 2(3) and 12(3) → 2 becomes parent: 2(4)\n\nFinal heap has root list [2(4)] with 2 as global min.\n\n\nTiny Code (Easy Versions)\nC (merge plus link, minimal skeleton)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key;\n    int degree;\n    struct Node* parent;\n    struct Node* child;\n    struct Node* sibling; // next root or next sibling in child list\n} Node;\n\nNode* make_node(int key){\n    Node* x = calloc(1, sizeof(Node));\n    x-&gt;key = key;\n    return x;\n}\n\n// Make 'y' a child of 'x' assuming x-&gt;key &lt;= y-&gt;key and degree equal\nstatic Node* link_tree(Node* x, Node* y){\n    y-&gt;parent = x;\n    y-&gt;sibling = x-&gt;child;\n    x-&gt;child = y;\n    x-&gt;degree += 1;\n    return x;\n}\n\n// Merge root lists by degree (no linking yet)\nstatic Node* merge_root_lists(Node* a, Node* b){\n    if(!a) return b;\n    if(!b) return a;\n    Node dummy = {0};\n    Node* tail = &dummy;\n    while(a && b){\n        if(a-&gt;degree &lt;= b-&gt;degree){\n            tail-&gt;sibling = a; a = a-&gt;sibling;\n        }else{\n            tail-&gt;sibling = b; b = b-&gt;sibling;\n        }\n        tail = tail-&gt;sibling;\n    }\n    tail-&gt;sibling = a ? a : b;\n    return dummy.sibling;\n}\n\n// Union with carry logic\nNode* binomial_union(Node* h1, Node* h2){\n    Node* head = merge_root_lists(h1, h2);\n    if(!head) return NULL;\n\n    Node* prev = NULL;\n    Node* curr = head;\n    Node* next = curr-&gt;sibling;\n\n    while(next){\n        if(curr-&gt;degree != next-&gt;degree || (next-&gt;sibling && next-&gt;sibling-&gt;degree == curr-&gt;degree)){\n            prev = curr;\n            curr = next;\n        }else{\n            if(curr-&gt;key &lt;= next-&gt;key){\n                curr-&gt;sibling = next-&gt;sibling;\n                curr = link_tree(curr, next);\n            }else{\n                if(prev) prev-&gt;sibling = next;\n                else head = next;\n                curr = link_tree(next, curr);\n            }\n        }\n        next = curr-&gt;sibling;\n    }\n    return head;\n}\n\n// Convenience: insert by union with 1-node heap\nNode* insert(Node* heap, int key){\n    return binomial_union(heap, make_node(key));\n}\n\nint main(void){\n    Node* h1 = NULL;\n    Node* h2 = NULL;\n    h1 = insert(h1, 2);\n    h1 = insert(h1, 7);\n    h1 = insert(h1, 12); // degrees will normalize after unions\n    h2 = insert(h2, 3);\n    h2 = insert(h2, 9);\n    Node* h = binomial_union(h1, h2);\n    // h now holds the merged heap; find-min is a scan of root list\n    for(Node* r = h; r; r = r-&gt;sibling)\n        printf(\"root key=%d deg=%d\\n\", r-&gt;key, r-&gt;degree);\n    return 0;\n}\nPython (succinct union and link)\nclass Node:\n    __slots__ = (\"key\", \"degree\", \"parent\", \"child\", \"sibling\")\n    def __init__(self, key):\n        self.key = key\n        self.degree = 0\n        self.parent = None\n        self.child = None\n        self.sibling = None\n\ndef link_tree(x, y):\n    # assume x.key &lt;= y.key and degrees equal\n    y.parent = x\n    y.sibling = x.child\n    x.child = y\n    x.degree += 1\n    return x\n\ndef merge_root_lists(a, b):\n    if not a: return b\n    if not b: return a\n    dummy = Node(-1)\n    t = dummy\n    while a and b:\n        if a.degree &lt;= b.degree:\n            t.sibling, a = a, a.sibling\n        else:\n            t.sibling, b = b, b.sibling\n        t = t.sibling\n    t.sibling = a if a else b\n    return dummy.sibling\n\ndef binomial_union(h1, h2):\n    head = merge_root_lists(h1, h2)\n    if not head: return None\n    prev, curr, nxt = None, head, head.sibling\n    while nxt:\n        if curr.degree != nxt.degree or (nxt.sibling and nxt.sibling.degree == curr.degree):\n            prev, curr, nxt = curr, nxt, nxt.sibling\n        else:\n            if curr.key &lt;= nxt.key:\n                curr.sibling = nxt.sibling\n                curr = link_tree(curr, nxt)\n                nxt = curr.sibling\n            else:\n                if prev: prev.sibling = nxt\n                else: head = nxt\n                curr = link_tree(nxt, curr)\n                nxt = curr.sibling\n    return head\n\ndef insert(heap, key):\n    return binomial_union(heap, Node(key))\n\n# Example\nh1 = None\nfor x in [2, 7, 12]:\n    h1 = insert(h1, x)\nh2 = None\nfor x in [3, 9]:\n    h2 = insert(h2, x)\nh = binomial_union(h1, h2)\nroots = []\nr = h\nwhile r:\n    roots.append((r.key, r.degree))\n    r = r.sibling\nprint(roots)  # e.g. [(2, 4)] or a small set of unique degrees with min root smallest\n\n\nWhy It Matters\n\nMeld-friendly: merging heaps is first-class, not an afterthought\nClean, provable bounds with a binary-counter intuition\nFoundation for Fibonacci heaps and variations\nGreat when frequent melds are required in algorithms or multi-queue systems\n\n\n\nA Gentle Proof (Why It Works)\nMerging root lists produces degrees in nondecreasing order. Linking only happens between adjacent roots of the same degree, producing exactly one tree of each degree after all carries settle. This mirrors binary addition: each link corresponds to carrying a 1 to the next bit. Since the maximum degree is O(log n), the merge performs O(log n) links and scans, giving O(log n) time.\n\n\nTry It Yourself\n\nImplement find_min by scanning root list and keep a pointer to the min root.\nImplement delete_min: remove min root, reverse its child list into a separate heap, then union.\nAdd decrease_key by cutting and reinserting a node in the root list, then fixing parent order.\nCompare union time with binary heap and pairing heap on large random workloads.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nCase\nH1 Roots (deg)\nH2 Roots (deg)\nResult\nNotes\n\n\n\n\nSimple merge\n[2(0), 7(1)]\n[3(0)]\nroots unique after link\n2 becomes parent of 3\n\n\nChain of carries\n[2(0), 7(1), 12(3)]\n[3(0), 9(2)]\nsingle root 2(4)\ncascading links\n\n\nInsert by union\nH with roots\nplus [x(0)]\nmerged in O(1) amortized\nsingle carry possible\n\n\n\nEdge Cases\n\nMerging with empty heap returns the other heap\nDuplicate keys work; tie break arbitrarily\nMaintain stable sibling pointers when linking\n\n\n\nComplexity\n\n\n\nOperation\nTime\n\n\n\n\nmeld (union)\nO(log n)\n\n\ninsert\nO(1) amortized via union with 1-node heap\n\n\nfind-min\nO(log n) scan or keep pointer for O(1) peek\n\n\ndelete-min\nO(log n)\n\n\ndecrease-key\nO(log n) typical implementation\n\n\n\nMerging binomial heaps feels like adding binary numbers: equal degrees collide, link, and carry forward until the structure is tidy and the minimum stands at a root.\n\n\n\n230 Leftist Heap Merge\nA leftist heap is a binary tree heap optimized for efficient merges. Its structure skews to the left so that merging two heaps can be done recursively in O(log n) time. The clever trick is storing each node’s null path length (npl), ensuring the shortest path to a null child is always on the right, which keeps merges shallow.\n\nWhat Problem Are We Solving?\nWe want a merge-friendly heap with simpler structure than Fibonacci or pairing heaps but faster merges than standard binary heaps. The leftist heap is a sweet spot: fast merges, simple code, and still supports all key heap operations.\nGoal: Design a heap that keeps its shortest subtree on the right, so recursive merges stay logarithmic.\n\n\nHow Does It Work (Plain Language)?\nEach node stores:\n\nkey – value used for ordering\nleft, right – child pointers\nnpl – null path length (distance to nearest null)\n\nRules:\n\nHeap order: parent key ≤ child keys (min-heap)\nLeftist property: npl(left) ≥ npl(right)\n\nMerge(a, b):\n\nIf one is null, return the other.\nCompare roots, smaller root becomes new root.\nRecursively merge a.right and b.\nAfter merge, swap children if needed to keep leftist property.\nUpdate npl.\n\nOther operations via merge\n\nInsert(x): merge heap with single-node heap x.\nDeleteMin(): merge left and right subtrees of root.\n\nExample (Min-Heap Merge)\nHeap A: root 3\n   3\n  / \\\n 5   9\nHeap B: root 4\n  4\n / \\\n 8  10\nMerge(3, 4):\n\n3 &lt; 4 → new root 3\nMerge right(9) with heap 4\nAfter merge:\n\n     3\n    / \\\n   5   4\n      / \\\n     8  10\n        /\n       9\nRebalance by swapping if right’s npl &gt; left’s → ensures leftist shape.\n\n\nTiny Code (Easy Versions)\nC (Merge, Insert, Delete-Min)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key;\n    int npl;\n    struct Node *left, *right;\n} Node;\n\nNode* make_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;npl = 0;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nint npl(Node* x) { return x ? x-&gt;npl : -1; }\n\nNode* merge(Node* a, Node* b) {\n    if (!a) return b;\n    if (!b) return a;\n    if (b-&gt;key &lt; a-&gt;key) { Node* t = a; a = b; b = t; }\n    a-&gt;right = merge(a-&gt;right, b);\n    // maintain leftist property\n    if (npl(a-&gt;left) &lt; npl(a-&gt;right)) {\n        Node* t = a-&gt;left;\n        a-&gt;left = a-&gt;right;\n        a-&gt;right = t;\n    }\n    a-&gt;npl = npl(a-&gt;right) + 1;\n    return a;\n}\n\nNode* insert(Node* h, int key) {\n    return merge(h, make_node(key));\n}\n\nNode* delete_min(Node* h, int* out) {\n    if (!h) return NULL;\n    *out = h-&gt;key;\n    Node* new_root = merge(h-&gt;left, h-&gt;right);\n    free(h);\n    return new_root;\n}\n\nint main(void) {\n    Node* h1 = NULL;\n    int vals[] = {5, 3, 9, 7, 4};\n    for (int i = 0; i &lt; 5; i++)\n        h1 = insert(h1, vals[i]);\n\n    int m;\n    h1 = delete_min(h1, &m);\n    printf(\"Deleted min: %d\\n\", m);\n    return 0;\n}\nPython\nclass Node:\n    __slots__ = (\"key\", \"npl\", \"left\", \"right\")\n    def __init__(self, key):\n        self.key = key\n        self.npl = 0\n        self.left = None\n        self.right = None\n\ndef npl(x): return x.npl if x else -1\n\ndef merge(a, b):\n    if not a: return b\n    if not b: return a\n    if b.key &lt; a.key:\n        a, b = b, a\n    a.right = merge(a.right, b)\n    # enforce leftist property\n    if npl(a.left) &lt; npl(a.right):\n        a.left, a.right = a.right, a.left\n    a.npl = npl(a.right) + 1\n    return a\n\ndef insert(h, key):\n    return merge(h, Node(key))\n\ndef delete_min(h):\n    if not h: return None, None\n    m = h.key\n    h = merge(h.left, h.right)\n    return m, h\n\n# Example\nh = None\nfor x in [5, 3, 9, 7, 4]:\n    h = insert(h, x)\nm, h = delete_min(h)\nprint(\"Deleted min:\", m)\n\n\nWhy It Matters\n\nFast merge with simple recursion\nIdeal for priority queues with frequent unions\nCleaner implementation than Fibonacci heaps\nGuarantees logarithmic merge and delete-min\n\n\n\nA Gentle Proof (Why It Works)\nThe null path length (npl) ensures that the right spine is always the shortest. Therefore, each merge step recurses down only one right path, not both. This bounds recursion depth by O(log n). Every other operation (insert, delete-min) is defined as a small number of merges, hence O(log n).\n\n\nTry It Yourself\n\nTrace the merge process for two 3-node heaps.\nVisualize npl values after each step.\nImplement find_min() (just return root key).\nTry making a max-heap variant by flipping comparisons.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation\nInput\nOutput\nNotes\n\n\n\n\nInsert\n[5,3,9,7,4]\nroot=3\nmin-heap property holds\n\n\nDeleteMin\nremove 3\nnew root=4\nleftist property maintained\n\n\nMerge\n[3,5] + [4,6]\nroot=3\nright spine ≤ log n\n\n\nEmpty merge\nNone + [5]\n[5]\nsafe\n\n\n\nEdge Cases\n\nMerge with null heap → returns the other\nDuplicate keys → ties fine\nSingle node → npl = 0\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nMerge\nO(log n)\nO(1)\n\n\nInsert\nO(log n)\nO(1)\n\n\nDeleteMin\nO(log n)\nO(1)\n\n\nFindMin\nO(1)\nO(1)\n\n\n\nThe leftist heap is like a river always bending toward the left, shaping itself so merges flow swiftly and naturally.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-24.-balanced-trees",
    "href": "books/en-us/list-3.html#section-24.-balanced-trees",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 24. Balanced Trees",
    "text": "Section 24. Balanced Trees\n\n231 AVL Tree Insert\nAn AVL tree is a self-balancing binary search tree where the height difference (balance factor) between the left and right subtrees of any node is at most 1. This invariant guarantees O(log n) lookup, insertion, and deletion, making AVL trees a classic example of maintaining order dynamically.\n\nWhat Problem Are We Solving?\nOrdinary binary search trees can become skewed (like linked lists) after unlucky insertions, degrading performance to O(n). The AVL tree restores balance automatically after each insertion, ensuring searches and updates stay fast.\nGoal: Maintain a balanced search tree by rotating nodes after insertions so that height difference ≤ 1 everywhere.\n\n\nHow Does It Work (Plain Language)?\n\nInsert the key as in a normal BST.\nWalk back up the recursion updating heights.\nCheck balance factor = height(left) - height(right).\nIf it’s outside {−1, 0, +1}, perform one of four rotations to restore balance:\n\n\n\n\n\n\n\n\n\nCase\nPattern\nFix\n\n\n\n\nLeft-Left (LL)\nInserted into left-left subtree\nRotate right\n\n\nRight-Right (RR)\nInserted into right-right subtree\nRotate left\n\n\nLeft-Right (LR)\nInserted into left-right subtree\nRotate left at child, then right\n\n\nRight-Left (RL)\nInserted into right-left subtree\nRotate right at child, then left\n\n\n\nExample\nInsert 30, 20, 10\n\n30 → root\n20 → left of 30\n10 → left of 20 → imbalance at 30: balance factor = 2 → LL case → right rotate on 30\n\nBalanced tree:\n     20\n    /  \\\n   10   30\n\n\nStep-by-Step Example\nInsert sequence: 10, 20, 30, 40, 50\n\n\n\n\n\n\n\n\n\n\n\nStep\nInsert\nTree (in-order)\nImbalance\nRotation\nRoot After\n\n\n\n\n1\n10\n[10]\n-\n-\n10\n\n\n2\n20\n[10,20]\nbalanced\n-\n10\n\n\n3\n30\n[10,20,30]\nat 10 (RR)\nleft\n20\n\n\n4\n40\n[10,20,30,40]\nbalanced\n-\n20\n\n\n5\n50\n[10,20,30,40,50]\nat 20 (RR)\nleft\n30\n\n\n\nBalanced final tree:\n     30\n    /  \\\n   20   40\n  /       \\\n 10        50\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key, height;\n    struct Node *left, *right;\n} Node;\n\nint height(Node* n) { return n ? n-&gt;height : 0; }\nint max(int a, int b) { return a &gt; b ? a : b; }\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;height = 1;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_right(Node* y) {\n    Node* x = y-&gt;left;\n    Node* T2 = x-&gt;right;\n    x-&gt;right = y;\n    y-&gt;left = T2;\n    y-&gt;height = 1 + max(height(y-&gt;left), height(y-&gt;right));\n    x-&gt;height = 1 + max(height(x-&gt;left), height(x-&gt;right));\n    return x;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    Node* T2 = y-&gt;left;\n    y-&gt;left = x;\n    x-&gt;right = T2;\n    x-&gt;height = 1 + max(height(x-&gt;left), height(x-&gt;right));\n    y-&gt;height = 1 + max(height(y-&gt;left), height(y-&gt;right));\n    return y;\n}\n\nint balance(Node* n) { return n ? height(n-&gt;left) - height(n-&gt;right) : 0; }\n\nNode* insert(Node* node, int key) {\n    if (!node) return new_node(key);\n    if (key &lt; node-&gt;key) node-&gt;left = insert(node-&gt;left, key);\n    else if (key &gt; node-&gt;key) node-&gt;right = insert(node-&gt;right, key);\n    else return node; // no duplicates\n\n    node-&gt;height = 1 + max(height(node-&gt;left), height(node-&gt;right));\n    int bf = balance(node);\n\n    // LL\n    if (bf &gt; 1 && key &lt; node-&gt;left-&gt;key)\n        return rotate_right(node);\n    // RR\n    if (bf &lt; -1 && key &gt; node-&gt;right-&gt;key)\n        return rotate_left(node);\n    // LR\n    if (bf &gt; 1 && key &gt; node-&gt;left-&gt;key) {\n        node-&gt;left = rotate_left(node-&gt;left);\n        return rotate_right(node);\n    }\n    // RL\n    if (bf &lt; -1 && key &lt; node-&gt;right-&gt;key) {\n        node-&gt;right = rotate_right(node-&gt;right);\n        return rotate_left(node);\n    }\n    return node;\n}\n\nvoid inorder(Node* root) {\n    if (!root) return;\n    inorder(root-&gt;left);\n    printf(\"%d \", root-&gt;key);\n    inorder(root-&gt;right);\n}\n\nint main(void) {\n    Node* root = NULL;\n    int keys[] = {10, 20, 30, 40, 50};\n    for (int i = 0; i &lt; 5; i++)\n        root = insert(root, keys[i]);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = self.right = None\n        self.height = 1\n\ndef height(n): return n.height if n else 0\ndef balance(n): return height(n.left) - height(n.right) if n else 0\n\ndef rotate_right(y):\n    x, T2 = y.left, y.left.right\n    x.right, y.left = y, T2\n    y.height = 1 + max(height(y.left), height(y.right))\n    x.height = 1 + max(height(x.left), height(x.right))\n    return x\n\ndef rotate_left(x):\n    y, T2 = x.right, x.right.left\n    y.left, x.right = x, T2\n    x.height = 1 + max(height(x.left), height(x.right))\n    y.height = 1 + max(height(y.left), height(y.right))\n    return y\n\ndef insert(node, key):\n    if not node: return Node(key)\n    if key &lt; node.key: node.left = insert(node.left, key)\n    elif key &gt; node.key: node.right = insert(node.right, key)\n    else: return node\n\n    node.height = 1 + max(height(node.left), height(node.right))\n    bf = balance(node)\n\n    if bf &gt; 1 and key &lt; node.left.key:  # LL\n        return rotate_right(node)\n    if bf &lt; -1 and key &gt; node.right.key:  # RR\n        return rotate_left(node)\n    if bf &gt; 1 and key &gt; node.left.key:  # LR\n        node.left = rotate_left(node.left)\n        return rotate_right(node)\n    if bf &lt; -1 and key &lt; node.right.key:  # RL\n        node.right = rotate_right(node.right)\n        return rotate_left(node)\n    return node\n\ndef inorder(root):\n    if not root: return\n    inorder(root.left)\n    print(root.key, end=' ')\n    inorder(root.right)\n\n# Example\nroot = None\nfor k in [10, 20, 30, 40, 50]:\n    root = insert(root, k)\ninorder(root)\n\n\nWhy It Matters\n\nGuarantees O(log n) operations\nPrevents degeneration into linear chains\nClear rotation-based balancing logic\nBasis for other balanced trees (e.g., Red-Black)\n\n\n\nA Gentle Proof (Why It Works)\nEach insertion may unbalance at most one node, the lowest ancestor of the inserted node. A single rotation (or double rotation) restores the balance factor of that node to {−1, 0, +1}, and updates all affected heights in constant time. Thus each insertion performs O(1) rotations, O(log n) recursive updates.\n\n\nTry It Yourself\n\nInsert 30, 20, 10 → LL case\nInsert 10, 30, 20 → LR case\nInsert 30, 10, 20 → RL case\nInsert 10, 20, 30 → RR case\nDraw each tree before and after rotation\n\n\n\nTest Cases\n\n\n\nSequence\nRotation Type\nFinal Root\nHeight\n\n\n\n\n[30, 20, 10]\nLL\n20\n2\n\n\n[10, 30, 20]\nLR\n20\n2\n\n\n[30, 10, 20]\nRL\n20\n2\n\n\n[10, 20, 30]\nRR\n20\n2\n\n\n\nEdge Cases\n\nDuplicate keys ignored\nInsertion into empty tree → new node\nAll ascending or descending inserts → balanced via rotations\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n)\nO(h) recursion\n\n\nSearch\nO(log n)\nO(h)\n\n\nDelete\nO(log n)\nO(h)\n\n\n\nThe AVL tree is a careful gardener, pruning imbalance wherever it grows, so your searches always find a straight path.\n\n\n\n232 AVL Tree Delete\nDeleting a node in an AVL tree is like removing a block from a carefully balanced tower, you take it out, then perform rotations to restore equilibrium. The key is to combine BST deletion rules with balance factor checks and rebalancing up the path.\n\nWhat Problem Are We Solving?\nDeletion in a plain BST can break the shape, making it skewed and inefficient. In an AVL tree, we want to:\n\nRemove a node (using standard BST deletion)\nRecalculate heights and balance factors\nRestore balance with rotations\n\nThis ensures the tree remains height-balanced, keeping operations at O(log n).\n\n\nHow Does It Work (Plain Language)?\n\nFind the node as in a normal BST.\nDelete it:\n\nIf leaf → remove directly.\nIf one child → replace with child.\nIf two children → find inorder successor, copy its value, delete it recursively.\n\nWalk upward, update height and balance factor at each ancestor.\nApply one of four rotation cases if unbalanced:\n\n\n\n\n\n\n\n\n\nCase\nCondition\nFix\n\n\n\n\nLL\nbalance &gt; 1 and balance(left) ≥ 0\nRotate right\n\n\nLR\nbalance &gt; 1 and balance(left) &lt; 0\nRotate left at child, then right\n\n\nRR\nbalance &lt; -1 and balance(right) ≤ 0\nRotate left\n\n\nRL\nbalance &lt; -1 and balance(right) &gt; 0\nRotate right at child, then left\n\n\n\n\n\nExample\nDelete 10 from\n     20\n    /  \\\n   10   30\n\nRemove leaf 10\nNode 20: balance = 0 → balanced\n\nNow delete 30:\n    20\n   /\n 10\n\nbalance(20) = +1 → still balanced\n\nDelete 10 next:\n20\nTree becomes single node, still AVL.\n\n\nStep-by-Step Example\nInsert [10, 20, 30, 40, 50, 25] Then delete 40\n\n\n\nStep\nAction\nImbalance\nRotation\nRoot\n\n\n\n\n1\nDelete 40\nat 30 (balance = -2)\nRL\n30\n\n\n2\nAfter rotate\nbalanced\n-\n30\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key, height;\n    struct Node *left, *right;\n} Node;\n\nint height(Node* n) { return n ? n-&gt;height : 0; }\nint max(int a, int b) { return a &gt; b ? a : b; }\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key; n-&gt;height = 1;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_right(Node* y) {\n    Node* x = y-&gt;left;\n    Node* T2 = x-&gt;right;\n    x-&gt;right = y;\n    y-&gt;left = T2;\n    y-&gt;height = 1 + max(height(y-&gt;left), height(y-&gt;right));\n    x-&gt;height = 1 + max(height(x-&gt;left), height(x-&gt;right));\n    return x;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    Node* T2 = y-&gt;left;\n    y-&gt;left = x;\n    x-&gt;right = T2;\n    x-&gt;height = 1 + max(height(x-&gt;left), height(x-&gt;right));\n    y-&gt;height = 1 + max(height(y-&gt;left), height(y-&gt;right));\n    return y;\n}\n\nint balance(Node* n) { return n ? height(n-&gt;left) - height(n-&gt;right) : 0; }\n\nNode* min_node(Node* n) {\n    Node* cur = n;\n    while (cur-&gt;left) cur = cur-&gt;left;\n    return cur;\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else if (key &gt; root-&gt;key) root-&gt;right = insert(root-&gt;right, key);\n    else return root;\n\n    root-&gt;height = 1 + max(height(root-&gt;left), height(root-&gt;right));\n    int bf = balance(root);\n\n    // Rebalance\n    if (bf &gt; 1 && key &lt; root-&gt;left-&gt;key) return rotate_right(root);\n    if (bf &lt; -1 && key &gt; root-&gt;right-&gt;key) return rotate_left(root);\n    if (bf &gt; 1 && key &gt; root-&gt;left-&gt;key) {\n        root-&gt;left = rotate_left(root-&gt;left);\n        return rotate_right(root);\n    }\n    if (bf &lt; -1 && key &lt; root-&gt;right-&gt;key) {\n        root-&gt;right = rotate_right(root-&gt;right);\n        return rotate_left(root);\n    }\n    return root;\n}\n\nNode* delete(Node* root, int key) {\n    if (!root) return root;\n\n    if (key &lt; root-&gt;key) root-&gt;left = delete(root-&gt;left, key);\n    else if (key &gt; root-&gt;key) root-&gt;right = delete(root-&gt;right, key);\n    else {\n        // Node with one or no child\n        if (!root-&gt;left || !root-&gt;right) {\n            Node* tmp = root-&gt;left ? root-&gt;left : root-&gt;right;\n            if (!tmp) { tmp = root; root = NULL; }\n            else *root = *tmp;\n            free(tmp);\n        } else {\n            Node* tmp = min_node(root-&gt;right);\n            root-&gt;key = tmp-&gt;key;\n            root-&gt;right = delete(root-&gt;right, tmp-&gt;key);\n        }\n    }\n    if (!root) return root;\n\n    root-&gt;height = 1 + max(height(root-&gt;left), height(root-&gt;right));\n    int bf = balance(root);\n\n    // Rebalance\n    if (bf &gt; 1 && balance(root-&gt;left) &gt;= 0)\n        return rotate_right(root);\n    if (bf &gt; 1 && balance(root-&gt;left) &lt; 0) {\n        root-&gt;left = rotate_left(root-&gt;left);\n        return rotate_right(root);\n    }\n    if (bf &lt; -1 && balance(root-&gt;right) &lt;= 0)\n        return rotate_left(root);\n    if (bf &lt; -1 && balance(root-&gt;right) &gt; 0) {\n        root-&gt;right = rotate_right(root-&gt;right);\n        return rotate_left(root);\n    }\n    return root;\n}\n\nvoid inorder(Node* r) {\n    if (!r) return;\n    inorder(r-&gt;left);\n    printf(\"%d \", r-&gt;key);\n    inorder(r-&gt;right);\n}\n\nint main(void) {\n    Node* root = NULL;\n    int keys[] = {10, 20, 30, 40, 50, 25};\n    for (int i = 0; i &lt; 6; i++) root = insert(root, keys[i]);\n    root = delete(root, 40);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.height = 1\n        self.left = self.right = None\n\ndef height(n): return n.height if n else 0\ndef balance(n): return height(n.left) - height(n.right) if n else 0\n\ndef rotate_right(y):\n    x, T2 = y.left, y.left.right\n    x.right, y.left = y, T2\n    y.height = 1 + max(height(y.left), height(y.right))\n    x.height = 1 + max(height(x.left), height(x.right))\n    return x\n\ndef rotate_left(x):\n    y, T2 = x.right, x.right.left\n    y.left, x.right = x, T2\n    x.height = 1 + max(height(x.left), height(x.right))\n    y.height = 1 + max(height(y.left), height(y.right))\n    return y\n\ndef min_node(n):\n    while n.left: n = n.left\n    return n\n\ndef insert(r, k):\n    if not r: return Node(k)\n    if k &lt; r.key: r.left = insert(r.left, k)\n    elif k &gt; r.key: r.right = insert(r.right, k)\n    else: return r\n    r.height = 1 + max(height(r.left), height(r.right))\n    bf = balance(r)\n    if bf &gt; 1 and k &lt; r.left.key: return rotate_right(r)\n    if bf &lt; -1 and k &gt; r.right.key: return rotate_left(r)\n    if bf &gt; 1 and k &gt; r.left.key:\n        r.left = rotate_left(r.left); return rotate_right(r)\n    if bf &lt; -1 and k &lt; r.right.key:\n        r.right = rotate_right(r.right); return rotate_left(r)\n    return r\n\ndef delete(r, k):\n    if not r: return r\n    if k &lt; r.key: r.left = delete(r.left, k)\n    elif k &gt; r.key: r.right = delete(r.right, k)\n    else:\n        if not r.left: return r.right\n        elif not r.right: return r.left\n        temp = min_node(r.right)\n        r.key = temp.key\n        r.right = delete(r.right, temp.key)\n    r.height = 1 + max(height(r.left), height(r.right))\n    bf = balance(r)\n    if bf &gt; 1 and balance(r.left) &gt;= 0: return rotate_right(r)\n    if bf &gt; 1 and balance(r.left) &lt; 0:\n        r.left = rotate_left(r.left); return rotate_right(r)\n    if bf &lt; -1 and balance(r.right) &lt;= 0: return rotate_left(r)\n    if bf &lt; -1 and balance(r.right) &gt; 0:\n        r.right = rotate_right(r.right); return rotate_left(r)\n    return r\n\ndef inorder(r):\n    if not r: return\n    inorder(r.left); print(r.key, end=' '); inorder(r.right)\n\nroot = None\nfor k in [10,20,30,40,50,25]:\n    root = insert(root, k)\nroot = delete(root, 40)\ninorder(root)\n\n\nWhy It Matters\n\nKeeps search, insert, delete all O(log n)\nAuto-rebalances after removal\nShows how rotations maintain structure consistency\nFoundation for all balanced trees (like Red-Black, AVL variants)\n\n\n\nA Gentle Proof (Why It Works)\nEvery deletion changes subtree height by at most 1. Each ancestor’s balance factor is recomputed; if imbalance found, a single rotation (or double rotation) restores balance. At most O(log n) nodes are visited, and each fix is O(1).\n\n\nTry It Yourself\n\nBuild [10, 20, 30, 40, 50, 25], then delete 50\nObserve RR rotation at 30\nDelete 10, check rebalancing at 20\nDelete all sequentially, confirm sorted order\n\n\n\nTest Cases\n\n\n\nInsert Sequence\nDelete\nRotation\nRoot After\nBalanced\n\n\n\n\n[10,20,30,40,50,25]\n40\nRL\n30\n✅\n\n\n[10,20,30]\n10\nRR\n20\n✅\n\n\n[30,20,10]\n30\nLL\n20\n✅\n\n\n\nEdge Cases\n\nDeleting from empty tree → safe\nSingle node → becomes NULL\nDuplicate keys → ignored\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDelete\nO(log n)\nO(h) recursion\n\n\nSearch\nO(log n)\nO(h)\n\n\nInsert\nO(log n)\nO(h)\n\n\n\nAn AVL deletion is a gentle art, remove the key, rebalance the branches, and harmony is restored.\n\n\n\n233 Red-Black Tree Insert\nA Red-Black Tree (RBT) is a self-balancing binary search tree that uses color bits (red or black) to control balance indirectly. Unlike AVL trees that balance by height, RBTs balance by color rules, allowing more flexible, faster insertions with fewer rotations.\n\nWhat Problem Are We Solving?\nA plain BST can degrade into a linked list with O(n) operations. An RBT maintains a near-balanced height, ensuring O(log n) for search, insert, and delete. Instead of exact height balance like AVL, RBTs enforce color invariants that keep paths roughly equal.\n\n\nRed-Black Tree Properties\n\nEach node is red or black.\nThe root is always black.\nNull (NIL) nodes are considered black.\nNo two consecutive red nodes (no red parent with red child).\nEvery path from a node to its descendant NIL nodes has the same number of black nodes.\n\nThese rules guarantee height ≤ 2 × log₂(n + 1).\n\n\nHow Does It Work (Plain Language)?\n\nInsert node like in a normal BST (color it red).\nFix violations if any property breaks.\nUse rotations and recoloring based on where the red node appears.\n\n\n\n\n\n\n\n\n\nCase\nCondition\nFix\n\n\n\n\nCase 1\nNew node is root\nRecolor black\n\n\nCase 2\nParent black\nNo fix needed\n\n\nCase 3\nParent red, Uncle red\nRecolor parent & uncle black, grandparent red\n\n\nCase 4\nParent red, Uncle black, Triangle (LR/RL)\nRotate to line up\n\n\nCase 5\nParent red, Uncle black, Line (LL/RR)\nRotate and recolor grandparent\n\n\n\n\n\nExample\nInsert sequence: 10, 20, 30\n\nInsert 10 → root → black\nInsert 20 → red child → balanced\nInsert 30 → red parent (20) → Case 5 (RR) → rotate left on 10 → recolor root black, children red\n\nResult:\n    20(B)\n   /    \\\n10(R)   30(R)\n\n\nStep-by-Step Example\nInsert [7, 3, 18, 10, 22, 8, 11, 26]\n\n\n\n\n\n\n\n\n\n\nStep\nInsert\nViolation\nFix\nRoot\n\n\n\n\n1\n7\nroot\nmake black\n7(B)\n\n\n2\n3\nparent black\nnone\n7(B)\n\n\n3\n18\nparent black\nnone\n7(B)\n\n\n4\n10\nparent red, uncle red\nrecolor, move up\n7(B)\n\n\n5\n22\nparent black\nnone\n7(B)\n\n\n6\n8\nparent red, uncle red\nrecolor\n7(B)\n\n\n7\n11\nparent red, uncle black, LR\nrotate + recolor\n7(B)\n\n\n8\n26\nparent black\nnone\n7(B)\n\n\n\nFinal tree balanced by color invariants.\n\n\nTiny Code (Easy Versions)\nC (Simplified)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef enum { RED, BLACK } Color;\n\ntypedef struct Node {\n    int key;\n    Color color;\n    struct Node *left, *right, *parent;\n} Node;\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;color = RED;\n    n-&gt;left = n-&gt;right = n-&gt;parent = NULL;\n    return n;\n}\n\nvoid rotate_left(Node root, Node* x) {\n    Node* y = x-&gt;right;\n    x-&gt;right = y-&gt;left;\n    if (y-&gt;left) y-&gt;left-&gt;parent = x;\n    y-&gt;parent = x-&gt;parent;\n    if (!x-&gt;parent) *root = y;\n    else if (x == x-&gt;parent-&gt;left) x-&gt;parent-&gt;left = y;\n    else x-&gt;parent-&gt;right = y;\n    y-&gt;left = x;\n    x-&gt;parent = y;\n}\n\nvoid rotate_right(Node root, Node* y) {\n    Node* x = y-&gt;left;\n    y-&gt;left = x-&gt;right;\n    if (x-&gt;right) x-&gt;right-&gt;parent = y;\n    x-&gt;parent = y-&gt;parent;\n    if (!y-&gt;parent) *root = x;\n    else if (y == y-&gt;parent-&gt;left) y-&gt;parent-&gt;left = x;\n    else y-&gt;parent-&gt;right = x;\n    x-&gt;right = y;\n    y-&gt;parent = x;\n}\n\nvoid fix_violation(Node root, Node* z) {\n    while (z-&gt;parent && z-&gt;parent-&gt;color == RED) {\n        Node* gp = z-&gt;parent-&gt;parent;\n        if (z-&gt;parent == gp-&gt;left) {\n            Node* uncle = gp-&gt;right;\n            if (uncle && uncle-&gt;color == RED) {\n                z-&gt;parent-&gt;color = BLACK;\n                uncle-&gt;color = BLACK;\n                gp-&gt;color = RED;\n                z = gp;\n            } else {\n                if (z == z-&gt;parent-&gt;right) {\n                    z = z-&gt;parent;\n                    rotate_left(root, z);\n                }\n                z-&gt;parent-&gt;color = BLACK;\n                gp-&gt;color = RED;\n                rotate_right(root, gp);\n            }\n        } else {\n            Node* uncle = gp-&gt;left;\n            if (uncle && uncle-&gt;color == RED) {\n                z-&gt;parent-&gt;color = BLACK;\n                uncle-&gt;color = BLACK;\n                gp-&gt;color = RED;\n                z = gp;\n            } else {\n                if (z == z-&gt;parent-&gt;left) {\n                    z = z-&gt;parent;\n                    rotate_right(root, z);\n                }\n                z-&gt;parent-&gt;color = BLACK;\n                gp-&gt;color = RED;\n                rotate_left(root, gp);\n            }\n        }\n    }\n    (*root)-&gt;color = BLACK;\n}\n\nNode* bst_insert(Node* root, Node* z) {\n    if (!root) return z;\n    if (z-&gt;key &lt; root-&gt;key) {\n        root-&gt;left = bst_insert(root-&gt;left, z);\n        root-&gt;left-&gt;parent = root;\n    } else if (z-&gt;key &gt; root-&gt;key) {\n        root-&gt;right = bst_insert(root-&gt;right, z);\n        root-&gt;right-&gt;parent = root;\n    }\n    return root;\n}\n\nvoid insert(Node root, int key) {\n    Node* z = new_node(key);\n    *root = bst_insert(*root, z);\n    fix_violation(root, z);\n}\n\nvoid inorder(Node* r) {\n    if (!r) return;\n    inorder(r-&gt;left);\n    printf(\"%d(%c) \", r-&gt;key, r-&gt;color == RED ? 'R' : 'B');\n    inorder(r-&gt;right);\n}\n\nint main(void) {\n    Node* root = NULL;\n    int keys[] = {10, 20, 30};\n    for (int i = 0; i &lt; 3; i++) insert(&root, keys[i]);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython (Simplified)\nclass Node:\n    def __init__(self, key, color=\"R\", parent=None):\n        self.key = key\n        self.color = color\n        self.left = self.right = None\n        self.parent = parent\n\ndef rotate_left(root, x):\n    y = x.right\n    x.right = y.left\n    if y.left: y.left.parent = x\n    y.parent = x.parent\n    if not x.parent: root = y\n    elif x == x.parent.left: x.parent.left = y\n    else: x.parent.right = y\n    y.left = x\n    x.parent = y\n    return root\n\ndef rotate_right(root, y):\n    x = y.left\n    y.left = x.right\n    if x.right: x.right.parent = y\n    x.parent = y.parent\n    if not y.parent: root = x\n    elif y == y.parent.left: y.parent.left = x\n    else: y.parent.right = x\n    x.right = y\n    y.parent = x\n    return root\n\ndef fix_violation(root, z):\n    while z.parent and z.parent.color == \"R\":\n        gp = z.parent.parent\n        if z.parent == gp.left:\n            uncle = gp.right\n            if uncle and uncle.color == \"R\":\n                z.parent.color = uncle.color = \"B\"\n                gp.color = \"R\"\n                z = gp\n            else:\n                if z == z.parent.right:\n                    z = z.parent\n                    root = rotate_left(root, z)\n                z.parent.color = \"B\"\n                gp.color = \"R\"\n                root = rotate_right(root, gp)\n        else:\n            uncle = gp.left\n            if uncle and uncle.color == \"R\":\n                z.parent.color = uncle.color = \"B\"\n                gp.color = \"R\"\n                z = gp\n            else:\n                if z == z.parent.left:\n                    z = z.parent\n                    root = rotate_right(root, z)\n                z.parent.color = \"B\"\n                gp.color = \"R\"\n                root = rotate_left(root, gp)\n    root.color = \"B\"\n    return root\n\ndef bst_insert(root, z):\n    if not root: return z\n    if z.key &lt; root.key:\n        root.left = bst_insert(root.left, z)\n        root.left.parent = root\n    elif z.key &gt; root.key:\n        root.right = bst_insert(root.right, z)\n        root.right.parent = root\n    return root\n\ndef insert(root, key):\n    z = Node(key)\n    root = bst_insert(root, z)\n    return fix_violation(root, z)\n\ndef inorder(r):\n    if not r: return\n    inorder(r.left)\n    print(f\"{r.key}({r.color})\", end=\" \")\n    inorder(r.right)\n\nroot = None\nfor k in [10,20,30]:\n    root = insert(root, k)\ninorder(root)\n\n\nWhy It Matters\n\nFewer rotations than AVL\nGuarantees O(log n) for all operations\nUsed in real systems: Linux kernel, Java TreeMap, C++ map\nEasy insertion logic using coloring + rotation\n\n\n\nA Gentle Proof (Why It Works)\nThe black-height invariant ensures every path length is between h and 2h. Balancing via recolor + single rotation ensures logarithmic height. Each insert requires at most 2 rotations, O(log n) traversal.\n\n\nTry It Yourself\n\nInsert [10, 20, 30] → RR rotation\nInsert [30, 15, 10] → LL rotation\nInsert [10, 15, 5] → recolor, no rotation\nDraw colors, confirm invariants\n\n\n\nTest Cases\n\n\n\nSequence\nRotations\nRoot\nBlack Height\n\n\n\n\n[10,20,30]\nLeft\n20(B)\n2\n\n\n[7,3,18,10,22,8,11,26]\nMixed\n7(B)\n3\n\n\n[1,2,3,4,5]\nMultiple recolor+rot\n2(B)\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n)\nO(1) rotations\n\n\nSearch\nO(log n)\nO(h)\n\n\nDelete\nO(log n)\nO(1) rotations\n\n\n\nThe Red-Black Tree paints order into chaos, each red spark balanced by a calm black shadow.\n\n\n\n234 Red-Black Tree Delete\nDeletion in a Red-Black Tree (RBT) is a delicate operation: remove a node, then restore balance by adjusting colors and rotations to maintain all five RBT invariants. While insertion fixes “too much red,” deletion often fixes “too much black.”\n\nWhat Problem Are We Solving?\nAfter deleting a node from an RBT, the black-height property may break (some paths lose a black node). Our goal: restore all red-black invariants while keeping time complexity O(log n).\n\n\nRed-Black Properties (Reminder)\n\nRoot is black.\nEvery node is red or black.\nAll NIL leaves are black.\nRed nodes cannot have red children.\nEvery path from a node to NIL descendants has the same number of black nodes.\n\n\n\nHow Does It Work (Plain Language)?\n\nPerform standard BST deletion.\nTrack if the removed node was black (this might cause “double black” issue).\nFix violations moving upward until the tree is balanced again.\n\n\n\n\n\n\n\n\n\nCase\nCondition\nFix\n\n\n\n\n1\nNode is red\nSimply remove (no rebalance)\n\n\n2\nNode is black, child red\nReplace and recolor child black\n\n\n3\nNode is black, child black (“double black”)\nUse sibling cases below\n\n\n\nDouble Black Fix Cases\n\n\n\n\n\n\n\n\nCase\nDescription\nAction\n\n\n\n\n1\nSibling red\nRotate and recolor to make sibling black\n\n\n2\nSibling black, both children black\nRecolor sibling red, move double black up\n\n\n3\nSibling black, near child red, far child black\nRotate sibling toward node, swap colors\n\n\n4\nSibling black, far child red\nRotate parent, recolor sibling and parent, set far child black\n\n\n\n\n\nExample\nDelete 30 from:\n     20(B)\n    /    \\\n 10(R)   30(R)\n\n30 is red → remove directly\nNo property violated\n\nDelete 10:\n     20(B)\n    /\n   NIL\n10 was red → simple remove → tree valid\nDelete 20: Tree becomes empty → fine\n\n\nStep-by-Step Example\nInsert [10, 20, 30, 15, 25] Delete 20\n\n\n\n\n\n\n\n\n\n\nStep\nAction\nViolation\nFix\n\n\n\n\n\n1\nDelete 20\ndouble black at 25\nsibling 10 black, far child red → Case 4\nrotate\n\n\n2\nAfter rotate\nall properties restored\n-\n\n\n\n\n\n\nTiny Code (Simplified)\nC (Conceptual)\n// This snippet omits full BST insertion for brevity.\n\ntypedef enum { RED, BLACK } Color;\ntypedef struct Node {\n    int key;\n    Color color;\n    struct Node *left, *right, *parent;\n} Node;\n\n// Utility: get sibling\nNode* sibling(Node* n) {\n    if (!n-&gt;parent) return NULL;\n    return n == n-&gt;parent-&gt;left ? n-&gt;parent-&gt;right : n-&gt;parent-&gt;left;\n}\n\nvoid fix_delete(Node root, Node* x) {\n    while (x != *root && (!x || x-&gt;color == BLACK)) {\n        Node* s = sibling(x);\n        if (x == x-&gt;parent-&gt;left) {\n            if (s-&gt;color == RED) {\n                s-&gt;color = BLACK;\n                x-&gt;parent-&gt;color = RED;\n                rotate_left(root, x-&gt;parent);\n                s = x-&gt;parent-&gt;right;\n            }\n            if ((!s-&gt;left || s-&gt;left-&gt;color == BLACK) &&\n                (!s-&gt;right || s-&gt;right-&gt;color == BLACK)) {\n                s-&gt;color = RED;\n                x = x-&gt;parent;\n            } else {\n                if (!s-&gt;right || s-&gt;right-&gt;color == BLACK) {\n                    if (s-&gt;left) s-&gt;left-&gt;color = BLACK;\n                    s-&gt;color = RED;\n                    rotate_right(root, s);\n                    s = x-&gt;parent-&gt;right;\n                }\n                s-&gt;color = x-&gt;parent-&gt;color;\n                x-&gt;parent-&gt;color = BLACK;\n                if (s-&gt;right) s-&gt;right-&gt;color = BLACK;\n                rotate_left(root, x-&gt;parent);\n                x = *root;\n            }\n        } else {\n            // mirror logic for right child\n            if (s-&gt;color == RED) {\n                s-&gt;color = BLACK;\n                x-&gt;parent-&gt;color = RED;\n                rotate_right(root, x-&gt;parent);\n                s = x-&gt;parent-&gt;left;\n            }\n            if ((!s-&gt;left || s-&gt;left-&gt;color == BLACK) &&\n                (!s-&gt;right || s-&gt;right-&gt;color == BLACK)) {\n                s-&gt;color = RED;\n                x = x-&gt;parent;\n            } else {\n                if (!s-&gt;left || s-&gt;left-&gt;color == BLACK) {\n                    if (s-&gt;right) s-&gt;right-&gt;color = BLACK;\n                    s-&gt;color = RED;\n                    rotate_left(root, s);\n                    s = x-&gt;parent-&gt;left;\n                }\n                s-&gt;color = x-&gt;parent-&gt;color;\n                x-&gt;parent-&gt;color = BLACK;\n                if (s-&gt;left) s-&gt;left-&gt;color = BLACK;\n                rotate_right(root, x-&gt;parent);\n                x = *root;\n            }\n        }\n    }\n    if (x) x-&gt;color = BLACK;\n}\nPython (Simplified Pseudocode)\ndef fix_delete(root, x):\n    while x != root and (not x or x.color == \"B\"):\n        if x == x.parent.left:\n            s = x.parent.right\n            if s.color == \"R\":\n                s.color, x.parent.color = \"B\", \"R\"\n                root = rotate_left(root, x.parent)\n                s = x.parent.right\n            if all(c is None or c.color == \"B\" for c in [s.left, s.right]):\n                s.color = \"R\"\n                x = x.parent\n            else:\n                if not s.right or s.right.color == \"B\":\n                    if s.left: s.left.color = \"B\"\n                    s.color = \"R\"\n                    root = rotate_right(root, s)\n                    s = x.parent.right\n                s.color, x.parent.color = x.parent.color, \"B\"\n                if s.right: s.right.color = \"B\"\n                root = rotate_left(root, x.parent)\n                x = root\n        else:\n            # mirror logic\n            ...\n    if x: x.color = \"B\"\n    return root\n\n\nWhy It Matters\n\nPreserves logarithmic height after deletion\nUsed in core data structures (std::map, TreeSet)\nDemonstrates color logic as a soft balancing scheme\nHandles edge cases gracefully via double black fix\n\n\n\nA Gentle Proof (Why It Works)\nWhen a black node is removed, one path loses a black count. The fix cases re-distribute black heights using rotations and recolors. Each loop iteration moves double-black upward, O(log n) steps.\n\n\nTry It Yourself\n\nBuild [10, 20, 30, 15, 25, 5]. Delete 10 (leaf).\nDelete 30 (red leaf) → no fix.\nDelete 20 (black node with one child) → recolor fix.\nVisualize each case: sibling red, sibling black with red child.\n\n\n\nTest Cases\n\n\n\nInsert Sequence\nDelete\nCase Triggered\nFix\n\n\n\n\n[10,20,30]\n20\nCase 4\nRotate & recolor\n\n\n[7,3,18,10,22,8,11,26]\n18\nCase 2\nRecolor sibling\n\n\n[10,5,1]\n5\nCase 1\nRotate parent\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDelete\nO(log n)\nO(1) rotations\n\n\nInsert\nO(log n)\nO(1)\n\n\nSearch\nO(log n)\nO(h)\n\n\n\nA Red-Black delete is like a careful tune-up, one color at a time, harmony restored across every path.\n\n\n\n235 Splay Tree Access\nA Splay Tree is a self-adjusting binary search tree that brings frequently accessed elements closer to the root through splaying, a series of tree rotations. Unlike AVL or Red-Black trees, it doesn’t maintain strict balance but guarantees amortized O(log n) performance for access, insert, and delete.\n\nWhat Problem Are We Solving?\nIn many workloads, some elements are accessed far more frequently than others. Ordinary BSTs give no advantage for “hot” keys, while balanced trees maintain shape but not recency. A Splay Tree optimizes for temporal locality: recently accessed items move near the root, making repeated access faster.\n\n\nHow Does It Work (Plain Language)?\nWhenever you access a node (via search, insert, or delete), perform a splay operation, repeatedly rotate the node toward the root according to its position and parent relationships.\nThe three rotation patterns:\n\n\n\n\n\n\n\n\nCase\nStructure\nOperation\n\n\n\n\nZig\nNode is child of root\nSingle rotation\n\n\nZig-Zig\nNode and parent are both left or both right children\nDouble rotation (rotate parent, then grandparent)\n\n\nZig-Zag\nNode and parent are opposite children\nDouble rotation (rotate node twice upward)\n\n\n\nAfter splaying, the accessed node becomes the root.\n\n\nExample\nAccess sequence: 10, 20, 30\n\nInsert 10 (root)\nInsert 20 → 20 right of 10\nAccess 20 → Zig rotation → 20 becomes root\nInsert 30 → 30 right of 20\nAccess 30 → Zig rotation → 30 root\n\nResulting tree (after all accesses):\n   30\n  /\n20\n/\n10\nFrequently used nodes rise to the top automatically.\n\n\nStep-by-Step Example\nStart with keys [5, 3, 8, 1, 4, 7, 9]. Access key 1.\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nCase\nRotation\nNew Root\n\n\n\n\n1\nAccess 1\nZig-Zig (left-left)\nRotate 3, then 5\n1\n\n\n2\nAfter splay\n-\n-\n1\n\n\n\nFinal tree: 1 becomes root, path shortened for next access.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key;\n    struct Node *left, *right;\n} Node;\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_right(Node* x) {\n    Node* y = x-&gt;left;\n    x-&gt;left = y-&gt;right;\n    y-&gt;right = x;\n    return y;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    x-&gt;right = y-&gt;left;\n    y-&gt;left = x;\n    return y;\n}\n\nNode* splay(Node* root, int key) {\n    if (!root || root-&gt;key == key) return root;\n\n    // Key in left subtree\n    if (key &lt; root-&gt;key) {\n        if (!root-&gt;left) return root;\n        if (key &lt; root-&gt;left-&gt;key) {\n            root-&gt;left-&gt;left = splay(root-&gt;left-&gt;left, key);\n            root = rotate_right(root);\n        } else if (key &gt; root-&gt;left-&gt;key) {\n            root-&gt;left-&gt;right = splay(root-&gt;left-&gt;right, key);\n            if (root-&gt;left-&gt;right)\n                root-&gt;left = rotate_left(root-&gt;left);\n        }\n        return root-&gt;left ? rotate_right(root) : root;\n    }\n    // Key in right subtree\n    else {\n        if (!root-&gt;right) return root;\n        if (key &gt; root-&gt;right-&gt;key) {\n            root-&gt;right-&gt;right = splay(root-&gt;right-&gt;right, key);\n            root = rotate_left(root);\n        } else if (key &lt; root-&gt;right-&gt;key) {\n            root-&gt;right-&gt;left = splay(root-&gt;right-&gt;left, key);\n            if (root-&gt;right-&gt;left)\n                root-&gt;right = rotate_right(root-&gt;right);\n        }\n        return root-&gt;right ? rotate_left(root) : root;\n    }\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n    root = splay(root, key);\n    if (root-&gt;key == key) return root;\n    Node* n = new_node(key);\n    if (key &lt; root-&gt;key) {\n        n-&gt;right = root;\n        n-&gt;left = root-&gt;left;\n        root-&gt;left = NULL;\n    } else {\n        n-&gt;left = root;\n        n-&gt;right = root-&gt;right;\n        root-&gt;right = NULL;\n    }\n    return n;\n}\n\nvoid inorder(Node* r) {\n    if (!r) return;\n    inorder(r-&gt;left);\n    printf(\"%d \", r-&gt;key);\n    inorder(r-&gt;right);\n}\n\nint main(void) {\n    Node* root = NULL;\n    int keys[] = {5, 3, 8, 1, 4, 7, 9};\n    for (int i = 0; i &lt; 7; i++) root = insert(root, keys[i]);\n    root = splay(root, 1);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.left = self.right = None\n\ndef rotate_right(x):\n    y = x.left\n    x.left = y.right\n    y.right = x\n    return y\n\ndef rotate_left(x):\n    y = x.right\n    x.right = y.left\n    y.left = x\n    return y\n\ndef splay(root, key):\n    if not root or root.key == key:\n        return root\n    if key &lt; root.key:\n        if not root.left:\n            return root\n        if key &lt; root.left.key:\n            root.left.left = splay(root.left.left, key)\n            root = rotate_right(root)\n        elif key &gt; root.left.key:\n            root.left.right = splay(root.left.right, key)\n            if root.left.right:\n                root.left = rotate_left(root.left)\n        return rotate_right(root) if root.left else root\n    else:\n        if not root.right:\n            return root\n        if key &gt; root.right.key:\n            root.right.right = splay(root.right.right, key)\n            root = rotate_left(root)\n        elif key &lt; root.right.key:\n            root.right.left = splay(root.right.left, key)\n            if root.right.left:\n                root.right = rotate_right(root.right)\n        return rotate_left(root) if root.right else root\n\ndef insert(root, key):\n    if not root:\n        return Node(key)\n    root = splay(root, key)\n    if root.key == key:\n        return root\n    n = Node(key)\n    if key &lt; root.key:\n        n.right = root\n        n.left = root.left\n        root.left = None\n    else:\n        n.left = root\n        n.right = root.right\n        root.right = None\n    return n\n\ndef inorder(r):\n    if not r: return\n    inorder(r.left)\n    print(r.key, end=\" \")\n    inorder(r.right)\n\nroot = None\nfor k in [5,3,8,1,4,7,9]:\n    root = insert(root, k)\nroot = splay(root, 1)\ninorder(root)\n\n\nWhy It Matters\n\nAmortized O(log n) performance\nAdapts dynamically to access patterns\nIdeal for caches, text editors, network routing tables\nSimple logic: no height or color tracking needed\n\n\n\nA Gentle Proof (Why It Works)\nEach splay operation may take O(h), but the amortized cost across multiple operations is O(log n). Accessing frequently used elements keeps them near the root, improving future operations.\n\n\nTry It Yourself\n\nBuild [5, 3, 8, 1, 4, 7, 9]\nAccess 1 → Observe Zig-Zig rotation\nAccess 8 → Observe Zig-Zag\nInsert 10 → check rebalancing\nAccess 7 repeatedly → moves to root\n\n\n\nTest Cases\n\n\n\nSequence\nAccess\nCase\nRoot After\n\n\n\n\n[5,3,8,1,4]\n1\nZig-Zig\n1\n\n\n[10,20,30]\n30\nZig\n30\n\n\n[5,3,8,1,4]\n4\nZig-Zag\n4\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nAccess\nAmortized O(log n)\nO(h) recursion\n\n\nInsert\nAmortized O(log n)\nO(h)\n\n\nDelete\nAmortized O(log n)\nO(h)\n\n\n\nThe Splay Tree is like a memory, the more you touch something, the closer it stays to you.\n\n\n\n236 Treap Insert\nA Treap (Tree + Heap) is a brilliant hybrid: it’s a binary search tree by keys and a heap by priorities. Every node carries a key and a random priority. By combining ordering on keys with random heap priorities, the treap stays balanced on average, no strict rotations like AVL or color rules like Red-Black trees needed.\n\nWhat Problem Are We Solving?\nWe want a simple balanced search tree with expected O(log n) performance, but without maintaining height or color properties explicitly. Treaps solve this by assigning random priorities, keeping the structure balanced in expectation.\n\n\nHow It Works (Plain Language)\nEach node (key, priority) must satisfy:\n\nBST property: key(left) &lt; key &lt; key(right)\nHeap property: priority(parent) &lt; priority(children) (min-heap or max-heap convention)\n\nInsert like a normal BST by key, then fix heap property by rotations if the priority is violated.\n\n\n\nStep\nRule\n\n\n\n\n1\nInsert node by key (like BST)\n\n\n2\nAssign a random priority\n\n\n3\nWhile parent priority &gt; node priority → rotate node up\n\n\n\nThis randomization ensures expected logarithmic height.\n\n\nExample\nInsert sequence (key, priority):\n\n\n\n\n\n\n\n\n\nStep\nKey\nPriority\nStructure\n\n\n\n\n1\n(50, 15)\nroot\n50\n\n\n2\n(30, 10)\nsmaller priority → rotate right\n30(root) → 50(right child)\n\n\n3\n(70, 25)\nstays right of 50\nbalanced\n\n\n\nResult (BST by key, min-heap by priority):\n     (30,10)\n         \\\n         (50,15)\n             \\\n             (70,25)\n\n\nStep-by-Step Example\nInsert keys: [40, 20, 60, 10, 30, 50, 70] Priorities: [80, 90, 70, 100, 85, 60, 75]\n\n\n\nKey\nPriority\nRotation\nResult\n\n\n\n\n40\n80\nroot\n40\n\n\n20\n90\nnone\nleft child\n\n\n60\n70\nrotate left (heap fix)\n60 root\n\n\n10\n100\nnone\nleaf\n\n\n30\n85\nnone\nunder 20\n\n\n50\n60\nrotate left\n50 root\n\n\n70\n75\nnone\nleaf\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\ntypedef struct Node {\n    int key, priority;\n    struct Node *left, *right;\n} Node;\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;priority = rand() % 100; // random priority\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_right(Node* y) {\n    Node* x = y-&gt;left;\n    y-&gt;left = x-&gt;right;\n    x-&gt;right = y;\n    return x;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    x-&gt;right = y-&gt;left;\n    y-&gt;left = x;\n    return y;\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n\n    if (key &lt; root-&gt;key) {\n        root-&gt;left = insert(root-&gt;left, key);\n        if (root-&gt;left-&gt;priority &lt; root-&gt;priority)\n            root = rotate_right(root);\n    } else if (key &gt; root-&gt;key) {\n        root-&gt;right = insert(root-&gt;right, key);\n        if (root-&gt;right-&gt;priority &lt; root-&gt;priority)\n            root = rotate_left(root);\n    }\n    return root;\n}\n\nvoid inorder(Node* root) {\n    if (!root) return;\n    inorder(root-&gt;left);\n    printf(\"(%d,%d) \", root-&gt;key, root-&gt;priority);\n    inorder(root-&gt;right);\n}\n\nint main(void) {\n    srand(time(NULL));\n    Node* root = NULL;\n    int keys[] = {40, 20, 60, 10, 30, 50, 70};\n    for (int i = 0; i &lt; 7; i++)\n        root = insert(root, keys[i]);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython\nimport random\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.priority = random.randint(1, 100)\n        self.left = self.right = None\n\ndef rotate_right(y):\n    x = y.left\n    y.left = x.right\n    x.right = y\n    return x\n\ndef rotate_left(x):\n    y = x.right\n    x.right = y.left\n    y.left = x\n    return y\n\ndef insert(root, key):\n    if not root:\n        return Node(key)\n    if key &lt; root.key:\n        root.left = insert(root.left, key)\n        if root.left.priority &lt; root.priority:\n            root = rotate_right(root)\n    elif key &gt; root.key:\n        root.right = insert(root.right, key)\n        if root.right.priority &lt; root.priority:\n            root = rotate_left(root)\n    return root\n\ndef inorder(root):\n    if not root: return\n    inorder(root.left)\n    print(f\"({root.key},{root.priority})\", end=\" \")\n    inorder(root.right)\n\nroot = None\nfor k in [40, 20, 60, 10, 30, 50, 70]:\n    root = insert(root, k)\ninorder(root)\n\n\nWhy It Matters\n\nSimple implementation with expected balance\nProbabilistic alternative to AVL / Red-Black\nGreat for randomized data structures and Cartesian tree applications\nNo explicit height or color tracking\n\n\n\nA Gentle Proof (Why It Works)\nRandom priorities imply random structure; the probability of a node being high in the tree drops exponentially with depth. Hence, expected height is O(log n) with high probability.\n\n\nTry It Yourself\n\nInsert keys [10, 20, 30, 40] with random priorities.\nObserve random structure (not sorted by insertion).\nChange priority generator → test skewness.\nVisualize both BST and heap invariants.\n\n\n\nTest Cases\n\n\n\nKeys\nPriorities\nResulting Root\nHeight\n\n\n\n\n[10,20,30]\n[5,3,4]\n20\n2\n\n\n[5,2,8,1,3]\nrandom\nvaries\n~log n\n\n\n[1..100]\nrandom\nbalanced\nO(log n)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n) expected\nO(h) recursion\n\n\nSearch\nO(log n) expected\nO(h)\n\n\nDelete\nO(log n) expected\nO(h)\n\n\n\nA Treap dances to the rhythm of chance, balancing order with randomness to stay nimble and fair.\n\n\n\n237 Treap Delete\nDeleting from a Treap blends the best of both worlds: BST search for locating the node and heap-based rotations to remove it while keeping balance. Instead of rebalancing explicitly, we rotate the target node downward until it becomes a leaf, then remove it.\n\nWhat Problem Are We Solving?\nWe want to delete a key while preserving both:\n\nBST property: keys in order\nHeap property: priorities follow min-/max-heap rule\n\nTreaps handle this by rotating down the target node until it has at most one child, then removing it directly.\n\n\nHow It Works (Plain Language)\n\nSearch for the node by key (BST search).\nOnce found, rotate it downward until it has ≤ 1 child:\n\nRotate left if right child’s priority &lt; left child’s priority\nRotate right otherwise\n\nRemove it once it’s a leaf or single-child node.\n\nBecause priorities are random, the tree remains balanced in expectation.\n\n\nExample\nTreap (key, priority):\n      (40,20)\n     /      \\\n (30,10)   (50,25)\nDelete 40:\n\nCompare children: (30,10) &lt; (50,25) → rotate right on 40\nNew root (30,10)\n40 now right child → continue rotation if needed → eventually becomes leaf → delete\n\nNew structure maintains BST + heap properties.\n\n\nStep-by-Step Example\nStart with nodes:\n\n\n\nKey\nPriority\n\n\n\n\n40\n50\n\n\n20\n60\n\n\n60\n70\n\n\n10\n90\n\n\n30\n80\n\n\n50\n65\n\n\n70\n75\n\n\n\nDelete 40:\n\n\n\n\n\n\n\n\n\n\nStep\nNode\nAction\nRotation\nRoot\n\n\n\n\n1\n40\nCompare children priorities\nLeft child higher\nRotate right\n\n\n2\n30\nNow parent of 40\n40 demoted\nContinue if needed\n\n\n3\n40\nBecomes leaf\nRemove\n30\n\n\n\nFinal root: (30,80) All properties hold.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\ntypedef struct Node {\n    int key, priority;\n    struct Node *left, *right;\n} Node;\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;priority = rand() % 100;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_right(Node* y) {\n    Node* x = y-&gt;left;\n    y-&gt;left = x-&gt;right;\n    x-&gt;right = y;\n    return x;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    x-&gt;right = y-&gt;left;\n    y-&gt;left = x;\n    return y;\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n    if (key &lt; root-&gt;key) {\n        root-&gt;left = insert(root-&gt;left, key);\n        if (root-&gt;left-&gt;priority &lt; root-&gt;priority)\n            root = rotate_right(root);\n    } else if (key &gt; root-&gt;key) {\n        root-&gt;right = insert(root-&gt;right, key);\n        if (root-&gt;right-&gt;priority &lt; root-&gt;priority)\n            root = rotate_left(root);\n    }\n    return root;\n}\n\nNode* delete(Node* root, int key) {\n    if (!root) return NULL;\n\n    if (key &lt; root-&gt;key)\n        root-&gt;left = delete(root-&gt;left, key);\n    else if (key &gt; root-&gt;key)\n        root-&gt;right = delete(root-&gt;right, key);\n    else {\n        // Found node to delete\n        if (!root-&gt;left && !root-&gt;right) {\n            free(root);\n            return NULL;\n        } else if (!root-&gt;left)\n            root = rotate_left(root);\n        else if (!root-&gt;right)\n            root = rotate_right(root);\n        else if (root-&gt;left-&gt;priority &lt; root-&gt;right-&gt;priority)\n            root = rotate_right(root);\n        else\n            root = rotate_left(root);\n\n        root = delete(root, key);\n    }\n    return root;\n}\n\nvoid inorder(Node* root) {\n    if (!root) return;\n    inorder(root-&gt;left);\n    printf(\"(%d,%d) \", root-&gt;key, root-&gt;priority);\n    inorder(root-&gt;right);\n}\n\nint main(void) {\n    srand(time(NULL));\n    Node* root = NULL;\n    int keys[] = {40, 20, 60, 10, 30, 50, 70};\n    for (int i = 0; i &lt; 7; i++)\n        root = insert(root, keys[i]);\n\n    printf(\"Before deletion:\\n\");\n    inorder(root);\n    printf(\"\\n\");\n\n    root = delete(root, 40);\n\n    printf(\"After deleting 40:\\n\");\n    inorder(root);\n    printf(\"\\n\");\n}\nPython\nimport random\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.priority = random.randint(1, 100)\n        self.left = self.right = None\n\ndef rotate_right(y):\n    x = y.left\n    y.left = x.right\n    x.right = y\n    return x\n\ndef rotate_left(x):\n    y = x.right\n    x.right = y.left\n    y.left = x\n    return y\n\ndef insert(root, key):\n    if not root:\n        return Node(key)\n    if key &lt; root.key:\n        root.left = insert(root.left, key)\n        if root.left.priority &lt; root.priority:\n            root = rotate_right(root)\n    elif key &gt; root.key:\n        root.right = insert(root.right, key)\n        if root.right.priority &lt; root.priority:\n            root = rotate_left(root)\n    return root\n\ndef delete(root, key):\n    if not root:\n        return None\n    if key &lt; root.key:\n        root.left = delete(root.left, key)\n    elif key &gt; root.key:\n        root.right = delete(root.right, key)\n    else:\n        if not root.left and not root.right:\n            return None\n        elif not root.left:\n            root = rotate_left(root)\n        elif not root.right:\n            root = rotate_right(root)\n        elif root.left.priority &lt; root.right.priority:\n            root = rotate_right(root)\n        else:\n            root = rotate_left(root)\n        root = delete(root, key)\n    return root\n\ndef inorder(root):\n    if not root: return\n    inorder(root.left)\n    print(f\"({root.key},{root.priority})\", end=\" \")\n    inorder(root.right)\n\nroot = None\nfor k in [40,20,60,10,30,50,70]:\n    root = insert(root, k)\nprint(\"Before:\", end=\" \")\ninorder(root)\nprint()\nroot = delete(root, 40)\nprint(\"After:\", end=\" \")\ninorder(root)\n\n\nWhy It Matters\n\nNo explicit rebalancing, rotations emerge from heap priorities\nExpected O(log n) deletion\nNatural, simple structure for randomized search trees\nUsed in randomized algorithms, dynamic sets, and order-statistics\n\n\n\nA Gentle Proof (Why It Works)\nAt each step, the node rotates toward a child with smaller priority, preserving heap property. Expected height remains O(log n) due to independent random priorities.\n\n\nTry It Yourself\n\nBuild treap with [10, 20, 30, 40, 50].\nDelete 30 → observe rotations to leaf.\nDelete 10 (root) → rotation chooses smaller-priority child.\nRepeat deletions, verify BST + heap invariants hold.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nKeys\nDelete\nRotations\nBalanced\nRoot After\n\n\n\n\n[40,20,60,10,30,50,70]\n40\nRight, Left\n✅\n30\n\n\n[10,20,30]\n20\nLeft\n✅\n10\n\n\n[50,30,70,20,40,60,80]\n30\nRight\n✅\n50\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDelete\nO(log n) expected\nO(h) recursion\n\n\nInsert\nO(log n) expected\nO(h)\n\n\nSearch\nO(log n) expected\nO(h)\n\n\n\nThe Treap deletes gracefully, rotating away until the target fades like a falling leaf, leaving balance behind.\n\n\n\n238 Weight Balanced Tree\nA Weight Balanced Tree (WBT) maintains balance not by height or color, but by subtree sizes (or “weights”). Each node keeps track of how many elements are in its left and right subtrees, and balance is enforced by requiring their weights to stay within a constant ratio.\n\nWhat Problem Are We Solving?\nIn ordinary BSTs, imbalance can grow as keys are inserted in sorted order, degrading performance to O(n). Weight Balanced Trees fix this by keeping subtree sizes within a safe range. They are particularly useful when you need order-statistics (like “find the k-th element”) or split/merge operations that depend on sizes rather than heights.\n\n\nHow It Works (Plain Language)\nEach node maintains a weight = total number of nodes in its subtree. When inserting or deleting, you update weights and check the balance condition:\nIf\nweight(left) ≤ α × weight(node)\nweight(right) ≤ α × weight(node)\nfor some balance constant α (e.g., 0.7), then the tree is considered balanced.\nIf not, perform rotations (like AVL) to restore balance.\n\n\n\n\n\n\n\nOperation\nSteps\n\n\n\n\nInsert\nInsert like BST → Update weights → If ratio violated → Rotate\n\n\nDelete\nRemove node → Update weights → If ratio violated → Rebuild or rotate\n\n\nSearch\nBST search (weights guide decisions for rank queries)\n\n\n\nBecause weights reflect actual sizes, the tree ensures O(log n) expected height.\n\n\nExample\nLet’s use α = 0.7\nInsert [10, 20, 30, 40, 50]\n\n\n\n\n\n\n\n\n\n\n\nStep\nInsert\nWeight(left)\nWeight(right)\nBalanced?\nFix\n\n\n\n\n1\n10\n0\n0\n✅\n-\n\n\n2\n20\n1\n0\n✅\n-\n\n\n3\n30\n2\n0\n❌\nRotate left\n\n\n4\n40\n3\n0\n❌\nRotate left\n\n\n5\n50\n4\n0\n❌\nRotate left\n\n\n\nTree remains balanced by rotations once left-right ratio exceeds α.\n\n\nStep-by-Step Example\nInsert [1, 2, 3, 4, 5] with α = 0.7\n\n\n\nStep\nAction\nWeights\nBalanced?\nRotation\n\n\n\n\n1\nInsert 1\n(0,0)\n✅\n-\n\n\n2\nInsert 2\n(1,0)\n✅\n-\n\n\n3\nInsert 3\n(2,0)\n❌\nLeft Rotate\n\n\n4\nInsert 4\n(3,0)\n❌\nLeft Rotate\n\n\n5\nInsert 5\n(4,0)\n❌\nLeft Rotate\n\n\n\nTree remains height-balanced based on weights.\n\n\nTiny Code (Easy Versions)\nC (Simplified)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key;\n    int size; // weight = size of subtree\n    struct Node *left, *right;\n} Node;\n\nint size(Node* n) { return n ? n-&gt;size : 0; }\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;size = 1;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    x-&gt;right = y-&gt;left;\n    y-&gt;left = x;\n    x-&gt;size = 1 + size(x-&gt;left) + size(x-&gt;right);\n    y-&gt;size = 1 + size(y-&gt;left) + size(y-&gt;right);\n    return y;\n}\n\nNode* rotate_right(Node* y) {\n    Node* x = y-&gt;left;\n    y-&gt;left = x-&gt;right;\n    x-&gt;right = y;\n    y-&gt;size = 1 + size(y-&gt;left) + size(y-&gt;right);\n    x-&gt;size = 1 + size(x-&gt;left) + size(x-&gt;right);\n    return x;\n}\n\ndouble alpha = 0.7;\n\nint balanced(Node* n) {\n    if (!n) return 1;\n    int l = size(n-&gt;left), r = size(n-&gt;right);\n    return l &lt;= alpha * n-&gt;size && r &lt;= alpha * n-&gt;size;\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n    if (key &lt; root-&gt;key) root-&gt;left = insert(root-&gt;left, key);\n    else if (key &gt; root-&gt;key) root-&gt;right = insert(root-&gt;right, key);\n    root-&gt;size = 1 + size(root-&gt;left) + size(root-&gt;right);\n    if (!balanced(root)) {\n        if (size(root-&gt;left) &gt; size(root-&gt;right))\n            root = rotate_right(root);\n        else\n            root = rotate_left(root);\n    }\n    return root;\n}\n\nvoid inorder(Node* n) {\n    if (!n) return;\n    inorder(n-&gt;left);\n    printf(\"%d(%d) \", n-&gt;key, n-&gt;size);\n    inorder(n-&gt;right);\n}\n\nint main(void) {\n    Node* root = NULL;\n    int keys[] = {10, 20, 30, 40, 50};\n    for (int i = 0; i &lt; 5; i++)\n        root = insert(root, keys[i]);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython (Simplified)\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.size = 1\n        self.left = self.right = None\n\ndef size(n): return n.size if n else 0\n\ndef update(n):\n    if n:\n        n.size = 1 + size(n.left) + size(n.right)\n\ndef rotate_left(x):\n    y = x.right\n    x.right = y.left\n    y.left = x\n    update(x)\n    update(y)\n    return y\n\ndef rotate_right(y):\n    x = y.left\n    y.left = x.right\n    x.right = y\n    update(y)\n    update(x)\n    return x\n\nalpha = 0.7\n\ndef balanced(n):\n    return not n or (size(n.left) &lt;= alpha * n.size and size(n.right) &lt;= alpha * n.size)\n\ndef insert(root, key):\n    if not root: return Node(key)\n    if key &lt; root.key: root.left = insert(root.left, key)\n    elif key &gt; root.key: root.right = insert(root.right, key)\n    update(root)\n    if not balanced(root):\n        if size(root.left) &gt; size(root.right):\n            root = rotate_right(root)\n        else:\n            root = rotate_left(root)\n    return root\n\ndef inorder(n):\n    if not n: return\n    inorder(n.left)\n    print(f\"{n.key}({n.size})\", end=\" \")\n    inorder(n.right)\n\nroot = None\nfor k in [10,20,30,40,50]:\n    root = insert(root, k)\ninorder(root)\n\n\nWhy It Matters\n\nBalances based on true subtree size, not height\nExcellent for order-statistics (kth, rank)\nSupports split, merge, and range queries naturally\nDeterministic balancing without randomness\n\n\n\nA Gentle Proof (Why It Works)\nMaintaining weight ratios guarantees logarithmic height. If α &lt; 1, each subtree has ≤ α × n nodes, so height ≤ log₁/α(n) → O(log n).\n\n\nTry It Yourself\n\nBuild [10, 20, 30, 40, 50] with α = 0.7.\nPrint sizes at each node.\nDelete 20 → rebalance by rotation.\nTry α = 0.6 and α = 0.8 → compare shapes.\n\n\n\nTest Cases\n\n\n\nSequence\nα\nBalanced?\nHeight\n\n\n\n\n[10,20,30,40,50]\n0.7\n✅\nlog n\n\n\n[1..100]\n0.7\n✅\nO(log n)\n\n\n[sorted 1..10]\n0.6\n✅\n~4\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n)\nO(h)\n\n\nDelete\nO(log n)\nO(h)\n\n\nSearch\nO(log n)\nO(h)\n\n\n\nA Weight Balanced Tree is like a tightrope walker, always adjusting its stance to keep perfect equilibrium, no matter how the sequence unfolds.\n\n\n\n239 Scapegoat Tree Rebuild\nA Scapegoat Tree is a binary search tree that maintains balance by occasionally rebuilding entire subtrees.\nInstead of performing rotations on every insertion, it monitors the depth of nodes.\nWhen a node’s depth exceeds the allowed limit, the algorithm locates a scapegoat ancestor whose subtree is too unbalanced, flattens that subtree into a sorted list, and rebuilds it into a perfectly balanced tree.\nWith a balance parameter \\(\\alpha\\) in \\((0.5, 1)\\), a Scapegoat Tree guarantees: - \\(O(\\log n)\\) amortized time for insertions and deletions\n- \\(O(\\log n)\\) worst-case height\n\nWhat Problem Are We Solving?\nStandard BSTs can become skewed and degrade to O(n). Rotational trees like AVL and Red Black maintain strict local invariants but add per update overhead. Scapegoat trees choose a middle path\n\nDo nothing most of the time\nOccasionally rebuild an entire subtree when a global bound is exceeded\n\nGoal Keep height near \\(\\log_{1/\\alpha} n\\) while keeping code simple and rotations out of the hot path.\n\n\nHow It Works (Plain Language)\nParameters and invariants\n\nChoose \\(\\alpha \\in (0.5, 1)\\) such as \\(\\alpha = \\tfrac{2}{3}\\)\nMaintain n = current size and n_max = maximum size since the last global rebuild\nHeight bound: if an insertion lands deeper than \\(\\lfloor \\log_{1/\\alpha} n \\rfloor\\), the tree is too tall\n\nInsert algorithm\n\nInsert like a normal BST and track the path length depth.\nIf depth is within the allowed bound, stop.\nOtherwise, walk up to find the scapegoat node s such that\n\\[\n\\max\\big(\\text{size}(s.\\text{left}),\\ \\text{size}(s.\\text{right})\\big) &gt; \\alpha \\cdot \\text{size}(s)\n\\]\nRebuild the subtree rooted at s into a perfectly balanced BST in time linear in that subtree’s size.\n\nDelete algorithm\n\nPerform a normal BST delete.\n\nDecrement n.\n\nIf \\(n &lt; \\alpha \\cdot n_{\\text{max}}\\), rebuild the entire tree and set \\(n_{\\text{max}} = n\\).\n\nWhy rebuilding works\n\nRebuilding performs an inorder traversal to collect nodes into a sorted array, then constructs a balanced BST from that array.\n\nRebuilds are infrequent, and their costs are amortized over many cheap insertions and deletions, ensuring logarithmic time on average.\n\n\n\nExample Walkthrough\nLet \\(\\alpha = \\tfrac{2}{3}\\).\nInsert the keys [10, 20, 30, 40, 50, 60] in ascending order.\n\n\n\n\n\n\n\n\n\n\nStep\nAction\nDepth vs bound\nScapegoat found\nRebuild\n\n\n\n\n1\ninsert 10\ndepth 0 within\nnone\nno\n\n\n2\ninsert 20\ndepth 1 within\nnone\nno\n\n\n3\ninsert 30\ndepth 2 within\nnone\nno\n\n\n4\ninsert 40\ndepth 3 exceeds bound\nancestor violates α\nrebuild that subtree\n\n\n5\ninsert 50\nlikely within\nnone\nno\n\n\n6\ninsert 60\nif depth exceeds\nfind ancestor\nrebuild subtree\n\n\n\nThe occasional rebuild produces a nearly perfect BST despite sorted input.\n\n\nTiny Code (Easy Versions)\nPython (concise reference implementation)\nfrom math import log, floor\n\nclass Node:\n    __slots__ = (\"key\", \"left\", \"right\", \"size\")\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n        self.size = 1\n\ndef size(x): return x.size if x else 0\ndef update(x): \n    if x: x.size = 1 + size(x.left) + size(x.right)\n\ndef flatten_inorder(x, arr):\n    if not x: return\n    flatten_inorder(x.left, arr)\n    arr.append(x)\n    flatten_inorder(x.right, arr)\n\ndef build_balanced(nodes, lo, hi):\n    if lo &gt;= hi: return None\n    mid = (lo + hi) // 2\n    root = nodes[mid]\n    root.left = build_balanced(nodes, lo, mid)\n    root.right = build_balanced(nodes, mid + 1, hi)\n    update(root)\n    return root\n\nclass ScapegoatTree:\n    def __init__(self, alpha=2/3):\n        self.alpha = alpha\n        self.root = None\n        self.n = 0\n        self.n_max = 0\n\n    def _log_alpha(self, n):\n        # height bound floor(log_{1/alpha} n)\n        if n &lt;= 1: return 0\n        return floor(log(n, 1 / self.alpha))\n\n    def _find_and_rebuild(self, path):\n        # path is list of nodes from root to inserted node\n        for i in range(len(path) - 1, -1, -1):\n            x = path[i]\n            l, r = size(x.left), size(x.right)\n            if max(l, r) &gt; self.alpha * size(x):\n                # rebuild subtree rooted at x and reattach to parent\n                nodes = []\n                flatten_inorder(x, nodes)\n                new_sub = build_balanced(nodes, 0, len(nodes))\n                if i == 0:\n                    self.root = new_sub\n                else:\n                    p = path[i - 1]\n                    if p.left is x: p.left = new_sub\n                    else: p.right = new_sub\n                    # fix sizes upward from parent\n                    for j in range(i - 1, -1, -1):\n                        update(path[j])\n                return\n\n    def insert(self, key):\n        self.n += 1\n        self.n_max = max(self.n_max, self.n)\n        if not self.root:\n            self.root = Node(key)\n            return\n\n        # standard BST insert with path tracking\n        path = []\n        cur = self.root\n        while cur:\n            path.append(cur)\n            if key &lt; cur.key:\n                if cur.left: cur = cur.left\n                else:\n                    cur.left = Node(key)\n                    path.append(cur.left)\n                    break\n            elif key &gt; cur.key:\n                if cur.right: cur = cur.right\n                else:\n                    cur.right = Node(key)\n                    path.append(cur.right)\n                    break\n            else:\n                # duplicate ignore\n                self.n -= 1\n                return\n        # update sizes\n        for node in reversed(path[:-1]):\n            update(node)\n\n        # depth check and possible rebuild\n        if len(path) - 1 &gt; self._log_alpha(self.n):\n            self._find_and_rebuild(path)\n\n    def _join_left_max(self, t):\n        # remove and return max node from subtree t, plus the new subtree\n        if not t.right:\n            return t, t.left\n        m, t.right = self._join_left_max(t.right)\n        update(t)\n        return m, t\n\n    def delete(self, key):\n        # standard BST delete\n        def _del(x, key):\n            if not x: return None, False\n            if key &lt; x.key:\n                x.left, removed = _del(x.left, key)\n            elif key &gt; x.key:\n                x.right, removed = _del(x.right, key)\n            else:\n                removed = True\n                if not x.left: return x.right, True\n                if not x.right: return x.left, True\n                # replace with predecessor\n                m, x.left = self._join_left_max(x.left)\n                m.left, m.right = x.left, x.right\n                x = m\n            update(x)\n            return x, removed\n\n        self.root, removed = _del(self.root, key)\n        if not removed: return\n        self.n -= 1\n        if self.n &lt; self.alpha * self.n_max:\n            # global rebuild\n            nodes = []\n            flatten_inorder(self.root, nodes)\n            self.root = build_balanced(nodes, 0, len(nodes))\n            self.n_max = self.n\nC (outline of the rebuild idea)\n// Sketch only: show rebuild path\n// 1) Do BST insert while recording path stack\n// 2) If depth &gt; bound, walk path upward to find scapegoat\n// 3) Inorder-copy subtree nodes to an array\n// 4) Recursively rebuild balanced subtree from that array\n// 5) Reattach rebuilt subtree to parent and fix sizes up the path\n\n\nWhy It Matters\n\nSimple balancing approach with rare but powerful subtree rebuilds\n\nWell suited for workloads where insertions arrive in bursts and frequent rotations are undesirable\n\nGuarantees height \\(O(\\log n)\\) with a tunable constant controlled by \\(\\alpha\\)\n\nThe inorder-based rebuild process makes ordered operations efficient and easy to implement\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(\\alpha \\in (0.5, 1)\\).\n\nHeight bound: if height exceeds \\(\\log_{1/\\alpha} n\\), a scapegoat exists since some ancestor must violate \\(\\max\\{|L|,\\ |R|\\} \\le \\alpha\\,|T|\\).\nAmortization: each node participates in \\(O(1)\\) rebuilds with \\(\\Theta(\\text{size of its subtree})\\) work, so the total cost over \\(m\\) operations is \\(O(m \\log n)\\).\nDeletion rule: rebuilding when \\(n &lt; \\alpha\\,n_{\\max}\\) prevents slack from accumulating.\n\n\n\nTry It Yourself\n\nInsert ascending keys to trigger rebuilds with \\(\\alpha = \\tfrac{2}{3}\\)\nDelete many keys so that \\(n &lt; \\alpha\\,n_{\\max}\\) and observe the global rebuild trigger\nExperiment with different \\(\\alpha\\) values\nAdd an order statistic query using stored subtree sizes\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation\nInput\nα\nExpected\n\n\n\n\nInsert ascending\n[1..1000]\n0.66\nHeight stays O(log n) with periodic rebuilds\n\n\nMixed ops\nrandom inserts and deletes\n0.7\nAmortized O(log n) per op\n\n\nShrink check\ninsert 1..200, delete 101..200\n0.7\nGlobal rebuild when size drops below α n_max\n\n\nDuplicate insert\ninsert 42 twice\n0.66\nsize unchanged after second insert\n\n\n\nEdge Cases\n\nRebuilding an empty or singleton subtree is a no-op\n\nDuplicate keys are ignored or handled according to policy\n\nChoose \\(\\alpha &gt; 0.5\\) to guarantee the existence of a scapegoat\n\n\n\nComplexity\n\n\n\n\n\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\nO(h) worst, O(log n) amortized\nO(1) extra\n\n\nInsert\nAmortized O(log n)\nO(1) extra plus rebuild buffer\n\n\nDelete\nAmortized O(log n)\nO(1) extra plus rebuild buffer\n\n\nRebuild subtree\nO(k) for k nodes in subtree\nO(k) temporary array\n\n\n\nScapegoat trees wait calmly until imbalance is undeniable, then rebuild decisively. The result is a BST that stays lean with minimal fuss.\n\n\n\n240 AA Tree\nAn AA Tree is a simplified version of a red-black tree that maintains balance using a single level value per node (instead of color). It enforces balance by a small set of easy-to-code rules using two operations, skew and split. The simplicity of AA trees makes them a popular teaching and practical implementation choice when you want red-black performance without complex cases.\n\nWhat Problem Are We Solving?\nWe want a balanced binary search tree with\n\nO(log n) time for insert/search/delete\nSimpler code than red-black or AVL\nFewer rotation cases\n\nAA trees achieve this by using levels (like black-heights in RBTs) to enforce structure, ensuring a balanced shape with one rotation per fix.\n\n\nHow It Works (Plain Language)\nEach node has\n\nkey: stored value\nlevel: like black height (root = 1)\n\nAA-tree invariants:\n\nLeft child level &lt; node level\nRight child level ≤ node level\nRight-right grandchild level &lt; node level\nEvery leaf has level 1\n\nTo restore balance after insert/delete, apply skew and split operations:\n\n\n\n\n\n\n\n\nOperation\nRule\nAction\n\n\n\n\nSkew\nIf left.level == node.level\nRotate right\n\n\nSplit\nIf right.right.level == node.level\nRotate left, level++\n\n\n\n\n\nExample\nInsert keys [10, 20, 30]\n\n\n\n\n\n\n\n\nStep\nTree\nFix\n\n\n\n\n1\n10 (lvl 1)\n-\n\n\n2\n10 → 20\nSkew none\n\n\n3\n10 → 20 → 30\nRight-right violation → Split → Rotate left on 10\n\n\n\nResult:\n    20 (2)\n   /  \\\n 10(1) 30(1)\n\n\nStep-by-Step Example\nInsert [30, 20, 10, 25]\n\n\n\n\n\n\n\n\n\n\nStep\nInsertion\nViolation\nFix\n\n\n\n\n\n1\n30\nnone\n-\n\n\n\n2\n20\nleft child level = node level\nSkew → Rotate right\n\n\n\n3\n10\ndeeper left\nSkew & Split\n20 root\n\n\n4\n25\ninsert right of 20\nSplit if needed\nbalanced\n\n\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int key;\n    int level;\n    struct Node *left, *right;\n} Node;\n\nNode* new_node(int key) {\n    Node* n = malloc(sizeof(Node));\n    n-&gt;key = key;\n    n-&gt;level = 1;\n    n-&gt;left = n-&gt;right = NULL;\n    return n;\n}\n\nNode* rotate_left(Node* x) {\n    Node* y = x-&gt;right;\n    x-&gt;right = y-&gt;left;\n    y-&gt;left = x;\n    return y;\n}\n\nNode* rotate_right(Node* y) {\n    Node* x = y-&gt;left;\n    y-&gt;left = x-&gt;right;\n    x-&gt;right = y;\n    return x;\n}\n\nNode* skew(Node* x) {\n    if (x && x-&gt;left && x-&gt;left-&gt;level == x-&gt;level)\n        x = rotate_right(x);\n    return x;\n}\n\nNode* split(Node* x) {\n    if (x && x-&gt;right && x-&gt;right-&gt;right && x-&gt;right-&gt;right-&gt;level == x-&gt;level) {\n        x = rotate_left(x);\n        x-&gt;level++;\n    }\n    return x;\n}\n\nNode* insert(Node* root, int key) {\n    if (!root) return new_node(key);\n    if (key &lt; root-&gt;key)\n        root-&gt;left = insert(root-&gt;left, key);\n    else if (key &gt; root-&gt;key)\n        root-&gt;right = insert(root-&gt;right, key);\n    root = skew(root);\n    root = split(root);\n    return root;\n}\n\nvoid inorder(Node* r) {\n    if (!r) return;\n    inorder(r-&gt;left);\n    printf(\"(%d, %d) \", r-&gt;key, r-&gt;level);\n    inorder(r-&gt;right);\n}\n\nint main(void) {\n    Node* root = NULL;\n    int keys[] = {10, 20, 30, 15, 25};\n    for (int i = 0; i &lt; 5; i++)\n        root = insert(root, keys[i]);\n    inorder(root);\n    printf(\"\\n\");\n}\nPython\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.level = 1\n        self.left = None\n        self.right = None\n\ndef rotate_left(x):\n    y = x.right\n    x.right = y.left\n    y.left = x\n    return y\n\ndef rotate_right(y):\n    x = y.left\n    y.left = x.right\n    x.right = y\n    return x\n\ndef skew(x):\n    if x and x.left and x.left.level == x.level:\n        x = rotate_right(x)\n    return x\n\ndef split(x):\n    if x and x.right and x.right.right and x.right.right.level == x.level:\n        x = rotate_left(x)\n        x.level += 1\n    return x\n\ndef insert(root, key):\n    if not root:\n        return Node(key)\n    if key &lt; root.key:\n        root.left = insert(root.left, key)\n    elif key &gt; root.key:\n        root.right = insert(root.right, key)\n    root = skew(root)\n    root = split(root)\n    return root\n\ndef inorder(r):\n    if not r: return\n    inorder(r.left)\n    print(f\"({r.key},{r.level})\", end=\" \")\n    inorder(r.right)\n\nroot = None\nfor k in [10, 20, 30, 15, 25]:\n    root = insert(root, k)\ninorder(root)\n\n\nWhy It Matters\n\nSimpler than red-black\nOnly two balancing functions (skew, split)\nMaintains O(log n) performance\nGreat teaching and reference structure for self-balancing BSTs\n\n\n\nA Gentle Proof (Why It Works)\nAA-trees are a special case of red-black trees where\n\nRight-leaning red links represent level\nSkew fixes left-leaning reds\nSplit fixes two consecutive rights\n\nThese rules ensure black-height invariants and logarithmic height.\n\n\nTry It Yourself\n\nInsert ascending keys → observe periodic right rotations\nDraw tree after each step → check levels\nCompare structure to equivalent red-black tree\nModify split condition → see imbalance appear\n\n\n\nTest Cases\n\n\n\nInput Keys\nExpected Root\nHeight\nBalanced?\n\n\n\n\n[10,20,30]\n20\n2\n✅\n\n\n[1..10]\n4–5\n~log n\n✅\n\n\n[30,20,10,25]\n20\n3\n✅\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(log n)\nO(h)\n\n\nSearch\nO(log n)\nO(1)\n\n\nDelete\nO(log n)\nO(h)\n\n\n\nAA Trees blend red-black efficiency with AVL simplicity, balanced, elegant, and easy to reason about.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-25.-segment-trees-and-fenwick-trees",
    "href": "books/en-us/list-3.html#section-25.-segment-trees-and-fenwick-trees",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 25. Segment Trees and Fenwick Trees",
    "text": "Section 25. Segment Trees and Fenwick Trees\n\n241 Build Segment Tree\nA segment tree is a binary tree over an array that stores range information for fast queries and updates. Building it means constructing the tree from the base array so each node summarizes a segment, like a sum, min, max, or gcd. The classic build time is O(n) and it enables O(log n) queries and updates.\n\nWhat Problem Are We Solving?\nWe want to answer range questions quickly\n\nSum on [l, r]\nMin or max on [l, r]\nCounts or bitwise ops on [l, r]\n\nIf we precompute a segment tree, each internal node stores the merge of its two children. Then queries and updates touch only O(log n) nodes.\nGoal Turn an array A[0..n-1] into a tree T where T[v] summarizes a segment A[L..R].\n\n\nHow It Works (Plain Language)\nRepresent the tree in an array (1 indexed for simplicity)\n\nNode v covers segment [L, R]\nLeft child 2v covers [L, mid]\nRight child 2v+1 covers [mid+1, R]\nT[v] = merge(T[2v], T[2v+1]) where merge is sum, min, max, etc.\n\nBuild Steps\n\nIf L == R, store the leaf value A[L] in T[v].\nElse split at mid, recursively build left and right, then T[v] = merge(left, right).\n\nExample Build (sum)\nArray A = [2, 1, 3, 4]\n\n\n\nNode v\nSegment [L,R]\nValue\n\n\n\n\n1\n[0,3]\n10\n\n\n2\n[0,1]\n3\n\n\n3\n[2,3]\n7\n\n\n4\n[0,0]\n2\n\n\n5\n[1,1]\n1\n\n\n6\n[2,2]\n3\n\n\n7\n[3,3]\n4\n\n\n\nmerge = sum, so T[1] = 3 + 7 = 10, leaves store the original values.\nAnother Example Build (min)\nArray A = [5, 2, 6, 1]\n\n\n\nNode v\nSegment [L,R]\nValue (min)\n\n\n\n\n1\n[0,3]\n1\n\n\n2\n[0,1]\n2\n\n\n3\n[2,3]\n1\n\n\n4\n[0,0]\n5\n\n\n5\n[1,1]\n2\n\n\n6\n[2,2]\n6\n\n\n7\n[3,3]\n1\n\n\n\n\n\nTiny Code (Easy Versions)\nC (iterative storage size 4n, recursive build for sum)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100000\nint A[MAXN];\nlong long T[4*MAXN];\n\nlong long merge(long long a, long long b) { return a + b; }\n\nvoid build(int v, int L, int R) {\n    if (L == R) {\n        T[v] = A[L];\n        return;\n    }\n    int mid = (L + R) / 2;\n    build(2*v, L, mid);\n    build(2*v+1, mid+1, R);\n    T[v] = merge(T[2*v], T[2*v+1]);\n}\n\nint main(void) {\n    int n = 4;\n    A[0]=2; A[1]=1; A[2]=3; A[3]=4;\n    build(1, 0, n-1);\n    for (int v = 1; v &lt; 8; v++) printf(\"T[%d]=%lld\\n\", v, T[v]);\n    return 0;\n}\nPython (sum segment tree, recursive build)\ndef build(arr):\n    n = len(arr)\n    size = 1\n    while size &lt; n:\n        size &lt;&lt;= 1\n    T = [0] * (2 * size)\n\n    # leaves\n    for i in range(n):\n        T[size + i] = arr[i]\n    # internal nodes\n    for v in range(size - 1, 0, -1):\n        T[v] = T[2*v] + T[2*v + 1]\n    return T, size  # T is 1..size*2-1, leaves start at index size\n\n# Example\nA = [2, 1, 3, 4]\nT, base = build(A)\n# T[1] holds sum of all, T[base+i] holds A[i]\nprint(\"Root sum:\", T[1])\nNotes\n\nThe C version shows classic recursive top down build.\nThe Python version shows an iterative bottom up build using a power of two base, also O(n).\n\n\n\nWhy It Matters\n\nPreprocessing time O(n) enables\n\nRange queries in O(log n)\nPoint updates in O(log n)\n\nChoice of merge adapts the tree to many tasks\n\nsum, min, max, gcd, bitwise and, custom structs\n\n\n\n\nA Gentle Proof (Why It Works)\nEach element appears in exactly one leaf and contributes to O(1) nodes per level. The tree has O(log n) levels. Total number of nodes built is at most 2n, so total work is O(n).\n\n\nTry It Yourself\n\nChange merge to min or max and rebuild.\nBuild then verify that T[1] equals sum(A).\nPrint the tree level by level to visualize segments.\nExtend the structure to support point update and range query.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nArray A\nMerge\nExpected Root T[1]\nNotes\n\n\n\n\n[2,1,3,4]\nsum\n10\nbasic sum tree\n\n\n[5,2,6,1]\nmin\n1\nswitch merge to min\n\n\n[7]\nsum\n7\nsingle element\n\n\n[]\nsum\n0 or no tree\nhandle empty as special case\n\n\n\n\n\nComplexity\n\n\n\nPhase\nTime\nSpace\n\n\n\n\nBuild\nO(n)\nO(n)\n\n\nQuery\nO(log n)\nO(1) extra\n\n\nPoint update\nO(log n)\nO(1) extra\n\n\n\nBuild once, answer fast forever. A segment tree turns an array into a range answering machine.\n\n\n\n242 Range Sum Query\nA Range Sum Query (RSQ) retrieves the sum of elements in a subarray [L, R] using a segment tree. Once the tree is built, each query runs in O(log n) time, by combining results from the minimal set of segments covering [L, R].\n\nWhat Problem Are We Solving?\nGiven an array A[0..n-1], we want to answer queries like\nsum(A[L..R])  \nquickly, without recalculating from scratch each time.\nNaive approach: O(R–L+1) per query Segment tree approach: O(log n) per query after O(n) build.\n\n\nHow It Works (Plain Language)\nWe maintain a segment tree T built as before (T[v] = sum of segment). To answer query(v, L, R, qL, qR)\n\nIf segment [L, R] is fully outside [qL, qR], return 0.\nIf segment [L, R] is fully inside, return T[v].\nOtherwise, split at mid and combine results from left and right children.\n\nSo each query visits only O(log n) nodes, each exactly covering part of the query range.\n\n\nExample\nArray: A = [2, 1, 3, 4, 5]\nQuery: sum on [1,3] (1-based: elements 1..3 = 1+3+4=8)\n\n\n\n\n\n\n\n\n\n\nNode v\nSegment [L,R]\nValue\nRelationship to [1,3]\nContribution\n\n\n\n\n1\n[0,4]\n15\noverlaps\nrecurse\n\n\n2\n[0,2]\n6\noverlaps\nrecurse\n\n\n3\n[3,4]\n9\noverlaps\nrecurse\n\n\n4\n[0,1]\n3\npartial\nrecurse\n\n\n5\n[2,2]\n3\ninside\n+3\n\n\n8\n[0,0]\n2\noutside\nskip\n\n\n9\n[1,1]\n1\ninside\n+1\n\n\n6\n[3,3]\n4\ninside\n+4\n\n\n7\n[4,4]\n5\noutside\nskip\n\n\n\nSum = 1 + 3 + 4 = 8 ✅\n\n\nStep-by-Step Table (for clarity)\n\n\n\nStep\nCurrent Segment\nCovered?\nAction\n\n\n\n\n1\n[0,4]\nOverlaps [1,3]\nSplit\n\n\n2\n[0,2]\nOverlaps [1,3]\nSplit\n\n\n3\n[3,4]\nOverlaps [1,3]\nSplit\n\n\n4\n[0,1]\nOverlaps\nSplit\n\n\n5\n[0,0]\nOutside\nReturn 0\n\n\n6\n[1,1]\nInside\nReturn 1\n\n\n7\n[2,2]\nInside\nReturn 3\n\n\n8\n[3,3]\nInside\nReturn 4\n\n\n9\n[4,4]\nOutside\nReturn 0\n\n\n\nTotal = 1+3+4 = 8\n\n\nTiny Code (Easy Versions)\nC (Recursive RSQ)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100000\nint A[MAXN];\nlong long T[4*MAXN];\n\nlong long merge(long long a, long long b) { return a + b; }\n\nvoid build(int v, int L, int R) {\n    if (L == R) {\n        T[v] = A[L];\n        return;\n    }\n    int mid = (L + R) / 2;\n    build(2*v, L, mid);\n    build(2*v+1, mid+1, R);\n    T[v] = merge(T[2*v], T[2*v+1]);\n}\n\nlong long query(int v, int L, int R, int qL, int qR) {\n    if (qR &lt; L || R &lt; qL) return 0; // disjoint\n    if (qL &lt;= L && R &lt;= qR) return T[v]; // fully covered\n    int mid = (L + R) / 2;\n    long long left = query(2*v, L, mid, qL, qR);\n    long long right = query(2*v+1, mid+1, R, qL, qR);\n    return merge(left, right);\n}\n\nint main(void) {\n    int n = 5;\n    int Avals[5] = {2,1,3,4,5};\n    for (int i=0;i&lt;n;i++) A[i]=Avals[i];\n    build(1, 0, n-1);\n    printf(\"Sum [1,3] = %lld\\n\", query(1,0,n-1,1,3)); // expect 8\n}\nPython (Iterative)\ndef build(arr):\n    n = len(arr)\n    size = 1\n    while size &lt; n:\n        size &lt;&lt;= 1\n    T = [0]*(2*size)\n    # leaves\n    for i in range(n):\n        T[size+i] = arr[i]\n    # parents\n    for v in range(size-1, 0, -1):\n        T[v] = T[2*v] + T[2*v+1]\n    return T, size\n\ndef query(T, base, l, r):\n    l += base\n    r += base\n    res = 0\n    while l &lt;= r:\n        if l % 2 == 1:\n            res += T[l]\n            l += 1\n        if r % 2 == 0:\n            res += T[r]\n            r -= 1\n        l //= 2\n        r //= 2\n    return res\n\nA = [2,1,3,4,5]\nT, base = build(A)\nprint(\"Sum [1,3] =\", query(T, base, 1, 3))  # expect 8\n\n\nWhy It Matters\n\nEnables fast range queries on static or dynamic arrays.\nFoundation for many extensions:\n\nRange min/max query (change merge)\nLazy propagation (for range updates)\n2D and persistent trees\n\n\n\n\nA Gentle Proof (Why It Works)\nEach level of recursion splits into disjoint segments. At most 2 segments per level are added to the result. Depth = O(log n), so total work = O(log n).\n\n\nTry It Yourself\n\nQuery different [L, R] ranges after build.\nReplace merge with min() or max() and verify results.\nCombine queries to verify overlapping segments are counted once.\nAdd update operation to change elements and re-query.\n\n\n\nTest Cases\n\n\n\nArray A\nQuery [L,R]\nExpected\nNotes\n\n\n\n\n[2,1,3,4,5]\n[1,3]\n8\n1+3+4\n\n\n[5,5,5,5]\n[0,3]\n20\nuniform\n\n\n[1,2,3,4,5]\n[2,4]\n12\n3+4+5\n\n\n[7]\n[0,0]\n7\nsingle element\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nQuery\nO(log n)\nO(1)\n\n\nBuild\nO(n)\nO(n)\n\n\nUpdate\nO(log n)\nO(1)\n\n\n\nSegment trees let you ask “what’s in this range?”, and get the answer fast, no matter how big the array.\n\n\n\n243 Range Update (Lazy Propagation Technique)\nA range update modifies all elements in a segment [L, R] efficiently. Without optimization, you’d touch every element, O(n). With lazy propagation, you defer work, storing pending updates in a separate array, achieving O(log n) time per update and query.\nThis pattern is essential when many updates overlap, like adding +5 to every element in [2, 6] repeatedly.\n\nWhat Problem Are We Solving?\nWe want to efficiently support both:\n\nRange updates: Add or set a value over [L, R]\nRange queries: Get sum/min/max over [L, R]\n\nNaive solution: O(R–L+1) per update. Lazy propagation: O(log n) per update/query.\nExample goal:\nadd +3 to A[2..5]\nthen query sum(A[0..7])\n\n\nHow It Works (Plain Language)\nEach segment node T[v] stores summary (e.g. sum). Each node lazy[v] stores pending update (not yet pushed to children).\nWhen updating [L, R]:\n\nIf node’s segment is fully inside, apply update directly to T[v] and mark lazy[v] (no recursion).\nIf partially overlapping, push pending updates down, then recurse.\n\nWhen querying [L, R]:\n\nPush any pending updates first\nCombine child results as usual\n\nThis way, each node is updated at most once per path, giving O(log n).\n\n\nExample\nArray: A = [1, 2, 3, 4, 5, 6, 7, 8] Build tree for sum.\nStep 1: Range update [2, 5] += 3\n\n\n\nNode\nSegment\nAction\nlazy[]\nT[] sum\n\n\n\n\n1\n[0,7]\noverlaps → recurse\n0\nunchanged\n\n\n2\n[0,3]\noverlaps → recurse\n0\nunchanged\n\n\n3\n[4,7]\noverlaps → recurse\n0\nunchanged\n\n\n4\n[0,1]\noutside\n-\n-\n\n\n5\n[2,3]\ninside → add +3×2=6\nlazy[5]=3\nT[5]+=6\n\n\n6\n[4,5]\ninside → add +3×2=6\nlazy[6]=3\nT[6]+=6\n\n\n\nLater queries automatically apply +3 to affected subranges.\n\n\nStep-by-Step Example\nLet’s trace two operations:\n\nupdate(2,5,+3)\nquery(0,7)\n\n\n\n\nStep\nAction\nResult\n\n\n\n\nBuild\nsum = [1,2,3,4,5,6,7,8] → 36\nT[1]=36\n\n\nUpdate\nmark lazy for segments fully in [2,5]\nT[v]+=3×len\n\n\nQuery\npropagate lazy before using T[v]\nsum = 36 + 3×4 = 48\n\n\n\nResult after update: total = 48 ✅\n\n\nTiny Code (Easy Versions)\nC (Recursive, sum tree with lazy add)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100000\nint A[MAXN];\nlong long T[4*MAXN], lazy[4*MAXN];\n\nlong long merge(long long a, long long b) { return a + b; }\n\nvoid build(int v, int L, int R) {\n    if (L == R) { T[v] = A[L]; return; }\n    int mid = (L + R) / 2;\n    build(2*v, L, mid);\n    build(2*v+1, mid+1, R);\n    T[v] = merge(T[2*v], T[2*v+1]);\n}\n\nvoid push(int v, int L, int R) {\n    if (lazy[v] != 0) {\n        T[v] += lazy[v] * (R - L + 1);\n        if (L != R) {\n            lazy[2*v] += lazy[v];\n            lazy[2*v+1] += lazy[v];\n        }\n        lazy[v] = 0;\n    }\n}\n\nvoid update(int v, int L, int R, int qL, int qR, int val) {\n    push(v, L, R);\n    if (qR &lt; L || R &lt; qL) return;\n    if (qL &lt;= L && R &lt;= qR) {\n        lazy[v] += val;\n        push(v, L, R);\n        return;\n    }\n    int mid = (L + R) / 2;\n    update(2*v, L, mid, qL, qR, val);\n    update(2*v+1, mid+1, R, qL, qR, val);\n    T[v] = merge(T[2*v], T[2*v+1]);\n}\n\nlong long query(int v, int L, int R, int qL, int qR) {\n    push(v, L, R);\n    if (qR &lt; L || R &lt; qL) return 0;\n    if (qL &lt;= L && R &lt;= qR) return T[v];\n    int mid = (L + R) / 2;\n    return merge(query(2*v, L, mid, qL, qR), query(2*v+1, mid+1, R, qL, qR));\n}\n\nint main(void) {\n    int n = 8;\n    int vals[] = {1,2,3,4,5,6,7,8};\n    for (int i=0;i&lt;n;i++) A[i]=vals[i];\n    build(1,0,n-1);\n    update(1,0,n-1,2,5,3);\n    printf(\"Sum [0,7] = %lld\\n\", query(1,0,n-1,0,7)); // expect 48\n}\nPython (Iterative version)\nclass LazySegTree:\n    def __init__(self, arr):\n        n = len(arr)\n        size = 1\n        while size &lt; n: size &lt;&lt;= 1\n        self.n = n; self.size = size\n        self.T = [0]*(2*size)\n        self.lazy = [0]*(2*size)\n        for i in range(n): self.T[size+i] = arr[i]\n        for i in range(size-1, 0, -1):\n            self.T[i] = self.T[2*i] + self.T[2*i+1]\n    \n    def _apply(self, v, val, length):\n        self.T[v] += val * length\n        if v &lt; self.size:\n            self.lazy[v] += val\n\n    def _push(self, v, length):\n        if self.lazy[v]:\n            self._apply(2*v, self.lazy[v], length//2)\n            self._apply(2*v+1, self.lazy[v], length//2)\n            self.lazy[v] = 0\n\n    def update(self, l, r, val):\n        def _upd(v, L, R):\n            if r &lt; L or R &lt; l: return\n            if l &lt;= L and R &lt;= r:\n                self._apply(v, val, R-L+1)\n                return\n            self._push(v, R-L+1)\n            mid = (L+R)//2\n            _upd(2*v, L, mid)\n            _upd(2*v+1, mid+1, R)\n            self.T[v] = self.T[2*v] + self.T[2*v+1]\n        _upd(1, 0, self.size-1)\n\n    def query(self, l, r):\n        def _qry(v, L, R):\n            if r &lt; L or R &lt; l: return 0\n            if l &lt;= L and R &lt;= r:\n                return self.T[v]\n            self._push(v, R-L+1)\n            mid = (L+R)//2\n            return _qry(2*v, L, mid) + _qry(2*v+1, mid+1, R)\n        return _qry(1, 0, self.size-1)\n\nA = [1,2,3,4,5,6,7,8]\nst = LazySegTree(A)\nst.update(2,5,3)\nprint(\"Sum [0,7] =\", st.query(0,7))  # expect 48\n\n\nWhy It Matters\n\nCrucial for problems with many overlapping updates\nUsed in range-add, range-assign, interval covering, and 2D extensions\nFoundation for Segment Tree Beats\n\n\n\nA Gentle Proof (Why It Works)\nEach update marks at most one node per level as “lazy.” When queried later, we “push” those updates downward once. Each node’s pending updates applied O(1) times → total cost O(log n).\n\n\nTry It Yourself\n\nBuild [1,2,3,4,5] → update [1,3] += 2 → query sum [0,4] → expect 21\nUpdate [0,4] += 1 → query [2,4] → expect 3+2+2+1+1 = 15\nCombine multiple updates → verify cumulative results\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nOperation\nArray\nQuery\nExpected\n\n\n\n\nupdate [2,5] += 3\n[1,2,3,4,5,6,7,8]\nsum [0,7]\n48\n\n\nupdate [0,3] += 2\n[5,5,5,5]\nsum [1,2]\n14\n\n\ntwo updates\n[1,1,1,1,1]\n[1,3] +1, [2,4] +2\n[1,3] sum = 10\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nRange update\nO(log n)\nO(n)\n\n\nRange query\nO(log n)\nO(n)\n\n\nBuild\nO(n)\nO(n)\n\n\n\nLazy propagation, work smarter, not harder. Apply only what’s needed, when it’s needed.\n\n\n\n244 Point Update\nA point update changes a single element in the array and updates all relevant segment tree nodes along the path to the root. This operation ensures the segment tree remains consistent for future range queries.\nUnlike range updates (which mark many elements at once), point updates touch only O(log n) nodes, one per level.\n\nWhat Problem Are We Solving?\nGiven a segment tree built over A[0..n-1], we want to:\n\nChange one element: A[pos] = new_value\nReflect the change in the segment tree so all range queries stay correct\n\nGoal: Update efficiently, no full rebuild\nNaive: rebuild tree → O(n) Segment tree point update: O(log n)\n\n\nHow It Works (Plain Language)\nEvery tree node T[v] represents a segment [L, R]. When pos lies within [L, R], that node’s value might need adjustment.\nAlgorithm (recursive):\n\nIf L == R == pos, assign A[pos] = val, set T[v] = val.\nElse, find mid.\n\nRecurse into left or right child depending on pos.\nAfter child updates, recompute T[v] = merge(T[2v], T[2v+1]).\n\n\nNo lazy propagation needed, it’s a direct path down the tree.\n\n\nExample\nArray: A = [2, 1, 3, 4] Tree stores sum.\n\n\n\nNode\nRange\nValue\n\n\n\n\n1\n[0,3]\n10\n\n\n2\n[0,1]\n3\n\n\n3\n[2,3]\n7\n\n\n4\n[0,0]\n2\n\n\n5\n[1,1]\n1\n\n\n6\n[2,2]\n3\n\n\n7\n[3,3]\n4\n\n\n\nOperation: A[1] = 5\nPath: [1, 2, 5]\n\n\n\nStep\nNode\nOld Value\nNew Value\nUpdate\n\n\n\n\nLeaf\n5\n1\n5\nT[5]=5\n\n\nParent\n2\n3\n7\nT[2]=2+5=7\n\n\nRoot\n1\n10\n14\nT[1]=7+7=14\n\n\n\nNew array: [2, 5, 3, 4] New sum: 14 ✅\n\n\nStep-by-Step Trace\nupdate(v=1, L=0, R=3, pos=1, val=5)\n  mid=1\n  -&gt; pos&lt;=mid → left child (v=2)\n    update(v=2, L=0, R=1)\n      mid=0\n      -&gt; pos&gt;mid → right child (v=5)\n        update(v=5, L=1, R=1)\n        T[5]=5\n      T[2]=merge(T[4]=2, T[5]=5)=7\n  T[1]=merge(T[2]=7, T[3]=7)=14\n\n\nTiny Code (Easy Versions)\nC (Recursive, sum merge)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100000\nint A[MAXN];\nlong long T[4*MAXN];\n\nlong long merge(long long a, long long b) { return a + b; }\n\nvoid build(int v, int L, int R) {\n    if (L == R) { T[v] = A[L]; return; }\n    int mid = (L + R)/2;\n    build(2*v, L, mid);\n    build(2*v+1, mid+1, R);\n    T[v] = merge(T[2*v], T[2*v+1]);\n}\n\nvoid point_update(int v, int L, int R, int pos, int val) {\n    if (L == R) {\n        T[v] = val;\n        A[pos] = val;\n        return;\n    }\n    int mid = (L + R)/2;\n    if (pos &lt;= mid) point_update(2*v, L, mid, pos, val);\n    else point_update(2*v+1, mid+1, R, pos, val);\n    T[v] = merge(T[2*v], T[2*v+1]);\n}\n\nlong long query(int v, int L, int R, int qL, int qR) {\n    if (qR &lt; L || R &lt; qL) return 0;\n    if (qL &lt;= L && R &lt;= qR) return T[v];\n    int mid = (L + R)/2;\n    return merge(query(2*v, L, mid, qL, qR), query(2*v+1, mid+1, R, qL, qR));\n}\n\nint main(void) {\n    int n = 4;\n    int vals[] = {2,1,3,4};\n    for (int i=0;i&lt;n;i++) A[i]=vals[i];\n    build(1,0,n-1);\n    printf(\"Before: sum [0,3] = %lld\\n\", query(1,0,n-1,0,3)); // 10\n    point_update(1,0,n-1,1,5);\n    printf(\"After:  sum [0,3] = %lld\\n\", query(1,0,n-1,0,3)); // 14\n}\nPython (Iterative)\ndef build(arr):\n    n = len(arr)\n    size = 1\n    while size &lt; n: size &lt;&lt;= 1\n    T = [0]*(2*size)\n    for i in range(n):\n        T[size+i] = arr[i]\n    for v in range(size-1, 0, -1):\n        T[v] = T[2*v] + T[2*v+1]\n    return T, size\n\ndef update(T, base, pos, val):\n    v = base + pos\n    T[v] = val\n    v //= 2\n    while v &gt;= 1:\n        T[v] = T[2*v] + T[2*v+1]\n        v //= 2\n\ndef query(T, base, l, r):\n    l += base; r += base\n    res = 0\n    while l &lt;= r:\n        if l%2==1: res += T[l]; l+=1\n        if r%2==0: res += T[r]; r-=1\n        l//=2; r//=2\n    return res\n\nA = [2,1,3,4]\nT, base = build(A)\nprint(\"Before:\", query(T, base, 0, 3))  # 10\nupdate(T, base, 1, 5)\nprint(\"After:\", query(T, base, 0, 3))   # 14\n\n\nWhy It Matters\n\nFoundation for dynamic data, quick local edits\nUsed in segment trees, Fenwick trees, and BIT\nGreat for applications like dynamic scoring, cumulative sums, real-time data updates\n\n\n\nA Gentle Proof (Why It Works)\nEach level has 1 node affected by position pos. Tree height = O(log n). Thus, exactly O(log n) nodes recomputed.\n\n\nTry It Yourself\n\nBuild [2,1,3,4], update A[2]=10 → sum [0,3]=17\nReplace merge with min → update element → test queries\nCompare time with full rebuild for large n\n\n\n\nTest Cases\n\n\n\nInput A\nUpdate\nQuery\nExpected\n\n\n\n\n[2,1,3,4]\nA[1]=5\nsum[0,3]\n14\n\n\n[5,5,5]\nA[2]=2\nsum[0,2]\n12\n\n\n[1]\nA[0]=9\nsum[0,0]\n9\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nPoint update\nO(log n)\nO(1)\n\n\nQuery\nO(log n)\nO(1)\n\n\nBuild\nO(n)\nO(n)\n\n\n\nA point update is like a ripple in a pond, a single change flows upward, keeping the whole structure in harmony.\n\n\n\n245 Fenwick Tree Build\nA Fenwick Tree, or Binary Indexed Tree (BIT), is a compact data structure for prefix queries and point updates. It’s perfect for cumulative sums, frequencies, or any associative operation. Building it efficiently sets the stage for O(log n) queries and updates.\nUnlike segment trees, a Fenwick Tree uses clever index arithmetic to represent overlapping ranges in a single array.\n\nWhat Problem Are We Solving?\nWe want to precompute a structure to:\n\nAnswer prefix queries: sum(0..i)\nSupport updates: A[i] += delta\n\nNaive prefix sum array:\n\nQuery: O(1)\nUpdate: O(n)\n\nFenwick Tree:\n\nQuery: O(log n)\nUpdate: O(log n)\nBuild: O(n)\n\n\n\nHow It Works (Plain Language)\nA Fenwick Tree uses the last set bit (LSB) of an index to determine segment length.\nEach node BIT[i] stores sum of range:\n(i - LSB(i) + 1) .. i\nSo:\n\nBIT[1] stores A[1]\nBIT[2] stores A[1..2]\nBIT[3] stores A[3]\nBIT[4] stores A[1..4]\nBIT[5] stores A[5]\nBIT[6] stores A[5..6]\n\nPrefix sum = sum of these overlapping ranges.\n\n\nExample\nArray A = [2, 1, 3, 4, 5] (1-indexed for clarity)\n\n\n\ni\nA[i]\nLSB(i)\nRange Stored\nBIT[i] (sum)\n\n\n\n\n1\n2\n1\n[1]\n2\n\n\n2\n1\n2\n[1..2]\n3\n\n\n3\n3\n1\n[3]\n3\n\n\n4\n4\n4\n[1..4]\n10\n\n\n5\n5\n1\n[5]\n5\n\n\n\n\n\nStep-by-Step Build (O(n))\nIterate i from 1 to n:\n\nBIT[i] += A[i]\nAdd BIT[i] to its parent: BIT[i + LSB(i)] += BIT[i]\n\n\n\n\nStep\ni\nLSB(i)\nUpdate BIT[i+LSB(i)]\nResult BIT\n\n\n\n\n1\n1\n1\nBIT[2]+=2\n[2,3,0,0,0]\n\n\n2\n2\n2\nBIT[4]+=3\n[2,3,0,3,0]\n\n\n3\n3\n1\nBIT[4]+=3\n[2,3,3,6,0]\n\n\n4\n4\n4\nBIT[8]+=6 (ignore, out of range)\n[2,3,3,6,0]\n\n\n5\n5\n1\nBIT[6]+=5 (ignore, out of range)\n[2,3,3,6,5]\n\n\n\nBuilt BIT: [2, 3, 3, 6, 5] ✅\n\n\nTiny Code (Easy Versions)\nC (O(n) Build)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100005\nint A[MAXN];\nlong long BIT[MAXN];\nint n;\n\nint lsb(int i) { return i & -i; }\n\nvoid build() {\n    for (int i = 1; i &lt;= n; i++) {\n        BIT[i] += A[i];\n        int parent = i + lsb(i);\n        if (parent &lt;= n)\n            BIT[parent] += BIT[i];\n    }\n}\n\nlong long prefix_sum(int i) {\n    long long s = 0;\n    while (i &gt; 0) {\n        s += BIT[i];\n        i -= lsb(i);\n    }\n    return s;\n}\n\nint main(void) {\n    n = 5;\n    int vals[6] = {0, 2, 1, 3, 4, 5}; // 1-indexed\n    for (int i=1;i&lt;=n;i++) A[i]=vals[i];\n    build();\n    printf(\"Prefix sum [1..3] = %lld\\n\", prefix_sum(3)); // expect 6\n}\nPython (1-indexed)\ndef build(A):\n    n = len(A) - 1\n    BIT = [0]*(n+1)\n    for i in range(1, n+1):\n        BIT[i] += A[i]\n        parent = i + (i & -i)\n        if parent &lt;= n:\n            BIT[parent] += BIT[i]\n    return BIT\n\ndef prefix_sum(BIT, i):\n    s = 0\n    while i &gt; 0:\n        s += BIT[i]\n        i -= (i & -i)\n    return s\n\nA = [0,2,1,3,4,5]  # 1-indexed\nBIT = build(A)\nprint(\"BIT =\", BIT[1:])\nprint(\"Sum[1..3] =\", prefix_sum(BIT, 3))  # expect 6\n\n\nWhy It Matters\n\nLightweight alternative to segment tree\nO(n) build, O(log n) query, O(log n) update\nUsed in frequency tables, inversion count, prefix queries, cumulative histograms\n\n\n\nA Gentle Proof (Why It Works)\nEach index i contributes to at most log n BIT entries. Every BIT[i] stores sum of a disjoint range defined by LSB. Building with parent propagation ensures correct overlapping coverage.\n\n\nTry It Yourself\n\nBuild BIT from [2, 1, 3, 4, 5] → query sum(3)=6\nUpdate A[2]+=2 → prefix(3)=8\nCompare with cumulative sum array for correctness\n\n\n\nTest Cases\n\n\n\nA (1-indexed)\nQuery\nExpected\n\n\n\n\n[2,1,3,4,5]\nprefix(3)\n6\n\n\n[5,5,5,5]\nprefix(4)\n20\n\n\n[1]\nprefix(1)\n1\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\nO(n)\nO(n)\n\n\nQuery\nO(log n)\nO(1)\n\n\nUpdate\nO(log n)\nO(1)\n\n\n\nA Fenwick Tree is the art of doing less, storing just enough to make prefix sums lightning-fast.\n\n\n\n246 Fenwick Update\nA Fenwick Tree (Binary Indexed Tree) supports point updates, adjusting a single element and efficiently reflecting that change across all relevant cumulative sums. The update propagates upward through parent indices determined by the Least Significant Bit (LSB).\n\nWhat Problem Are We Solving?\nGiven a built Fenwick Tree, we want to perform an operation like:\nA[pos] += delta\nand keep all prefix sums sum(0..i) consistent.\nNaive approach: update every prefix → O(n) Fenwick Tree: propagate through selected indices → O(log n)\n\n\nHow It Works (Plain Language)\nIn a Fenwick Tree, BIT[i] covers the range (i - LSB(i) + 1) .. i. So when we update A[pos], we must add delta to all BIT[i] where i includes pos in its range.\nRule:\nfor (i = pos; i &lt;= n; i += LSB(i))\n    BIT[i] += delta\n\nLSB(i) jumps to the next index covering pos\nStops when index exceeds n\n\n\n\nExample\nArray A = [2, 1, 3, 4, 5] (1-indexed) BIT built as [2, 3, 3, 10, 5]\nNow perform update(2, +2) → A[2] = 3\n\n\n\nStep\ni\nLSB(i)\nBIT[i] Change\nNew BIT\n\n\n\n\n1\n2\n2\nBIT[2]+=2\n[2,5,3,10,5]\n\n\n2\n4\n4\nBIT[4]+=2\n[2,5,3,12,5]\n\n\n3\n8\nstop\n-\ndone\n\n\n\nNew prefix sums reflect updated array [2,3,3,4,5]\nCheck prefix(3) = 2+3+3=8 ✅\n\n\nStep-by-Step Table\n\n\n\nPrefix\nOld Sum\nNew Sum\n\n\n\n\n1\n2\n2\n\n\n2\n3\n5\n\n\n3\n6\n8\n\n\n4\n10\n12\n\n\n5\n15\n17\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Fenwick Update)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100005\nlong long BIT[MAXN];\nint n;\n\nint lsb(int i) { return i & -i; }\n\nvoid update(int pos, int delta) {\n    for (int i = pos; i &lt;= n; i += lsb(i))\n        BIT[i] += delta;\n}\n\nlong long prefix_sum(int i) {\n    long long s = 0;\n    for (; i &gt; 0; i -= lsb(i))\n        s += BIT[i];\n    return s;\n}\n\nint main(void) {\n    n = 5;\n    // Build from A = [2, 1, 3, 4, 5]\n    BIT[1]=2; BIT[2]=3; BIT[3]=3; BIT[4]=10; BIT[5]=5;\n    update(2, 2); // A[2]+=2\n    printf(\"Sum [1..3] = %lld\\n\", prefix_sum(3)); // expect 8\n}\nPython (1-indexed)\ndef update(BIT, n, pos, delta):\n    while pos &lt;= n:\n        BIT[pos] += delta\n        pos += (pos & -pos)\n\ndef prefix_sum(BIT, pos):\n    s = 0\n    while pos &gt; 0:\n        s += BIT[pos]\n        pos -= (pos & -pos)\n    return s\n\n# Example\nBIT = [0,2,3,3,10,5]  # built from [2,1,3,4,5]\nupdate(BIT, 5, 2, 2)\nprint(\"Sum [1..3] =\", prefix_sum(BIT, 3))  # expect 8\n\n\nWhy It Matters\n\nEnables real-time data updates with fast prefix queries\nSimpler and more space-efficient than segment trees for sums\nCore of many algorithms: inversion count, frequency accumulation, order statistics\n\n\n\nA Gentle Proof (Why It Works)\nEach BIT[i] covers a fixed range (i - LSB(i) + 1 .. i). If pos lies in that range, incrementing BIT[i] ensures correct future prefix sums. Since each update moves by LSB(i), at most log₂(n) steps occur.\n\n\nTry It Yourself\n\nBuild BIT from [2,1,3,4,5] → update(2,+2)\nQuery prefix(3) → expect 8\nUpdate(5, +5) → prefix(5) = 22\nChain multiple updates → verify incremental sums\n\n\n\nTest Cases\n\n\n\nA (1-indexed)\nUpdate\nQuery\nExpected\n\n\n\n\n[2,1,3,4,5]\n(2,+2)\nsum[1..3]\n8\n\n\n[5,5,5,5]\n(4,+1)\nsum[1..4]\n21\n\n\n[1,2,3,4,5]\n(5,-2)\nsum[1..5]\n13\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nUpdate\nO(log n)\nO(1)\n\n\nQuery\nO(log n)\nO(1)\n\n\nBuild\nO(n)\nO(n)\n\n\n\nEach update is a ripple that climbs the tree, keeping all prefix sums in perfect sync.\n\n\n\n247 Fenwick Query\nOnce you’ve built or updated a Fenwick Tree (Binary Indexed Tree), you’ll want to extract useful information, usually prefix sums. The query operation walks downward through the tree using index arithmetic, gathering partial sums along the way.\nThis simple yet powerful routine turns a linear prefix-sum scan into an elegant O(log n) solution.\n\nWhat Problem Are We Solving?\nWe need to compute:\nsum(1..i) = A[1] + A[2] + ... + A[i]\nefficiently, after updates.\nIn a Fenwick Tree, each index holds the sum of a segment determined by its LSB (Least Significant Bit). By moving downward (subtracting LSB each step), we collect disjoint ranges that together cover [1..i].\n\n\nHow It Works (Plain Language)\nEach BIT[i] stores sum of range (i - LSB(i) + 1 .. i). So to get the prefix sum, we combine all these segments by walking downward:\nsum = 0\nwhile i &gt; 0:\n    sum += BIT[i]\n    i -= LSB(i)\nKey Insight:\n\nUpdate: moves upward (i += LSB(i))\nQuery: moves downward (i -= LSB(i))\n\nTogether, they mirror each other like yin and yang of cumulative logic.\n\n\nExample\nSuppose we have A = [2, 3, 3, 4, 5] (1-indexed), Built BIT = [2, 5, 3, 12, 5]\nLet’s compute prefix_sum(5)\n\n\n\nStep\ni\nBIT[i]\nLSB(i)\nAccumulated Sum\nExplanation\n\n\n\n\n1\n5\n5\n1\n5\nAdd BIT[5] (A[5])\n\n\n2\n4\n12\n4\n17\nAdd BIT[4] (A[1..4])\n\n\n3\n0\n-\n-\nstop\ndone\n\n\n\n✅ prefix_sum(5) = 17 matches 2+3+3+4+5=17\nNow try prefix_sum(3)\n\n\n\nStep\ni\nBIT[i]\nLSB(i)\nSum\n\n\n\n\n1\n3\n3\n1\n3\n\n\n2\n2\n5\n2\n8\n\n\n3\n0\n-\n-\nstop\n\n\n\n✅ prefix_sum(3) = 8\n\n\nTiny Code (Easy Versions)\nC (Fenwick Query)\n#include &lt;stdio.h&gt;\n\n#define MAXN 100005\nlong long BIT[MAXN];\nint n;\n\nint lsb(int i) { return i & -i; }\n\nlong long prefix_sum(int i) {\n    long long s = 0;\n    while (i &gt; 0) {\n        s += BIT[i];\n        i -= lsb(i);\n    }\n    return s;\n}\n\nint main(void) {\n    n = 5;\n    long long B[6] = {0,2,5,3,12,5}; // built BIT\n    for (int i=1;i&lt;=n;i++) BIT[i] = B[i];\n    printf(\"Prefix sum [1..3] = %lld\\n\", prefix_sum(3)); // expect 8\n    printf(\"Prefix sum [1..5] = %lld\\n\", prefix_sum(5)); // expect 17\n}\nPython (1-indexed)\ndef prefix_sum(BIT, i):\n    s = 0\n    while i &gt; 0:\n        s += BIT[i]\n        i -= (i & -i)\n    return s\n\nBIT = [0,2,5,3,12,5]\nprint(\"sum[1..3] =\", prefix_sum(BIT, 3))  # 8\nprint(\"sum[1..5] =\", prefix_sum(BIT, 5))  # 17\n\n\nWhy It Matters\n\nFast prefix sums after dynamic updates\nEssential for frequency tables, order statistics, inversion count\nCore to many competitive programming tricks (like “count less than k”)\n\n\n\nA Gentle Proof (Why It Works)\nEach index i contributes to a fixed set of BIT nodes. When querying, we collect all BIT segments that together form [1..i]. The LSB ensures no overlap, each range is disjoint. Total steps = number of bits set in i = O(log n).\n\n\nTry It Yourself\n\nBuild BIT from [2,3,3,4,5]\nQuery prefix_sum(3) → expect 8\nUpdate(2,+2), query(3) → expect 10\nQuery prefix_sum(5) → confirm correctness\n\n\n\nTest Cases\n\n\n\nA (1-indexed)\nQuery\nExpected\n\n\n\n\n[2,3,3,4,5]\nsum(3)\n8\n\n\n[2,3,3,4,5]\nsum(5)\n17\n\n\n[1,2,3,4,5]\nsum(4)\n10\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nQuery\nO(log n)\nO(1)\n\n\nUpdate\nO(log n)\nO(1)\n\n\nBuild\nO(n)\nO(n)\n\n\n\nFenwick query is the graceful descent, follow the bits down to gather every piece of the sum puzzle.\n\n\n\n248 Segment Tree Merge\nA Segment Tree can handle more than sums, it’s a versatile structure that can merge results from two halves of a range using any associative operation (sum, min, max, gcd, etc.). The merge function is the heart of the segment tree: it tells us how to combine child nodes into a parent node.\n\nWhat Problem Are We Solving?\nWe want to combine results from left and right child intervals into one parent result. Without merge logic, the tree can’t aggregate data or answer queries.\nFor example:\n\nFor sum queries → merge(a, b) = a + b\nFor min queries → merge(a, b) = min(a, b)\nFor gcd queries → merge(a, b) = gcd(a, b)\n\nSo the merge function defines what the tree means.\n\n\nHow It Works (Plain Language)\nEach node represents a segment [L, R]. Its value is derived from its two children:\nnode = merge(left_child, right_child)\nWhen you:\n\nBuild: compute node from children recursively\nQuery: merge partial overlaps\nUpdate: recompute affected nodes using merge\n\nSo merge is the unifying rule that glues the tree together.\n\n\nExample (Sum Segment Tree)\nArray A = [2, 1, 3, 4]\n\n\n\nNode\nRange\nLeft\nRight\nMerge (sum)\n\n\n\n\nroot\n[1..4]\n6\n4\n10\n\n\nleft\n[1..2]\n2\n1\n3\n\n\nright\n[3..4]\n3\n4\n7\n\n\n\nEach parent = sum(left, right)\nTree structure:\n          [1..4]=10\n         /         \\\n   [1..2]=3       [3..4]=7\n   /    \\         /     \\\n$$1]=2 [2]=1   [3]=3   [4]=4\nMerge rule: merge(a, b) = a + b\n\n\nExample (Min Segment Tree)\nArray A = [5, 2, 7, 1] Merge rule: min(a, b)\n\n\n\nNode\nRange\nLeft\nRight\nMerge (min)\n\n\n\n\nroot\n[1..4]\n2\n1\n1\n\n\nleft\n[1..2]\n5\n2\n2\n\n\nright\n[3..4]\n7\n1\n1\n\n\n\nResult: root stores 1, the global minimum.\n\n\nTiny Code (Easy Versions)\nC (Sum Segment Tree Merge)\n#include &lt;stdio.h&gt;\n#define MAXN 100005\n\nint A[MAXN], tree[4*MAXN];\nint n;\n\nint merge(int left, int right) {\n    return left + right;\n}\n\nvoid build(int node, int l, int r) {\n    if (l == r) {\n        tree[node] = A[l];\n        return;\n    }\n    int mid = (l + r) / 2;\n    build(node*2, l, mid);\n    build(node*2+1, mid+1, r);\n    tree[node] = merge(tree[node*2], tree[node*2+1]);\n}\n\nint query(int node, int l, int r, int ql, int qr) {\n    if (qr &lt; l || ql &gt; r) return 0; // neutral element\n    if (ql &lt;= l && r &lt;= qr) return tree[node];\n    int mid = (l + r) / 2;\n    int left = query(node*2, l, mid, ql, qr);\n    int right = query(node*2+1, mid+1, r, ql, qr);\n    return merge(left, right);\n}\n\nint main(void) {\n    n = 4;\n    int vals[5] = {0, 2, 1, 3, 4};\n    for (int i=1; i&lt;=n; i++) A[i] = vals[i];\n    build(1,1,n);\n    printf(\"Sum [1..4] = %d\\n\", query(1,1,n,1,4)); // 10\n    printf(\"Sum [2..3] = %d\\n\", query(1,1,n,2,3)); // 4\n}\nPython\ndef merge(a, b):\n    return a + b  # define operation here\n\ndef build(arr, tree, node, l, r):\n    if l == r:\n        tree[node] = arr[l]\n        return\n    mid = (l + r) // 2\n    build(arr, tree, 2*node, l, mid)\n    build(arr, tree, 2*node+1, mid+1, r)\n    tree[node] = merge(tree[2*node], tree[2*node+1])\n\ndef query(tree, node, l, r, ql, qr):\n    if qr &lt; l or ql &gt; r:\n        return 0  # neutral element for sum\n    if ql &lt;= l and r &lt;= qr:\n        return tree[node]\n    mid = (l + r)//2\n    left = query(tree, 2*node, l, mid, ql, qr)\n    right = query(tree, 2*node+1, mid+1, r, ql, qr)\n    return merge(left, right)\n\nA = [0, 2, 1, 3, 4]\nn = 4\ntree = [0]*(4*n)\nbuild(A, tree, 1, 1, n)\nprint(\"Sum[1..4] =\", query(tree, 1, 1, n, 1, 4))  # 10\n\n\nWhy It Matters\n\nMerge is the “soul” of the segment tree, define merge, and you define the tree’s purpose.\nFlexible across many tasks: sums, min/max, GCD, XOR, matrix multiplication.\nUnified pattern: build, query, update all rely on the same operation.\n\n\n\nA Gentle Proof (Why It Works)\nSegment tree works by divide and conquer. If an operation is associative (like +, min, max, gcd), merging partial results yields the same answer as computing on the whole range. So as long as merge(a, b) = merge(b, a) and associative, correctness follows.\n\n\nTry It Yourself\n\nReplace merge with min(a,b) → build min segment tree\nReplace merge with max(a,b) → build max segment tree\nReplace merge with __gcd(a,b) → build gcd tree\nTest query(2,3) after changing merge rule\n\n\n\nTest Cases\n\n\n\nA\nOperation\nQuery\nExpected\n\n\n\n\n[2,1,3,4]\nsum\n[1..4]\n10\n\n\n[5,2,7,1]\nmin\n[1..4]\n1\n\n\n[3,6,9,12]\ngcd\n[2..4]\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\nO(n)\nO(n)\n\n\nQuery\nO(log n)\nO(n)\n\n\nUpdate\nO(log n)\nO(n)\n\n\n\nA merge function is the heartbeat of every segment tree, once you define it, your tree learns what “combine” means.\n\n\n\n249 Persistent Segment Tree\nA Persistent Segment Tree is a magical structure that lets you time-travel. Instead of overwriting nodes on update, it creates new versions while preserving old ones. Each update returns a new root, giving you full history and O(log n) access to every version.\n\nWhat Problem Are We Solving?\nWe need a data structure that supports:\n\nPoint updates without losing past state\nRange queries on any historical version\n\nUse cases:\n\nUndo/rollback systems\nVersioned databases\nOffline queries (“What was sum[1..3] after the 2nd update?”)\n\nEach version is immutable, perfect for functional programming or auditability.\n\n\nHow It Works (Plain Language)\nA persistent segment tree clones only nodes along the path from root to updated leaf. All other nodes are shared.\nFor each update:\n\nCreate a new root.\nCopy nodes along the path to the changed index.\nReuse unchanged subtrees.\n\nResult: O(log n) memory per version, not O(n).\n\n\nExample\nArray A = [1, 2, 3, 4]\nVersion 0: built tree for [1, 2, 3, 4]\n\nsum = 10\n\nUpdate A[2] = 5\n\nCreate Version 1, copy path to A[2]\nNew sum = 1 + 5 + 3 + 4 = 13\n\n\n\n\nVersion\nA\nSum(1..4)\nSum(1..2)\n\n\n\n\n0\n[1,2,3,4]\n10\n3\n\n\n1\n[1,5,3,4]\n13\n6\n\n\n\nBoth versions exist side by side. Version 0 is unchanged. Version 1 reflects new value.\n\n\nVisualization\nVersion 0: root0\n          /       \\\n      [1..2]=3   [3..4]=7\n      /    \\      /     \\\n   [1]=1 [2]=2 [3]=3 [4]=4\n\nVersion 1: root1 (new)\n          /       \\\n  [1..2]=6*       [3..4]=7 (shared)\n    /     \\\n$$1]=1   [2]=5*\nAsterisks mark newly created nodes.\n\n\nTiny Code (Easy Version)\nC (Pointer-based, sum tree)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int val;\n    struct Node *left, *right;\n} Node;\n\nNode* build(int arr[], int l, int r) {\n    Node* node = malloc(sizeof(Node));\n    if (l == r) {\n        node-&gt;val = arr[l];\n        node-&gt;left = node-&gt;right = NULL;\n        return node;\n    }\n    int mid = (l + r) / 2;\n    node-&gt;left = build(arr, l, mid);\n    node-&gt;right = build(arr, mid+1, r);\n    node-&gt;val = node-&gt;left-&gt;val + node-&gt;right-&gt;val;\n    return node;\n}\n\nNode* update(Node* prev, int l, int r, int pos, int new_val) {\n    Node* node = malloc(sizeof(Node));\n    if (l == r) {\n        node-&gt;val = new_val;\n        node-&gt;left = node-&gt;right = NULL;\n        return node;\n    }\n    int mid = (l + r) / 2;\n    if (pos &lt;= mid) {\n        node-&gt;left = update(prev-&gt;left, l, mid, pos, new_val);\n        node-&gt;right = prev-&gt;right;\n    } else {\n        node-&gt;left = prev-&gt;left;\n        node-&gt;right = update(prev-&gt;right, mid+1, r, pos, new_val);\n    }\n    node-&gt;val = node-&gt;left-&gt;val + node-&gt;right-&gt;val;\n    return node;\n}\n\nint query(Node* node, int l, int r, int ql, int qr) {\n    if (qr &lt; l || ql &gt; r) return 0;\n    if (ql &lt;= l && r &lt;= qr) return node-&gt;val;\n    int mid = (l + r)/2;\n    return query(node-&gt;left, l, mid, ql, qr)\n         + query(node-&gt;right, mid+1, r, ql, qr);\n}\n\nint main(void) {\n    int A[5] = {0,1,2,3,4}; // 1-indexed\n    Node* root0 = build(A, 1, 4);\n    Node* root1 = update(root0, 1, 4, 2, 5);\n    printf(\"v0 sum[1..2]=%d\\n\", query(root0,1,4,1,2)); // 3\n    printf(\"v1 sum[1..2]=%d\\n\", query(root1,1,4,1,2)); // 6\n}\nPython (Recursive, sum tree)\nclass Node:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef build(arr, l, r):\n    if l == r:\n        return Node(arr[l])\n    mid = (l + r) // 2\n    left = build(arr, l, mid)\n    right = build(arr, mid+1, r)\n    return Node(left.val + right.val, left, right)\n\ndef update(prev, l, r, pos, val):\n    if l == r:\n        return Node(val)\n    mid = (l + r) // 2\n    if pos &lt;= mid:\n        return Node(prev.val - prev.left.val + val,\n                    update(prev.left, l, mid, pos, val),\n                    prev.right)\n    else:\n        return Node(prev.val - prev.right.val + val,\n                    prev.left,\n                    update(prev.right, mid+1, r, pos, val))\n\ndef query(node, l, r, ql, qr):\n    if qr &lt; l or ql &gt; r: return 0\n    if ql &lt;= l and r &lt;= qr: return node.val\n    mid = (l + r)//2\n    return query(node.left, l, mid, ql, qr) + query(node.right, mid+1, r, ql, qr)\n\nA = [0,1,2,3,4]\nroot0 = build(A, 1, 4)\nroot1 = update(root0, 1, 4, 2, 5)\nprint(\"v0 sum[1..2] =\", query(root0, 1, 4, 1, 2))  # 3\nprint(\"v1 sum[1..2] =\", query(root1, 1, 4, 1, 2))  # 6\n\n\nWhy It Matters\n\nImmutable versions → ideal for undo systems, snapshots, persistent databases\nSaves memory (O(log n) per version)\nEach version is fully functional and independent\n\n\n\nA Gentle Proof (Why It Works)\nEach update affects O(log n) nodes. By copying only those nodes, we maintain O(log n) new memory. Because all old nodes remain referenced, no data is lost. Thus, each version is consistent and immutable.\n\n\nTry It Yourself\n\nBuild version 0 with [1,2,3,4]\nUpdate(2,5) → version 1\nQuery sum(1,4) in both versions → 10, 13\nCreate version 2 by update(3,1)\nQuery sum(1,3) in each version\n\n\n\nTest Cases\n\n\n\nVersion\nA\nQuery\nExpected\n\n\n\n\n0\n[1,2,3,4]\nsum[1..2]\n3\n\n\n1\n[1,5,3,4]\nsum[1..2]\n6\n\n\n2\n[1,5,1,4]\nsum[1..3]\n7\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace (per version)\n\n\n\n\nBuild\nO(n)\nO(n)\n\n\nUpdate\nO(log n)\nO(log n)\n\n\nQuery\nO(log n)\nO(1)\n\n\n\nEach version of a persistent segment tree is a snapshot in time, perfect memory of every past, with no cost of forgetting.\n\n\n\n250 2D Segment Tree\nA 2D Segment Tree extends the classic segment tree into two dimensions, perfect for handling range queries over matrices, such as sums, minimums, or maximums across subrectangles.\nIt’s the data structure that lets you ask:\n\n“What’s the sum of elements in the rectangle (x1, y1) to (x2, y2)?” and still answer in O(log² n) time.\n\n\nWhat Problem Are We Solving?\nWe want to perform two operations efficiently on a 2D grid:\n\nRange Query: sum/min/max over a submatrix\nPoint Update: modify one element and reflect the change\n\nNaive approach → O(n²) per query 2D Segment Tree → O(log² n) per query and update\n\n\nHow It Works (Plain Language)\nThink of a 2D segment tree as a segment tree of segment trees.\n\nOuter tree partitions rows.\nEach node of the outer tree holds an inner segment tree for its rows.\n\nEach node thus represents a rectangle in the matrix.\nAt build time:\n\nCombine children horizontally and vertically using a merge rule (like sum).\n\nAt query time:\n\nCombine answers from nodes overlapping the query rectangle.\n\n\n\nExample\nMatrix A (3×3):\n\n\n\n\ny=1\ny=2\ny=3\n\n\n\n\nx=1\n2\n1\n3\n\n\nx=2\n4\n5\n6\n\n\nx=3\n7\n8\n9\n\n\n\nQuery rectangle (1,1) to (2,2) → 2 + 1 + 4 + 5 = 12\n\n\nKey Idea\nFor each row range, build a column segment tree. For each parent node (row range), merge children column trees:\ntree[x][y] = merge(tree[2*x][y], tree[2*x+1][y])\n\n\nExample Walkthrough (Sum Tree)\n\nBuild row segment tree\nInside each row node, build column segment tree\nTo query rectangle (x1,y1) to (x2,y2):\n\nQuery over x-range → merge vertical results\nInside each x-node, query y-range → merge horizontal results\n\n\n\n\nTiny Code (Easy Version)\nPython (Sum 2D Segment Tree)\nclass SegmentTree2D:\n    def __init__(self, mat):\n        self.n = len(mat)\n        self.m = len(mat[0])\n        self.tree = [[0]*(4*self.m) for _ in range(4*self.n)]\n        self.mat = mat\n        self.build_x(1, 0, self.n-1)\n\n    def merge(self, a, b):\n        return a + b  # sum merge\n\n    # build column tree for a fixed row range\n    def build_y(self, nodex, lx, rx, nodey, ly, ry):\n        if ly == ry:\n            if lx == rx:\n                self.tree[nodex][nodey] = self.mat[lx][ly]\n            else:\n                self.tree[nodex][nodey] = self.merge(\n                    self.tree[2*nodex][nodey], self.tree[2*nodex+1][nodey]\n                )\n            return\n        midy = (ly + ry)//2\n        self.build_y(nodex, lx, rx, 2*nodey, ly, midy)\n        self.build_y(nodex, lx, rx, 2*nodey+1, midy+1, ry)\n        self.tree[nodex][nodey] = self.merge(\n            self.tree[nodex][2*nodey], self.tree[nodex][2*nodey+1]\n        )\n\n    # build row tree\n    def build_x(self, nodex, lx, rx):\n        if lx != rx:\n            midx = (lx + rx)//2\n            self.build_x(2*nodex, lx, midx)\n            self.build_x(2*nodex+1, midx+1, rx)\n        self.build_y(nodex, lx, rx, 1, 0, self.m-1)\n\n    def query_y(self, nodex, nodey, ly, ry, qly, qry):\n        if qry &lt; ly or qly &gt; ry: return 0\n        if qly &lt;= ly and ry &lt;= qry:\n            return self.tree[nodex][nodey]\n        midy = (ly + ry)//2\n        return self.merge(\n            self.query_y(nodex, 2*nodey, ly, midy, qly, qry),\n            self.query_y(nodex, 2*nodey+1, midy+1, ry, qly, qry)\n        )\n\n    def query_x(self, nodex, lx, rx, qlx, qrx, qly, qry):\n        if qrx &lt; lx or qlx &gt; rx: return 0\n        if qlx &lt;= lx and rx &lt;= qrx:\n            return self.query_y(nodex, 1, 0, self.m-1, qly, qry)\n        midx = (lx + rx)//2\n        return self.merge(\n            self.query_x(2*nodex, lx, midx, qlx, qrx, qly, qry),\n            self.query_x(2*nodex+1, midx+1, rx, qlx, qrx, qly, qry)\n        )\n\n    def query(self, x1, y1, x2, y2):\n        return self.query_x(1, 0, self.n-1, x1, x2, y1, y2)\n\n# Example\nA = [\n    [2, 1, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n$$\nseg2d = SegmentTree2D(A)\nprint(seg2d.query(0, 0, 1, 1))  # expect 12\n\n\nWhy It Matters\n\nSupports 2D queries with log² complexity\nWorks for sum, min, max, gcd, XOR\nA foundation for advanced 2D data structures (Fenwick 2D, KD-trees, etc.)\n\n\n\nA Gentle Proof (Why It Works)\nSegment tree correctness relies on associativity. In 2D, we extend this property across both dimensions. Each node represents a rectangular region; merging children yields correct aggregate for parent.\n\n\nTry It Yourself\n\nBuild from 3×3 matrix\nQuery (0,0)-(2,2) → full sum\nQuery (1,1)-(2,2) → bottom-right 5+6+8+9=28\nModify A[1][2]=10, rebuild, recheck sum\n\n\n\nTest Cases\n\n\n\nMatrix\nQuery\nResult\n\n\n\n\n[[2,1,3],[4,5,6],[7,8,9]]\n(0,0)-(1,1)\n12\n\n\n[[2,1,3],[4,5,6],[7,8,9]]\n(1,1)-(2,2)\n28\n\n\n[[1,2],[3,4]]\n(0,0)-(1,1)\n10\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\nO(n·m·log n·log m)\nO(n·m)\n\n\nQuery\nO(log² n)\nO(1)\n\n\nUpdate\nO(log² n)\nO(1)\n\n\n\nA 2D segment tree is your grid-wise superpower, merging rectangles like puzzle pieces, one log at a time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-26.-disjoint-set-union",
    "href": "books/en-us/list-3.html#section-26.-disjoint-set-union",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 26. Disjoint Set Union",
    "text": "Section 26. Disjoint Set Union\n\n251 Make-Set\nThe Make-Set operation is the starting point of the Disjoint Set Union (DSU), also known as Union-Find, a fundamental data structure for managing partitions of elements into disjoint sets.\nEach element begins in its own set, serving as its own parent. Later operations (Find and Union) will merge and track relationships among them efficiently.\n\nWhat Problem Are We Solving?\nWe need a way to represent a collection of disjoint sets, groups that don’t overlap, while supporting these operations:\n\nMake-Set(x): create a new set containing only x\nFind(x): find representative (leader) of x’s set\nUnion(x, y): merge sets containing x and y\n\nThis structure is the backbone of many graph algorithms like Kruskal’s MST, connected components, and clustering.\n\n\nHow It Works (Plain Language)\nInitially, each element is its own parent, a self-loop. We store two arrays (or maps):\n\nparent[x] → points to x’s parent (initially itself)\nrank[x] or size[x] → helps balance unions later\n\nSo:\nparent[x] = x  \nrank[x] = 0\nEach element is an isolated tree. Over time, unions connect trees.\n\n\nExample\nInitialize n = 5 elements: {1, 2, 3, 4, 5}\n\n\n\nx\nparent[x]\nrank[x]\nMeaning\n\n\n\n\n1\n1\n0\nown leader\n\n\n2\n2\n0\nown leader\n\n\n3\n3\n0\nown leader\n\n\n4\n4\n0\nown leader\n\n\n5\n5\n0\nown leader\n\n\n\nAfter all Make-Set, each element is in its own group:\n{1}, {2}, {3}, {4}, {5}\n\n\nVisualization\n1   2   3   4   5\n↑   ↑   ↑   ↑   ↑\n|   |   |   |   |\nself self self self self\nEach node points to itself, five separate trees.\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n\n#define MAXN 1000\n\nint parent[MAXN];\nint rank_[MAXN];\n\nvoid make_set(int v) {\n    parent[v] = v;   // self parent\n    rank_[v] = 0;    // initial rank\n}\n\nint main(void) {\n    int n = 5;\n    for (int i = 1; i &lt;= n; i++)\n        make_set(i);\n    \n    printf(\"Initial sets:\\n\");\n    for (int i = 1; i &lt;= n; i++)\n        printf(\"Element %d: parent=%d, rank=%d\\n\", i, parent[i], rank_[i]);\n    return 0;\n}\nPython Implementation\ndef make_set(x, parent, rank):\n    parent[x] = x\n    rank[x] = 0\n\nn = 5\nparent = {}\nrank = {}\n\nfor i in range(1, n+1):\n    make_set(i, parent, rank)\n\nprint(\"Parent:\", parent)\nprint(\"Rank:\", rank)\n# Parent: {1:1, 2:2, 3:3, 4:4, 5:5}\n\n\nWhy It Matters\n\nThe foundation of Disjoint Set Union\nEnables efficient graph algorithms\nA building block for Union by Rank and Path Compression\nEvery DSU begins with Make-Set\n\n\n\nA Gentle Proof (Why It Works)\nEach element begins as a singleton. Since no pointers cross between elements, sets are disjoint. Subsequent Union operations preserve this property by merging trees, never duplicating nodes.\nThus, Make-Set guarantees:\n\nEach new node is independent\nParent pointers form valid forests\n\n\n\nTry It Yourself\n\nInitialize {1..5} with Make-Set\nPrint parent and rank arrays\nAdd Union(1,2) and check parent changes\nVerify all parent[x] = x before unions\n\n\n\nTest Cases\n\n\n\nInput\nOperation\nExpected Output\n\n\n\n\nn=3\nMake-Set(1..3)\nparent = [1,2,3]\n\n\nn=5\nMake-Set(1..5)\nrank = [0,0,0,0,0]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nMake-Set\nO(1)\nO(1)\n\n\nFind\nO(α(n)) with compression\nO(1)\n\n\nUnion\nO(α(n)) with rank\nO(1)\n\n\n\nThe Make-Set step is your first move in the DSU dance, simple, constant time, and crucial for what follows.\n\n\n\n252 Find\nThe Find operation is the heart of the Disjoint Set Union (DSU), also known as Union-Find. It locates the representative (leader) of the set containing a given element. Every element in the same set shares the same leader, this is how DSU identifies which elements belong together.\nTo make lookups efficient, Find uses a clever optimization called Path Compression, which flattens the structure of the tree so future queries become nearly constant time.\n\nWhat Problem Are We Solving?\nGiven an element x, we want to determine which set it belongs to. Each set is represented by a root node (the leader).\nWe maintain a parent[] array such that:\n\nparent[x] = x if x is the root (leader)\notherwise parent[x] = parent of x\n\nThe Find(x) operation recursively follows parent[x] pointers until it reaches the root.\n\n\nHow It Works (Plain Language)\nThink of each set as a tree, where the root is the representative.\nFor example:\n1 ← 2 ← 3    4 ← 5\nmeans {1,2,3} is one set, {4,5} is another. The Find(3) operation follows 3→2→1, discovering 1 is the root.\nPath Compression flattens the tree by pointing every node directly to the root, reducing depth and speeding up future finds.\nAfter compression:\n1 ← 2   1 ← 3\n4 ← 5\nNow Find(3) is O(1).\n\n\nExample\nStart:\n\n\n\nx\nparent[x]\nrank[x]\n\n\n\n\n1\n1\n1\n\n\n2\n1\n0\n\n\n3\n2\n0\n\n\n4\n4\n0\n\n\n5\n4\n0\n\n\n\nPerform Find(3):\n\n3 → parent[3] = 2\n2 → parent[2] = 1\n1 → root found\n\nPath compression rewires parent[3] = 1\nResult:\n\n\n\nx\nparent[x]\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n1\n\n\n4\n4\n\n\n5\n4\n\n\n\n\n\nVisualization\nBefore compression:\n1  \n↑  \n2  \n↑  \n3\nAfter compression:\n1  \n↑ ↑  \n2 3\nNow all nodes point directly to root 1.\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n\n#define MAXN 1000\nint parent[MAXN];\nint rank_[MAXN];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank_[v] = 0;\n}\n\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    // Path compression\n    parent[v] = find_set(parent[v]);\n    return parent[v];\n}\n\nint main(void) {\n    int n = 5;\n    for (int i = 1; i &lt;= n; i++) make_set(i);\n    parent[2] = 1;\n    parent[3] = 2;\n    printf(\"Find(3) before: %d\\n\", parent[3]);\n    printf(\"Root of 3: %d\\n\", find_set(3));\n    printf(\"Find(3) after: %d\\n\", parent[3]);\n}\nPython Implementation\ndef make_set(x, parent, rank):\n    parent[x] = x\n    rank[x] = 0\n\ndef find_set(x, parent):\n    if parent[x] != x:\n        parent[x] = find_set(parent[x], parent)  # Path compression\n    return parent[x]\n\nn = 5\nparent = {}\nrank = {}\nfor i in range(1, n+1):\n    make_set(i, parent, rank)\n\nparent[2] = 1\nparent[3] = 2\n\nprint(\"Find(3) before:\", parent)\nprint(\"Root of 3:\", find_set(3, parent))\nprint(\"Find(3) after:\", parent)\n\n\nWhy It Matters\n\nEnables O(α(n)) almost-constant-time queries\nFundamental for Union-Find efficiency\nReduces tree height dramatically\nUsed in graph algorithms (Kruskal, connected components, etc.)\n\n\n\nA Gentle Proof (Why It Works)\nEvery set is a rooted tree. Without compression, a sequence of unions could build a deep chain. With path compression, each Find flattens paths, ensuring amortized constant time per operation (Ackermann-inverse time).\nSo after repeated operations, DSU trees become very shallow.\n\n\nTry It Yourself\n\nInitialize {1..5}\nSet parent[2]=1, parent[3]=2\nCall Find(3) and observe compression\nCheck parent[3] == 1 after\n\n\n\nTest Cases\n\n\n\nparent before\nFind(x)\nparent after\n\n\n\n\n[1,1,2]\nFind(3)\n[1,1,1]\n\n\n[1,2,3,4,5]\nFind(5)\n[1,2,3,4,5]\n\n\n[1,1,1,1,1]\nFind(3)\n[1,1,1,1,1]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Amortized)\nSpace\n\n\n\n\nFind\nO(α(n))\nO(1)\n\n\nMake-Set\nO(1)\nO(1)\n\n\nUnion\nO(α(n))\nO(1)\n\n\n\nThe Find operation is your compass, it always leads you to the leader of your set, and with path compression, it gets faster every step.\n\n\n\n253 Union\nThe Union operation is what ties the Disjoint Set Union (DSU) together. After initializing sets with Make-Set and locating leaders with Find, Union merges two disjoint sets into one, a cornerstone of dynamic connectivity.\nTo keep the trees shallow and efficient, we combine it with heuristics like Union by Rank or Union by Size.\n\nWhat Problem Are We Solving?\nWe want to merge the sets containing two elements a and b.\nIf a and b belong to different sets, their leaders (Find(a) and Find(b)) are different. The Union operation connects these leaders, ensuring both now share the same representative.\nWe must do it efficiently, no unnecessary tree height. That’s where Union by Rank comes in.\n\n\nHow It Works (Plain Language)\n\nFind roots of both elements:\nrootA = Find(a)\nrootB = Find(b)\nIf they’re already equal → same set, no action.\nOtherwise, attach the shorter tree under the taller one:\n\nIf rank[rootA] &lt; rank[rootB]: parent[rootA] = rootB\nElse if rank[rootA] &gt; rank[rootB]: parent[rootB] = rootA\nElse: parent[rootB] = rootA, and rank[rootA]++\n\n\nThis keeps the resulting forest balanced, maintaining O(α(n)) time for operations.\n\n\nExample\nInitial sets: {1}, {2}, {3}, {4}\n\n\n\nx\nparent[x]\nrank[x]\n\n\n\n\n1\n1\n0\n\n\n2\n2\n0\n\n\n3\n3\n0\n\n\n4\n4\n0\n\n\n\nPerform Union(1,2) → attach 2 under 1\nparent[2] = 1  \nrank[1] = 1\nNow sets: {1,2}, {3}, {4}\nPerform Union(3,4) → attach 4 under 3 Sets: {1,2}, {3,4}\nThen Union(2,3) → merge leaders (1 and 3) Attach lower rank under higher (both rank 1 → tie) → attach 3 under 1, rank[1] = 2\nFinal parent table:\n\n\n\nx\nparent[x]\nrank[x]\n\n\n\n\n1\n1\n2\n\n\n2\n1\n0\n\n\n3\n1\n1\n\n\n4\n3\n0\n\n\n\nNow all are connected under root 1. ✅\n\n\nVisualization\nStart:\n1   2   3   4\nUnion(1,2):\n  1\n  |\n  2\nUnion(3,4):\n  3\n  |\n  4\nUnion(2,3):\n    1\n  / | \\\n 2  3  4\nAll connected under 1.\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n\n#define MAXN 1000\nint parent[MAXN];\nint rank_[MAXN];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank_[v] = 0;\n}\n\nint find_set(int v) {\n    if (v == parent[v]) return v;\n    return parent[v] = find_set(parent[v]); // path compression\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank_[a] &lt; rank_[b])\n            parent[a] = b;\n        else if (rank_[a] &gt; rank_[b])\n            parent[b] = a;\n        else {\n            parent[b] = a;\n            rank_[a]++;\n        }\n    }\n}\n\nint main(void) {\n    for (int i=1;i&lt;=4;i++) make_set(i);\n    union_sets(1,2);\n    union_sets(3,4);\n    union_sets(2,3);\n    for (int i=1;i&lt;=4;i++)\n        printf(\"Element %d: parent=%d, rank=%d\\n\", i, parent[i], rank_[i]);\n}\nPython Implementation\ndef make_set(x, parent, rank):\n    parent[x] = x\n    rank[x] = 0\n\ndef find_set(x, parent):\n    if parent[x] != x:\n        parent[x] = find_set(parent[x], parent)\n    return parent[x]\n\ndef union_sets(a, b, parent, rank):\n    a = find_set(a, parent)\n    b = find_set(b, parent)\n    if a != b:\n        if rank[a] &lt; rank[b]:\n            parent[a] = b\n        elif rank[a] &gt; rank[b]:\n            parent[b] = a\n        else:\n            parent[b] = a\n            rank[a] += 1\n\nn = 4\nparent = {}\nrank = {}\nfor i in range(1, n+1): make_set(i, parent, rank)\nunion_sets(1,2,parent,rank)\nunion_sets(3,4,parent,rank)\nunion_sets(2,3,parent,rank)\nprint(\"Parent:\", parent)\nprint(\"Rank:\", rank)\n\n\nWhy It Matters\n\nCore operation in Union-Find\nEnables dynamic set merging efficiently\nEssential for graph algorithms:\n\nKruskal’s Minimum Spanning Tree\nConnected Components\nCycle Detection\n\n\nCombining Union by Rank and Path Compression yields near-constant performance for huge datasets.\n\n\nA Gentle Proof (Why It Works)\nUnion ensures disjointness:\n\nOnly merges sets with different roots\nMaintains one leader per set Union by Rank keeps trees balanced → tree height ≤ log n With path compression, amortized cost per operation is α(n), effectively constant for all practical n.\n\n\n\nTry It Yourself\n\nCreate 5 singleton sets\nUnion(1,2), Union(3,4), Union(2,3)\nVerify all share same root\nInspect ranks before/after unions\n\n\n\nTest Cases\n\n\n\nOperations\nExpected Sets\nRoots\n\n\n\n\nMake-Set(1..4)\n{1},{2},{3},{4}\n[1,2,3,4]\n\n\nUnion(1,2)\n{1,2}\n[1,1,3,4]\n\n\nUnion(3,4)\n{3,4}\n[1,1,3,3]\n\n\nUnion(2,3)\n{1,2,3,4}\n[1,1,1,1]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Amortized)\nSpace\n\n\n\n\nUnion\nO(α(n))\nO(1)\n\n\nFind\nO(α(n))\nO(1)\n\n\nMake-Set\nO(1)\nO(1)\n\n\n\nUnion is the handshake between sets, careful, balanced, and lightning-fast when combined with smart optimizations.\n\n\n\n254 Union by Rank\nUnion by Rank is a balancing strategy used in the Disjoint Set Union (DSU) structure to keep trees shallow. When merging two sets, instead of arbitrarily attaching one root under another, we attach the shorter tree (lower rank) under the taller tree (higher rank).\nThis small trick, combined with Path Compression, gives DSU its legendary near-constant performance, almost O(1) per operation.\n\nWhat Problem Are We Solving?\nWithout balancing, repeated Union operations might create tall chains like:\n1 ← 2 ← 3 ← 4 ← 5\nIn the worst case, Find(x) becomes O(n).\nUnion by Rank prevents this by maintaining a rough measure of tree height. Each rank[root] tracks the approximate height of its tree.\nWhen merging:\n\nAttach the lower rank tree under the higher rank tree.\nIf ranks are equal, choose one as parent and increase its rank by 1.\n\n\n\nHow It Works (Plain Language)\nEach node x has:\n\nparent[x] → leader pointer\nrank[x] → estimate of tree height\n\nWhen performing Union(a, b):\n\nrootA = Find(a)\nrootB = Find(b)\nIf ranks differ, attach smaller under larger:\nif rank[rootA] &lt; rank[rootB]:\n    parent[rootA] = rootB\nelse if rank[rootA] &gt; rank[rootB]:\n    parent[rootB] = rootA\nelse:\n    parent[rootB] = rootA\n    rank[rootA] += 1\n\nThis ensures the tree grows only when necessary.\n\n\nExample\nStart with {1}, {2}, {3}, {4} All have rank = 0\nPerform Union(1,2) → same rank → attach 2 under 1, increment rank[1]=1\n1\n|\n2\nPerform Union(3,4) → same rank → attach 4 under 3, increment rank[3]=1\n3\n|\n4\nNow Union(1,3) → both roots rank=1 → tie → attach 3 under 1, rank[1]=2\n   1(rank=2)\n  / \\\n 2   3\n     |\n     4\nTree height stays small, well-balanced.\n\n\n\nElement\nParent\nRank\n\n\n\n\n1\n1\n2\n\n\n2\n1\n0\n\n\n3\n1\n1\n\n\n4\n3\n0\n\n\n\n\n\nVisualization\nBefore balancing:\n1 ← 2 ← 3 ← 4\nAfter union by rank:\n    1\n   / \\\n  2   3\n      |\n      4\nBalanced and efficient ✅\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n\n#define MAXN 1000\nint parent[MAXN];\nint rank_[MAXN];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank_[v] = 0;\n}\n\nint find_set(int v) {\n    if (v == parent[v]) return v;\n    return parent[v] = find_set(parent[v]); // Path compression\n}\n\nvoid union_by_rank(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank_[a] &lt; rank_[b]) parent[a] = b;\n        else if (rank_[a] &gt; rank_[b]) parent[b] = a;\n        else {\n            parent[b] = a;\n            rank_[a]++;\n        }\n    }\n}\n\nint main(void) {\n    for (int i=1;i&lt;=4;i++) make_set(i);\n    union_by_rank(1,2);\n    union_by_rank(3,4);\n    union_by_rank(2,3);\n    for (int i=1;i&lt;=4;i++)\n        printf(\"Element %d: parent=%d rank=%d\\n\", i, parent[i], rank_[i]);\n}\nPython Implementation\ndef make_set(x, parent, rank):\n    parent[x] = x\n    rank[x] = 0\n\ndef find_set(x, parent):\n    if parent[x] != x:\n        parent[x] = find_set(parent[x], parent)\n    return parent[x]\n\ndef union_by_rank(a, b, parent, rank):\n    a = find_set(a, parent)\n    b = find_set(b, parent)\n    if a != b:\n        if rank[a] &lt; rank[b]:\n            parent[a] = b\n        elif rank[a] &gt; rank[b]:\n            parent[b] = a\n        else:\n            parent[b] = a\n            rank[a] += 1\n\nn = 4\nparent = {}\nrank = {}\nfor i in range(1, n+1): make_set(i, parent, rank)\nunion_by_rank(1,2,parent,rank)\nunion_by_rank(3,4,parent,rank)\nunion_by_rank(2,3,parent,rank)\nprint(\"Parent:\", parent)\nprint(\"Rank:\", rank)\n\n\nWhy It Matters\n\nKeeps trees balanced and shallow\nEnsures amortized O(α(n)) performance\nCritical for massive-scale connectivity problems\nUsed in Kruskal’s MST, Union-Find with rollback, and network clustering\n\n\n\nA Gentle Proof (Why It Works)\nThe rank grows only when two trees of equal height merge. Thus, the height of any tree is bounded by log₂ n. Combining this with Path Compression, the amortized complexity per operation becomes O(α(n)), where α(n) (inverse Ackermann function) &lt; 5 for all practical n.\n\n\nTry It Yourself\n\nCreate 8 singleton sets\nPerform unions in sequence: (1,2), (3,4), (1,3), (5,6), (7,8), (5,7), (1,5)\nObserve ranks and parent structure\n\n\n\nTest Cases\n\n\n\nOperation\nResult (Parent Array)\nRank\n\n\n\n\nUnion(1,2)\n[1,1,3,4]\n[1,0,0,0]\n\n\nUnion(3,4)\n[1,1,3,3]\n[1,0,1,0]\n\n\nUnion(2,3)\n[1,1,1,3]\n[2,0,1,0]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Amortized)\nSpace\n\n\n\n\nUnion by Rank\nO(α(n))\nO(1)\n\n\nFind\nO(α(n))\nO(1)\n\n\nMake-Set\nO(1)\nO(1)\n\n\n\nUnion by Rank is the art of merging gracefully, always lifting smaller trees, keeping your forest light, flat, and fast.\n\n\n\n255 Path Compression\nPath Compression is the secret sauce that makes Disjoint Set Union (DSU) lightning fast. Every time you perform a Find operation, it flattens the structure by making each visited node point directly to the root. Over time, this transforms deep trees into almost flat structures, turning expensive lookups into near-constant time.\n\nWhat Problem Are We Solving?\nIn a basic DSU, Find(x) follows parent pointers up the tree until it reaches the root. Without compression, frequent unions can form long chains:\n1 ← 2 ← 3 ← 4 ← 5\nA Find(5) would take 5 steps. Multiply this over many queries, and performance tanks.\nPath Compression fixes this inefficiency by rewiring all nodes on the search path to the root directly, effectively flattening the tree.\n\n\nHow It Works (Plain Language)\nWhenever we call Find(x), we recursively find the root, then make every node along the way point directly to that root.\nPseudocode:\nFind(x):\n    if parent[x] != x:\n        parent[x] = Find(parent[x])\n    return parent[x]\nNow, future lookups for x and its descendants become instant.\n\n\nExample\nStart with a chain:\n1 ← 2 ← 3 ← 4 ← 5\nPerform Find(5):\n\nFind(5) calls Find(4)\nFind(4) calls Find(3)\nFind(3) calls Find(2)\nFind(2) calls Find(1) (root)\nOn the way back, each node gets updated:\nparent[5] = 1\nparent[4] = 1\nparent[3] = 1\nparent[2] = 1\n\nAfter compression, structure becomes flat:\n1\n├── 2\n├── 3\n├── 4\n└── 5\n\n\n\nElement\nParent Before\nParent After\n\n\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n2\n1\n\n\n4\n3\n1\n\n\n5\n4\n1\n\n\n\nNext call Find(5) → single step.\n\n\nVisualization\nBefore compression:\n1 ← 2 ← 3 ← 4 ← 5\nAfter compression:\n1\n├─2\n├─3\n├─4\n└─5\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n\n#define MAXN 100\nint parent[MAXN];\n\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find_set(int v) {\n    if (v != parent[v])\n        parent[v] = find_set(parent[v]); // Path compression\n    return parent[v];\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b)\n        parent[b] = a;\n}\n\nint main(void) {\n    for (int i = 1; i &lt;= 5; i++) make_set(i);\n    union_sets(1, 2);\n    union_sets(2, 3);\n    union_sets(3, 4);\n    union_sets(4, 5);\n    find_set(5); // compress path\n    for (int i = 1; i &lt;= 5; i++)\n        printf(\"Element %d → Parent %d\\n\", i, parent[i]);\n}\nPython Implementation\ndef make_set(x, parent):\n    parent[x] = x\n\ndef find_set(x, parent):\n    if parent[x] != x:\n        parent[x] = find_set(parent[x], parent)\n    return parent[x]\n\ndef union_sets(a, b, parent):\n    a = find_set(a, parent)\n    b = find_set(b, parent)\n    if a != b:\n        parent[b] = a\n\nparent = {}\nfor i in range(1, 6):\n    make_set(i, parent)\nunion_sets(1, 2, parent)\nunion_sets(2, 3, parent)\nunion_sets(3, 4, parent)\nunion_sets(4, 5, parent)\nfind_set(5, parent)\nprint(\"Parent map after compression:\", parent)\n\n\nWhy It Matters\n\nSpeeds up Find drastically by flattening trees.\nPairs beautifully with Union by Rank.\nAchieves amortized O(α(n)) performance.\nEssential in graph algorithms like Kruskal’s MST, connectivity checks, and dynamic clustering.\n\n\n\nA Gentle Proof (Why It Works)\nPath Compression ensures each node’s parent jumps directly to the root. Each node’s depth decreases exponentially with every Find. After a few operations, trees become almost flat, and each subsequent Find becomes O(1).\nCombining with Union by Rank:\n\nEvery operation (Find or Union) becomes O(α(n)), where α(n) is the inverse Ackermann function (smaller than 5 for any practical input).\n\n\n\nTry It Yourself\n\nCreate 6 singleton sets.\nPerform unions: (1,2), (2,3), (3,4), (4,5), (5,6).\nCall Find(6) and print parent map before and after.\nObserve how the chain flattens.\nMeasure calls count difference before and after compression.\n\n\n\nTest Cases\n\n\n\nOperation Sequence\nParent Map After\nTree Depth\n\n\n\n\nNo Compression\n{1:1, 2:1, 3:2, 4:3, 5:4}\n4\n\n\nAfter Find(5)\n{1:1, 2:1, 3:1, 4:1, 5:1}\n1\n\n\nAfter Find(3)\n{1:1, 2:1, 3:1, 4:1, 5:1}\n1\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Amortized)\nSpace\n\n\n\n\nFind with Path Compression\nO(α(n))\nO(1)\n\n\nUnion (with rank)\nO(α(n))\nO(1)\n\n\nMake-Set\nO(1)\nO(1)\n\n\n\nPath Compression is the flattening spell of DSU, once cast, your sets become sleek, your lookups swift, and your unions unstoppable.\n\n\n\n256 DSU with Rollback\nDSU with Rollback extends the classic Disjoint Set Union to support undoing recent operations. This is vital in scenarios where you need to explore multiple states, like backtracking algorithms, dynamic connectivity queries, or offline problems where unions might need to be reversed.\nInstead of destroying past states, this version remembers what changed and can roll back to a previous version in constant time.\n\nWhat Problem Are We Solving?\nStandard DSU operations (Find, Union) mutate the structure, parent pointers and ranks get updated, so you can’t easily go back.\nBut what if you’re exploring a search tree or processing offline queries where you need to:\n\nAdd an edge (Union),\nExplore a path,\nThen revert to the previous structure?\n\nA rollback DSU lets you undo changes, perfect for divide-and-conquer over time, Mo’s algorithm on trees, and offline dynamic graphs.\n\n\nHow It Works (Plain Language)\nThe idea is simple:\n\nEvery time you modify the DSU, record what you changed on a stack.\nWhen you need to revert, pop from the stack and undo the last operation.\n\nOperations to track:\n\nWhen parent[b] changes, store (b, old_parent)\nWhen rank[a] changes, store (a, old_rank)\n\nYou never perform path compression, because it’s not easily reversible. Instead, rely on union by rank for efficiency.\n\n\nExample\nLet’s build sets {1}, {2}, {3}, {4}\nPerform:\n\nUnion(1,2) → attach 2 under 1\n\nPush (2, parent=2, rank=None)\n\nUnion(3,4) → attach 4 under 3\n\nPush (4, parent=4, rank=None)\n\nUnion(1,3) → attach 3 under 1\n\nPush (3, parent=3, rank=None)\nRank of 1 increases → push (1, rank=0)\n\n\nRollback once → undo last union:\n\nRestore parent[3] = 3\nRestore rank[1] = 0\n\nNow DSU returns to state after step 2.\n\n\n\n\n\n\n\n\n\nStep\nParent Map\nRank\nStack\n\n\n\n\nInit\n[1,2,3,4]\n[0,0,0,0]\n[]\n\n\nAfter Union(1,2)\n[1,1,3,4]\n[1,0,0,0]\n[(2,2,None)]\n\n\nAfter Union(3,4)\n[1,1,3,3]\n[1,0,1,0]\n[(2,2,None),(4,4,None)]\n\n\nAfter Union(1,3)\n[1,1,1,3]\n[2,0,1,0]\n[(2,2,None),(4,4,None),(3,3,None),(1,None,0)]\n\n\nAfter Rollback\n[1,1,3,3]\n[1,0,1,0]\n[(2,2,None),(4,4,None)]\n\n\n\n\n\nTiny Code (Easy Versions)\nC Implementation (Conceptual)\n#include &lt;stdio.h&gt;\n\n#define MAXN 1000\nint parent[MAXN], rank_[MAXN];\ntypedef struct { int node, parent, rank_val, rank_changed; } Change;\nChange stack[MAXN * 10];\nint top = 0;\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank_[v] = 0;\n}\n\nint find_set(int v) {\n    while (v != parent[v]) v = parent[v];\n    return v; // no path compression\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a == b) return;\n    if (rank_[a] &lt; rank_[b]) { int tmp = a; a = b; b = tmp; }\n    stack[top++] = (Change){b, parent[b], 0, 0};\n    parent[b] = a;\n    if (rank_[a] == rank_[b]) {\n        stack[top++] = (Change){a, 0, rank_[a], 1};\n        rank_[a]++;\n    }\n}\n\nvoid rollback() {\n    if (top == 0) return;\n    Change ch = stack[--top];\n    if (ch.rank_changed) rank_[ch.node] = ch.rank_val;\n    else parent[ch.node] = ch.parent;\n}\n\nint main() {\n    for (int i=1;i&lt;=4;i++) make_set(i);\n    union_sets(1,2);\n    union_sets(3,4);\n    union_sets(1,3);\n    printf(\"Before rollback: parent[3]=%d\\n\", parent[3]);\n    rollback();\n    printf(\"After rollback: parent[3]=%d\\n\", parent[3]);\n}\nPython Implementation\nclass RollbackDSU:\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.rank = [0]*n\n        self.stack = []\n\n    def find(self, x):\n        while x != self.parent[x]:\n            x = self.parent[x]\n        return x  # no path compression\n\n    def union(self, a, b):\n        a, b = self.find(a), self.find(b)\n        if a == b:\n            return False\n        if self.rank[a] &lt; self.rank[b]:\n            a, b = b, a\n        self.stack.append(('p', b, self.parent[b]))\n        self.parent[b] = a\n        if self.rank[a] == self.rank[b]:\n            self.stack.append(('r', a, self.rank[a]))\n            self.rank[a] += 1\n        return True\n\n    def rollback(self):\n        if not self.stack:\n            return\n        typ, node, val = self.stack.pop()\n        if typ == 'r':\n            self.rank[node] = val\n        else:\n            self.parent[node] = val\n\ndsu = RollbackDSU(5)\ndsu.union(1,2)\ndsu.union(3,4)\ndsu.union(1,3)\nprint(\"Before rollback:\", dsu.parent)\ndsu.rollback()\nprint(\"After rollback:\", dsu.parent)\n\n\nWhy It Matters\n\nEnables reversible union operations\nPerfect for offline dynamic connectivity\nCore in divide-and-conquer over time algorithms\nUsed in Mo’s algorithm on trees\nHelps in exploration backtracking (e.g., DSU on recursion)\n\n\n\nA Gentle Proof (Why It Works)\nRollback DSU is efficient because:\n\nEach union modifies O(1) fields\nEach rollback reverts O(1) fields\nFind runs in O(log n) (no path compression) Thus, each operation is O(log n) or better, fully reversible.\n\n\n\nTry It Yourself\n\nCreate 6 sets {1}..{6}\nPerform unions: (1,2), (3,4), (2,3)\nRollback once, verify sets {1,2} and {3,4} remain separate\nRollback again, check individual sets restored\nPrint parent and rank at each step\n\n\n\nTest Cases\n\n\n\nStep\nOperation\nParents\nRanks\nStack Size\n\n\n\n\n1\nUnion(1,2)\n[1,1,3,4]\n[1,0,0,0]\n1\n\n\n2\nUnion(3,4)\n[1,1,3,3]\n[1,0,1,0]\n2\n\n\n3\nUnion(1,3)\n[1,1,1,3]\n[2,0,1,0]\n4\n\n\n4\nRollback\n[1,1,3,3]\n[1,0,1,0]\n2\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nMake-Set\nO(1)\nO(n)\n\n\nUnion\nO(log n)\nO(1)\n\n\nRollback\nO(1)\nO(1)\n\n\n\nDSU with Rollback is your time machine, merge, explore, undo. Perfect balance between persistence and performance.\n\n\n\n257 DSU on Tree\nDSU on Tree is a hybrid technique combining Disjoint Set Union (DSU) and Depth-First Search (DFS) to process subtree queries efficiently. It’s often called the “Small-to-Large” merging technique, and it shines in problems where you need to answer queries like:\n\n“For each node, count something inside its subtree.”\n\nInstead of recomputing from scratch at every node, we use DSU logic to reuse computed data, merging smaller subtrees into larger ones for near-linear complexity.\n\nWhat Problem Are We Solving?\nMany tree problems ask for aggregated properties of subtrees:\n\nNumber of distinct colors in a subtree\nFrequency of labels, values, or weights\nSubtree sums, counts, or modes\n\nA naive DFS recomputes at every node → O(n²) time.\nDSU on Tree avoids recomputation by merging information from child subtrees cleverly.\n\n\nHow It Works (Plain Language)\nThink of each node’s subtree as a bag of information (like a multiset of colors). We process subtrees in DFS order, and at each step:\n\nProcess all small children first and discard their data after use.\nProcess the heavy child last and keep its data (reuse it).\nMerge small subtrees into the large one incrementally.\n\nThis “small-to-large” merging ensures each element moves O(log n) times, leading to O(n log n) total complexity.\n\n\nExample\nSuppose we have a tree:\n     1\n   / | \\\n  2  3  4\n / \\    \\\n5   6    7\nEach node has a color:\nColor = [1, 2, 2, 1, 3, 3, 2]\nGoal: For every node, count distinct colors in its subtree.\nNaive way: recompute every subtree from scratch, O(n²). DSU-on-Tree way: reuse large child’s color set, merge small ones.\n\n\n\nNode\nSubtree\nColors\nDistinct Count\n\n\n\n\n5\n[5]\n{3}\n1\n\n\n6\n[6]\n{3}\n1\n\n\n2\n[2,5,6]\n{2,3}\n2\n\n\n3\n[3]\n{2}\n1\n\n\n7\n[7]\n{2}\n1\n\n\n4\n[4,7]\n{1,2}\n2\n\n\n1\n[1,2,3,4,5,6,7]\n{1,2,3}\n3\n\n\n\nEach subtree merges small color sets into the big one only once → efficient.\n\n\nStep-by-Step Idea\n\nDFS to compute subtree sizes\nIdentify heavy child (largest subtree)\nDFS again:\n\nProcess all light children (small subtrees), discarding results\nProcess heavy child, keep its results\nMerge all light children’s data into heavy child’s data\n\nRecord answers for each node after merging\n\n\n\nTiny Code (Easy Versions)\nC Implementation (Conceptual)\n#include &lt;stdio.h&gt;\n#include &lt;vector&gt;\n#include &lt;set&gt;\n\n#define MAXN 100005\nusing namespace std;\n\nvector&lt;int&gt; tree[MAXN];\nint color[MAXN];\nint subtree_size[MAXN];\nint answer[MAXN];\nint freq[MAXN];\nint n;\n\nvoid dfs_size(int u, int p) {\n    subtree_size[u] = 1;\n    for (int v : tree[u])\n        if (v != p) {\n            dfs_size(v, u);\n            subtree_size[u] += subtree_size[v];\n        }\n}\n\nvoid add_color(int u, int p, int val) {\n    freq[color[u]] += val;\n    for (int v : tree[u])\n        if (v != p) add_color(v, u, val);\n}\n\nvoid dfs(int u, int p, bool keep) {\n    int bigChild = -1, maxSize = -1;\n    for (int v : tree[u])\n        if (v != p && subtree_size[v] &gt; maxSize)\n            maxSize = subtree_size[v], bigChild = v;\n\n    // Process small children\n    for (int v : tree[u])\n        if (v != p && v != bigChild)\n            dfs(v, u, false);\n\n    // Process big child\n    if (bigChild != -1) dfs(bigChild, u, true);\n\n    // Merge small children's info\n    for (int v : tree[u])\n        if (v != p && v != bigChild)\n            add_color(v, u, +1);\n\n    freq[color[u]]++;\n    // Example query: count distinct colors\n    answer[u] = 0;\n    for (int i = 1; i &lt;= n; i++)\n        if (freq[i] &gt; 0) answer[u]++;\n\n    if (!keep) add_color(u, p, -1);\n}\n\nint main() {\n    n = 7;\n    // Build tree, set colors...\n    // dfs_size(1,0); dfs(1,0,true);\n}\nPython Implementation (Simplified)\nfrom collections import defaultdict\n\ndef dfs_size(u, p, tree, size):\n    size[u] = 1\n    for v in tree[u]:\n        if v != p:\n            dfs_size(v, u, tree, size)\n            size[u] += size[v]\n\ndef add_color(u, p, tree, color, freq, val):\n    freq[color[u]] += val\n    for v in tree[u]:\n        if v != p:\n            add_color(v, u, tree, color, freq, val)\n\ndef dfs(u, p, tree, size, color, freq, ans, keep):\n    bigChild, maxSize = -1, -1\n    for v in tree[u]:\n        if v != p and size[v] &gt; maxSize:\n            maxSize, bigChild = size[v], v\n\n    for v in tree[u]:\n        if v != p and v != bigChild:\n            dfs(v, u, tree, size, color, freq, ans, False)\n\n    if bigChild != -1:\n        dfs(bigChild, u, tree, size, color, freq, ans, True)\n\n    for v in tree[u]:\n        if v != p and v != bigChild:\n            add_color(v, u, tree, color, freq, 1)\n\n    freq[color[u]] += 1\n    ans[u] = len([c for c in freq if freq[c] &gt; 0])\n\n    if not keep:\n        add_color(u, p, tree, color, freq, -1)\n\n# Example usage\nn = 7\ntree = {1:[2,3,4], 2:[1,5,6], 3:[1], 4:[1,7], 5:[2], 6:[2], 7:[4]}\ncolor = {1:1,2:2,3:2,4:1,5:3,6:3,7:2}\nsize = {}\nans = {}\nfreq = defaultdict(int)\ndfs_size(1,0,tree,size)\ndfs(1,0,tree,size,color,freq,ans,True)\nprint(ans)\n\n\nWhy It Matters\n\nHandles subtree queries in O(n log n)\nWorks well with static trees and offline queries\nReuses computed results for heavy subtrees\nFoundation for Mo’s algorithm on trees and color frequency problems\n\n\n\nA Gentle Proof (Why It Works)\nEach node’s color (or element) is merged only O(log n) times:\n\nEach time, it moves from a smaller set to a larger one.\nHence total merges = O(n log n).\n\nThe keep flag ensures we only retain big subtrees, discarding light ones to save memory and time.\n\n\nTry It Yourself\n\nAssign random colors to a tree of 8 nodes.\nUse DSU on Tree to count distinct colors per subtree.\nCompare with brute-force DFS result.\nVerify both match, but DSU-on-Tree runs faster.\n\n\n\nTest Cases\n\n\n\nNode\nSubtree Colors\nDistinct Count\n\n\n\n\n5\n{3}\n1\n\n\n6\n{3}\n1\n\n\n2\n{2,3}\n2\n\n\n3\n{2}\n1\n\n\n7\n{2}\n1\n\n\n4\n{1,2}\n2\n\n\n1\n{1,2,3}\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nDFS Traversal\nO(n)\nO(n)\n\n\nDSU Merge\nO(n log n)\nO(n)\n\n\nOverall\nO(n log n)\nO(n)\n\n\n\nDSU on Tree is your subtree superpower, merge smart, discard light, and conquer queries with elegance.\n\n\n\n258 Kruskal’s MST (Using DSU)\nKruskal’s Algorithm is a classic greedy method to build a Minimum Spanning Tree (MST), a subset of edges connecting all vertices with the smallest total weight and no cycles. It relies on Disjoint Set Union (DSU) to efficiently check whether adding an edge would form a cycle.\nWith Union-Find, Kruskal’s MST becomes clean, fast, and conceptually elegant, building the tree edge by edge in sorted order.\n\nWhat Problem Are We Solving?\nGiven a connected weighted graph with \\(n\\) vertices and \\(m\\) edges, we want to find a tree that:\n\nConnects all vertices (spanning)\n\nHas no cycles (tree)\n\nMinimizes the total edge weight\n\nA naive approach would test every subset of edges, \\(O(2^m)\\).\nKruskal’s algorithm uses edge sorting and a Disjoint Set Union (DSU) structure to reduce this to \\(O(m \\log m)\\).\n\n\nHow It Works (Plain Language)\nThe algorithm follows three steps:\n\nSort all edges by weight in ascending order.\n\nInitialize each vertex as its own set (Make-Set).\n\nFor each edge \\((u, v, w)\\) in order:\n\nIf \\(\\text{Find}(u) \\ne \\text{Find}(v)\\), the vertices are in different components → add the edge to the MST and merge the sets using Union.\n\nOtherwise, skip the edge since it would form a cycle.\n\n\nRepeat until the MST contains \\(n - 1\\) edges.\n\n\nExample\nGraph:\n\n\n\nEdge\nWeight\n\n\n\n\nA–B\n1\n\n\nB–C\n4\n\n\nA–C\n3\n\n\nC–D\n2\n\n\n\nSort edges: (A–B, 1), (C–D, 2), (A–C, 3), (B–C, 4)\nStep-by-step:\n\n\n\n\n\n\n\n\n\n\n\nStep\nEdge\nAction\nMST Edges\nMST Weight\nParent Map\n\n\n\n\n1\n(A,B,1)\nAdd\n[(A,B)]\n1\nA→A, B→A\n\n\n2\n(C,D,2)\nAdd\n[(A,B), (C,D)]\n3\nC→C, D→C\n\n\n3\n(A,C,3)\nAdd\n[(A,B), (C,D), (A,C)]\n6\nA→A, B→A, C→A, D→C\n\n\n4\n(B,C,4)\nSkip (cycle)\n–\n–\n–\n\n\n\n✅ MST Weight = 6 ✅ Edges = 3 = n − 1\n\n\nVisualization\nBefore:\nA --1-- B\n|      /\n3    4\n|  /\nC --2-- D\nAfter:\nA\n| \\\n1  3\nB   C\n    |\n    2\n    D\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n#define MAXN 100\n#define MAXM 1000\n\ntypedef struct {\n    int u, v, w;\n} Edge;\n\nint parent[MAXN], rank_[MAXN];\nEdge edges[MAXM];\n\nint cmp(const void *a, const void *b) {\n    return ((Edge*)a)-&gt;w - ((Edge*)b)-&gt;w;\n}\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank_[v] = 0;\n}\n\nint find_set(int v) {\n    if (v != parent[v]) parent[v] = find_set(parent[v]);\n    return parent[v];\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank_[a] &lt; rank_[b]) parent[a] = b;\n        else if (rank_[a] &gt; rank_[b]) parent[b] = a;\n        else { parent[b] = a; rank_[a]++; }\n    }\n}\n\nint main() {\n    int n = 4, m = 4;\n    edges[0] = (Edge){0,1,1};\n    edges[1] = (Edge){1,2,4};\n    edges[2] = (Edge){0,2,3};\n    edges[3] = (Edge){2,3,2};\n    qsort(edges, m, sizeof(Edge), cmp);\n\n    for (int i = 0; i &lt; n; i++) make_set(i);\n\n    int total = 0;\n    printf(\"Edges in MST:\\n\");\n    for (int i = 0; i &lt; m; i++) {\n        int u = edges[i].u, v = edges[i].v, w = edges[i].w;\n        if (find_set(u) != find_set(v)) {\n            union_sets(u, v);\n            total += w;\n            printf(\"%d - %d (w=%d)\\n\", u, v, w);\n        }\n    }\n    printf(\"Total Weight = %d\\n\", total);\n}\nPython Implementation\ndef make_set(parent, rank, v):\n    parent[v] = v\n    rank[v] = 0\n\ndef find_set(parent, v):\n    if parent[v] != v:\n        parent[v] = find_set(parent, parent[v])\n    return parent[v]\n\ndef union_sets(parent, rank, a, b):\n    a, b = find_set(parent, a), find_set(parent, b)\n    if a != b:\n        if rank[a] &lt; rank[b]:\n            a, b = b, a\n        parent[b] = a\n        if rank[a] == rank[b]:\n            rank[a] += 1\n\ndef kruskal(n, edges):\n    parent, rank = {}, {}\n    for i in range(n):\n        make_set(parent, rank, i)\n    mst, total = [], 0\n    for u, v, w in sorted(edges, key=lambda e: e[2]):\n        if find_set(parent, u) != find_set(parent, v):\n            union_sets(parent, rank, u, v)\n            mst.append((u, v, w))\n            total += w\n    return mst, total\n\nedges = [(0,1,1),(1,2,4),(0,2,3),(2,3,2)]\nmst, total = kruskal(4, edges)\nprint(\"MST:\", mst)\nprint(\"Total Weight:\", total)\n\n\nWhy It Matters\n\nGreedy and elegant: simple sorting + DSU logic\nFoundation for advanced topics:\n\nMinimum spanning forests\nDynamic connectivity\nMST variations (Maximum, Second-best, etc.)\n\nWorks great with edge list input\n\n\n\nA Gentle Proof (Why It Works)\nBy the Cut Property: The smallest edge crossing any cut belongs to the MST. Since Kruskal’s always picks the smallest non-cycling edge, it constructs a valid MST.\nEach union merges components without cycles → spanning tree in the end.\n\n\nTry It Yourself\n\nBuild a graph with 5 nodes, random edges and weights\nSort edges, trace unions step-by-step\nDraw MST\nCompare with Prim’s algorithm result, they’ll match\n\n\n\nTest Cases\n\n\n\nGraph\nMST Edges\nWeight\n\n\n\n\nTriangle (1–2:1, 2–3:2, 1–3:3)\n(1–2, 2–3)\n3\n\n\nSquare (4 edges, weights 1,2,3,4)\n3 smallest edges\n6\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\n\n\n\n\nSorting Edges\nO(m log m)\n\n\nDSU Operations\nO(m α(n))\n\n\nTotal\nO(m log m)\n\n\n\nSpace | O(n + m) |\nKruskal’s MST is the elegant handshake between greed and union, always connecting lightly, never circling back.\n\n\n\n259 Connected Components (Using DSU)\nConnected Components are groups of vertices where each node can reach any other through a sequence of edges. Using Disjoint Set Union (DSU), we can efficiently identify and label these components in graphs, even for massive datasets.\nInstead of exploring each region via DFS or BFS, DSU builds the connectivity relationships incrementally, merging nodes as edges appear.\n\nWhat Problem Are We Solving?\nGiven a graph (directed or undirected), we want to answer:\n\nHow many connected components exist?\nWhich vertices belong to the same component?\nIs there a path between u and v?\n\nA naive approach (DFS for each node) runs in O(n + m) but may require recursion or adjacency traversal. With DSU, we can process edge lists directly in nearly constant amortized time.\n\n\nHow It Works (Plain Language)\nEach vertex starts in its own component. For every edge (u, v):\n\nIf Find(u) != Find(v), they are in different components → Union(u, v)\nOtherwise, skip (already connected)\n\nAfter all edges are processed, all vertices sharing the same root belong to the same connected component.\n\n\nExample\nGraph:\n1, 2     3, 4\n      \\   /\n        5\nEdges: (1–2), (2–5), (3–5), (3–4)\nStep-by-step:\n\n\n\nStep\nEdge\nAction\nComponents\n\n\n\n\n1\n(1,2)\nUnion\n{1,2}, {3}, {4}, {5}\n\n\n2\n(2,5)\nUnion\n{1,2,5}, {3}, {4}\n\n\n3\n(3,5)\nUnion\n{1,2,3,5}, {4}\n\n\n4\n(3,4)\nUnion\n{1,2,3,4,5}\n\n\n\n✅ All connected → 1 component\n\n\nVisualization\nBefore:\n1   2   3   4   5\nAfter unions:\n1—2—5—3—4\nOne large connected component.\n\n\nTiny Code (Easy Versions)\nC Implementation\n#include &lt;stdio.h&gt;\n\n#define MAXN 100\nint parent[MAXN], rank_[MAXN];\n\nvoid make_set(int v) {\n    parent[v] = v;\n    rank_[v] = 0;\n}\n\nint find_set(int v) {\n    if (v != parent[v])\n        parent[v] = find_set(parent[v]);\n    return parent[v];\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank_[a] &lt; rank_[b]) parent[a] = b;\n        else if (rank_[a] &gt; rank_[b]) parent[b] = a;\n        else { parent[b] = a; rank_[a]++; }\n    }\n}\n\nint main() {\n    int n = 5;\n    int edges[][2] = {{1,2},{2,5},{3,5},{3,4}};\n    for (int i=1; i&lt;=n; i++) make_set(i);\n    for (int i=0; i&lt;4; i++)\n        union_sets(edges[i][0], edges[i][1]);\n    \n    int count = 0;\n    for (int i=1; i&lt;=n; i++)\n        if (find_set(i) == i) count++;\n    printf(\"Number of components: %d\\n\", count);\n}\nPython Implementation\ndef make_set(parent, rank, v):\n    parent[v] = v\n    rank[v] = 0\n\ndef find_set(parent, v):\n    if parent[v] != v:\n        parent[v] = find_set(parent, parent[v])\n    return parent[v]\n\ndef union_sets(parent, rank, a, b):\n    a, b = find_set(parent, a), find_set(parent, b)\n    if a != b:\n        if rank[a] &lt; rank[b]:\n            a, b = b, a\n        parent[b] = a\n        if rank[a] == rank[b]:\n            rank[a] += 1\n\ndef connected_components(n, edges):\n    parent, rank = {}, {}\n    for i in range(1, n+1):\n        make_set(parent, rank, i)\n    for u, v in edges:\n        union_sets(parent, rank, u, v)\n    roots = {find_set(parent, i) for i in parent}\n    components = {}\n    for i in range(1, n+1):\n        root = find_set(parent, i)\n        components.setdefault(root, []).append(i)\n    return components\n\nedges = [(1,2),(2,5),(3,5),(3,4)]\ncomponents = connected_components(5, edges)\nprint(\"Components:\", components)\nprint(\"Count:\", len(components))\nOutput:\nComponents: {1: [1, 2, 3, 4, 5]}\nCount: 1\n\n\nWhy It Matters\n\nQuickly answers connectivity questions\nWorks directly on edge list (no adjacency matrix needed)\nForms the backbone of algorithms like Kruskal’s MST\nExtensible to dynamic connectivity and offline queries\n\n\n\nA Gentle Proof (Why It Works)\nUnion-Find forms a forest of trees, one per component. Each union merges two trees if and only if there’s an edge connecting them. No cycles are introduced; final roots mark distinct connected components.\nEach vertex ends up linked to exactly one representative.\n\n\nTry It Yourself\n\nBuild a graph with 6 nodes and 2 disconnected clusters.\nRun DSU unions across edges.\nCount unique roots.\nPrint grouping {root: [members]}\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nComponents\n\n\n\n\n1–2–3, 4–5\n(1,2),(2,3),(4,5)\n{1,2,3}, {4,5}, {6}\n\n\nComplete Graph (1–n)\nall pairs\n1\n\n\nEmpty Graph\nnone\nn\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Amortized)\nSpace\n\n\n\n\nMake-Set\nO(1)\nO(n)\n\n\nUnion\nO(α(n))\nO(1)\n\n\nFind\nO(α(n))\nO(1)\n\n\nTotal (m edges)\nO(m α(n))\nO(n)\n\n\n\nConnected Components (DSU), a clean and scalable way to reveal the hidden clusters of any graph.\n\n\n\n260 Offline Query DSU\nOffline Query DSU is a clever twist on the standard Disjoint Set Union, used when you need to answer connectivity queries in a graph that changes over time, especially when edges are added or removed.\nInstead of handling updates online (in real time), we collect all queries first, then process them in reverse, using DSU to efficiently track connections as we “undo” deletions or simulate the timeline backward.\n\nWhat Problem Are We Solving?\nWe often face questions like:\n\n“After removing these edges, are nodes u and v still connected?”\n“If we add edges over time, when do u and v become connected?”\n\nOnline handling is hard because DSU doesn’t support deletions directly. The trick: reverse time, treat deletions as additions in reverse, and answer queries offline.\n\n\nHow It Works (Plain Language)\n\nRecord all events in the order they occur:\n\nEdge additions or deletions\nConnectivity queries\n\nReverse the timeline:\n\nProcess from the last event backward\nEvery “delete edge” becomes an “add edge”\nQueries are answered in reverse order\n\nUse DSU:\n\nEach union merges components as edges appear (in reverse)\nWhen processing a query, check if Find(u) == Find(v)\n\nFinally, reverse the answers to match the original order.\n\n\n\nExample\nImagine a graph:\n1, 2, 3\nEvents (in time order):\n\nQuery(1,3)?\nRemove edge (2,3)\nQuery(1,3)?\n\nWe can’t handle removals easily online, so we reverse:\nReverse order:\n1. Query(1,3)?\n2. Add (2,3)\n3. Query(1,3)?\nStep-by-step (in reverse):\n\n\n\nStep\nOperation\nAction\nAnswer\n\n\n\n\n1\nQuery(1,3)\n1 and 3 not connected\nNo\n\n\n2\nAdd(2,3)\nUnion(2,3)\n–\n\n\n3\nQuery(1,3)\n1–2–3 connected\nYes\n\n\n\nReverse answers: [Yes, No]\n✅ Final output:\n\nQuery 1: Yes\nQuery 2: No\n\n\n\nVisualization\nForward time:\n1—2—3\n→ remove (2,3) →\n1—2   3\nBackward time: Start with 1—2   3 → Add (2,3) → 1—2—3\nWe rebuild connectivity over time by unioning edges in reverse.\n\n\nTiny Code (Easy Versions)\nPython Implementation\ndef make_set(parent, rank, v):\n    parent[v] = v\n    rank[v] = 0\n\ndef find_set(parent, v):\n    if parent[v] != v:\n        parent[v] = find_set(parent, parent[v])\n    return parent[v]\n\ndef union_sets(parent, rank, a, b):\n    a, b = find_set(parent, a), find_set(parent, b)\n    if a != b:\n        if rank[a] &lt; rank[b]:\n            a, b = b, a\n        parent[b] = a\n        if rank[a] == rank[b]:\n            rank[a] += 1\n\n# Example\nn = 3\nedges = {(1,2), (2,3)}\nqueries = [\n    (\"?\", 1, 3),\n    (\"-\", 2, 3),\n    (\"?\", 1, 3)\n$$\n\n# Reverse events\nevents = list(reversed(queries))\n\nparent, rank = {}, {}\nfor i in range(1, n+1):\n    make_set(parent, rank, i)\n\nactive_edges = set(edges)\nanswers = []\n\nfor e in events:\n    if e[0] == \"?\":\n        _, u, v = e\n        answers.append(\"YES\" if find_set(parent, u) == find_set(parent, v) else \"NO\")\n    elif e[0] == \"-\":\n        _, u, v = e\n        union_sets(parent, rank, u, v)\n\nanswers.reverse()\nfor ans in answers:\n    print(ans)\nOutput:\nYES\nNO\n\n\nWhy It Matters\n\nHandles edge deletions without needing rollback\nPerfect for offline dynamic connectivity\nUsed in problems like:\n\n“Are u and v connected after k deletions?”\n“What is the earliest time u and v become connected?”\n\nCore idea behind Dynamic Trees and Divide & Conquer over time\n\n\n\nA Gentle Proof (Why It Works)\nDSU is monotonic, it supports adding edges, not deleting them. By reversing time, all deletions become additions. Thus, we maintain valid connectivity information backward in time, and can correctly answer queries that depend only on graph connectivity.\nReversing answers afterward restores their original sequence.\n\n\nTry It Yourself\n\nCreate a graph with 5 nodes and edges (1–2, 2–3, 3–4, 4–5)\nRemove (3–4), (2–3) sequentially\nAsk connectivity between (1,5) after each removal\nReverse timeline, simulate with DSU\n\n\n\nTest Cases\n\n\n\nEvent Sequence\nResult\n\n\n\n\n[?, 1–3], [–, 2–3], [?, 1–3]\n[YES, NO]\n\n\n[–, 1–2], [?, 1–3]\n[NO]\n\n\n[?, 4–5] (no edge)\n[NO]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Amortized)\nSpace\n\n\n\n\nMake-Set\nO(1)\nO(n)\n\n\nUnion\nO(α(n))\nO(1)\n\n\nFind\nO(α(n))\nO(1)\n\n\nTotal (Q queries, E edges)\nO((Q+E) α(n))\nO(n+E)\n\n\n\nOffline Query DSU is your time-reversing tool, flip the story, add edges back, and reveal connectivity across history.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-27.-probabilistic-data-structure",
    "href": "books/en-us/list-3.html#section-27.-probabilistic-data-structure",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 27. Probabilistic Data Structure",
    "text": "Section 27. Probabilistic Data Structure\n\n261 Bloom Filter Insert\nA Bloom Filter is a compact, probabilistic data structure used for membership testing, it can tell you if an element is definitely not present or possibly present, but never gives false negatives.\nInsertion in a Bloom Filter is simple and elegant: hash the item with multiple hash functions and set the corresponding bits to 1 in a bit array.\n\nWhat Problem Are We Solving?\nYou have a massive dataset, maybe millions or billions of keys, and you just want to ask:\n\n“Have I seen this before?”\n\nA normal hash set would explode in memory. A Bloom Filter gives a lightweight alternative:\n\nNo false negatives (safe for skipping)\nSmall memory footprint\nFixed-size bit array\n\nUsed in systems like:\n\nDatabases (caching, deduplication)\nWeb crawlers (visited URLs)\nDistributed systems (HBase, Cassandra, Bigtable)\n\n\n\nHow It Works (Plain Language)\nA Bloom Filter is just a bit array of length m (all zeroes initially), plus k independent hash functions.\nTo insert an element x:\n\nCompute k hash values: h1(x), h2(x), ..., hk(x)\nMap each hash to an index in [0, m-1]\nSet all bit[h_i(x)] = 1\n\nSo every element lights up multiple bits. Later, to check membership, we look at those same bits, if any is 0, the item was never inserted.\n\n\nExample\nLet’s build a Bloom Filter with:\n\nm = 10 bits\nk = 3 hash functions\n\nInsert “cat”:\nh1(cat) = 2\nh2(cat) = 5\nh3(cat) = 7\nSet bits 2, 5, and 7 to 1:\n\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nValue\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n\nInsert “dog”:\nh1(dog) = 1\nh2(dog) = 5\nh3(dog) = 9\nNow bit array:\n\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nValue\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n\n\n\nVisualization\nEach insertion adds “footprints” in multiple spots:\nInsert(x):\n  for i in [1..k]:\n     bit[ h_i(x) ] = 1\nThe overlap of bits allows huge compression, but leads to false positives when unrelated keys share bits.\n\n\nTiny Code (Easy Versions)\nC Implementation (Conceptual)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define M 10\n#define K 3\n\nint bitset[M];\n\nint hash1(int x) { return x % M; }\nint hash2(int x) { return (x * 3 + 1) % M; }\nint hash3(int x) { return (x * 7 + 5) % M; }\n\nvoid insert(int x) {\n    int h[K] = {hash1(x), hash2(x), hash3(x)};\n    for (int i = 0; i &lt; K; i++)\n        bitset[h[i]] = 1;\n}\n\nvoid print_bits() {\n    for (int i = 0; i &lt; M; i++) printf(\"%d \", bitset[i]);\n    printf(\"\\n\");\n}\n\nint main() {\n    memset(bitset, 0, sizeof(bitset));\n    insert(42);\n    insert(23);\n    print_bits();\n}\nPython Implementation\nm, k = 10, 3\nbitset = [0] * m\n\ndef hash_functions(x):\n    return [(hash(x) + i * i) % m for i in range(k)]\n\ndef insert(x):\n    for h in hash_functions(x):\n        bitset[h] = 1\n\ndef display():\n    print(\"Bit array:\", bitset)\n\ninsert(\"cat\")\ninsert(\"dog\")\ndisplay()\n\n\nWhy It Matters\n\nExtremely space-efficient\nNo need to store actual data\nIdeal for membership filters, duplicate detection, and pre-checks before expensive lookups\nThe backbone of approximate data structures\n\n\n\nA Gentle Proof (Why It Works)\nEach bit starts at 0. Each insertion flips k bits to 1. Query returns “maybe” if all k bits = 1, otherwise “no”. Thus:\n\nFalse negative: impossible (never unset a bit)\nFalse positive: possible, due to collisions\n\nProbability of false positive ≈ ((1 - e{-kn/m})k)\nChoosing (m) and (k) well balances accuracy vs. memory.\n\n\nTry It Yourself\n\nChoose m = 20, k = 3\nInsert {“apple”, “banana”, “grape”}\nPrint bit array\nQuery for “mango” → likely “maybe” (false positive)\n\n\n\nTest Cases\n\n\n\nInserted Elements\nQuery\nResult\n\n\n\n\n{cat, dog}\ncat\nmaybe (true positive)\n\n\n{cat, dog}\ndog\nmaybe (true positive)\n\n\n{cat, dog}\nfox\nmaybe / no (false positive possible)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(k)\nO(m)\n\n\nQuery\nO(k)\nO(m)\n\n\nFalse Positives\n≈ (1 - e{-kn/m})k\n–\n\n\n\nBloom Filter Insert, write once, maybe forever. Compact, fast, and probabilistically powerful.\n\n\n\n262 Bloom Filter Query\nA Bloom Filter Query checks whether an element might be present in the set. It uses the same k hash functions and bit array as insertion, but instead of setting bits, it simply tests them.\nThe magic:\n\nIf any bit is 0, the element was never inserted (definitely not present).\nIf all bits are 1, the element is possibly present (maybe yes).\n\nNo false negatives, if the Bloom Filter says “no,” it’s always correct.\n\nWhat Problem Are We Solving?\nWhen handling huge datasets (web crawlers, caches, key-value stores), we need a fast and memory-efficient way to answer:\n\n“Have I seen this before?”\n\nBut full storage is expensive. Bloom Filters let us skip expensive lookups by confidently ruling out items early.\nTypical use cases:\n\nDatabases: Avoid disk lookups for missing keys\nWeb crawlers: Skip revisiting known URLs\nNetworking: Cache membership checks\n\n\n\nHow It Works (Plain Language)\nA Bloom Filter has:\n\nBit array bits[0..m-1]\nk hash functions\n\nTo query element x:\n\nCompute all hashes: h1(x), h2(x), ..., hk(x)\nCheck bits at those positions\n\nIf any bit[h_i(x)] == 0, return “No” (definitely not present)\nIf all are 1, return “Maybe” (possible false positive)\n\n\nThe key rule: bits can only turn on, never off, so “no” answers are reliable.\n\n\nExample\nLet’s use a filter of size m = 10, k = 3:\nBit array after inserting “cat” and “dog”:\nIndex:  0 1 2 3 4 5 6 7 8 9\nBits:   0 1 1 0 0 1 0 1 0 1\nNow query “cat”:\nh1(cat)=2, h2(cat)=5, h3(cat)=7\nbits[2]=1, bits[5]=1, bits[7]=1 → maybe present ✅\nQuery “fox”:\nh1(fox)=3, h2(fox)=5, h3(fox)=8\nbits[3]=0 → definitely not present ❌\n\n\nVisualization\nQuery(x):\nfor i in 1..k:\n  if bit[ h_i(x) ] == 0:\n     return \"NO\"\nreturn \"MAYBE\"\nBloom Filters say “maybe” for safety, but never lie with “no”.\n\n\nTiny Code (Easy Versions)\nC Implementation (Conceptual)\n#include &lt;stdio.h&gt;\n\n#define M 10\n#define K 3\nint bitset[M];\n\nint hash1(int x) { return x % M; }\nint hash2(int x) { return (x * 3 + 1) % M; }\nint hash3(int x) { return (x * 7 + 5) % M; }\n\nint query(int x) {\n    int h[K] = {hash1(x), hash2(x), hash3(x)};\n    for (int i = 0; i &lt; K; i++)\n        if (bitset[h[i]] == 0)\n            return 0; // definitely not\n    return 1; // possibly yes\n}\n\nint main() {\n    bitset[2] = bitset[5] = bitset[7] = 1; // insert \"cat\"\n    printf(\"Query 42: %s\\n\", query(42) ? \"maybe\" : \"no\");\n    printf(\"Query 23: %s\\n\", query(23) ? \"maybe\" : \"no\");\n}\nPython Implementation\nm, k = 10, 3\nbitset = [0] * m\n\ndef hash_functions(x):\n    return [(hash(x) + i * i) % m for i in range(k)]\n\ndef insert(x):\n    for h in hash_functions(x):\n        bitset[h] = 1\n\ndef query(x):\n    for h in hash_functions(x):\n        if bitset[h] == 0:\n            return \"NO\"\n    return \"MAYBE\"\n\ninsert(\"cat\")\ninsert(\"dog\")\nprint(\"Query cat:\", query(\"cat\"))\nprint(\"Query dog:\", query(\"dog\"))\nprint(\"Query fox:\", query(\"fox\"))\n\n\nWhy It Matters\n\nConstant-time membership test\nNo false negatives, only rare false positives\nGreat for pre-filtering before heavy lookups\nCommon in distributed systems and caching layers\n\n\n\nA Gentle Proof (Why It Works)\nEach inserted element sets \\(k\\) bits to 1 in the bit array.\nFor a query, if all \\(k\\) bits are 1, the element might be present (or collisions caused those bits).\nIf any bit is 0, the element was definitely never inserted.\nThe false positive probability is:\n\\[\np = \\left(1 - e^{-kn/m}\\right)^k\n\\]\nwhere:\n\n\\(m\\): size of the bit array\n\n\\(n\\): number of inserted elements\n\n\\(k\\): number of hash functions\n\nChoose \\(m\\) and \\(k\\) to minimize \\(p\\) for the target false positive rate.\n\n\nTry It Yourself\n\nCreate a filter with m=20, k=3\nInsert {“apple”, “banana”}\nQuery {“apple”, “grape”}\nSee how “grape” may return “maybe”\n\n\n\nTest Cases\n\n\n\nInserted Elements\nQuery\nResult\n\n\n\n\n{cat, dog}\ncat\nmaybe\n\n\n{cat, dog}\ndog\nmaybe\n\n\n{cat, dog}\nfox\nno\n\n\n{}\nanything\nno\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nFalse Negatives\nFalse Positives\n\n\n\n\nQuery\nO(k)\nO(m)\nNone\nPossible\n\n\n\nBloom Filter Query, fast, memory-light, and trustable when it says “no.”\n\n\n\n263 Counting Bloom Filter\nA Counting Bloom Filter (CBF) extends the classic Bloom Filter by allowing deletions. Instead of a simple bit array, it uses an integer counter array, so each bit becomes a small counter tracking how many elements mapped to that position.\nWhen inserting, increment the counters; when deleting, decrement them. If all required counters are greater than zero, the element is possibly present.\n\nWhat Problem Are We Solving?\nA regular Bloom Filter is write-only: you can insert, but not remove. Once a bit is set to 1, it stays 1 forever.\nBut what if:\n\nYou’re tracking active sessions?\nYou need to remove expired cache keys?\nYou want to maintain a sliding window of data?\n\nThen you need a Counting Bloom Filter, which supports safe deletions.\n\n\nHow It Works (Plain Language)\nWe replace the bit array with a counter array of size \\(m\\).\nFor each element \\(x\\):\nInsert(x)\n- For each hash \\(h_i(x)\\): increment count[h_i(x)]++\nQuery(x)\n- Check all count[h_i(x)] &gt; 0 → “maybe”\n- If any are 0, → “no”\nDelete(x)\n- For each hash \\(h_i(x)\\): decrement count[h_i(x)]--\n- Ensure counters never become negative\n\n\nExample\nLet \\(m = 10\\), \\(k = 3\\)\nInitial state\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nInsert “cat”\n\\(h_1(\\text{cat}) = 2\\)\n\\(h_2(\\text{cat}) = 5\\)\n\\(h_3(\\text{cat}) = 7\\)\n→ increment \\(count[2]\\), \\(count[5]\\), \\(count[7]\\)\nArray after insertion\n[0, 0, 1, 0, 0, 1, 0, 1, 0, 0]\nInsert “dog”\n\\(h_1(\\text{dog}) = 1\\)\n\\(h_2(\\text{dog}) = 5\\)\n\\(h_3(\\text{dog}) = 9\\)\n→ increment \\(count[1]\\), \\(count[5]\\), \\(count[9]\\)\nArray after insertion\n[0, 1, 1, 0, 0, 2, 0, 1, 0, 1]\nDelete “cat”\n\\(h_1(\\text{cat}) = 2\\)\n\\(h_2(\\text{cat}) = 5\\)\n\\(h_3(\\text{cat}) = 7\\)\n→ decrement \\(count[2]\\), \\(count[5]\\), \\(count[7]\\)\nArray after deletion\n[0, 1, 0, 0, 0, 1, 0, 0, 0, 1]\nAfter deletion, “cat” is removed while “dog” remains.\n\n\nVisualization\nCounters evolve with each insert/delete:\n\n\n\nOperation\nIndex 1\n2\n5\n7\n9\n\n\n\n\nInsert(cat)\n–\n1\n1\n1\n–\n\n\nInsert(dog)\n1\n1\n2\n1\n1\n\n\nDelete(cat)\n1\n0\n1\n0\n1\n\n\n\n\n\nTiny Code (Easy Versions)\nPython Implementation\nm, k = 10, 3\ncounts = [0] * m\n\ndef hash_functions(x):\n    return [(hash(x) + i * i) % m for i in range(k)]\n\ndef insert(x):\n    for h in hash_functions(x):\n        counts[h] += 1\n\ndef query(x):\n    return all(counts[h] &gt; 0 for h in hash_functions(x))\n\ndef delete(x):\n    for h in hash_functions(x):\n        if counts[h] &gt; 0:\n            counts[h] -= 1\n\n# Example\ninsert(\"cat\")\ninsert(\"dog\")\nprint(\"After insert:\", counts)\ndelete(\"cat\")\nprint(\"After delete(cat):\", counts)\nprint(\"Query cat:\", \"Maybe\" if query(\"cat\") else \"No\")\nprint(\"Query dog:\", \"Maybe\" if query(\"dog\") else \"No\")\nOutput\nAfter insert: [0,1,1,0,0,2,0,1,0,1]\nAfter delete(cat): [0,1,0,0,0,1,0,0,0,1]\nQuery cat: No\nQuery dog: Maybe\n\n\nWhy It Matters\n\nEnables safe deletions without full reset\nUseful for cache invalidation, session tracking, streaming windows\nMemory-efficient alternative to dynamic hash sets\n\n\n\nA Gentle Proof (Why It Works)\nEach counter approximates how many items mapped to it. Deletion only decrements counters, if multiple elements shared a hash, it stays ≥1. Thus:\n\nNo false negatives, unless you over-decrement (bug)\nFalse positives remain possible, as in classic Bloom Filters\n\nProbability of false positive remains: \\[\np = \\left(1 - e^{-kn/m}\\right)^k\n\\]\n\n\nTry It Yourself\n\nCreate a filter with m=20, k=3\nInsert 5 words\nDelete 2 of them\nQuery all 5, deleted ones should say no, others maybe\n\n\n\nTest Cases\n\n\n\nOperation\nArray Snapshot\nQuery Result\n\n\n\n\nInsert(cat)\n[0,0,1,0,0,1,0,1,0,0]\n–\n\n\nInsert(dog)\n[0,1,1,0,0,2,0,1,0,1]\n–\n\n\nDelete(cat)\n[0,1,0,0,0,1,0,0,0,1]\ncat → No, dog → Maybe\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(k)\nO(m)\n\n\nQuery\nO(k)\nO(m)\n\n\nDelete\nO(k)\nO(m)\n\n\n\nCounting Bloom Filter, flexible and reversible, keeping memory lean and deletions clean.\n\n\n\n264 Cuckoo Filter\nA Cuckoo Filter is a space-efficient alternative to Bloom filters that supports both insertions and deletions while maintaining low false positive rates. Instead of using a bit array, it stores small fingerprints of keys in hash buckets, using cuckoo hashing to resolve collisions.\nIt’s like a smarter, tidier roommate, always making room by moving someone else when things get crowded.\n\nWhat Problem Are We Solving?\nBloom filters are fast and compact, but they can’t delete elements efficiently. Counting Bloom filters fix that, but they’re more memory-hungry.\nWe want:\n\nFast membership queries (O(1))\nInsert + Delete support\nHigh load factor (~95%)\nCompact memory footprint\n\nThe Cuckoo Filter solves all three by using cuckoo hashing + small fingerprints.\n\n\nHow It Works (Plain Language)\nEach element is represented by a short fingerprint (e.g., 8 bits). Each fingerprint can be placed in two possible buckets, determined by two hash functions.\nIf a bucket is full, we evict an existing fingerprint and relocate it to its alternate bucket (cuckoo style).\nOperations:\n\nInsert(x)\n\nCompute fingerprint f = hash_fingerprint(x)\nCompute i1 = hash(x) % m\nCompute i2 = i1 ⊕ hash(f) (alternate index)\nTry to place f in either i1 or i2\nIf both full, evict one fingerprint and relocate\n\nQuery(x)\n\nCheck if f is present in bucket i1 or i2\n\nDelete(x)\n\nRemove f if found in either bucket\n\n\n\n\nExample (Step-by-Step)\nAssume:\n\nBuckets: m = 4\nBucket size: b = 2\nFingerprint size: 4 bits\n\nStart (empty):\n\n\n\nBucket 0\nBucket 1\nBucket 2\nBucket 3\n\n\n\n\n\n\n\n\n\n\n\nInsert A\nf(A) = 1010  \ni1 = 1, i2 = 1 ⊕ hash(1010) = 3  \n→ Place in bucket 1\n\n\n\nB0\nB1\nB2\nB3\n\n\n\n\n\n1010\n\n\n\n\n\nInsert B\nf(B) = 0111  \ni1 = 3, i2 = 3 ⊕ hash(0111) = 0  \n→ Place in bucket 3\n\n\n\nB0\nB1\nB2\nB3\n\n\n\n\n\n1010\n\n0111\n\n\n\nInsert C\nf(C) = 0100  \ni1 = 1, i2 = 1 ⊕ hash(0100) = 2  \n→ Bucket 1 full? Move (cuckoo) if needed  \n→ Place in bucket 2\n\n\n\nB0\nB1\nB2\nB3\n\n\n\n\n\n1010\n0100\n0111\n\n\n\nQuery C → found in bucket 2 ✅ Delete A → remove from bucket 1\n\n\nTiny Code (Easy Version)\nPython Example\nimport random\n\nclass CuckooFilter:\n    def __init__(self, size=4, bucket_size=2, fingerprint_bits=4):\n        self.size = size\n        self.bucket_size = bucket_size\n        self.buckets = [[] for _ in range(size)]\n        self.mask = (1 &lt;&lt; fingerprint_bits) - 1\n\n    def _fingerprint(self, item):\n        return hash(item) & self.mask\n\n    def _alt_index(self, i, fp):\n        return (i ^ hash(fp)) % self.size\n\n    def insert(self, item, max_kicks=4):\n        fp = self._fingerprint(item)\n        i1 = hash(item) % self.size\n        i2 = self._alt_index(i1, fp)\n        for i in (i1, i2):\n            if len(self.buckets[i]) &lt; self.bucket_size:\n                self.buckets[i].append(fp)\n                return True\n        # Cuckoo eviction\n        i = random.choice([i1, i2])\n        for _ in range(max_kicks):\n            fp, self.buckets[i][0] = self.buckets[i][0], fp\n            i = self._alt_index(i, fp)\n            if len(self.buckets[i]) &lt; self.bucket_size:\n                self.buckets[i].append(fp)\n                return True\n        return False  # insert failed\n\n    def contains(self, item):\n        fp = self._fingerprint(item)\n        i1 = hash(item) % self.size\n        i2 = self._alt_index(i1, fp)\n        return fp in self.buckets[i1] or fp in self.buckets[i2]\n\n    def delete(self, item):\n        fp = self._fingerprint(item)\n        i1 = hash(item) % self.size\n        i2 = self._alt_index(i1, fp)\n        for i in (i1, i2):\n            if fp in self.buckets[i]:\n                self.buckets[i].remove(fp)\n                return True\n        return False\n\n\nWhy It Matters\n\nSupports deletions efficiently\nHigh load factor (~95%) before failure\nSmaller than Counting Bloom Filter for same error rate\nPractical for caches, membership checks, deduplication\n\n\n\nA Gentle Proof (Why It Works)\nEach element has two potential homes → high flexibility Cuckoo eviction ensures the table remains compact Short fingerprints preserve memory while keeping collisions low\nFalse positive rate: \\[\np \\approx \\frac{2b}{2^f}\n\\] where b is bucket size, f is fingerprint bits\n\n\nTry It Yourself\n\nBuild a filter with 8 buckets, 2 slots each\nInsert 5 words\nDelete 1 word\nQuery all 5, deleted one should return “no”\n\n\n\nTest Cases\n\n\n\nOperation\nBucket 0\nBucket 1\nBucket 2\nBucket 3\nResult\n\n\n\n\nInsert(A)\n–\n1010\n–\n–\nOK\n\n\nInsert(B)\n–\n1010\n–\n0111\nOK\n\n\nInsert(C)\n–\n1010\n0100\n0111\nOK\n\n\nQuery(C)\n–\n1010\n0100\n0111\nMaybe\n\n\nDelete(A)\n–\n–\n0100\n0111\nDeleted ✅\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(1) amortized\nO(m × b × f)\n\n\nQuery\nO(1)\nO(m × b × f)\n\n\nDelete\nO(1)\nO(m × b × f)\n\n\n\nCuckoo Filter, a nimble, compact, and deletable membership structure built on playful eviction.\n\n\n\n265 Count-Min Sketch\nA Count-Min Sketch (CMS) is a compact data structure for estimating the frequency of elements in a data stream. Instead of storing every item, it keeps a small 2D array of counters updated by multiple hash functions. It never underestimates counts, but may slightly overestimate due to hash collisions.\nThink of it as a memory-efficient radar, it doesn’t see every car, but it knows roughly how many are on each lane.\n\nWhat Problem Are We Solving?\nIn streaming or massive datasets, we can’t store every key-value count exactly. We want to track approximate frequencies with limited memory, supporting:\n\nStreaming updates: count items as they arrive\nApproximate queries: estimate item frequency\nMemory efficiency: sublinear space\n\nUsed in network monitoring, NLP (word counts), heavy hitter detection, and online analytics.\n\n\nHow It Works (Plain Language)\nCMS uses a 2D array with d rows and w columns. Each row has a different hash function. Each item updates one counter per row, all at its hashed index.\nInsert(x): For each row i:\nindex = hash_i(x) % w\ncount[i][index] += 1\nQuery(x): For each row i:\nestimate = min(count[i][hash_i(x) % w])\nWe take the minimum across rows, hence “Count-Min”.\n\n\nExample (Step-by-Step)\nLet d = 3 hash functions, w = 10 width.\nInitialize table:\nRow1: [0 0 0 0 0 0 0 0 0 0]\nRow2: [0 0 0 0 0 0 0 0 0 0]\nRow3: [0 0 0 0 0 0 0 0 0 0]\nInsert “apple”\nh1(apple)=2, h2(apple)=5, h3(apple)=9\n→ increment positions (1,2), (2,5), (3,9)\nInsert “banana”\nh1(banana)=2, h2(banana)=3, h3(banana)=1\n→ increment (1,2), (2,3), (3,1)\nNow:\nRow1: [0 0 2 0 0 0 0 0 0 0]\nRow2: [0 0 0 1 0 1 0 0 0 0]\nRow3: [0 1 0 0 0 0 0 0 0 1]\nQuery “apple”:\nmin(count[1][2], count[2][5], count[3][9]) = min(2,1,1) = 1\nEstimate frequency ≈ 1 (may be slightly high if collisions overlap).\n\n\nTable Visualization\n\n\n\nItem\nh₁(x)\nh₂(x)\nh₃(x)\nEstimated Count\n\n\n\n\napple\n2\n5\n9\n1\n\n\nbanana\n2\n3\n1\n1\n\n\n\n\n\nTiny Code (Easy Version)\nPython Example\nimport mmh3\n\nclass CountMinSketch:\n    def __init__(self, width=10, depth=3):\n        self.width = width\n        self.depth = depth\n        self.table = [[0] * width for _ in range(depth)]\n        self.seeds = [i * 17 for i in range(depth)]  # different hash seeds\n\n    def _hash(self, item, seed):\n        return mmh3.hash(str(item), seed) % self.width\n\n    def add(self, item, count=1):\n        for i, seed in enumerate(self.seeds):\n            idx = self._hash(item, seed)\n            self.table[i][idx] += count\n\n    def query(self, item):\n        estimates = []\n        for i, seed in enumerate(self.seeds):\n            idx = self._hash(item, seed)\n            estimates.append(self.table[i][idx])\n        return min(estimates)\n\n# Example usage\ncms = CountMinSketch(width=10, depth=3)\ncms.add(\"apple\")\ncms.add(\"banana\")\ncms.add(\"apple\")\nprint(\"apple:\", cms.query(\"apple\"))\nprint(\"banana:\", cms.query(\"banana\"))\nOutput\napple: 2\nbanana: 1\n\n\nWhy It Matters\n\nCompact: O(w × d) memory\nFast: O(1) updates and queries\nScalable: Works on unbounded data streams\nDeterministic upper bound: never underestimates\n\nUsed in:\n\nWord frequency estimation\nNetwork flow counting\nClickstream analysis\nApproximate histograms\n\n\n\nA Gentle Proof (Why It Works)\nEach item is hashed to d positions. Collisions can cause overestimation, never underestimation. By taking the minimum, we get the best upper bound estimate.\nError bound: \\[\n\\text{error} \\le \\epsilon N, \\quad \\text{with probability } 1 - \\delta\n\\] Choose: \\[\nw = \\lceil e / \\epsilon \\rceil,\\quad d = \\lceil \\ln(1/\\delta) \\rceil\n\\]\n\n\nTry It Yourself\n\nCreate a CMS with (w=20, d=4)\nStream 1000 random items\nCompare estimated vs. actual counts\nObserve overestimation patterns\n\n\n\nTest Cases\n\n\n\nOperation\nAction\nQuery Result\n\n\n\n\nInsert(apple)\n+1\n–\n\n\nInsert(apple)\n+1\n–\n\n\nInsert(banana)\n+1\n–\n\n\nQuery(apple)\n–\n2\n\n\nQuery(banana)\n–\n1\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(d)\nO(w × d)\n\n\nQuery\nO(d)\nO(w × d)\n\n\n\nCount-Min Sketch, lightweight, accurate-enough, and built for streams that never stop flowing.\n\n\n\n266 HyperLogLog\nA HyperLogLog (HLL) is a probabilistic data structure for cardinality estimation, that is, estimating the number of distinct elements in a massive dataset or stream using very little memory.\nIt doesn’t remember which items you saw, only how many distinct ones there likely were. Think of it as a memory-efficient crowd counter, it doesn’t know faces, but it knows how full the stadium is.\n\nWhat Problem Are We Solving?\nWhen processing large data streams (web analytics, logs, unique visitors, etc.), exact counting of distinct elements (using sets or hash tables) is too memory-heavy.\nWe want a solution that:\n\nTracks distinct counts approximately\nUses constant memory\nSupports mergeability (combine two sketches easily)\n\nHyperLogLog delivers O(1) time per update and ~1.04/√m error rate with just kilobytes of memory.\n\n\nHow It Works (Plain Language)\nEach incoming element is hashed to a large binary number. HLL uses the position of the leftmost 1-bit in the hash to estimate how rare (and thus how many) elements exist.\nIt maintains an array of m registers (buckets). Each bucket stores the maximum leading zero count seen so far for its hash range.\nSteps:\n\nHash element x to 64 bits: h = hash(x)\nBucket index: use first p bits of h to pick one of m = 2^p buckets\nRank: count leading zeros in the remaining bits + 1\nUpdate: store max(existing, rank) in that bucket\n\nFinal count = harmonic mean of 2^rank values, scaled by a bias-corrected constant.\n\n\nExample (Step-by-Step)\nLet p = 2 → m = 4 buckets. Initialize registers: [0,0,0,0]\nInsert “apple”:\nhash(\"apple\") = 110010100…  \nbucket = first 2 bits = 11 (3)\nrank = position of first 1 after prefix = 2\n→ bucket[3] = max(0, 2) = 2\nInsert “banana”:\nhash(\"banana\") = 01000100…  \nbucket = 01 (1)\nrank = 3\n→ bucket[1] = 3\nInsert “pear”:\nhash(\"pear\") = 10010000…  \nbucket = 10 (2)\nrank = 4\n→ bucket[2] = 4\nRegisters: [0, 3, 4, 2]\nEstimate: \\[\nE = \\alpha_m \\cdot m^2 / \\sum 2^{-M[i]}\n\\] where α_m is a bias-correction constant (≈0.673 for small m).\n\n\nVisualization\n\n\n\nBucket\nValues Seen\nLeading Zeros\nStored Rank\n\n\n\n\n0\nnone\n–\n0\n\n\n1\nbanana\n2\n3\n\n\n2\npear\n3\n4\n\n\n3\napple\n1\n2\n\n\n\n\n\nTiny Code (Easy Version)\nPython Implementation\nimport mmh3\nimport math\n\nclass HyperLogLog:\n    def __init__(self, p=4):\n        self.p = p\n        self.m = 1 &lt;&lt; p\n        self.registers = [0] * self.m\n        self.alpha = 0.673 if self.m == 16 else 0.709 if self.m == 32 else 0.7213 / (1 + 1.079 / self.m)\n\n    def _hash(self, x):\n        return mmh3.hash(str(x), 42) & 0xffffffff\n\n    def add(self, x):\n        h = self._hash(x)\n        idx = h &gt;&gt; (32 - self.p)\n        w = (h &lt;&lt; self.p) & 0xffffffff\n        rank = self._rank(w, 32 - self.p)\n        self.registers[idx] = max(self.registers[idx], rank)\n\n    def _rank(self, w, bits):\n        r = 1\n        while w & (1 &lt;&lt; (bits - 1)) == 0 and r &lt;= bits:\n            r += 1\n            w &lt;&lt;= 1\n        return r\n\n    def count(self):\n        Z = sum([2.0  -v for v in self.registers])\n        E = self.alpha * self.m * self.m / Z\n        return round(E)\n\n# Example\nhll = HyperLogLog(p=4)\nfor x in [\"apple\", \"banana\", \"pear\", \"apple\"]:\n    hll.add(x)\nprint(\"Estimated distinct count:\", hll.count())\nOutput\nEstimated distinct count: 3\n\n\nWhy It Matters\n\nTiny memory footprint (kilobytes for billions of elements)\nMergeable: HLL(A∪B) = max(HLL(A), HLL(B)) bucket-wise\nUsed in: Redis, Google BigQuery, Apache DataSketches, analytics systems\n\n\n\nA Gentle Proof (Why It Works)\nThe position of the first 1-bit follows a geometric distribution, rare long zero streaks mean more unique elements. By keeping the max observed rank per bucket, HLL captures global rarity efficiently. Combining across buckets gives a harmonic mean that balances under/over-counting.\nError ≈ 1.04 / √m, so doubling m halves the error.\n\n\nTry It Yourself\n\nCreate HLL with p = 10 (m = 1024)\nAdd 1M random numbers\nCompare HLL estimate with true count\nTest merging two sketches\n\n\n\nTest Cases\n\n\n\nItems\nTrue Count\nEstimated\nError\n\n\n\n\n10\n10\n10\n0%\n\n\n1000\n1000\n995\n~0.5%\n\n\n1e6\n1,000,000\n1,010,000\n~1%\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nAdd\nO(1)\nO(m)\n\n\nCount\nO(m)\nO(m)\n\n\nMerge\nO(m)\nO(m)\n\n\n\nHyperLogLog, counting the uncountable, one leading zero at a time.\n\n\n\n267 Flajolet–Martin Algorithm\nThe Flajolet–Martin (FM) algorithm is one of the earliest and simplest approaches for probabilistic counting, estimating the number of distinct elements in a data stream using tiny memory.\nIt’s the conceptual ancestor of HyperLogLog, showing the brilliant idea that the position of the first 1-bit in a hash tells us something about rarity.\n\nWhat Problem Are We Solving?\nWhen elements arrive in a stream too large to store exactly (web requests, IP addresses, words), we want to estimate the number of unique elements without storing them all.\nA naive approach (hash set) is O(n) memory. Flajolet–Martin achieves O(1) memory and O(1) updates.\nWe need:\n\nA streaming algorithm\nWith constant space\nFor distinct count estimation\n\n\n\nHow It Works (Plain Language)\nEach element is hashed to a large binary number (uniformly random).\nWe then find the position of the least significant 1-bit — the number of trailing zeros.\nA value with many trailing zeros is rare, which indicates a larger underlying population.\nWe track the maximum number of trailing zeros observed, denoted as R.\nThe estimated number of distinct elements is:\n\\[\n\\hat{N} = \\phi \\times 2^{R}\n\\]\nwhere \\(\\phi \\approx 0.77351\\) is a correction constant.\nSteps:\n\nInitialize \\(R = 0\\)\n\nFor each element \\(x\\):\n\n\\(h = \\text{hash}(x)\\)\n\n\\(r = \\text{count\\_trailing\\_zeros}(h)\\)\n\n\\(R = \\max(R, r)\\)\n\n\nEstimate distinct count as \\(\\hat{N} \\approx \\phi \\times 2^{R}\\)\n\n\n\nExample (Step-by-Step)\nStream: [apple, banana, apple, cherry, date]\n\n\n\nElement\nHash (Binary)\nTrailing Zeros\nR\n\n\n\n\napple\n10110\n1\n1\n\n\nbanana\n10000\n4\n4\n\n\napple\n10110\n1\n4\n\n\ncherry\n11000\n3\n4\n\n\ndate\n01100\n2\n4\n\n\n\nFinal value: \\(R = 4\\)\nEstimate: \\[\n\\hat{N} = 0.77351 \\times 2^{4} = 0.77351 \\times 16 \\approx 12.38\n\\]\nSo the estimated number of distinct elements is about 12\n(overestimation due to small sample size).\nIn practice, multiple independent hash functions or registers are used,\nand their results are averaged to reduce variance and improve accuracy.\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nHash\nBinary Form\nTrailing Zeros\nMeaning\n\n\n\n\n10000\n16\n4\nvery rare pattern → suggests large population\n\n\n11000\n24\n3\nrare-ish\n\n\n10110\n22\n1\ncommon\n\n\n\nThe longest zero-run gives the scale of rarity.\n\n\nTiny Code (Easy Version)\nPython Example\nimport mmh3\nimport math\n\ndef trailing_zeros(x):\n    if x == 0:\n        return 32\n    tz = 0\n    while (x & 1) == 0:\n        tz += 1\n        x &gt;&gt;= 1\n    return tz\n\ndef flajolet_martin(stream, seed=42):\n    R = 0\n    for x in stream:\n        h = mmh3.hash(str(x), seed) & 0xffffffff\n        r = trailing_zeros(h)\n        R = max(R, r)\n    phi = 0.77351\n    return int(phi * (2  R))\n\n# Example\nstream = [\"apple\", \"banana\", \"apple\", \"cherry\", \"date\"]\nprint(\"Estimated distinct count:\", flajolet_martin(stream))\nOutput\nEstimated distinct count: 12\n\n\nWhy It Matters\n\nFoundational for modern streaming algorithms\nInspired LogLog and HyperLogLog\nMemory-light: just a few integers\nUseful in approximate analytics, network telemetry, data warehouses\n\n\n\nA Gentle Proof (Why It Works)\nEach hash output is uniformly random.\nThe probability of observing a value with \\(r\\) trailing zeros is:\n\\[\nP(r) = \\frac{1}{2^{r+1}}\n\\]\nIf such a value appears, it suggests the stream size is roughly \\(2^{r}\\).\nTherefore, the estimate \\(2^{R}\\) naturally scales with the number of unique elements.\nBy using multiple independent estimators and averaging their results,\nthe variance can be reduced from about 50% down to around 10%.\n\n\nTry It Yourself\n\nGenerate 100 random numbers, feed to FM\nCompare estimated vs. true count\nRepeat 10 runs, compute average error\nTry combining multiple estimators (median of means)\n\n\n\nTest Cases\n\n\n\nStream Size\nTrue Distinct\nR\nEstimate\nError\n\n\n\n\n10\n10\n3\n6\n-40%\n\n\n100\n100\n7\n99\n-1%\n\n\n1000\n1000\n10\n791\n-21%\n\n\n\nUsing multiple registers improves accuracy significantly.\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\nO(1)\nO(1)\n\n\nQuery\nO(1)\nO(1)\n\n\n\nFlajolet–Martin Algorithm, the original spark of probabilistic counting, turning randomness into estimation magic.\n\n\n\n268 MinHash\nA MinHash is a probabilistic algorithm for estimating set similarity, particularly the Jaccard similarity, without comparing all elements directly. Instead of storing every element, it constructs a signature of compact hash values. The more overlap in signatures, the more similar the sets.\nMinHash is foundational in large-scale similarity estimation, fast, memory-efficient, and mathematically elegant.\n\nWhat Problem Are We Solving?\nTo compute the exact Jaccard similarity between two sets \\(A\\) and \\(B\\):\n\\[\nJ(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\\]\nThis requires comparing all elements, which is infeasible for large datasets.\nWe need methods that provide:\n\nCompact sketches using small memory\n\nFast approximate comparisons\n\nAdjustable accuracy through the number of hash functions\n\nMinHash meets these requirements by applying multiple random hash functions\nand recording the minimum hash value from each function as the set’s signature.\n\n\nHow It Works (Plain Language)\nFor each set, we apply several independent hash functions. For each hash function, we record the minimum hash value among all elements.\nTwo sets that share many elements tend to share the same minimums across these hash functions. Thus, the fraction of matching minimums estimates their Jaccard similarity.\nAlgorithm:\n\nChoose \\(k\\) hash functions \\(h_1, h_2, \\ldots, h_k\\).\n\nFor each set \\(S\\), compute its signature:\n\n\\[\n\\text{sig}(S)[i] = \\min_{x \\in S} h_i(x)\n\\]\n\nEstimate the Jaccard similarity by:\n\n\\[\n\\widehat{J}(A, B) = \\frac{1}{k} \\sum_{i=1}^{k} \\mathbf{1}\\{\\text{sig}(A)[i] = \\text{sig}(B)[i]\\}\n\\]\nEach matching position between \\(\\text{sig}(A)\\) and \\(\\text{sig}(B)\\) corresponds\nto one agreeing hash function, and the fraction of matches approximates \\(J(A,B)\\).\n\n\nExample (Step by Step)\nLet:\n\\[\nA = {\\text{apple}, \\text{banana}, \\text{cherry}}, \\quad\nB = {\\text{banana}, \\text{cherry}, \\text{date}}\n\\]\nSuppose we have 3 hash functions:\n\n\n\nItem\n\\(h_1\\)\n\\(h_2\\)\n\\(h_3\\)\n\n\n\n\napple\n5\n1\n7\n\n\nbanana\n2\n4\n3\n\n\ncherry\n3\n2\n1\n\n\ndate\n4\n3\n2\n\n\n\nSignature(A): \\[\n\\text{sig}(A) = [\\min(5,2,3), \\min(1,4,2), \\min(7,3,1)] = [2,1,1]\n\\]\nSignature(B): \\[\n\\text{sig}(B) = [\\min(2,3,4), \\min(4,2,3), \\min(3,1,2)] = [2,2,1]\n\\]\nCompare element-wise:\nMatches occur at positions 1 and 3 → \\(\\tfrac{2}{3} \\approx 0.67\\)\nActual Jaccard similarity:\n\\[\nJ(A, B) = \\frac{|\\{\\text{banana}, \\text{cherry}\\}|}{|\\{\\text{apple}, \\text{banana}, \\text{cherry}, \\text{date}\\}|}\n= \\frac{2}{4} = 0.5\n\\]\nThe MinHash estimate of \\(0.67\\) is reasonably close to the true value \\(0.5\\),\ndemonstrating that even a small number of hash functions can yield a good approximation.\n\n\nVisualization\n\n\n\nHash Function\n\\(\\text{sig}(A)\\)\n\\(\\text{sig}(B)\\)\nMatch\n\n\n\n\n\\(h_1\\)\n2\n2\n✓\n\n\n\\(h_2\\)\n1\n2\n✗\n\n\n\\(h_3\\)\n1\n1\n✓\n\n\nSimilarity\n–\n–\n\\((2/3 = 0.67)\\)\n\n\n\n\n\nTiny Code (Easy Version)\nimport mmh3\nimport math\n\ndef minhash_signature(elements, num_hashes=5, seed=42):\n    sig = [math.inf] * num_hashes\n    for x in elements:\n        for i in range(num_hashes):\n            h = mmh3.hash(str(x), seed + i)\n            if h &lt; sig[i]:\n                sig[i] = h\n    return sig\n\ndef jaccard_minhash(sigA, sigB):\n    matches = sum(1 for a, b in zip(sigA, sigB) if a == b)\n    return matches / len(sigA)\n\n# Example\nA = {\"apple\", \"banana\", \"cherry\"}\nB = {\"banana\", \"cherry\", \"date\"}\n\nsigA = minhash_signature(A, 10)\nsigB = minhash_signature(B, 10)\nprint(\"Approx similarity:\", jaccard_minhash(sigA, sigB))\nOutput\nApprox similarity: 0.6\n\n\nWhy It Matters\n\nScalable similarity: enables fast comparison of very large sets\n\nCompact representation: stores only \\(k\\) integers per set\n\nComposable: supports set unions using componentwise minimum\n\nCommon applications:\n\nDocument deduplication\n\nWeb crawling and search indexing\n\nRecommendation systems\n\nLarge-scale clustering\n\n\n\n\nA Gentle Proof (Why It Works)\nFor a random permutation \\(h\\):\n\\[\nP[\\min(h(A)) = \\min(h(B))] = J(A, B)\n\\]\nEach hash function behaves like a Bernoulli trial with success probability \\(J(A, B)\\).\nThe MinHash estimator is unbiased:\n\\[\nE[\\widehat{J}] = J(A, B)\n\\]\nThe variance decreases as \\(\\tfrac{1}{k}\\),\nso increasing the number of hash functions improves accuracy.\n\n\nTry It Yourself\n\nChoose two sets with partial overlap.\n\nGenerate MinHash signatures using \\(k = 20\\) hash functions.\n\nCompute both the estimated and true Jaccard similarities.\n\nIncrease \\(k\\) and observe how the estimated similarity converges toward the true value — larger \\(k\\) reduces variance and improves accuracy.\n\n\n\nTest Cases\n\n\n\nSets\nTrue \\(J(A,B)\\)\n\\(k\\)\nEstimated\nError\n\n\n\n\n\\(A=\\{1,2,3\\}, B=\\{2,3,4\\}\\)\n0.5\n10\n0.6\n+0.1\n\n\n\\(A=\\{1,2\\}, B=\\{1,2,3,4\\}\\)\n0.5\n20\n0.45\n-0.05\n\n\n\\(A=B\\)\n1.0\n10\n1.0\n0.0\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild Signature\n\\(O(n \\times k)\\)\n\\(O(k)\\)\n\n\nCompare\n\\(O(k)\\)\n\\(O(k)\\)\n\n\n\nMinHash turns set similarity into compact signatures — a small sketch that captures the essence of large sets with statistical grace.\n\n\n\n269 Reservoir Sampling\nReservoir Sampling is a classic algorithm for randomly sampling k elements from a stream of unknown or very large size, ensuring every element has an equal probability of being selected.\nIt’s the perfect tool when you can’t store everything, like catching a few fish from an endless river, one by one, without bias.\n\nWhat Problem Are We Solving?\nWhen data arrives as a stream too large to store in memory, we cannot know its total size in advance.\nYet, we often need to maintain a uniform random sample of fixed size \\(k\\).\nA naive approach would store all items and then sample,\nbut this becomes infeasible for large or unbounded data.\nReservoir Sampling provides a one-pass solution with these guarantees:\n\nEach item in the stream has equal probability \\(\\tfrac{k}{n}\\) of being included\n\nUses only \\(O(k)\\) memory\n\nProcesses data in a single pass\n\n\n\nHow It Works (Plain Language)\nWe maintain a reservoir (array) of size \\(k\\).\nAs each new element arrives, we decide probabilistically whether it replaces one of the existing items.\nSteps:\n\nFill the reservoir with the first \\(k\\) elements.\n\nFor each element at index \\(i\\) (starting from \\(i = k + 1\\)):\n\nGenerate a random integer \\(j \\in [1, i]\\)\n\nIf \\(j \\le k\\), replace \\(\\text{reservoir}[j]\\) with the new element\n\n\nThis ensures every element has an equal chance \\(\\tfrac{k}{n}\\) to remain.\n\n\nExample (step by step)\nStream: [A, B, C, D, E]\nGoal: \\(k = 2\\)\n\nStart with the first 2 → [A, B]\n\n\\(i = 3\\), item = C\n\nPick random \\(j \\in [1, 3]\\)\n\nSuppose \\(j = 2\\) → replace B → [A, C]\n\n\n\\(i = 4\\), item = D\n\nPick random \\(j \\in [1, 4]\\)\n\nSuppose \\(j = 4\\) → do nothing → [A, C]\n\n\n\\(i = 5\\), item = E\n\nPick random \\(j \\in [1, 5]\\)\n\nSuppose \\(j = 1\\) → replace A → [E, C]\n\n\nFinal sample: [E, C]\nEach item A–E has equal probability to appear in the final reservoir.\n\n\n\nMathematical Intuition\nEach element \\(x_i\\) at position \\(i\\) has probability\n\\[\nP(x_i \\text{ in final sample}) = \\frac{k}{i} \\cdot \\prod_{j=i+1}^{n} \\left(1 - \\frac{1}{j}\\right) = \\frac{k}{n}\n\\]\nThus, every item is equally likely to be chosen, ensuring perfect uniformity in the final sample.\n\nVisualization\n\n\n\nStep\nItem\nRandom \\(j\\)\nAction\nReservoir\n\n\n\n\n1\nA\n–\nAdd\n[A]\n\n\n2\nB\n–\nAdd\n[A, B]\n\n\n3\nC\n2\nReplace B\n[A, C]\n\n\n4\nD\n4\nNo Replace\n[A, C]\n\n\n5\nE\n1\nReplace A\n[E, C]\n\n\n\n\n\nTiny Code (Easy Version)\nPython Implementation\nimport random\n\ndef reservoir_sample(stream, k):\n    reservoir = []\n    for i, item in enumerate(stream, 1):\n        if i &lt;= k:\n            reservoir.append(item)\n        else:\n            j = random.randint(1, i)\n            if j &lt;= k:\n                reservoir[j - 1] = item\n    return reservoir\n\n# Example\nstream = [\"A\", \"B\", \"C\", \"D\", \"E\"]\nsample = reservoir_sample(stream, 2)\nprint(\"Reservoir sample:\", sample)\nOutput (random):\nReservoir sample: ['E', 'C']\nEach run produces a different uniform random sample.\n\n\nWhy It Matters\n\nWorks on streaming data\nNeeds only O(k) memory\nProvides uniform unbiased sampling\nUsed in:\n\nBig data analytics\nRandomized algorithms\nOnline learning\nNetwork monitoring\n\n\n\n\nA Gentle Proof (Why It Works)\n\nFirst \\(k\\) elements: probability \\(= 1\\) to enter the reservoir initially.\n\nEach new element at index \\(i\\): probability \\(\\tfrac{k}{i}\\) to replace one of the existing items.\n\nEarlier items may be replaced, but each remains with probability\n\n\\[\nP(\\text{survive}) = \\prod_{j=i+1}^{n} \\left(1 - \\frac{1}{j}\\right)\n\\]\nMultiplying these terms gives the final inclusion probability \\(\\tfrac{k}{n}\\).\nUniformity of selection is guaranteed by induction.\n\n\nTry It Yourself\n\nStream 10 numbers with \\(k = 3\\).\n\nRun the algorithm multiple times — all 3-element subsets appear with roughly equal frequency.\n\nIncrease \\(k\\) and observe that the sample becomes more stable, with less variation between runs.\n\n\n\nTest Cases\n\n\n\nStream\nk\nSample Size\nNotes\n\n\n\n\n[1,2,3,4,5]\n2\n2\nUniform random pairs\n\n\n[A,B,C,D]\n1\n1\nEach 25% chance\n\n\nRange(1000)\n10\n10\nWorks in one pass\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\n\\(O(1)\\)\n\\(O(k)\\)\n\n\nQuery\n\\(O(1)\\)\n\\(O(k)\\)\n\n\n\nReservoir Sampling, elegantly fair, perfectly simple, and ready for infinite streams.\n\n\n\n270 Skip Bloom Filter\nA Skip Bloom Filter is a probabilistic data structure that extends the Bloom Filter to support range queries, determining whether any element exists within a given interval, not just checking for a single item.\nIt combines Bloom filters with hierarchical range segmentation, allowing approximate range lookups while keeping space usage compact.\n\nWhat Problem Are We Solving?\nA classic Bloom Filter answers only point queries:\n\\[\n\\text{\"Is } x \\text{ in the set?\"}\n\\]\nHowever, many real-world applications require range queries, such as:\n\nDatabases: “Are there any keys between 10 and 20?”\nTime-series: “Were there any events during this time interval?”\nNetworks: “Is any IP in this subnet?”\n\nWe need a space-efficient, stream-friendly, and probabilistic structure that can:\n\nHandle range membership checks,\nMaintain a low false-positive rate,\nScale logarithmically with the universe size.\n\nThe Skip Bloom Filter solves this by layering Bloom filters over aligned ranges of increasing size.\n\n\nHow It Works (Plain Language)\nA Skip Bloom Filter maintains multiple Bloom filters, each corresponding to a level that covers intervals (buckets) of sizes \\(2^0, 2^1, 2^2, \\ldots\\)\nEach element is inserted into all Bloom filters representing the ranges that contain it.\nWhen querying a range, the query is decomposed into aligned subranges that correspond to these levels, and each is checked in its respective filter.\nAlgorithm:\n\nDivide the universe into intervals of size \\(2^\\ell\\) for each level \\(\\ell\\).\n\nEach level \\(\\ell\\) maintains a Bloom filter representing those buckets.\n\nTo insert a key \\(x\\): mark all buckets across levels that include \\(x\\).\n\nTo query a range \\([a,b]\\): decompose it into a set of disjoint aligned intervals and check the corresponding Bloom filters.\n\n\n\nExample (Step by Step)\nSuppose we store keys\n\\[\nS = {3, 7, 14}\n\\]\nin a universe ([0, 15]). We build filters at levels with range sizes (1, 2, 4, 8):\n\n\n\nLevel\nBucket Size\nBuckets (Ranges)\n\n\n\n\n0\n1\n[0], [1], [2], …, [15]\n\n\n1\n2\n[0–1], [2–3], [4–5], …, [14–15]\n\n\n2\n4\n[0–3], [4–7], [8–11], [12–15]\n\n\n3\n8\n[0–7], [8–15]\n\n\n\nInsert key 3:\n\nLevel 0: [3]\nLevel 1: [2–3]\nLevel 2: [0–3]\nLevel 3: [0–7]\n\nInsert key 7:\n\nLevel 0: [7]\nLevel 1: [6–7]\nLevel 2: [4–7]\nLevel 3: [0–7]\n\nInsert key 14:\n\nLevel 0: [14]\nLevel 1: [14–15]\nLevel 2: [12–15]\nLevel 3: [8–15]\n\nQuery range [2, 6]:\n\nDecompose into aligned intervals: ([2–3], [4–5], [6])\nCheck filters:\n\n[2–3] → hit\n[4–5] → miss\n[6] → miss\n\n\nResult: possibly non-empty, since [2–3] contains 3.\n\n\nVisualization\n\n\n\nLevel\nBucket\nContains Key\nBloom Entry\n\n\n\n\n0\n[3]\nYes\n1\n\n\n1\n[2–3]\nYes\n1\n\n\n2\n[0–3]\nYes\n1\n\n\n3\n[0–7]\nYes\n1\n\n\n\nEach key is represented in multiple levels, enabling multi-scale range coverage.\n\n\nTiny Code (Simplified Python)\nimport math, mmh3\n\nclass Bloom:\n    def __init__(self, size=64, hash_count=3):\n        self.size = size\n        self.hash_count = hash_count\n        self.bits = [0] * size\n\n    def _hashes(self, key):\n        return [mmh3.hash(str(key), i) % self.size for i in range(self.hash_count)]\n\n    def add(self, key):\n        for h in self._hashes(key):\n            self.bits[h] = 1\n\n    def query(self, key):\n        return all(self.bits[h] for h in self._hashes(key))\n\nclass SkipBloom:\n    def __init__(self, levels=4, size=64, hash_count=3):\n        self.levels = [Bloom(size, hash_count) for _ in range(levels)]\n\n    def add(self, key):\n        level = 0\n        while (1 &lt;&lt; level) &lt;= key:\n            bucket = key // (1 &lt;&lt; level)\n            self.levels[level].add(bucket)\n            level += 1\n\n    def query_range(self, start, end):\n        l = int(math.log2(end - start + 1))\n        bucket = start // (1 &lt;&lt; l)\n        return self.levels[l].query(bucket)\n\n# Example\nsb = SkipBloom(levels=4)\nfor x in [3, 7, 14]:\n    sb.add(x)\n\nprint(\"Query [2,6]:\", sb.query_range(2,6))\nOutput:\nQuery [2,6]: True\n\n\nWhy It Matters\n\nEnables range queries in probabilistic manner\nCompact and hierarchical\nNo false negatives (for properly configured filters)\nWidely applicable in:\n\nApproximate database indexing\nNetwork prefix search\nTime-series event detection\n\n\n\n\nA Gentle Proof (Why It Works)\nEach inserted key participates in \\(O(\\log U)\\) Bloom filters, one per level.\nA range query \\([a,b]\\) is decomposed into \\(O(\\log U)\\) aligned subranges.\nA Bloom filter with \\(m\\) bits, \\(k\\) hash functions, and \\(n\\) inserted elements has false positive probability:\n\\[\np = \\left(1 - e^{-kn/m}\\right)^k\n\\]\nFor a Skip Bloom Filter, the total false positive rate is bounded by:\n\\[\nP_{fp} \\le O(\\log U) \\cdot p\n\\]\nEach level guarantees no false negatives, since every range containing an element is marked.\nThus, correctness is ensured, and only overestimation (false positives) can occur.\nSpace complexity\nEach level has \\(m\\) bits, and there are \\(\\log U\\) levels:\n\\[\n\\text{Total space} = O(m \\log U)\n\\]\nTime complexity\nEach query checks \\(O(\\log U)\\) buckets, each requiring \\(O(k)\\) time:\n\\[\nT_{\\text{query}} = O(k \\log U)\n\\]\n\n\nTry It Yourself\n\nInsert keys \\(\\{3, 7, 14\\}\\)\n\nQuery ranges \\([2,6]\\), \\([8,12]\\), \\([0,15]\\)\n\nCompare true contents with the results\n\nAdjust parameters \\(m\\), \\(k\\), or the number of levels, and observe how the false positive rate changes\n\n\n\nTest Cases\n\n\n\nQuery Range\nResult\nTrue Contents\n\n\n\n\n[2,6]\nTrue\n{3}\n\n\n[8,12]\nFalse\n∅\n\n\n[12,15]\nTrue\n{14}\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nInsert\n\\(O(\\log U)\\)\n\\(O(m \\log U)\\)\n\n\nQuery\n\\(O(\\log U)\\)\n\\(O(m \\log U)\\)\n\n\n\nA Skip Bloom Filter is a range-aware extension of standard Bloom filters.\nBy combining hierarchical decomposition with standard hashing, it enables fast, memory-efficient, and approximate range queries across very large universes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-28.-skip-lists-and-b-trees",
    "href": "books/en-us/list-3.html#section-28.-skip-lists-and-b-trees",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 28. Skip Lists and B-Trees",
    "text": "Section 28. Skip Lists and B-Trees\n\n271 Skip List Insert\nSkip Lists are probabilistic alternatives to balanced trees. They maintain multiple levels of sorted linked lists, where each level skips over more elements than the one below it. Insertion relies on randomization to achieve expected O(log n) search and update times, without strict rebalancing like AVL or Red-Black trees.\n\nWhat Problem Are We Solving?\nWe want to store elements in sorted order and support:\n\nFast search, insert, and delete operations.\nSimple structure and easy implementation.\nExpected logarithmic performance without complex rotations.\n\nBalanced BSTs achieve \\(O(\\log n)\\) time but require intricate rotations. Skip Lists solve this with randomized promotion: each inserted node is promoted to higher levels with decreasing probability, forming a tower.\n\n\nHow It Works (Plain Language)\nSkip list insertion for value \\(x\\)\nSteps 1. Start at the top level. 2. While the next node’s key &lt; \\(x\\), move right. 3. If you cannot move right, drop down one level. 4. Repeat until you reach level 0. 5. Insert the new node at its sorted position on level 0. 6. Randomly choose the node height \\(h\\) (for example, flip a fair coin per level until tails, so \\(P(h \\ge t) = 2^{-t}\\)). 7. For each level \\(1 \\dots h\\), link the new node into that level by repeating the right-then-drop search with the update pointers saved from the descent.\nNotes - Duplicate handling is policy dependent. Often you skip insertion if a node with key \\(x\\) already exists. - Typical height is \\(O(\\log n)\\), expected search and insert time is \\(O(\\log n)\\), space is \\(O(n)\\).\n\n\nExample Step by Step\nLet’s insert \\(x = 17\\) into a skip list that currently contains: [ 5, 10, 15, 20, 25 ]\n\n\n\n\n\n\n\n\nLevel\nNodes (before insertion)\nPath Taken\n\n\n\n\n3\n5 → 15 → 25\nMove from 5 to 15, then down\n\n\n2\n5 → 10 → 15 → 25\nMove from 5 to 10 to 15, then down\n\n\n1\n5 → 10 → 15 → 20 → 25\nMove from 15 to 20, then down\n\n\n0\n5 → 10 → 15 → 20 → 25\nInsert after 15\n\n\n\nSuppose the random level for 17 is 2. We insert 17 at level 0 and level 1.\n\n\n\nLevel\nNodes (after insertion)\n\n\n\n\n3\n5 → 15 → 25\n\n\n2\n5 → 10 → 15 → 25\n\n\n1\n5 → 10 → 15 → 17 → 20 → 25\n\n\n0\n5 → 10 → 15 → 17 → 20 → 25\n\n\n\n\n\nTiny Code (Simplified Python)\nimport random\n\nclass Node:\n    def __init__(self, key, level):\n        self.key = key\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level=4, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = Node(-1, max_level)\n        self.level = 0\n\n    def random_level(self):\n        lvl = 0\n        while random.random() &lt; self.p and lvl &lt; self.max_level:\n            lvl += 1\n        return lvl\n\n    def insert(self, key):\n        update = [None] * (self.max_level + 1)\n        current = self.header\n\n        # Move right and down\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].key &lt; key:\n                current = current.forward[i]\n            update[i] = current\n\n        current = current.forward[0]\n        if current is None or current.key != key:\n            lvl = self.random_level()\n            if lvl &gt; self.level:\n                for i in range(self.level + 1, lvl + 1):\n                    update[i] = self.header\n                self.level = lvl\n            new_node = Node(key, lvl)\n            for i in range(lvl + 1):\n                new_node.forward[i] = update[i].forward[i]\n                update[i].forward[i] = new_node\n\n\nWhy It Matters\n\nExpected \\(O(\\log n)\\) time for search, insert, and delete\n\nSimpler than AVL or Red-Black Trees\n\nProbabilistic balancing avoids rigid rotations\n\nCommonly used in databases and key-value stores such as LevelDB and Redis\n\n\n\nA Gentle Proof (Why It Works)\nEach node appears in level \\(i\\) with probability \\(p^i\\).\nThe expected number of nodes per level is \\(n p^i\\).\nThe total number of levels is \\(O(\\log_{1/p} n)\\).\nExpected search path length:\n\\[\nE[\\text{steps}] = \\frac{1}{1 - p} \\log_{1/p} n = O(\\log n)\n\\]\nExpected space usage:\n\\[\nO(n) \\text{ nodes} \\times O\\!\\left(\\frac{1}{1 - p}\\right) \\text{ pointers per node}\n\\]\nThus, Skip Lists achieve expected logarithmic performance and linear space.\n\n\nTry It Yourself\n\nBuild a Skip List and insert \\(\\{5, 10, 15, 20, 25\\}\\).\n\nInsert \\(17\\) and trace which pointers are updated at each level.\n\nExperiment with \\(p = 0.25, 0.5, 0.75\\).\n\nObserve how random heights influence the overall balance.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nExpected Structure (Level 0)\n\n\n\n\nInsert\n10\n10\n\n\nInsert\n5\n5 → 10\n\n\nInsert\n15\n5 → 10 → 15\n\n\nInsert\n17\n5 → 10 → 15 → 17\n\n\nSearch\n15\nFound\n\n\nSearch\n12\nNot Found\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Expected)\nSpace\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\n\nA Skip List is a simple yet powerful data structure. With randomness as its balancing force, it achieves the elegance of trees and the flexibility of linked lists.\n\n\n\n272 Skip List Delete\nDeletion in a Skip List mirrors insertion: we traverse levels from top to bottom, keep track of predecessor nodes at each level, and unlink the target node across all levels it appears in. The structure maintains probabilistic balance, so no rebalancing is needed, deletion is expected \\(O(\\log n)\\).\n\nWhat Problem Are We Solving?\nWe want to remove an element efficiently from a sorted, probabilistically balanced structure. Naive linked lists require \\(O(n)\\) traversal; balanced BSTs need complex rotations. A Skip List gives us a middle ground, simple pointer updates with expected logarithmic time.\n\n\nHow It Works (Plain Language)\nEach node in a skip list can appear at multiple levels. To delete a key \\(x\\):\n\nStart from the top level.\nMove right while next node’s key &lt; \\(x\\).\nIf next node’s key == \\(x\\), record current node in an update array.\nDrop one level down and repeat.\nOnce you reach the bottom, remove all forward references to the node from the update array.\nIf the topmost level becomes empty, reduce list level.\n\n\n\nExample Step by Step\nDelete \\(x = 17\\) from this skip list:\n\n\n\nLevel\nNodes (Before)\n\n\n\n\n3\n5 → 15 → 25\n\n\n2\n5 → 10 → 15 → 25\n\n\n1\n5 → 10 → 15 → 17 → 20 → 25\n\n\n0\n5 → 10 → 15 → 17 → 20 → 25\n\n\n\nTraversal:\n\nStart at Level 3: 15 &lt; 17 → move right → 25 &gt; 17 → drop down\nLevel 2: 15 &lt; 17 → move right → 25 &gt; 17 → drop down\nLevel 1: 15 &lt; 17 → move right → 17 found → record predecessor\nLevel 0: 15 &lt; 17 → move right → 17 found → record predecessor\n\nRemove all forward pointers to 17 from recorded nodes.\n\n\n\nLevel\nNodes (After)\n\n\n\n\n3\n5 → 15 → 25\n\n\n2\n5 → 10 → 15 → 25\n\n\n1\n5 → 10 → 15 → 20 → 25\n\n\n0\n5 → 10 → 15 → 20 → 25\n\n\n\n\n\nTiny Code (Simplified Python)\nimport random\n\nclass Node:\n    def __init__(self, key, level):\n        self.key = key\n        self.forward = [None] * (level + 1)\n\nclass SkipList:\n    def __init__(self, max_level=4, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = Node(-1, max_level)\n        self.level = 0\n\n    def delete(self, key):\n        update = [None] * (self.max_level + 1)\n        current = self.header\n\n        # Traverse from top to bottom\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].key &lt; key:\n                current = current.forward[i]\n            update[i] = current\n\n        current = current.forward[0]\n\n        # Found the node\n        if current and current.key == key:\n            for i in range(self.level + 1):\n                if update[i].forward[i] != current:\n                    continue\n                update[i].forward[i] = current.forward[i]\n            # Reduce level if highest level empty\n            while self.level &gt; 0 and self.header.forward[self.level] is None:\n                self.level -= 1\n\n\nWhy It Matters\n\nSymmetric to insertion\nNo rotations or rebalancing\nExpected \\(O(\\log n)\\) performance\nPerfect for ordered maps, databases, key-value stores\n\n\n\nA Gentle Proof (Why It Works)\nEach level contains a fraction \\(p^i\\) of nodes. The expected number of levels traversed is \\(O(\\log_{1/p} n)\\). At each level, we move horizontally \\(O(1)\\) on average.\nSo expected cost:\n\\[\nE[T_{\\text{delete}}] = O(\\log n)\n\\]\n\n\nTry It Yourself\n\nInsert \\({5, 10, 15, 17, 20, 25}\\).\nDelete \\(17\\).\nTrace all pointer changes level by level.\nCompare with AVL tree deletion complexity.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nExpected Level 0 Result\n\n\n\n\nInsert\n5,10,15,17,20\n5 → 10 → 15 → 17 → 20\n\n\nDelete\n17\n5 → 10 → 15 → 20\n\n\nDelete\n10\n5 → 15 → 20\n\n\nDelete\n5\n15 → 20\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Expected)\nSpace\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\n\nSkip List Deletion keeps elegance through simplicity, a clean pointer adjustment instead of tree surgery.\n\n\n\n273 Skip List Search\nSearching in a Skip List is a dance across levels, we move right until we can’t, then down, repeating until we either find the key or conclude it doesn’t exist. Thanks to the randomized level structure, the expected time complexity is \\(O(\\log n)\\), just like balanced BSTs but with simpler pointer logic.\n\nWhat Problem Are We Solving?\nWe need a fast search in a sorted collection that adapts gracefully to dynamic insertions and deletions. Balanced trees guarantee \\(O(\\log n)\\) search but need rotations. Skip Lists achieve the same expected time using randomization instead of strict balancing rules.\n\n\nHow It Works (Plain Language)\nA skip list has multiple levels of linked lists. Each level acts as a fast lane, skipping over multiple nodes. To search for a key \\(x\\):\n\nStart at the top-left header node.\nAt each level, move right while next.key &lt; x.\nWhen next.key ≥ x, drop down one level.\nRepeat until level 0.\nIf current.forward[0].key == x, found; else, not found.\n\nThe search path “zigzags” through levels, visiting roughly \\(\\log n\\) nodes on average.\n\n\nExample Step by Step\nSearch for \\(x = 17\\) in the following skip list:\n\n\n\nLevel\nNodes\n\n\n\n\n3\n5 → 15 → 25\n\n\n2\n5 → 10 → 15 → 25\n\n\n1\n5 → 10 → 15 → 17 → 20 → 25\n\n\n0\n5 → 10 → 15 → 17 → 20 → 25\n\n\n\nTraversal:\n\nLevel 3: 5 → 15 → (next = 25 &gt; 17) → drop down\nLevel 2: 15 → (next = 25 &gt; 17) → drop down\nLevel 1: 15 → 17 found → success\nLevel 0: confirm 17 exists\n\nPath: 5 → 15 → (down) → 15 → (down) → 15 → 17\n\n\nVisualization\nSkip list search follows a staircase pattern:\nLevel 3:  5 --------&gt; 15 -----↓\nLevel 2:  5 ----&gt; 10 --&gt; 15 --↓\nLevel 1:  5 -&gt; 10 -&gt; 15 -&gt; 17 -&gt; 20\nLevel 0:  5 -&gt; 10 -&gt; 15 -&gt; 17 -&gt; 20\nEach “↓” means dropping a level when the next node is too large.\n\n\nTiny Code (Simplified Python)\nclass SkipList:\n    def __init__(self, max_level=4, p=0.5):\n        self.max_level = max_level\n        self.p = p\n        self.header = Node(-1, max_level)\n        self.level = 0\n\n    def search(self, key):\n        current = self.header\n        # Traverse top-down\n        for i in reversed(range(self.level + 1)):\n            while current.forward[i] and current.forward[i].key &lt; key:\n                current = current.forward[i]\n        current = current.forward[0]\n        return current and current.key == key\n\n\nWhy It Matters\n\nSimple and efficient: expected \\(O(\\log n)\\) time\nProbabilistic balance: avoids tree rotations\nFoundation for ordered maps, indexes, and databases\nSearch path length is logarithmic on average\n\n\n\nA Gentle Proof (Why It Works)\nEach level contains approximately a fraction \\(p\\) of the nodes from the level below. Expected number of levels: \\(O(\\log_{1/p} n)\\).\nAt each level, expected number of horizontal moves: \\(O(1)\\).\nSo total expected search time:\n\\[\nE[T_{\\text{search}}] = O(\\log n)\n\\]\n\n\nTry It Yourself\n\nBuild a skip list with \\({5, 10, 15, 17, 20, 25}\\).\nSearch for \\(17\\) and trace the path at each level.\nSearch for \\(13\\), where do you stop?\nCompare path length with a binary search tree of same size.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nExpected Output\nPath\n\n\n\n\nSearch\n17\nFound\n5 → 15 → 17\n\n\nSearch\n10\nFound\n5 → 10\n\n\nSearch\n13\nNot Found\n5 → 10 → 15\n\n\nSearch\n5\nFound\n5\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime (Expected)\nSpace\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\n\nSkip List Search shows how probabilistic structure yields deterministic-like efficiency, walking a staircase of randomness toward certainty.\n\n\n\n274 B-Tree Insert\nA B-Tree is a balanced search tree designed for external memory systems such as disks or SSDs. Unlike binary trees, each node can store multiple keys and multiple children, minimizing disk I/O by packing more data into a single node. Insertion into a B-Tree preserves sorted order and balance by splitting full nodes as needed.\n\nWhat Problem Are We Solving?\nWhen data is too large to fit in memory, standard binary trees perform poorly because each node access may trigger a disk read. We need a structure that:\n\nReduces the number of I/O operations\nKeeps height small\nMaintains keys in sorted order\nSupports search, insert, and delete in \\(O(\\log n)\\)\n\nB-Trees solve this by storing many keys per node and balancing themselves through controlled splits.\n\n\nHow It Works (Plain Language)\nEach B-Tree node can contain up to \\(2t - 1\\) keys and \\(2t\\) children, where \\(t\\) is the minimum degree.\nInsertion steps:\n\nStart at root and traverse down like in binary search.\nIf a child node is full (2t − 1 keys), split it before descending.\nInsert the new key into the appropriate non-full node.\n\nSplitting a full node:\n\nMiddle key moves up to parent\nLeft and right halves become separate child nodes\n\nThis ensures every node stays within allowed size bounds, keeping height \\(O(\\log_t n)\\).\n\n\nExample Step by Step\nLet \\(t = 2\\) (max 3 keys per node). Insert keys in order: \\([10, 20, 5, 6, 12, 30, 7, 17]\\)\nStep 1: Insert 10 → Root = [10] Step 2: Insert 20 → [10, 20] Step 3: Insert 5 → [5, 10, 20] Step 4: Insert 6 → Node full → Split\nSplit [5, 6, 10, 20]:\n\nMiddle key 10 moves up\nLeft child [5, 6], right child [20] Tree:\n\n      [10]\n     /    \\\n [5, 6]   [20]\nStep 5: Insert 12 → go right → [12, 20] Step 6: Insert 30 → [12, 20, 30] Step 7: Insert 7 → go left → [5, 6, 7] → full → split\n\nMiddle 6 moves up\n\nTree now:\n        [6, 10]\n       /   |    \\\n [5]  [7]  [12, 20, 30]\nStep 8: Insert 17 → go to [12, 20, 30] → insert [12, 17, 20, 30] → split\n\nMiddle 20 moves up\n\nFinal tree:\n          [6, 10, 20]\n         /   |   |   \\\n       [5] [7] [12,17] [30]\n\n\nVisualization\nB-Tree maintains sorted keys at each level and guarantees minimal height by splitting nodes during insertion.\nRoot: [6, 10, 20]\nChildren: [5], [7], [12, 17], [30]\n\n\nTiny Code (Simplified Python)\nclass BTreeNode:\n    def __init__(self, t, leaf=False):\n        self.keys = []\n        self.children = []\n        self.leaf = leaf\n        self.t = t\n\n    def insert_non_full(self, key):\n        i = len(self.keys) - 1\n        if self.leaf:\n            self.keys.append(key)\n            self.keys.sort()\n        else:\n            while i &gt;= 0 and key &lt; self.keys[i]:\n                i -= 1\n            i += 1\n            if len(self.children[i].keys) == 2 * self.t - 1:\n                self.split_child(i)\n                if key &gt; self.keys[i]:\n                    i += 1\n            self.children[i].insert_non_full(key)\n\n    def split_child(self, i):\n        t = self.t\n        y = self.children[i]\n        z = BTreeNode(t, y.leaf)\n        mid = y.keys[t - 1]\n        z.keys = y.keys[t:]\n        y.keys = y.keys[:t - 1]\n        if not y.leaf:\n            z.children = y.children[t:]\n            y.children = y.children[:t]\n        self.children.insert(i + 1, z)\n        self.keys.insert(i, mid)\n\nclass BTree:\n    def __init__(self, t):\n        self.root = BTreeNode(t, True)\n        self.t = t\n\n    def insert(self, key):\n        r = self.root\n        if len(r.keys) == 2 * self.t - 1:\n            s = BTreeNode(self.t)\n            s.children.insert(0, r)\n            s.split_child(0)\n            i = 0\n            if key &gt; s.keys[0]:\n                i += 1\n            s.children[i].insert_non_full(key)\n            self.root = s\n        else:\n            r.insert_non_full(key)\n\n\nWhy It Matters\n\nDisk-friendly: each node fits into one page\nShallow height: \\(O(\\log_t n)\\) levels → few disk reads\nDeterministic balance: no randomness, always balanced\nFoundation of file systems, databases, indexes (e.g., NTFS, MySQL, PostgreSQL)\n\n\n\nA Gentle Proof (Why It Works)\nEach node has between \\(t-1\\) and \\(2t-1\\) keys (except root).\nEach split increases height only when the root splits.\nThus, height \\(h\\) satisfies:\n\\[\nt^h \\le n \\le (2t)^h\n\\]\nTaking logs:\n\\[\nh = O(\\log_t n)\n\\]\nSo insertions and searches take \\(O(t \\cdot \\log_t n)\\), often simplified to \\(O(\\log n)\\) when \\(t\\) is constant.\n\n\nTry It Yourself\n\nBuild a B-Tree with \\(t=2\\).\nInsert \\([10, 20, 5, 6, 12, 30, 7, 17]\\).\nDraw the tree after each insertion.\nObserve when splits occur and which keys promote upward.\n\n\n\nTest Cases\n\n\n\nInput Keys\nt\nFinal Root\nHeight\n\n\n\n\n[10,20,5,6,12,30,7,17]\n2\n[6,10,20]\n2\n\n\n[1,2,3,4,5,6,7,8,9]\n2\n[4]\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\n\nB-Tree insertion is the heartbeat of external memory algorithms, split, promote, balance, ensuring data stays close, shallow, and sorted.\n\n\n\n275 B-Tree Delete\nDeletion in a B-Tree is more intricate than insertion, we must carefully remove a key while preserving the B-Tree’s balance properties. Every node must maintain at least \\(t - 1\\) keys (except the root), so deletion may involve borrowing from siblings or merging nodes.\nThe goal is to maintain the B-Tree invariants:\n\nKeys sorted within each node\nNode key count between \\(t - 1\\) and \\(2t - 1\\)\nBalanced height\n\n\nWhat Problem Are We Solving?\nWe want to remove a key from a B-Tree without violating balance or occupancy constraints. Unlike binary search trees, where we can simply replace or prune nodes, B-Trees must maintain minimum degree to ensure consistent height and I/O efficiency.\n\n\nHow It Works (Plain Language)\nTo delete a key \\(k\\) from a B-Tree:\n\nIf \\(k\\) is in a leaf node:\n\nSimply remove it.\n\nIf \\(k\\) is in an internal node:\n\nCase A: If left child has ≥ \\(t\\) keys → replace \\(k\\) with predecessor.\nCase B: Else if right child has ≥ \\(t\\) keys → replace \\(k\\) with successor.\nCase C: Else both children have \\(t - 1\\) keys → merge them and recurse.\n\nIf \\(k\\) is not in the current node:\n\nMove to the correct child.\nBefore descending, ensure the child has at least \\(t\\) keys. If not,\n\nBorrow from a sibling with ≥ \\(t\\) keys, or\nMerge with a sibling to guarantee occupancy.\n\n\n\nThis ensures no underflow occurs during traversal.\n\n\nExample Step by Step\nLet \\(t = 2\\) (max 3 keys per node). B-Tree before deletion:\n          [6, 10, 20]\n         /    |     |    \\\n       [5]  [7]  [12,17] [30]\n\n\nDelete key 17\n\n17 is in leaf [12, 17] → remove it directly\n\nResult:\n          [6, 10, 20]\n         /    |     |    \\\n       [5]  [7]   [12]   [30]\n\n\nDelete key 10\n\n10 is in internal node\nLeft child [7] has \\(t - 1 = 1\\) key\nRight child [12] also has 1 key → Merge [7], 10, [12] → [7, 10, 12]\n\nTree becomes:\n       [6, 20]\n      /    |    \\\n    [5] [7,10,12] [30]\n\n\nDelete key 6\n\n6 in internal node\nLeft [5] has 1 key, right [7,10,12] has ≥ 2 → borrow from right\nReplace 6 with successor 7\n\nTree after rebalancing:\n       [7, 20]\n      /    |    \\\n    [5,6] [10,12] [30]\n\n\nVisualization\nEvery deletion keeps the tree balanced by ensuring all nodes (except root) stay ≥ \\(t - 1\\) full.\nBefore:             After Deleting 10:\n$$6,10,20]           [6,20]\n / | | \\             / | \\\n$$5][7][12,17][30]   [5][7,10,12][30]\n\n\nTiny Code (Simplified Python)\nclass BTreeNode:\n    def __init__(self, t, leaf=False):\n        self.t = t\n        self.keys = []\n        self.children = []\n        self.leaf = leaf\n\n    def find_key(self, k):\n        for i, key in enumerate(self.keys):\n            if key &gt;= k:\n                return i\n        return len(self.keys)\n\nclass BTree:\n    def __init__(self, t):\n        self.root = BTreeNode(t, True)\n        self.t = t\n\n    def delete(self, node, k):\n        t = self.t\n        i = node.find_key(k)\n\n        # Case 1: key in node\n        if i &lt; len(node.keys) and node.keys[i] == k:\n            if node.leaf:\n                node.keys.pop(i)\n            else:\n                if len(node.children[i].keys) &gt;= t:\n                    pred = self.get_predecessor(node, i)\n                    node.keys[i] = pred\n                    self.delete(node.children[i], pred)\n                elif len(node.children[i+1].keys) &gt;= t:\n                    succ = self.get_successor(node, i)\n                    node.keys[i] = succ\n                    self.delete(node.children[i+1], succ)\n                else:\n                    self.merge(node, i)\n                    self.delete(node.children[i], k)\n        else:\n            # Case 2: key not in node\n            if node.leaf:\n                return  # not found\n            if len(node.children[i].keys) &lt; t:\n                self.fill(node, i)\n            self.delete(node.children[i], k)\n(Helper methods merge, fill, borrow_from_prev, and borrow_from_next omitted for brevity)\n\n\nWhy It Matters\n\nMaintains balanced height after deletions\nPrevents underflow in child nodes\nEnsures \\(O(\\log n)\\) complexity\nUsed heavily in databases and filesystems where stable performance is critical\n\n\n\nA Gentle Proof (Why It Works)\nA B-Tree node always satisfies:\n\\[\nt - 1 \\le \\text{keys per node} \\le 2t - 1\n\\]\nMerging or borrowing ensures all nodes remain within bounds. The height \\(h\\) satisfies:\n\\[\nh \\le \\log_t n\n\\]\nSo deletions require visiting at most \\(O(\\log_t n)\\) nodes and performing constant-time merges/borrows per level.\nHence:\n\\[\nT_{\\text{delete}} = O(\\log n)\n\\]\n\n\nTry It Yourself\n\nBuild a B-Tree with \\(t = 2\\).\nInsert \\([10, 20, 5, 6, 12, 30, 7, 17]\\).\nDelete keys in order: \\(17, 10, 6\\).\nDraw the tree after each deletion and observe merges/borrows.\n\n\n\nTest Cases\n\n\n\nInput Keys\nDelete\nResult (Level 0)\n\n\n\n\n[5,6,7,10,12,17,20,30]\n17\n[5,6,7,10,12,20,30]\n\n\n[5,6,7,10,12,20,30]\n10\n[5,6,7,12,20,30]\n\n\n[5,6,7,12,20,30]\n6\n[5,7,12,20,30]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nDelete\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\n\nB-Tree deletion is a surgical balancing act, merging, borrowing, and promoting keys just enough to keep the tree compact, shallow, and sorted.\n\n\n\n276 B+ Tree Search\nA B+ Tree is an extension of the B-Tree, optimized for range queries and sequential access. All actual data (records or values) reside in leaf nodes, which are linked together to form a sorted list. Internal nodes contain only keys that guide the search.\nSearching in a B+ Tree follows the same principle as a B-Tree, top-down traversal based on key comparisons, but ends at the leaf level where the actual data is stored.\n\nWhat Problem Are We Solving?\nWe need a disk-friendly search structure that:\n\nKeeps height small (few disk I/Os)\nSupports fast range scans\nSeparates index keys (internal nodes) from records (leaves)\n\nB+ Trees meet these needs with:\n\nHigh fan-out: many keys per node\nLinked leaves: for efficient sequential traversal\nDeterministic balance: height always \\(O(\\log n)\\)\n\n\n\nHow It Works (Plain Language)\nEach internal node acts as a router. Each leaf node contains keys + data pointers.\nTo search for key \\(k\\):\n\nStart at the root.\nAt each internal node, find the child whose key range contains \\(k\\).\nFollow that pointer to the next level.\nContinue until reaching a leaf node.\nPerform a linear scan within the leaf to find \\(k\\).\n\nIf not found in the leaf, \\(k\\) is not in the tree.\n\n\nExample Step by Step\nLet \\(t = 2\\) (each node holds up to 3 keys). B+ Tree:\n          [10 | 20]\n         /     |     \\\n   [1 5 8]  [12 15 18]  [22 25 30]\nSearch for 15:\n\nRoot [10 | 20]: 15 &gt; 10 and &lt; 20 → follow middle pointer\nNode [12 15 18]: found 15\n\nSearch for 17:\n\nRoot [10 | 20]: 17 &gt; 10 and &lt; 20 → middle pointer\nNode [12 15 18]: not found → not in tree\n\n\n\nVisualization\n         [10 | 20]\n        /     |     \\\n [1 5 8] [12 15 18] [22 25 30]\n\nInternal nodes guide the path\nLeaf nodes hold data (and link to next leaf)\nSearch always ends at a leaf\n\n\n\nTiny Code (Simplified Python)\nclass BPlusNode:\n    def __init__(self, t, leaf=False):\n        self.t = t\n        self.leaf = leaf\n        self.keys = []\n        self.children = []\n        self.next = None  # link to next leaf\n\nclass BPlusTree:\n    def __init__(self, t):\n        self.root = BPlusNode(t, True)\n        self.t = t\n\n    def search(self, node, key):\n        if node.leaf:\n            return key in node.keys\n        i = 0\n        while i &lt; len(node.keys) and key &gt;= node.keys[i]:\n            i += 1\n        return self.search(node.children[i], key)\n\n    def find(self, key):\n        return self.search(self.root, key)\n\n\nWhy It Matters\n\nEfficient disk I/O: high branching factor keeps height low\nAll data in leaves: simplifies range queries\nLinked leaves: enable sequential traversal (sorted order)\nUsed in: databases, filesystems, key-value stores (e.g., MySQL, InnoDB, NTFS)\n\n\n\nA Gentle Proof (Why It Works)\nEach internal node has between \\(t\\) and \\(2t\\) children. Each leaf holds between \\(t - 1\\) and \\(2t - 1\\) keys. Thus, height \\(h\\) satisfies:\n\\[\nt^h \\le n \\le (2t)^h\n\\]\nTaking logs:\n\\[\nh = O(\\log_t n)\n\\]\nSo search time is:\n\\[\nT_{\\text{search}} = O(\\log n)\n\\]\nAnd since the final scan within the leaf is constant (small), total cost remains logarithmic.\n\n\nTry It Yourself\n\nBuild a B+ Tree with \\(t = 2\\) and insert keys \\([1, 5, 8, 10, 12, 15, 18, 20, 22, 25, 30]\\).\nSearch for 15, 17, and 8.\nTrace your path from root → internal → leaf.\nObserve that all searches end in leaves.\n\n\n\nTest Cases\n\n\n\nSearch Key\nExpected Result\nPath\n\n\n\n\n15\nFound\nRoot → Middle → Leaf\n\n\n8\nFound\nRoot → Left → Leaf\n\n\n17\nNot Found\nRoot → Middle → Leaf\n\n\n25\nFound\nRoot → Right → Leaf\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\n\nRange Query\n\\(O(\\log n + k)\\)\n\\(O(n)\\)\n\n\n\nB+ Tree Search exemplifies I/O-aware design, every pointer followed is a disk page, every leaf scan is cache-friendly, and every key lives exactly where range queries want it.\n\n\n\n278 B* Tree\nA B* Tree is a refined version of the B-Tree, designed to achieve higher node occupancy and fewer splits. It enforces that each node (except root) must be at least two-thirds full, compared to the half-full guarantee in a standard B-Tree.\nTo achieve this, B* Trees use redistribution between siblings before splitting, which improves space utilization and I/O efficiency, making them ideal for database and file system indexes.\n\nWhat Problem Are We Solving?\nIn a standard B-Tree, each node maintains at least \\(t - 1\\) keys (50% occupancy). But frequent splits can cause fragmentation and wasted space.\nWe want to:\n\nIncrease space efficiency (reduce empty slots)\nDefer splitting when possible\nMaintain balance and sorted order\n\nB* Trees solve this by borrowing and redistributing keys between siblings before splitting, ensuring \\(\\ge 2/3\\) occupancy.\n\n\nHow It Works (Plain Language)\nA B* Tree works like a B-Tree but with smarter split logic:\n\nInsertion path:\n\nTraverse top-down to find target leaf.\nIf the target node is full, check its sibling.\n\nRedistribution step:\n\nIf sibling has room, redistribute keys between them and the parent key.\n\nDouble split step:\n\nIf both siblings are full, split both into three nodes (two full + one new node), redistributing keys evenly among them.\n\n\nThis ensures every node (except root) is at least 2/3 full, leading to better disk utilization.\n\n\nExample Step by Step\nLet \\(t = 2\\) (max 3 keys per node). Insert keys: \\([5, 10, 15, 20, 25, 30, 35]\\)\nStep 1–4: Build like B-Tree until root [10, 20].\n        [10 | 20]\n       /    |    \\\n   [5]   [15]   [25, 30, 35]\nNow insert 40 → rightmost node [25, 30, 35] is full.\n\nCheck sibling: left sibling [15] has room → redistribute keys Combine [15], [25, 30, 35], and parent key 20 → [15, 20, 25, 30, 35] Split into three nodes evenly: [15, 20], [25, 30], [35] Parent updated with new separator.\n\nResult:\n         [20 | 30]\n        /     |     \\\n    [5,10,15] [20,25] [30,35,40]\nEach node ≥ 2/3 full, no wasted space.\n\n\nVisualization\n          [20 | 30]\n         /     |     \\\n [5 10 15] [20 25] [30 35 40]\nRedistribution ensures balance and density before splitting.\n\n\nTiny Code (Simplified Pseudocode)\ndef insert_bstar(tree, key):\n    node = find_leaf(tree.root, key)\n    if node.full():\n        sibling = node.get_sibling()\n        if sibling and not sibling.full():\n            redistribute(node, sibling, tree.parent(node))\n        else:\n            split_three(node, sibling, tree.parent(node))\n    insert_into_leaf(node, key)\n(Actual implementation is more complex, involving parent updates and sibling pointers.)\n\n\nWhy It Matters\n\nBetter space utilization: nodes ≥ 66% full\nFewer splits: more stable performance under heavy inserts\nImproved I/O locality: fewer disk blocks accessed\nUsed in: database systems (IBM DB2), file systems (ReiserFS), B*-based caching structures\n\n\n\nA Gentle Proof (Why It Works)\nIn B* Trees:\n\nEach non-root node contains \\(\\ge \\frac{2}{3} (2t - 1)\\) keys.\nHeight \\(h\\) satisfies:\n\n\\[\n\\left( \\frac{3}{2} t \\right)^h \\le n\n\\]\nTaking logs:\n\\[\nh = O(\\log_t n)\n\\]\nThus, height remains logarithmic, but nodes pack more data per level.\nFewer levels → fewer I/Os → better performance.\n\n\nTry It Yourself\n\nBuild a B* Tree with \\(t = 2\\).\nInsert keys: \\([5, 10, 15, 20, 25, 30, 35, 40]\\).\nWatch how redistribution occurs before splits.\nCompare with B-Tree splits for same sequence.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nInput Keys\nt\nResult\nNotes\n\n\n\n\n[5,10,15,20,25]\n2\nRoot [15]\nFull utilization\n\n\n[5,10,15,20,25,30,35]\n2\nRoot [20,30]\nRedistribution before split\n\n\n[1..15]\n2\nBalanced, 2/3 full nodes\nHigh density\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nOccupancy\n\n\n\n\nSearch\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\\(\\ge 66%\\)\n\n\nInsert\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\\(\\ge 66%\\)\n\n\nDelete\n\\(O(\\log n)\\)\n\\(O(n)\\)\n\\(\\ge 66%\\)\n\n\n\nB* Trees take the elegance of B-Trees and push them closer to perfect, fewer splits, denser nodes, and smoother scaling for large datasets.\n\n\n\n279 Adaptive Radix Tree\nAn Adaptive Radix Tree (ART) is a space-efficient, cache-friendly data structure that combines ideas from tries and radix trees. It dynamically adapts its node representation based on the number of children, optimizing both memory usage and lookup speed.\nUnlike a fixed-size radix tree (which wastes space with sparse nodes), ART chooses a compact node type (like Node4, Node16, Node48, Node256) depending on occupancy, growing as needed.\n\nWhat Problem Are We Solving?\nStandard tries and radix trees are fast but memory-heavy. If keys share long prefixes, many nodes hold only one child, wasting memory.\nWe want a structure that:\n\nKeeps O(L) lookup time (L = key length)\nAdapts node size to occupancy\nMinimizes pointer overhead\nExploits cache locality\n\nART achieves this by dynamically switching node types as the number of children grows.\n\n\nHow It Works (Plain Language)\nEach internal node in ART can be one of four types:\n\n\n\nNode Type\nCapacity\nDescription\n\n\n\n\nNode4\n4 children\nsmallest, uses linear search\n\n\nNode16\n16 children\nsmall array, vectorized search\n\n\nNode48\n48 children\nindex map, stores child pointers\n\n\nNode256\n256 children\ndirect addressing by byte value\n\n\n\nKeys are processed byte by byte, branching at each level. When a node fills beyond its capacity, it upgrades to the next node type.\n\n\nExample\nInsert keys: [\"A\", \"AB\", \"AC\", \"AD\", \"AE\"]\n\nStart with root Node4 (can store 4 children).\nAfter inserting “AE”, Node4 exceeds capacity → upgrade to Node16.\nChildren remain in sorted order by key byte.\n\nThis adaptive upgrade keeps nodes dense and efficient.\n\n\nExample Step by Step\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nNode Type\nKeys Stored\nNote\n\n\n\n\n1\nInsert “A”\nNode4\nA\nCreate root\n\n\n2\nInsert “AB”\nNode4\nA, AB\nAdd branch\n\n\n3\nInsert “AC”\nNode4\nA, AB, AC\nStill under 4\n\n\n4\nInsert “AD”\nNode4\nA, AB, AC, AD\nFull\n\n\n5\nInsert “AE”\nUpgrade to Node16\nA, AB, AC, AD, AE\nAdaptive growth\n\n\n\n\n\nVisualization\nRoot (Node16)\n ├── 'A' → Node\n      ├── 'B' (Leaf)\n      ├── 'C' (Leaf)\n      ├── 'D' (Leaf)\n      └── 'E' (Leaf)\nEach node type adapts its layout for the best performance.\n\n\nTiny Code (Simplified Pseudocode)\nclass Node:\n    def __init__(self):\n        self.children = {}\n\ndef insert_art(root, key):\n    node = root\n    for byte in key:\n        if byte not in node.children:\n            node.children[byte] = Node()\n        node = node.children[byte]\n    node.value = True\n(A real ART dynamically switches between Node4, Node16, Node48, Node256 representations.)\n\n\nWhy It Matters\n\nAdaptive memory use, no wasted space for sparse nodes\nCache-friendly, contiguous memory layout\nFast lookups, vectorized search for Node16\nUsed in modern databases (e.g., HyPer, Umbra, DuckDB)\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(L\\) = key length, \\(b\\) = branching factor (max 256 per byte). In a naive trie, each node allocates \\(O(b)\\) slots, many unused.\nIn ART:\n\nEach node stores only actual children, so \\[\n\\text{space} \\approx O(n + L)\n\\]\nLookup remains \\(O(L)\\) since we traverse one node per byte.\nSpace improves by factor proportional to sparsity.\n\nThus ART maintains trie-like performance with hash table-like compactness.\n\n\nTry It Yourself\n\nInsert [\"dog\", \"dot\", \"door\", \"dorm\"]\nObserve how Node4 → Node16 transitions happen\nCount number of nodes, compare with naive trie\nMeasure memory usage and access speed\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nKeys\nResulting Root Type\nNotes\n\n\n\n\n[\"a\", \"b\"]\nNode4\n2 children\n\n\n[\"a\", \"b\", \"c\", \"d\", \"e\"]\nNode16\nUpgrade after 5th insert\n\n\n[\"aa\", \"ab\", \"ac\"... \"az\"]\nNode48 or Node256\nDense branching\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nAdaptive Behavior\n\n\n\n\nSearch\n\\(O(L)\\)\n\\(O(n + L)\\)\nNode grows/shrinks\n\n\nInsert\n\\(O(L)\\)\n\\(O(n)\\)\nNode type upgrade\n\n\nDelete\n\\(O(L)\\)\n\\(O(n)\\)\nDowngrade if sparse\n\n\n\nAn Adaptive Radix Tree gives the best of both worlds: prefix compression of tries and space efficiency of hash maps, a modern weapon for high-performance indexing.\n\n\n\n280 Trie Compression\nA compressed trie (also called a radix tree or Patricia trie) is an optimized form of a trie where chains of single-child nodes are merged into a single edge. Instead of storing one character per node, each edge can hold an entire substring.\nThis reduces the height of the trie, minimizes memory usage, and accelerates searches, perfect for applications like prefix lookup, routing tables, and dictionary storage.\n\nWhat Problem Are We Solving?\nA naive trie wastes space when many nodes have only one child.\nFor example, inserting [\"cat\", \"car\", \"dog\"] into a naive trie yields long, skinny paths:\nc → a → t  \nc → a → r  \nd → o → g\nWe can compress those linear chains into edges labeled with substrings:\nc → a → \"t\"  \nc → a → \"r\"  \nd → \"og\"\nThis saves memory and reduces traversal depth.\n\n\nHow It Works (Plain Language)\nThe key idea is path compression: whenever a node has a single child, merge them into one edge containing the combined substring.\n\n\n\n\n\n\n\n\nStep\nOperation\nResult\n\n\n\n\n1\nBuild a normal trie\nOne character per edge\n\n\n2\nTraverse each path\nIf node has one child, merge\n\n\n3\nReplace chain with a substring edge\nFewer nodes, shorter height\n\n\n\nCompressed tries store edge labels as substrings rather than single characters.\n\n\nExample\nInsert [\"bear\", \"bell\", \"bid\", \"bull\", \"buy\"]\n\nStart with naive trie.\nIdentify single-child paths.\nMerge paths:\n\nb\n ├── e → \"ar\"\n │    └── \"ll\"\n └── u → \"ll\"\n      └── \"y\"\nEach edge now carries a substring rather than a single letter.\n\n\nExample Step by Step\n\n\n\n\n\n\n\n\n\nStep\nInsert\nAction\nResult\n\n\n\n\n1\n“bear”\nCreate path b-e-a-r\n4 nodes\n\n\n2\n“bell”\nShares prefix “be”\nMerge prefix\n\n\n3\n“bid”\nNew branch at “b”\nAdd new edge\n\n\n4\nCompress single-child paths\nReplace edges with substrings\n\n\n\n\n\n\nTiny Code (Simplified Pseudocode)\nclass Node:\n    def __init__(self):\n        self.children = {}\n        self.is_end = False\n\ndef insert_trie_compressed(root, word):\n    node = root\n    i = 0\n    while i &lt; len(word):\n        for edge, child in node.children.items():\n            prefix_len = common_prefix(edge, word[i:])\n            if prefix_len &gt; 0:\n                if prefix_len &lt; len(edge):\n                    # Split edge\n                    remainder = edge[prefix_len:]\n                    new_node = Node()\n                    new_node.children[remainder] = child\n                    node.children[word[i:i+prefix_len]] = new_node\n                    del node.children[edge]\n                node = node.children[word[i:i+prefix_len]]\n                i += prefix_len\n                break\n        else:\n            node.children[word[i:]] = Node()\n            node.children[word[i:]].is_end = True\n            break\n    node.is_end = True\nThis simplified version merges edges whenever possible.\n\n\nWhy It Matters\n\nSaves memory by merging chains\nFaster search (fewer hops per lookup)\nIdeal for prefix-based queries\nUsed in routing tables, autocomplete systems, and dictionaries\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n\\) be the total length of all keys and \\(k\\) the number of keys.\n\nA naive trie can have up to \\(O(n)\\) nodes.\nA compressed trie has at most \\(k - 1\\) internal nodes and \\(k\\) leaves, since each branching point corresponds to a unique prefix shared by at least two keys.\n\nThus, compressed tries reduce both height and node count:\n\\[\nO(n) \\text{ nodes (naive)} \\quad \\to \\quad O(k) \\text{ nodes (compressed)}\n\\]\nSearch and insert remain \\(O(L)\\), where \\(L\\) is key length, but with fewer steps.\n\n\nTry It Yourself\n\nInsert [\"car\", \"cat\", \"cart\", \"dog\"]\nDraw both naive and compressed tries\nCount number of nodes before and after compression\nVerify edge labels as substrings\n\n\n\nTest Cases\n\n\n\nKeys\nNaive Trie Nodes\nCompressed Trie Nodes\n\n\n\n\n[\"a\", \"b\"]\n3\n3\n\n\n[\"apple\", \"app\"]\n6\n4\n\n\n[\"abc\", \"abd\", \"aef\"]\n8\n6\n\n\n[\"car\", \"cart\", \"cat\"]\n9\n6\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nSearch\n\\(O(L)\\)\n\\(O(k)\\)\n\\(L\\) = key length\n\n\nInsert\n\\(O(L)\\)\n\\(O(k)\\)\nSplit edges when needed\n\n\nDelete\n\\(O(L)\\)\n\\(O(k)\\)\nMerge edges if path shrinks\n\n\n\nA compressed trie elegantly blends trie structure and path compression, turning long chains into compact edges, a key step toward efficient prefix trees, routing tables, and tries for text indexing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-29.-persistent-and-functional-data-structures",
    "href": "books/en-us/list-3.html#section-29.-persistent-and-functional-data-structures",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 29. Persistent and Functional Data Structures",
    "text": "Section 29. Persistent and Functional Data Structures\n\n281 Persistent Stack\nA persistent stack is a versioned data structure that remembers all its past states. Instead of overwriting data, every push or pop operation creates a new version of the stack while keeping access to the old ones.\nThis concept is part of functional data structures, where immutability and version history are first-class citizens.\n\nWhat Problem Are We Solving?\nIn traditional stacks, each operation mutates the structure, old versions are lost.\nPersistent stacks solve this by allowing:\n\nAccess to previous states at any time\nUndo or time-travel features\nPurely functional programs where data is never mutated\n\nUsed in compilers, backtracking systems, and functional programming languages.\n\n\nHow It Works (Plain Language)\nA stack is a linked list:\n\npush(x) adds a new head node\npop() returns the next node\n\nFor persistence, we never modify nodes, instead, each operation creates a new head pointing to existing tails.\n\n\n\nVersion\nOperation\nTop Element\nStructure\n\n\n\n\nv0\nempty\n,\n∅\n\n\nv1\npush(10)\n10\n10 → ∅\n\n\nv2\npush(20)\n20\n20 → 10 → ∅\n\n\nv3\npop()\n10\n10 → ∅\n\n\n\nEach version reuses previous nodes, no data copying.\n\n\nExample\nStart with an empty stack v0:\n\nv1 = push(v0, 10) → stack [10]\nv2 = push(v1, 20) → stack [20, 10]\nv3 = pop(v2) → returns 20, new stack [10]\n\nNow we have three accessible versions:\nv0: ∅  \nv1: 10  \nv2: 20 → 10  \nv3: 10  \n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, value, next_node=None):\n        self.value = value\n        self.next = next_node\n\nclass PersistentStack:\n    def __init__(self, top=None):\n        self.top = top\n\n    def push(self, value):\n        # new node points to current top\n        return PersistentStack(Node(value, self.top))\n\n    def pop(self):\n        if not self.top:\n            return self, None\n        return PersistentStack(self.top.next), self.top.value\n\n    def peek(self):\n        return None if not self.top else self.top.value\n\n# Example\nv0 = PersistentStack()\nv1 = v0.push(10)\nv2 = v1.push(20)\nv3, popped = v2.pop()\n\nprint(v2.peek())  # 20\nprint(v3.peek())  # 10\nThis approach reuses nodes, creating new versions without mutation.\n\n\nTiny Code (C, Conceptual)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int value;\n    struct Node* next;\n} Node;\n\ntypedef struct {\n    Node* top;\n} Stack;\n\nStack* push(Stack* s, int value) {\n    Node* node = malloc(sizeof(Node));\n    node-&gt;value = value;\n    node-&gt;next = s-&gt;top;\n    Stack* new_stack = malloc(sizeof(Stack));\n    new_stack-&gt;top = node;\n    return new_stack;\n}\n\nStack* pop(Stack* s, int* popped_value) {\n    if (!s-&gt;top) return s;\n    *popped_value = s-&gt;top-&gt;value;\n    Stack* new_stack = malloc(sizeof(Stack));\n    new_stack-&gt;top = s-&gt;top-&gt;next;\n    return new_stack;\n}\nEvery push or pop creates a new Stack* that points to the previous structure.\n\n\nWhy It Matters\n\nImmutability ensures data safety and concurrency-friendly design\nVersioning allows backtracking, undo, or branching computations\nFoundation for functional programming and persistent data stores\n\n\n\nA Gentle Proof (Why It Works)\nEach version of the stack shares unchanged nodes with previous versions. Because push and pop only modify the head reference, older versions remain intact.\nIf \\(n\\) is the total number of operations,\n\nEach new version adds \\(O(1)\\) space\nShared tails ensure total space = \\(O(n)\\)\n\nMathematically:\n\\[\nS_n = S_{n-1} + O(1)\n\\]\nand old versions never get overwritten, ensuring persistence.\n\n\nTry It Yourself\n\nBuild a persistent stack with values [1, 2, 3]\nPop once, and confirm earlier versions still have their values\nCompare with a mutable stack implementation\nVisualize the shared linked nodes between versions\n\n\n\nTest Cases\n\n\n\nOperation\nResult\nNotes\n\n\n\n\nv1 = push(v0, 10)\n[10]\nnew version\n\n\nv2 = push(v1, 20)\n[20, 10]\nshares 10-node\n\n\nv3, val = pop(v2)\nval = 20, [10]\nold v2 intact\n\n\nv1.peek()\n10\nunaffected by later pops\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nPush\n\\(O(1)\\)\n\\(O(1)\\)\nnew head\n\n\nPop\n\\(O(1)\\)\n\\(O(1)\\)\nnew top pointer\n\n\nAccess old version\n\\(O(1)\\)\n,\nstore reference\n\n\n\nA persistent stack elegantly combines immutability, sharing, and time-travel, a small but powerful step into the world of functional data structures.\n\n\n\n282 Persistent Array\nA persistent array is an immutable, versioned structure that allows access to all past states. Instead of overwriting elements, every update creates a new version that shares most of its structure with previous ones.\nThis makes it possible to “time travel”, view or restore any earlier version in constant or logarithmic time, without copying the entire array.\n\nWhat Problem Are We Solving?\nA normal array is mutable, every arr[i] = x destroys the old value. If we want history, undo, or branching computation, this is unacceptable.\nA persistent array keeps all versions:\n\n\n\nVersion\nOperation\nState\n\n\n\n\nv0\n[]\nempty\n\n\nv1\nset(0, 10)\n[10]\n\n\nv2\nset(0, 20)\n[20] (v1 still [10])\n\n\n\nEach version reuses unmodified parts of the array, avoiding full duplication.\n\n\nHow It Works (Plain Language)\nA persistent array can be implemented using copy-on-write or tree-based structures.\n\nCopy-on-Write (Small Arrays)\n\nCreate a new array copy only when an element changes.\nSimple but \\(O(n)\\) update cost.\n\nPath Copying with Trees (Large Arrays)\n\nRepresent the array as a balanced binary tree (like a segment tree).\nEach update copies only the path to the changed leaf.\nSpace per update = \\(O(\\log n)\\)\n\n\nSo, each version points to a root node. When you modify index \\(i\\), a new path is created down the tree, while untouched subtrees are shared.\n\n\nExample\nLet’s build a persistent array of size 4.\n\n\nStep 1: Initial version\nv0 = [0, 0, 0, 0]\n\n\nStep 2: Update index 2\nv1 = set(v0, 2, 5)  → [0, 0, 5, 0]\n\n\nStep 3: Update index 1\nv2 = set(v1, 1, 9)  → [0, 9, 5, 0]\nv0, v1, and v2 all coexist independently.\n\n\nExample Step-by-Step (Tree Representation)\nEach node covers a range:\nRoot: [0..3]\n  ├── Left [0..1]\n  │     ├── [0] → 0\n  │     └── [1] → 9\n  └── Right [2..3]\n        ├── [2] → 5\n        └── [3] → 0\nUpdating index 1 copies only the path [0..3] → [0..1] → [1], not the entire tree.\n\n\nTiny Code (Python, Tree-based)\nclass Node:\n    def __init__(self, left=None, right=None, value=0):\n        self.left = left\n        self.right = right\n        self.value = value\n\ndef build(l, r):\n    if l == r:\n        return Node()\n    m = (l + r) // 2\n    return Node(build(l, m), build(m+1, r))\n\ndef update(node, l, r, idx, val):\n    if l == r:\n        return Node(value=val)\n    m = (l + r) // 2\n    if idx &lt;= m:\n        return Node(update(node.left, l, m, idx, val), node.right)\n    else:\n        return Node(node.left, update(node.right, m+1, r, idx, val))\n\ndef query(node, l, r, idx):\n    if l == r:\n        return node.value\n    m = (l + r) // 2\n    return query(node.left, l, m, idx) if idx &lt;= m else query(node.right, m+1, r, idx)\n\n# Example\nn = 4\nv0 = build(0, n-1)\nv1 = update(v0, 0, n-1, 2, 5)\nv2 = update(v1, 0, n-1, 1, 9)\n\nprint(query(v2, 0, n-1, 2))  # 5\nprint(query(v1, 0, n-1, 1))  # 0\nEach update creates a new version root.\n\n\nWhy It Matters\n\nTime-travel debugging: retrieve old states\nUndo/redo systems in editors\nBranching computations in persistent algorithms\nFunctional programming without mutation\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n\\) be array size, \\(u\\) number of updates.\nEach update copies \\(O(\\log n)\\) nodes. So total space:\n\\[\nO(n + u \\log n)\n\\]\nEach query traverses one path \\(O(\\log n)\\). No version ever invalidates another, all roots remain accessible.\nPersistence holds because we never mutate existing nodes, only allocate new ones and reuse subtrees.\n\n\nTry It Yourself\n\nBuild an array of size 8, all zeros.\nCreate v1 = set index 4 → 7\nCreate v2 = set index 2 → 9\nPrint values from v0, v1, v2\nConfirm that old versions remain unchanged.\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\nNotes\n\n\n\n\nbuild(4)\n[0,0,0,0]\nv0\nbase\n\n\nset(v0,2,5)\n\n[0,0,5,0]\nnew version\n\n\nset(v1,1,9)\n\n[0,9,5,0]\nv1 reused\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nBuild\n\\(O(n)\\)\n\\(O(n)\\)\ninitial\n\n\nUpdate\n\\(O(\\log n)\\)\n\\(O(\\log n)\\)\npath copy\n\n\nQuery\n\\(O(\\log n)\\)\n,\none path\n\n\nAccess Old Version\n\\(O(1)\\)\n,\nroot reference\n\n\n\nA persistent array turns ephemeral memory into a versioned timeline, each change is a branch, every version eternal. Perfect for functional programming, debugging, and algorithmic history.\n\n\n\n283 Persistent Segment Tree\nA persistent segment tree is a versioned data structure that supports range queries and point updates, while keeping every past version accessible.\nIt’s a powerful combination of segment trees and persistence, allowing you to query historical states, perform undo operations, and even compare past and present results efficiently.\n\nWhat Problem Are We Solving?\nA standard segment tree allows:\n\nPoint updates: arr[i] = x\nRange queries: sum(l, r) or min(l, r)\n\nBut every update overwrites old values. A persistent segment tree solves this by creating a new version on each update, reusing unchanged nodes.\n\n\n\nVersion\nOperation\nState\n\n\n\n\nv0\nbuild\n[1, 2, 3, 4]\n\n\nv1\nupdate(2, 5)\n[1, 2, 5, 4]\n\n\nv2\nupdate(1, 7)\n[1, 7, 5, 4]\n\n\n\nNow you can query any version:\n\nquery(v0, 1, 3) → old sum\nquery(v2, 1, 3) → updated sum\n\n\n\nHow It Works (Plain Language)\nA segment tree is a binary tree where each node stores an aggregate (sum, min, max) over a segment.\nPersistence is achieved by path copying:\n\nWhen updating index i, only nodes on the path from root to leaf are replaced.\nAll other nodes are shared between versions.\n\nSo each new version costs \\(O(\\log n)\\) nodes and space.\n\n\n\nStep\nOperation\nAffected Nodes\n\n\n\n\n1\nBuild\n\\(O(n)\\) nodes\n\n\n2\nUpdate(2, 5)\n\\(O(\\log n)\\) new nodes\n\n\n3\nUpdate(1, 7)\n\\(O(\\log n)\\) new nodes\n\n\n\n\n\nExample\nLet initial array be [1, 2, 3, 4]\n\nBuild tree (v0)\nv1 = update(v0, 2 → 5)\nv2 = update(v1, 1 → 7)\n\nNow:\n\nquery(v0, 1, 4) = 10\nquery(v1, 1, 4) = 12\nquery(v2, 1, 4) = 17\n\nAll versions share most nodes, saving memory.\n\n\nExample Step-by-Step\n\n\nUpdate v0 → v1 at index 2\n\n\n\nVersion\nTree Nodes Copied\nShared\n\n\n\n\nv1\nPath [root → left → right]\nOthers unchanged\n\n\n\nSo v1 differs only along one path.\n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef build(arr, l, r):\n    if l == r:\n        return Node(arr[l])\n    m = (l + r) // 2\n    left = build(arr, l, m)\n    right = build(arr, m + 1, r)\n    return Node(left.val + right.val, left, right)\n\ndef update(node, l, r, idx, val):\n    if l == r:\n        return Node(val)\n    m = (l + r) // 2\n    if idx &lt;= m:\n        left = update(node.left, l, m, idx, val)\n        return Node(left.val + node.right.val, left, node.right)\n    else:\n        right = update(node.right, m + 1, r, idx, val)\n        return Node(node.left.val + right.val, node.left, right)\n\ndef query(node, l, r, ql, qr):\n    if qr &lt; l or ql &gt; r:\n        return 0\n    if ql &lt;= l and r &lt;= qr:\n        return node.val\n    m = (l + r) // 2\n    return query(node.left, l, m, ql, qr) + query(node.right, m + 1, r, ql, qr)\n\n# Example\narr = [1, 2, 3, 4]\nv0 = build(arr, 0, 3)\nv1 = update(v0, 0, 3, 2, 5)\nv2 = update(v1, 0, 3, 1, 7)\n\nprint(query(v0, 0, 3, 0, 3))  # 10\nprint(query(v1, 0, 3, 0, 3))  # 12\nprint(query(v2, 0, 3, 0, 3))  # 17\n\n\nWhy It Matters\n\nAccess any version instantly\nEnables time-travel queries\nSupports immutable analytics\nUsed in offline queries, competitive programming, and functional databases\n\n\n\nA Gentle Proof (Why It Works)\nEach update only modifies \\(O(\\log n)\\) nodes. All other subtrees are shared, so total space:\n\\[\nS(u) = O(n + u \\log n)\n\\]\nQuerying any version costs \\(O(\\log n)\\) since only one path is traversed.\nPersistence holds since no nodes are mutated, only replaced.\n\n\nTry It Yourself\n\nBuild [1, 2, 3, 4]\nUpdate index 2 → 5 (v1)\nUpdate index 1 → 7 (v2)\nQuery sum(1, 4) in v0, v1, v2\nVerify shared subtrees via visualization\n\n\n\nTest Cases\n\n\n\nVersion\nOperation\nQuery(0,3)\nNotes\n\n\n\n\nv0\n[1,2,3,4]\n10\nbase\n\n\nv1\nset(2,5)\n12\nchanged one leaf\n\n\nv2\nset(1,7)\n17\nanother update\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nBuild\n\\(O(n)\\)\n\\(O(n)\\)\nfull tree\n\n\nUpdate\n\\(O(\\log n)\\)\n\\(O(\\log n)\\)\npath copy\n\n\nQuery\n\\(O(\\log n)\\)\n,\none path\n\n\nVersion Access\n\\(O(1)\\)\n,\nvia root\n\n\n\nA persistent segment tree is your immutable oracle, each version a snapshot in time, forever queryable, forever intact.\n\n\n\n284 Persistent Linked List\nA persistent linked list is a versioned variant of the classic singly linked list, where every insertion or deletion produces a new version without destroying the old one.\nEach version represents a distinct state of the list, and all versions coexist by sharing structure, unchanged nodes are reused, only the modified path is copied.\nThis technique is core to functional programming, undo systems, and immutable data structures.\n\nWhat Problem Are We Solving?\nA mutable linked list loses its history after every change.\nWith persistence, we preserve all past versions:\n\n\n\nVersion\nOperation\nList\n\n\n\n\nv0\nempty\n∅\n\n\nv1\npush_front(10)\n[10]\n\n\nv2\npush_front(20)\n[20, 10]\n\n\nv3\npop_front()\n[10]\n\n\n\nEach version is a first-class citizen, you can traverse, query, or compare any version at any time.\n\n\nHow It Works (Plain Language)\nEach node in a singly linked list has:\n\nvalue\nnext pointer\n\nFor persistence, we never mutate nodes. Instead, operations return a new head:\n\npush_front(x): create a new node n = Node(x, old_head)\npop_front(): return old_head.next as new head\n\nAll old nodes remain intact and shared.\n\n\n\nOperation\nNew Node\nShared Structure\n\n\n\n\npush_front(20)\nnew head\ntail reused\n\n\npop_front()\nnew head (next)\nold head still exists\n\n\n\n\n\nExample\n\n\nStep-by-step versioning\n\nv0 = []\nv1 = push_front(v0, 10) → [10]\nv2 = push_front(v1, 20) → [20, 10]\nv3 = pop_front(v2) → [10]\n\nVersions:\nv0: ∅  \nv1: 10  \nv2: 20 → 10  \nv3: 10  \nAll coexist and share structure.\n\n\nExample (Graph View)\nv0: ∅\nv1: 10 → ∅\nv2: 20 → 10 → ∅\nv3: 10 → ∅\nNotice: v2.tail is reused from v1.\n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, value, next_node=None):\n        self.value = value\n        self.next = next_node\n\nclass PersistentList:\n    def __init__(self, head=None):\n        self.head = head\n\n    def push_front(self, value):\n        new_node = Node(value, self.head)\n        return PersistentList(new_node)\n\n    def pop_front(self):\n        if not self.head:\n            return self, None\n        return PersistentList(self.head.next), self.head.value\n\n    def to_list(self):\n        result, curr = [], self.head\n        while curr:\n            result.append(curr.value)\n            curr = curr.next\n        return result\n\n# Example\nv0 = PersistentList()\nv1 = v0.push_front(10)\nv2 = v1.push_front(20)\nv3, popped = v2.pop_front()\n\nprint(v1.to_list())  # [10]\nprint(v2.to_list())  # [20, 10]\nprint(v3.to_list())  # [10]\n\n\nTiny Code (C, Conceptual)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Node {\n    int value;\n    struct Node* next;\n} Node;\n\ntypedef struct {\n    Node* head;\n} PList;\n\nPList push_front(PList list, int value) {\n    Node* new_node = malloc(sizeof(Node));\n    new_node-&gt;value = value;\n    new_node-&gt;next = list.head;\n    PList new_list = { new_node };\n    return new_list;\n}\n\nPList pop_front(PList list, int* popped) {\n    if (!list.head) return list;\n    *popped = list.head-&gt;value;\n    PList new_list = { list.head-&gt;next };\n    return new_list;\n}\nNo mutations, only new nodes allocated.\n\n\nWhy It Matters\n\nImmutable, perfect for functional programs\nUndo / Time-travel, revisit old versions\nSafe concurrency, no data races\nMemory-efficient, tail sharing reuses old structure\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n\\) be number of operations. Each push_front or pop_front creates at most one new node.\nThus:\n\nTotal space after \\(n\\) ops: \\(O(n)\\)\nTime per operation: \\(O(1)\\)\n\nPersistence is guaranteed since no node is modified in place.\nAll versions share the unchanged suffix:\n\\[\nv_k = \\text{Node}(x_k, v_{k-1})\n\\]\nHence, structure sharing is linear and safe.\n\n\nTry It Yourself\n\nStart with empty list\nPush 3 → Push 2 → Push 1\nPop once\nPrint all versions\nObserve how tails are shared\n\n\n\nTest Cases\n\n\n\nOperation\nInput\nOutput\nVersion\n\n\n\n\npush_front\n10\n[10]\nv1\n\n\npush_front\n20\n[20, 10]\nv2\n\n\npop_front\n,\n[10]\nv3\n\n\nto_list\nv1\n[10]\n,\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\npush_front\n\\(O(1)\\)\n\\(O(1)\\)\nnew head only\n\n\npop_front\n\\(O(1)\\)\n\\(O(1)\\)\nreuse next\n\n\naccess\n\\(O(n)\\)\n,\nsame as linked list\n\n\n\nA persistent linked list is the simplest gateway to persistence, each operation is \\(O(1)\\), each version immortal. It’s the backbone of functional stacks, queues, and immutable collections.\n\n\n\n286 Finger Tree\nA finger tree is a versatile, persistent data structure that provides amortized O(1) access to both ends (front and back) and O(log n) access or updates in the middle.\nIt’s a functional, immutable sequence structure, a balanced tree augmented with fingers (fast access points) to its ends. Finger trees form the foundation for many persistent data types, such as queues, deques, priority sequences, and even rope-like text editors.\n\nWhat Problem Are We Solving?\nImmutable lists are fast at the front but slow at the back. Immutable arrays are the opposite. Deques with persistence are hard to maintain efficiently.\nWe want:\n\n\\(O(1)\\) front access\n\\(O(1)\\) back access\n\\(O(\\log n)\\) middle access\nPersistence and immutability\n\nFinger trees achieve this by combining shallow digit buffers at the edges with balanced nodes in the middle.\n\n\nHow It Works (Plain Language)\nA finger tree is built recursively:\nFingerTree = Empty\n           | Single(a)\n           | Deep(prefix, deeper_tree, suffix)\n\nprefix and suffix: small arrays (digits) with 1–4 elements\ndeeper_tree: recursively holds nodes of higher rank\n\nThe fingers (prefix/suffix) give constant-time access to both ends. Insertions push elements into digits; when full, they roll into the deeper tree.\n\n\nExample\nInsert 1, 2, 3, 4, 5:\nDeep [1,2] (Node [3,4]) [5]\nYou can:\n\nPush front → prepend into prefix\nPush back → append into suffix\nAccess ends in O(1)\nInsert middle → recurse into deeper tree (O(log n))\n\nEach operation returns a new version sharing unchanged subtrees.\n\n\nExample State\n\n\n\nOperation\nStructure\nNotes\n\n\n\n\nempty\nEmpty\nbase\n\n\npush_front(1)\nSingle(1)\none element\n\n\npush_front(2)\nDeep [2] Empty [1]\ntwo ends\n\n\npush_back(3)\nDeep [2] Empty [1,3]\nadd suffix\n\n\npush_back(4)\nDeep [2,4] Empty [1,3]\nbalanced growth\n\n\n\nEach version reuses most of its structure, ensuring persistence.\n\n\nTiny Code (Python – Conceptual)\nThis is a simplified model, not a complete implementation (real finger trees rely on more type-level machinery).\nclass Empty:\n    pass\n\nclass Single:\n    def __init__(self, value):\n        self.value = value\n\nclass Deep:\n    def __init__(self, prefix, middle, suffix):\n        self.prefix = prefix\n        self.middle = middle\n        self.suffix = suffix\n\ndef push_front(tree, x):\n    if isinstance(tree, Empty):\n        return Single(x)\n    if isinstance(tree, Single):\n        return Deep([x], Empty(), [tree.value])\n    if len(tree.prefix) &lt; 4:\n        return Deep([x] + tree.prefix, tree.middle, tree.suffix)\n    else:\n        # roll prefix into deeper tree\n        new_middle = push_front(tree.middle, tree.prefix)\n        return Deep([x], new_middle, tree.suffix)\n\ndef to_list(tree):\n    if isinstance(tree, Empty):\n        return []\n    if isinstance(tree, Single):\n        return [tree.value]\n    return tree.prefix + to_list(tree.middle) + tree.suffix\nThis captures the core recursive flavor, constant-time fingers, logarithmic recursion.\n\n\nWhy It Matters\n\nGeneric framework for sequences\nAmortized O(1) insertion/removal at both ends\nO(log n) concatenation, split, or search\nBasis for:\n\nFunctional deques\nPriority queues\nOrdered sequences (like RRB-trees)\nIncremental editors\n\n\n\n\nA Gentle Proof (Why It Works)\nDigits store up to 4 elements, guaranteeing bounded overhead. Each recursive step reduces the size by a constant factor, ensuring depth = \\(O(\\log n)\\).\nFor each operation:\n\nEnds touched in \\(O(1)\\)\nStructural changes at depth \\(O(\\log n)\\)\n\nThus, total cost:\n\\[\nT(n) = O(\\log n), \\quad \\text{amortized O(1) at ends}\n\\]\nPersistence is ensured because all updates build new nodes without modifying existing ones.\n\n\nTry It Yourself\n\nStart with empty tree.\nPush 1, 2, 3, 4.\nPop front, observe structure.\nPush 5, inspect sharing between versions.\nConvert each version to list, compare results.\n\n\n\nTest Cases\n\n\n\nOperation\nResult\nNotes\n\n\n\n\npush_front(1)\n[1]\nbase\n\n\npush_front(2)\n[2, 1]\nprefix growth\n\n\npush_back(3)\n[2, 1, 3]\nsuffix add\n\n\npop_front()\n[1, 3]\nremove from prefix\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\npush_front/back\n\\(O(1)\\) amortized\n\\(O(1)\\)\nuses digits\n\n\npop_front/back\n\\(O(1)\\) amortized\n\\(O(1)\\)\nconstant fingers\n\n\nrandom access\n\\(O(\\log n)\\)\n\\(O(\\log n)\\)\nrecursion\n\n\nconcat/split\n\\(O(\\log n)\\)\n\\(O(\\log n)\\)\nefficient split\n\n\n\nA finger tree is the Swiss Army knife of persistent sequences, fast at both ends, balanced within, and beautifully immutable. It’s the blueprint behind countless functional data structures.\n\n\n\n287 Zipper Structure\nA zipper is a powerful technique that makes immutable data structures behave like mutable ones. It provides a focus—a pointer-like position—inside a persistent structure (list, tree, etc.), allowing localized updates, navigation, and edits without mutation.\nThink of it as a cursor in a purely functional world. Every movement or edit yields a new version, while sharing unmodified parts.\n\nWhat Problem Are We Solving?\nImmutable data structures can’t be “modified in place.” You can’t just “move a cursor” or “replace an element” without reconstructing the entire structure.\nA zipper solves this by maintaining:\n\nThe focused element, and\nThe context (what’s on the left/right or above/below).\n\nYou can then move, update, or rebuild efficiently, reusing everything else.\n\n\nHow It Works (Plain Language)\nA zipper separates a structure into:\n\nFocus: current element under attention\nContext: reversible description of the path you took\n\nWhen you move the focus, you update the context. When you change the focused element, you create a new node and rebuild from context.\nFor lists:\nZipper = (Left, Focus, Right)\nFor trees:\nZipper = (ParentContext, FocusNode)\nYou can think of it like a tape in a Turing machine—everything to the left and right is preserved.\n\n\nExample (List Zipper)\nWe represent a list [a, b, c, d] with a cursor on c:\nLeft: [b, a]   Focus: c   Right: [d]\nFrom here:\n\nmove_left → focus = b\nmove_right → focus = d\nupdate(x) → replace c with x\n\nAll in O(1), returning a new zipper version.\n\n\nExample Operations\n\n\n\nOperation\nResult\nDescription\n\n\n\n\nfrom_list([a,b,c,d])\n([ ], a, [b,c,d])\ninit at head\n\n\nmove_right\n([a], b, [c,d])\nshift focus right\n\n\nupdate('X')\n([a], X, [c,d])\nreplace focus\n\n\nto_list\n[a,X,c,d]\nrebuild full list\n\n\n\n\n\nTiny Code (Python – List Zipper)\nclass Zipper:\n    def __init__(self, left=None, focus=None, right=None):\n        self.left = left or []\n        self.focus = focus\n        self.right = right or []\n\n    @staticmethod\n    def from_list(lst):\n        if not lst: return Zipper([], None, [])\n        return Zipper([], lst[0], lst[1:])\n\n    def move_left(self):\n        if not self.left: return self\n        return Zipper(self.left[:-1], self.left[-1], [self.focus] + self.right)\n\n    def move_right(self):\n        if not self.right: return self\n        return Zipper(self.left + [self.focus], self.right[0], self.right[1:])\n\n    def update(self, value):\n        return Zipper(self.left, value, self.right)\n\n    def to_list(self):\n        return self.left + [self.focus] + self.right\n\n# Example\nz = Zipper.from_list(['a', 'b', 'c', 'd'])\nz1 = z.move_right().move_right()   # focus on 'c'\nz2 = z1.update('X')\nprint(z2.to_list())  # ['a', 'b', 'X', 'd']\nEach operation returns a new zipper, persistent editing made simple.\n\n\nExample (Tree Zipper – Conceptual)\nA tree zipper stores the path to the root as context:\nZipper = (ParentPath, FocusNode)\nEach parent in the path remembers which side you came from, so you can rebuild upward after an edit.\nFor example, editing a leaf L creates a new L' and rebuilds only the nodes along the path, leaving other subtrees untouched.\n\n\nWhy It Matters\n\nEnables localized updates in immutable structures\nUsed in functional editors, parsers, navigation systems\nProvides O(1) local movement, O(depth) rebuilds\nCore concept in Huet’s zipper, a foundational idea in functional programming\n\n\n\nA Gentle Proof (Why It Works)\nEach movement or edit affects only the local context:\n\nMove left/right in a list → \\(O(1)\\)\nMove up/down in a tree → \\(O(1)\\)\nRebuilding full structure → \\(O(\\text{depth})\\)\n\nNo mutation occurs; each version reuses all untouched substructures.\nFormally, if \\(S\\) is the original structure and \\(f\\) the focus, \\[\n\\text{zip}(S) = (\\text{context}, f)\n\\] and \\[\n\\text{unzip}(\\text{context}, f) = S\n\\] ensuring reversibility.\n\n\nTry It Yourself\n\nCreate a zipper from [1,2,3,4]\nMove focus to 3\nUpdate focus to 99\nRebuild full list\nVerify older zipper still has old value\n\n\n\nTest Cases\n\n\n\nStep\nOperation\nResult\nNotes\n\n\n\n\n1\nfrom_list([a,b,c,d])\n([ ], a, [b,c,d])\ninit\n\n\n2\nmove_right()\n([a], b, [c,d])\nshift focus\n\n\n3\nupdate('X')\n([a], X, [c,d])\nedit\n\n\n4\nto_list()\n[a, X, c, d]\nrebuild\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nMove left/right\n\\(O(1)\\)\n\\(O(1)\\)\nshift focus\n\n\nUpdate\n\\(O(1)\\)\n\\(O(1)\\)\nlocal replacement\n\n\nRebuild\n\\(O(n)\\)\n\\(O(1)\\)\nwhen unzipping\n\n\nAccess old version\n\\(O(1)\\)\n,\npersistent\n\n\n\nA zipper turns immutability into interactivity. With a zipper, you can move, focus, and edit, all without breaking persistence. It’s the bridge between static structure and dynamic navigation.\n\n\n\n289 Trie with Versioning\nA Trie with Versioning is a persistent data structure that stores strings (or sequences) across multiple historical versions. Each new update—an insertion, deletion, or modification—creates a new version of the trie without mutating previous ones, using path copying for structural sharing.\nThis enables time-travel queries: you can look up keys as they existed at any point in history.\n\nWhat Problem Are We Solving?\nWe want to maintain a versioned dictionary of strings or sequences, supporting:\n\nFast prefix search (\\(O(\\text{length})\\))\nEfficient updates without mutation\nAccess to past versions (e.g., snapshots, undo/redo, history)\n\nA versioned trie achieves all three by copying only the path from the root to modified nodes while sharing all other subtrees.\nCommon use cases:\n\nVersioned symbol tables\nHistorical dictionaries\nAutocomplete with rollback\nPersistent tries in functional languages\n\n\n\nHow It Works (Plain Language)\nEach trie node contains:\n\nA mapping from character → child node\nA flag marking end of word\n\nFor persistence:\n\nWhen you insert or delete, copy only nodes on the affected path.\nOld nodes remain untouched and shared by old versions.\n\nThus, version \\(v_{k+1}\\) differs from \\(v_k\\) only along the modified path.\n\n\nTiny Code (Conceptual Python)\nclass TrieNode:\n    def __init__(self, children=None, is_end=False):\n        self.children = dict(children or {})\n        self.is_end = is_end\n\ndef insert(root, word):\n    def _insert(node, i):\n        node = TrieNode(node.children, node.is_end)\n        if i == len(word):\n            node.is_end = True\n            return node\n        ch = word[i]\n        node.children[ch] = _insert(node.children.get(ch, TrieNode()), i + 1)\n        return node\n    return _insert(root, 0)\n\ndef search(root, word):\n    node = root\n    for ch in word:\n        if ch not in node.children:\n            return False\n        node = node.children[ch]\n    return node.is_end\nEach call to insert returns a new root (new version), sharing all unmodified branches.\n\n\nExample\n\n\n\nVersion\nOperation\nTrie Content\n\n\n\n\nv1\ninsert(“cat”)\n{ “cat” }\n\n\nv2\ninsert(“car”)\n{ “cat”, “car” }\n\n\nv3\ninsert(“dog”)\n{ “cat”, “car”, “dog” }\n\n\nv4\ndelete(“car”)\n{ “cat”, “dog” }\n\n\n\nVersions share nodes for prefix \"c\" between all earlier versions.\n\n\nWhy It Matters\n\nImmutable and Safe: No in-place mutation, perfect for functional systems\nEfficient Rollback: Access any prior version in \\(O(1)\\) time\nPrefix Sharing: Saves memory through structural reuse\nPractical for History: Ideal for versioned dictionaries, IDEs, search indices\n\n\n\nA Gentle Proof (Why It Works)\nEach insertion copies one path of length \\(L\\) (word length). Total time complexity: \\[\nT_{\\text{insert}} = O(L)\n\\]\nEach node on the new path shares all other unchanged subtrees. If \\(N\\) is total stored characters across all versions, \\[\n\\text{Space} = O(N)\n\\]\nEach version root is a single pointer, enabling \\(O(1)\\) access: \\[\n\\text{Version}_i = \\text{Root}_i\n\\]\nOld versions remain fully usable, as they never mutate.\n\n\nTry It Yourself\n\nInsert “cat”, “car”, “dog” into versioned trie\nDelete “car” to form new version\nQuery prefix “ca” in all versions\nCheck that “car” exists only before deletion\nPrint shared node counts across versions\n\n\n\n\nTest Case\n\n\n\nStep\nOperation\nVersion\nExists in Version\nResult\n\n\n\n\n1\ninsert(“cat”)\nv1\ncat\nTrue\n\n\n2\ninsert(“car”)\nv2\ncar, cat\nTrue\n\n\n3\ninsert(“dog”)\nv3\ncat, car, dog\nTrue\n\n\n4\ndelete(“car”)\nv4\ncar (no), cat, dog\nFalse\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nInsert\n\\(O(L)\\)\n\\(O(L)\\)\nPath-copying\n\n\nDelete\n\\(O(L)\\)\n\\(O(L)\\)\nPath-copying\n\n\nLookup\n\\(O(L)\\)\n\\(O(1)\\)\nFollows shared structure\n\n\nAccess old version\n\\(O(1)\\)\n,\nVersion pointer access\n\n\n\nA Trie with Versioning blends structural sharing with prefix indexing. Each version is a frozen snapshot—compact, queryable, and immutable—perfect for versioned word histories.\n\n\n\n290 Persistent Union-Find\nA Persistent Union-Find extends the classical Disjoint Set Union (DSU) structure to support time-travel queries. Instead of mutating the parent and rank arrays in place, each union operation produces a new version, enabling queries like:\n\n“Are \\(x\\) and \\(y\\) connected at version \\(v\\)?”\n“What did the set look like before the last merge?”\n\nThis structure is vital for dynamic connectivity problems where the history of unions matters.\n\nWhat Problem Are We Solving?\nClassical DSU supports find and union efficiently in near-constant time, but only for a single evolving state. Once you merge two sets, the old version is gone.\nWe need a versioned DSU that keeps all previous states intact, supporting:\n\nUndo/rollback of operations\nQueries over past connectivity\nOffline dynamic connectivity analysis\n\n\n\nHow It Works (Plain Language)\nA Persistent Union-Find uses path copying (similar to persistent arrays) to maintain multiple versions:\n\nEach union creates a new version\nOnly affected parent and rank entries are updated in a new structure\nAll other nodes share structure with the previous version\n\nThere are two main designs:\n\nFull persistence using path copying in functional style\nPartial persistence using rollback stack (undo operations)\n\nWe focus here on full persistence.\n\n\nTiny Code (Conceptual Python)\nclass PersistentDSU:\n    def __init__(self, n):\n        self.versions = []\n        parent = list(range(n))\n        rank = [0] * n\n        self.versions.append((parent, rank))\n    \n    def find(self, parent, x):\n        if parent[x] == x:\n            return x\n        return self.find(parent, parent[x])\n    \n    def union(self, ver, x, y):\n        parent, rank = [*ver[0]], [*ver[1]]  # copy arrays\n        rx, ry = self.find(parent, x), self.find(parent, y)\n        if rx != ry:\n            if rank[rx] &lt; rank[ry]:\n                parent[rx] = ry\n            elif rank[rx] &gt; rank[ry]:\n                parent[ry] = rx\n            else:\n                parent[ry] = rx\n                rank[rx] += 1\n        self.versions.append((parent, rank))\n        return len(self.versions) - 1  # new version index\n    \n    def connected(self, ver, x, y):\n        parent, _ = self.versions[ver]\n        return self.find(parent, x) == self.find(parent, y)\nEach union returns a new version index. You can query connected(version, a, b) at any time.\n\n\nExample\n\n\n\nStep\nOperation\nVersion\nConnections\n\n\n\n\n1\nmake-set(5)\nv0\n0 1 2 3 4\n\n\n2\nunion(0,1)\nv1\n{0,1}, 2, 3, 4\n\n\n3\nunion(2,3)\nv2\n{0,1}, {2,3}, 4\n\n\n4\nunion(1,2)\nv3\n{0,1,2,3}, 4\n\n\n5\nquery connected(0,1)\nv1\nTrue\n\n\n6\nquery connected(1,3)\nv1\nFalse (not yet merged)\n\n\n\nYou can check connections at any version.\n\n\nWhy It Matters\n\nTime-travel queries across historical versions\nNon-destructive updates allow safe rollback\nCrucial for offline dynamic connectivity, e.g., edge insertions over time\nSimplifies debugging, simulation, and version tracking\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(n\\) be the number of elements and \\(q\\) the number of versions.\nEach version differs in only a small number of parent/rank entries. If each union copies \\(O(\\alpha(n))\\) elements, total space after \\(q\\) operations is:\n\\[\nO(n + q \\alpha(n))\n\\]\nEach query operates on a fixed version in:\n\\[\nO(\\alpha(n))\n\\]\nPath compression is often replaced with partial compression or omitted to ensure persistence (full path compression breaks immutability).\n\n\nTry It Yourself\n\nInitialize DSU with 5 elements\nPerform unions step by step, saving each version\nQuery connectivity across multiple versions\nUndo merges by reverting to older versions\nVisualize parent tree evolution\n\n\n\nTest Cases\n\n\n\nVersion\nQuery\nResult\n\n\n\n\nv0\nconnected(0,1)\nFalse\n\n\nv1\nconnected(0,1)\nTrue\n\n\nv2\nconnected(2,3)\nTrue\n\n\nv3\nconnected(1,3)\nTrue\n\n\nv1\nconnected(1,3)\nFalse\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nFind\n\\(O(\\alpha(n))\\)\n\\(O(1)\\)\nPer version\n\n\nUnion\n\\(O(\\alpha(n))\\)\n\\(O(n)\\) copy\nCopy-on-write path\n\n\nConnected\n\\(O(\\alpha(n))\\)\n\\(O(1)\\)\nVersioned query\n\n\nAccess old version\n\\(O(1)\\)\n,\nVersion pointer lookup\n\n\n\nA Persistent Union-Find is a historical map of connectivity. Each version captures a snapshot of relationships—immutable, queryable, and efficient—ideal for evolving graphs and rollback-capable algorithms.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-3.html#section-30.-advanced-trees-and-range-queries",
    "href": "books/en-us/list-3.html#section-30.-advanced-trees-and-range-queries",
    "title": "Chapter 3. Data Structure in Action",
    "section": "Section 30. Advanced Trees and Range Queries",
    "text": "Section 30. Advanced Trees and Range Queries\n\n291 Sparse Table Build\nA Sparse Table is a static data structure for answering idempotent range queries in O(1) time after O(n log n) preprocessing. It is perfect for Range Minimum/Maximum Query (RMQ), GCD, and any operation where combining overlapping answers is valid, such as min, max, gcd, lcm (with care), and bitwise and/or. It is not suitable for sum or other non-idempotent operations if you require O(1) queries.\n\nWhat Problem Are We Solving?\nGiven an array A[0..n-1], we want to answer queries like\n\nRMQ: minimum on interval [L, R]\nRMaxQ: maximum on interval [L, R] in O(1) time per query, with no updates.\n\n\n\nHow It Works (Plain Language)\nPrecompute answers for all ranges whose lengths are powers of two. Let st[k][i] store the answer on the interval of length 2^k starting at i, that is [i, i + 2^k - 1].\nBuild recurrence:\n\nBase layer k = 0: intervals of length 1 st[0][i] = A[i]\nHigher layers: combine two halves of length 2^{k-1} st[k][i] = op(st[k-1][i], st[k-1][i + 2^{k-1}])\n\nTo answer a query on [L, R], let len = R - L + 1, k = floor(log2(len)). For idempotent operations like min or max, we can cover the range with two overlapping blocks:\n\nBlock 1: [L, L + 2^k - 1]\nBlock 2: [R - 2^k + 1, R] Then \\[\n\\text{ans} = \\operatorname{op}\\big(\\text{st}[k][L],\\ \\text{st}[k][R - 2^k + 1]\\big)\n\\]\n\n\n\nExample Step by Step\nArray A = [7, 2, 3, 0, 5, 10, 3, 12, 18], op = min.\n\nBuild st[0] (length 1): st[0] = [7, 2, 3, 0, 5, 10, 3, 12, 18]\nBuild st[1] (length 2): st[1][i] = min(st[0][i], st[0][i+1]) st[1] = [2, 2, 0, 0, 5, 3, 3, 12]\nBuild st[2] (length 4): st[2][i] = min(st[1][i], st[1][i+2]) st[2] = [0, 0, 0, 0, 3, 3]\nBuild st[3] (length 8): st[3][i] = min(st[2][i], st[2][i+4]) st[3] = [0, 0]\n\nQuery example: RMQ on [3, 8] len = 6, k = floor(log2(6)) = 2, 2^k = 4\n\nBlock 1: [3, 6] uses st[2][3]\nBlock 2: [5, 8] uses st[2][5] Answer \\[\n\\min\\big(\\text{st}[2][3], \\text{st}[2][5]\\big) = \\min(0, 3) = 0\n\\]\n\n\n\nTiny Code (Python, RMQ with min)\nimport math\n\ndef build_sparse_table(arr, op=min):\n    n = len(arr)\n    K = math.floor(math.log2(n)) + 1\n    st = [[0] * n for _ in range(K)]\n    for i in range(n):\n        st[0][i] = arr[i]\n    j = 1\n    while (1 &lt;&lt; j) &lt;= n:\n        step = 1 &lt;&lt; (j - 1)\n        for i in range(n - (1 &lt;&lt; j) + 1):\n            st[j][i] = op(st[j - 1][i], st[j - 1][i + step])\n        j += 1\n    # Precompute logs for O(1) queries\n    lg = [0] * (n + 1)\n    for i in range(2, n + 1):\n        lg[i] = lg[i // 2] + 1\n    return st, lg\n\ndef query(st, lg, L, R, op=min):\n    length = R - L + 1\n    k = lg[length]\n    return op(st[k][L], st[k][R - (1 &lt;&lt; k) + 1])\n\n# Example\nA = [7, 2, 3, 0, 5, 10, 3, 12, 18]\nst, lg = build_sparse_table(A, op=min)\nprint(query(st, lg, 3, 8, op=min))  # 0\nprint(query(st, lg, 0, 2, op=min))  # 2\nFor max, just pass op=max. For gcd, pass math.gcd.\n\n\nWhy It Matters\n\nO(1) query time for static arrays\nO(n log n) preprocessing with simple transitions\nExcellent for RMQ style tasks, LCA via RMQ on Euler tours, and many competitive programming problems\nCache friendly and implementation simple compared to segment trees when no updates are needed\n\n\n\nA Gentle Proof (Why It Works)\nThe table stores answers for all intervals of length 2^k. Any interval [L, R] can be covered by two overlapping power of two blocks of equal length 2^k, where k = floor(log2(R - L + 1)). For idempotent operations op, overlap does not affect correctness, so \\[\n\\text{op}\\big([L, R]\\big) = \\text{op}\\big([L, L + 2^k - 1],\\ [R - 2^k + 1, R]\\big)\n\\] Both blocks are precomputed, so the query is constant time.\n\n\nTry It Yourself\n\nBuild a table for A = [5, 4, 3, 6, 1, 2] with op = min.\nAnswer RMQ on [1, 4] and [0, 5].\nSwap op to max and recheck.\nUse op = gcd and verify results on several ranges.\n\n\n\nTest Cases\n\n\n\nArray\nop\nQuery\nExpected\n\n\n\n\n[7, 2, 3, 0, 5, 10, 3]\nmin\n[0, 2]\n2\n\n\n[7, 2, 3, 0, 5, 10, 3]\nmin\n[3, 6]\n0\n\n\n[1, 5, 2, 4, 6, 1, 3]\nmax\n[2, 5]\n6\n\n\n[12, 18, 6, 9, 3]\ngcd\n[1, 4]\n3\n\n\n\n\n\nComplexity\n\n\n\nPhase\nTime\nSpace\n\n\n\n\nPreprocess\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\n\nQuery\n\\(O(1)\\)\n,\n\n\n\nNote Sparse Table supports static data. If you need updates, consider a segment tree or Fenwick tree. Sparse Table excels when the array is fixed and you need very fast queries.\n\n\n\n292 Cartesian Tree\nA Cartesian Tree is a binary tree built from an array such that:\n\nIn-order traversal of the tree reproduces the array, and\nThe tree satisfies the heap property with respect to the array values (min-heap or max-heap).\n\nThis structure elegantly bridges arrays and binary trees, and plays a key role in algorithms for Range Minimum Query (RMQ), Lowest Common Ancestor (LCA), and sequence decomposition.\n\nWhat Problem Are We Solving?\nWe want to represent an array \\(A[0..n-1]\\) as a tree that encodes range relationships. For RMQ, if the tree is a min-heap Cartesian Tree, then the LCA of nodes \\(i\\) and \\(j\\) corresponds to the index of the minimum element in the range \\([i, j]\\).\nThus, building a Cartesian Tree gives us an elegant path from RMQ to LCA in \\(O(1)\\) after \\(O(n)\\) preprocessing.\n\n\nHow It Works (Plain Language)\nA Cartesian Tree is built recursively:\n\nThe root is the smallest element (for min-heap) or largest (for max-heap).\nThe left subtree is built from elements to the left of the root.\nThe right subtree is built from elements to the right of the root.\n\nA more efficient linear-time construction uses a stack:\n\nTraverse the array from left to right.\nMaintain a stack of nodes in increasing order.\nFor each new element, pop while the top is greater, then attach the new node as the right child of the last popped node or the left child of the current top.\n\n\n\nExample\nLet \\(A = [3, 2, 6, 1, 9]\\)\n\nStart with empty stack\nInsert 3 → stack = [3]\nInsert 2 → pop 3 (since 3 &gt; 2)\n\n2 becomes parent of 3\nstack = [2]\n\nInsert 6 → 6 &gt; 2 → right child\n\nstack = [2, 6]\n\nInsert 1 → pop 6, pop 2 → 1 is new root\n\n2 becomes right child of 1\n\nInsert 9 → right child of 6\n\nTree structure (min-heap):\n       1\n      / \\\n     2   9\n    / \\\n   3   6\nIn-order traversal: [3, 2, 6, 1, 9] Heap property: every parent is smaller than its children ✅\n\n\nTiny Code (Python, Min-Heap Cartesian Tree)\nclass Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef build_cartesian_tree(arr):\n    stack = []\n    root = None\n    for val in arr:\n        node = Node(val)\n        last = None\n        while stack and stack[-1].val &gt; val:\n            last = stack.pop()\n        node.left = last\n        if stack:\n            stack[-1].right = node\n        else:\n            root = node\n        stack.append(node)\n    return root\n\ndef inorder(node):\n    return inorder(node.left) + [node.val] + inorder(node.right) if node else []\nExample usage:\nA = [3, 2, 6, 1, 9]\nroot = build_cartesian_tree(A)\nprint(inorder(root))  # [3, 2, 6, 1, 9]\n\n\nWhy It Matters\n\nRMQ in O(1): RMQ becomes LCA in Cartesian Tree (after Euler Tour + Sparse Table).\nMonotonic Stack Connection: Linear construction mirrors the logic of stack-based range problems (Next Greater Element, Histogram).\nDivide-and-Conquer Decomposition: Represents array’s recursive structure.\nEfficient Building: Linear time with stack.\n\n\n\nA Gentle Proof (Why It Works)\nEach element is pushed and popped at most once, so total operations = \\(O(n)\\). Heap property ensures RMQ correctness:\n\nIn min-heap tree, root of any subtree is the minimum of that segment.\nThus, LCA(i, j) gives the index of \\(\\min(A[i..j])\\).\n\nSo by reducing RMQ to LCA, we achieve:\n\\[\n\\text{RMQ}(i, j) = \\text{index}( \\text{LCA}(i, j) )\n\\]\n\n\nTry It Yourself\n\nBuild a Cartesian Tree for \\(A = [4, 5, 2, 3, 1]\\) (min-heap).\nVerify in-order traversal equals original array.\nMark parents smaller than children.\nIdentify RMQ(1, 3) from the tree using LCA.\n\n\n\nTest Cases\n\n\n\nArray\nTree Type\nRoot\nRMQ(1, 3)\nInorder Matches\n\n\n\n\n[3, 2, 6, 1, 9]\nmin-heap\n1\n2\n✅\n\n\n[5, 4, 3, 2, 1]\nmin-heap\n1\n2\n✅\n\n\n[1, 2, 3, 4, 5]\nmin-heap\n1\n2\n✅\n\n\n[2, 7, 5, 9]\nmin-heap\n2\n5\n✅\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nOperation\nTime\nSpace\nNotes\n\n\n\n\nBuild\n\\(O(n)\\)\n\\(O(n)\\)\nStack-based construction\n\n\nQuery (RMQ)\n\\(O(1)\\)\n,\nAfter Euler + Sparse Table\n\n\nLCA Preprocess\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\nSparse Table method\n\n\n\nA Cartesian Tree weaves together order and hierarchy: in-order for sequence, heap for dominance, a silent bridge between arrays and trees.\n\n\n\n293 Segment Tree Beats\nSegment Tree Beats is an advanced variant of the classical segment tree that can handle non-trivial range queries and updates beyond sum, min, or max. It’s designed for problems where the operation is not linear or not invertible, such as range chmin/chmax, range add with min tracking, or range second-min queries.\nIt “beats” the limitation of classical lazy propagation by storing extra state (like second minimum, second maximum) to decide when updates can stop early.\n\nWhat Problem Are We Solving?\nStandard segment trees can’t efficiently handle complex updates like:\n\n“Set all \\(A[i]\\) in \\([L, R]\\) to min(A[i], x)”\n“Set all \\(A[i]\\) in \\([L, R]\\) to max(A[i], x)”\n\nBecause different elements in a segment may behave differently depending on their value relative to x.\nSegment Tree Beats solves this by maintaining extra constraints in each node so we can “beat” recursion and skip branches early when conditions are met.\n\n\nHow It Works (Plain Language)\nEach node stores not just an aggregate but enough info to know when an operation can be fully applied.\nFor Range Chmin (A[i] = min(A[i], x)):\n\nIf max &lt;= x: nothing changes\nIf second_max &lt; x &lt; max: only elements with max are updated\nIf x &lt; second_max: recurse to children\n\nEach node stores:\n\nmax (maximum value in range)\nsecond_max (second largest value)\ncount_max (number of times max appears)\nsum (sum over range)\n\nThis lets us decide update logic without touching all elements.\n\n\nExample (Range Chmin)\nLet \\(A = [4, 7, 6, 3]\\)\nWe apply chmin(L=0, R=3, x=5) → every element &gt; 5 is set to 5.\nStep:\n\nNode [0, 3]: max = 7, second_max = 6\nSince x = 5 &lt; second_max = 6, recurse\nUpdate left child [0,1]: has 7 → 7 becomes 5\nUpdate right [2,3]: max = 6 → 6 becomes 5\n\nNew array: [4, 5, 5, 3]\n\n\nTiny Code (Simplified Range Chmin)\nclass Node:\n    def __init__(self):\n        self.sum = 0\n        self.max = 0\n        self.smax = -float('inf')\n        self.cnt = 0\n        self.l = None\n        self.r = None\n\ndef merge(a, b):\n    node = Node()\n    node.sum = a.sum + b.sum\n    if a.max == b.max:\n        node.max = a.max\n        node.smax = max(a.smax, b.smax)\n        node.cnt = a.cnt + b.cnt\n    elif a.max &gt; b.max:\n        node.max = a.max\n        node.smax = max(a.smax, b.max)\n        node.cnt = a.cnt\n    else:\n        node.max = b.max\n        node.smax = max(a.max, b.smax)\n        node.cnt = b.cnt\n    return node\n\ndef push_chmin(node, x, length):\n    if node.max &lt;= x:\n        return\n    node.sum -= (node.max - x) * node.cnt\n    node.max = x\n\ndef update_chmin(node, l, r, ql, qr, x):\n    if r &lt; ql or qr &lt; l or node.max &lt;= x:\n        return\n    if ql &lt;= l and r &lt;= qr and node.smax &lt; x:\n        push_chmin(node, x, r - l + 1)\n        return\n    m = (l + r) // 2\n    update_chmin(node.l, l, m, ql, qr, x)\n    update_chmin(node.r, m+1, r, ql, qr, x)\n    new = merge(node.l, node.r)\n    node.max, node.smax, node.cnt, node.sum = new.max, new.smax, new.cnt, new.sum\nThis is the essence: skip updates when possible, split when necessary.\n\n\nWhy It Matters\n\nHandles non-linear updates efficiently\nPreserves logarithmic complexity by reducing unnecessary recursion\nUsed in many competitive programming RMQ-like challenges with range cap operations\nGeneralizes segment tree to “hard” problems (range min caps, range max caps, conditional sums)\n\n\n\nA Gentle Proof (Why It Works)\nThe trick: by storing max, second max, and count, we can stop descending if the operation affects only elements equal to max. At most O(log n) nodes per update because:\n\nEach update lowers some max values\nEach element’s value decreases logarithmically before stabilizing\n\nThus total complexity amortizes to: \\[\nO((n + q) \\log n)\n\\]\n\n\nTry It Yourself\n\nBuild a Segment Tree Beats for \\(A = [4, 7, 6, 3]\\).\nApply chmin(0,3,5) → verify [4,5,5,3].\nApply chmin(0,3,4) → verify [4,4,4,3].\nTrack sum after each operation.\n\n\n\nTest Cases\n\n\n\nArray\nOperation\nResult\n\n\n\n\n[4,7,6,3]\nchmin(0,3,5)\n[4,5,5,3]\n\n\n[4,5,5,3]\nchmin(1,2,4)\n[4,4,4,3]\n\n\n[1,10,5,2]\nchmin(0,3,6)\n[1,6,5,2]\n\n\n[5,5,5]\nchmin(0,2,4)\n[4,4,4]\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\n\nOperation\nTime (Amortized)\nSpace\nNotes\n\n\n\n\nBuild\n\\(O(n)\\)\n\\(O(n)\\)\nSame as normal segment tree\n\n\nRange Chmin\n\\(O(\\log n)\\)\n\\(O(n)\\)\nAmortized over operations\n\n\nQuery (Sum)\n\\(O(\\log n)\\)\n,\nCombine like usual\n\n\n\nSegment Tree Beats lives at the intersection of elegance and power, retaining \\(O(\\log n)\\) intuition while tackling the kinds of operations classical segment trees can’t touch.\n\n\n\n294 Merge Sort Tree\nA Merge Sort Tree is a segment tree where each node stores a sorted list of the elements in its range. It allows efficient queries that depend on order statistics, such as counting how many elements fall within a range or finding the \\(k\\)-th smallest element in a subarray.\nIt’s called “Merge Sort Tree” because it is built exactly like merge sort: divide, conquer, and merge sorted halves.\n\nWhat Problem Are We Solving?\nClassical segment trees handle sum, min, or max, but not value-based queries. Merge Sort Trees enable operations like:\n\nCount how many numbers in \\(A[L..R]\\) are \\(\\le x\\)\nCount elements in a value range \\([a,b]\\)\nFind the \\(k\\)-th smallest element in \\(A[L..R]\\)\n\nThese problems often arise in range frequency queries, inversions counting, and offline queries.\n\n\nHow It Works (Plain Language)\nEach node of the tree covers a segment [l, r] of the array. Instead of storing a single number, it stores a sorted list of all elements in that segment.\nBuilding:\n\nIf l == r, store [A[l]].\nOtherwise, recursively build left and right children.\nMerge the two sorted lists to form this node’s list.\n\nQuerying: To count numbers \\(\\le x\\) in range [L, R]:\n\nVisit all segment tree nodes that fully or partially overlap [L, R].\nIn each node, binary search for position of x in the node’s sorted list.\n\n\n\nExample\nLet \\(A = [2, 5, 1, 4, 3]\\)\n\n\nStep 1: Build Tree\nEach leaf stores one value:\n\n\n\nNode Range\nStored List\n\n\n\n\n[0,0]\n[2]\n\n\n[1,1]\n[5]\n\n\n[2,2]\n[1]\n\n\n[3,3]\n[4]\n\n\n[4,4]\n[3]\n\n\n\nNow merge:\n\n\n\nNode Range\nStored List\n\n\n\n\n[0,1]\n[2,5]\n\n\n[2,3]\n[1,4]\n\n\n[2,4]\n[1,3,4]\n\n\n[0,4]\n[1,2,3,4,5]\n\n\n\n\n\nStep 2: Query Example\nCount elements \\(\\le 3\\) in range [1, 4].\nWe query nodes that cover [1,4]: [1,1], [2,3], [4,4].\n\n[1,1] → list = [5] → count = 0\n[2,3] → list = [1,4] → count = 1\n[4,4] → list = [3] → count = 1\n\nTotal = 2 elements ≤ 3.\n\n\nTiny Code (Python, Count ≤ x)\nimport bisect\n\nclass MergeSortTree:\n    def __init__(self, arr):\n        self.n = len(arr)\n        self.tree = [[] for _ in range(4 * self.n)]\n        self._build(arr, 1, 0, self.n - 1)\n\n    def _build(self, arr, node, l, r):\n        if l == r:\n            self.tree[node] = [arr[l]]\n            return\n        m = (l + r) // 2\n        self._build(arr, node * 2, l, m)\n        self._build(arr, node * 2 + 1, m + 1, r)\n        self.tree[node] = sorted(self.tree[node * 2] + self.tree[node * 2 + 1])\n\n    def query_leq(self, node, l, r, ql, qr, x):\n        if r &lt; ql or qr &lt; l:\n            return 0\n        if ql &lt;= l and r &lt;= qr:\n            return bisect.bisect_right(self.tree[node], x)\n        m = (l + r) // 2\n        return (self.query_leq(node * 2, l, m, ql, qr, x) +\n                self.query_leq(node * 2 + 1, m + 1, r, ql, qr, x))\n\n# Example\nA = [2, 5, 1, 4, 3]\nmst = MergeSortTree(A)\nprint(mst.query_leq(1, 0, 4, 1, 4, 3))  # count ≤ 3 in [1,4] = 2\n\n\nWhy It Matters\n\nEnables order-based queries (≤, ≥, count, rank)\nUseful for offline range counting, kth-smallest, and inversion queries\nCombines divide-and-conquer sorting with range decomposition\n\n\n\nA Gentle Proof (Why It Works)\nEach level of the tree merges sorted lists from children.\n\nEach element appears in \\(O(\\log n)\\) nodes.\nEach merge is linear in subarray size.\n\nThus, total build time: \\[\nO(n \\log n)\n\\]\nEach query visits \\(O(\\log n)\\) nodes, each with \\(O(\\log n)\\) binary search: \\[\nO(\\log^2 n)\n\\]\n\n\nTry It Yourself\n\nBuild tree for \\(A = [5, 1, 4, 2, 3]\\).\nQuery count ≤ 3 in [0, 4].\nQuery count ≤ 2 in [1, 3].\nImplement query for “count between \\([a,b]\\)” using two query_leq calls.\n\n\n\nTest Cases\n\n\n\nArray\nQuery\nCondition\nAnswer\n\n\n\n\n[2,5,1,4,3]\n[1,4], ≤3\ncount\n2\n\n\n[2,5,1,4,3]\n[0,2], ≤2\ncount\n2\n\n\n[1,2,3,4,5]\n[2,4], ≤4\ncount\n3\n\n\n[5,4,3,2,1]\n[0,4], ≤3\ncount\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\n\nQuery (≤ x)\n\\(O(\\log^2 n)\\)\n,\n\n\nQuery (range count)\n\\(O(\\log^2 n)\\)\n,\n\n\n\nMerge Sort Trees elegantly bridge sorting and segmentation, empowering range queries that depend not on aggregates, but on the distribution of values themselves.\n\n\n\n295 Wavelet Tree\nA Wavelet Tree is a compact, indexable structure over a sequence that supports rank, select, and range queries by value in O(log ) time with O(n log ) space, where () is the alphabet size. Think of it as a value-aware segment tree built on bitvectors that let you jump between levels using rank counts.\n\nWhat Problem Are We Solving?\nGiven an array \\(A[1..n]\\) over values in \\([1..\\sigma]\\), define the following queries:\n\n\\(\\text{rank}(x, r)\\): number of occurrences of value \\(x\\) in \\(A[1..r]\\)\n\\(\\text{select}(x, k)\\): position of the \\(k\\)-th occurrence of \\(x\\) in \\(A\\)\n\\(\\text{kth}(l, r, k)\\): the \\(k\\)-th smallest value in the subarray \\(A[l..r]\\)\n\\(\\text{range\\_count}(l, r, a, b)\\): number of values in \\(A[l..r]\\) that lie in \\([a, b]\\)\n\n\n\nHow It Works\n\nValue partitioning by halves\nRecursively partition the value domain \\([v_{\\min}, v_{\\max}]\\) into two halves at midpoint \\(m\\).\n\nLeft child stores all elements \\(\\le m\\)\n\nRight child stores all elements \\(&gt; m\\)\n\nStable partition plus bitvector\nAt each node, keep the original order and record a bitvector \\(B\\) of length equal to the number of elements that arrive at the node:\n\n\\(B[i] = 0\\) if the \\(i\\)-th element goes to the left child\n\n\\(B[i] = 1\\) if it goes to the right child\n\nSupport fast ranks on \\(B\\): \\(\\mathrm{rank}_0(B,i)\\) and \\(\\mathrm{rank}_1(B,i)\\).\nNavigating queries\n\nPosition based descent: translate positions using prefix counts from \\(B\\).\nIf a query interval at this node is \\([l,r]\\), then the corresponding interval in the left child is \\([\\mathrm{rank}_0(B,l-1)+1,\\ \\mathrm{rank}_0(B,r)]\\) and in the right child is \\([\\mathrm{rank}_1(B,l-1)+1,\\ \\mathrm{rank}_1(B,r)]\\).\nValue based descent: choose left or right child by comparing the value with \\(m\\).\n\n\nHeight is \\(O(\\log \\sigma)\\). Each step uses \\(O(1)\\) bitvector rank operations.\n\n\nExample\nArray: \\(A = [3, 1, 4, 1, 5, 9, 2, 6]\\), values in \\([1..9]\\)\nRoot split by midpoint \\(m = 5\\)\n\nLeft child receives elements \\(\\le 5\\): \\(\\{3, 1, 4, 1, 5, 2\\}\\)\nRight child receives elements \\(&gt; 5\\): \\(\\{9, 6\\}\\)\n\nRoot bitvector marks routing left (0) or right (1), preserving order:\n\n\\(A\\) by node: [3, 1, 4, 1, 5, 9, 2, 6]\n\n\\(B\\): [0, 0, 0, 0, 0, 1, 0, 1]\n\nLeft child split on \\([1..5]\\) by \\(m = 3\\)\n\nLeft-left (values \\(\\le 3\\)): positions where \\(B=0\\) at root, then route by \\(m=3\\)\nSequence arriving: [3, 1, 4, 1, 5, 2]\nBitvector at this node \\(B_L\\): [0, 0, 1, 0, 1, 0]\nChildren:\n\n\\(\\le 3\\): [3, 1, 1, 2]\n\\(&gt; 3\\): [4, 5]\n\nRight-right subtree of root contains [9, 6] (no further split shown)\n\nRank translation example at the root\nFor an interval \\([l,r]\\) at the root, the corresponding intervals are: - Left child: \\([\\,\\mathrm{rank}_0(B,l-1)+1,\\ \\mathrm{rank}_0(B,r)\\,]\\) - Right child: \\([\\,\\mathrm{rank}_1(B,l-1)+1,\\ \\mathrm{rank}_1(B,r)\\,]\\)\nExample: take \\([l,r]=[2,7]\\) at the root with \\(B=[0,0,0,0,0,1,0,1]\\) - \\(\\mathrm{rank}_0(B,1)=1\\), \\(\\mathrm{rank}_0(B,7)=6\\) → left interval \\([2,6]\\) - \\(\\mathrm{rank}_1(B,1)=0\\), \\(\\mathrm{rank}_1(B,7)=1\\) → right interval \\([1,1]\\)\nHeight is \\(O(\\log \\sigma)\\), each descent step uses \\(O(1)\\) rank operations on the local bitvector.\n\n\n\nCore Operations\n\nrank(x, r)\nWalk top down. At a node with split value m:\n\nIf \\(x \\le m\\), set \\(r \\leftarrow \\mathrm{rank}_0(B, r)\\) and go left\n\nOtherwise set \\(r \\leftarrow \\mathrm{rank}_1(B, r)\\) and go right\n\nWhen you reach the leaf for value \\(x\\), the current \\(r\\) is the answer.\nselect(x, k)\nStart at the leaf for value \\(x\\) with local index \\(k\\).\nMove upward to the root, inverting the position mapping at each parent:\n\nIf you came from the left child, set \\(k \\leftarrow \\mathrm{select\\_pos\\_0}(B, k)\\)\n\nIf you came from the right child, set \\(k \\leftarrow \\mathrm{select\\_pos\\_1}(B, k)\\)\nThe final \\(k\\) at the root is the global position of the \\(k\\)-th \\(x\\).\n\nkth(l, r, k)\nAt a node with split value \\(m\\), let \\[\nc \\;=\\; \\mathrm{rank}_0(B, r)\\;-\\;\\mathrm{rank}_0(B, l-1)\n\\] which is the number of items routed left within \\([l, r]\\).\n\nIf \\(k \\le c\\), map the interval to the left child via \\[\nl' \\;=\\; \\mathrm{rank}_0(B, l-1) + 1,\\quad\nr' \\;=\\; \\mathrm{rank}_0(B, r)\n\\] and recurse on \\((l', r', k)\\).\nOtherwise go right with \\[\nk \\leftarrow k - c,\\quad\nl' \\;=\\; \\mathrm{rank}_1(B, l-1) + 1,\\quad\nr' \\;=\\; \\mathrm{rank}_1(B, r)\n\\] and recurse on \\((l', r', k)\\).\n\nrange_count(l, r, a, b)\nRecurse only into value intervals that intersect \\([a, b]\\).\nAt each visited node, map the position interval using \\(B\\):\n\nLeft child interval \\[\n[\\,\\mathrm{rank}_0(B, l-1)+1,\\ \\mathrm{rank}_0(B, r)\\,]\n\\]\nRight child interval \\[\n[\\,\\mathrm{rank}_1(B, l-1)+1,\\ \\mathrm{rank}_1(B, r)\\,]\n\\] Stop when a node’s value range is fully inside or outside \\([a, b]\\):\nFully inside: add its interval length\n\nDisjoint: add 0\n\n\n\nTiny Code Sketch in Python\nThis sketch shows the structure and kth query. A production version needs succinct rank structures for (B) to guarantee (O(1)) ranks.\nimport bisect\n\nclass WaveletTree:\n    def __init__(self, arr, lo=None, hi=None):\n        self.lo = min(arr) if lo is None else lo\n        self.hi = max(arr) if hi is None else hi\n        self.b = []          # bitvector as 0-1 list\n        self.pref = [0]      # prefix sums of ones for O(1) rank1\n        if self.lo == self.hi or not arr:\n            self.left = self.right = None\n            return\n        mid = (self.lo + self.hi) // 2\n        left_part, right_part = [], []\n        for x in arr:\n            go_right = 1 if x &gt; mid else 0\n            self.b.append(go_right)\n            self.pref.append(self.pref[-1] + go_right)\n            if go_right:\n                right_part.append(x)\n            else:\n                left_part.append(x)\n        self.left = WaveletTree(left_part, self.lo, mid)\n        self.right = WaveletTree(right_part, mid + 1, self.hi)\n\n    def rank1(self, idx):  # ones in b[1..idx]\n        return self.pref[idx]\n\n    def rank0(self, idx):  # zeros in b[1..idx]\n        return idx - self.rank1(idx)\n\n    def kth(self, l, r, k):\n        # 1-indexed positions\n        if self.lo == self.hi:\n            return self.lo\n        mid = (self.lo + self.hi) // 2\n        cnt_left = self.rank0(r) - self.rank0(l - 1)\n        if k &lt;= cnt_left:\n            nl = self.rank0(l - 1) + 1\n            nr = self.rank0(r)\n            return self.left.kth(nl, nr, k)\n        else:\n            nl = self.rank1(l - 1) + 1\n            nr = self.rank1(r)\n            return self.right.kth(nl, nr, k - cnt_left)\nUsage example:\nA = [3,1,4,1,5,9,2,6]\nwt = WaveletTree(A)\nprint(wt.kth(1, 8, 3))  # 3rd smallest in the whole array\n\n\nWhy It Matters\n\nCombines value partitioning with positional stability, enabling order statistics on subranges\nUnderpins succinct indexes, FM indexes, rank select dictionaries, and fast offline range queries\nEfficient when () is moderate or compressible\n\n\n\nA Gentle Proof of Bounds\nThe tree has height \\(O(\\log \\sigma)\\) since each level halves the value domain. Each query descends one level and performs \\(O(1)\\) rank operations on a bitvector. Therefore \\[\nT_{\\text{query}} = O(\\log \\sigma)\n\\] Space stores one bit per element per level on average, so \\[\nS = O(n \\log \\sigma)\n\\] With compressed bitvectors supporting constant time rank and select, these bounds hold in practice.\n\n\nTry It Yourself\n\nBuild a wavelet tree for \\(A = [2, 7, 1, 8, 2, 8, 1]\\).\nCompute \\(\\text{kth}(2, 6, 2)\\).\nCompute \\(\\text{range\\_count}(2, 7, 2, 8)\\).\nCompare against a naive sort on \\(A[l..r]\\).\n\n\n\nTest Cases\n\n\n\nArray\nQuery\nAnswer\n\n\n\n\n[3,1,4,1,5,9,2,6]\nkth(1,8,3)\n3\n\n\n[3,1,4,1,5,9,2,6]\nrange_count(3,7,2,5)\n3\n\n\n[1,1,1,1,1]\nrank(1,5)\n5\n\n\n[5,4,3,2,1]\nkth(2,5,2)\n3\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\n\\(O(n \\log \\sigma)\\)\n\\(O(n \\log \\sigma)\\)\n\n\nrank, select\n\\(O(\\log \\sigma)\\)\n–\n\n\nkth, range_count\n\\(O(\\log \\sigma)\\)\n–\n\n\n\nWavelet trees are a sharp tool for order aware range queries. By weaving bitvectors with stable partitions, they deliver succinct, logarithmic time answers on top of the original sequence.\n\n\n\n296 KD-Tree\nA KD-Tree (k-dimensional tree) is a binary space partitioning data structure for organizing points in a k-dimensional space. It enables fast range searches, nearest neighbor queries, and spatial indexing, often used in geometry, graphics, and machine learning (like k-NN).\n\nWhat Problem Are We Solving?\nWe need to store and query \\(n\\) points in \\(k\\)-dimensional space such that:\n\nNearest neighbor query: find the point closest to a query \\(q\\).\n\nRange query: find all points within a given region (rectangle or hypersphere).\n\nk-NN query: find the \\(k\\) closest points to \\(q\\).\n\nA naive search checks all \\(n\\) points, which takes \\(O(n)\\) time.\nA KD-Tree reduces this to \\(O(\\log n)\\) expected query time for balanced trees.\n\n\nHow It Works\n\n\n1. Recursive Partitioning by Dimension\nAt each level, the KD-Tree splits the set of points along one dimension, cycling through all dimensions.\nIf the current depth is \\(d\\), use the axis \\(a = d \\bmod k\\):\n\nSort points by their \\(a\\)-th coordinate.\n\nChoose the median point as the root to maintain balance.\n\nLeft child: points with smaller \\(a\\)-th coordinate.\n\nRight child: points with larger \\(a\\)-th coordinate.\n\n\n\n2. Search (Nearest Neighbor)\nTo find nearest neighbor of query \\(q\\):\n\nDescend tree following split planes (like BST).\nTrack current best (closest point found so far).\nBacktrack if potential closer point exists across split plane (distance to plane &lt; current best).\n\nThis ensures pruning of subtrees that cannot contain closer points.\n\n\nExample\nSuppose 2D points: \\[\nP = { (2,3), (5,4), (9,6), (4,7), (8,1), (7,2) }\n\\]\nStep 1: Root splits by (x)-axis (axis 0). Sorted by (x): ((2,3), (4,7), (5,4), (7,2), (8,1), (9,6)). Median = ((7,2)). Root = (7,2).\nStep 2:\n\nLeft subtree (points with (x &lt; 7)) split by (y)-axis.\nRight subtree (points with (x &gt; 7)) split by (y)-axis.\n\nThis creates alternating partitions by x and y, forming axis-aligned rectangles.\n\n\nTiny Code (Python)\nclass Node:\n    def __init__(self, point, axis):\n        self.point = point\n        self.axis = axis\n        self.left = None\n        self.right = None\n\ndef build_kdtree(points, depth=0):\n    if not points:\n        return None\n    k = len(points[0])\n    axis = depth % k\n    points.sort(key=lambda p: p[axis])\n    median = len(points) // 2\n    node = Node(points[median], axis)\n    node.left = build_kdtree(points[:median], depth + 1)\n    node.right = build_kdtree(points[median + 1:], depth + 1)\n    return node\n\n\n\nHow Nearest Neighbor Works\nGiven query (q), maintain best distance (d_{}). For each visited node:\n\nCompute distance (d = |q - p|)\nIf (d &lt; d_{}), update best\nCheck opposite branch only if (|q[a] - p[a]| &lt; d_{})\n\n\nExample Table\n\n\n\n\n\n\n\n\n\n\n\nStep\nNode Visited\nAxis\nCurrent Best\nDistance to Plane\nSearch Next\n\n\n\n\n1\n(7,2)\nx\n(7,2), 0.0\n0.0\nLeft\n\n\n2\n(5,4)\ny\n(5,4), 2.8\n2.0\nLeft\n\n\n3\n(2,3)\nx\n(5,4), 2.8\n3.0\nStop\n\n\n\n\n\nWhy It Matters\n\nEfficient spatial querying in multidimensional data.\nCommon in k-NN classification, computer graphics, and robotics pathfinding.\nBasis for libraries like scipy.spatial.KDTree.\n\n\n\nA Gentle Proof (Why It Works)\nEach level splits points into halves, forming \\(O(\\log n)\\) height. Each query visits a bounded number of nodes (dependent on dimension). Expected nearest neighbor cost:\n\\[\nT_{\\text{query}} = O(\\log n)\n\\]\nBuilding sorts points at each level:\n\\[\nT_{\\text{build}} = O(n \\log n)\n\\]\n\n\nTry It Yourself\n\nBuild KD-Tree for 2D points ([(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)]).\nQuery nearest neighbor of (q = (9,2)).\nTrace visited nodes, prune subtrees when possible.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nPoints\nQuery\nNearest\nExpected Path\n\n\n\n\n[(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)]\n(9,2)\n(8,1)\nRoot → Right → Leaf\n\n\n[(1,1),(2,2),(3,3)]\n(2,3)\n(2,2)\nRoot → Right\n\n\n[(0,0),(10,10)]\n(5,5)\n(10,10)\nRoot → Right\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\n\\(O(n \\log n)\\)\n\\(O(n)\\)\n\n\nNearest Neighbor\n\\(O(\\log n)\\) (expected)\n\\(O(\\log n)\\)\n\n\nRange Query\n\\(O(n^{1 - 1/k} + m)\\)\n–\n\n\n\nKD-Trees blend geometry with binary search, cutting space by dimensions to answer questions faster than brute force.\n\n\n\n297 Range Tree\nA Range Tree is a multi-level search structure for answering orthogonal range queries in multidimensional space, such as finding all points inside an axis-aligned rectangle. It extends 1D balanced search trees to higher dimensions using recursive trees on projections.\n\nWhat Problem Are We Solving?\nGiven a set of \\(n\\) points in \\(k\\)-dimensional space, we want to efficiently answer queries like:\n\n“List all points \\((x, y)\\) such that \\(x_1 \\le x \\le x_2\\) and \\(y_1 \\le y \\le y_2\\).”\n\nA naive scan is \\(O(n)\\) per query. Range Trees reduce this to \\(O(\\log^k n + m)\\), where \\(m\\) is the number of reported points.\n\n\nHow It Works\n\n\n1. 1D Case (Baseline)\nA simple balanced BST (e.g. AVL) on \\(x\\) coordinates supports range queries by traversing paths and collecting nodes in range.\n\n\n2. 2D Case (Extension)\n\nBuild a primary tree on \\(x\\) coordinates.\nAt each node, store a secondary tree built on \\(y\\) coordinates of the points in its subtree.\n\nEach level recursively maintains sorted views along other axes.\n\n\n3. Range Query\n\nSearch primary tree for split node \\(s\\), where paths to \\(x_1\\) and \\(x_2\\) diverge.\nFor nodes fully in \\([x_1, x_2]\\), query their associated \\(y\\)-tree for \\([y_1, y_2]\\).\nCombine results.\n\n\n\nExample\nGiven points:\n\\[\nP = {(2,3), (4,7), (5,1), (7,2), (8,5)}\n\\]\nQuery: \\([3,7] \\times [1,5]\\)\n\nPrimary tree on \\(x\\): median \\((5,1)\\) as root.\nSecondary trees at each node on \\(y\\).\n\nSearch path:\n\nSplit node \\((5,1)\\) covers \\(x \\in [3,7]\\).\nVisit subtrees with \\(x \\in [4,7]\\).\nQuery \\(y\\) in \\([1,5]\\) inside secondary trees.\n\nReturned points: \\((5,1), (7,2), (4,7)\\) → filter \\(y \\le 5\\) → \\((5,1),(7,2)\\).\n\n\nTiny Code (Python-like Pseudocode)\nclass RangeTree:\n    def __init__(self, points, depth=0):\n        if not points: \n            self.node = None\n            return\n        axis = depth % 2\n        points.sort(key=lambda p: p[axis])\n        mid = len(points) // 2\n        self.node = points[mid]\n        self.left = RangeTree(points[:mid], depth + 1)\n        self.right = RangeTree(points[mid + 1:], depth + 1)\n        self.sorted_y = sorted(points, key=lambda p: p[1])\nQuery recursively:\n\nFilter nodes by \\(x\\).\nBinary search on \\(y\\)-lists.\n\n\n\n\nStep-by-Step Table (2D Query)\n\n\n\n\n\n\n\n\n\n\nStep\nOperation\nAxis\nCondition\nAction\n\n\n\n\n1\nSplit at (5,1)\nx\n\\(3 \\le 5 \\le 7\\)\nRecurse both sides\n\n\n2\nLeft (2,3),(4,7)\nx\n\\(x &lt; 5\\)\nVisit (4,7) subtree\n\n\n3\nRight (7,2),(8,5)\nx\n\\(x \\le 7\\)\nVisit (7,2) subtree\n\n\n4\nFilter by \\(y\\)\ny\n\\(1 \\le y \\le 5\\)\nKeep (5,1),(7,2)\n\n\n\n\nWhy It Matters\nRange Trees provide deterministic performance for multidimensional queries, unlike kd-trees (which may degrade). They’re ideal for:\n\nOrthogonal range counting\nDatabase indexing\nComputational geometry problems\n\nThey are static structures, suited when data doesn’t change often.\n\n\nA Gentle Proof (Why It Works)\nEach dimension adds a logarithmic factor. In 2D, build time:\n\\[\nT(n) = O(n \\log n)\n\\]\nQuery visits \\(O(\\log n)\\) nodes in primary tree, each querying \\(O(\\log n)\\) secondary trees:\n\\[\nQ(n) = O(\\log^2 n + m)\n\\]\nSpace is \\(O(n \\log n)\\) due to secondary trees.\n\n\nTry It Yourself\n\nBuild a 2D Range Tree for \\(P = {(1,2),(2,3),(3,4),(4,5),(5,6)}\\)\nQuery rectangle \\([2,4] \\times [3,5]\\).\nTrace visited nodes and verify output.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nPoints\nQuery Rectangle\nResult\n\n\n\n\n[(2,3),(4,7),(5,1),(7,2),(8,5)]\n[3,7] × [1,5]\n(5,1),(7,2)\n\n\n[(1,2),(2,4),(3,6),(4,8),(5,10)]\n[2,4] × [4,8]\n(2,4),(3,6),(4,8)\n\n\n[(1,1),(2,2),(3,3)]\n[1,2] × [1,3]\n(1,1),(2,2)\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nBuild\n\\(O(n \\log n)\\)\n\\(O(n \\log n)\\)\n\n\nQuery (2D)\n\\(O(\\log^2 n + m)\\)\n,\n\n\nUpdate\n, (rebuild needed)\n,\n\n\n\nRange Trees are precise geometric indexes: each axis divides the space, and nested trees give fast access to all points in any axis-aligned box.\n\n\n\n298 Fenwick 2D Tree\nA 2D Fenwick Tree (also known as 2D Binary Indexed Tree) extends the 1D Fenwick Tree to handle range queries and point updates on 2D grids such as matrices. It efficiently computes prefix sums and supports dynamic updates in \\(O(\\log^2 n)\\) time.\n\nWhat Problem Are We Solving?\nGiven an \\(n \\times m\\) matrix \\(A\\), we want to support two operations efficiently:\n\nUpdate: Add a value \\(v\\) to element \\(A[x][y]\\).\nQuery: Compute the sum of all elements in submatrix \\([1..x][1..y]\\).\n\nA naive approach takes \\(O(nm)\\) per query. The 2D Fenwick Tree reduces both update and query to \\(O(\\log n \\cdot \\log m)\\).\n\n\nHow It Works\nEach node \\((i, j)\\) in the tree stores the sum of a submatrix region determined by the binary representation of its indices:\n\\[\nT[i][j] = \\sum_{x = i - 2^{r_i} + 1}^{i} \\sum_{y = j - 2^{r_j} + 1}^{j} A[x][y]\n\\]\nwhere \\(r_i\\) and \\(r_j\\) denote the least significant bit (LSB) of \\(i\\) and \\(j\\).\n\n\nUpdate Rule\nWhen updating \\((x, y)\\) by \\(v\\):\nfor i in range(x, n+1, i & -i):\n    for j in range(y, m+1, j & -j):\n        tree[i][j] += v\n\n\nQuery Rule\nTo compute prefix sum \\((1,1)\\) to \\((x,y)\\):\nres = 0\nfor i in range(x, 0, -i & -i):\n    for j in range(y, 0, -j & -j):\n        res += tree[i][j]\nreturn res\n\n\nRange Query\nSum of submatrix \\([(x_1,y_1),(x_2,y_2)]\\):\n\\[\nS = Q(x_2, y_2) - Q(x_1-1, y_2) - Q(x_2, y_1-1) + Q(x_1-1, y_1-1)\n\\]\n\n\nExample\nGiven a \\(4 \\times 4\\) matrix:\n\n\n\n\\(x/y\\)\n1\n2\n3\n4\n\n\n\n\n1\n2\n1\n0\n3\n\n\n2\n1\n2\n3\n1\n\n\n3\n0\n1\n2\n0\n\n\n4\n4\n0\n1\n2\n\n\n\nBuild Fenwick 2D Tree, then query sum in submatrix \\([2,2]\\) to \\([3,3]\\).\nExpected result:\n\\[\nA[2][2] + A[2][3] + A[3][2] + A[3][3] = 2 + 3 + 1 + 2 = 8\n\\]\n\n\n\nStep-by-Step Update Example\nSuppose we add \\(v = 5\\) at \\((2, 3)\\):\n\n\n\n\n\n\n\n\n\nStep\n\\((i,j)\\) Updated\nAdded Value\nReason\n\n\n\n\n1\n\\((2,3)\\)\n+5\nBase position\n\n\n2\n\\((2,4)\\)\n+5\nNext by \\(j += j \\& -j\\)\n\n\n3\n\\((4,3)\\)\n+5\nNext by \\(i += i \\& -i\\)\n\n\n4\n\\((4,4)\\)\n+5\nBoth indices propagate upward\n\n\n\n\nTiny Code (Python-like Pseudocode)\nclass Fenwick2D:\n    def __init__(self, n, m):\n        self.n, self.m = n, m\n        self.tree = [[0]*(m+1) for _ in range(n+1)]\n\n    def update(self, x, y, val):\n        i = x\n        while i &lt;= self.n:\n            j = y\n            while j &lt;= self.m:\n                self.tree[i][j] += val\n                j += j & -j\n            i += i & -i\n\n    def query(self, x, y):\n        res = 0\n        i = x\n        while i &gt; 0:\n            j = y\n            while j &gt; 0:\n                res += self.tree[i][j]\n                j -= j & -j\n            i -= i & -i\n        return res\n\n    def range_sum(self, x1, y1, x2, y2):\n        return (self.query(x2, y2) - self.query(x1-1, y2)\n                - self.query(x2, y1-1) + self.query(x1-1, y1-1))\n\n\nWhy It Matters\n2D Fenwick Trees are lightweight, dynamic structures for prefix sums and submatrix queries. They’re widely used in:\n\nImage processing (integral image updates)\nGrid-based dynamic programming\nCompetitive programming for 2D range queries\n\nThey trade slightly higher code complexity for excellent update-query efficiency.\n\n\nA Gentle Proof (Why It Works)\nEach update/query operation performs \\(\\log n\\) steps on \\(x\\) and \\(\\log m\\) on \\(y\\):\n\\[\nT(n,m) = O(\\log n \\cdot \\log m)\n\\]\nEach level adds contributions from subregions determined by LSB decomposition, ensuring every cell contributes exactly once to the query sum.\n\n\nTry It Yourself\n\nInitialize \\(4 \\times 4\\) Fenwick 2D.\nAdd \\(5\\) to \\((2,3)\\).\nQuery sum \\((1,1)\\) to \\((2,3)\\).\nVerify it matches manual computation.\n\n\n\nTest Cases\n\n\n\nMatrix (Partial)\nUpdate\nQuery Rectangle\nResult\n\n\n\n\n\\([ [2,1,0],[1,2,3],[0,1,2] ]\\)\n\\((2,3)+5\\)\n\\([1,1]\\)–\\([2,3]\\)\n14\n\n\n\\([ [1,1,1],[1,1,1],[1,1,1] ]\\)\n\\((3,3)+2\\)\n\\([2,2]\\)–\\([3,3]\\)\n5\n\n\n\\([ [4,0],[0,4] ]\\)\n,\n\\([1,1]\\)–\\([2,2]\\)\n8\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nUpdate\n\\(O(\\log n \\cdot \\log m)\\)\n\\(O(nm)\\)\n\n\nQuery\n\\(O(\\log n \\cdot \\log m)\\)\n,\n\n\n\nThe 2D Fenwick Tree is an elegant bridge between prefix sums and spatial queries, simple, powerful, and efficient for dynamic 2D grids.\n\n\n\n299 Treap Split/Merge\nA Treap Split/Merge algorithm allows you to divide and combine treaps (randomized balanced binary search trees) efficiently, using priority-based rotations and key-based ordering. This is the foundation for range operations like splits, merges, range updates, and segment queries on implicit treaps.\n\nWhat Problem Are We Solving?\nWe often need to:\n\nSplit a treap into two parts:\n\nAll keys \\(\\le k\\) go to the left treap\nAll keys \\(&gt; k\\) go to the right treap\n\nMerge two treaps \\(T_1\\) and \\(T_2\\) where all keys in \\(T_1 &lt; T_2\\)\n\nThese operations enable efficient range queries, persistent edits, and order-statistics while maintaining balanced height.\n\n\nHow It Works (Plain Language)\nTreaps combine two properties:\n\nBST Property: Left &lt; Root &lt; Right\nHeap Property: Node priority &gt; children priorities\n\nSplit and merge rely on recursive descent guided by key and priority.\n\n\nSplit Operation\nSplit treap \\(T\\) by key \\(k\\):\n\nIf \\(T.key \\le k\\), split \\(T.right\\) into \\((t2a, t2b)\\) and set \\(T.right = t2a\\)\nElse, split \\(T.left\\) into \\((t1a, t1b)\\) and set \\(T.left = t1b\\)\n\nReturn \\((T.left, T.right)\\)\n\n\nMerge Operation\nMerge \\(T_1\\) and \\(T_2\\):\n\nIf \\(T_1.priority &gt; T_2.priority\\), set \\(T_1.right = \\text{merge}(T_1.right, T_2)\\)\nElse, set \\(T_2.left = \\text{merge}(T_1, T_2.left)\\)\n\nReturn new root.\n\n\nExample\nSuppose we have treap with keys: \\[ [1, 2, 3, 4, 5, 6, 7] \\]\n\n\nSplit by \\(k = 4\\):\nLeft Treap: \\([1, 2, 3, 4]\\) Right Treap: \\([5, 6, 7]\\)\nNow, merge them back restores original order.\n\n\n\nStep-by-Step Split Example\n\n\n\nStep\nNode Key\nCompare to \\(k=4\\)\nAction\n\n\n\n\n1\n4\n\\(\\le\\) 4\nGo right\n\n\n2\n5\n\\(&gt;\\) 4\nSplit left subtree\n\n\n3\n5.left=∅$\nreturn (null,5)\nCombine back\n\n\n\nResult: left = \\([1,2,3,4]\\), right = \\([5,6,7]\\)\n\nTiny Code (Python-like Pseudocode)\nimport random\n\nclass Node:\n    def __init__(self, key):\n        self.key = key\n        self.priority = random.random()\n        self.left = None\n        self.right = None\n\ndef split(root, key):\n    if not root:\n        return (None, None)\n    if root.key &lt;= key:\n        left, right = split(root.right, key)\n        root.right = left\n        return (root, right)\n    else:\n        left, right = split(root.left, key)\n        root.left = right\n        return (left, root)\n\ndef merge(t1, t2):\n    if not t1 or not t2:\n        return t1 or t2\n    if t1.priority &gt; t2.priority:\n        t1.right = merge(t1.right, t2)\n        return t1\n    else:\n        t2.left = merge(t1, t2.left)\n        return t2\n\n\nWhy It Matters\nTreap split/merge unlocks flexible sequence manipulation and range-based operations:\n\nRange sum / min / max queries\nInsert or delete in \\(O(\\log n)\\)\nPersistent or implicit treaps for lists\nLazy propagation for intervals\n\nIt’s a key building block in functional and competitive programming data structures.\n\n\nA Gentle Proof (Why It Works)\nEach split or merge operation traverses down the height of the treap. Since treaps are expected balanced, height is \\(O(\\log n)\\).\n\nSplit correctness: Each recursive call preserves BST ordering.\nMerge correctness: Maintains heap property since highest priority becomes root.\n\nThus, both return valid treaps.\n\n\nTry It Yourself\n\nBuild treap with keys \\([1..7]\\).\nSplit by \\(k=4\\).\nPrint inorder traversals of both sub-treaps.\nMerge back. Confirm structure matches original.\n\n\n\nTest Cases\n\n\n\nInput Keys\nSplit Key\nLeft Treap Keys\nRight Treap Keys\n\n\n\n\n[1,2,3,4,5,6,7]\n4\n[1,2,3,4]\n[5,6,7]\n\n\n[10,20,30,40]\n25\n[10,20]\n[30,40]\n\n\n[5,10,15,20]\n5\n[5]\n[10,15,20]\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nSplit\n\\(O(\\log n)\\)\n\\(O(1)\\)\n\n\nMerge\n\\(O(\\log n)\\)\n\\(O(1)\\)\n\n\n\nTreap Split/Merge is the elegant heart of many dynamic set and sequence structures, one key, one random priority, two simple operations, infinite flexibility.\n\n\n\n300 Mo’s Algorithm on Tree\nMo’s Algorithm on Tree is an extension of the classical Mo’s algorithm used on arrays. It allows efficient processing of offline queries on trees, especially those involving subtrees or paths, by converting them into a linear order (Euler tour) and then applying a square root decomposition strategy.\n\nWhat Problem Are We Solving?\nWhen you need to answer multiple queries like:\n\n“How many distinct values are in the subtree of node \\(u\\)?”\n“What is the sum over the path from \\(u\\) to \\(v\\)?”\n\nA naive approach may require \\(O(n)\\) traversal per query, leading to \\(O(nq)\\) total complexity. Mo’s algorithm on trees reduces this to approximately \\(O((n + q)\\sqrt{n})\\), by reusing results from nearby queries.\n\n\nHow It Works (Plain Language)\n\nEuler Tour Flattening Transform the tree into a linear array using an Euler tour. Each node’s first appearance marks its position in the linearized sequence.\nQuery Transformation\n\nFor subtree queries, a subtree becomes a continuous range in the Euler array.\nFor path queries, break into two subranges and handle Least Common Ancestor (LCA) separately.\n\nMo’s Ordering Sort queries by:\n\nBlock of left endpoint (using \\(\\text{block} = \\lfloor L / \\sqrt{N} \\rfloor\\))\nRight endpoint (ascending or alternating per block)\n\nAdd/Remove Function Maintain a frequency map or running result as the window moves.\n\n\n\nExample\nGiven a tree:\n1\n├── 2\n│   ├── 4\n│   └── 5\n└── 3\nEuler Tour: [1, 2, 4, 4, 5, 5, 2, 3, 3, 1]\nSubtree(2): Range covering [2, 4, 4, 5, 5, 2]\nEach subtree query becomes a range query over the Euler array. Mo’s algorithm processes ranges efficiently in sorted order.\n\n\n\nStep-by-Step Example\n\n\n\nStep\nQuery (L,R)\nCurrent Range\nAdd/Remove\nResult\n\n\n\n\n1\n(2,6)\n[2,6]\n+4,+5\nCount=2\n\n\n2\n(2,8)\n[2,8]\n+3\nCount=3\n\n\n3\n(1,6)\n[1,6]\n-3,+1\nCount=3\n\n\n\nEach query is answered by incremental adjustment, not recomputation.\n\nTiny Code (Python-like Pseudocode)\nimport math\n\n# Preprocessing\ndef euler_tour(u, p, g, order):\n    order.append(u)\n    for v in g[u]:\n        if v != p:\n            euler_tour(v, u, g, order)\n            order.append(u)\n\n# Mo's structure\nclass Query:\n    def __init__(self, l, r, idx):\n        self.l, self.r, self.idx = l, r, idx\n\ndef mo_on_tree(n, queries, order, value):\n    block = int(math.sqrt(len(order)))\n    queries.sort(key=lambda q: (q.l // block, q.r))\n\n    freq = [0]*(n+1)\n    answer = [0]*len(queries)\n    cur = 0\n    L, R = 0, -1\n\n    def add(pos):\n        nonlocal cur\n        node = order[pos]\n        freq[node] += 1\n        if freq[node] == 1:\n            cur += value[node]\n\n    def remove(pos):\n        nonlocal cur\n        node = order[pos]\n        freq[node] -= 1\n        if freq[node] == 0:\n            cur -= value[node]\n\n    for q in queries:\n        while L &gt; q.l:\n            L -= 1\n            add(L)\n        while R &lt; q.r:\n            R += 1\n            add(R)\n        while L &lt; q.l:\n            remove(L)\n            L += 1\n        while R &gt; q.r:\n            remove(R)\n            R -= 1\n        answer[q.idx] = cur\n    return answer\n\n\nWhy It Matters\n\nConverts tree queries into range queries efficiently\nReuses computation by sliding window technique\nUseful for frequency, sum, or distinct count queries\nSupports subtree queries, path queries (with LCA handling), and color-based queries\n\n\n\nA Gentle Proof (Why It Works)\n\nEuler Tour guarantees every subtree is a contiguous range.\nMo’s algorithm ensures total number of add/remove operations is \\(O((n + q)\\sqrt{n})\\).\nCombining them, each query is handled incrementally within logarithmic amortized cost.\n\nThus, offline complexity is sublinear per query.\n\n\nTry It Yourself\n\nBuild Euler tour for tree of 7 nodes.\nWrite subtree queries for nodes \\(2,3,4\\).\nSort queries by block order.\nImplement add/remove logic to count distinct colors or sums.\nCompare performance with naive DFS per query.\n\n\n\nTest Cases\n\n\n\nNodes\nQuery\nExpected Result\n\n\n\n\n[1-2-3-4-5]\nSubtree(2)\nSum or count over nodes 2–5\n\n\n[1: {2,3}, 2:{4,5}]\nSubtree(1)\nAll nodes\n\n\n[1-2,1-3]\nPath(2,3)\nLCA=1 handled separately\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\nSpace\n\n\n\n\nPreprocessing (Euler)\n\\(O(n)\\)\n\\(O(n)\\)\n\n\nQuery Sorting\n\\(O(q \\log q)\\)\n\\(O(q)\\)\n\n\nProcessing\n\\(O((n + q)\\sqrt{n})\\)\n\\(O(n)\\)\n\n\n\nMo’s Algorithm on Tree is the elegant meeting point of graph traversal, offline range query, and amortized optimization, bringing sublinear query handling to complex hierarchical data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 3. Data Structure in Action</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html",
    "href": "books/en-us/list-4.html",
    "title": "Chapter 4. Graph Algorithms",
    "section": "",
    "text": "Section 31. Traversals",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-31.-traversals",
    "href": "books/en-us/list-4.html#section-31.-traversals",
    "title": "Chapter 4. Graph Algorithms",
    "section": "",
    "text": "301 Depth-First Search (Recursive)\nDepth-First Search (DFS) is the classic way to explore a graph deeply before backtracking. You pick a starting node, walk as far as possible along one path, and only when you hit a dead end do you turn back. It’s the “go deep first, ask questions later” of graph algorithms.\n\nWhat Problem Are We Solving?\nDFS helps us visit every vertex and edge in a connected component systematically. It’s the foundation for exploring graphs, detecting cycles, classifying edges, and building more complex algorithms like topological sort, strongly connected components, and articulation point detection.\nWe want an algorithm that:\n\nExplores all reachable vertices from a start node\nAvoids revisiting nodes\nRecords traversal order\n\nExample: You have a maze. DFS is the explorer that picks a path, goes as far as it can, and only turns back when stuck.\n\n\nHow Does It Work (Plain Language)?\nThink of DFS like a curious traveler: always dive deeper whenever you see a new path. When you can’t go further, step back one level and continue exploring.\nWe use recursion to model this behavior naturally, each recursive call represents entering a new node, and returning means backtracking.\n\n\n\n\n\n\n\n\n\nStep\nCurrent Node\nAction\nStack (Call Path)\n\n\n\n\n1\nA\nVisit A\n[A]\n\n\n2\nB\nVisit B (A→B)\n[A, B]\n\n\n3\nD\nVisit D (B→D)\n[A, B, D]\n\n\n4\nD has no unvisited neighbors\nBacktrack\n[A, B]\n\n\n5\nB’s next neighbor C\nVisit C\n[A, B, C]\n\n\n6\nC done\nBacktrack\n[A, B] → [A]\n\n\n7\nA’s remaining neighbors\nVisit next\n[…]\n\n\n\nWhen recursion unwinds, we’ve explored the whole reachable graph.\n\n\nTiny Code (Easy Versions)\nC (Adjacency List Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nbool visited[MAX];\nint n;\n\nvoid dfs(int v) {\n    visited[v] = true;\n    printf(\"%d \", v);\n    for (int u = 0; u &lt; n; u++) {\n        if (graph[v][u] && !visited[u]) {\n            dfs(u);\n        }\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n\n    printf(\"Enter adjacency matrix:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    printf(\"DFS starting from vertex 0:\\n\");\n    dfs(0);\n}\nPython (Adjacency List)\ngraph = {\n    0: [1, 2],\n    1: [2],\n    2: [0, 3],\n    3: [3]\n}\n\nvisited = set()\n\ndef dfs(v):\n    visited.add(v)\n    print(v, end=\" \")\n    for u in graph[v]:\n        if u not in visited:\n            dfs(u)\n\ndfs(0)\n\n\nWhy It Matters\n\nCore for graph exploration and reachability\nForms the basis of topological sort, SCC, bridges, and cycles\nSimple recursive structure reveals natural hierarchy of a graph\nHelps understand backtracking and stack-based thinking\n\n\n\nA Gentle Proof (Why It Works)\nEach vertex is visited exactly once:\n\nWhen a node is first discovered, it’s marked visited\nThe recursion ensures all its neighbors are explored\nOnce all children are done, the function returns (backtrack)\n\nSo every vertex v triggers one call dfs(v), giving O(V + E) time (each edge explored once).\n\n\nTry It Yourself\n\nDraw a small graph (A–B–C–D) and trace DFS.\nModify to print entry and exit times.\nTrack parent nodes to build DFS tree.\nAdd detection for back edges (cycle test).\nCompare with BFS traversal order.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nStart\nExpected Order\nNotes\n\n\n\n\nA–B–C–D chain\nA\nA B C D\nStraight path\n\n\nTriangle (A–B–C–A)\nA\nA B C\nVisits all, stops at visited A\n\n\nDisconnected {A–B}, {C–D}\nA\nA B\nOnly reachable component\n\n\nDirected A→B→C\nA\nA B C\nLinear chain\n\n\nTree root 0\n0\n0 1 3 4 2 5\nDepends on adjacency order\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V) (recursion stack + visited)\n\nDFS is your first lens into graph structure, recursive, elegant, and revealing hidden pathways one stack frame at a time.\n\n\n\n302 Depth-First Search (Iterative)\nDepth-First Search can run without recursion too. Instead of leaning on the call stack, we build our own stack explicitly. It’s the same journey, diving deep before backtracking, just with manual control over what’s next.\n\nWhat Problem Are We Solving?\nRecursion is elegant but not always practical. Some graphs are deep, and recursive DFS can overflow the call stack. The iterative version solves that by using a stack data structure directly, mirroring the same traversal order.\nWe want an algorithm that:\n\nWorks even when recursion is too deep\nExplicitly manages visited nodes and stack\nProduces the same traversal as recursive DFS\n\nExample: Think of it like keeping your own to-do list of unexplored paths, each time you go deeper, you add new destinations on top of the stack.\n\n\nHow Does It Work (Plain Language)?\nWe maintain a stack:\n\nStart from a node s, push it on the stack.\nPop the top node v.\nIf v is unvisited, mark and process it.\nPush all unvisited neighbors of v onto the stack.\nRepeat until the stack is empty.\n\n\n\n\nStep\nStack (Top → Bottom)\nAction\nVisited\n\n\n\n\n1\n[A]\nStart, pop A\n{A}\n\n\n2\n[B, C]\nPush neighbors of A\n{A}\n\n\n3\n[C, B]\nPop B, visit B\n{A, B}\n\n\n4\n[C, D]\nPush neighbors of B\n{A, B}\n\n\n5\n[D, C]\nPop D, visit D\n{A, B, D}\n\n\n6\n[C]\nContinue\n{A, B, D}\n\n\n7\n[ ]\nPop C, visit C\n{A, B, C, D}\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Adjacency Matrix Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nbool visited[MAX];\nint stack[MAX];\nint top = -1;\nint n;\n\nvoid push(int v) { stack[++top] = v; }\nint pop() { return stack[top--]; }\nbool is_empty() { return top == -1; }\n\nvoid dfs_iterative(int start) {\n    push(start);\n    while (!is_empty()) {\n        int v = pop();\n        if (!visited[v]) {\n            visited[v] = true;\n            printf(\"%d \", v);\n            for (int u = n - 1; u &gt;= 0; u--) { // reverse for consistent order\n                if (graph[v][u] && !visited[u])\n                    push(u);\n            }\n        }\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n\n    printf(\"Enter adjacency matrix:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    printf(\"Iterative DFS from 0:\\n\");\n    dfs_iterative(0);\n}\nPython (Using List as Stack)\ngraph = {\n    0: [1, 2],\n    1: [2],\n    2: [0, 3],\n    3: [3]\n}\n\nvisited = set()\nstack = [0]\n\nwhile stack:\n    v = stack.pop()\n    if v not in visited:\n        visited.add(v)\n        print(v, end=\" \")\n        for u in reversed(graph[v]):  # reversed for DFS-like order\n            if u not in visited:\n                stack.append(u)\n\n\nWhy It Matters\n\nAvoids recursion limits and stack overflow\nClear control over traversal order\nGood for systems with limited call stack\nBuilds understanding of explicit stack simulation\n\n\n\nA Gentle Proof (Why It Works)\nThe stack mimics recursion: Each vertex v is processed once when popped, and its neighbors are pushed. Every edge is examined exactly once. So total operations = O(V + E), same as recursive DFS.\nEach push = one recursive call; each pop = one return.\n\n\nTry It Yourself\n\nTrace iterative DFS on a small graph.\nCompare the order with the recursive version.\nExperiment with neighbor push order, see how output changes.\nAdd discovery and finishing times.\nConvert to iterative topological sort by pushing finishing order to a second stack.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nStart\nOrder (One Possible)\nNotes\n\n\n\n\n0–1–2 chain\n0\n0 1 2\nSimple path\n\n\n0→1, 0→2, 1→3\n0\n0 1 3 2\nDepends on neighbor order\n\n\nCycle 0→1→2→0\n0\n0 1 2\nNo repeats\n\n\nDisconnected\n0\n0 1 2\nOnly connected part\n\n\nComplete graph (4 nodes)\n0\n0 1 2 3\nVisits all once\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V) for stack and visited array\n\nIterative DFS is your manual-gear version of recursion, same depth, same discovery, just no surprises from the call stack.\n\n\n\n303 Breadth-First Search (Queue)\nBreadth-First Search (BFS) is the explorer that moves level by level, radiating outward from the start. Instead of diving deep like DFS, BFS keeps things fair, it visits all neighbors before going deeper.\n\nWhat Problem Are We Solving?\nWe want a way to:\n\nExplore all reachable vertices in a graph\nDiscover the shortest path in unweighted graphs\nProcess nodes in increasing distance order\n\nBFS is perfect when:\n\nEdges all have equal weight (like 1)\nYou need the fewest steps to reach a goal\nYou’re finding connected components, levels, or distances\n\nExample: Imagine spreading a rumor. Each person tells all their friends before the next wave begins, that’s BFS in action.\n\n\nHow Does It Work (Plain Language)?\nBFS uses a queue, a first-in, first-out line.\n\nStart from a node s\nMark it visited and enqueue it\nWhile queue not empty:\n\nDequeue front node v\nVisit v\nEnqueue all unvisited neighbors of v\n\n\n\n\n\n\n\n\n\n\n\nStep\nQueue (Front → Back)\nVisited\nAction\n\n\n\n\n1\n[A]\n{A}\nStart\n\n\n2\n[B, C]\n{A, B, C}\nA’s neighbors\n\n\n3\n[C, D, E]\n{A, B, C, D, E}\nVisit B, add its neighbors\n\n\n4\n[D, E, F]\n{A, B, C, D, E, F}\nVisit C, add neighbors\n\n\n5\n[E, F]\n{A…F}\nContinue until empty\n\n\n\nThe order you dequeue = level order traversal.\n\n\nTiny Code (Easy Versions)\nC (Adjacency Matrix Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nbool visited[MAX];\nint queue[MAX];\nint front = 0, rear = 0;\nint n;\n\nvoid enqueue(int v) { queue[rear++] = v; }\nint dequeue() { return queue[front++]; }\nbool is_empty() { return front == rear; }\n\nvoid bfs(int start) {\n    visited[start] = true;\n    enqueue(start);\n\n    while (!is_empty()) {\n        int v = dequeue();\n        printf(\"%d \", v);\n\n        for (int u = 0; u &lt; n; u++) {\n            if (graph[v][u] && !visited[u]) {\n                visited[u] = true;\n                enqueue(u);\n            }\n        }\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n\n    printf(\"Enter adjacency matrix:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    printf(\"BFS starting from vertex 0:\\n\");\n    bfs(0);\n}\nPython (Adjacency List)\nfrom collections import deque\n\ngraph = {\n    0: [1, 2],\n    1: [3, 4],\n    2: [5],\n    3: [],\n    4: [],\n    5: []\n}\n\nvisited = set()\nqueue = deque([0])\nvisited.add(0)\n\nwhile queue:\n    v = queue.popleft()\n    print(v, end=\" \")\n    for u in graph[v]:\n        if u not in visited:\n            visited.add(u)\n            queue.append(u)\n\n\nWhy It Matters\n\nFinds shortest paths in unweighted graphs\nGuarantees level order visitation\nCore for algorithms like 0–1 BFS, SPFA, and Dijkstra’s\nExcellent for layer-based exploration and distance labeling\n\n\n\nA Gentle Proof (Why It Works)\nEach vertex is visited exactly once:\n\nIt’s enqueued when discovered\nIt’s dequeued once for processing\nEach edge is checked once\n\nIf all edges have weight 1, BFS discovers vertices in increasing distance order, proving shortest-path correctness.\nTime complexity:\n\nVisiting each vertex: O(V)\nScanning each edge: O(E) → Total: O(V + E)\n\n\n\nTry It Yourself\n\nDraw a small unweighted graph and run BFS by hand.\nRecord levels (distance from start).\nTrack parent of each vertex, reconstruct shortest path.\nTry BFS on a tree, compare with level-order traversal.\nExperiment on disconnected graphs, note what gets missed.\n\n\n\nTest Cases\n\n\n\nGraph\nStart\nOrder\nDistance\n\n\n\n\n0–1–2 chain\n0\n0 1 2\n[0,1,2]\n\n\nTriangle 0–1–2\n0\n0 1 2\n[0,1,1]\n\n\nStar 0→{1,2,3}\n0\n0 1 2 3\n[0,1,1,1]\n\n\nGrid 2×2\n0\n0 1 2 3\nLayered\n\n\nDisconnected\n0\n0 1\nOnly component of 0\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V) for queue and visited set\n\nBFS is your wavefront explorer, fair, systematic, and always shortest when edges are equal.\n\n\n\n304 Iterative Deepening DFS\nIterative Deepening Depth-First Search (IDDFS) blends the depth control of BFS with the space efficiency of DFS. It repeatedly performs DFS with increasing depth limits, uncovering nodes level by level, but through deep-first exploration each time.\n\nWhat Problem Are We Solving?\nPure DFS may wander too deep, missing nearer solutions. Pure BFS finds shortest paths but consumes large memory.\nWe need a search that:\n\nFinds shallowest solution like BFS\nUses O(depth) memory like DFS\nWorks in infinite or very large search spaces\n\nThat’s where IDDFS shines, it performs a DFS up to a limit, then restarts with a deeper limit, repeating until the goal is found.\nExample: Think of a diver who explores deeper with each dive, 1 meter, 2 meters, 3 meters, always sweeping from the surface down.\n\n\nHow Does It Work (Plain Language)?\nEach iteration increases the depth limit by one. At each stage, we perform a DFS that stops when depth exceeds the current limit.\n\nSet limit = 0\nRun DFS with depth limit = 0\nIf not found, increase limit and repeat\nContinue until goal found or all explored\n\n\n\n\nIteration\nDepth Limit\nNodes Explored\nFound Goal?\n\n\n\n\n1\n0\nStart node\nNo\n\n\n2\n1\nStart + neighbors\nNo\n\n\n3\n2\n+ deeper nodes\nPossibly\n\n\n…\n…\n…\n…\n\n\n\nAlthough nodes are revisited, total cost remains efficient, like BFS’s layer-wise discovery.\n\n\nTiny Code (Easy Versions)\nC (Depth-Limited DFS)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nint n;\nbool found = false;\n\nvoid dls(int v, int depth, int limit, bool visited[]) {\n    visited[v] = true;\n    printf(\"%d \", v);\n    if (depth == limit) return;\n    for (int u = 0; u &lt; n; u++) {\n        if (graph[v][u] && !visited[u]) {\n            dls(u, depth + 1, limit, visited);\n        }\n    }\n}\n\nvoid iddfs(int start, int max_depth) {\n    for (int limit = 0; limit &lt;= max_depth; limit++) {\n        bool visited[MAX] = {false};\n        printf(\"\\nDepth limit %d: \", limit);\n        dls(start, 0, limit, visited);\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n\n    printf(\"Enter adjacency matrix:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    iddfs(0, 3);\n}\nPython (Adjacency List + Depth Limit)\ngraph = {\n    0: [1, 2],\n    1: [3],\n    2: [4],\n    3: [],\n    4: []\n}\n\ndef dls(v, depth, limit, visited):\n    visited.add(v)\n    print(v, end=\" \")\n    if depth == limit:\n        return\n    for u in graph[v]:\n        if u not in visited:\n            dls(u, depth + 1, limit, visited)\n\ndef iddfs(start, max_depth):\n    for limit in range(max_depth + 1):\n        print(f\"\\nDepth limit {limit}:\", end=\" \")\n        visited = set()\n        dls(start, 0, limit, visited)\n\niddfs(0, 3)\n\n\nWhy It Matters\n\nCombines advantages of BFS and DFS\nFinds optimal solution in unweighted graphs\nUses linear space\nIdeal for state-space search (AI, puzzles)\n\n\n\nA Gentle Proof (Why It Works)\nBFS guarantees shortest path; DFS uses less space. IDDFS repeats DFS with increasing limits, ensuring that:\n\nAll nodes at depth d are visited before depth d+1\nSpace = O(d)\nTime ≈ O(b^d), similar to BFS in order\n\nRedundant work (revisiting nodes) is small compared to total nodes in deeper layers.\n\n\nTry It Yourself\n\nRun IDDFS on a tree; observe repeated shallow visits.\nCount nodes visited per iteration.\nCompare total visits with BFS.\nModify depth limit mid-run, what happens?\nUse IDDFS to find a goal node at depth 3.\n\n\n\nTest Cases\n\n\n\nGraph\nGoal\nMax Depth\nFound At\nOrder Example\n\n\n\n\n0→1→2→3\n3\n3\nDepth 3\n0 1 2 3\n\n\n0→{1,2}, 1→3\n3\n3\nDepth 2\n0 1 3\n\n\nStar 0→{1,2,3}\n3\n1\nDepth 1\n0 1 2 3\n\n\n0→1→2→Goal\nGoal=2\n2\nDepth 2\n0 1 2\n\n\n\n\n\nComplexity\n\nTime: O(b^d) (like BFS)\nSpace: O(d) (like DFS)\n\nIterative Deepening DFS is the patient climber, revisiting familiar ground, going deeper each time, ensuring no shallow treasure is missed.\n\n\n\n305 Bidirectional BFS\nBidirectional BFS is the meet-in-the-middle version of BFS. Instead of starting from one end and exploring everything outward, we launch two BFS waves, one from the source and one from the target, and stop when they collide in the middle. It’s like digging a tunnel from both sides of a mountain to meet halfway.\n\nWhat Problem Are We Solving?\nStandard BFS explores the entire search space outward from the start until it reaches the goal, great for small graphs, but expensive when the graph is huge.\nBidirectional BFS cuts that exploration dramatically by searching both directions at once, halving the effective search depth.\nWe want an algorithm that:\n\nFinds the shortest path in an unweighted graph\nExplores fewer nodes than single-source BFS\nStops as soon as the two waves meet\n\nExample: You’re finding the shortest route between two cities. Instead of exploring from one city across the whole map, you also send scouts from the destination. They meet somewhere, the midpoint of the shortest path.\n\n\nHow Does It Work (Plain Language)?\nRun two BFS searches simultaneously, one forward, one backward. At each step, expand the smaller frontier first to balance work. Stop when any node appears in both visited sets.\n\nStart BFS from source and target\nMaintain two queues and two visited sets\nAlternate expansions\nWhen visited sets overlap, meeting point found\nCombine paths for the final route\n\n\n\n\n\n\n\n\n\n\n\nStep\nForward Queue\nBackward Queue\nIntersection\nAction\n\n\n\n\n1\n[S]\n[T]\n∅\nStart\n\n\n2\n[S1, S2]\n[T1, T2]\n∅\nExpand both\n\n\n3\n[S2, S3]\n[T1, S2]\nS2\nFound meeting node\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Simplified Version)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nint n;\n\nbool bfs_step(bool visited[], int queue[], int *front, int *rear) {\n    int size = *rear - *front;\n    while (size--) {\n        int v = queue[(*front)++];\n        for (int u = 0; u &lt; n; u++) {\n            if (graph[v][u] && !visited[u]) {\n                visited[u] = true;\n                queue[(*rear)++] = u;\n            }\n        }\n    }\n    return false;\n}\n\nbool intersect(bool a[], bool b[]) {\n    for (int i = 0; i &lt; n; i++)\n        if (a[i] && b[i]) return true;\n    return false;\n}\n\nbool bidir_bfs(int src, int dest) {\n    bool vis_s[MAX] = {false}, vis_t[MAX] = {false};\n    int qs[MAX], qt[MAX];\n    int fs = 0, rs = 0, ft = 0, rt = 0;\n    qs[rs++] = src; vis_s[src] = true;\n    qt[rt++] = dest; vis_t[dest] = true;\n\n    while (fs &lt; rs && ft &lt; rt) {\n        if (bfs_step(vis_s, qs, &fs, &rs)) return true;\n        if (intersect(vis_s, vis_t)) return true;\n\n        if (bfs_step(vis_t, qt, &ft, &rt)) return true;\n        if (intersect(vis_s, vis_t)) return true;\n    }\n    return false;\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    int src = 0, dest = n - 1;\n    if (bidir_bfs(src, dest))\n        printf(\"Path found\\n\");\n    else\n        printf(\"No path\\n\");\n}\nPython (Readable Version)\nfrom collections import deque\n\ngraph = {\n    0: [1, 2],\n    1: [3],\n    2: [4],\n    3: [5],\n    4: [5],\n    5: []\n}\n\ndef bidirectional_bfs(src, dest):\n    if src == dest:\n        return True\n\n    q1, q2 = deque([src]), deque([dest])\n    visited1, visited2 = {src}, {dest}\n\n    while q1 and q2:\n        # Expand forward\n        for _ in range(len(q1)):\n            v = q1.popleft()\n            for u in graph[v]:\n                if u in visited2:\n                    return True\n                if u not in visited1:\n                    visited1.add(u)\n                    q1.append(u)\n\n        # Expand backward\n        for _ in range(len(q2)):\n            v = q2.popleft()\n            for u in graph[v]:\n                if u in visited1:\n                    return True\n                if u not in visited2:\n                    visited2.add(u)\n                    q2.append(u)\n\n    return False\n\nprint(bidirectional_bfs(0, 5))\n\n\nWhy It Matters\n\nFaster shortest-path search on large graphs\nReduces explored nodes from O(b^d) to roughly O(b^(d/2))\nExcellent for pathfinding in maps, puzzles, or networks\nDemonstrates search symmetry and frontier balancing\n\n\n\nA Gentle Proof (Why It Works)\nIf the shortest path length is d, BFS explores O(b^d) nodes, but Bidirectional BFS explores 2×O(b^(d/2)) nodes — a huge savings since b^(d/2) ≪ b^d.\nEach side guarantees the frontier grows level by level, and intersection ensures meeting at the middle of the shortest path.\n\n\nTry It Yourself\n\nTrace bidirectional BFS on a 5-node chain (0→1→2→3→4).\nCount nodes visited by single BFS vs bidirectional BFS.\nAdd print statements to see where the waves meet.\nModify to reconstruct the path.\nCompare performance on branching graphs.\n\n\n\nTest Cases\n\n\n\nGraph\nSource\nTarget\nFound?\nMeeting Node\n\n\n\n\n0–1–2–3–4\n0\n4\n✅\n2\n\n\n0→1, 1→2, 2→3\n0\n3\n✅\n1 or 2\n\n\n0→1, 2→3 (disconnected)\n0\n3\n❌\n–\n\n\nTriangle 0–1–2–0\n0\n2\n✅\n0 or 2\n\n\nStar 0→{1,2,3,4}\n1\n2\n✅\n0\n\n\n\n\n\nComplexity\n\nTime: O(b^(d/2))\nSpace: O(b^(d/2))\nOptimality: Finds shortest path in unweighted graphs\n\nBidirectional BFS is the bridge builder, starting from both shores, racing toward the meeting point in the middle.\n\n\n\n306 DFS on Grid\nDFS on a grid is your go-to for exploring 2D maps, mazes, or islands. It works just like DFS on graphs, but here, each cell is a node and its up/down/left/right neighbors form the edges. Perfect for connected component detection, region labeling, or maze solving.\n\nWhat Problem Are We Solving?\nWe want to explore or mark all connected cells in a grid, often used for:\n\nCounting islands in a binary matrix\nFlood-fill algorithms (coloring regions)\nMaze traversal (finding a path through walls)\nConnectivity detection in 2D maps\n\nExample: Think of a painter pouring ink into one cell, DFS shows how the ink spreads to fill the entire connected region.\n\n\nHow Does It Work (Plain Language)?\nDFS starts from a given cell, visits it, and recursively explores all valid, unvisited neighbors.\nWe check 4 directions (or 8 if diagonals count). Each neighbor is:\n\nWithin bounds\nNot yet visited\nSatisfies the condition (e.g., same color, value = 1)\n\n\n\n\nStep\nCurrent Cell\nAction\nStack (Call Path)\n\n\n\n\n1\n(0,0)\nVisit\n[(0,0)]\n\n\n2\n(0,1)\nMove right\n[(0,0),(0,1)]\n\n\n3\n(1,1)\nMove down\n[(0,0),(0,1),(1,1)]\n\n\n4\n(1,1) has no new neighbors\nBacktrack\n[(0,0),(0,1)]\n\n\n5\nContinue\n…\n\n\n\n\nThe traversal ends when all reachable cells are visited.\n\n\nTiny Code (Easy Versions)\nC (DFS for Island Counting)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint grid[MAX][MAX];\nbool visited[MAX][MAX];\nint n, m;\n\nint dx[4] = {-1, 1, 0, 0};\nint dy[4] = {0, 0, -1, 1};\n\nvoid dfs(int x, int y) {\n    visited[x][y] = true;\n    for (int k = 0; k &lt; 4; k++) {\n        int nx = x + dx[k];\n        int ny = y + dy[k];\n        if (nx &gt;= 0 && nx &lt; n && ny &gt;= 0 && ny &lt; m &&\n            grid[nx][ny] == 1 && !visited[nx][ny]) {\n            dfs(nx, ny);\n        }\n    }\n}\n\nint main(void) {\n    printf(\"Enter grid size (n m): \");\n    scanf(\"%d %d\", &n, &m);\n    printf(\"Enter grid (0/1):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; m; j++)\n            scanf(\"%d\", &grid[i][j]);\n\n    int count = 0;\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; m; j++)\n            if (grid[i][j] == 1 && !visited[i][j]) {\n                dfs(i, j);\n                count++;\n            }\n\n    printf(\"Number of islands: %d\\n\", count);\n}\nPython (Flood Fill)\ngrid = [\n    [1,1,0,0],\n    [1,0,0,1],\n    [0,0,1,1],\n    [0,0,0,1]\n]\n\nn, m = len(grid), len(grid[0])\nvisited = [[False]*m for _ in range(n)]\n\ndef dfs(x, y):\n    if x &lt; 0 or x &gt;= n or y &lt; 0 or y &gt;= m:\n        return\n    if grid[x][y] == 0 or visited[x][y]:\n        return\n    visited[x][y] = True\n    for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n        dfs(x + dx, y + dy)\n\ncount = 0\nfor i in range(n):\n    for j in range(m):\n        if grid[i][j] == 1 and not visited[i][j]:\n            dfs(i, j)\n            count += 1\n\nprint(\"Number of islands:\", count)\n\n\nWhy It Matters\n\nCore tool for grid exploration and region labeling\nForms the heart of island problems, maze solvers, and map connectivity\nDemonstrates DFS behavior in real-world layouts\nEasy visualization and debugging on 2D arrays\n\n\n\nA Gentle Proof (Why It Works)\nEach cell is visited exactly once, marked visited upon entry. Recursive calls spread to all valid neighbors. So total time = proportional to number of cells and edges (neighbors).\nIf grid has size n × m, and each cell checks 4 neighbors:\n\nTime: O(n × m)\nSpace: O(n × m) visited + recursion depth (≤ n × m)\n\nDFS guarantees every reachable cell is visited exactly once, forming connected components.\n\n\nTry It Yourself\n\nChange movement to 8 directions (include diagonals).\nModify to flood-fill a color (e.g., replace all 1s with 2s).\nCount components in a matrix of characters (‘X’, ‘O’).\nVisualize traversal order in a printed grid.\nCompare with BFS on the same grid.\n\n\n\nTest Cases\n\n\n\nGrid\nExpected\nDescription\n\n\n\n\n[[1,0,0],[0,1,0],[0,0,1]]\n3\nDiagonal not connected\n\n\n[[1,1,0],[1,0,0],[0,0,1]]\n2\nTwo clusters\n\n\n[[1,1,1],[1,1,1],[1,1,1]]\n1\nOne big island\n\n\n[[0,0,0],[0,0,0]]\n0\nNo land\n\n\n[[1,0,1],[0,1,0],[1,0,1]]\n5\nMany singles\n\n\n\n\n\nComplexity\n\nTime: O(n × m)\nSpace: O(n × m) (visited) or O(depth) recursion stack\n\nDFS on grid is your map explorer, sweeping through every reachable patch, one cell at a time.\n\n\n\n307 BFS on Grid\nBFS on a grid explores cells level by level, making it perfect for shortest paths in unweighted grids, minimum steps in mazes, and distance labeling from a source. Each cell is a node and edges connect to neighbors such as up, down, left, right.\n\nWhat Problem Are We Solving?\nWe want to:\n\nFind the shortest path from a start cell to a goal cell when each move costs the same\nCompute a distance map from a source to all reachable cells\nHandle obstacles cleanly and avoid revisiting\n\nExample: Given a maze as a 0 or 1 grid, where 0 is free and 1 is wall, BFS finds the fewest moves from start to target.\n\n\nHow Does It Work (Plain Language)?\nUse a queue. Start from the source, push it with distance 0, and expand in waves. At each step, pop the front cell, try its neighbors, mark unseen neighbors visited, and record their distance as current distance + 1.\n\n\n\n\n\n\n\n\n\n\nStep\nQueue (front to back)\nCurrent Cell\nAction\nDistance Updated\n\n\n\n\n1\n[(sx, sy)]\n(sx, sy)\nStart\ndist[sx][sy] = 0\n\n\n2\n[(n1), (n2)]\n(n1)\nVisit neighbors\ndist[n1] = 1\n\n\n3\n[(n2), (n3), (n4)]\n(n2)\nContinue\ndist[n2] = 1\n\n\n4\n[…]\n…\nWave expands\ndist[next] = dist[cur] + 1\n\n\n\nThe first time you reach the goal, the recorded distance is minimal.\n\n\nTiny Code (Easy Versions)\nC (Shortest Path on a 0 or 1 Grid)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 200\nint n, m;\nint grid[MAX][MAX];         // 0 free, 1 wall\nint distv[MAX][MAX];        // distance map\nbool vis[MAX][MAX];\n\nint qx[MAX*MAX], qy[MAX*MAX];\nint front = 0, rear = 0;\n\nint dx[4] = {-1, 1, 0, 0};\nint dy[4] = {0, 0, -1, 1};\n\nvoid enqueue(int x, int y) { qx[rear] = x; qy[rear] = y; rear++; }\nvoid dequeue(int *x, int *y) { *x = qx[front]; *y = qy[front]; front++; }\nbool empty() { return front == rear; }\n\nint bfs(int sx, int sy, int tx, int ty) {\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; m; j++) {\n            vis[i][j] = false;\n            distv[i][j] = -1;\n        }\n\n    vis[sx][sy] = true;\n    distv[sx][sy] = 0;\n    enqueue(sx, sy);\n\n    while (!empty()) {\n        int x, y;\n        dequeue(&x, &y);\n        if (x == tx && y == ty) return distv[x][y];\n\n        for (int k = 0; k &lt; 4; k++) {\n            int nx = x + dx[k], ny = y + dy[k];\n            if (nx &gt;= 0 && nx &lt; n && ny &gt;= 0 && ny &lt; m &&\n                !vis[nx][ny] && grid[nx][ny] == 0) {\n                vis[nx][ny] = true;\n                distv[nx][ny] = distv[x][y] + 1;\n                enqueue(nx, ny);\n            }\n        }\n    }\n    return -1; // unreachable\n}\n\nint main(void) {\n    printf(\"Enter n m: \");\n    scanf(\"%d %d\", &n, &m);\n    printf(\"Enter grid (0 free, 1 wall):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; m; j++)\n            scanf(\"%d\", &grid[i][j]);\n\n    int sx, sy, tx, ty;\n    printf(\"Enter start sx sy and target tx ty: \");\n    scanf(\"%d %d %d %d\", &sx, &sy, &tx, &ty);\n\n    int d = bfs(sx, sy, tx, ty);\n    if (d &gt;= 0) printf(\"Shortest distance: %d\\n\", d);\n    else printf(\"No path\\n\");\n}\nPython (Distance Map and Path Reconstruction)\nfrom collections import deque\n\ngrid = [\n    [0,0,0,1],\n    [1,0,0,0],\n    [0,0,1,0],\n    [0,0,0,0]\n]\nn, m = len(grid), len(grid[0])\n\ndef bfs_grid(sx, sy, tx, ty):\n    dist = [[-1]*m for _ in range(n)]\n    parent = [[None]*m for _ in range(n)]\n    q = deque()\n    q.append((sx, sy))\n    dist[sx][sy] = 0\n\n    for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n        pass  # only to show directions exist\n\n    while q:\n        x, y = q.popleft()\n        if (x, y) == (tx, ty):\n            break\n        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n            nx, ny = x + dx, y + dy\n            if 0 &lt;= nx &lt; n and 0 &lt;= ny &lt; m and grid[nx][ny] == 0 and dist[nx][ny] == -1:\n                dist[nx][ny] = dist[x][y] + 1\n                parent[nx][ny] = (x, y)\n                q.append((nx, ny))\n\n    # Reconstruct path if reachable\n    if dist[tx][ty] == -1:\n        return dist, []\n    path = []\n    cur = (tx, ty)\n    while cur:\n        path.append(cur)\n        cur = parent[cur[0]][cur[1]]\n    path.reverse()\n    return dist, path\n\ndist, path = bfs_grid(0, 0, 3, 3)\nprint(\"Distance to target:\", dist[3][3])\nprint(\"Path:\", path)\n\n\nWhy It Matters\n\nGuarantees shortest path in unweighted grids\nProduces a full distance transform useful for many tasks\nRobust and simple for maze solvers and robotics navigation\nNatural stepping stone to 0 1 BFS and Dijkstra\n\n\n\nA Gentle Proof (Why It Works)\nBFS processes cells by nondecreasing distance from the source. When a cell is first dequeued, the stored distance equals the minimum number of moves needed to reach it. Each free neighbor is discovered with distance plus one. Therefore the first time the goal is reached, that distance is minimal.\n\nEach cell enters the queue at most once\nEach edge between neighboring cells is considered once\n\nHence total work is linear in the number of cells and neighbor checks.\n\n\nTry It Yourself\n\nAdd 8 directional moves and compare paths with 4 directional moves.\nAdd teleporters by connecting listed cell pairs as edges.\nConvert to multi source BFS by enqueuing several starts with distance 0.\nBlock some cells and verify that BFS never steps through walls.\nRecord parents and print the maze with the path marked.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGrid\nStart\nTarget\nExpected\n\n\n\n\n2 x 2 all free\n(0,0)\n(1,1)\nDistance 2 via right then down\n\n\n3 x 3 with center wall\n(0,0)\n(2,2)\nDistance 4 around the wall\n\n\nLine 1 x 5 all free\n(0,0)\n(0,4)\nDistance 4\n\n\nBlocked target\n(0,0)\n(1,1)\nNo path\n\n\nMulti source wave\n{all corner starts}\ncenter\nMinimum among corners\n\n\n\n\n\nComplexity\n\nTime: O(n × m)\nSpace: O(n × m) for visited or distance map and queue\n\nBFS on grid is the wavefront that sweeps a map evenly, giving you the fewest steps from start to goal with clean, level by level logic.\n\n\n\n308 Multi-Source BFS\nMulti-Source BFS is the wavefront BFS that starts not from one node but from many sources at once. It’s perfect when several starting points all spread out simultaneously, like multiple fires burning through a forest, or signals radiating from several transmitters.\n\nWhat Problem Are We Solving?\nWe need to find minimum distances from multiple starting nodes, not just one. This is useful when:\n\nThere are several sources of influence (e.g. infections, signals, fires)\nYou want the nearest source for each node\nYou need simultaneous propagation (e.g. multi-start shortest path)\n\nExamples:\n\nSpread of rumors from multiple people\nFlooding time from multiple water sources\nMinimum distance to nearest hospital or supply center\n\n\n\nHow Does It Work (Plain Language)?\nWe treat all sources as level 0 and push them into the queue at once. Then BFS proceeds normally, each node is assigned a distance equal to the shortest path from any source.\n\n\n\n\n\n\n\n\n\nStep\nQueue (Front → Back)\nAction\nDistance Updated\n\n\n\n\n1\n[S1, S2, S3]\nInitialize all sources\ndist[S*] = 0\n\n\n2\n[Neighbors of S1, S2, S3]\nWave expands\ndist = 1\n\n\n3\n[Next Layer]\nContinue\ndist = 2\n\n\n…\n…\n…\n…\n\n\n\nThe first time a node is visited, we know it’s from the nearest source.\n\n\nTiny Code (Easy Versions)\nC (Multi-Source BFS on Grid)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint n, m;\nint grid[MAX][MAX];\nint distv[MAX][MAX];\nbool vis[MAX][MAX];\nint qx[MAX*MAX], qy[MAX*MAX];\nint front = 0, rear = 0;\n\nint dx[4] = {-1, 1, 0, 0};\nint dy[4] = {0, 0, -1, 1};\n\nvoid enqueue(int x, int y) { qx[rear] = x; qy[rear] = y; rear++; }\nvoid dequeue(int *x, int *y) { *x = qx[front]; *y = qy[front]; front++; }\nbool empty() { return front == rear; }\n\nvoid multi_source_bfs() {\n    while (!empty()) {\n        int x, y;\n        dequeue(&x, &y);\n        for (int k = 0; k &lt; 4; k++) {\n            int nx = x + dx[k], ny = y + dy[k];\n            if (nx &gt;= 0 && nx &lt; n && ny &gt;= 0 && ny &lt; m &&\n                grid[nx][ny] == 0 && !vis[nx][ny]) {\n                vis[nx][ny] = true;\n                distv[nx][ny] = distv[x][y] + 1;\n                enqueue(nx, ny);\n            }\n        }\n    }\n}\n\nint main(void) {\n    printf(\"Enter grid size n m: \");\n    scanf(\"%d %d\", &n, &m);\n    printf(\"Enter grid (0 free, 1 blocked, 2 source):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; m; j++) {\n            scanf(\"%d\", &grid[i][j]);\n            if (grid[i][j] == 2) {\n                vis[i][j] = true;\n                distv[i][j] = 0;\n                enqueue(i, j);\n            }\n        }\n\n    multi_source_bfs();\n\n    printf(\"Distance map:\\n\");\n    for (int i = 0; i &lt; n; i++) {\n        for (int j = 0; j &lt; m; j++)\n            printf(\"%2d \", distv[i][j]);\n        printf(\"\\n\");\n    }\n}\nPython (Simple Multi-Source BFS)\nfrom collections import deque\n\ngrid = [\n    [2,0,1,0],\n    [0,1,0,0],\n    [0,0,0,2]\n]\nn, m = len(grid), len(grid[0])\ndist = [[-1]*m for _ in range(n)]\nq = deque()\n\nfor i in range(n):\n    for j in range(m):\n        if grid[i][j] == 2:  # source\n            dist[i][j] = 0\n            q.append((i, j))\n\ndirs = [(-1,0),(1,0),(0,-1),(0,1)]\nwhile q:\n    x, y = q.popleft()\n    for dx, dy in dirs:\n        nx, ny = x+dx, y+dy\n        if 0 &lt;= nx &lt; n and 0 &lt;= ny &lt; m and grid[nx][ny] == 0 and dist[nx][ny] == -1:\n            dist[nx][ny] = dist[x][y] + 1\n            q.append((nx, ny))\n\nprint(\"Distance Map:\")\nfor row in dist:\n    print(row)\n\n\nWhy It Matters\n\nFinds nearest source distance for all nodes in one pass\nIdeal for multi-origin diffusion problems\nFoundation for tasks like multi-fire spread, influence zones, Voronoi partitioning on graphs\n\n\n\nA Gentle Proof (Why It Works)\nSince all sources start at distance 0 and BFS expands in order of increasing distance, the first time a node is visited, it’s reached by the shortest possible path from any source.\nEach cell is enqueued exactly once → O(V + E) time. No need to run BFS separately for each source.\n\n\nTry It Yourself\n\nMark multiple sources (2s) on a grid, verify distances radiate outward.\nChange obstacles (1s) and see how waves avoid them.\nCount how many steps each free cell is from nearest source.\nModify to return which source id reached each cell first.\nCompare total cost vs running single-source BFS repeatedly.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nGrid\nExpected Output (Distances)\nDescription\n\n\n\n\n[[2,0,2]]\n[0,1,0]\nTwo sources on edges\n\n\n[[2,0,0],[0,1,0],[0,0,2]]\nwave radiates from corners\nMixed obstacles\n\n\n[[2,2,2]]\n[0,0,0]\nAll sources\n\n\n[[0,0,0],[0,0,0]] + center source\ncenter = 0, corners = 2\nWave expanding\n\n\nAll blocked\nunchanged\nNo propagation\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V) for queue and distance map\n\nMulti-Source BFS is the chorus of wavefronts, expanding together, each note reaching its closest audience in perfect harmony.\n\n\n\n309 Topological Sort (DFS-based)\nTopological sort is the linear ordering of vertices in a Directed Acyclic Graph (DAG) such that for every directed edge ( u v ), vertex ( u ) appears before ( v ) in the order. The DFS-based approach discovers this order by exploring deeply and recording finishing times.\n\nWhat Problem Are We Solving?\nWe want a way to order tasks that have dependencies. Topological sort answers: In what order can we perform tasks so that prerequisites come first?\nTypical use cases:\n\nBuild systems (compile order)\nCourse prerequisite scheduling\nPipeline stage ordering\nDependency resolution (e.g. package installs)\n\nExample: If task A must finish before B and C, and C before D, then one valid order is A → C → D → B.\n\n\nHow Does It Work (Plain Language)?\nDFS explores from each unvisited node. When a node finishes (no more outgoing edges to explore), push it onto a stack. After all DFS calls, reverse the stack, that’s your topological order.\n\nInitialize all nodes as unvisited\nFor each node v:\n\nRun DFS if not visited\nAfter exploring all neighbors, push v to stack\n\nReverse stack to get topological order\n\n\n\n\nStep\nCurrent Node\nAction\nStack\n\n\n\n\n1\nA\nVisit neighbors\n[]\n\n\n2\nB\nVisit\n[]\n\n\n3\nD\nVisit\n[]\n\n\n4\nD done\nPush D\n[D]\n\n\n5\nB done\nPush B\n[D, B]\n\n\n6\nA done\nPush A\n[D, B, A]\n\n\n\nReverse: [A, B, D]\n\n\nTiny Code (Easy Versions)\nC (DFS-based Topological Sort)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nbool visited[MAX];\nint stack[MAX];\nint top = -1;\nint n;\n\nvoid dfs(int v) {\n    visited[v] = true;\n    for (int u = 0; u &lt; n; u++) {\n        if (graph[v][u] && !visited[u]) {\n            dfs(u);\n        }\n    }\n    stack[++top] = v; // push after exploring neighbors\n}\n\nvoid topological_sort() {\n    for (int i = 0; i &lt; n; i++) {\n        if (!visited[i]) dfs(i);\n    }\n    printf(\"Topological order: \");\n    while (top &gt;= 0) printf(\"%d \", stack[top--]);\n    printf(\"\\n\");\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix (DAG):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n    topological_sort();\n}\nPython (DFS-based)\ngraph = {\n    0: [1, 2],\n    1: [3],\n    2: [3],\n    3: []\n}\n\nvisited = set()\nstack = []\n\ndef dfs(v):\n    visited.add(v)\n    for u in graph[v]:\n        if u not in visited:\n            dfs(u)\n    stack.append(v)\n\nfor v in graph:\n    if v not in visited:\n        dfs(v)\n\nstack.reverse()\nprint(\"Topological order:\", stack)\n\n\nWhy It Matters\n\nEnsures dependency order in DAGs\nFundamental in compilers, schedulers, and build systems\nBasis for advanced algorithms:\n\nKahn’s Algorithm (queue-based)\nDAG shortest paths / DP\nCritical path analysis\n\n\n\n\nA Gentle Proof (Why It Works)\nEach vertex is pushed onto the stack after all its descendants are explored. So if there’s an edge ( u v ), DFS ensures ( v ) finishes first and is pushed earlier, meaning ( u ) will appear later in the stack. Reversing the stack thus guarantees ( u ) precedes ( v ).\nNo cycles allowed, if a back edge is found, topological sort is impossible.\n\n\nTry It Yourself\n\nDraw a DAG and label edges as prerequisites.\nRun DFS and record finish times.\nPush nodes on completion, reverse the order.\nAdd a cycle and see why it breaks.\nCompare with Kahn’s Algorithm results.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nTopological Order (Possible)\n\n\n\n\nA→B→C\nA→B, B→C\nA B C\n\n\nA→C, B→C\nA→C, B→C\nA B C or B A C\n\n\n0→1, 0→2, 1→3, 2→3\nDAG\n0 2 1 3 or 0 1 2 3\n\n\nChain 0→1→2→3\nLinear DAG\n0 1 2 3\n\n\nCycle 0→1→2→0\nNot DAG\nNo valid order\n\n\n\n\n\nComplexity\n\nTime: O(V + E) (each node and edge visited once)\nSpace: O(V) (stack + visited array)\n\nTopological sort (DFS-based) is your dependency detective, exploring deeply, marking completion, and leaving behind a perfect trail of prerequisites.\n\n\n\n310 Topological Sort (Kahn’s Algorithm)\nKahn’s Algorithm is the queue-based way to perform topological sorting. Instead of relying on recursion, it tracks in-degrees (how many edges point into each node) and repeatedly removes nodes with zero in-degree. It’s clean, iterative, and naturally detects cycles.\n\nWhat Problem Are We Solving?\nWe want a linear ordering of tasks in a Directed Acyclic Graph (DAG) such that each task appears after all its prerequisites.\nKahn’s method is especially handy when:\n\nYou want an iterative (non-recursive) algorithm\nYou need to detect cycles automatically\nYou’re building a scheduler or compiler dependency resolver\n\nExample: If A must happen before B and C, and C before D, then valid orders: A C D B or A B C D. Kahn’s algorithm builds this order by peeling off “ready” nodes (those with no remaining prerequisites).\n\n\nHow Does It Work (Plain Language)?\nEach node starts with an in-degree (count of incoming edges). Nodes with in-degree = 0 are ready to process.\n\nCompute in-degree for each node\nEnqueue all nodes with in-degree = 0\nWhile queue not empty:\n\nPop node v and add it to the topological order\nFor each neighbor u of v, decrement in-degree[u]\nIf in-degree[u] becomes 0, enqueue u\n\nIf all nodes processed → valid topological order Otherwise → cycle detected\n\n\n\n\nStep\nQueue\nPopped\nUpdated In-Degree\nOrder\n\n\n\n\n1\n[A]\nA\nB:0, C:1, D:2\n[A]\n\n\n2\n[B]\nB\nC:0, D:2\n[A, B]\n\n\n3\n[C]\nC\nD:1\n[A, B, C]\n\n\n4\n[D]\nD\n–\n[A, B, C, D]\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Kahn’s Algorithm)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nint indeg[MAX];\nint queue[MAX];\nint front = 0, rear = 0;\nint n;\n\nvoid enqueue(int v) { queue[rear++] = v; }\nint dequeue() { return queue[front++]; }\nbool empty() { return front == rear; }\n\nvoid kahn_topo() {\n    for (int v = 0; v &lt; n; v++) {\n        if (indeg[v] == 0) enqueue(v);\n    }\n\n    int count = 0;\n    int order[MAX];\n\n    while (!empty()) {\n        int v = dequeue();\n        order[count++] = v;\n        for (int u = 0; u &lt; n; u++) {\n            if (graph[v][u]) {\n                indeg[u]--;\n                if (indeg[u] == 0) enqueue(u);\n            }\n        }\n    }\n\n    if (count != n) {\n        printf(\"Graph has a cycle, topological sort not possible\\n\");\n    } else {\n        printf(\"Topological order: \");\n        for (int i = 0; i &lt; count; i++) printf(\"%d \", order[i]);\n        printf(\"\\n\");\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n\n    printf(\"Enter adjacency matrix:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++) {\n            scanf(\"%d\", &graph[i][j]);\n            if (graph[i][j]) indeg[j]++;\n        }\n\n    kahn_topo();\n}\nPython (Using Queue)\nfrom collections import deque\n\ngraph = {\n    0: [1, 2],\n    1: [3],\n    2: [3],\n    3: []\n}\n\nindeg = {v: 0 for v in graph}\nfor v in graph:\n    for u in graph[v]:\n        indeg[u] += 1\n\nq = deque([v for v in graph if indeg[v] == 0])\norder = []\n\nwhile q:\n    v = q.popleft()\n    order.append(v)\n    for u in graph[v]:\n        indeg[u] -= 1\n        if indeg[u] == 0:\n            q.append(u)\n\nif len(order) == len(graph):\n    print(\"Topological order:\", order)\nelse:\n    print(\"Cycle detected, no valid order\")\n\n\nWhy It Matters\n\nFully iterative (no recursion stack)\nNaturally detects cycles\nEfficient for build systems, task planners, and dependency graphs\nForms the base for Kahn’s scheduling algorithm in DAG processing\n\n\n\nA Gentle Proof (Why It Works)\nNodes with in-degree 0 have no prerequisites, they can appear first. Once processed, they are removed (decrementing in-degree of successors). This ensures:\n\nEach node is processed only after all its dependencies\nIf a cycle exists, some nodes never reach in-degree 0 → detection built-in\n\nSince each edge is considered once, runtime = O(V + E).\n\n\nTry It Yourself\n\nBuild a small DAG manually and run the algorithm step by step.\nIntroduce a cycle (A→B→A) and observe detection.\nCompare with DFS-based order, both valid.\nAdd priority to queue (min vertex first) to get lexicographically smallest order.\nApply to course prerequisite planner.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges\nOutput\nNotes\n\n\n\n\nA→B→C\nA→B, B→C\nA B C\nLinear chain\n\n\nA→C, B→C\nA→C, B→C\nA B C or B A C\nMultiple sources\n\n\n0→1, 0→2, 1→3, 2→3\nDAG\n0 1 2 3 or 0 2 1 3\nMultiple valid\n\n\n0→1, 1→2, 2→0\nCycle\nNo order\nCycle detected\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V) (queue, indegrees)\n\nKahn’s Algorithm is the dependency peeler, stripping away nodes layer by layer until a clean, linear order emerges.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-32.-strongly-connected-components",
    "href": "books/en-us/list-4.html#section-32.-strongly-connected-components",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 32. Strongly Connected Components",
    "text": "Section 32. Strongly Connected Components\n\n311 Kosaraju’s Algorithm\nKosaraju’s algorithm is one of the clearest ways to find strongly connected components (SCCs) in a directed graph. It uses two depth-first searches, one on the original graph, and one on its reversed version, to peel off SCCs layer by layer.\n\nWhat Problem Are We Solving?\nIn a directed graph, a strongly connected component is a maximal set of vertices such that each vertex is reachable from every other vertex in the same set.\nKosaraju’s algorithm groups the graph into these SCCs.\nThis is useful for:\n\nCondensing a graph into a DAG (meta-graph)\nDependency analysis in compilers\nFinding cycles or redundant modules\nGraph simplification before DP or optimization\n\nExample: Imagine a one-way road network, SCCs are groups of cities where you can travel between any pair.\n\n\nHow Does It Work (Plain Language)?\nKosaraju’s algorithm runs in two DFS passes:\n\nFirst DFS (Original Graph): Explore all vertices. Each time a node finishes (recursion ends), record it on a stack (by finish time).\nReverse the Graph: Reverse all edges (flip direction).\nSecond DFS (Reversed Graph): Pop nodes from the stack (highest finish time first). Each DFS from an unvisited node forms one strongly connected component.\n\n\n\n\nPhase\nGraph\nAction\nResult\n\n\n\n\n1\nOriginal\nDFS all, push finish order\nStack of vertices\n\n\n2\nReversed\nDFS by pop order\nIdentify SCCs\n\n\n3\nOutput\nEach DFS tree\nSCC list\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Adjacency List)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint n;\nint graph[MAX][MAX];\nint rev[MAX][MAX];\nbool visited[MAX];\nint stack[MAX];\nint top = -1;\n\nvoid dfs1(int v) {\n    visited[v] = true;\n    for (int u = 0; u &lt; n; u++) {\n        if (graph[v][u] && !visited[u]) dfs1(u);\n    }\n    stack[++top] = v; // push after finishing\n}\n\nvoid dfs2(int v) {\n    printf(\"%d \", v);\n    visited[v] = true;\n    for (int u = 0; u &lt; n; u++) {\n        if (rev[v][u] && !visited[u]) dfs2(u);\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n\n    printf(\"Enter adjacency matrix (directed):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++) {\n            scanf(\"%d\", &graph[i][j]);\n            rev[j][i] = graph[i][j]; // build reverse\n        }\n\n    // Step 1: first DFS\n    for (int i = 0; i &lt; n; i++)\n        visited[i] = false;\n\n    for (int i = 0; i &lt; n; i++)\n        if (!visited[i]) dfs1(i);\n\n    // Step 2: second DFS on reversed\n    for (int i = 0; i &lt; n; i++)\n        visited[i] = false;\n\n    printf(\"Strongly Connected Components:\\n\");\n    while (top &gt;= 0) {\n        int v = stack[top--];\n        if (!visited[v]) {\n            dfs2(v);\n            printf(\"\\n\");\n        }\n    }\n}\nPython (Using Lists)\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nrev = defaultdict(list)\n\nedges = [(0,1),(1,2),(2,0),(1,3)]\nfor u, v in edges:\n    graph[u].append(v)\n    rev[v].append(u)\n\nvisited = set()\nstack = []\n\ndef dfs1(v):\n    visited.add(v)\n    for u in graph[v]:\n        if u not in visited:\n            dfs1(u)\n    stack.append(v)\n\nfor v in graph:\n    if v not in visited:\n        dfs1(v)\n\nvisited.clear()\n\ndef dfs2(v, comp):\n    visited.add(v)\n    comp.append(v)\n    for u in rev[v]:\n        if u not in visited:\n            dfs2(u, comp)\n\nprint(\"Strongly Connected Components:\")\nwhile stack:\n    v = stack.pop()\n    if v not in visited:\n        comp = []\n        dfs2(v, comp)\n        print(comp)\n\n\nWhy It Matters\n\nSplits a directed graph into mutually reachable groups\nUsed in condensation (convert cyclic graph → DAG)\nHelps detect cyclic dependencies\nFoundation for component-level optimization\n\n\n\nA Gentle Proof (Why It Works)\n\nFinishing times from the first DFS ensure we process “sinks” first (post-order).\nReversing edges turns sinks into sources.\nThe second DFS finds all nodes reachable from that source, i.e. an SCC.\nEvery node is assigned to exactly one SCC.\n\nCorrectness follows from properties of finishing times and reachability symmetry.\n\n\nTry It Yourself\n\nDraw a directed graph with cycles and run the steps manually.\nTrack finish order stack.\nReverse all edges and start popping nodes.\nEach DFS tree = one SCC.\nCompare results with Tarjan’s algorithm.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nSCCs\n\n\n\n\n0→1, 1→2, 2→0\nCycle\n{0,1,2}\n\n\n0→1, 1→2\nChain\n{0}, {1}, {2}\n\n\n0→1, 1→2, 2→0, 2→3\nCycle + tail\n{0,1,2}, {3}\n\n\n0→1, 1→0, 1→2, 2→3, 3→2\nTwo SCCs\n{0,1}, {2,3}\n\n\n\n\n\nComplexity\n\nTime: O(V + E) (2 DFS passes)\nSpace: O(V + E) (graph + stack)\n\nKosaraju’s Algorithm is your mirror explorer, traverse once to record the story, flip the graph, then replay it backward to reveal every strongly bound group.\n\n\n\n312 Tarjan’s Algorithm\nTarjan’s algorithm finds all strongly connected components (SCCs) in a directed graph in one DFS pass, without reversing the graph. It’s an elegant and efficient method that tracks each node’s discovery time and lowest reachable ancestor to identify SCC roots.\n\nWhat Problem Are We Solving?\nWe need to group vertices of a directed graph into SCCs, where each node is reachable from every other node in the same group. Unlike Kosaraju’s two-pass method, Tarjan’s algorithm finds all SCCs in a single DFS, making it faster in practice and easy to integrate into larger graph algorithms.\nCommon applications:\n\nCycle detection in directed graphs\nComponent condensation for DAG processing\nDeadlock analysis\nStrong connectivity queries in compilers, networks, and systems\n\nExample: Imagine a group of cities connected by one-way roads. SCCs are clusters of cities that can all reach each other, forming a tightly connected region.\n\n\nHow Does It Work (Plain Language)?\nEach vertex gets:\n\nA discovery time (disc), when it’s first visited\nA low-link value (low), the smallest discovery time reachable (including back edges)\n\nA stack keeps track of the active recursion path (current DFS stack). When a vertex’s disc equals its low, it’s the root of an SCC, pop nodes from the stack until this vertex reappears.\n\n\n\n\n\n\n\n\n\n\nStep\nAction\nStack\nSCC Found\n\n\n\n\n\n1\nVisit node, assign disc & low\n[A]\n–\n\n\n\n2\nGo deeper (DFS neighbors)\n[A, B, C]\n–\n\n\n\n3\nReach node with no new neighbors\nUpdate low\n[A, B, C]\n–\n\n\n4\nBacktrack, compare lows\n[A, B]\nSCC {C}\n\n\n\n5\nWhen disc == low\nPop SCC\n[A]\nSCC {B, C} (if connected)\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Tarjan’s Algorithm)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint n;\nint graph[MAX][MAX];\nint disc[MAX], low[MAX], stack[MAX];\nbool inStack[MAX];\nint time_counter = 0, top = -1;\n\nvoid dfs_tarjan(int v) {\n    disc[v] = low[v] = ++time_counter;\n    stack[++top] = v;\n    inStack[v] = true;\n\n    for (int u = 0; u &lt; n; u++) {\n        if (!graph[v][u]) continue;\n        if (disc[u] == 0) {\n            dfs_tarjan(u);\n            if (low[u] &lt; low[v]) low[v] = low[u];\n        } else if (inStack[u]) {\n            if (disc[u] &lt; low[v]) low[v] = disc[u];\n        }\n    }\n\n    if (disc[v] == low[v]) {\n        printf(\"SCC: \");\n        int w;\n        do {\n            w = stack[top--];\n            inStack[w] = false;\n            printf(\"%d \", w);\n        } while (w != v);\n        printf(\"\\n\");\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix (directed):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    for (int i = 0; i &lt; n; i++) {\n        disc[i] = 0;\n        inStack[i] = false;\n    }\n\n    printf(\"Strongly Connected Components:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        if (disc[i] == 0)\n            dfs_tarjan(i);\n}\nPython (Using Adjacency List)\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nedges = [(0,1),(1,2),(2,0),(1,3)]\nfor u,v in edges:\n    graph[u].append(v)\n\ntime = 0\ndisc = {}\nlow = {}\nstack = []\nin_stack = set()\nsccs = []\n\ndef dfs(v):\n    global time\n    time += 1\n    disc[v] = low[v] = time\n    stack.append(v)\n    in_stack.add(v)\n\n    for u in graph[v]:\n        if u not in disc:\n            dfs(u)\n            low[v] = min(low[v], low[u])\n        elif u in in_stack:\n            low[v] = min(low[v], disc[u])\n\n    if disc[v] == low[v]:\n        scc = []\n        while True:\n            w = stack.pop()\n            in_stack.remove(w)\n            scc.append(w)\n            if w == v:\n                break\n        sccs.append(scc)\n\nfor v in list(graph.keys()):\n    if v not in disc:\n        dfs(v)\n\nprint(\"Strongly Connected Components:\", sccs)\n\n\nWhy It Matters\n\nRuns in one DFS, efficient and elegant\nDetects SCCs on the fly (no reversing edges)\nUseful for online algorithms (process SCCs as discovered)\nPowers cycle detection, condensation graphs, component-based optimization\n\n\n\nA Gentle Proof (Why It Works)\n\ndisc[v]: time when v is first visited\nlow[v]: smallest discovery time reachable via descendants or back edges\nWhen disc[v] == low[v], v is the root of its SCC (no back edges go above it)\nPopping from the stack gives all nodes reachable within the component\n\nEach edge examined once → linear time.\n\n\nTry It Yourself\n\nRun Tarjan on a graph with multiple cycles.\nObserve disc and low values.\nPrint stack content at each step to see grouping.\nCompare SCC output with Kosaraju’s result.\nTry adding a cycle and check grouping changes.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nSCCs\n\n\n\n\n0→1, 1→2, 2→0\nCycle\n{0,1,2}\n\n\n0→1, 1→2\nChain\n{2}, {1}, {0}\n\n\n0→1, 1→2, 2→0, 2→3\nCycle + tail\n{0,1,2}, {3}\n\n\n1→2, 2→3, 3→1, 3→4\nTwo groups\n{1,2,3}, {4}\n\n\n\n\n\nComplexity\n\nTime: O(V + E) (one DFS pass)\nSpace: O(V) (stack, arrays)\n\nTarjan’s Algorithm is your clockwork explorer, tagging each vertex by time, tracing the deepest paths, and snapping off every strongly connected cluster in one graceful pass.\n\n\n\n313 Gabow’s Algorithm\nGabow’s algorithm is another elegant one-pass method for finding strongly connected components (SCCs). It’s less well-known than Tarjan’s, but equally efficient, using two stacks to track active vertices and roots. It’s a perfect example of “stack discipline” in graph exploration.\n\nWhat Problem Are We Solving?\nWe want to find all strongly connected components in a directed graph, subsets where every node can reach every other.\nGabow’s algorithm, like Tarjan’s, works in a single DFS traversal, but instead of computing low-link values, it uses two stacks to manage component discovery and boundaries.\nThis approach is especially helpful in streaming, online, or iterative DFS environments, where explicit low-link computations can get messy.\nApplications:\n\nCycle decomposition\nProgram dependency analysis\nComponent condensation (DAG creation)\nStrong connectivity testing\n\nExample: Think of traversing a web of roads. One stack tracks where you’ve been, another stack marks “checkpoints” where loops close, each loop is a component.\n\n\nHow Does It Work (Plain Language)?\nGabow’s algorithm keeps track of discovery order and component boundaries using two stacks:\n\nS (main stack): stores all currently active nodes\nP (boundary stack): stores potential roots of SCCs\n\nSteps:\n\nPerform DFS. Assign each node an increasing index (preorder)\nPush node onto both stacks (S and P)\nFor each edge ( v u ):\n\nIf u unvisited → recurse\nIf u on stack S → adjust boundary stack P\n\nAfter exploring neighbors:\n\nIf the top of P is current node v:\n\nPop P\nPop from S until v\nThose popped form an SCC\n\n\n\n\n\n\nStep\nStack S\nStack P\nAction\n\n\n\n\n1\n[A]\n[A]\nVisit A\n\n\n2\n[A, B]\n[A, B]\nVisit B\n\n\n3\n[A, B, C]\n[A, B, C]\nVisit C\n\n\n4\n[A, B, C]\n[A, B]\nBack edge C→B\n\n\n5\n[A, B]\n[A]\nPop SCC {C}\n\n\n6\n[A]\n[]\nPop SCC {B}\n\n\n7\n[]\n[]\nPop SCC {A}\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Gabow’s Algorithm)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nint n;\nint index_counter = 0;\nint preorder[MAX];\nbool onStack[MAX];\nint stackS[MAX], topS = -1;\nint stackP[MAX], topP = -1;\n\nvoid dfs_gabow(int v) {\n    preorder[v] = ++index_counter;\n    stackS[++topS] = v;\n    stackP[++topP] = v;\n    onStack[v] = true;\n\n    for (int u = 0; u &lt; n; u++) {\n        if (!graph[v][u]) continue;\n        if (preorder[u] == 0) {\n            dfs_gabow(u);\n        } else if (onStack[u]) {\n            while (preorder[stackP[topP]] &gt; preorder[u]) topP--;\n        }\n    }\n\n    if (stackP[topP] == v) {\n        topP--;\n        printf(\"SCC: \");\n        int w;\n        do {\n            w = stackS[topS--];\n            onStack[w] = false;\n            printf(\"%d \", w);\n        } while (w != v);\n        printf(\"\\n\");\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix (directed):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    for (int i = 0; i &lt; n; i++) {\n        preorder[i] = 0;\n        onStack[i] = false;\n    }\n\n    printf(\"Strongly Connected Components:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        if (preorder[i] == 0)\n            dfs_gabow(i);\n}\nPython (Readable Implementation)\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nedges = [(0,1),(1,2),(2,0),(1,3)]\nfor u,v in edges:\n    graph[u].append(v)\n\nindex_counter = 0\npreorder = {}\non_stack = set()\nstackS, stackP = [], []\nsccs = []\n\ndef dfs(v):\n    global index_counter\n    index_counter += 1\n    preorder[v] = index_counter\n    stackS.append(v)\n    stackP.append(v)\n    on_stack.add(v)\n\n    for u in graph[v]:\n        if u not in preorder:\n            dfs(u)\n        elif u in on_stack:\n            while preorder[stackP[-1]] &gt; preorder[u]:\n                stackP.pop()\n\n    if stackP and stackP[-1] == v:\n        stackP.pop()\n        comp = []\n        while True:\n            w = stackS.pop()\n            on_stack.remove(w)\n            comp.append(w)\n            if w == v:\n                break\n        sccs.append(comp)\n\nfor v in graph:\n    if v not in preorder:\n        dfs(v)\n\nprint(\"Strongly Connected Components:\", sccs)\n\n\nWhy It Matters\n\nSingle DFS pass, no reverse graph\nPurely stack-based, avoids recursion depth issues in some variants\nEfficient and practical for large graphs\nSimpler low-link logic than Tarjan’s in some applications\n\n\n\nA Gentle Proof (Why It Works)\nEach node gets a preorder index. The second stack P tracks the earliest root reachable. Whenever the top of P equals the current node, all nodes above it in S form one SCC.\nInvariant:\n\nS contains active nodes\nP contains possible roots (ordered by discovery)\nWhen back edges discovered, P trimmed to smallest reachable ancestor\n\nThis ensures each SCC is identified exactly once when its root finishes.\n\n\nTry It Yourself\n\nDraw a graph and trace preorder indices.\nObserve how stack P shrinks on back edges.\nRecord SCCs as they’re popped.\nCompare output to Tarjan’s algorithm.\nTry it on DAGs, cycles, and mixed graphs.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nSCCs\n\n\n\n\n0→1, 1→2, 2→0\n{0,1,2}\nOne big SCC\n\n\n0→1, 1→2\n{2}, {1}, {0}\nChain\n\n\n0→1, 1→0, 2→3\n{0,1}, {2}, {3}\nMixed\n\n\n0→1, 1→2, 2→3, 3→1\n{1,2,3}, {0}\nNested cycle\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V) (two stacks + metadata)\n\nGabow’s Algorithm is your two-stack sculptor, carving SCCs from the graph in one graceful sweep, balancing exploration and boundaries like a craftsman marking edges before the final cut.\n\n\n\n314 SCC DAG Construction\nOnce we’ve found strongly connected components (SCCs), we can build the condensation graph, a Directed Acyclic Graph (DAG) where each node represents an SCC, and edges connect them if any vertex in one SCC points to a vertex in another.\nThis structure is crucial because it transforms a messy cyclic graph into a clean acyclic skeleton, perfect for topological sorting, dynamic programming, and dependency analysis.\n\nWhat Problem Are We Solving?\nWe want to take a directed graph and reduce it into a simpler form by collapsing each SCC into a single node. The resulting graph (called the condensation graph) is always a DAG.\nWhy do this?\n\nTo simplify reasoning about complex systems\nTo run DAG algorithms on cyclic graphs (by condensing cycles)\nTo perform component-level optimization\nTo study dependencies between strongly connected subsystems\n\nExample: Think of a city map where SCCs are tightly connected neighborhoods. The DAG shows how traffic flows between neighborhoods, not within them.\n\n\nHow Does It Work (Plain Language)?\n\nFind SCCs (using Kosaraju, Tarjan, or Gabow).\nAssign each node an SCC ID (e.g., comp[v] = c_id).\nCreate a new graph with one node per SCC.\nFor each edge ( (u, v) ) in the original graph:\n\nIf comp[u] != comp[v], add an edge from comp[u] to comp[v]\n\nRemove duplicates (or store edges in sets).\n\nNow the new graph has no cycles, since cycles are already condensed inside SCCs.\n\n\n\nOriginal Graph\nSCCs\nDAG\n\n\n\n\n0→1, 1→2, 2→0, 2→3\n{0,1,2}, {3}\nC0 → C1\n\n\n0→1, 1→2, 2→3, 3→1\n{1,2,3}, {0}\nC0 → C1\n\n\n0→1, 1→2\n{0}, {1}, {2}\n0 → 1 → 2\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Using Tarjan’s Output)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint n;\nint graph[MAX][MAX];\nint comp[MAX];   // component ID\nint comp_count = 0;\n\n// Suppose comp[] already filled by Tarjan or Kosaraju\n\nvoid build_condensed_graph() {\n    int new_graph[MAX][MAX] = {0};\n\n    for (int u = 0; u &lt; n; u++) {\n        for (int v = 0; v &lt; n; v++) {\n            if (graph[u][v] && comp[u] != comp[v]) {\n                new_graph[comp[u]][comp[v]] = 1;\n            }\n        }\n    }\n\n    printf(\"Condensation Graph (Adjacency Matrix):\\n\");\n    for (int i = 0; i &lt; comp_count; i++) {\n        for (int j = 0; j &lt; comp_count; j++) {\n            printf(\"%d \", new_graph[i][j]);\n        }\n        printf(\"\\n\");\n    }\n}\n(Assumes comp[v] and comp_count were computed before.)\nPython (with Tarjan’s or Kosaraju’s Components)\nfrom collections import defaultdict\n\n# Suppose we already have SCCs\nsccs = [[0,1,2], [3], [4,5]]\ngraph = {\n    0: [1],\n    1: [2,3],\n    2: [0],\n    3: [4],\n    4: [5],\n    5: []\n}\n\n# Step 1: assign component ID\ncomp_id = {}\nfor i, comp in enumerate(sccs):\n    for v in comp:\n        comp_id[v] = i\n\n# Step 2: build DAG\ndag = defaultdict(set)\nfor u in graph:\n    for v in graph[u]:\n        if comp_id[u] != comp_id[v]:\n            dag[comp_id[u]].add(comp_id[v])\n\nprint(\"Condensation Graph (as DAG):\")\nfor c in dag:\n    print(c, \"-&gt;\", sorted(dag[c]))\n\n\nWhy It Matters\n\nTurns cyclic graph → acyclic graph\nEnables topological sorting, dynamic programming, path counting\nClarifies inter-component dependencies\nUsed in compiler analysis, SCC-based optimizations, graph condensation\n\n\n\nA Gentle Proof (Why It Works)\nInside each SCC, every vertex can reach every other. When edges cross SCC boundaries, they go in one direction only (since returning would merge the components). Thus, the condensation graph cannot contain cycles, proving it’s a DAG.\nEach edge in the DAG represents at least one edge between components in the original graph.\n\n\nTry It Yourself\n\nRun Tarjan’s algorithm to get comp[v] for each vertex.\nBuild DAG edges using component IDs.\nVisualize original vs condensed graphs.\nTopologically sort the DAG.\nUse DP on DAG to compute longest path or reachability.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nOriginal Graph\nSCCs\nCondensed Edges\n\n\n\n\n0→1, 1→2, 2→0, 2→3\n{0,1,2}, {3}\n0→1\n\n\n0→1, 1→0, 1→2, 2→3, 3→2\n{0,1}, {2,3}\n0→1\n\n\n0→1, 1→2, 2→3\n{0}, {1}, {2}, {3}\n0→1→2→3\n\n\n0→1, 1→2, 2→0\n{0,1,2}\nNone (single node DAG)\n\n\n\n\n\nComplexity\n\nTime: O(V + E) (using precomputed SCCs)\nSpace: O(V + E) (new DAG structure)\n\nSCC DAG Construction is your mapmaker’s step, compressing tangled roads into clean highways, where each city (SCC) is a hub, and the new map is finally acyclic, ready for analysis.\n\n\n\n315 SCC Online Merge\nSCC Online Merge is a dynamic approach to maintain strongly connected components when a graph is growing over time (new edges are added). Instead of recomputing SCCs from scratch after each update, we incrementally merge components as they become connected.\nIt’s the foundation of dynamic graph algorithms where edges arrive one by one, useful in online systems, incremental compilers, and evolving dependency graphs.\n\nWhat Problem Are We Solving?\nWe want to maintain SCC structure as we add edges to a directed graph.\nIn static algorithms like Tarjan or Kosaraju, SCCs are computed once. But if new edges appear over time, recomputing everything is too slow.\nSCC Online Merge gives us:\n\nEfficient incremental updates (no full recompute)\nFast component merging\nUp-to-date condensation DAG\n\nTypical use cases:\n\nIncremental program analysis (new dependencies)\nDynamic network reachability\nStreaming graph processing\nOnline algorithm design\n\n\n\nHow Does It Work (Plain Language)?\nWe start with each node as its own SCC. When a new edge ( u v ) is added:\n\nCheck if ( u ) and ( v ) are already in the same SCC, if yes, nothing changes.\nIf not, check whether v’s SCC can reach u’s SCC (cycle detection).\nIf reachable, merge both SCCs into one.\nOtherwise, add a DAG edge from SCC(u) → SCC(v).\n\nWe maintain:\n\nUnion-Find / DSU structure for SCC groups\nReachability or DAG edges between SCCs\nOptional topological order for fast cycle checks\n\n\n\n\nEdge Added\nAction\nNew SCCs\nNotes\n\n\n\n\n0→1\nCreate edge\n{0}, {1}\nSeparate\n\n\n1→2\nCreate edge\n{0}, {1}, {2}\nSeparate\n\n\n2→0\nCycle formed\nMerge {0,1,2}\nNew SCC\n\n\n3→1\nAdd edge\n{3}, {0,1,2}\nNo merge\n\n\n\n\n\nTiny Code (Conceptual Demo)\nPython (Simplified DSU + DAG Check)\nclass DSU:\n    def __init__(self, n):\n        self.parent = list(range(n))\n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n    def union(self, a, b):\n        ra, rb = self.find(a), self.find(b)\n        if ra != rb:\n            self.parent[rb] = ra\n\nclass OnlineSCC:\n    def __init__(self, n):\n        self.n = n\n        self.dsu = DSU(n)\n        self.graph = [set() for _ in range(n)]\n\n    def add_edge(self, u, v):\n        su, sv = self.dsu.find(u), self.dsu.find(v)\n        if su == sv:\n            return  # already connected\n        # check if v's component reaches u's component\n        if self._reachable(sv, su):\n            # merge components\n            self.dsu.union(su, sv)\n            merged = self.dsu.find(su)\n            self.graph[merged] = self.graph[su] | self.graph[sv]\n        else:\n            self.graph[su].add(sv)\n\n    def _reachable(self, start, target, seen=None):\n        if seen is None: seen = set()\n        if start == target: return True\n        seen.add(start)\n        for nxt in self.graph[start]:\n            if nxt not in seen and self._reachable(nxt, target, seen):\n                return True\n        return False\n\n    def components(self):\n        groups = {}\n        for v in range(self.n):\n            root = self.dsu.find(v)\n            groups.setdefault(root, []).append(v)\n        return list(groups.values())\n\n# Example\nscc = OnlineSCC(4)\nscc.add_edge(0, 1)\nscc.add_edge(1, 2)\nprint(\"SCCs:\", scc.components())  # [[0], [1], [2], [3]]\nscc.add_edge(2, 0)\nprint(\"SCCs:\", scc.components())  # [[0,1,2], [3]]\n\n\nWhy It Matters\n\nDynamic SCC maintenance without recomputation\nHandles edge insertions in O(V + E) amortized\nEnables real-time graph updates\nBasis for more advanced algorithms (fully dynamic SCC with deletions)\n\n\n\nA Gentle Proof (Why It Works)\nEvery time we add an edge, one of two things happens:\n\nIt connects existing SCCs without creating a cycle → add DAG edge\nIt creates a cycle → merge involved SCCs\n\nSince merging SCCs preserves the DAG structure (merging collapses cycles), the algorithm keeps the condensation graph valid at all times.\nBy maintaining reachability between SCCs, we can detect cycle formation efficiently.\n\n\nTry It Yourself\n\nStart with 5 nodes, no edges.\nAdd edges step-by-step, printing SCCs.\nAdd a back-edge forming a cycle → watch SCCs merge.\nVisualize condensation DAG after each update.\nCompare with recomputing using Tarjan’s, they match!\n\n\n\nTest Cases\n\n\n\nStep\nEdge\nSCCs\n\n\n\n\n1\n0→1\n{0}, {1}, {2}, {3}\n\n\n2\n1→2\n{0}, {1}, {2}, {3}\n\n\n3\n2→0\n{0,1,2}, {3}\n\n\n4\n3→1\n{0,1,2}, {3}\n\n\n5\n2→3\n{0,1,2,3}\n\n\n\n\n\nComplexity\n\nTime: O(V + E) amortized (per edge addition)\nSpace: O(V + E) (graph + DSU)\n\nSCC Online Merge is your dynamic sculptor, merging components as new edges appear, maintaining structure without ever starting over.\n\n\n\n316 Component Label Propagation\nComponent Label Propagation is a simple, iterative algorithm to find connected components (or strongly connected components in symmetric graphs) by repeatedly propagating minimum labels across edges until all nodes in a component share the same label.\nIt’s conceptually clean, highly parallelizable, and forms the backbone of graph processing frameworks like Google’s Pregel, Apache Giraph, and GraphX, perfect for large-scale or distributed systems.\n\nWhat Problem Are We Solving?\nWe want to identify components, groups of vertices that are mutually reachable. Instead of deep recursion or complex stacks, we iteratively propagate labels across the graph until convergence.\nThis approach is ideal when:\n\nThe graph is massive (too large for recursion)\nYou’re using parallel / distributed computation\nYou want a message-passing style algorithm\n\nFor undirected graphs, it finds connected components. For directed graphs, it can approximate SCCs (often used as a preprocessing step).\nExample: Think of spreading an ID through a crowd, each node tells its neighbors its smallest known label, and everyone updates to match their smallest neighbor’s label. Eventually, all in a group share the same number.\n\n\nHow Does It Work (Plain Language)?\n\nInitialize: Each vertex’s label = its own ID.\nIterate: For each vertex:\n\nLook at all neighbors’ labels.\nUpdate to the smallest label seen.\n\nRepeat until no label changes (convergence).\n\nAll nodes that end up sharing a label belong to the same component.\n\n\n\nStep\nNode\nCurrent Label\nNeighbor Labels\nNew Label\n\n\n\n\n1\nA\nA\n{B, C}\nmin(A, B, C) = A\n\n\n2\nB\nB\n{A}\nmin(B, A) = A\n\n\n3\nC\nC\n{A}\nmin(C, A) = A\n\n\n\nEventually, A, B, C → all labeled A.\n\n\nTiny Code (Easy Versions)\nC (Iterative Label Propagation)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint n, graph[MAX][MAX];\nint label[MAX];\n\nvoid label_propagation() {\n    bool changed = true;\n    while (changed) {\n        changed = false;\n        for (int v = 0; v &lt; n; v++) {\n            int min_label = label[v];\n            for (int u = 0; u &lt; n; u++) {\n                if (graph[v][u]) {\n                    if (label[u] &lt; min_label)\n                        min_label = label[u];\n                }\n            }\n            if (min_label &lt; label[v]) {\n                label[v] = min_label;\n                changed = true;\n            }\n        }\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix (undirected):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    for (int i = 0; i &lt; n; i++)\n        label[i] = i;\n\n    label_propagation();\n\n    printf(\"Component Labels:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        printf(\"Vertex %d → Label %d\\n\", i, label[i]);\n}\nPython (Simple Version)\ngraph = {\n    0: [1],\n    1: [0, 2],\n    2: [1],\n    3: [4],\n    4: [3]\n}\n\nlabels = {v: v for v in graph}\n\nchanged = True\nwhile changed:\n    changed = False\n    for v in graph:\n        min_label = min([labels[v]] + [labels[u] for u in graph[v]])\n        if min_label &lt; labels[v]:\n            labels[v] = min_label\n            changed = True\n\nprint(\"Component labels:\", labels)\n\n\nWhy It Matters\n\nSimple and parallelizable, ideal for big data systems\nNo recursion or stack, suitable for GPUs, clusters\nLocal computation, fits the “think like a vertex” model\nWorks on massive graphs where DFS is impractical\n\n\n\nA Gentle Proof (Why It Works)\nEach iteration allows label information to flow along edges. Since the smallest label always propagates, and each propagation only decreases label values, the process must converge (no infinite updates). At convergence, all vertices in a connected component share the same minimal label.\nThe number of iterations ≤ graph diameter.\n\n\nTry It Yourself\n\nRun it on small undirected graphs, label flow is easy to track.\nTry a graph with two disconnected parts, they’ll stabilize separately.\nAdd edges between components and rerun, watch labels merge.\nUse directed edges and see how approximation differs from SCCs.\nImplement in parallel (multi-threaded loop).\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nFinal Labels\n\n\n\n\n0–1–2\n(0,1), (1,2)\n{0,0,0}\n\n\n0–1, 2–3\n(0,1), (2,3)\n{0,0,2,2}\n\n\n0–1–2–3\nChain\n{0,0,0,0}\n\n\n0–1, 1–2, 2–0\nCycle\n{0,0,0}\n\n\n\n\n\nComplexity\n\nTime: O(V × E) (in worst case, or O(D × E) for D = diameter)\nSpace: O(V + E)\n\nComponent Label Propagation is your whispering algorithm, every node shares its name with neighbors, again and again, until all who can reach each other call themselves by the same name.\n\n\n\n317 Path-Based SCC\nThe Path-Based SCC algorithm is another elegant one-pass method for finding strongly connected components in a directed graph. It’s similar in spirit to Tarjan’s algorithm, but instead of computing explicit low-link values, it maintains path stacks to detect when a full component has been traversed.\nDeveloped by Donald B. Johnson, it uses two stacks to keep track of DFS path order and potential roots. When a vertex cannot reach any earlier vertex, it becomes the root of an SCC, and the algorithm pops all nodes in that SCC from the path.\n\nWhat Problem Are We Solving?\nWe want to find SCCs in a directed graph, subsets of vertices where each node can reach every other node. Path-Based SCC offers a conceptually simple and efficient way to do this without low-link math.\nWhy it’s useful:\n\nSingle DFS traversal\nClean stack-based logic\nGreat for teaching, reasoning, and implementation clarity\nEasy to extend for incremental or streaming SCC detection\n\nApplications:\n\nCompiler analysis (strongly connected variables)\nCircuit analysis\nDeadlock detection\nDataflow optimization\n\n\n\nHow Does It Work (Plain Language)?\nWe maintain:\n\nindex[v]: discovery order\nS stack: DFS path (nodes in current exploration)\nP stack: candidates for SCC roots\n\nAlgorithm steps:\n\nAssign index[v] when visiting a node.\nPush v onto both stacks (S and P).\nFor each neighbor u:\n\nIf u is unvisited → recurse\nIf u is on S → adjust P by popping until its top has an index ≤ index[u]\n\nIf v is at the top of P after processing all neighbors:\n\nPop P\nPop from S until v is removed\nThe popped vertices form one SCC\n\n\n\n\n\nStep\nCurrent Node\nStack S\nStack P\nAction\n\n\n\n\n1\nA\n[A]\n[A]\nVisit A\n\n\n2\nB\n[A,B]\n[A,B]\nVisit B\n\n\n3\nC\n[A,B,C]\n[A,B,C]\nVisit C\n\n\n4\nBack edge C→B\n[A,B,C]\n[A,B]\nAdjust\n\n\n5\nC done\n[A,B,C]\n[A,B]\nContinue\n\n\n6\nP top == B\n[A]\n[A]\nFound SCC {B,C}\n\n\n\n\n\nTiny Code (Easy Versions)\nC (Path-Based SCC)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\nint graph[MAX][MAX];\nint n;\nint index_counter = 0;\nint indexv[MAX];\nbool onStack[MAX];\nint stackS[MAX], topS = -1;\nint stackP[MAX], topP = -1;\n\nvoid dfs_scc(int v) {\n    indexv[v] = ++index_counter;\n    stackS[++topS] = v;\n    stackP[++topP] = v;\n    onStack[v] = true;\n\n    for (int u = 0; u &lt; n; u++) {\n        if (!graph[v][u]) continue;\n        if (indexv[u] == 0) {\n            dfs_scc(u);\n        } else if (onStack[u]) {\n            while (indexv[stackP[topP]] &gt; indexv[u])\n                topP--;\n        }\n    }\n\n    if (stackP[topP] == v) {\n        topP--;\n        printf(\"SCC: \");\n        int w;\n        do {\n            w = stackS[topS--];\n            onStack[w] = false;\n            printf(\"%d \", w);\n        } while (w != v);\n        printf(\"\\n\");\n    }\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix (directed):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n\n    for (int i = 0; i &lt; n; i++) indexv[i] = 0;\n\n    printf(\"Strongly Connected Components:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        if (indexv[i] == 0)\n            dfs_scc(i);\n}\nPython (Readable Implementation)\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nedges = [(0,1),(1,2),(2,0),(1,3)]\nfor u,v in edges:\n    graph[u].append(v)\n\nindex_counter = 0\nindex = {}\nstackS, stackP = [], []\non_stack = set()\nsccs = []\n\ndef dfs(v):\n    global index_counter\n    index_counter += 1\n    index[v] = index_counter\n    stackS.append(v)\n    stackP.append(v)\n    on_stack.add(v)\n\n    for u in graph[v]:\n        if u not in index:\n            dfs(u)\n        elif u in on_stack:\n            while index[stackP[-1]] &gt; index[u]:\n                stackP.pop()\n\n    if stackP and stackP[-1] == v:\n        stackP.pop()\n        comp = []\n        while True:\n            w = stackS.pop()\n            on_stack.remove(w)\n            comp.append(w)\n            if w == v:\n                break\n        sccs.append(comp)\n\nfor v in list(graph.keys()):\n    if v not in index:\n        dfs(v)\n\nprint(\"Strongly Connected Components:\", sccs)\n\n\nWhy It Matters\n\nSingle DFS\nNo low-link math, purely path-based reasoning\nCompact and intuitive for stack lovers\nWell-suited for theoretical clarity and educational use\nMatches Tarjan’s O(V + E) performance\n\n\n\nA Gentle Proof (Why It Works)\n\nEach vertex gets a discovery index;\nStack S stores active path nodes;\nStack P tracks potential SCC roots (lowest index still reachable). When a vertex finishes and equals top of P, all nodes above it in S form an SCC, they’re mutually reachable, and none can reach earlier nodes.\n\nThe invariant ensures:\n\nNodes stay on stack until their SCC is found\nSCCs are discovered in reverse topological order\n\n\n\nTry It Yourself\n\nRun it on a small graph, print index[v], stack states after each call.\nAdd a cycle, trace how P adjusts.\nCompare output with Tarjan’s algorithm, they match!\nVisualize path-based pops as SCC boundaries.\nTry graphs with multiple disjoint SCCs.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nSCCs\n\n\n\n\n0→1, 1→2, 2→0\nCycle\n{0,1,2}\n\n\n0→1, 1→2\nChain\n{0}, {1}, {2}\n\n\n0→1, 1→0, 2→3\nMixed\n{0,1}, {2}, {3}\n\n\n0→1, 1→2, 2→3, 3→1\nNested\n{1,2,3}, {0}\n\n\n\n\n\nComplexity\n\nTime: O(V + E)\nSpace: O(V)\n\nPath-Based SCC is your stack ballet, each vertex steps forward, marks its place, and retreats gracefully, leaving behind a tightly choreographed component.\n\n\n\n318 Kosaraju Parallel Version\nThe Parallel Kosaraju Algorithm adapts Kosaraju’s classic two-pass SCC method to run on multiple processors or threads, making it suitable for large-scale graphs that can’t be processed efficiently by a single thread. It divides the heavy lifting, DFS traversals and graph reversals, across many workers.\nIt’s the natural evolution of Kosaraju’s idea in the age of parallel computing: split the graph, explore concurrently, merge SCCs.\n\nWhat Problem Are We Solving?\nWe want to compute SCCs in a massive directed graph efficiently, by taking advantage of parallel hardware, multicore CPUs, GPUs, or distributed systems.\nThe classic Kosaraju algorithm:\n\nDFS on the original graph to record finishing times\nReverse all edges to create the transpose graph\nDFS on the reversed graph in decreasing order of finishing time\n\nThe parallel version accelerates each phase by partitioning vertices or edges among processors.\nApplications include:\n\nLarge dependency graphs (package managers, compilers)\nWeb graphs (page connectivity)\nSocial networks (mutual reachability)\nGPU-accelerated analytics and graph mining\n\n\n\nHow Does It Work (Plain Language)?\nWe parallelize Kosaraju’s two key passes:\n\nParallel Forward DFS (Finishing Order):\n\nPartition vertices across threads.\nEach thread runs DFS independently on its subgraph.\nMaintain a shared stack of finish times (atomic appends).\n\nGraph Reversal:\n\nReverse each edge \\((u, v)\\) into \\((v, u)\\) in parallel.\nEach thread processes a slice of the edge list.\n\nParallel Reverse DFS (SCC Labeling):\n\nThreads pop vertices from the global stack.\nEach unvisited node starts a new component.\nDFS labeling runs concurrently with atomic visited flags.\n\nMerge Components:\n\nCombine local SCC results using union–find if overlapping sets appear.\n\n\n\n\n\nPhase\nDescription\nParallelized\n\n\n\n\n1\nDFS on original graph\n✅ Yes\n\n\n2\nReverse edges\n✅ Yes\n\n\n3\nDFS on reversed graph\n✅ Yes\n\n\n4\nMerge labels\n✅ Yes\n\n\n\n\n\nTiny Code (Pseudocode / Python Threaded Sketch)\n\nThis example is conceptual, real parallel implementations use task queues, GPU kernels, or work-stealing schedulers.\n\nimport threading\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nedges = [(0,1), (1,2), (2,0), (2,3), (3,4)]\nfor u, v in edges:\n    graph[u].append(v)\n\nn = 5\nvisited = [False] * n\nfinish_order = []\nlock = threading.Lock()\n\ndef dfs_forward(v):\n    visited[v] = True\n    for u in graph[v]:\n        if not visited[u]:\n            dfs_forward(u)\n    with lock:\n        finish_order.append(v)\n\ndef parallel_forward():\n    threads = []\n    for v in range(n):\n        if not visited[v]:\n            t = threading.Thread(target=dfs_forward, args=(v,))\n            t.start()\n            threads.append(t)\n    for t in threads:\n        t.join()\n\n# Reverse graph in parallel\nrev = defaultdict(list)\nfor u in graph:\n    for v in graph[u]:\n        rev[v].append(u)\n\nvisited = [False] * n\ncomponents = []\n\ndef dfs_reverse(v, comp):\n    visited[v] = True\n    comp.append(v)\n    for u in rev[v]:\n        if not visited[u]:\n            dfs_reverse(u, comp)\n\ndef parallel_reverse():\n    while finish_order:\n        v = finish_order.pop()\n        if not visited[v]:\n            comp = []\n            dfs_reverse(v, comp)\n            components.append(comp)\n\nparallel_forward()\nparallel_reverse()\nprint(\"SCCs:\", components)\n\n\nWhy It Matters\n\nEnables SCC computation at scale\nExploits multicore / GPU parallelism\nCritical for dataflow analysis, reachability, and graph condensation\nPowers large-scale graph analytics in scientific computing\n\n\n\nA Gentle Proof (Why It Works)\nKosaraju’s correctness depends on finishing times and reversed reachability:\n\nIn the forward pass, each vertex \\(v\\) gets a finishing time \\(t(v)\\).\nIn the reversed graph, DFS in descending \\(t(v)\\) ensures that every SCC is discovered as a contiguous DFS tree.\n\nParallel execution maintains these invariants because:\n\nAll threads respect atomic finishing-time insertion\nThe global finishing order preserves a valid topological order\nThe reversed DFS still discovers mutually reachable vertices together\n\nThus, correctness holds under synchronized access, even with concurrent DFS traversals.\n\n\nTry It Yourself\n\nSplit vertices into \\(p\\) partitions.\nRun forward DFS in parallel; record global finish stack.\nReverse edges concurrently.\nRun backward DFS by popping from the stack.\nCompare results with single-threaded Kosaraju, they match.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nSCCs\n\n\n\n\nCycle\n0→1, 1→2, 2→0\n{0, 1, 2}\n\n\nChain\n0→1, 1→2\n{0}, {1}, {2}\n\n\nTwo cycles\n0→1, 1→0, 2→3, 3→2\n{0, 1}, {2, 3}\n\n\nMixed\n0→1, 1→2, 2→3, 3→0, 4→5\n{0, 1, 2, 3}, {4, 5}\n\n\n\n\n\nComplexity\nLet \\(V\\) be vertices, \\(E\\) edges, \\(p\\) processors.\n\nWork: \\(O(V + E)\\) (same as sequential)\nParallel Time: \\(T_p = O!\\left(\\frac{V + E}{p} + \\text{sync\\_cost}\\right)\\)\nSpace: \\(O(V + E)\\)\n\nKosaraju Parallel is your multi-voice chorus, each DFS sings in harmony, covering its section of the graph, and when the echoes settle, the full harmony of strongly connected components is revealed.\n\n\n\n319 Dynamic SCC Maintenance\nDynamic SCC Maintenance deals with maintaining strongly connected components as a directed graph changes over time, edges or vertices may be added or removed. The goal is to update SCCs incrementally rather than recomputing them from scratch after each change.\nThis approach is crucial in streaming, interactive, or evolving systems, where graphs represent real-world structures that shift continuously.\n\nWhat Problem Are We Solving?\nWe want to keep track of SCCs under dynamic updates:\n\nInsertions: New edges can connect SCCs and form larger ones.\nDeletions: Removing edges can split SCCs into smaller ones.\n\nStatic algorithms like Tarjan or Kosaraju must restart completely. Dynamic maintenance updates only the affected components, improving efficiency for large, frequently changing graphs.\nUse cases include:\n\nIncremental compilation\nDynamic program analysis\nReal-time dependency resolution\nContinuous graph query systems\n\n\n\nHow Does It Work (Plain Language)?\nThe dynamic SCC algorithm maintains:\n\nA condensation DAG representing SCCs.\nA reachability structure to detect cycles upon insertion.\nLocal re-evaluation for affected nodes.\n\nWhen an edge \\((u, v)\\) is added:\n\nIdentify components \\(C_u\\) and \\(C_v\\).\nIf \\(C_u = C_v\\), no change.\nIf \\(C_v\\) can reach \\(C_u\\), a new cycle forms → merge SCCs.\nOtherwise, add edge \\(C_u \\to C_v\\) in the condensation DAG.\n\nWhen an edge \\((u, v)\\) is removed:\n\nRemove it from the graph.\nCheck if \\(C_u\\) and \\(C_v\\) remain mutually reachable.\nIf not, recompute SCCs locally on the affected subgraph.\n\n\n\n\nUpdate\nAction\nResult\n\n\n\n\nAdd edge forming cycle\nMerge SCCs\nLarger component\n\n\nAdd edge without cycle\nDAG edge only\nNo merge\n\n\nRemove edge breaking cycle\nSplit SCC\nNew components\n\n\n\n\n\nTiny Code (Simplified Example)\nclass DSU:\n    def __init__(self, n):\n        self.parent = list(range(n))\n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n    def union(self, a, b):\n        ra, rb = self.find(a), self.find(b)\n        if ra != rb:\n            self.parent[rb] = ra\n\nclass DynamicSCC:\n    def __init__(self, n):\n        self.n = n\n        self.dsu = DSU(n)\n        self.graph = [set() for _ in range(n)]\n\n    def add_edge(self, u, v):\n        su, sv = self.dsu.find(u), self.dsu.find(v)\n        if su == sv:\n            return\n        if self._reachable(sv, su):\n            self.dsu.union(su, sv)\n        else:\n            self.graph[su].add(sv)\n\n    def _reachable(self, start, target, seen=None):\n        if seen is None: seen = set()\n        if start == target: return True\n        seen.add(start)\n        for nxt in self.graph[start]:\n            if nxt not in seen and self._reachable(nxt, target, seen):\n                return True\n        return False\n\n    def components(self):\n        comps = {}\n        for v in range(self.n):\n            root = self.dsu.find(v)\n            comps.setdefault(root, []).append(v)\n        return list(comps.values())\n\nscc = DynamicSCC(4)\nscc.add_edge(0, 1)\nscc.add_edge(1, 2)\nprint(scc.components())  # [[0], [1], [2], [3]]\nscc.add_edge(2, 0)\nprint(scc.components())  # [[0,1,2], [3]]\n\n\nWhy It Matters\n\nEfficient for long-running systems where graphs evolve\nUpdates SCCs incrementally rather than rebuilding\nSupports real-time queries of connectivity\nUseful for streaming graph databases, incremental compilers, interactive modeling tools\n\n\n\nA Gentle Proof (Why It Works)\nFor insertion:\n\nIf \\((u, v)\\) connects components \\(C_u\\) and \\(C_v\\)\nAnd if \\(C_v\\) can reach \\(C_u\\), then a cycle forms\nMerging \\(C_u\\) and \\(C_v\\) yields a valid SCC\nThe condensation DAG remains acyclic\n\nFor deletion:\n\nIf removal breaks reachability, SCC splits\nRecomputing locally ensures correctness\nOther unaffected SCCs remain valid\n\nEach update modifies only the local neighborhood of the affected components.\n\n\nTry It Yourself\n\nBuild a small directed graph.\nInsert edges step by step and print components.\nAdd a back-edge to create a cycle and observe merging.\nRemove an edge and check local recomputation.\nCompare results with full Tarjan recomputation.\n\n\n\nTest Cases\n\n\n\nStep\nEdge\nSCCs\n\n\n\n\n1\n0→1\n{0}, {1}, {2}\n\n\n2\n1→2\n{0}, {1}, {2}\n\n\n3\n2→0\n{0,1,2}\n\n\n4\n2→3\n{0,1,2}, {3}\n\n\n5\nremove 1→2\n{0,1}, {2}, {3}\n\n\n\n\n\nComplexity\n\nInsertion: \\(O(V + E)\\) amortized (with reachability check)\nDeletion: Local recomputation, typically sublinear\nSpace: \\(O(V + E)\\)\n\nDynamic SCC Maintenance provides a framework to keep SCCs consistent as the graph evolves, adapting efficiently to both incremental growth and structural decay.\n\n\n\n320 SCC for Weighted Graph\nSCC detection is usually discussed for unweighted graphs, where edge weights are irrelevant to reachability. However, in many real-world systems, weights encode constraints (cost, capacity, priority, probability), and we need to identify strong connectivity under these weighted conditions. This variant integrates SCC algorithms with weighted edge logic, allowing selective inclusion or exclusion of edges based on weight criteria.\n\nWhat Problem Are We Solving?\nWe want to find strongly connected components in a weighted directed graph. A standard SCC algorithm ignores weights, it only checks reachability. Here, we define SCCs based on edges satisfying a given weight predicate:\n\\[\n(u, v) \\in E_w \\quad \\text{iff} \\quad w(u, v) \\leq \\theta\n\\]\nWe can then run SCC algorithms (Tarjan, Kosaraju, Gabow, Path-based) on the subgraph induced by edges satisfying the constraint.\nCommon use cases:\n\nThresholded connectivity: Keep edges below cost \\(\\theta\\).\nCapacity-limited systems: Only include edges with capacity ≥ threshold.\nDynamic constraint graphs: Recompute SCCs as thresholds shift.\nProbabilistic networks: Consider edges with probability ≥ \\(p\\).\n\n\n\nHow Does It Work (Plain Language)?\n\nStart with a weighted directed graph \\(G = (V, E, w)\\)\nApply a predicate on weights (e.g. \\(w(u, v) \\le \\theta\\))\nBuild a filtered subgraph \\(G_\\theta = (V, E_\\theta)\\)\nRun a standard SCC algorithm on \\(G_\\theta\\)\n\nThe result groups vertices that are strongly connected under the weight constraint.\nIf \\(\\theta\\) changes, components can merge or split:\n\nIncreasing \\(\\theta\\) (loosening) → SCCs merge\nDecreasing \\(\\theta\\) (tightening) → SCCs split\n\n\n\n\nThreshold \\(\\theta\\)\nIncluded Edges\nSCCs\n\n\n\n\n3\n\\(w \\le 3\\)\n{A,B}, {C}\n\n\n5\n\\(w \\le 5\\)\n{A,B,C}\n\n\n\n\n\nTiny Code (Threshold-Based Filtering)\nPython Example\nfrom collections import defaultdict\n\nedges = [\n    (0, 1, 2),\n    (1, 2, 4),\n    (2, 0, 1),\n    (2, 3, 6),\n    (3, 2, 6)\n]\n\ndef build_subgraph(edges, theta):\n    g = defaultdict(list)\n    for u, v, w in edges:\n        if w &lt;= theta:\n            g[u].append(v)\n    return g\n\ndef dfs(v, g, visited, stack):\n    visited.add(v)\n    for u in g[v]:\n        if u not in visited:\n            dfs(u, g, visited, stack)\n    stack.append(v)\n\ndef reverse_graph(g):\n    rg = defaultdict(list)\n    for u in g:\n        for v in g[u]:\n            rg[v].append(u)\n    return rg\n\ndef kosaraju(g):\n    visited, stack = set(), []\n    for v in g:\n        if v not in visited:\n            dfs(v, g, visited, stack)\n    rg = reverse_graph(g)\n    visited.clear()\n    comps = []\n    while stack:\n        v = stack.pop()\n        if v not in visited:\n            comp = []\n            dfs(v, rg, visited, comp)\n            comps.append(comp)\n    return comps\n\ntheta = 4\ng_theta = build_subgraph(edges, theta)\nprint(\"SCCs with threshold\", theta, \":\", kosaraju(g_theta))\nOutput:\nSCCs with threshold 4 : [[2, 0, 1], [3]]\n\n\nWhy It Matters\n\nIncorporates weight constraints into connectivity\nUseful in optimization, routing, and clustering\nSupports incremental recomputation under shifting thresholds\nEnables multi-layer graph analysis (vary \\(\\theta\\) to see component evolution)\n\n\n\nA Gentle Proof (Why It Works)\nReachability in a weighted graph depends on which edges are active. Filtering edges by predicate preserves a subset of original reachability:\nIf there is a path \\(u \\to v\\) in \\(G_\\theta\\), all edges on that path satisfy \\(w(e) \\le \\theta\\). Since SCCs depend solely on reachability, standard algorithms applied to \\(G_\\theta\\) correctly identify weight-constrained SCCs.\nAs \\(\\theta\\) increases, the edge set \\(E_\\theta\\) grows monotonically:\n\\[\nE_{\\theta_1} \\subseteq E_{\\theta_2} \\quad \\text{for} \\quad \\theta_1 &lt; \\theta_2\n\\]\nTherefore, the SCC partition becomes coarser (components merge).\n\n\nTry It Yourself\n\nBuild a weighted graph.\nPick thresholds \\(\\theta = 2, 4, 6\\) and record SCCs.\nPlot how components merge as \\(\\theta\\) increases.\nTry predicates like \\(w(u, v) \\ge \\theta\\).\nCombine with Dynamic SCC Maintenance for evolving thresholds.\n\n\n\nTest Cases\n\n\n\n\\(\\theta\\)\nEdges Included\nSCCs\n\n\n\n\n2\n(0→1,2), (2→0)\n{0,2}, {1}, {3}\n\n\n4\n(0→1), (1→2), (2→0)\n{0,1,2}, {3}\n\n\n6\nAll edges\n{0,1,2,3}\n\n\n\n\n\nComplexity\n\nFiltering: \\(O(E)\\)\nSCC computation: \\(O(V + E_\\theta)\\)\nTotal: \\(O(V + E)\\)\nSpace: \\(O(V + E)\\)\n\nSCC for Weighted Graphs extends classic connectivity to contexts where not all edges are equal, revealing the layered structure of a graph as thresholds vary.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-33.-shortest-paths",
    "href": "books/en-us/list-4.html#section-33.-shortest-paths",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 33. Shortest Paths",
    "text": "Section 33. Shortest Paths\n\n321 Dijkstra (Binary Heap)\nDijkstra’s Algorithm is the cornerstone of shortest path computation in weighted graphs with nonnegative edge weights. It grows a frontier of known shortest paths, always expanding from the vertex with the smallest current distance, much like a wavefront advancing through the graph.\nUsing a binary heap (priority queue) keeps the next closest vertex selection efficient, making this version the standard for practical use.\n\nWhat Problem Are We Solving?\nWe need to find the shortest path from a single source vertex \\(s\\) to all other vertices in a directed or undirected graph with nonnegative weights.\nGiven a weighted graph \\(G = (V, E, w)\\) where \\(w(u, v) \\ge 0\\) for all \\((u, v)\\), the task is to compute:\n\\[\n\\text{dist}[v] = \\min_{\\text{path } s \\to v} \\sum_{(u, v) \\in \\text{path}} w(u, v)\n\\]\nTypical use cases:\n\nGPS navigation (road networks)\nNetwork routing\nPathfinding in games\nDependency resolution in weighted systems\n\n\n\nHow Does It Work (Plain Language)?\nThe algorithm maintains a distance array dist[], initialized with infinity for all vertices except the source.\n\nSet dist[s] = 0.\nUse a min-priority queue to repeatedly extract the vertex with the smallest distance.\nFor each neighbor, try to relax the edge:\n\n\\[\n\\text{if } \\text{dist}[u] + w(u, v) &lt; \\text{dist}[v], \\text{ then update } \\text{dist}[v]\n\\]\n\nPush the neighbor into the queue with updated distance.\nContinue until the queue is empty.\n\n\n\n\nStep\nVertex Extracted\nUpdated Distances\n\n\n\n\n1\n\\(s\\)\n\\(0\\)\n\n\n2\nNext smallest\nUpdate neighbors\n\n\n3\nRepeat\nUntil all vertices finalized\n\n\n\nThis is a greedy algorithm, once a vertex is visited, its shortest distance is final.\n\n\nTiny Code (Binary Heap)\nC (Using a Simple Priority Queue with qsort)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define MAX 100\n#define INF INT_MAX\n\nint n, graph[MAX][MAX];\n\nvoid dijkstra(int src) {\n    int dist[MAX], visited[MAX];\n    for (int i = 0; i &lt; n; i++) {\n        dist[i] = INF;\n        visited[i] = 0;\n    }\n    dist[src] = 0;\n\n    for (int count = 0; count &lt; n - 1; count++) {\n        int u = -1, min = INF;\n        for (int v = 0; v &lt; n; v++)\n            if (!visited[v] && dist[v] &lt; min)\n                min = dist[v], u = v;\n\n        if (u == -1) break;\n        visited[u] = 1;\n\n        for (int v = 0; v &lt; n; v++)\n            if (graph[u][v] && !visited[v] &&\n                dist[u] + graph[u][v] &lt; dist[v])\n                dist[v] = dist[u] + graph[u][v];\n    }\n\n    printf(\"Vertex\\tDistance\\n\");\n    for (int i = 0; i &lt; n; i++)\n        printf(\"%d\\t%d\\n\", i, dist[i]);\n}\n\nint main(void) {\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    printf(\"Enter adjacency matrix (0 for no edge):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &graph[i][j]);\n    dijkstra(0);\n}\nPython (Heap-Based Implementation)\nimport heapq\n\ndef dijkstra(graph, src):\n    n = len(graph)\n    dist = [float('inf')] * n\n    dist[src] = 0\n    pq = [(0, src)]\n    \n    while pq:\n        d, u = heapq.heappop(pq)\n        if d &gt; dist[u]:\n            continue\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                heapq.heappush(pq, (dist[v], v))\n    return dist\n\ngraph = {\n    0: [(1, 2), (2, 4)],\n    1: [(2, 1), (3, 7)],\n    2: [(4, 3)],\n    3: [(4, 1)],\n    4: []\n}\n\nprint(dijkstra(graph, 0))\n\n\nWhy It Matters\n\nEfficient shortest path when weights are nonnegative\nDeterministic and greedy, produces optimal paths\nWidely applicable to routing, logistics, and AI search\nForms the foundation for A* and Johnson’s algorithm\n\n\n\nA Gentle Proof (Why It Works)\nDijkstra’s invariant: When a vertex \\(u\\) is extracted from the priority queue, \\(\\text{dist}[u]\\) is final.\nProof idea: All alternative paths to \\(u\\) must go through vertices with greater or equal tentative distances, since edges are nonnegative. Thus, no shorter path exists.\nBy induction, the algorithm assigns the correct shortest distance to every vertex.\n\n\nTry It Yourself\n\nRun on a graph with 5 vertices and random weights.\nAdd an edge with a smaller weight and see how paths update.\nRemove negative edges and note incorrect results.\nVisualize the frontier expansion step by step.\nCompare with Bellman–Ford on the same graph.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nGraph\nEdges\nShortest Paths from 0\n\n\n\n\nSimple\n0→1(2), 0→2(4), 1→2(1), 2→4(3), 1→3(7), 3→4(1)\n[0, 2, 3, 7, 6]\n\n\nChain\n0→1(1), 1→2(1), 2→3(1)\n[0, 1, 2, 3]\n\n\nStar\n0→1(5), 0→2(2), 0→3(8)\n[0, 5, 2, 8]\n\n\n\n\n\nComplexity\n\nTime: \\(O((V + E)\\log V)\\) with binary heap\nSpace: \\(O(V + E)\\)\nWorks only if: \\(w(u, v) \\ge 0\\)\n\nDijkstra (Binary Heap) is the workhorse of graph search, greedy but precise, always chasing the next closest frontier until all paths fall into place.\n\n\n\n322 Dijkstra (Fibonacci Heap)\nDijkstra’s algorithm can be further optimized by replacing the binary heap with a Fibonacci heap, which offers faster decrease-key operations. This improvement reduces the overall time complexity, making it more suitable for dense graphs or theoretical analysis where asymptotic efficiency matters.\nWhile the constant factors are higher, the asymptotic time is improved to:\n\\[\nO(E + V \\log V)\n\\]\ncompared to the binary heap’s \\(O((V + E)\\log V)\\).\n\nWhat Problem Are We Solving?\nWe are computing single-source shortest paths in a directed or undirected weighted graph with nonnegative weights, but we want to optimize the priority queue operations to improve theoretical performance.\nGiven \\(G = (V, E, w)\\) with \\(w(u, v) \\ge 0\\), the task remains:\n\\[\n\\text{dist}[v] = \\min_{\\text{path } s \\to v} \\sum_{(u,v) \\in \\text{path}} w(u, v)\n\\]\nThe difference lies in how we manage the priority queue that selects the next vertex to process.\n\n\nHow Does It Work (Plain Language)?\nThe logic of Dijkstra’s algorithm is unchanged, only the data structure used for vertex selection and updates differs.\n\nInitialize all distances to \\(\\infty\\), except \\(\\text{dist}[s] = 0\\).\nInsert all vertices into a Fibonacci heap keyed by their current distance.\nRepeatedly extract the vertex \\(u\\) with smallest distance.\nFor each neighbor \\((u, v)\\):\n\nIf \\(\\text{dist}[u] + w(u, v) &lt; \\text{dist}[v]\\), update: \\[\n\\text{dist}[v] \\gets \\text{dist}[u] + w(u, v)\n\\] and call decrease-key on \\(v\\) in the heap.\n\nContinue until all vertices are finalized.\n\nThe Fibonacci heap provides:\n\nextract-min: \\(O(\\log V)\\) amortized\ndecrease-key: \\(O(1)\\) amortized\n\nThis improves the performance for dense graphs where edge relaxations dominate.\n\n\nTiny Code (Python, Simplified Fibonacci Heap)\nThis code illustrates the structure but omits full heap details, production implementations use libraries like networkx or specialized data structures.\nfrom heapq import heappush, heappop  # stand-in for demonstration\n\ndef dijkstra_fib(graph, src):\n    n = len(graph)\n    dist = [float('inf')] * n\n    dist[src] = 0\n    visited = [False] * n\n    heap = [(0, src)]\n\n    while heap:\n        d, u = heappop(heap)\n        if visited[u]:\n            continue\n        visited[u] = True\n        for v, w in graph[u]:\n            if not visited[v] and dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                heappush(heap, (dist[v], v))\n    return dist\n\ngraph = {\n    0: [(1, 1), (2, 4)],\n    1: [(2, 2), (3, 6)],\n    2: [(3, 3)],\n    3: []\n}\n\nprint(dijkstra_fib(graph, 0))\n(The above uses heapq for illustration; true Fibonacci heap gives better theoretical bounds.)\n\n\nWhy It Matters\n\nImproves theoretical time to \\(O(E + V \\log V)\\)\nDemonstrates asymptotic optimization using advanced heaps\nUsed in dense networks, theoretical research, and competition problems\nFoundation for algorithms like Johnson’s APSP and minimum mean cycle\n\n\n\nA Gentle Proof (Why It Works)\nThe correctness remains identical to Dijkstra’s original proof: When a vertex \\(u\\) is extracted (minimum key), its shortest distance is final because all edge weights are nonnegative.\nThe heap choice only affects efficiency:\n\nBinary heap: every decrease-key = \\(O(\\log V)\\)\nFibonacci heap: every decrease-key = \\(O(1)\\) amortized\n\nTotal operations:\n\n\\(V\\) extractions × \\(O(\\log V)\\)\n\\(E\\) decreases × \\(O(1)\\)\n\nHence total time:\n\\[\nO(V \\log V + E) = O(E + V \\log V)\n\\]\n\n\nTry It Yourself\n\nBuild a dense graph (e.g. \\(V=1000, E \\approx V^2\\)).\nCompare runtimes with binary heap version.\nVisualize priority queue operations.\nImplement decrease-key manually for insight.\nExplore Johnson’s algorithm using this version.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nShortest Paths from 0\n\n\n\n\nChain\n0→1(1), 1→2(2), 2→3(3)\n[0, 1, 3, 6]\n\n\nTriangle\n0→1(2), 1→2(2), 0→2(5)\n[0, 2, 4]\n\n\nDense\nAll pairs with small weights\nWorks efficiently\n\n\n\n\n\nComplexity\n\nTime: \\(O(E + V \\log V)\\)\nSpace: \\(O(V + E)\\)\nWorks only if: \\(w(u, v) \\ge 0\\)\n\nDijkstra (Fibonacci Heap) shows how data structure choice transforms an algorithm, the same idea, made sharper through careful engineering of priority operations.\n\n\n\n323 Bellman–Ford\nThe Bellman–Ford algorithm solves the single-source shortest path problem for graphs that may contain negative edge weights. Unlike Dijkstra’s algorithm, it does not rely on greedy selection and can handle edges with \\(w(u, v) &lt; 0\\), as long as there are no negative-weight cycles reachable from the source.\nIt systematically relaxes every edge multiple times, ensuring all paths up to length \\(V-1\\) are considered.\n\nWhat Problem Are We Solving?\nWe want to compute shortest paths from a source \\(s\\) in a weighted directed graph that may include negative weights.\nGiven \\(G = (V, E, w)\\), find for all \\(v \\in V\\):\n\\[\n\\text{dist}[v] = \\min_{\\text{path } s \\to v} \\sum_{(u,v) \\in \\text{path}} w(u,v)\n\\]\nIf a negative-weight cycle is reachable from \\(s\\), the shortest path is undefined (it can be reduced indefinitely). Bellman–Ford detects this situation explicitly.\n\n\nHow Does It Work (Plain Language)?\nBellman–Ford uses edge relaxation repeatedly.\n\nInitialize \\(\\text{dist}[s] = 0\\), others \\(\\infty\\).\nRepeat \\(V - 1\\) times: For every edge \\((u, v)\\): \\[\n\\text{if } \\text{dist}[u] + w(u, v) &lt; \\text{dist}[v], \\text{ then update } \\text{dist}[v]\n\\]\nAfter \\(V - 1\\) passes, all shortest paths are settled.\nPerform one more pass: if any edge can still relax, a negative-weight cycle exists.\n\n\n\n\nIteration\nUpdated Vertices\nNotes\n\n\n\n\n1\nNeighbors of source\nFirst layer\n\n\n2\nNext layer\nPropagate\n\n\n…\n…\n…\n\n\n\\(V-1\\)\nAll shortest paths stabilized\nDone\n\n\n\n\n\nTiny Code (C Example)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAX 100\n#define INF 1000000000\n\ntypedef struct { int u, v, w; } Edge;\nEdge edges[MAX];\nint dist[MAX];\n\nint main(void) {\n    int V, E, s;\n    printf(\"Enter vertices, edges, source: \");\n    scanf(\"%d %d %d\", &V, &E, &s);\n    printf(\"Enter edges (u v w):\\n\");\n    for (int i = 0; i &lt; E; i++)\n        scanf(\"%d %d %d\", &edges[i].u, &edges[i].v, &edges[i].w);\n\n    for (int i = 0; i &lt; V; i++) dist[i] = INF;\n    dist[s] = 0;\n\n    for (int i = 1; i &lt; V; i++)\n        for (int j = 0; j &lt; E; j++) {\n            int u = edges[j].u, v = edges[j].v, w = edges[j].w;\n            if (dist[u] != INF && dist[u] + w &lt; dist[v])\n                dist[v] = dist[u] + w;\n        }\n\n    for (int j = 0; j &lt; E; j++) {\n        int u = edges[j].u, v = edges[j].v, w = edges[j].w;\n        if (dist[u] != INF && dist[u] + w &lt; dist[v]) {\n            printf(\"Negative cycle detected\\n\");\n            return 0;\n        }\n    }\n\n    printf(\"Vertex\\tDistance\\n\");\n    for (int i = 0; i &lt; V; i++)\n        printf(\"%d\\t%d\\n\", i, dist[i]);\n}\nPython (Readable Version)\ndef bellman_ford(V, edges, src):\n    dist = [float('inf')] * V\n    dist[src] = 0\n\n    for _ in range(V - 1):\n        for u, v, w in edges:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n\n    # Detect negative cycle\n    for u, v, w in edges:\n        if dist[u] + w &lt; dist[v]:\n            raise ValueError(\"Negative cycle detected\")\n\n    return dist\n\nedges = [(0, 1, 6), (0, 2, 7), (1, 2, 8), (1, 3, 5),\n         (1, 4, -4), (2, 3, -3), (2, 4, 9), (3, 1, -2),\n         (4, 3, 7)]\n\nprint(bellman_ford(5, edges, 0))\n\n\nWhy It Matters\n\nWorks with negative weights\nDetects negative-weight cycles\nSimpler logic, easier to prove correctness\nUsed in currency arbitrage, dynamic programming, policy evaluation\n\n\n\nA Gentle Proof (Why It Works)\nA shortest path has at most \\(V-1\\) edges (a path longer than that must contain a cycle). Each iteration ensures all paths up to that length are relaxed. Thus after \\(V-1\\) rounds, all shortest paths are found.\nThe \\((V)\\)-th round detects further improvement, indicating a negative cycle.\nFormally, after iteration \\(k\\), \\(\\text{dist}[v]\\) is the length of the shortest path from \\(s\\) to \\(v\\) using at most \\(k\\) edges.\n\n\nTry It Yourself\n\nRun on graphs with negative weights.\nAdd a negative cycle and observe detection.\nCompare results with Dijkstra (fails with negative edges).\nVisualize relaxation per iteration.\nUse it to detect arbitrage in currency exchange graphs.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nShortest Distances\n\n\n\n\nChain\n0→1(5), 1→2(-2)\n[0, 5, 3]\n\n\nNegative edge\n0→1(4), 0→2(5), 1→2(-10)\n[0, 4, -6]\n\n\nNegative cycle\n0→1(1), 1→2(-2), 2→0(-1)\nDetected\n\n\n\n\n\nComplexity\n\nTime: \\(O(VE)\\)\nSpace: \\(O(V)\\)\nHandles: \\(w(u, v) \\ge -\\infty\\), no negative cycles\n\nBellman–Ford is the steady walker of shortest path algorithms, slower than Dijkstra, but unshaken by negative edges and always alert to cycles that break the rules.\n\n\n\n324 SPFA (Queue Optimization)\nThe Shortest Path Faster Algorithm (SPFA) is an optimized implementation of Bellman–Ford that uses a queue to avoid unnecessary relaxations. Instead of relaxing all edges in every iteration, SPFA only processes vertices whose distances were recently updated, often resulting in much faster average performance, especially in sparse graphs or those without negative cycles.\nIn the worst case, it still runs in \\(O(VE)\\), but typical performance is closer to \\(O(E)\\).\n\nWhat Problem Are We Solving?\nWe want to find single-source shortest paths in a graph that may contain negative weights, but no negative-weight cycles.\nGiven a directed graph \\(G = (V, E, w)\\) with edge weights \\(w(u, v)\\) possibly negative, we compute:\n\\[\n\\text{dist}[v] = \\min_{\\text{path } s \\to v} \\sum_{(u, v) \\in \\text{path}} w(u, v)\n\\]\nBellman–Ford’s \\(V-1\\) rounds of edge relaxation can be wasteful; SPFA avoids rechecking edges from vertices that haven’t improved.\n\n\nHow Does It Work (Plain Language)?\nSPFA keeps a queue of vertices whose outgoing edges might lead to relaxation. Each time a vertex’s distance improves, it’s enqueued for processing.\n\nInitialize \\(\\text{dist}[s] = 0\\), others \\(\\infty\\).\nPush \\(s\\) into a queue.\nWhile the queue is not empty:\n\nPop vertex \\(u\\).\nFor each \\((u, v)\\): \\[\n\\text{if } \\text{dist}[u] + w(u, v) &lt; \\text{dist}[v], \\text{ then update } \\text{dist}[v]\n\\]\nIf \\(\\text{dist}[v]\\) changed and \\(v\\) is not in queue, enqueue \\(v\\).\n\nContinue until queue is empty.\n\n\n\n\nStep\nQueue\nOperation\n\n\n\n\n1\n[s]\nStart\n\n\n2\nPop u, relax neighbors\nPush improved vertices\n\n\n3\nRepeat\nUntil no more improvements\n\n\n\nSPFA uses a lazy relaxation strategy, guided by actual updates.\n\n\nTiny Code (C Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAX 100\n#define INF 1000000000\n\ntypedef struct { int v, w; } Edge;\nEdge graph[MAX][MAX];\nint deg[MAX];\n\nint queue[MAX], front = 0, rear = 0;\nbool in_queue[MAX];\nint dist[MAX];\n\nvoid enqueue(int x) {\n    queue[rear++] = x;\n    in_queue[x] = true;\n}\nint dequeue() {\n    int x = queue[front++];\n    in_queue[x] = false;\n    return x;\n}\n\nint main(void) {\n    int V, E, s;\n    printf(\"Enter vertices, edges, source: \");\n    scanf(\"%d %d %d\", &V, &E, &s);\n\n    for (int i = 0; i &lt; V; i++) deg[i] = 0;\n    printf(\"Enter edges (u v w):\\n\");\n    for (int i = 0; i &lt; E; i++) {\n        int u, v, w;\n        scanf(\"%d %d %d\", &u, &v, &w);\n        graph[u][deg[u]].v = v;\n        graph[u][deg[u]].w = w;\n        deg[u]++;\n    }\n\n    for (int i = 0; i &lt; V; i++) dist[i] = INF;\n    dist[s] = 0;\n    enqueue(s);\n\n    while (front &lt; rear) {\n        int u = dequeue();\n        for (int i = 0; i &lt; deg[u]; i++) {\n            int v = graph[u][i].v, w = graph[u][i].w;\n            if (dist[u] + w &lt; dist[v]) {\n                dist[v] = dist[u] + w;\n                if (!in_queue[v]) enqueue(v);\n            }\n        }\n    }\n\n    printf(\"Vertex\\tDistance\\n\");\n    for (int i = 0; i &lt; V; i++)\n        printf(\"%d\\t%d\\n\", i, dist[i]);\n}\nPython (Queue-Based Implementation)\nfrom collections import deque\n\ndef spfa(V, edges, src):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n    \n    dist = [float('inf')] * V\n    in_queue = [False] * V\n    dist[src] = 0\n\n    q = deque([src])\n    in_queue[src] = True\n\n    while q:\n        u = q.popleft()\n        in_queue[u] = False\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                if not in_queue[v]:\n                    q.append(v)\n                    in_queue[v] = True\n    return dist\n\nedges = [(0,1,2),(0,2,4),(1,2,-1),(2,3,2)]\nprint(spfa(4, edges, 0))\n\n\nWhy It Matters\n\nPractical improvement over Bellman–Ford\nEfficient for sparse and nearly acyclic graphs\nCan handle negative weights\nUsed in network optimization, real-time routing, flow systems\n\n\n\nA Gentle Proof (Why It Works)\nEach vertex enters the queue when its distance improves. At most \\(V-1\\) improvements per vertex (no shorter path with more edges). Thus, every relaxation converges to the same fixed point as Bellman–Ford.\nSPFA is an asynchronous relaxation method:\n\nStill guarantees correctness under nonnegative cycles\nDetects negative cycles if a vertex is enqueued \\(\\ge V\\) times\n\nTo check for negative cycles:\n\nMaintain a count[v] of relaxations\nIf count[v] &gt; V, report a cycle\n\n\n\nTry It Yourself\n\nTest on graphs with negative edges.\nCompare runtime with Bellman–Ford.\nAdd negative cycle detection.\nTry both sparse and dense graphs.\nMeasure how queue length varies during execution.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nResult\n\n\n\n\nSimple\n0→1(2), 0→2(4), 1→2(-1), 2→3(2)\n[0, 2, 1, 3]\n\n\nNegative edge\n0→1(5), 1→2(-3)\n[0, 5, 2]\n\n\nCycle\n0→1(1), 1→2(-2), 2→0(1)\nDetect\n\n\n\n\n\nComplexity\n\nAverage: \\(O(E)\\)\nWorst case: \\(O(VE)\\)\nSpace: \\(O(V + E)\\)\n\nSPFA (Queue Optimization) is the agile Bellman–Ford, reacting only when change is needed, converging faster in practice while preserving the same correctness guarantees.\n\n\n\n325 A* Search\nThe A* (A-star) algorithm combines Dijkstra’s shortest path with best-first search, guided by a heuristic function. It efficiently finds the shortest path from a start node to a goal node by always expanding the vertex that seems closest to the goal, according to the estimate:\n\\[\nf(v) = g(v) + h(v)\n\\]\nwhere\n\n\\(g(v)\\) = cost from start to \\(v\\) (known),\n\\(h(v)\\) = heuristic estimate from \\(v\\) to goal (guessed),\n\\(f(v)\\) = total estimated cost through \\(v\\).\n\nWhen the heuristic is admissible (never overestimates), A* guarantees optimality.\n\nWhat Problem Are We Solving?\nWe want to find the shortest path from a source \\(s\\) to a target \\(t\\) in a weighted graph (often spatial), using additional knowledge about the goal to guide the search.\nGiven \\(G = (V, E, w)\\) and a heuristic \\(h(v)\\), the task is to minimize:\n\\[\n\\text{cost}(s, t) = \\min_{\\text{path } s \\to t} \\sum_{(u, v) \\in \\text{path}} w(u, v)\n\\]\nApplications:\n\nPathfinding (games, robotics, navigation)\nPlanning systems (AI, logistics)\nGrid and map searches\nState-space exploration\n\n\n\nHow Does It Work (Plain Language)?\nA* behaves like Dijkstra’s algorithm, but instead of expanding the closest node to the start (\\(g\\)), it expands the one with smallest estimated total cost (\\(f = g + h\\)).\n\nInitialize all distances: g[start] = 0, others \\(\\infty\\).\nCompute f[start] = h[start].\nPush (f, node) into a priority queue.\nWhile queue not empty:\n\nPop node \\(u\\) with smallest \\(f(u)\\).\nIf \\(u = \\text{goal}\\), stop, path found.\nFor each neighbor \\(v\\): \\[\ng'(v) = g(u) + w(u, v)\n\\] If \\(g'(v) &lt; g(v)\\), update: \\[\ng(v) = g'(v), \\quad f(v) = g(v) + h(v)\n\\] Push \\(v\\) into the queue.\n\nReconstruct path using parent pointers.\n\n\n\n\nNode\n\\(g(v)\\)\n\\(h(v)\\)\n\\(f(v) = g + h\\)\nExpanded?\n\n\n\n\nstart\n0\nheuristic\nheuristic\n✅\n\n\n…\n…\n…\n…\n…\n\n\n\nThe heuristic guides exploration, focusing on promising routes.\n\n\nTiny Code (Python Example)\nGrid-based A* (Manhattan heuristic):\nimport heapq\n\ndef heuristic(a, b):\n    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\ndef astar(grid, start, goal):\n    rows, cols = len(grid), len(grid[0])\n    g = {start: 0}\n    f = {start: heuristic(start, goal)}\n    pq = [(f[start], start)]\n    parent = {start: None}\n    visited = set()\n\n    while pq:\n        _, current = heapq.heappop(pq)\n        if current == goal:\n            path = []\n            while current:\n                path.append(current)\n                current = parent[current]\n            return path[::-1]\n\n        visited.add(current)\n        x, y = current\n        for dx, dy in [(1,0),(-1,0),(0,1),(0,-1)]:\n            nx, ny = x + dx, y + dy\n            neighbor = (nx, ny)\n            if 0 &lt;= nx &lt; rows and 0 &lt;= ny &lt; cols and grid[nx][ny] == 0:\n                tentative = g[current] + 1\n                if tentative &lt; g.get(neighbor, float('inf')):\n                    g[neighbor] = tentative\n                    f[neighbor] = tentative + heuristic(neighbor, goal)\n                    parent[neighbor] = current\n                    heapq.heappush(pq, (f[neighbor], neighbor))\n    return None\n\ngrid = [\n    [0,0,0,0],\n    [1,1,0,1],\n    [0,0,0,0],\n    [0,1,1,0],\n]\nstart = (0,0)\ngoal = (3,3)\npath = astar(grid, start, goal)\nprint(path)\n\n\nWhy It Matters\n\nFaster than Dijkstra for goal-directed search\nOptimal if heuristic is admissible (\\(h(v) \\le \\text{true cost}\\))\nEfficient if heuristic is also consistent (triangle inequality)\nWidely used in AI, robotics, navigation, route planning\n\n\n\nA Gentle Proof (Why It Works)\nIf the heuristic \\(h(v)\\) never overestimates the true remaining cost:\n\\[\nh(v) \\le \\text{cost}(v, t)\n\\]\nthen \\(f(v) = g(v) + h(v)\\) is always a lower bound on the true cost. Therefore, when the goal is extracted (smallest \\(f\\)), the path is guaranteed optimal.\nIf \\(h\\) also satisfies consistency: \\[\nh(u) \\le w(u, v) + h(v)\n\\] then \\(f\\)-values are nondecreasing, and each node is expanded only once.\n\n\nTry It Yourself\n\nImplement A* on a grid with obstacles.\nExperiment with different heuristics (Manhattan, Euclidean).\nSet \\(h(v) = 0\\) → becomes Dijkstra.\nSet \\(h(v) = \\text{true distance}\\) → ideal search.\nTry inadmissible \\(h\\) → faster but possibly suboptimal.\n\n\n\nTest Cases\n\n\n\nGraph Type\nHeuristic\nResult\n\n\n\n\nGrid (4×4)\nManhattan\nShortest path found\n\n\nWeighted\nEuclidean\nOptimal route\n\n\nAll \\(h=0\\)\nNone\nBecomes Dijkstra\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log V)\\) (depends on heuristic)\nSpace: \\(O(V)\\)\nOptimal if: \\(h\\) admissible\nComplete if: finite branching factor\n\nA* Search is Dijkstra with foresight, driven not just by cost so far, but by an informed guess of the journey ahead.\n\n\n\n326 Floyd–Warshall\nThe Floyd–Warshall algorithm is a dynamic programming approach for computing all-pairs shortest paths (APSP) in a weighted directed graph. It iteratively refines the shortest path estimates between every pair of vertices by allowing intermediate vertices step by step.\nIt works even with negative weights, as long as there are no negative cycles.\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\n\\text{dist}(u, v) = \\min_{\\text{paths } u \\to v} \\sum_{(x, y) \\in \\text{path}} w(x, y)\n\\]\nfor all pairs \\((u, v)\\) in a graph \\(G = (V, E, w)\\).\nWe allow negative weights but no negative cycles. It’s particularly useful when:\n\nWe need all-pairs shortest paths.\nThe graph is dense (\\(E \\approx V^2\\)).\nWe want transitive closure or reachability (set \\(w(u, v) = 1\\)).\n\n\n\nHow Does It Work (Plain Language)?\nWe progressively allow each vertex as a possible intermediate waypoint. Initially, the shortest path from \\(i\\) to \\(j\\) is just the direct edge. Then, for each vertex \\(k\\), we check if a path through \\(k\\) improves the distance.\nThe recurrence relation:\n\\[\nd_k(i, j) = \\min \\big( d_{k-1}(i, j),; d_{k-1}(i, k) + d_{k-1}(k, j) \\big)\n\\]\nImplementation uses in-place updates:\n\\[\n\\text{dist}[i][j] = \\min(\\text{dist}[i][j],; \\text{dist}[i][k] + \\text{dist}[k][j])\n\\]\nThree nested loops:\n\nk (intermediate)\ni (source)\nj (destination)\n\n\n\n\nk\ni\nj\nUpdate\n\n\n\n\n0\nall\nall\nconsider vertex 0 as waypoint\n\n\n1\nall\nall\nconsider vertex 1 as waypoint\n\n\n…\n…\n…\n…\n\n\n\nAfter \\(V\\) iterations, all shortest paths are finalized.\n\n\nTiny Code (C Example)\n#include &lt;stdio.h&gt;\n#define INF 1000000000\n#define MAX 100\n\nint main(void) {\n    int n;\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    int dist[MAX][MAX];\n    printf(\"Enter adjacency matrix (INF=9999, 0 for self):\\n\");\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &dist[i][j]);\n\n    for (int k = 0; k &lt; n; k++)\n        for (int i = 0; i &lt; n; i++)\n            for (int j = 0; j &lt; n; j++)\n                if (dist[i][k] + dist[k][j] &lt; dist[i][j])\n                    dist[i][j] = dist[i][k] + dist[k][j];\n\n    printf(\"All-Pairs Shortest Distances:\\n\");\n    for (int i = 0; i &lt; n; i++) {\n        for (int j = 0; j &lt; n; j++)\n            printf(\"%8d \", dist[i][j]);\n        printf(\"\\n\");\n    }\n}\nPython Version\ndef floyd_warshall(graph):\n    n = len(graph)\n    dist = [row[:] for row in graph]\n\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                if dist[i][k] + dist[k][j] &lt; dist[i][j]:\n                    dist[i][j] = dist[i][k] + dist[k][j]\n    return dist\n\nINF = float('inf')\ngraph = [\n    [0, 3, INF, 7],\n    [8, 0, 2, INF],\n    [5, INF, 0, 1],\n    [2, INF, INF, 0]\n]\n\nres = floyd_warshall(graph)\nfor row in res:\n    print(row)\n\n\nWhy It Matters\n\nComputes all-pairs shortest paths in one pass\nWorks with negative weights\nDetects negative cycles if \\(\\text{dist}[i][i] &lt; 0\\)\nUseful for transitive closure, routing tables, graph condensation\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(d_k(i, j)\\) be the shortest path from \\(i\\) to \\(j\\) using only intermediate vertices from \\({1, 2, \\dots, k}\\).\nBase case (\\(k=0\\)): \\(d_0(i, j) = w(i, j)\\), the direct edge.\nInductive step: For each \\(k\\), either the shortest path avoids \\(k\\) or goes through \\(k\\). Thus:\n\\[\nd_k(i, j) = \\min \\big(d_{k-1}(i, j),; d_{k-1}(i, k) + d_{k-1}(k, j)\\big)\n\\]\nBy induction, after \\(V\\) iterations, all shortest paths are covered.\n\n\nTry It Yourself\n\nBuild a 4×4 weighted matrix.\nIntroduce a negative edge (but no cycle).\nCheck results after each iteration.\nDetect cycle: observe if \\(\\text{dist}[i][i] &lt; 0\\).\nUse it to compute reachability (replace INF with 0/1).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nGraph\nDistances\n\n\n\n\n\nSimple\n0→1(3), 0→3(7), 1→2(2), 2→0(5), 3→0(2)\nAll-pairs paths computed\n\n\nNegative edge\n0→1(1), 1→2(-2), 2→0(4)\nValid shortest paths\n\n\nNegative cycle\n0→1(1), 1→2(-2), 2→0(-1)\n\\(\\text{dist}[0][0] &lt; 0\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(V^3)\\)\nSpace: \\(O(V^2)\\)\nDetects: negative cycles if \\(\\text{dist}[i][i] &lt; 0\\)\n\nFloyd–Warshall is the complete memory of a graph, every distance, every route, all computed through careful iteration over all possible intermediates.\n\n\n\n327 Johnson’s Algorithm\nJohnson’s Algorithm efficiently computes all-pairs shortest paths (APSP) in a sparse weighted directed graph, even when negative weights are present (but no negative cycles). It cleverly combines Bellman–Ford and Dijkstra, using reweighting to eliminate negative edges while preserving shortest path relationships.\nThe result: \\[\nO(VE + V^2 \\log V)\n\\] which is far more efficient than Floyd–Warshall (\\(O(V^3)\\)) for sparse graphs.\n\nWhat Problem Are We Solving?\nWe need shortest paths between every pair of vertices, even if some edges have negative weights. Directly running Dijkstra fails with negative edges, and running Bellman–Ford for each vertex would cost \\(O(V^2E)\\).\nJohnson’s approach fixes this by reweighting edges to make them nonnegative, then applying Dijkstra from every vertex.\nGiven a weighted directed graph \\(G = (V, E, w)\\):\n\\[\nw(u, v) \\in \\mathbb{R}, \\quad \\text{no negative cycles.}\n\\]\nWe seek:\n\\[\n\\text{dist}(u, v) = \\min_{\\text{path } u \\to v} \\sum w(x, y)\n\\]\n\n\nHow Does It Work (Plain Language)?\n\nAdd a new vertex \\(s\\), connect it to every other vertex with edge weight \\(0\\).\nRun Bellman–Ford from \\(s\\) to compute potential values \\(h(v)\\): \\[\nh(v) = \\text{dist}_s(v)\n\\]\nReweight edges: \\[\nw'(u, v) = w(u, v) + h(u) - h(v)\n\\] This ensures all \\(w'(u, v) \\ge 0\\).\nRemove \\(s\\).\nFor each vertex \\(u\\), run Dijkstra on the reweighted graph \\(w'\\).\nRecover original distances: \\[\n\\text{dist}(u, v) = \\text{dist}'(u, v) + h(v) - h(u)\n\\]\n\n\n\n\nStep\nAction\nResult\n\n\n\n\n1\nAdd source \\(s\\)\nConnect to all\n\n\n2\nBellman–Ford\nCompute potentials\n\n\n3\nReweight edges\nAll nonnegative\n\n\n4\nRun Dijkstra\n\\(O(VE + V^2 \\log V)\\)\n\n\n5\nRestore distances\nAdjust using \\(h\\)\n\n\n\n\n\nTiny Code (Python Example)\nimport heapq\n\ndef bellman_ford(V, edges, s):\n    dist = [float('inf')] * V\n    dist[s] = 0\n    for _ in range(V - 1):\n        for u, v, w in edges:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n    for u, v, w in edges:\n        if dist[u] + w &lt; dist[v]:\n            raise ValueError(\"Negative cycle detected\")\n    return dist\n\ndef dijkstra(V, adj, src):\n    dist = [float('inf')] * V\n    dist[src] = 0\n    pq = [(0, src)]\n    while pq:\n        d, u = heapq.heappop(pq)\n        if d &gt; dist[u]:\n            continue\n        for v, w in adj[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                heapq.heappush(pq, (dist[v], v))\n    return dist\n\ndef johnson(V, edges):\n    # Step 1: add new source s\n    s = V\n    new_edges = edges + [(s, v, 0) for v in range(V)]\n    h = bellman_ford(V + 1, new_edges, s)\n\n    # Step 2: reweight edges\n    adj = [[] for _ in range(V)]\n    for u, v, w in edges:\n        adj[u].append((v, w + h[u] - h[v]))\n\n    # Step 3: run Dijkstra from each vertex\n    dist_matrix = [[float('inf')] * V for _ in range(V)]\n    for u in range(V):\n        d = dijkstra(V, adj, u)\n        for v in range(V):\n            dist_matrix[u][v] = d[v] + h[v] - h[u]\n    return dist_matrix\n\nedges = [\n    (0, 1, 1),\n    (1, 2, -2),\n    (2, 0, 4),\n    (2, 3, 2),\n    (3, 1, 7)\n]\n\nres = johnson(4, edges)\nfor row in res:\n    print(row)\n\n\nWhy It Matters\n\nWorks with negative weights\nCombines Bellman–Ford’s flexibility with Dijkstra’s speed\nMuch faster than Floyd–Warshall for sparse graphs\nUsed in routing, dependency graphs, and AI navigation\n\n\n\nA Gentle Proof (Why It Works)\nReweighting preserves shortest path order:\nFor any path \\(P = (v_0, v_1, \\dots, v_k)\\):\n\\[\nw'(P) = w(P) + h(v_0) - h(v_k)\n\\]\nTherefore:\n\\[\nw'(u, v) &lt; w'(u, x) \\iff w(u, v) &lt; w(u, x)\n\\]\nAll shortest paths in \\(w\\) are shortest in \\(w'\\), but now all weights are nonnegative, allowing Dijkstra.\nFinally, distances are restored:\n\\[\n\\text{dist}(u, v) = \\text{dist}'(u, v) + h(v) - h(u)\n\\]\n\n\nTry It Yourself\n\nAdd negative-weight edges (no cycles) and compare results with Floyd–Warshall.\nVisualize reweighting: show \\(w'(u, v)\\).\nTest on sparse vs dense graphs.\nIntroduce negative cycle to trigger detection.\nReplace Dijkstra with Fibonacci heap for \\(O(VE + V^2 \\log V)\\).\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nResult\n\n\n\n\nTriangle\n0→1(1), 1→2(-2), 2→0(4)\nAll-pairs distances\n\n\nNegative Edge\n0→1(-1), 1→2(2)\nCorrect\n\n\nNegative Cycle\n0→1(-2), 1→0(-3)\nDetected\n\n\n\n\n\nComplexity\n\nTime: \\(O(VE + V^2 \\log V)\\)\nSpace: \\(O(V^2)\\)\nWorks if: no negative cycles\n\nJohnson’s Algorithm is the harmonizer of shortest paths, reweighting the melody so every note becomes nonnegative, letting Dijkstra play across the entire graph with speed and precision.\n\n\n\n328 0–1 BFS\nThe 0–1 BFS algorithm is a specialized shortest path technique for graphs where edge weights are only 0 or 1. It uses a deque (double-ended queue) instead of a priority queue, allowing efficient relaxation in linear time.\nBy pushing 0-weight edges to the front and 1-weight edges to the back, the algorithm maintains an always-correct frontier, effectively simulating Dijkstra’s behavior in \\(O(V + E)\\) time.\n\nWhat Problem Are We Solving?\nWe want to compute single-source shortest paths in a directed or undirected graph with edge weights in \\({0, 1}\\):\n\\[\nw(u, v) \\in {0, 1}\n\\]\nWe need:\n\\[\n\\text{dist}[v] = \\min_{\\text{path } s \\to v} \\sum_{(u, v) \\in \\text{path}} w(u, v)\n\\]\nTypical applications:\n\nUnweighted graphs with special transitions (e.g. toggles, switches)\nState-space searches with free vs costly actions\nBinary grids, bitmask problems, or minimum operations graphs\n\n\n\nHow Does It Work (Plain Language)?\n0–1 BFS replaces the priority queue in Dijkstra’s algorithm with a deque, exploiting the fact that edge weights are only 0 or 1.\n\nInitialize dist[v] = ∞ for all \\(v\\), set dist[s] = 0.\nPush \\(s\\) into deque.\nWhile deque not empty:\n\nPop from front.\nFor each neighbor \\((u, v)\\):\n\nIf \\(w(u, v) = 0\\) and improves distance, push front.\nIf \\(w(u, v) = 1\\) and improves distance, push back.\n\n\n\nThis ensures vertices are always processed in non-decreasing order of distance, just like Dijkstra.\n\n\n\nStep\nEdge Type\nAction\n\n\n\n\n\\(w=0\\)\npush front\nprocess immediately\n\n\n\\(w=1\\)\npush back\nprocess later\n\n\n\n\n\nTiny Code (C Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAX 1000\n#define INF 1000000000\n\ntypedef struct { int v, w; } Edge;\nEdge graph[MAX][MAX];\nint deg[MAX];\nint dist[MAX];\nint deque[MAX * 2], front = MAX, back = MAX;\n\nvoid push_front(int x) { deque[--front] = x; }\nvoid push_back(int x) { deque[back++] = x; }\nint pop_front() { return deque[front++]; }\nbool empty() { return front == back; }\n\nint main(void) {\n    int V, E, s;\n    scanf(\"%d %d %d\", &V, &E, &s);\n    for (int i = 0; i &lt; E; i++) {\n        int u, v, w;\n        scanf(\"%d %d %d\", &u, &v, &w);\n        graph[u][deg[u]].v = v;\n        graph[u][deg[u]].w = w;\n        deg[u]++;\n    }\n\n    for (int i = 0; i &lt; V; i++) dist[i] = INF;\n    dist[s] = 0;\n    push_front(s);\n\n    while (!empty()) {\n        int u = pop_front();\n        for (int i = 0; i &lt; deg[u]; i++) {\n            int v = graph[u][i].v;\n            int w = graph[u][i].w;\n            if (dist[u] + w &lt; dist[v]) {\n                dist[v] = dist[u] + w;\n                if (w == 0) push_front(v);\n                else push_back(v);\n            }\n        }\n    }\n\n    for (int i = 0; i &lt; V; i++)\n        printf(\"%d: %d\\n\", i, dist[i]);\n}\nPython (Deque-Based)\nfrom collections import deque\n\ndef zero_one_bfs(V, edges, src):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n\n    dist = [float('inf')] * V\n    dist[src] = 0\n    dq = deque([src])\n\n    while dq:\n        u = dq.popleft()\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                if w == 0:\n                    dq.appendleft(v)\n                else:\n                    dq.append(v)\n    return dist\n\nedges = [(0,1,0),(1,2,1),(0,2,1),(2,3,0)]\nprint(zero_one_bfs(4, edges, 0))\n\n\nWhy It Matters\n\nLinear time for graphs with 0/1 weights\nSimpler than Dijkstra, faster in special cases\nWorks on state-transition graphs (bit flips, BFS + cost)\nCommon in competitive programming, AI, robotics\n\n\n\nA Gentle Proof (Why It Works)\nBecause all edge weights are either 0 or 1, distances increase by at most 1 each step. The deque ensures that nodes are processed in order of nondecreasing distance:\n\nWhen relaxing a \\(0\\)-edge, we push the vertex to the front (same distance).\nWhen relaxing a \\(1\\)-edge, we push to the back (distance +1).\n\nThus, the deque acts like a monotonic priority queue, guaranteeing correctness equivalent to Dijkstra.\n\n\nTry It Yourself\n\nBuild a small graph with edges of 0 and 1.\nCompare output with Dijkstra’s algorithm.\nVisualize deque operations.\nTry a grid where moving straight costs 0, turning costs 1.\nMeasure runtime vs Dijkstra on sparse graphs.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nResult\n\n\n\n\nSimple\n0→1(0), 1→2(1), 0→2(1), 2→3(0)\n[0, 0, 1, 1]\n\n\nZero edges\n0→1(0), 1→2(0), 2→3(0)\n[0, 0, 0, 0]\n\n\nMixed\n0→1(1), 0→2(0), 2→3(1)\n[0, 1, 0, 1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(V + E)\\)\nSpace: \\(O(V + E)\\)\nConditions: \\(w(u, v) \\in {0, 1}\\)\n\n0–1 BFS is the binary Dijkstra, a two-speed traveler that knows when to sprint ahead for free and when to patiently queue for a cost.\n\n\n\n329 Dial’s Algorithm\nDial’s Algorithm is a variant of Dijkstra’s algorithm optimized for graphs with nonnegative integer weights that are small and bounded. Instead of a heap, it uses an array of buckets, one for each possible distance modulo the maximum edge weight. This yields \\(O(V + E + C)\\) performance, where \\(C\\) is the maximum edge cost.\nIt’s ideal when edge weights are integers in a small range, such as \\({0, 1, \\dots, C}\\).\n\nWhat Problem Are We Solving?\nWe want single-source shortest paths in a graph where:\n\\[\nw(u, v) \\in {0, 1, 2, \\dots, C}, \\quad C \\text{ small}\n\\]\nGiven a weighted directed graph \\(G = (V, E, w)\\), the goal is to compute:\n\\[\n\\text{dist}[v] = \\min_{\\text{path } s \\to v} \\sum_{(u, v) \\in \\text{path}} w(u, v)\n\\]\nIf we know the maximum edge weight \\(C\\), we can replace a heap with an array of queues, cycling through distances efficiently.\nApplications:\n\nNetwork routing with small costs\nGrid-based movement with few weight levels\nTelecommunication scheduling\nTraffic flow problems\n\n\n\nHow Does It Work (Plain Language)?\nDial’s algorithm groups vertices by their current tentative distance. Instead of a priority queue, it maintains buckets B[0..C], where each bucket stores vertices with distance congruent modulo \\((C+1)\\).\n\nInitialize \\(\\text{dist}[v] = \\infty\\); set \\(\\text{dist}[s] = 0\\).\nPlace \\(s\\) in B[0].\nFor current bucket index i, process all vertices in B[i]:\n\nFor each edge \\((u, v, w)\\): \\[\n\\text{if } \\text{dist}[u] + w &lt; \\text{dist}[v], \\text{ then update } \\text{dist}[v]\n\\] Place \\(v\\) in bucket B[(i + w) \\bmod (C+1)].\n\nMove i to next nonempty bucket (cyclic).\nStop when all buckets empty.\n\n\n\n\nStep\nBucket\nVertices\nAction\n\n\n\n\n0\n[0]\nstart vertex\nrelax edges\n\n\n1\n[1]\nnext layer\npropagate\n\n\n…\n…\n…\n…\n\n\n\nThis bucket-based relaxation ensures we always process vertices in increasing distance order, just like Dijkstra.\n\n\nTiny Code (Python Example)\nfrom collections import deque\n\ndef dial_algorithm(V, edges, src, C):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n\n    INF = float('inf')\n    dist = [INF] * V\n    dist[src] = 0\n\n    buckets = [deque() for _ in range(C + 1)]\n    buckets[0].append(src)\n\n    idx = 0\n    processed = 0\n    while processed &lt; V:\n        while not buckets[idx % (C + 1)]:\n            idx += 1\n        dq = buckets[idx % (C + 1)]\n        u = dq.popleft()\n        processed += 1\n\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                old = dist[v]\n                dist[v] = dist[u] + w\n                new_idx = dist[v] % (C + 1)\n                buckets[new_idx].append(v)\n    return dist\n\nedges = [\n    (0, 1, 2),\n    (0, 2, 1),\n    (1, 3, 3),\n    (2, 3, 1)\n]\nprint(dial_algorithm(4, edges, 0, 3))\nOutput:\n[0, 2, 1, 2]\n\n\nWhy It Matters\n\nReplaces heap with fixed-size array of queues\nFaster for small integer weights\nLinear-time behavior when \\(C\\) is constant\nSimpler than Fibonacci heaps, but often as effective in practice\n\n\n\nA Gentle Proof (Why It Works)\nAll edge weights are nonnegative integers bounded by \\(C\\). Each relaxation increases distance by at most \\(C\\), so we only need \\(C+1\\) buckets to track possible remainders modulo \\(C+1\\).\nBecause each bucket is processed in cyclic order and vertices are only revisited when their distance decreases, the algorithm maintains nondecreasing distance order, ensuring correctness equivalent to Dijkstra.\n\n\nTry It Yourself\n\nRun on graphs with small integer weights (0–5).\nCompare runtime vs binary heap Dijkstra.\nTry \\(C=1\\) → becomes 0–1 BFS.\nTest \\(C=10\\) → more buckets, but still fast.\nPlot number of relaxations per bucket.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges\nMax Weight \\(C\\)\nDistances\n\n\n\n\nSimple\n0→1(2), 0→2(1), 2→3(1), 1→3(3)\n3\n[0, 2, 1, 2]\n\n\nUniform\n0→1(1), 1→2(1), 2→3(1)\n1\n[0, 1, 2, 3]\n\n\nZero edges\n0→1(0), 1→2(0)\n1\n[0, 0, 0]\n\n\n\n\n\nComplexity\n\nTime: \\(O(V + E + C)\\)\nSpace: \\(O(V + C)\\)\nCondition: all edge weights \\(\\in [0, C]\\)\n\nDial’s Algorithm is the bucketed Dijkstra, it walks through distances layer by layer, storing each vertex in the slot that fits its cost, never needing a heap to know who’s next.\n\n\n\n330 Multi-Source Dijkstra\nMulti-Source Dijkstra is a variant of Dijkstra’s algorithm designed to find the shortest distance from multiple starting vertices to all others in a weighted graph. Instead of running Dijkstra repeatedly, we initialize the priority queue with all sources at distance 0, and let the algorithm propagate the minimum distances simultaneously.\nIt’s a powerful technique when you have several origins (cities, servers, entry points) and want the nearest path from any of them.\n\nWhat Problem Are We Solving?\nGiven a weighted graph \\(G = (V, E, w)\\) and a set of source vertices \\(S = {s_1, s_2, \\dots, s_k}\\), we want to compute:\n\\[\n\\text{dist}[v] = \\min_{s_i \\in S} \\text{dist}(s_i, v)\n\\]\nIn other words, the distance to the closest source.\nTypical use cases:\n\nMulti-depot routing (shortest route from any facility)\nNearest service center (hospital, server, store)\nMulti-seed propagation (fire spread, BFS-like effects)\nVoronoi partitions in graphs\n\n\n\nHow Does It Work (Plain Language)?\nThe logic is simple:\n\nStart with all sources in the priority queue, each at distance 0.\nPerform Dijkstra’s algorithm as usual.\nWhenever a vertex is relaxed, the source that reached it first determines its distance.\n\nBecause we process vertices in increasing distance order, every vertex’s distance reflects the nearest source.\n\n\n\nStep\nAction\n\n\n\n\nInitialize dist[v] = ∞\n\n\n\nFor each source s ∈ S: dist[s] = 0\n\n\n\nPush all s into priority queue\n\n\n\nRun standard Dijkstra\n\n\n\n\n\n\nTiny Code (Python Example)\nimport heapq\n\ndef multi_source_dijkstra(V, edges, sources):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))  # for undirected graph\n\n    INF = float('inf')\n    dist = [INF] * V\n    pq = []\n\n    for s in sources:\n        dist[s] = 0\n        heapq.heappush(pq, (0, s))\n\n    while pq:\n        d, u = heapq.heappop(pq)\n        if d &gt; dist[u]:\n            continue\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                heapq.heappush(pq, (dist[v], v))\n    return dist\n\nedges = [\n    (0, 1, 2),\n    (1, 2, 3),\n    (0, 3, 1),\n    (3, 4, 4),\n    (2, 4, 2)\n]\n\nsources = [0, 4]\nprint(multi_source_dijkstra(5, edges, sources))\nOutput:\n[0, 2, 3, 1, 0]\nEach vertex now knows its shortest distance from either source 0 or 4.\n\n\nWhy It Matters\n\nEfficient for multiple origins (no need for \\(k\\) separate Dijkstra runs)\nGreat for nearest-neighbor labeling or multi-region BFS\nWorks on weighted graphs, unlike basic multi-source BFS\nA building block for graph Voronoi diagrams\n\n\n\nA Gentle Proof (Why It Works)\nDijkstra’s algorithm ensures vertices are processed in nondecreasing distance order. By initializing all sources with distance 0, we treat them as a super-source connected by 0-weight edges:\n\\[\nS^* \\to s_i, \\quad w(S^*, s_i) = 0\n\\]\nThus, multi-source Dijkstra is equivalent to a single-source Dijkstra from a virtual node connected to all sources, which guarantees correctness.\n\n\nTry It Yourself\n\nAdd multiple sources to a city map graph.\nObserve which source each node connects to first.\nCompare results with \\(k\\) separate Dijkstra runs.\nModify to also store source label (for Voronoi assignment).\nTry on a grid where certain cells are starting fires or signals.\n\n\n\nTest Cases\n\n\n\nGraph\nSources\nResult\n\n\n\n\nLine graph 0–4\n[0, 4]\n[0, 1, 2, 1, 0]\n\n\nTriangle 0–1–2\n[0, 2]\n[0, 1, 0]\n\n\nGrid\nCorners\nMinimum steps from corners\n\n\n\n\n\nComplexity\n\nTime: \\(O((V + E) \\log V)\\)\nSpace: \\(O(V + E)\\)\nCondition: Nonnegative weights\n\nMulti-Source Dijkstra is the chorus of shortest paths, all sources sing together, and every vertex listens to the closest voice.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-34.-shortest-path-variants",
    "href": "books/en-us/list-4.html#section-34.-shortest-path-variants",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 34. Shortest Path Variants",
    "text": "Section 34. Shortest Path Variants\n\n331 0–1 BFS\nThe 0–1 BFS algorithm is a specialized shortest path technique for graphs whose edge weights are only 0 or 1. It’s a streamlined version of Dijkstra’s algorithm that replaces the priority queue with a deque (double-ended queue), taking advantage of the fact that only two possible edge weights exist. This allows computing all shortest paths in linear time:\n\\[\nO(V + E)\n\\]\n\nWhat Problem Are We Solving?\nWe want to find the shortest path from a source vertex \\(s\\) to all other vertices in a graph where\n\\[\nw(u, v) \\in {0, 1}\n\\]\nStandard Dijkstra’s algorithm works, but maintaining a heap is unnecessary overhead when edge weights are so simple. The insight: edges with weight 0 do not increase distance, so they should be explored immediately, while edges with weight 1 should be explored next.\n\n\nHow Does It Work (Plain Language)?\nInstead of a heap, we use a deque to manage vertices by their current shortest distance.\n\nInitialize all distances to \\(\\infty\\), except \\(\\text{dist}[s] = 0\\).\nPush \\(s\\) into the deque.\nWhile the deque is not empty:\n\nPop vertex \\(u\\) from the front.\nFor each edge \\((u, v)\\) with weight \\(w\\):\n\nIf \\(\\text{dist}[u] + w &lt; \\text{dist}[v]\\), update \\(\\text{dist}[v]\\).\nIf \\(w = 0\\), push \\(v\\) to the front (no distance increase).\nIf \\(w = 1\\), push \\(v\\) to the back (distance +1).\n\n\n\nBecause all edge weights are 0 or 1, this preserves correct ordering without a heap.\n\n\n\nWeight\nAction\nReason\n\n\n\n\n0\npush front\nexplore immediately\n\n\n1\npush back\nexplore later\n\n\n\n\n\nTiny Code (Python Example)\nfrom collections import deque\n\ndef zero_one_bfs(V, edges, src):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        # for undirected graph, also add graph[v].append((u, w))\n\n    dist = [float('inf')] * V\n    dist[src] = 0\n    dq = deque([src])\n\n    while dq:\n        u = dq.popleft()\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                if w == 0:\n                    dq.appendleft(v)\n                else:\n                    dq.append(v)\n    return dist\n\nedges = [(0, 1, 0), (1, 2, 1), (0, 2, 1), (2, 3, 0)]\nprint(zero_one_bfs(4, edges, 0))\nOutput:\n[0, 0, 1, 1]\n\n\nWhy It Matters\n\nRuns in \\(O(V + E)\\), faster than Dijkstra’s \\(O(E \\log V)\\)\nSimplifies implementation when weights are 0 or 1\nWorks on directed or undirected graphs\nPerfect for problems like:\n\nMinimum number of flips/operations\nShortest path in binary grids\nBFS with special cost transitions\n\n\n\n\nA Gentle Proof (Why It Works)\nAt every step, the deque contains vertices ordered by nondecreasing distance. When an edge of weight 0 is relaxed, the neighbor’s distance equals \\(\\text{dist}[u]\\), so we process it immediately (push front). When an edge of weight 1 is relaxed, the neighbor’s distance increases by 1, so it goes to the back.\nThis maintains the same invariant as Dijkstra’s:\n\nEvery vertex is processed when its shortest distance is finalized.\n\nThus, correctness follows.\n\n\nTry It Yourself\n\nCompare with Dijkstra’s algorithm on graphs with 0–1 weights.\nCreate a grid where moving straight costs 0 and turning costs 1.\nModify to handle undirected edges.\nUse it for “minimum walls to break” problems.\nDraw the deque contents step-by-step to visualize progression.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nDistances\n\n\n\n\nSimple\n0→1(0), 1→2(1), 0→2(1), 2→3(0)\n[0, 0, 1, 1]\n\n\nAll 0\n0→1(0), 1→2(0), 2→3(0)\n[0, 0, 0, 0]\n\n\nMixed\n0→1(1), 1→2(0), 0→2(1)\n[0, 1, 1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(V + E)\\)\nSpace: \\(O(V + E)\\)\nCondition: \\(w(u, v) \\in {0, 1}\\)\n\n0–1 BFS is a binary Dijkstra, moving through zero-cost edges first and one-cost edges next, fast, simple, and perfectly ordered.\n\n\n\n332 Bidirectional Dijkstra\nBidirectional Dijkstra is an optimization of the classic Dijkstra’s algorithm for single-pair shortest paths. Instead of searching from just the source, we run two simultaneous Dijkstra searches, one forward from the source and one backward from the target, and stop when they meet.\nThis dramatically reduces the explored search space, especially in sparse or road-network-like graphs.\n\nWhat Problem Are We Solving?\nWe want the shortest path between two specific vertices, \\(s\\) (source) and \\(t\\) (target), in a graph with nonnegative weights. Standard Dijkstra explores the entire reachable graph, which is wasteful if we only need \\(s \\to t\\).\nBidirectional Dijkstra searches from both ends and meets in the middle:\n\\[\n\\text{dist}(s, t) = \\min_{v \\in V} \\left( \\text{dist}*\\text{fwd}[v] + \\text{dist}*\\text{bwd}[v] \\right)\n\\]\n\n\nHow Does It Work (Plain Language)?\nThe algorithm maintains two priority queues, one for the forward search (from \\(s\\)) and one for the backward search (from \\(t\\)). Each search relaxes edges as in standard Dijkstra, but they alternate steps until their frontiers intersect.\n\nInitialize all \\(\\text{dist}*\\text{fwd}\\) and \\(\\text{dist}*\\text{bwd}\\) to \\(\\infty\\).\nSet \\(\\text{dist}*\\text{fwd}[s] = 0\\), \\(\\text{dist}*\\text{bwd}[t] = 0\\).\nInsert \\(s\\) into the forward heap, \\(t\\) into the backward heap.\nAlternate expanding one step forward and one step backward.\nWhen a vertex \\(v\\) is visited by both searches, compute candidate path: \\[\n\\text{dist}(s, v) + \\text{dist}(t, v)\n\\]\nStop when both queues are empty or the current minimum key exceeds the best candidate path.\n\n\n\n\nDirection\nHeap\nDistance Array\n\n\n\n\nForward\nFrom source\n\\(\\text{dist}_\\text{fwd}\\)\n\n\nBackward\nFrom target\n\\(\\text{dist}_\\text{bwd}\\)\n\n\n\n\n\nTiny Code (Python Example)\nimport heapq\n\ndef bidirectional_dijkstra(V, edges, s, t):\n    graph = [[] for _ in range(V)]\n    rev_graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        rev_graph[v].append((u, w))  # reverse for backward search\n\n    INF = float('inf')\n    dist_f = [INF] * V\n    dist_b = [INF] * V\n    visited_f = [False] * V\n    visited_b = [False] * V\n\n    dist_f[s] = 0\n    dist_b[t] = 0\n    pq_f = [(0, s)]\n    pq_b = [(0, t)]\n    best = INF\n\n    while pq_f or pq_b:\n        if pq_f:\n            d, u = heapq.heappop(pq_f)\n            if d &gt; dist_f[u]:\n                continue\n            visited_f[u] = True\n            if visited_b[u]:\n                best = min(best, dist_f[u] + dist_b[u])\n            for v, w in graph[u]:\n                if dist_f[u] + w &lt; dist_f[v]:\n                    dist_f[v] = dist_f[u] + w\n                    heapq.heappush(pq_f, (dist_f[v], v))\n\n        if pq_b:\n            d, u = heapq.heappop(pq_b)\n            if d &gt; dist_b[u]:\n                continue\n            visited_b[u] = True\n            if visited_f[u]:\n                best = min(best, dist_f[u] + dist_b[u])\n            for v, w in rev_graph[u]:\n                if dist_b[u] + w &lt; dist_b[v]:\n                    dist_b[v] = dist_b[u] + w\n                    heapq.heappush(pq_b, (dist_b[v], v))\n\n        if best &lt; min(pq_f[0][0] if pq_f else INF, pq_b[0][0] if pq_b else INF):\n            break\n\n    return best if best != INF else None\n\nedges = [\n    (0, 1, 2),\n    (1, 2, 3),\n    (0, 3, 1),\n    (3, 4, 4),\n    (4, 2, 2)\n]\n\nprint(bidirectional_dijkstra(5, edges, 0, 2))\nOutput:\n5\n\n\nWhy It Matters\n\nHalf the work of standard Dijkstra on average\nBest suited for sparse, road-like networks\nGreat for navigation, routing, pathfinding\nFoundation for advanced methods like ALT and Contraction Hierarchies\n\n\n\nA Gentle Proof (Why It Works)\nDijkstra’s invariant: vertices are processed in nondecreasing distance order. By running two searches, we maintain this invariant in both directions. When a vertex is reached by both searches, any further expansion can only find paths longer than the current best:\n\\[\n\\text{dist}*\\text{fwd}[u] + \\text{dist}*\\text{bwd}[u] = \\text{candidate shortest path}\n\\]\nThus, the first intersection yields the optimal distance.\n\n\nTry It Yourself\n\nCompare explored nodes vs single Dijkstra.\nVisualize frontiers meeting in the middle.\nAdd a grid graph with uniform weights.\nCombine with heuristics → bidirectional A*.\nUse backward search on reverse edges.\n\n\n\nTest Cases\n\n\n\nGraph\nSource\nTarget\nResult\n\n\n\n\n\nLine\n0→1→2→3\n0\n3\n3\n\n\nTriangle\n0→1(2), 1→2(2), 0→2(5)\n0\n2\n4\n\n\nRoad\n0→1(1), 1→2(2), 0→3(3), 3→2(1)\n0\n2\n3\n\n\n\n\n\nComplexity\n\nTime: \\(O((V + E) \\log V)\\)\nSpace: \\(O(V + E)\\)\nCondition: Nonnegative weights\n\nBidirectional Dijkstra is a meeting-in-the-middle pathfinder, two explorers start from opposite ends, racing toward each other until they share the shortest route.\n\n\n\n333 A* with Euclidean Heuristic\nThe A* algorithm is a heuristic-guided shortest path search, blending Dijkstra’s rigor with informed direction. By introducing a heuristic function \\(h(v)\\) that estimates the remaining distance, it expands fewer nodes and focuses the search toward the goal. When using Euclidean distance as the heuristic, A* is perfect for spatial graphs, grids, maps, road networks.\n\nWhat Problem Are We Solving?\nWe want the shortest path from a source \\(s\\) to a target \\(t\\) in a weighted graph with nonnegative weights, but we also want to avoid exploring unnecessary regions.\nDijkstra expands all nodes in order of true cost \\(g(v)\\) (distance so far). A* expands nodes in order of estimated total cost:\n\\[\nf(v) = g(v) + h(v)\n\\]\nwhere\n\n\\(g(v)\\) = cost so far from \\(s\\) to \\(v\\),\n\\(h(v)\\) = heuristic estimate from \\(v\\) to \\(t\\).\n\nIf \\(h(v)\\) never overestimates the true cost, A* guarantees the optimal path.\n\n\nHow Does It Work (Plain Language)?\nThink of A* as Dijkstra with a compass. While Dijkstra explores all directions equally, A* uses \\(h(v)\\) to bias exploration toward the target.\n\nInitialize \\(\\text{dist}[s] = 0\\), \\(\\text{f}[s] = h(s)\\)\nPush \\((f(s), s)\\) into a priority queue\nWhile queue not empty:\n\nPop vertex \\(u\\) with smallest \\(f(u)\\)\nIf \\(u = t\\), stop, path found\nFor each neighbor \\((u, v)\\) with weight \\(w\\):\n\nCompute tentative cost \\(g' = \\text{dist}[u] + w\\)\nIf \\(g' &lt; \\text{dist}[v]\\): \\[\n\\text{dist}[v] = g', \\quad f(v) = g' + h(v)\n\\] Push \\((f(v), v)\\) into queue\n\n\n\nHeuristic types:\n\nEuclidean: \\(h(v) = \\sqrt{(x_v - x_t)^2 + (y_v - y_t)^2}\\)\nManhattan: \\(h(v) = |x_v - x_t| + |y_v - y_t|\\)\nZero: \\(h(v) = 0\\) → reduces to Dijkstra\n\n\n\nTiny Code (Python Example)\nimport heapq\nimport math\n\ndef a_star_euclidean(V, edges, coords, s, t):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))  # undirected\n\n    def h(v):\n        x1, y1 = coords[v]\n        x2, y2 = coords[t]\n        return math.sqrt((x1 - x2)2 + (y1 - y2)2)\n\n    dist = [float('inf')] * V\n    dist[s] = 0\n    pq = [(h(s), s)]\n\n    while pq:\n        f, u = heapq.heappop(pq)\n        if u == t:\n            return dist[u]\n        for v, w in graph[u]:\n            g_new = dist[u] + w\n            if g_new &lt; dist[v]:\n                dist[v] = g_new\n                heapq.heappush(pq, (g_new + h(v), v))\n    return None\n\ncoords = [(0,0), (1,0), (1,1), (2,1)]\nedges = [(0,1,1), (1,2,1), (0,2,2), (2,3,1)]\nprint(a_star_euclidean(4, edges, coords, 0, 3))\nOutput:\n3\n\n\nWhy It Matters\n\nOptimal if \\(h(v)\\) is admissible (\\(h(v) \\le\\) true distance)\nFast if \\(h(v)\\) is consistent (\\(h(u) \\le w(u,v) + h(v)\\))\nPerfect for spatial navigation and grid-based pathfinding\nUnderpins many AI systems: games, robots, GPS routing\n\n\n\nA Gentle Proof (Why It Works)\nA* ensures correctness through admissibility: \\[\nh(v) \\le \\text{dist}(v, t)\n\\]\nThis means \\(f(v) = g(v) + h(v)\\) never underestimates the total path cost, so the first time \\(t\\) is dequeued, the shortest path is found.\nConsistency ensures that \\(f(v)\\) values are nondecreasing, mimicking Dijkstra’s invariant.\nThus, A* retains Dijkstra’s guarantees while guiding exploration efficiently.\n\n\nTry It Yourself\n\nCompare explored nodes with Dijkstra’s algorithm.\nUse Euclidean and Manhattan heuristics on a grid.\nTry a bad heuristic (e.g. double true distance) → see failure.\nVisualize search frontier for each step.\nApply to a maze or road map.\n\n\n\nTest Cases\n\n\n\nGraph\nHeuristic\nResult\n\n\n\n\nGrid\nEuclidean\nStraight path\n\n\nTriangle\n\\(h=0\\)\nDijkstra\n\n\nOverestimate\n\\(h(v) &gt; \\text{dist}(v,t)\\)\nMay fail\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log V)\\)\nSpace: \\(O(V)\\)\nCondition: Nonnegative weights, admissible heuristic\n\nA* with Euclidean heuristic is the navigator’s Dijkstra, guided by distance, it finds the shortest route while knowing where it’s headed.\n\n\n\n334 ALT Algorithm (A* Landmarks + Triangle Inequality)\nThe ALT Algorithm enhances A* search with precomputed landmarks and the triangle inequality, giving it a strong, admissible heuristic that dramatically speeds up shortest path queries on large road networks.\nThe name “ALT” comes from A* (search), Landmarks, and Triangle inequality, a trio that balances preprocessing with query-time efficiency.\n\nWhat Problem Are We Solving?\nWe want to find shortest paths efficiently in large weighted graphs (like road maps), where a single-source search (like Dijkstra or A*) may explore millions of nodes.\nTo guide the search more effectively, we precompute distances from special nodes (landmarks) and use them to build tight heuristic bounds during A*.\nGiven nonnegative edge weights, we define a heuristic based on the triangle inequality:\n\\[\nd(a, b) \\le d(a, L) + d(L, b)\n\\]\nFrom this we derive a lower bound for \\(d(a, b)\\):\n\\[\nh(a) = \\max_{L \\in \\text{landmarks}} |d(L, t) - d(L, a)|\n\\]\nThis \\(h(a)\\) is admissible (never overestimates) and consistent (monotone).\n\n\nHow Does It Work (Plain Language)?\nALT augments A* with preprocessing and landmark distances:\nPreprocessing:\n\nChoose a small set of landmarks \\(L_1, L_2, \\dots, L_k\\) (spread across the graph).\nRun Dijkstra (or BFS) from each landmark to compute distances to all nodes: \\[\nd(L_i, v) \\text{ and } d(v, L_i)\n\\]\n\nQuery phase:\n\nFor a query \\((s, t)\\), compute: \\[\nh(v) = \\max_{i=1}^k |d(L_i, t) - d(L_i, v)|\n\\]\nRun A* with this heuristic: \\[\nf(v) = g(v) + h(v)\n\\]\nGuaranteed optimal path (admissible and consistent).\n\n\n\n\n\n\n\n\n\nPhase\nTask\nCost\n\n\n\n\nPreprocessing\nMulti-source Dijkstra from landmarks\n\\(O(k(V+E)\\log V)\\)\n\n\nQuery\nA* with landmark heuristic\nFast (\\(O(E'\\log V)\\))\n\n\n\n\n\nTiny Code (Python Example)\nimport heapq\n\ndef dijkstra(V, graph, src):\n    dist = [float('inf')] * V\n    dist[src] = 0\n    pq = [(0, src)]\n    while pq:\n        d, u = heapq.heappop(pq)\n        if d &gt; dist[u]:\n            continue\n        for v, w in graph[u]:\n            if d + w &lt; dist[v]:\n                dist[v] = d + w\n                heapq.heappush(pq, (dist[v], v))\n    return dist\n\ndef alt_search(V, graph, landmarks, d_landmark_to, s, t):\n    def h(v):\n        # max of triangle inequality heuristic\n        return max(abs(d_landmark_to[L][t] - d_landmark_to[L][v]) for L in landmarks)\n\n    dist = [float('inf')] * V\n    dist[s] = 0\n    pq = [(h(s), s)]\n\n    while pq:\n        f, u = heapq.heappop(pq)\n        if u == t:\n            return dist[u]\n        for v, w in graph[u]:\n            g_new = dist[u] + w\n            if g_new &lt; dist[v]:\n                dist[v] = g_new\n                heapq.heappush(pq, (g_new + h(v), v))\n    return None\n\n# Example usage\nV = 5\ngraph = [\n    [(1, 2), (2, 4)],\n    [(2, 1), (3, 7)],\n    [(3, 3)],\n    [(4, 1)],\n    []\n]\n\nlandmarks = [0, 4]\nd_landmark_to = [dijkstra(V, graph, L) for L in landmarks]\n\nprint(alt_search(V, graph, landmarks, d_landmark_to, 0, 4))\nOutput:\n11\n\n\nWhy It Matters\n\nFar fewer node expansions than Dijkstra or vanilla A*\nAdmissible (never overestimates) and consistent\nEspecially effective on road networks, navigation systems, GIS, and logistics routing\nPreprocessing is offline, query is real-time fast\n\n\n\nA Gentle Proof (Why It Works)\nFor any nodes \\(a, b\\), and landmark \\(L\\):\n\\[\n|d(L, b) - d(L, a)| \\le d(a, b)\n\\]\nBy taking the maximum over all chosen landmarks:\n\\[\nh(a) = \\max_L |d(L, t) - d(L, a)| \\le d(a, t)\n\\]\nTherefore \\(h(a)\\) is admissible, and since triangle inequality is symmetric, it is consistent:\n\\[\nh(a) \\le w(a, b) + h(b)\n\\]\nThus, A* with \\(h(a)\\) preserves optimality.\n\n\nTry It Yourself\n\nChoose 2–3 landmarks spread across your graph.\nCompare A* expansions with and without ALT heuristic.\nVisualize heuristic contours around landmarks.\nUse in city maps to speed up routing queries.\nExperiment with random vs central landmarks.\n\n\n\nTest Cases\n\n\n\nGraph\nLandmarks\nResult\n\n\n\n\nChain\n[first, last]\nExact heuristic\n\n\nGrid\n4 corners\nSmooth guidance\n\n\nRandom\nrandom nodes\nVarying performance\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(k(V + E) \\log V)\\)\nQuery: \\(O(E' \\log V)\\) (small subset)\nSpace: \\(O(kV)\\)\nCondition: Nonnegative weights\n\nThe ALT Algorithm is A* on steroids, guided by precomputed wisdom (landmarks), it leaps across the graph using geometry, not guesswork.\n\n\n\n335 Contraction Hierarchies\nContraction Hierarchies (CH) is a powerful speedup technique for shortest path queries on large, static road networks. It preprocesses the graph by adding shortcuts and ordering vertices by importance, enabling queries that run orders of magnitude faster than plain Dijkstra.\nIt’s the backbone of many modern routing engines (like OSRM, GraphHopper, Valhalla) used in GPS systems.\n\nWhat Problem Are We Solving?\nWe want to answer many shortest path queries quickly on a large, unchanging graph (like a road map). Running Dijkstra or even A* for each query is too slow.\nContraction Hierarchies solves this by:\n\nPreprocessing once to create a hierarchy of nodes.\nAnswering each query with a much smaller search (bidirectional).\n\nTradeoff: expensive preprocessing, blazing-fast queries.\n\n\nHow Does It Work (Plain Language)?\nContraction Hierarchies is a two-phase algorithm:\n\n1. Preprocessing Phase (Build Hierarchy)\nWe order nodes by importance (e.g., degree, centrality, traffic volume). Then we contract them one by one, adding shortcut edges so that shortest path distances remain correct.\nFor each node \\(v\\) being removed:\n\nFor each pair of neighbors \\((u, w)\\):\n\nIf the shortest path \\(u \\to v \\to w\\) is the only shortest path between \\(u\\) and \\(w\\), add a shortcut \\((u, w)\\) with weight \\(w(u, v) + w(v, w)\\).\n\n\nWe record the order in which nodes are contracted.\n\n\n\nStep\nAction\nResult\n\n\n\n\nPick node \\(v\\)\nContract\nAdd shortcuts\n\n\nContinue\nUntil all nodes\nBuild hierarchy\n\n\n\nThe graph becomes layered: low-importance nodes contracted first, high-importance last.\n\n\n2. Query Phase (Up-Down Search)\nGiven a query \\((s, t)\\):\n\nRun bidirectional Dijkstra, but only “upward” along higher-ranked nodes.\nStop when both searches meet.\n\nThis Up-Down constraint keeps the search small, only a tiny fraction of the graph is explored.\n\n\n\nDirection\nConstraint\n\n\n\n\nForward\nVisit only higher-ranked nodes\n\n\nBackward\nVisit only higher-ranked nodes\n\n\n\nThe shortest path is the lowest point where the two searches meet.\n\n\n\nTiny Code (Python Example, Conceptual)\nimport heapq\n\ndef add_shortcuts(graph, order):\n    V = len(graph)\n    shortcuts = [[] for _ in range(V)]\n    for v in order:\n        neighbors = graph[v]\n        for u, wu in neighbors:\n            for w, ww in neighbors:\n                if u == w:\n                    continue\n                new_dist = wu + ww\n                # If no shorter path exists between u and w, add shortcut\n                exists = any(n == w and cost &lt;= new_dist for n, cost in graph[u])\n                if not exists:\n                    shortcuts[u].append((w, new_dist))\n        # Remove v (contract)\n        graph[v] = []\n    return [graph[i] + shortcuts[i] for i in range(V)]\n\ndef upward_edges(graph, order):\n    rank = {v: i for i, v in enumerate(order)}\n    up = [[] for _ in range(len(graph))]\n    for u in range(len(graph)):\n        for v, w in graph[u]:\n            if rank[v] &gt; rank[u]:\n                up[u].append((v, w))\n    return up\n\ndef ch_query(up_graph, s, t):\n    def dijkstra_dir(start):\n        dist = {}\n        pq = [(0, start)]\n        while pq:\n            d, u = heapq.heappop(pq)\n            if u in dist:\n                continue\n            dist[u] = d\n            for v, w in up_graph[u]:\n                heapq.heappush(pq, (d + w, v))\n        return dist\n\n    dist_s = dijkstra_dir(s)\n    dist_t = dijkstra_dir(t)\n    best = float('inf')\n    for v in dist_s:\n        if v in dist_t:\n            best = min(best, dist_s[v] + dist_t[v])\n    return best\n\n# Example graph\ngraph = [\n    [(1, 2), (2, 4)],\n    [(2, 1), (3, 7)],\n    [(3, 3)],\n    []\n]\norder = [0, 1, 2, 3]  # Simplified\nup_graph = upward_edges(add_shortcuts(graph, order), order)\nprint(ch_query(up_graph, 0, 3))\nOutput:\n8\n\n\nWhy It Matters\n\nQuery speed: microseconds, even on million-node graphs\nUsed in GPS navigation, road routing, logistics planning\nPreprocessing preserves correctness via shortcuts\nEasily combined with ALT, A*, and Multi-level Dijkstra\n\n\n\nA Gentle Proof (Why It Works)\nWhen contracting node \\(v\\), we add shortcuts to preserve all shortest paths that pass through \\(v\\). Thus, removing \\(v\\) never breaks shortest-path correctness.\nDuring query:\n\nWe only travel upward in rank.\nSince all paths can be expressed as up-down-up, the meeting point of the forward and backward searches must lie on the true shortest path.\n\nBy limiting exploration to “upward” edges, CH keeps searches small without losing completeness.\n\n\nTry It Yourself\n\nBuild a small graph, choose an order, and add shortcuts.\nCompare Dijkstra vs CH query times.\nVisualize hierarchy (contracted vs remaining).\nExperiment with random vs heuristic node orderings.\nAdd landmarks (ALT+CH) for further optimization.\n\n\n\nTest Cases\n\n\n\nGraph\nNodes\nQuery\nResult\n\n\n\n\nChain\n0–1–2–3\n0→3\n3 edges\n\n\nTriangle\n0–1–2\n0→2\nShortcut 0–2 added\n\n\nGrid\n3×3\nCorner→Corner\nShortcuts reduce hops\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(V \\log V + E)\\) (with heuristic ordering)\nQuery: \\(O(\\log V)\\) (tiny search space)\nSpace: \\(O(V + E + \\text{shortcuts})\\)\nCondition: Static graph, nonnegative weights\n\nContraction Hierarchies is the architect’s Dijkstra, it reshapes the city first, then navigates with near-instant precision.\n\n\n\n336 CH Query Algorithm (Shortcut-Based Routing)\nThe CH Query Algorithm is the online phase of Contraction Hierarchies (CH). Once preprocessing builds the shortcut-augmented hierarchy, queries can be answered in microseconds by performing an upward bidirectional Dijkstra, searching only along edges that lead to more important (higher-ranked) nodes.\nIt’s the practical magic behind instantaneous route planning in navigation systems.\n\nWhat Problem Are We Solving?\nGiven a contracted graph (with shortcuts) and a node ranking, we want to compute the shortest distance between two vertices \\(s\\) and \\(t\\) — without exploring the full graph.\nRather than scanning every node, CH Query:\n\nSearches upward from both \\(s\\) and \\(t\\) (following rank order).\nStops when both searches meet.\nThe meeting point with the smallest sum of forward + backward distances gives the shortest path.\n\n\n\nHow Does It Work (Plain Language)?\nAfter preprocessing (which contracts nodes and inserts shortcuts), the query algorithm runs two simultaneous upward Dijkstra searches:\n\nInitialize two heaps:\n\nForward from \\(s\\)\nBackward from \\(t\\)\n\nRelax only upward edges, from lower-rank nodes to higher-rank ones. (Edge \\((u, v)\\) is upward if \\(\\text{rank}(v) &gt; \\text{rank}(u)\\).)\nWhenever a node \\(v\\) is settled by both searches, compute potential path: \\[\nd = \\text{dist}_f[v] + \\text{dist}_b[v]\n\\]\nTrack the minimum such \\(d\\).\nStop when current best \\(d\\) is smaller than remaining unvisited frontier keys.\n\n\n\n\nPhase\nDescription\n\n\n\n\nForward Search\nUpward edges from \\(s\\)\n\n\nBackward Search\nUpward edges from \\(t\\) (in reversed graph)\n\n\nMeet in Middle\nCombine distances at intersection\n\n\n\n\n\nTiny Code (Python Example, Conceptual)\nimport heapq\n\ndef ch_query(up_graph, rank, s, t):\n    INF = float('inf')\n    n = len(up_graph)\n    dist_f = [INF] * n\n    dist_b = [INF] * n\n    visited_f = [False] * n\n    visited_b = [False] * n\n\n    dist_f[s] = 0\n    dist_b[t] = 0\n    pq_f = [(0, s)]\n    pq_b = [(0, t)]\n    best = INF\n\n    while pq_f or pq_b:\n        # Forward direction\n        if pq_f:\n            d, u = heapq.heappop(pq_f)\n            if d &gt; dist_f[u]:\n                continue\n            visited_f[u] = True\n            if visited_b[u]:\n                best = min(best, dist_f[u] + dist_b[u])\n            for v, w in up_graph[u]:\n                if rank[v] &gt; rank[u] and dist_f[u] + w &lt; dist_f[v]:\n                    dist_f[v] = dist_f[u] + w\n                    heapq.heappush(pq_f, (dist_f[v], v))\n\n        # Backward direction\n        if pq_b:\n            d, u = heapq.heappop(pq_b)\n            if d &gt; dist_b[u]:\n                continue\n            visited_b[u] = True\n            if visited_f[u]:\n                best = min(best, dist_f[u] + dist_b[u])\n            for v, w in up_graph[u]:\n                if rank[v] &gt; rank[u] and dist_b[u] + w &lt; dist_b[v]:\n                    dist_b[v] = dist_b[u] + w\n                    heapq.heappush(pq_b, (dist_b[v], v))\n\n        # Early stop condition\n        min_frontier = min(\n            pq_f[0][0] if pq_f else INF,\n            pq_b[0][0] if pq_b else INF\n        )\n        if min_frontier &gt;= best:\n            break\n\n    return best if best != INF else None\n\n# Example: upward graph with rank\nup_graph = [\n    [(1, 2), (2, 4)],  # 0\n    [(3, 7)],          # 1\n    [(3, 3)],          # 2\n    []                 # 3\n]\nrank = [0, 1, 2, 3]\nprint(ch_query(up_graph, rank, 0, 3))\nOutput:\n8\n\n\nWhy It Matters\n\nUltra-fast queries, typically microseconds on large graphs\nNo heuristics, 100% exact shortest paths\nUsed in real-time GPS navigation, logistics optimization, map routing APIs\nCan be combined with ALT, turn penalties, or time-dependent weights\n\n\n\nA Gentle Proof (Why It Works)\nDuring preprocessing, every node \\(v\\) is contracted with shortcuts preserving all shortest paths. Each valid shortest path can be represented as an up–down path:\n\nascending ranks (up), then descending ranks (down).\n\nBy running upward-only searches from both \\(s\\) and \\(t\\), we explore the up segments of all such paths. The first meeting point \\(v\\) with \\[\n\\text{dist}_f[v] + \\text{dist}_b[v]\n\\] minimal corresponds to the optimal path.\n\n\nTry It Yourself\n\nVisualize ranks and edges before and after contraction.\nCompare number of visited nodes with full Dijkstra.\nCombine with landmarks (ALT) for even faster queries.\nMeasure query times on grid vs road-like graphs.\nAdd path reconstruction by storing parent pointers.\n\n\n\nTest Cases\n\n\n\nGraph\nQuery (s,t)\nResult\nNodes Visited\n\n\n\n\n0–1–2–3\n(0,3)\n3 edges\n4 (Dijkstra: 4)\n\n\n0–1–2–3–4 (line)\n(0,4)\npath=4\n5 (Dijkstra: 5)\n\n\nGrid (5×5)\n(corner, corner)\ncorrect\n~20x fewer\n\n\n\n\n\nComplexity\n\nPreprocessing: handled by CH build (\\(O(V \\log V + E)\\))\nQuery: \\(O(\\log V)\\) (tiny frontier)\nSpace: \\(O(V + E + \\text{shortcuts})\\)\nCondition: Nonnegative weights, static graph\n\nThe CH Query Algorithm is the express lane of graph search — prebuilt shortcuts and a smart hierarchy let it fly from source to target in a fraction of the time.\n\n\n\n337 Bellman–Ford Queue Variant (Early Termination SPFA)\nThe Bellman–Ford Queue Variant, commonly known as SPFA (Shortest Path Faster Algorithm), improves upon the standard Bellman–Ford by using a queue to relax only active vertices, those whose distances were updated. In practice, it’s often much faster, though in the worst case it still runs in \\(O(VE)\\).\nIt’s a clever hybrid: Bellman–Ford’s correctness, Dijkstra’s selectivity.\n\nWhat Problem Are We Solving?\nWe want to compute shortest paths from a single source \\(s\\) in a weighted graph that may include negative edges (but no negative cycles):\n\\[\nw(u, v) \\in \\mathbb{R}, \\quad \\text{and no negative cycles}\n\\]\nThe classic Bellman–Ford relaxes all edges \\(V-1\\) times — too slow when most nodes don’t change often.\nSPFA optimizes by:\n\nTracking which nodes were updated last round,\nPushing them into a queue,\nRelaxing only their outgoing edges.\n\n\n\nHow Does It Work (Plain Language)?\nThe algorithm uses a queue of “active” nodes. When a node’s distance improves, we enqueue it (if not already in the queue). Each iteration pulls one node, relaxes its neighbors, and enqueues those whose distances improve.\nIt’s like a wavefront of updates spreading only where needed.\n\n\n\n\n\n\n\nStep\nAction\n\n\n\n\n1\nInitialize \\(\\text{dist}[s] = 0\\), others \\(\\infty\\)\n\n\n2\nPush \\(s\\) into queue\n\n\n3\nWhile queue not empty:\n\n\n\nPop \\(u\\), relax \\((u, v)\\)\n\n\n\nIf \\(\\text{dist}[u] + w(u,v) &lt; \\text{dist}[v]\\), update and push \\(v\\)\n\n\n4\nOptional: detect negative cycles\n\n\n\nWhen every node leaves the queue with no new updates, we’re done, early termination!\n\n\nTiny Code (C Example)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAXV 1000\n#define INF 1000000000\n\ntypedef struct { int v, w; } Edge;\nEdge graph[MAXV][MAXV];\nint deg[MAXV], dist[MAXV];\nbool in_queue[MAXV];\nint queue[MAXV*10], front, back;\n\nvoid spfa(int V, int s) {\n    for (int i = 0; i &lt; V; i++) dist[i] = INF, in_queue[i] = false;\n    front = back = 0;\n    dist[s] = 0;\n    queue[back++] = s;\n    in_queue[s] = true;\n\n    while (front != back) {\n        int u = queue[front++];\n        if (front == MAXV*10) front = 0;\n        in_queue[u] = false;\n\n        for (int i = 0; i &lt; deg[u]; i++) {\n            int v = graph[u][i].v, w = graph[u][i].w;\n            if (dist[u] + w &lt; dist[v]) {\n                dist[v] = dist[u] + w;\n                if (!in_queue[v]) {\n                    queue[back++] = v;\n                    if (back == MAXV*10) back = 0;\n                    in_queue[v] = true;\n                }\n            }\n        }\n    }\n}\nPython (Simplified)\nfrom collections import deque\n\ndef spfa(V, edges, src):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n\n    dist = [float('inf')] * V\n    inq = [False] * V\n    dist[src] = 0\n\n    dq = deque([src])\n    inq[src] = True\n\n    while dq:\n        u = dq.popleft()\n        inq[u] = False\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                if not inq[v]:\n                    dq.append(v)\n                    inq[v] = True\n    return dist\n\n\nWhy It Matters\n\nOften faster than Bellman–Ford on sparse graphs\nEarly termination when no further updates\nHandles negative edges safely\nBasis for Min-Cost Max-Flow (SPFA version)\nPopular in competitive programming and network routing\n\n\n\nA Gentle Proof (Why It Works)\nBellman–Ford relies on edge relaxation propagating shortest paths through \\(V-1\\) iterations.\nSPFA dynamically schedules relaxations:\n\nEach vertex enters the queue only when \\(\\text{dist}\\) improves.\nSince each relaxation respects edge constraints, shortest paths converge after at most \\(V-1\\) relaxations per vertex.\n\nThus, SPFA preserves correctness and can terminate earlier when convergence is reached.\nWorst case (e.g. negative-weight grid), each vertex relaxes \\(O(V)\\) times, so time is \\(O(VE)\\). But in practice, it’s near \\(O(E)\\).\n\n\nTry It Yourself\n\nRun on graphs with negative weights but no cycles.\nCompare steps with Bellman–Ford, fewer relaxations.\nAdd a counter per node to detect negative cycles.\nUse as core for Min-Cost Max-Flow.\nTest dense vs sparse graphs, watch runtime difference.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nResult\n\n\n\n\n0→1(2), 1→2(-1), 0→2(4)\nno neg cycle\n[0, 2, 1]\n\n\n0→1(1), 1→2(2), 2→0(-4)\nneg cycle\ndetect\n\n\n0→1(5), 0→2(2), 2→1(-3)\nno cycle\n[0, -1, 2]\n\n\n\n\n\nComplexity\n\nTime: \\(O(VE)\\) worst, often \\(O(E)\\) average\nSpace: \\(O(V + E)\\)\nCondition: Negative edges allowed, no negative cycles\n\nThe Bellman–Ford Queue Variant is the smart scheduler — instead of looping blindly, it listens for updates and moves only where change happens.\n\n\n\n338 Dijkstra with Early Stop\nDijkstra with Early Stop is a target-aware optimization of the classic Dijkstra’s algorithm. It leverages the fact that Dijkstra processes vertices in nondecreasing order of distance, so as soon as the target node is extracted from the priority queue, its shortest distance is final and the search can safely terminate.\nThis simple tweak can cut runtime dramatically for single-pair shortest path queries.\n\nWhat Problem Are We Solving?\nIn standard Dijkstra’s algorithm, the search continues until all reachable vertices have been settled, even if we only care about one destination \\(t\\). That’s wasteful for point-to-point routing, where we only need \\(\\text{dist}(s, t)\\).\nThe Early Stop version stops immediately when \\(t\\) is extracted from the priority queue:\n\\[\n\\text{dist}(t) = \\text{final shortest distance}\n\\]\n\n\nHow Does It Work (Plain Language)?\nSame setup as Dijkstra, but with an exit condition:\n\nInitialize \\(\\text{dist}[s] = 0\\), all others \\(\\infty\\).\nPush \\((0, s)\\) into priority queue.\nWhile queue not empty:\n\nPop \\((d, u)\\) with smallest tentative distance.\nIf \\(u = t\\): stop, we found the shortest path.\nFor each neighbor \\((v, w)\\):\n\nIf \\(\\text{dist}[u] + w &lt; \\text{dist}[v]\\), relax and push.\n\n\n\nBecause Dijkstra’s invariant guarantees we always pop nodes in order of increasing distance, the first time we pop \\(t\\), we have found its true shortest distance.\n\n\n\nStep\nAction\n\n\n\n\nExtract min node \\(u\\)\nExpand neighbors\n\n\nIf \\(u = t\\)\nStop immediately\n\n\nElse\nContinue\n\n\n\nNo need to explore the entire graph.\n\n\nTiny Code (Python Example)\nimport heapq\n\ndef dijkstra_early_stop(V, edges, s, t):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))  # undirected\n\n    INF = float('inf')\n    dist = [INF] * V\n    dist[s] = 0\n    pq = [(0, s)]\n\n    while pq:\n        d, u = heapq.heappop(pq)\n        if d &gt; dist[u]:\n            continue\n        if u == t:\n            return dist[t]\n        for v, w in graph[u]:\n            if dist[u] + w &lt; dist[v]:\n                dist[v] = dist[u] + w\n                heapq.heappush(pq, (dist[v], v))\n    return None  # unreachable\nExample:\nedges = [\n    (0, 1, 2),\n    (1, 2, 3),\n    (0, 3, 1),\n    (3, 4, 4),\n    (4, 2, 2)\n]\nprint(dijkstra_early_stop(5, edges, 0, 2))\nOutput:\n5\n\n\nWhy It Matters\n\nEarly termination = fewer expansions = faster queries\nIdeal for point-to-point routing\nCombines well with A* (guided early stop) and ALT\nSimple to implement (just one if condition)\n\nUsed in:\n\nGPS navigation (city-to-city routes)\nNetwork routing (specific endpoints)\nGame AI pathfinding\n\n\n\nA Gentle Proof (Why It Works)\nDijkstra’s correctness relies on the fact that when a node is extracted from the heap, its shortest distance is final (no smaller distance can appear later).\nThus, when \\(t\\) is extracted: \\[\n\\text{dist}(t) = \\min_{u \\in V} \\text{dist}(u)\n\\] and any other unvisited vertex has \\(\\text{dist}[v] \\ge \\text{dist}[t]\\).\nTherefore, stopping immediately when \\(u = t\\) preserves correctness.\n\n\nTry It Yourself\n\nCompare runtime vs full Dijkstra on large graphs.\nVisualize heap operations with and without early stop.\nApply to grid graphs (start vs goal corner).\nCombine with A* to reduce visited nodes even more.\nTest on disconnected graphs (target unreachable).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nGraph\nSource\nTarget\nResult\nNotes\n\n\n\n\nChain 0–1–2–3\n0\n3\n3 edges\nStops at 3\n\n\nStar (0–others)\n0\n4\ndirect edge\n1 step\n\n\nGrid\n(0,0)→(n,n)\nEarly stop saves\nMany nodes skipped\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log V)\\) (best-case ≪ full graph)\nSpace: \\(O(V + E)\\)\nCondition: Nonnegative weights\n\nDijkstra with Early Stop is the sniper version — it locks onto the target and halts the moment the job’s done, saving every wasted move.\n\n\n\n339 Goal-Directed Search\nGoal-Directed Search is a general strategy for focusing graph exploration toward a specific target, rather than scanning the entire space. It modifies shortest-path algorithms (like BFS or Dijkstra) by biasing expansions in the direction of the goal using geometry, landmarks, or precomputed heuristics.\nWhen the bias is admissible (never overestimates cost), it still guarantees optimal paths while greatly reducing explored nodes.\n\nWhat Problem Are We Solving?\nIn standard BFS or Dijkstra, exploration radiates uniformly outward, even in directions that clearly don’t lead to the target. For large graphs (grids, road maps), that’s wasteful.\nWe need a way to steer the search toward the destination without losing correctness.\nFormally, for source \\(s\\) and target \\(t\\), we want to find\n\\[\n\\text{dist}(s, t) = \\min_{\\text{path } P: s \\to t} \\sum_{(u,v)\\in P} w(u,v)\n\\]\nwhile visiting as few nodes as possible.\n\n\nHow Does It Work (Plain Language)?\nGoal-directed search attaches a heuristic bias to each node’s priority in the queue, so nodes closer (or more promising) to the target are expanded earlier.\nTypical scoring rule:\n\\[\nf(v) = g(v) + \\lambda \\cdot h(v)\n\\]\nwhere\n\n\\(g(v)\\) = distance from \\(s\\) so far,\n\\(h(v)\\) = heuristic estimate from \\(v\\) to \\(t\\),\n\\(\\lambda\\) = bias factor (often \\(1\\) for A*, or \\(&lt;1\\) for partial guidance).\n\nAdmissible heuristics (\\(h(v) \\le \\text{true distance}(v,t)\\)) preserve optimality. Even simple directional heuristics, like Euclidean or Manhattan distance, can dramatically reduce search space.\n\n\n\n\n\n\n\n\n\n\n\n\nHeuristic Type\nDefinition\nUse Case\n\n\n\n\n\n\n\n\nEuclidean\n\\(\\sqrt{(x_v-x_t)^2+(y_v-y_t)^2}\\)\ngeometric graphs\n\n\n\n\n\n\nManhattan\n$\nx_v-x_t\n+\ny_v-y_t\n$\ngrid worlds\n\n\nLandmark (ALT)\n$\nd(L,t)-d(L,v)\n$\nroad networks\n\n\n\n\n\n\n\nTiny Code (Python Example)\nimport heapq\nimport math\n\ndef goal_directed_search(V, edges, coords, s, t):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))  # undirected\n\n    def heuristic(v):\n        x1, y1 = coords[v]\n        x2, y2 = coords[t]\n        return math.sqrt((x1 - x2)2 + (y1 - y2)2)\n\n    dist = [float('inf')] * V\n    dist[s] = 0\n    pq = [(heuristic(s), s)]\n\n    while pq:\n        f, u = heapq.heappop(pq)\n        if u == t:\n            return dist[u]\n        for v, w in graph[u]:\n            g_new = dist[u] + w\n            if g_new &lt; dist[v]:\n                dist[v] = g_new\n                f_v = g_new + heuristic(v)\n                heapq.heappush(pq, (f_v, v))\n    return None\nExample:\ncoords = [(0,0), (1,0), (1,1), (2,1)]\nedges = [(0,1,1), (1,2,1), (0,2,2), (2,3,1)]\nprint(goal_directed_search(4, edges, coords, 0, 3))\nOutput:\n3\n\n\nWhy It Matters\n\nFewer node expansions than unguided Dijkstra\nNaturally integrates with A*, ALT, CH, landmark heuristics\nApplicable to navigation, pathfinding, planning\nEasily adapted to grids, 3D spaces, or weighted networks\n\nUsed in:\n\nGPS navigation\nAI game agents\nRobotics (motion planning)\n\n\n\nA Gentle Proof (Why It Works)\nWhen \\(h(v)\\) is admissible, \\[\nh(v) \\le \\text{true distance}(v,t),\n\\] then \\(f(v) = g(v) + h(v)\\) never underestimates the cost of the optimal path through \\(v\\).\nTherefore, the first time the target \\(t\\) is popped from the queue, \\(\\text{dist}(t)\\) is guaranteed to be the true shortest distance.\nIf \\(h\\) is also consistent, then \\[\nh(u) \\le w(u,v) + h(v),\n\\] which ensures the priority order behaves like Dijkstra’s, preserving monotonicity.\n\n\nTry It Yourself\n\nRun with \\(h=0\\) → becomes normal Dijkstra.\nTry different \\(\\lambda\\):\n\n\\(\\lambda=1\\) → A*\n\\(\\lambda&lt;1\\) → softer guidance (more exploration).\n\nCompare expansions with plain Dijkstra on a grid.\nVisualize frontier growth, goal-directed forms a cone instead of a circle.\nTest non-admissible \\(h\\) (may speed up but lose optimality).\n\n\n\nTest Cases\n\n\n\nGraph\nHeuristic\nResult\nNodes Visited\n\n\n\n\n4-node line\nEuclidean\n3\nfewer\n\n\nGrid 5×5\nManhattan\nOptimal\n~½ nodes\n\n\nZero heuristic\n\\(h=0\\)\nDijkstra\nall nodes\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log V)\\) (fewer relaxations in practice)\nSpace: \\(O(V)\\)\nCondition: Nonnegative weights, admissible \\(h(v)\\)\n\nGoal-Directed Search is the compass-guided Dijkstra — it still guarantees the shortest route, but marches confidently toward the goal instead of wandering in every direction.\n\n\n\n340 Yen’s K Shortest Paths\nYen’s Algorithm finds not just the single shortest path, but the K shortest loopless paths between two nodes in a weighted directed graph. It’s a natural extension of Dijkstra’s, instead of stopping at the first solution, it systematically explores path deviations to discover the next-best routes in ascending order of total length.\nUsed widely in network routing, multi-route planning, and alternatives in navigation systems.\n\nWhat Problem Are We Solving?\nGiven a graph \\(G = (V, E)\\) with nonnegative edge weights, a source \\(s\\), and a target \\(t\\), we want to compute the first \\(K\\) distinct shortest paths:\n\\[\nP_1, P_2, \\dots, P_K\n\\]\nordered by total weight:\n\\[\n\\text{len}(P_1) \\le \\text{len}(P_2) \\le \\cdots \\le \\text{len}(P_K)\n\\]\nEach path must be simple (no repeated nodes).\n\n\nHow Does It Work (Plain Language)?\nYen’s Algorithm builds upon Dijkstra’s algorithm and the deviation path concept.\n\nCompute the shortest path \\(P_1\\) using Dijkstra.\nFor each \\(i = 2, \\dots, K\\):\n\nLet \\(P_{i-1}\\) be the previous path.\nFor each node (spur node) in \\(P_{i-1}\\):\n\nSplit the path into root path (prefix up to spur node).\nTemporarily remove:\n\nAny edge that would recreate a previously found path.\nAny node in the root path (except spur node) to prevent cycles.\n\nRun Dijkstra from spur node to \\(t\\).\nCombine root path + spur path → candidate path.\n\nAmong all candidates, choose the shortest one not yet selected.\nAdd it as \\(P_i\\).\n\n\nRepeat until \\(K\\) paths are found or no candidates remain.\n\n\n\nStep\nOperation\nPurpose\n\n\n\n\n1\n\\(P_1\\) = Dijkstra(s, t)\nbase path\n\n\n2\nDeviation from prefixes\nexplore alternatives\n\n\n3\nCollect candidates\nmin-heap\n\n\n4\nSelect shortest\nnext path\n\n\n\n\n\nTiny Code (Python Example)\nA simplified version for small graphs:\nimport heapq\n\ndef dijkstra(graph, s, t):\n    pq = [(0, s, [s])]\n    seen = set()\n    while pq:\n        d, u, path = heapq.heappop(pq)\n        if u == t:\n            return (d, path)\n        if u in seen: \n            continue\n        seen.add(u)\n        for v, w in graph[u]:\n            if v not in seen:\n                heapq.heappush(pq, (d + w, v, path + [v]))\n    return None\n\ndef yen_k_shortest_paths(graph, s, t, K):\n    A = []\n    B = []\n    first = dijkstra(graph, s, t)\n    if not first:\n        return A\n    A.append(first)\n    for k in range(1, K):\n        prev_path = A[k - 1][1]\n        for i in range(len(prev_path) - 1):\n            spur_node = prev_path[i]\n            root_path = prev_path[:i + 1]\n            removed_edges = []\n            # Temporarily remove edges\n            for d, p in A:\n                if p[:i + 1] == root_path and i + 1 &lt; len(p):\n                    u = p[i]\n                    v = p[i + 1]\n                    for e in graph[u]:\n                        if e[0] == v:\n                            graph[u].remove(e)\n                            removed_edges.append((u, e))\n                            break\n            spur = dijkstra(graph, spur_node, t)\n            if spur:\n                total_path = root_path[:-1] + spur[1]\n                total_cost = sum(graph[u][v][1] for u, v in zip(total_path, total_path[1:])) if False else spur[0] + sum(0 for _ in root_path)\n                if (total_cost, total_path) not in B:\n                    B.append((total_cost, total_path))\n            for u, e in removed_edges:\n                graph[u].append(e)\n        if not B:\n            break\n        B.sort(key=lambda x: x[0])\n        A.append(B.pop(0))\n    return A\nExample:\ngraph = {\n    0: [(1, 1), (2, 2)],\n    1: [(2, 1), (3, 3)],\n    2: [(3, 1)],\n    3: []\n}\nprint(yen_k_shortest_paths(graph, 0, 3, 3))\nOutput:\n[(3, [0, 1, 2, 3]), (4, [0, 2, 3]), (5, [0, 1, 3])]\n\n\nWhy It Matters\n\nProvides multiple distinct routes, not just one\nUsed in multi-path routing, backup planning, logistics optimization\nGuarantees simple paths (no loops)\nReuses Dijkstra, easy to integrate with existing solvers\n\n\n\nA Gentle Proof (Why It Works)\nEvery \\(P_i\\) is generated by deviating from earlier paths at some spur node, ensuring uniqueness. Dijkstra ensures each spur path is locally shortest. Since all candidates are stored and selected by global order, the sequence \\(P_1, P_2, \\dots, P_K\\) is globally sorted by total cost.\nThus: \\[\n\\text{len}(P_1) \\le \\text{len}(P_2) \\le \\cdots \\le \\text{len}(P_K)\n\\]\nand each path is simple.\n\n\nTry It Yourself\n\nGenerate the 3 shortest paths in a small network.\nCompare with Eppstein’s algorithm (faster for large K).\nRemove edges dynamically and re-run, observe reordering.\nVisualize path tree (root = \\(s\\), branches = deviations).\nAdapt for undirected or bi-directional graphs.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nGraph\n\\(s\\)\n\\(t\\)\n\\(K\\)\nPaths\n\n\n\n\nLine 0–1–2–3\n0\n3\n2\none main, one deviation\n\n\nTriangle\n0\n2\n3\n0–1–2, 0–2, 0–1–0–2 (if allowed)\n\n\nWeighted grid\nstart–goal\n3\nalternate routes\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(K \\cdot V \\cdot (E + V \\log V))\\)\nSpace: \\(O(KV)\\)\nCondition: Nonnegative weights, finite paths\n\nYen’s Algorithm is the explorer’s Dijkstra, it doesn’t stop at the first road found but keeps discovering better detours, one deviation at a time.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-35.-minimum-spanning-trees",
    "href": "books/en-us/list-4.html#section-35.-minimum-spanning-trees",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 35. Minimum Spanning Trees",
    "text": "Section 35. Minimum Spanning Trees\n\n341 Kruskal’s Algorithm\nKruskal’s algorithm builds a minimum spanning tree (MST) by repeatedly adding the lightest edge that does not create a cycle. It uses a disjoint set union (DSU) data structure to track components efficiently.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G=(V,E,w)\\), find a subset \\(T\\subseteq E\\) such that:\n\n\\(T\\) connects all vertices,\n\\(|T|=|V|-1\\),\n\\(\\sum_{e\\in T}w(e)\\) is minimized.\n\nIf the graph is not connected, Kruskal builds a minimum spanning forest.\n\n\nHow Does It Work (Plain Language)?\n\nSort all edges by nondecreasing weight.\nInitialize DSU with each vertex in its own set.\nScan edges in order. For edge \\((u,v)\\):\n\nIf \\(u\\) and \\(v\\) are in different sets, add the edge to the MST and union their sets.\nOtherwise, skip it to avoid a cycle.\n\nStop when you have \\(|V|-1\\) edges.\n\n\n\n\nStep\nAction\nDSU effect\n\n\n\n\nSort edges\nLight to heavy\nNone\n\n\nCheck \\((u,v)\\)\nIf find(u) ≠ find(v)\nunion(u,v) and keep edge\n\n\nCycle case\nIf find(u) = find(v)\nskip edge\n\n\n\n\n\nTiny Code\nC (DSU + Kruskal)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct { int u, v; int w; } Edge;\n\ntypedef struct {\n    int *p, *r;\n    int n;\n} DSU;\n\nDSU make_dsu(int n){\n    DSU d; d.n = n;\n    d.p = malloc(n * sizeof(int));\n    d.r = malloc(n * sizeof(int));\n    for(int i=0;i&lt;n;i++){ d.p[i]=i; d.r[i]=0; }\n    return d;\n}\nint find(DSU *d, int x){\n    if(d-&gt;p[x]!=x) d-&gt;p[x]=find(d, d-&gt;p[x]);\n    return d-&gt;p[x];\n}\nvoid unite(DSU *d, int a, int b){\n    a=find(d,a); b=find(d,b);\n    if(a==b) return;\n    if(d-&gt;r[a]&lt;d-&gt;r[b]) d-&gt;p[a]=b;\n    else if(d-&gt;r[b]&lt;d-&gt;r[a]) d-&gt;p[b]=a;\n    else { d-&gt;p[b]=a; d-&gt;r[a]++; }\n}\nint cmp_edge(const void* A, const void* B){\n    Edge *a=(Edge*)A, *b=(Edge*)B;\n    return a-&gt;w - b-&gt;w;\n}\n\nint main(void){\n    int n, m;\n    scanf(\"%d %d\", &n, &m);\n    Edge *E = malloc(m * sizeof(Edge));\n    for(int i=0;i&lt;m;i++) scanf(\"%d %d %d\", &E[i].u, &E[i].v, &E[i].w);\n\n    qsort(E, m, sizeof(Edge), cmp_edge);\n    DSU d = make_dsu(n);\n\n    int taken = 0;\n    long long cost = 0;\n    for(int i=0;i&lt;m && taken &lt; n-1;i++){\n        int a = find(&d, E[i].u), b = find(&d, E[i].v);\n        if(a != b){\n            unite(&d, a, b);\n            cost += E[i].w;\n            taken++;\n        }\n    }\n    if(taken != n-1) { printf(\"Graph not connected\\n\"); }\n    else { printf(\"MST cost: %lld\\n\", cost); }\n    return 0;\n}\nPython (DSU + Kruskal)\nclass DSU:\n    def __init__(self, n):\n        self.p = list(range(n))\n        self.r = [0]*n\n    def find(self, x):\n        if self.p[x] != x:\n            self.p[x] = self.find(self.p[x])\n        return self.p[x]\n    def union(self, a, b):\n        a, b = self.find(a), self.find(b)\n        if a == b: return False\n        if self.r[a] &lt; self.r[b]: self.p[a] = b\n        elif self.r[b] &lt; self.r[a]: self.p[b] = a\n        else: self.p[b] = a; self.r[a] += 1\n        return True\n\ndef kruskal(n, edges):\n    edges = sorted(edges)\n    dsu = DSU(n)\n    cost = 0\n    mst = []\n    for w, u, v in edges:\n        if dsu.union(u, v):\n            cost += w\n            mst.append((u, v, w))\n            if len(mst) == n-1:\n                break\n    return cost, mst\n\nn = 4\nedges = [(1,0,1),(4,0,2),(3,1,2),(2,1,3),(5,2,3)]\nprint(kruskal(n, edges))\n\n\nWhy It Matters\n\nSimple and fast with sorting plus DSU.\nWorks well on sparse graphs.\nProduces an MST that minimizes total edge weight.\nEasy to adapt for minimum spanning forest on disconnected graphs.\n\n\n\nA Gentle Proof (Why It Works)\nKruskal relies on the cut property: Let \\(S \\subset V\\) be any proper subset and consider the cut \\((S, V \\setminus S)\\). The lightest edge crossing the cut is safe to include in some MST.\nSorting edges by weight and always taking the next lightest that connects two different components is equivalent to repeatedly applying the cut property to the partition defined by current DSU components. This never creates a cycle and never excludes the possibility of an optimal MST. By induction on the number of chosen edges, the final set is an MST.\n\n\nTry It Yourself\n\nGenerate random sparse graphs and compare Kruskal vs Prim.\nRemove an edge from the MST and recompute to observe changes.\nForce ties in edge weights and confirm multiple valid MSTs.\nRun on a disconnected graph to obtain a minimum spanning forest.\nLog chosen edges to visualize the growth of components.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges \\((u,v,w)\\)\nMST Edges\nCost\n\n\n\n\nTriangle\n(0,1,1), (1,2,2), (0,2,3)\n(0,1,1), (1,2,2)\n3\n\n\nSquare\n(0,1,1), (1,2,1), (2,3,1), (3,0,1), (0,2,2)\nany 3 of weight 1\n3\n\n\nDisconnected\ntwo separate triangles\nMST per component\nsum of two\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log E)\\) for sorting plus almost linear DSU, which is \\(O(E \\alpha(V))\\). Usually written as \\(O(E \\log E)\\) or \\(O(E \\log V)\\).\nSpace: \\(O(V + E)\\).\nOutput size: \\(|V|-1\\) edges for a connected graph.\n\nKruskal is the sort then stitch approach to MSTs. Sort edges globally, then stitch components locally with DSU until the tree locks into place.\n\n\n\n342 Prim’s Algorithm (Heap)\nPrim’s algorithm builds a minimum spanning tree (MST) by growing a connected subtree one vertex at a time, always choosing the lightest edge that connects a vertex inside the tree to one outside. It’s a greedy algorithm, often implemented with a min-heap for efficiency.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G=(V,E,w)\\), find a subset of edges \\(T\\subseteq E\\) such that:\n\n\\(T\\) connects all vertices,\n\\(|T|=|V|-1\\),\n\\(\\sum_{e\\in T}w(e)\\) is minimized.\n\nPrim’s algorithm grows the MST from a single seed vertex.\n\n\nHow Does It Work (Plain Language)?\n\nChoose any starting vertex \\(s\\).\nInitialize all vertices with \\(\\text{key}[v]=\\infty\\), except \\(\\text{key}[s]=0\\).\nUse a min-heap (priority queue) keyed by edge weight.\nRepeatedly extract the vertex \\(u\\) with the smallest key (edge weight):\n\nMark \\(u\\) as part of the MST.\nFor each neighbor \\(v\\) of \\(u\\):\n\nIf \\(v\\) is not yet in the MST and \\(w(u,v)&lt;\\text{key}[v]\\), update \\(\\text{key}[v]=w(u,v)\\) and set \\(\\text{parent}[v]=u\\).\n\n\nContinue until all vertices are included.\n\n\n\n\nStep\nAction\nDescription\n\n\n\n\nInitialize\nChoose start vertex\n\\(\\text{key}[s]=0\\)\n\n\nExtract min\nAdd lightest edge\nExpands MST\n\n\nRelax neighbors\nUpdate cheaper edges\nMaintain frontier\n\n\n\n\n\nTiny Code\nPython (Min-Heap Version)\nimport heapq\n\ndef prim_mst(V, edges, start=0):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((w, v))\n        graph[v].append((w, u))  # undirected\n\n    visited = [False] * V\n    pq = [(0, start, -1)]  # (weight, vertex, parent)\n    mst_edges = []\n    total_cost = 0\n\n    while pq:\n        w, u, parent = heapq.heappop(pq)\n        if visited[u]:\n            continue\n        visited[u] = True\n        total_cost += w\n        if parent != -1:\n            mst_edges.append((parent, u, w))\n        for weight, v in graph[u]:\n            if not visited[v]:\n                heapq.heappush(pq, (weight, v, u))\n    return total_cost, mst_edges\n\nedges = [(0,1,2),(0,2,3),(1,2,1),(1,3,4),(2,3,5)]\ncost, mst = prim_mst(4, edges)\nprint(cost, mst)\nOutput:\n7 [(0,1,2), (1,2,1), (1,3,4)]\n\n\nWhy It Matters\n\nSuitable for dense graphs (especially with adjacency lists and heaps).\nBuilds MST incrementally (like Dijkstra).\nGreat for online construction where the tree must stay connected.\nEasier to integrate with adjacency matrix for small graphs.\n\n\n\nA Gentle Proof (Why It Works)\nPrim’s algorithm obeys the cut property: At each step, consider the cut between the current MST set \\(S\\) and the remaining vertices \\(V \\setminus S\\). The lightest edge crossing that cut is always safe to include. By repeatedly choosing the minimum such edge, Prim’s maintains a valid MST prefix. When all vertices are added, the resulting tree is minimal.\n\n\nTry It Yourself\n\nRun Prim’s on a dense graph vs Kruskal, compare edge choices.\nVisualize the growing frontier.\nTry with adjacency matrix (without heap).\nTest on disconnected graph, each component forms its own tree.\nReplace heap with a simple array to see \\(O(V^2)\\) version.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges \\((u,v,w)\\)\nMST Edges\nCost\n\n\n\n\nTriangle\n(0,1,1), (1,2,2), (0,2,3)\n(0,1,1), (1,2,2)\n3\n\n\nSquare\n(0,1,2), (1,2,3), (2,3,1), (3,0,4)\n(2,3,1), (0,1,2), (1,2,3)\n6\n\n\nLine\n(0,1,5), (1,2,1), (2,3,2)\n(1,2,1), (2,3,2), (0,1,5)\n8\n\n\n\n\n\nComplexity\n\nTime: \\(O(E\\log V)\\) with heap, \\(O(V^2)\\) with array.\nSpace: \\(O(V + E)\\)\nOutput size: \\(|V|-1\\) edges\n\nPrim’s is the grow from seed approach to MSTs. It builds the tree step by step, always pulling the next lightest edge from the frontier.\n\n\n\n343 Prim’s Algorithm (Adjacency Matrix)\nThis is the array-based version of Prim’s algorithm, optimized for dense graphs. Instead of using a heap, it directly scans all vertices to find the next minimum key vertex at each step. The logic is identical to Prim’s heap version but trades priority queues for simple loops.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G=(V,E,w)\\), represented by an adjacency matrix \\(W\\), we want to construct an MST, a subset of edges that:\n\nconnects all vertices,\ncontains \\(|V|-1\\) edges,\nand minimizes total weight \\(\\sum w(e)\\).\n\nThe adjacency matrix form simplifies edge lookups and is ideal for dense graphs, where \\(E\\approx V^2\\).\n\n\nHow Does It Work (Plain Language)?\n\nStart from an arbitrary vertex (say \\(0\\)).\nInitialize \\(\\text{key}[v]=\\infty\\) for all vertices, except \\(\\text{key}[0]=0\\).\nMaintain a set \\(\\text{inMST}[v]\\) marking vertices already included.\nRepeat \\(V-1\\) times:\n\nChoose the vertex \\(u\\) not yet in MST with the smallest \\(\\text{key}[u]\\).\nAdd \\(u\\) to MST.\nFor each vertex \\(v\\), if \\(W[u][v]\\) is smaller than \\(\\text{key}[v]\\), update it and record parent \\(v\\gets u\\).\n\n\nAt each iteration, one vertex joins the MST, the one connected by the lightest edge to the existing set.\n\n\n\nStep\nOperation\nDescription\n\n\n\n\nInitialize\n\\(\\text{key}[0]=0\\)\nstart from vertex 0\n\n\nExtract min\nfind smallest key\nnext vertex to add\n\n\nUpdate\nrelax edges\nupdate keys of neighbors\n\n\n\n\n\nTiny Code\nC (Adjacency Matrix Version)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n#include &lt;stdbool.h&gt;\n\n#define INF 1000000000\n#define N 100\n\nint minKey(int key[], bool inMST[], int n) {\n    int min = INF, idx = -1;\n    for (int v = 0; v &lt; n; v++)\n        if (!inMST[v] && key[v] &lt; min)\n            min = key[v], idx = v;\n    return idx;\n}\n\nvoid primMatrix(int graph[N][N], int n) {\n    int key[N], parent[N];\n    bool inMST[N];\n    for (int i = 0; i &lt; n; i++)\n        key[i] = INF, inMST[i] = false;\n    key[0] = 0, parent[0] = -1;\n\n    for (int count = 0; count &lt; n - 1; count++) {\n        int u = minKey(key, inMST, n);\n        inMST[u] = true;\n        for (int v = 0; v &lt; n; v++)\n            if (graph[u][v] && !inMST[v] && graph[u][v] &lt; key[v]) {\n                parent[v] = u;\n                key[v] = graph[u][v];\n            }\n    }\n    int total = 0;\n    for (int i = 1; i &lt; n; i++) {\n        printf(\"%d - %d: %d\\n\", parent[i], i, graph[i][parent[i]]);\n        total += graph[i][parent[i]];\n    }\n    printf(\"MST cost: %d\\n\", total);\n}\n\nint main() {\n    int n = 5;\n    int graph[N][N] = {\n        {0,2,0,6,0},\n        {2,0,3,8,5},\n        {0,3,0,0,7},\n        {6,8,0,0,9},\n        {0,5,7,9,0}\n    };\n    primMatrix(graph, n);\n    return 0;\n}\nOutput:\n0 - 1: 2\n1 - 2: 3\n0 - 3: 6\n1 - 4: 5\nMST cost: 16\nPython Version\ndef prim_matrix(graph):\n    V = len(graph)\n    key = [float('inf')] * V\n    parent = [-1] * V\n    in_mst = [False] * V\n    key[0] = 0\n\n    for _ in range(V - 1):\n        u = min((key[v], v) for v in range(V) if not in_mst[v])[1]\n        in_mst[u] = True\n        for v in range(V):\n            if graph[u][v] != 0 and not in_mst[v] and graph[u][v] &lt; key[v]:\n                key[v] = graph[u][v]\n                parent[v] = u\n\n    edges = [(parent[i], i, graph[i][parent[i]]) for i in range(1, V)]\n    cost = sum(w for _, _, w in edges)\n    return cost, edges\n\ngraph = [\n    [0,2,0,6,0],\n    [2,0,3,8,5],\n    [0,3,0,0,7],\n    [6,8,0,0,9],\n    [0,5,7,9,0]\n]\nprint(prim_matrix(graph))\nOutput:\n(16, [(0,1,2), (1,2,3), (0,3,6), (1,4,5)])\n\n\nWhy It Matters\n\nSimple to implement with adjacency matrices.\nBest for dense graphs where \\(E \\approx V^2\\).\nAvoids the complexity of heaps.\nEasy to visualize and debug for classroom or teaching use.\n\n\n\nA Gentle Proof (Why It Works)\nLike the heap-based version, this variant relies on the cut property: At each step, the chosen edge connects a vertex inside the tree to one outside with minimum weight, so it is always safe. Each iteration expands the MST without cycles, and after \\(V-1\\) iterations, all vertices are connected.\n\n\nTry It Yourself\n\nRun on a complete graph, expect \\(V-1\\) smallest edges.\nModify weights, see how edge choices change.\nCompare with heap-based Prim on runtime as \\(V\\) grows.\nImplement in \\(O(V^2)\\) and confirm complexity experimentally.\nTest on disconnected graph, see where algorithm stops.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nMST Edges\nCost\n\n\n\n\n\n5-node matrix\n(0,1,2), (1,2,3), (0,3,6), (1,4,5)\n16\n\n\n\n3-node triangle\n(0,1,1), (1,2,2)\n3\n\n\n\nDense complete 4-node\n6 edges\nchooses lightest 3 edges\nsum of min\n\n\n\n\n\nComplexity\n\nTime: \\(O(V^2)\\)\nSpace: \\(O(V^2)\\) (matrix)\nOutput size: \\(|V|-1\\) edges\n\nPrim (Adjacency Matrix) is the classic dense-graph version, it trades speed for simplicity and predictable access time.\n\n\n\n344 Borůvka’s Algorithm\nBorůvka’s algorithm is one of the earliest MST algorithms (1926), designed to build the minimum spanning tree by repeatedly connecting each component with its cheapest outgoing edge. It operates in phases, merging components until a single tree remains.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G = (V, E, w)\\), we want to find a minimum spanning tree (MST), a subset \\(T \\subseteq E\\) such that:\n\n\\(T\\) connects all vertices,\n\\(|T| = |V| - 1\\),\n\\(\\sum_{e \\in T} w(e)\\) is minimized.\n\nBorůvka’s approach grows multiple subtrees in parallel, making it highly suitable for parallel or distributed computation.\n\n\nHow Does It Work (Plain Language)?\nThe algorithm works in rounds. Each round connects every component to another via its lightest outgoing edge.\n\nStart with each vertex as its own component.\nFor each component, find the minimum-weight edge that connects it to another component.\nAdd all these edges to the MST, they are guaranteed safe (by the cut property).\nMerge components connected by these edges.\nRepeat until only one component remains.\n\nEach round at least halves the number of components, so the algorithm finishes in \\(O(\\log V)\\) phases.\n\n\n\nStep\nOperation\nDescription\n\n\n\n\n1\nInitialize components\neach vertex alone\n\n\n2\nFind cheapest edge per component\nlightest outgoing\n\n\n3\nAdd edges\nmerge components\n\n\n4\nRepeat\nuntil single component\n\n\n\n\n\nTiny Code\nPython (DSU Version)\nclass DSU:\n    def __init__(self, n):\n        self.p = list(range(n))\n        self.r = [0] * n\n    def find(self, x):\n        if self.p[x] != x:\n            self.p[x] = self.find(self.p[x])\n        return self.p[x]\n    def union(self, a, b):\n        a, b = self.find(a), self.find(b)\n        if a == b: return False\n        if self.r[a] &lt; self.r[b]: self.p[a] = b\n        elif self.r[a] &gt; self.r[b]: self.p[b] = a\n        else: self.p[b] = a; self.r[a] += 1\n        return True\n\ndef boruvka_mst(V, edges):\n    dsu = DSU(V)\n    mst = []\n    total_cost = 0\n    components = V\n\n    while components &gt; 1:\n        cheapest = [-1] * V\n        for i, (u, v, w) in enumerate(edges):\n            set_u = dsu.find(u)\n            set_v = dsu.find(v)\n            if set_u != set_v:\n                if cheapest[set_u] == -1 or edges[cheapest[set_u]][2] &gt; w:\n                    cheapest[set_u] = i\n                if cheapest[set_v] == -1 or edges[cheapest[set_v]][2] &gt; w:\n                    cheapest[set_v] = i\n        for i in range(V):\n            if cheapest[i] != -1:\n                u, v, w = edges[cheapest[i]]\n                if dsu.union(u, v):\n                    mst.append((u, v, w))\n                    total_cost += w\n                    components -= 1\n    return total_cost, mst\n\nedges = [(0,1,1), (0,2,3), (1,2,1), (1,3,4), (2,3,5)]\nprint(boruvka_mst(4, edges))\nOutput:\n(6, [(0,1,1), (1,2,1), (1,3,4)])\n\n\nWhy It Matters\n\nNaturally parallelizable (each component acts independently).\nSimple and elegant, based on repeated application of the cut property.\nIdeal for sparse graphs and distributed systems.\nOften used in hybrid MST algorithms (combining with Kruskal/Prim).\n\n\n\nA Gentle Proof (Why It Works)\nBy the cut property, for each component \\(C\\), the cheapest edge leaving \\(C\\) is always safe to include in the MST. Since edges are chosen simultaneously across all components, and no cycles are created within a single phase (components merge only across cuts), each round preserves correctness. After each phase, components merge, and the process repeats until all are unified.\nEach iteration reduces the number of components by at least half, ensuring \\(O(\\log V)\\) phases.\n\n\nTry It Yourself\n\nRun on a small graph and trace phases manually.\nCompare with Kruskal’s sorted-edge approach.\nAdd parallel logging to visualize simultaneous merges.\nObserve how components shrink exponentially.\nMix with Kruskal: use Borůvka until few components remain, then switch.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges \\((u,v,w)\\)\nMST\nCost\n\n\n\n\nTriangle\n(0,1,1),(1,2,2),(0,2,3)\n(0,1,1),(1,2,2)\n3\n\n\nSquare\n(0,1,1),(1,2,2),(2,3,1),(3,0,2)\n(0,1,1),(2,3,1),(1,2,2)\n4\n\n\nDense 4-node\n6 edges\nbuilds MST in 2 phases\nverified cost\n\n\n\n\n\nComplexity\n\nPhases: \\(O(\\log V)\\)\nTime per phase: \\(O(E)\\)\nTotal Time: \\(O(E \\log V)\\)\nSpace: \\(O(V + E)\\)\n\nBorůvka’s algorithm is the parallel grow-and-merge strategy for MSTs, every component reaches out through its lightest edge until the whole graph becomes one connected tree.\n\n\n\n345 Reverse-Delete MST\nThe Reverse-Delete Algorithm builds a minimum spanning tree (MST) by starting with the full graph and repeatedly removing edges, but only when their removal does not disconnect the graph. It’s the conceptual mirror image of Kruskal’s algorithm.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G = (V, E, w)\\), we want to find an MST — a spanning tree that connects all vertices with minimum total weight.\nInstead of adding edges like Kruskal, we start with all edges and delete them one by one, making sure the graph remains connected after each deletion.\n\n\nHow Does It Work (Plain Language)?\n\nSort all edges by decreasing weight.\nInitialize the working graph \\(T = G\\).\nFor each edge \\((u,v)\\) in that order:\n\nTemporarily remove \\((u,v)\\) from \\(T\\).\nIf \\(u\\) and \\(v\\) are still connected in \\(T\\), permanently delete the edge (it’s not needed).\nOtherwise, restore it (it’s essential).\n\nWhen all edges are processed, \\(T\\) is the MST.\n\nThis approach ensures only indispensable edges remain, forming a spanning tree of minimal total weight.\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nAction\nDescription\n\n\n\n\n\n\n\n\n1\nSort edges descending\nheavy edges first\n\n\n\n\n\n\n2\nRemove edge\ntest connectivity\n\n\n\n\n\n\n3\nKeep if needed\nif removal disconnects\n\n\n\n\n\n\n4\nStop\nwhen $\nT\n=\nV\n-1$\n\n\n\n\n\nTiny Code\nPython (Using DFS for Connectivity Check)\ndef dfs(graph, start, visited):\n    visited.add(start)\n    for v, _ in graph[start]:\n        if v not in visited:\n            dfs(graph, v, visited)\n\ndef is_connected(graph, u, v):\n    visited = set()\n    dfs(graph, u, visited)\n    return v in visited\n\ndef reverse_delete_mst(V, edges):\n    edges = sorted(edges, key=lambda x: x[2], reverse=True)\n    graph = {i: [] for i in range(V)}\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))\n\n    mst_cost = sum(w for _, _, w in edges)\n\n    for u, v, w in edges:\n        # remove edge\n        graph[u] = [(x, wx) for x, wx in graph[u] if x != v]\n        graph[v] = [(x, wx) for x, wx in graph[v] if x != u]\n\n        if is_connected(graph, u, v):\n            mst_cost -= w  # edge not needed\n        else:\n            # restore edge\n            graph[u].append((v, w))\n            graph[v].append((u, w))\n\n    mst_edges = []\n    visited = set()\n    def collect(u):\n        visited.add(u)\n        for v, w in graph[u]:\n            if (v, w) not in visited:\n                mst_edges.append((u, v, w))\n                if v not in visited:\n                    collect(v)\n    collect(0)\n    return mst_cost, mst_edges\n\nedges = [(0,1,1), (0,2,2), (1,2,3), (1,3,4), (2,3,5)]\nprint(reverse_delete_mst(4, edges))\nOutput:\n(7, [(0,1,1), (0,2,2), (1,3,4)])\n\n\nWhy It Matters\n\nSimple dual perspective to Kruskal’s algorithm.\nDemonstrates the cycle property:\n\nIn any cycle, the edge with the largest weight cannot be in an MST.\n\nGood for teaching proofs and conceptual understanding.\nCan be used for verifying MSTs or constructing them in reverse.\n\n\n\nA Gentle Proof (Why It Works)\nThe cycle property states:\n\nFor any cycle in a graph, the edge with the largest weight cannot belong to the MST.\n\nBy sorting edges in descending order and removing each maximum-weight edge that lies in a cycle (i.e., when \\(u\\) and \\(v\\) remain connected without it), we eliminate all non-MST edges. When no such edge remains, the result is an MST.\nSince each deletion preserves connectivity, the final graph is a spanning tree with minimal total weight.\n\n\nTry It Yourself\n\nRun on a triangle graph, see heaviest edge removed.\nCompare deletion order with Kruskal’s addition order.\nVisualize graph at each step.\nReplace DFS with BFS or Union-Find for speed.\nUse to verify MST output from another algorithm.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges \\((u,v,w)\\)\nMST\nCost\n\n\n\n\nTriangle\n(0,1,1),(1,2,2),(0,2,3)\n(0,1,1),(1,2,2)\n3\n\n\nSquare\n(0,1,1),(1,2,2),(2,3,3),(3,0,4)\n(0,1,1),(1,2,2),(2,3,3)\n6\n\n\nLine\n(0,1,2),(1,2,1),(2,3,3)\nall edges\n6\n\n\n\n\n\nComplexity\n\nTime: \\(O(E(E+V))\\) with naive DFS (each edge removal checks connectivity). With Union-Find optimizations or precomputed structures, it can be reduced.\nSpace: \\(O(V + E)\\).\n\nReverse-Delete is the subtract instead of add view of MSTs, peel away heavy edges until only the essential light structure remains.\n\n\n\n346 MST via Dijkstra Trick\nThis variant constructs a Minimum Spanning Tree (MST) using a Dijkstra-like process, repeatedly expanding the tree by selecting the lightest edge connecting any vertex inside the tree to one outside. It’s essentially Prim’s algorithm recast through Dijkstra’s lens, showing the deep parallel between shortest-path and spanning-tree growth.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G=(V,E,w)\\) with nonnegative edge weights, we want an MST — a subset \\(T \\subseteq E\\) that connects all vertices with \\[\n|T| = |V| - 1, \\quad \\text{and} \\quad \\sum_{e \\in T} w(e) \\text{ minimized.}\n\\]\nWhile Dijkstra’s algorithm builds shortest-path trees based on path cost, this version builds an MST by using edge weights directly as “distances” from the current tree.\n\n\nHow Does It Work (Plain Language)?\nThis is Prim’s algorithm in disguise: Instead of tracking the shortest path from a root, we track the lightest edge connecting each vertex to the growing tree.\n\nInitialize all vertices with \\(\\text{key}[v]=\\infty\\), except start vertex \\(s\\) with \\(\\text{key}[s]=0\\).\nUse a priority queue (min-heap) keyed by \\(\\text{key}[v]\\).\nRepeatedly extract vertex \\(u\\) with smallest key.\nFor each neighbor \\(v\\):\n\nIf \\(v\\) not yet in tree and \\(w(u,v)&lt;\\text{key}[v]\\), update \\(\\text{key}[v]=w(u,v)\\) and record parent.\n\nRepeat until all vertices are included.\n\nUnlike Dijkstra, we do not sum edge weights, we only care about the minimum edge to reach each vertex.\n\n\n\n\n\n\n\n\nStep\nDijkstra\nMST via Dijkstra Trick\n\n\n\n\nDistance update\n\\(\\text{dist}[v] = \\text{dist}[u] + w(u,v)\\)\n\\(\\text{key}[v] = \\min(\\text{key}[v], w(u,v))\\)\n\n\nPriority\nPath cost\nEdge cost\n\n\nGoal\nShortest path\nMST\n\n\n\n\n\nTiny Code\nPython (Heap Implementation)\nimport heapq\n\ndef mst_dijkstra_trick(V, edges, start=0):\n    graph = [[] for _ in range(V)]\n    for u, v, w in edges:\n        graph[u].append((v, w))\n        graph[v].append((u, w))\n\n    in_tree = [False] * V\n    key = [float('inf')] * V\n    parent = [-1] * V\n    key[start] = 0\n    pq = [(0, start)]\n\n    total_cost = 0\n    while pq:\n        w, u = heapq.heappop(pq)\n        if in_tree[u]:\n            continue\n        in_tree[u] = True\n        total_cost += w\n\n        for v, wt in graph[u]:\n            if not in_tree[v] and wt &lt; key[v]:\n                key[v] = wt\n                parent[v] = u\n                heapq.heappush(pq, (wt, v))\n\n    mst_edges = [(parent[v], v, key[v]) for v in range(V) if parent[v] != -1]\n    return total_cost, mst_edges\n\nedges = [(0,1,2),(0,2,3),(1,2,1),(1,3,4),(2,3,5)]\nprint(mst_dijkstra_trick(4, edges))\nOutput:\n(7, [(0,1,2), (1,2,1), (1,3,4)])\n\n\nWhy It Matters\n\nDemonstrates the duality between MST and shortest-path search.\nProvides a conceptual bridge between Prim and Dijkstra.\nUseful for teaching and algorithmic unification.\nIntuitive when already familiar with Dijkstra’s structure.\n\n\n\nA Gentle Proof (Why It Works)\nAt each iteration, we maintain a set \\(S\\) of vertices already in the MST. The cut property guarantees that the lightest edge connecting \\(S\\) to \\(V\\setminus S\\) is always safe to include.\nBy storing these edge weights in a priority queue and always selecting the smallest, we exactly follow this property, thus constructing an MST incrementally.\nThe algorithm stops when all vertices are included, yielding an MST.\n\n\nTry It Yourself\n\nCompare line by line with Dijkstra, only one change in relaxation.\nRun on complete graphs, observe star-like MSTs.\nTry graphs with multiple equal edges, see tie-handling.\nReplace heap with array, check \\(O(V^2)\\) version.\nVisualize with frontier highlighting, edges instead of distances.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges \\((u,v,w)\\)\nMST\nCost\n\n\n\n\nTriangle\n(0,1,1),(1,2,2),(0,2,3)\n(0,1,1),(1,2,2)\n3\n\n\nSquare\n(0,1,2),(1,2,3),(2,3,1),(3,0,4)\n(2,3,1),(0,1,2),(1,2,3)\n6\n\n\nLine\n(0,1,5),(1,2,2),(2,3,1)\nall edges\n8\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log V)\\) (heap) or \\(O(V^2)\\) (array)\nSpace: \\(O(V + E)\\)\nOutput size: \\(|V|-1\\) edges\n\nMST via Dijkstra Trick is Prim’s algorithm reimagined, it replaces distance summation with edge minimization, proving how two classic graph ideas share one greedy heart.\n\n\n\n347 Dynamic MST Maintenance\nDynamic MST maintenance addresses the question: how can we update an MST efficiently when the underlying graph changes, edges are added, removed, or their weights change, without rebuilding from scratch?\nThis problem arises in systems where the graph evolves over time, such as networks, road maps, or real-time optimization systems.\n\nWhat Problem Are We Solving?\nGiven a graph \\(G=(V,E,w)\\) and its MST \\(T\\), we want to maintain \\(T\\) under updates:\n\nEdge Insertion: add new edge \\((u,v,w)\\)\nEdge Deletion: remove existing edge \\((u,v)\\)\nEdge Weight Update: change weight \\(w(u,v)\\)\n\nNaively recomputing the MST after each change costs \\(O(E \\log V)\\). Dynamic maintenance reduces this significantly using incremental repair of \\(T\\).\n\n\nHow Does It Work (Plain Language)?\n\nEdge Insertion:\n\nAdd the new edge \\((u,v,w)\\).\nCheck if it creates a cycle in \\(T\\).\nIf the new edge is lighter than the heaviest edge on that cycle, replace the heavy one.\nOtherwise, discard it.\n\nEdge Deletion:\n\nRemove \\((u,v)\\) from \\(T\\), splitting it into two components.\nFind the lightest edge connecting the two components in \\(G\\).\nAdd that edge to restore connectivity.\n\nEdge Weight Update:\n\nIf an edge’s weight increases, treat as potential deletion.\nIf it decreases, treat as potential insertion.\n\n\nTo do this efficiently, we need data structures that can:\n\nfind max edge on path quickly (for cycles)\nfind min cross edge between components\n\nThese can be implemented via dynamic trees, Link-Cut Trees, or Euler Tour Trees.\n\n\nTiny Code (Simplified Static Version)\nThis version demonstrates edge insertion maintenance with cycle detection.\ndef find(parent, x):\n    if parent[x] != x:\n        parent[x] = find(parent, parent[x])\n    return parent[x]\n\ndef union(parent, rank, x, y):\n    rx, ry = find(parent, x), find(parent, y)\n    if rx == ry:\n        return False\n    if rank[rx] &lt; rank[ry]:\n        parent[rx] = ry\n    elif rank[rx] &gt; rank[ry]:\n        parent[ry] = rx\n    else:\n        parent[ry] = rx\n        rank[rx] += 1\n    return True\n\ndef dynamic_mst_insert(V, mst_edges, new_edge):\n    u, v, w = new_edge\n    parent = [i for i in range(V)]\n    rank = [0]*V\n    for x, y, _ in mst_edges:\n        union(parent, rank, x, y)\n    if find(parent, u) != find(parent, v):\n        mst_edges.append(new_edge)\n    else:\n        # would form cycle, pick lighter edge\n        cycle_edges = mst_edges + [new_edge]\n        heaviest = max(cycle_edges, key=lambda e: e[2])\n        if heaviest != new_edge:\n            mst_edges.remove(heaviest)\n            mst_edges.append(new_edge)\n    return mst_edges\nThis is conceptual; in practice, Link-Cut Trees make this dynamic in logarithmic time.\n\n\nWhy It Matters\n\nCrucial for streaming graphs, online networks, real-time routing.\nAvoids recomputation after each update.\nDemonstrates greedy invariants under change, the MST remains minimal if maintained properly.\nForms basis for fully dynamic graph algorithms.\n\n\n\nA Gentle Proof (Why It Works)\nMST invariants are preserved through cut and cycle properties:\n\nCycle Property: In a cycle, the heaviest edge cannot be in an MST.\nCut Property: In any cut, the lightest edge crossing it must be in the MST.\n\nWhen inserting an edge:\n\nIt forms a cycle, we drop the heaviest to maintain minimality.\n\nWhen deleting an edge:\n\nIt forms a cut, we insert the lightest crossing edge to maintain connectivity.\n\nThus each update restores a valid MST in local time.\n\n\nTry It Yourself\n\nStart from an MST of a small graph.\nInsert an edge and track the created cycle.\nDelete an MST edge and find replacement.\nTry batch updates, see structure evolve.\nCompare runtime with full recomputation.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nOperation\nDescription\nResult\n\n\n\n\nInsert \\((1,3,1)\\)\ncreates cycle \\((1,2,3,1)\\)\nremove heaviest edge\n\n\nDelete \\((0,1)\\)\nbreaks tree\nfind lightest reconnecting edge\n\n\nDecrease weight \\((2,3)\\)\nrecheck inclusion\nedge may enter MST\n\n\n\n\n\nComplexity\n\nInsertion/Deletion (Naive): \\(O(E)\\)\nDynamic Tree (Link-Cut): \\(O(\\log^2 V)\\) per update\nSpace: \\(O(V + E)\\)\n\nDynamic MST maintenance shows how local adjustments preserve global optimality, a powerful principle for evolving systems.\n\n\n\n348 Minimum Bottleneck Spanning Tree\nA Minimum Bottleneck Spanning Tree (MBST) is a spanning tree that minimizes the maximum edge weight among all edges in the tree. Unlike the standard MST, which minimizes the total sum of edge weights, an MBST focuses on the worst (heaviest) edge in the tree.\nIn many real-world systems, such as network design or transportation planning, you may care less about total cost and more about bottleneck constraints, the weakest or slowest connection.\n\nWhat Problem Are We Solving?\nGiven a connected, undirected, weighted graph \\(G = (V, E, w)\\), we want a spanning tree \\(T \\subseteq E\\) such that\n\\[\n\\max_{e \\in T} w(e)\n\\]\nis as small as possible.\nIn other words, among all spanning trees, pick one where the largest edge weight is minimal.\n\n\nHow Does It Work (Plain Language)?\nYou can build an MBST using the same algorithms as MST (Kruskal or Prim), because:\n\nEvery Minimum Spanning Tree is also a Minimum Bottleneck Spanning Tree.\n\nThe reasoning:\n\nAn MST minimizes the total sum.\nIn doing so, it also ensures no unnecessarily heavy edges remain.\n\nSo any MST automatically satisfies the bottleneck property.\nAlternatively, you can use a binary search + connectivity test:\n\nSort edges by weight.\nBinary search for a threshold \\(W\\).\nCheck if edges with \\(w(e) \\le W\\) can connect all vertices.\nThe smallest \\(W\\) for which graph is connected is the bottleneck weight.\nExtract any spanning tree using only edges \\(\\le W\\).\n\n\n\nTiny Code (Kruskal-based)\ndef find(parent, x):\n    if parent[x] != x:\n        parent[x] = find(parent, parent[x])\n    return parent[x]\n\ndef union(parent, rank, x, y):\n    rx, ry = find(parent, x), find(parent, y)\n    if rx == ry:\n        return False\n    if rank[rx] &lt; rank[ry]:\n        parent[rx] = ry\n    elif rank[rx] &gt; rank[ry]:\n        parent[ry] = rx\n    else:\n        parent[ry] = rx\n        rank[rx] += 1\n    return True\n\ndef minimum_bottleneck_spanning_tree(V, edges):\n    edges = sorted(edges, key=lambda x: x[2])\n    parent = [i for i in range(V)]\n    rank = [0]*V\n    bottleneck = 0\n    count = 0\n    for u, v, w in edges:\n        if union(parent, rank, u, v):\n            bottleneck = max(bottleneck, w)\n            count += 1\n            if count == V - 1:\n                break\n    return bottleneck\n\nedges = [(0,1,4),(0,2,3),(1,2,2),(1,3,5),(2,3,6)]\nprint(minimum_bottleneck_spanning_tree(4, edges))\nOutput:\n5\nHere, the MST has total weight 4+3+2=9, and bottleneck edge weight 5.\n\n\nWhy It Matters\n\nUseful for quality-of-service or bandwidth-constrained systems.\nEnsures no edge in the tree exceeds a critical capacity threshold.\nIllustrates that minimizing maximum and minimizing sum can align, a key insight in greedy algorithms.\nProvides a way to test MST correctness: if an MST doesn’t minimize the bottleneck, it’s invalid.\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T^*\\) be an MST and \\(B(T^*)\\) be its maximum edge weight.\nSuppose another tree \\(T'\\) had a smaller bottleneck: \\[\nB(T') &lt; B(T^*)\n\\] Then there exists an edge \\(e\\) in \\(T^*\\) with \\(w(e) = B(T^*)\\), but \\(T'\\) avoids it while staying connected using lighter edges.\nThis contradicts the cycle property, since \\(e\\) would be replaced by a lighter edge crossing the same cut, meaning \\(T^*\\) wasn’t minimal.\nThus, every MST is an MBST.\n\n\nTry It Yourself\n\nBuild a graph with multiple MSTs, check if they share the same bottleneck.\nCompare MST total weight vs MBST bottleneck.\nApply binary search approach, confirm consistency.\nVisualize all spanning trees and mark max edge in each.\nConstruct a case where multiple MBSTs exist.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges \\((u,v,w)\\)\nMST\nBottleneck\n\n\n\n\nTriangle\n(0,1,1),(1,2,2),(0,2,3)\n(0,1,1),(1,2,2)\n2\n\n\nSquare\n(0,1,1),(1,2,5),(2,3,2),(3,0,4)\n(0,1,1),(2,3,2),(3,0,4)\n4\n\n\nChain\n(0,1,2),(1,2,3),(2,3,4)\nsame\n4\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log E)\\) (same as Kruskal)\nSpace: \\(O(V)\\)\nOutput: bottleneck weight or edges\n\nA Minimum Bottleneck Spanning Tree highlights the heaviest load-bearing link, a critical measure when resilience, latency, or bandwidth limits matter more than total cost.\n\n\n\n349 Manhattan MST\nA Manhattan Minimum Spanning Tree (Manhattan MST) finds a spanning tree that minimizes the sum of Manhattan distances between connected points on a grid. This variant is common in VLSI design, city planning, and grid-based path optimization, where movement is constrained to axis-aligned directions.\n\nWhat Problem Are We Solving?\nGiven \\(n\\) points in 2D space with coordinates \\((x_i, y_i)\\), the Manhattan distance between points \\(p_i\\) and \\(p_j\\) is\n\\[\nd(p_i, p_j) = |x_i - x_j| + |y_i - y_j|\n\\]\nWe want to build a spanning tree connecting all points such that the total Manhattan distance is minimal.\nA naive solution considers all \\(\\binom{n}{2}\\) edges and runs Kruskal’s algorithm, but that’s too slow for large \\(n\\). The key is exploiting geometry to limit candidate edges.\n\n\nHow Does It Work (Plain Language)?\nThe Manhattan MST problem leverages a geometric property:\n\nFor each point, only a small number of nearest neighbors (under certain transformations) can belong to the MST.\n\nThus, instead of checking all pairs, we:\n\nSort points in specific directions (rotations/reflections).\nUse a sweep line or Fenwick tree to find candidate neighbors.\nCollect \\(O(n)\\) potential edges.\nRun Kruskal’s algorithm on this reduced set.\n\nBy considering 8 directional transforms, we ensure all possible nearest edges are included.\nExample transforms:\n\n\\((x, y)\\)\n\\((y, x)\\)\n\\((-x, y)\\)\n\\((x, -y)\\)\netc.\n\nFor each transform:\n\nSort points by \\((x+y)\\) or \\((x-y)\\).\nFor each, track candidate neighbor minimizing \\(|x|+|y|\\).\nAdd edge between each point and its nearest neighbor.\n\nFinally, compute MST using Kruskal on the collected edges.\n\n\nTiny Code (Simplified Illustration)\ndef manhattan_distance(p1, p2):\n    return abs(p1[0]-p2[0]) + abs(p1[1]-p2[1])\n\ndef manhattan_mst(points):\n    edges = []\n    n = len(points)\n    for i in range(n):\n        for j in range(i+1, n):\n            w = manhattan_distance(points[i], points[j])\n            edges.append((i, j, w))\n    edges.sort(key=lambda e: e[2])\n\n    parent = [i for i in range(n)]\n    def find(x):\n        if parent[x]!=x:\n            parent[x]=find(parent[x])\n        return parent[x]\n    def union(x, y):\n        rx, ry = find(x), find(y)\n        if rx!=ry:\n            parent[ry]=rx\n            return True\n        return False\n\n    mst_edges = []\n    cost = 0\n    for u, v, w in edges:\n        if union(u, v):\n            mst_edges.append((u, v, w))\n            cost += w\n    return cost, mst_edges\n\npoints = [(0,0), (2,2), (2,0), (0,2)]\nprint(manhattan_mst(points))\nOutput:\n(8, [(0,2,2),(0,3,2),(2,1,4)])\nThis brute-force example builds a Manhattan MST by checking all pairs. Efficient geometric variants reduce this from \\(O(n^2)\\) to \\(O(n \\log n)\\).\n\n\nWhy It Matters\n\nCaptures grid-based movement (no diagonals).\nCritical in VLSI circuit layout (wire length minimization).\nFoundational for city-block planning and delivery networks.\nShows how geometry and graph theory merge in spatial problems.\n\n\n\nA Gentle Proof (Why It Works)\nThe MST under Manhattan distance obeys the cut property, the lightest edge crossing any partition must be in the MST. By ensuring all directional neighbors are included, we never miss the minimal edge across any cut.\nThus, even though we prune candidate edges, correctness is preserved.\n\n\nTry It Yourself\n\nGenerate random points, visualize MST edges.\nCompare Manhattan MST vs Euclidean MST.\nAdd a diagonal, see how cost differs.\nTry optimized directional neighbor search.\nObserve symmetry, each transform covers a quadrant.\n\n\n\nTest Cases\n\n\n\nPoints\nMST Edges\nTotal Cost\n\n\n\n\n(0,0),(1,0),(1,1)\n(0,1),(1,2)\n2\n\n\n(0,0),(2,2),(2,0),(0,2)\n3 edges\n8\n\n\n(0,0),(3,0),(0,4)\n(0,1),(0,2)\n7\n\n\n\n\n\nComplexity\n\nNaive: \\(O(n^2 \\log n)\\)\nOptimized (geometry-based): \\(O(n \\log n)\\)\nSpace: \\(O(n)\\)\n\nManhattan MSTs bridge grid geometry and graph optimization, the perfect example of structure guiding efficiency.\n\n\n\n350 Euclidean MST (Kruskal + Geometry)\nA Euclidean Minimum Spanning Tree (EMST) connects a set of points in the plane with the shortest total Euclidean length, the minimal possible wiring, pipeline, or connection layout under straight-line distances.\nIt is the geometric counterpart to classical MST problems, central to computational geometry, network design, and spatial clustering.\n\nWhat Problem Are We Solving?\nGiven \\(n\\) points \\(P = {p_1, p_2, \\dots, p_n}\\) in 2D (or higher dimensions), we want a spanning tree \\(T\\) minimizing\n\\[\n\\text{cost}(T) = \\sum_{(p_i,p_j) \\in T} \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}\n\\]\nUnlike abstract graph MSTs, here the graph is complete, every pair of points has an edge weighted by Euclidean distance — but we cannot afford \\(O(n^2)\\) edges for large \\(n\\).\n\n\nHow Does It Work (Plain Language)?\nThe key geometric insight:\n\nThe EMST is always a subgraph of the Delaunay Triangulation (DT).\n\nSo we don’t need all \\(\\binom{n}{2}\\) edges, only those in the Delaunay graph (which has \\(O(n)\\) edges).\nAlgorithm sketch:\n\nCompute the Delaunay Triangulation (DT) of the points.\nExtract all DT edges with their Euclidean weights.\nRun Kruskal’s or Prim’s algorithm on these edges.\nThe resulting tree is the EMST.\n\nThis drastically reduces time from quadratic to near-linear.\n\n\nTiny Code (Brute-Force Demonstration)\nHere’s a minimal version using all pairs, good for small \\(n\\):\nimport math\n\ndef euclidean_distance(p1, p2):\n    return math.sqrt((p1[0]-p2[0])2 + (p1[1]-p2[1])2)\n\ndef euclidean_mst(points):\n    edges = []\n    n = len(points)\n    for i in range(n):\n        for j in range(i+1, n):\n            w = euclidean_distance(points[i], points[j])\n            edges.append((i, j, w))\n    edges.sort(key=lambda e: e[2])\n\n    parent = [i for i in range(n)]\n    def find(x):\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n    def union(x, y):\n        rx, ry = find(x), find(y)\n        if rx != ry:\n            parent[ry] = rx\n            return True\n        return False\n\n    mst_edges = []\n    cost = 0\n    for u, v, w in edges:\n        if union(u, v):\n            mst_edges.append((u, v, w))\n            cost += w\n    return cost, mst_edges\n\npoints = [(0,0),(1,0),(0,1),(1,1)]\nprint(euclidean_mst(points))\nOutput:\n(3.0, [(0,1,1.0),(0,2,1.0),(1,3,1.0)])\nFor real use, replace the all-pairs loop with Delaunay edges from a geometry library.\n\n\nWhy It Matters\n\nOptimally connects points with minimum total length.\nUsed in geographic information systems, road networks, clustering (single-linkage), sensor networks, and pattern analysis.\nDemonstrates interplay between geometry and graph theory.\nFoundation for Steiner trees and TSP approximations.\n\n\n\nA Gentle Proof (Why It Works)\nThe EMST obeys the cut property, for every partition of points, the shortest connecting edge must be in the EMST.\nEdges outside the Delaunay Triangulation can be replaced by shorter edges crossing the same cut (from DT), so the EMST must lie within the DT.\nThus, we can safely restrict candidate edges to the Delaunay set, preserving optimality.\n\n\nTry It Yourself\n\nPlot points on a plane, draw EMST edges.\nCompare EMST vs MST with Manhattan distances.\nGenerate points in clusters, visualize how EMST bridges clusters.\nUse a geometry library (e.g. scipy.spatial.Delaunay) to build fast DT edges.\nMeasure runtime difference: brute-force vs DT-based.\n\n\n\nTest Cases\n\n\n\nPoints\nEMST\nTotal Length\n\n\n\n\nSquare corners\n(0,0),(1,0),(1,1),(0,1)\n3.0\n\n\nTriangle\n(0,0),(2,0),(1,√3)\n4.0\n\n\nRandom 5 points\n,\nDepends on coordinates\n\n\n\n\n\nComplexity\n\nBrute-force: \\(O(n^2 \\log n)\\)\nDelaunay-based: \\(O(n \\log n)\\)\nSpace: \\(O(n)\\)\n\nThe Euclidean MST is the geometric minimal backbone, the shortest invisible thread that ties all points together in the plane.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-36.-flows",
    "href": "books/en-us/list-4.html#section-36.-flows",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 36. Flows",
    "text": "Section 36. Flows\n\n351 Ford–Fulkerson\nThe Ford–Fulkerson method is the foundational algorithm for computing the maximum flow in a directed network. It views flow as water moving through a system of pipes, pushing flow from a source to a sink, respecting capacity limits and flow conservation.\n\nWhat Problem Are We Solving?\nGiven a flow network \\(G = (V, E)\\) with\n\na source \\(s\\),\na sink \\(t\\),\nand capacities \\(c(u, v)\\) on each edge,\n\nwe want to find a maximum flow \\(f(u, v)\\) such that:\n\nCapacity constraint: \\[0 \\le f(u, v) \\le c(u, v)\\]\nFlow conservation: \\[\\sum_{(u,v)\\in E} f(u,v) = \\sum_{(v,u)\\in E} f(v,u) \\quad \\forall u \\in V \\setminus {s, t}\\]\nMaximize total flow: \\[\n\\text{Maximize } |f| = \\sum_{(s,v)\\in E} f(s,v)\n\\]\n\n\n\nHow Does It Work (Plain Language)?\nThe algorithm repeatedly searches for augmenting paths, routes from \\(s\\) to \\(t\\) where additional flow can still be sent. Each iteration increases total flow until no augmenting path remains.\nStep-by-step:\n\nInitialize all flows to 0.\nWhile an augmenting path \\(P\\) exists from \\(s\\) to \\(t\\) in the residual graph:\n\nFind the bottleneck capacity along \\(P\\): \\[b = \\min_{(u,v)\\in P} (c(u,v) - f(u,v))\\]\nAugment flow along \\(P\\) by \\(b\\): \\[f(u,v) \\gets f(u,v) + b\\] \\[f(v,u) \\gets f(v,u) - b\\]\n\nUpdate residual capacities and repeat.\nWhen no path remains, current flow is maximum.\n\nThe residual graph represents remaining capacity, allowing backward edges to cancel flow if needed.\n\n\nTiny Code\nPython (DFS-based Augmentation)\nfrom collections import defaultdict\n\ndef ford_fulkerson(capacity, s, t):\n    n = len(capacity)\n    flow = [[0]*n for _ in range(n)]\n\n    def dfs(u, t, f, visited):\n        if u == t:\n            return f\n        visited[u] = True\n        for v in range(n):\n            residual = capacity[u][v] - flow[u][v]\n            if residual &gt; 0 and not visited[v]:\n                pushed = dfs(v, t, min(f, residual), visited)\n                if pushed &gt; 0:\n                    flow[u][v] += pushed\n                    flow[v][u] -= pushed\n                    return pushed\n        return 0\n\n    max_flow = 0\n    while True:\n        visited = [False]*n\n        pushed = dfs(s, t, float('inf'), visited)\n        if pushed == 0:\n            break\n        max_flow += pushed\n    return max_flow\n\ncapacity = [\n    [0, 16, 13, 0, 0, 0],\n    [0, 0, 10, 12, 0, 0],\n    [0, 4, 0, 0, 14, 0],\n    [0, 0, 9, 0, 0, 20],\n    [0, 0, 0, 7, 0, 4],\n    [0, 0, 0, 0, 0, 0]\n]\nprint(ford_fulkerson(capacity, 0, 5))\nOutput:\n23\n\n\nWhy It Matters\n\nIntroduces max-flow concept, a cornerstone of network optimization.\nUnderpins algorithms for:\n\nbipartite matching\nminimum cut problems\ncirculations and network design\n\nIllustrates residual graphs and augmenting paths, used in many flow-based algorithms.\n\n\n\nA Gentle Proof (Why It Works)\nEach augmentation increases total flow by a positive amount and maintains feasibility. Because total capacity is finite, the process eventually terminates.\nBy the Max-Flow Min-Cut Theorem:\n\\[\n|f^*| = \\text{capacity of minimum } (S, T) \\text{ cut}\n\\]\nSo when no augmenting path exists, the flow is maximum.\nIf all capacities are integers, the algorithm converges in finite steps, since each augmentation adds at least 1 unit.\n\n\nTry It Yourself\n\nRun on small networks, draw residual graph at each step.\nTrack augmenting paths and bottleneck edges.\nCompare to Edmonds–Karp (BFS search).\nChange capacities to fractional, observe potential infinite loops.\nUse to solve bipartite matching (convert to flow network).\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nNotes\n\n\n\n\nSimple 3-node\n5\nsingle augment\n\n\nClassic 6-node (above)\n23\ntextbook example\n\n\nParallel edges\nSum of capacities\nadditive\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\cdot |f^*|)\\) for integer capacities\nSpace: \\(O(V^2)\\)\nOptimized (Edmonds–Karp): \\(O(VE^2)\\)\n\nFord–Fulkerson builds the intuition of pushing flow through capacity-constrained paths, the heart of network optimization.\n\n\n\n352 Edmonds–Karp\nThe Edmonds–Karp algorithm is a concrete, polynomial-time implementation of the Ford–Fulkerson method, where each augmenting path is chosen using Breadth-First Search (BFS). By always selecting the shortest path (in edge count) from source to sink, it guarantees efficient convergence.\n\nWhat Problem Are We Solving?\nGiven a directed flow network \\(G = (V, E)\\) with\n\na source \\(s\\),\na sink \\(t\\),\nand nonnegative capacities \\(c(u, v)\\),\n\nwe want to find a maximum flow \\(f(u, v)\\) that satisfies:\n\nCapacity constraints: \\[0 \\le f(u, v) \\le c(u, v)\\]\nFlow conservation: \\[\\sum_{(u,v)\\in E} f(u,v) = \\sum_{(v,u)\\in E} f(v,u) \\quad \\forall u \\in V \\setminus {s, t}\\]\nMaximize total flow: \\[|f| = \\sum_{(s,v)\\in E} f(s,v)\\]\n\nThe algorithm improves Ford–Fulkerson by enforcing shortest augmenting path order, which bounds the number of iterations.\n\n\nHow Does It Work (Plain Language)?\nEach iteration finds an augmenting path using BFS (so each edge in the path is part of the shortest route in terms of hops). Then we push the maximum possible flow through that path and update residual capacities.\nStep-by-step:\n\nInitialize \\(f(u,v)=0\\) for all edges.\nWhile there exists a path \\(P\\) from \\(s\\) to \\(t\\) in the residual graph (found by BFS):\n\nCompute bottleneck capacity: \\[b = \\min_{(u,v)\\in P} (c(u,v) - f(u,v))\\]\nAugment along path \\(P\\): \\[f(u,v) \\gets f(u,v) + b\\] \\[f(v,u) \\gets f(v,u) - b\\]\n\nRepeat until no augmenting path remains.\n\nResidual graph is updated after each augmentation, including reverse edges to allow flow cancellation.\n\n\nTiny Code\nPython Implementation (BFS Augmentation)\nfrom collections import deque\n\ndef bfs(capacity, flow, s, t):\n    n = len(capacity)\n    parent = [-1] * n\n    parent[s] = s\n    q = deque([s])\n    while q:\n        u = q.popleft()\n        for v in range(n):\n            residual = capacity[u][v] - flow[u][v]\n            if residual &gt; 0 and parent[v] == -1:\n                parent[v] = u\n                q.append(v)\n                if v == t:\n                    return parent\n    return None\n\ndef edmonds_karp(capacity, s, t):\n    n = len(capacity)\n    flow = [[0]*n for _ in range(n)]\n    max_flow = 0\n\n    while True:\n        parent = bfs(capacity, flow, s, t)\n        if not parent:\n            break\n        # find bottleneck\n        v = t\n        bottleneck = float('inf')\n        while v != s:\n            u = parent[v]\n            bottleneck = min(bottleneck, capacity[u][v] - flow[u][v])\n            v = u\n        # augment flow\n        v = t\n        while v != s:\n            u = parent[v]\n            flow[u][v] += bottleneck\n            flow[v][u] -= bottleneck\n            v = u\n        max_flow += bottleneck\n\n    return max_flow\n\ncapacity = [\n    [0, 16, 13, 0, 0, 0],\n    [0, 0, 10, 12, 0, 0],\n    [0, 4, 0, 0, 14, 0],\n    [0, 0, 9, 0, 0, 20],\n    [0, 0, 0, 7, 0, 4],\n    [0, 0, 0, 0, 0, 0]\n]\nprint(edmonds_karp(capacity, 0, 5))\nOutput:\n23\n\n\nWhy It Matters\n\nEnsures polynomial time termination.\nDemonstrates the power of BFS for finding shortest augmenting paths.\nServes as the canonical maximum flow algorithm in theory and practice.\nProvides a clean proof of the Max-Flow Min-Cut Theorem.\nFoundation for more advanced methods (Dinic, Push–Relabel).\n\n\n\nA Gentle Proof (Why It Works)\nEach augmentation uses a shortest path in terms of edges. After each augmentation, at least one edge on that path saturates (reaches full capacity).\nBecause every edge can only move from residual distance \\(d\\) to \\(d+2\\) a limited number of times, the total number of augmentations is \\(O(VE)\\).\nEach BFS runs in \\(O(E)\\), giving total runtime \\(O(VE^2)\\).\nThe algorithm terminates when no path exists, i.e. the residual graph disconnects \\(s\\) and \\(t\\), and by the Max-Flow Min-Cut theorem, the current flow is maximum.\n\n\nTry It Yourself\n\nRun BFS after each iteration, visualize residual network.\nCompare with Ford–Fulkerson (DFS), note fewer augmentations.\nModify capacities, see how path selection changes.\nImplement path reconstruction and print augmenting paths.\nUse to solve bipartite matching via flow transformation.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nNotes\n\n\n\n\nSimple 3-node\n5\nBFS finds direct path\n\n\nClassic 6-node\n23\ntextbook example\n\n\nStar network\nSum of edge capacities\neach edge unique path\n\n\n\n\n\nComplexity\n\nTime: \\(O(VE^2)\\)\nSpace: \\(O(V^2)\\)\nAugmentations: \\(O(VE)\\)\n\nEdmonds–Karp transforms Ford–Fulkerson into a predictable, efficient, and elegant BFS-based flow engine, ensuring progress, bounding iterations, and revealing the structure of optimal flow.\n\n\n\n353 Dinic’s Algorithm\nDinic’s Algorithm (or Dinitz’s algorithm) is a faster approach to computing maximum flow, improving upon Edmonds–Karp by introducing a level graph and sending blocking flows within it. It combines BFS (to layer the graph) with DFS (to find augmenting paths), achieving a strong polynomial bound.\n\nWhat Problem Are We Solving?\nGiven a directed flow network \\(G = (V, E)\\) with\n\nsource \\(s\\),\nsink \\(t\\),\ncapacities \\(c(u,v)\\) on each edge,\n\nfind the maximum flow \\(f(u,v)\\) such that:\n\nCapacity constraint: \\[0 \\le f(u, v) \\le c(u, v)\\]\nFlow conservation: \\[\\sum_{(u,v)\\in E} f(u,v) = \\sum_{(v,u)\\in E} f(v,u) \\quad \\forall u \\in V \\setminus {s, t}\\]\nMaximize total flow: \\[|f| = \\sum_{(s,v)\\in E} f(s,v)\\]\n\n\n\nHow Does It Work (Plain Language)?\nDinic’s Algorithm operates in phases. Each phase constructs a level graph using BFS, then pushes as much flow as possible within that layered structure using DFS. This approach ensures progress, each phase strictly increases the shortest path length from \\(s\\) to \\(t\\) in the residual graph.\nStep-by-step:\n\nBuild Level Graph (BFS):\n\nRun BFS from \\(s\\).\nAssign each vertex a level = distance from \\(s\\) in residual graph.\nOnly edges \\((u,v)\\) with \\(level[v] = level[u] + 1\\) are used.\n\nSend Blocking Flow (DFS):\n\nUse DFS to push flow from \\(s\\) to \\(t\\) along level-respecting paths.\nStop when no more flow can be sent (i.e., blocking flow).\n\nRepeat:\n\nRebuild level graph; continue until \\(t\\) is unreachable from \\(s\\).\n\n\nA blocking flow saturates at least one edge on every \\(s\\)–\\(t\\) path in the level graph, ensuring termination per phase.\n\n\nTiny Code\nPython Implementation (Adjacency List)\nfrom collections import deque\n\nclass Dinic:\n    def __init__(self, n):\n        self.n = n\n        self.adj = [[] for _ in range(n)]\n\n    def add_edge(self, u, v, cap):\n        self.adj[u].append([v, cap, len(self.adj[v])])\n        self.adj[v].append([u, 0, len(self.adj[u]) - 1])  # reverse edge\n\n    def bfs(self, s, t, level):\n        q = deque([s])\n        level[s] = 0\n        while q:\n            u = q.popleft()\n            for v, cap, _ in self.adj[u]:\n                if cap &gt; 0 and level[v] &lt; 0:\n                    level[v] = level[u] + 1\n                    q.append(v)\n        return level[t] &gt;= 0\n\n    def dfs(self, u, t, flow, level, it):\n        if u == t:\n            return flow\n        while it[u] &lt; len(self.adj[u]):\n            v, cap, rev = self.adj[u][it[u]]\n            if cap &gt; 0 and level[v] == level[u] + 1:\n                pushed = self.dfs(v, t, min(flow, cap), level, it)\n                if pushed &gt; 0:\n                    self.adj[u][it[u]][1] -= pushed\n                    self.adj[v][rev][1] += pushed\n                    return pushed\n            it[u] += 1\n        return 0\n\n    def max_flow(self, s, t):\n        flow = 0\n        level = [-1] * self.n\n        INF = float('inf')\n        while self.bfs(s, t, level):\n            it = [0] * self.n\n            while True:\n                pushed = self.dfs(s, t, INF, level, it)\n                if pushed == 0:\n                    break\n                flow += pushed\n            level = [-1] * self.n\n        return flow\n\n# Example\ndinic = Dinic(6)\nedges = [\n    (0,1,16),(0,2,13),(1,2,10),(2,1,4),(1,3,12),\n    (3,2,9),(2,4,14),(4,3,7),(3,5,20),(4,5,4)\n]\nfor u,v,c in edges:\n    dinic.add_edge(u,v,c)\nprint(dinic.max_flow(0,5))\nOutput:\n23\n\n\nWhy It Matters\n\nAchieves \\(O(V^2E)\\) complexity (faster in practice).\nExploits layering to avoid redundant augmentations.\nBasis for advanced flow algorithms like Dinic + scaling, Push–Relabel, and Blocking Flow variants.\nCommon in competitive programming, network routing, and bipartite matching.\n\n\n\nA Gentle Proof (Why It Works)\nEach phase increases the shortest distance from \\(s\\) to \\(t\\) in the residual graph. Because there are at most \\(V-1\\) distinct distances, the algorithm runs for at most \\(O(V)\\) phases. Within each phase, we find a blocking flow, which can be computed in \\(O(E)\\) DFS calls.\nHence total runtime: \\[\nO(VE)\n\\] for unit networks (each edge capacity = 1), and in general, \\[\nO(V^2E)\n\\]\nWhen no augmenting path exists, residual graph disconnects \\(s\\) and \\(t\\), so the current flow is maximum by the Max-Flow Min-Cut theorem.\n\n\nTry It Yourself\n\nCompare BFS layers between phases, note increasing depth.\nVisualize level graph and residual edges.\nTest on bipartite graph, confirm match size = flow.\nModify to store flow per edge.\nAdd capacity scaling to speed up dense graphs.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nNotes\n\n\n\n\n6-node sample\n23\nclassic example\n\n\nUnit network\nequals #disjoint paths\n\n\n\nBipartite graph\nequals max matching\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(V^2E)\\) general, \\(O(\\sqrt{V}E)\\) for unit capacities\nSpace: \\(O(V + E)\\)\nAugmentations per phase: \\(O(E)\\)\n\nDinic’s algorithm elegantly combines BFS-level layering with DFS-based flow pushing, a perfect synthesis of structure and greed, powering modern flow computations.\n\n\n\n354 Push–Relabel\nThe Push–Relabel algorithm (also called Preflow–Push) takes a completely different view of the maximum flow problem. Instead of finding paths from the source to the sink, it locally pushes flow along edges and adjusts vertex heights (labels) to guide flow downhill toward the sink.\nThis approach is highly efficient in practice and forms the basis for many modern flow solvers.\n\nWhat Problem Are We Solving?\nGiven a directed network \\(G = (V, E)\\) with\n\nsource \\(s\\),\nsink \\(t\\),\ncapacity \\(c(u, v)\\),\n\nwe want a flow \\(f(u,v)\\) satisfying:\n\nCapacity constraint: \\[0 \\le f(u, v) \\le c(u, v)\\]\nFlow conservation: \\[\\sum_{(u,v)\\in E} f(u,v) = \\sum_{(v,u)\\in E} f(v,u) \\quad \\forall u \\neq s,t\\]\nMaximize total flow: \\[|f| = \\sum_{(s,v)\\in E} f(s,v)\\]\n\n\n\nHow Does It Work (Plain Language)?\nUnlike Ford–Fulkerson, which finds augmenting paths, Push–Relabel maintains a preflow (where intermediate nodes may have excess flow) and fixes imbalances gradually.\nKey concepts:\n\nPreflow: flow can temporarily violate conservation (nodes can have excess).\nHeight (label): an integer guiding flow direction, flow only moves “downhill.”\nPush: send flow from \\(u\\) to \\(v\\) if possible.\nRelabel: when \\(u\\) is stuck, increase its height so flow can continue.\n\nStep-by-step:\n\nInitialize all \\(f(u,v)=0\\).\nSet \\(h(s)=|V|\\) and push as much as possible from \\(s\\) to its neighbors.\nWhile any vertex (other than \\(s,t\\)) has excess flow, do:\n\nPush: if \\((u,v)\\) is admissible (\\(h(u)=h(v)+1\\) and residual \\(&gt;0\\)), send \\[\\Delta = \\min(e(u), c(u,v)-f(u,v))\\]\nRelabel: if no admissible edge, set \\[h(u) = 1 + \\min_{(u,v): c(u,v)-f(u,v)&gt;0} h(v)\\]\n\n\nRepeat until all excess is at \\(t\\) or \\(s\\).\n\n\nTiny Code\nPython Implementation (Simplified)\ndef push_relabel(capacity, s, t):\n    n = len(capacity)\n    flow = [[0]*n for _ in range(n)]\n    excess = [0]*n\n    height = [0]*n\n    height[s] = n\n\n    def push(u, v):\n        delta = min(excess[u], capacity[u][v] - flow[u][v])\n        flow[u][v] += delta\n        flow[v][u] -= delta\n        excess[u] -= delta\n        excess[v] += delta\n\n    def relabel(u):\n        min_h = float('inf')\n        for v in range(n):\n            if capacity[u][v] - flow[u][v] &gt; 0:\n                min_h = min(min_h, height[v])\n        if min_h &lt; float('inf'):\n            height[u] = min_h + 1\n\n    def discharge(u):\n        while excess[u] &gt; 0:\n            for v in range(n):\n                if capacity[u][v] - flow[u][v] &gt; 0 and height[u] == height[v] + 1:\n                    push(u, v)\n                    if excess[u] == 0:\n                        break\n            else:\n                relabel(u)\n\n    # Initialize preflow\n    for v in range(n):\n        if capacity[s][v] &gt; 0:\n            flow[s][v] = capacity[s][v]\n            flow[v][s] = -capacity[s][v]\n            excess[v] = capacity[s][v]\n    excess[s] = sum(capacity[s])\n\n    # Discharge active vertices\n    active = [i for i in range(n) if i != s and i != t]\n    p = 0\n    while p &lt; len(active):\n        u = active[p]\n        old_height = height[u]\n        discharge(u)\n        if height[u] &gt; old_height:\n            active.insert(0, active.pop(p))  # move to front\n            p = 0\n        else:\n            p += 1\n\n    return sum(flow[s])\n\ncapacity = [\n    [0,16,13,0,0,0],\n    [0,0,10,12,0,0],\n    [0,4,0,0,14,0],\n    [0,0,9,0,0,20],\n    [0,0,0,7,0,4],\n    [0,0,0,0,0,0]\n]\nprint(push_relabel(capacity, 0, 5))\nOutput:\n23\n\n\nWhy It Matters\n\nLocal view: No need for global augmenting paths.\nHighly parallelizable.\nPerforms very well in dense graphs.\nServes as the basis for highest-label and FIFO variants.\nConceptually elegant: flow “falls downhill” guided by heights.\n\n\n\nA Gentle Proof (Why It Works)\n\nThe height invariant ensures no flow moves from lower to higher vertices.\nEvery push respects capacity and non-negativity.\nHeight always increases, ensuring termination.\nWhen no vertex (except \\(s\\) and \\(t\\)) has excess, all preflow constraints are satisfied, the preflow becomes a valid maximum flow.\n\nBy the Max-Flow Min-Cut Theorem, the final preflow’s value equals the capacity of the minimum cut.\n\n\nTry It Yourself\n\nTrack each vertex’s height and excess after each step.\nCompare FIFO vs highest-label variants.\nUse small networks to visualize flow movement.\nContrast with path-based algorithms (Ford–Fulkerson).\nAdd logging to observe relabel events.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nNotes\n\n\n\n\nClassic 6-node\n23\ntextbook\n\n\nDense complete\nhigh flow\nefficient\n\n\nSparse path\nsame as Edmonds–Karp\nsimilar\n\n\n\n\n\nComplexity\n\nTime (Generic): \\(O(V^2E)\\)\nWith FIFO / Highest-Label Heuristics: \\(O(V^3)\\) or better\nSpace: \\(O(V^2)\\)\n\nPush–Relabel transforms max-flow into a local balancing act, pushing, relabeling, and equalizing pressure until equilibrium is achieved.\n\n\n\n355 Capacity Scaling\nThe Capacity Scaling algorithm is a refined version of Ford–Fulkerson, designed to handle large edge capacities efficiently. Instead of augmenting arbitrarily, it focuses on high-capacity edges first, gradually refining the flow as the scaling parameter decreases.\nThis approach reduces the number of augmentations by focusing early on “big pipes” before worrying about smaller ones.\n\nWhat Problem Are We Solving?\nGiven a directed flow network \\(G = (V, E)\\) with\n\nsource \\(s\\),\nsink \\(t\\),\nnonnegative capacities \\(c(u, v)\\),\n\nwe want to compute a maximum flow \\(f(u, v)\\) satisfying:\n\nCapacity constraint: \\[0 \\le f(u, v) \\le c(u, v)\\]\nFlow conservation: \\[\\sum_{(u,v)\\in E} f(u,v) = \\sum_{(v,u)\\in E} f(v,u), \\quad \\forall u \\neq s,t\\]\nMaximize: \\[|f| = \\sum_{(s,v)\\in E} f(s,v)\\]\n\nWhen capacities are large, standard augmenting-path methods can take many iterations. Capacity scaling reduces this by grouping edges by capacity magnitude.\n\n\nHow Does It Work (Plain Language)?\nWe introduce a scaling parameter \\(\\Delta\\), starting from the highest power of 2 below the maximum capacity. We only consider edges with residual capacity ≥ Δ during augmentations. Once no such path exists, halve \\(\\Delta\\) and continue.\nThis ensures we push large flows early and refine later.\nStep-by-step:\n\nInitialize \\(f(u,v) = 0\\).\nLet \\[\\Delta = 2^{\\lfloor \\log_2 C_{\\max} \\rfloor}\\] where \\(C_{\\max} = \\max_{(u,v)\\in E} c(u,v)\\)\nWhile \\(\\Delta \\ge 1\\):\n\nBuild Δ-residual graph: edges with residual capacity \\(\\ge \\Delta\\)\nWhile an augmenting path exists in this graph:\n\nSend flow equal to the bottleneck along that path\n\nUpdate \\(\\Delta \\gets \\Delta / 2\\)\n\nReturn total flow.\n\n\n\nTiny Code\nPython (DFS Augmentation with Scaling)\ndef dfs(u, t, f, visited, capacity, flow, delta):\n    if u == t:\n        return f\n    visited[u] = True\n    for v in range(len(capacity)):\n        residual = capacity[u][v] - flow[u][v]\n        if residual &gt;= delta and not visited[v]:\n            pushed = dfs(v, t, min(f, residual), visited, capacity, flow, delta)\n            if pushed &gt; 0:\n                flow[u][v] += pushed\n                flow[v][u] -= pushed\n                return pushed\n    return 0\n\ndef capacity_scaling(capacity, s, t):\n    n = len(capacity)\n    flow = [[0]*n for _ in range(n)]\n    Cmax = max(max(row) for row in capacity)\n    delta = 1\n    while delta * 2 &lt;= Cmax:\n        delta *= 2\n\n    max_flow = 0\n    while delta &gt;= 1:\n        while True:\n            visited = [False]*n\n            pushed = dfs(s, t, float('inf'), visited, capacity, flow, delta)\n            if pushed == 0:\n                break\n            max_flow += pushed\n        delta //= 2\n    return max_flow\n\ncapacity = [\n    [0, 16, 13, 0, 0, 0],\n    [0, 0, 10, 12, 0, 0],\n    [0, 4, 0, 0, 14, 0],\n    [0, 0, 9, 0, 0, 20],\n    [0, 0, 0, 7, 0, 4],\n    [0, 0, 0, 0, 0, 0]\n]\nprint(capacity_scaling(capacity, 0, 5))\nOutput:\n23\n\n\nWhy It Matters\n\nReduces augmentations compared to plain Ford–Fulkerson.\nFocuses on big pushes first, improving convergence.\nDemonstrates the power of scaling ideas, a recurring optimization in algorithm design.\nServes as a stepping stone to cost scaling and capacity scaling in min-cost flow.\n\n\n\nA Gentle Proof (Why It Works)\nAt each scaling phase \\(\\Delta\\),\n\nEach augmenting path increases flow by at least \\(\\Delta\\).\nTotal flow value is at most \\(|f^*|\\).\nTherefore, each phase performs at most \\(O(|f^*| / \\Delta)\\) augmentations.\n\nSince \\(\\Delta\\) halves each phase, the total number of augmentations is \\[\nO(E \\log C_{\\max})\n\\]\nEach path search takes \\(O(E)\\), so \\[\nT = O(E^2 \\log C_{\\max})\n\\]\nTermination occurs when \\(\\Delta = 1\\) and no path remains, meaning max flow reached.\n\n\nTry It Yourself\n\nCompare augmentation order with Ford–Fulkerson.\nTrack \\(\\Delta\\) values and residual graphs per phase.\nObserve faster convergence for large capacities.\nCombine with BFS to mimic Edmonds–Karp structure.\nVisualize flow accumulation at each scale.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nNotes\n\n\n\n\nClassic 6-node\n23\ntextbook example\n\n\nLarge capacity edges\nsame flow, fewer steps\nscaling helps\n\n\nUnit capacities\nbehaves like Edmonds–Karp\nsmall gains\n\n\n\n\n\nComplexity\n\nTime: \\(O(E^2 \\log C_{\\max})\\)\nSpace: \\(O(V^2)\\)\nAugmentations: \\(O(E \\log C_{\\max})\\)\n\nCapacity scaling embodies a simple but powerful idea: solve coarse first, refine later, push the big flows early, and polish at the end.\n\n\n\n356 Cost Scaling\nThe Cost Scaling algorithm tackles the Minimum-Cost Maximum Flow (MCMF) problem by applying a scaling technique not to capacities but to edge costs. It gradually refines the precision of reduced costs, maintaining ε-optimality throughout, and converges to the true optimal flow.\nThis approach is both theoretically elegant and practically efficient, forming the foundation for high-performance network optimization solvers.\n\nWhat Problem Are We Solving?\nGiven a directed network \\(G = (V, E)\\) with\n\ncapacity \\(c(u, v) \\ge 0\\),\ncost (per unit flow) \\(w(u, v)\\),\nsource \\(s\\), and sink \\(t\\),\n\nwe want to send the maximum flow from \\(s\\) to \\(t\\) at minimum total cost.\nWe minimize: \\[\n\\text{Cost}(f) = \\sum_{(u,v)\\in E} f(u,v), w(u,v)\n\\]\nsubject to:\n\nCapacity constraint: \\(0 \\le f(u,v) \\le c(u,v)\\)\nConservation: \\(\\sum_v f(u,v) = \\sum_v f(v,u)\\) for all \\(u \\neq s,t\\)\n\n\n\nHow Does It Work (Plain Language)?\nCost scaling uses successive refinements of reduced costs, ensuring flows are ε-optimal at each phase.\nAn ε-optimal flow satisfies: \\[\nc_p(u,v) = w(u,v) + \\pi(u) - \\pi(v) \\ge -\\varepsilon\n\\] for all residual edges \\((u,v)\\), where \\(\\pi\\) is a potential function.\nThe algorithm begins with a large \\(\\varepsilon\\) (often \\(C_{\\max}\\) or \\(W_{\\max}\\)) and reduces it geometrically (e.g. \\(\\varepsilon \\gets \\varepsilon / 2\\)). During each phase, pushes are allowed only along admissible edges (\\(c_p(u,v) &lt; 0\\)), maintaining ε-optimality.\nStep-by-step:\n\nInitialize preflow (push as much as possible from \\(s\\)).\nSet \\(\\varepsilon = C_{\\max}\\).\nWhile \\(\\varepsilon \\ge 1\\):\n\nMaintain ε-optimality: adjust flows and potentials.\nFor each vertex with excess,\n\nPush flow along admissible edges (\\(c_p(u,v) &lt; 0\\)).\nIf stuck, relabel by increasing \\(\\pi(u)\\) (lowering reduced costs).\n\nHalve \\(\\varepsilon\\).\n\nWhen \\(\\varepsilon &lt; 1\\), solution is optimal.\n\n\n\nTiny Code (Simplified Skeleton)\nBelow is a conceptual outline (not a full implementation) for educational clarity:\ndef cost_scaling_mcmf(V, edges, s, t):\n    INF = float('inf')\n    adj = [[] for _ in range(V)]\n    for u, v, cap, cost in edges:\n        adj[u].append([v, cap, cost, len(adj[v])])\n        adj[v].append([u, 0, -cost, len(adj[u]) - 1])\n\n    pi = [0]*V  # potentials\n    excess = [0]*V\n    excess[s] = INF\n\n    def reduced_cost(u, v, cost):\n        return cost + pi[u] - pi[v]\n\n    epsilon = max(abs(c) for _,_,_,c in edges)\n    while epsilon &gt;= 1:\n        active = [i for i in range(V) if excess[i] &gt; 0 and i != s and i != t]\n        while active:\n            u = active.pop()\n            for e in adj[u]:\n                v, cap, cost, rev = e\n                rc = reduced_cost(u, v, cost)\n                if cap &gt; 0 and rc &lt; 0:\n                    delta = min(cap, excess[u])\n                    e[1] -= delta\n                    adj[v][rev][1] += delta\n                    excess[u] -= delta\n                    excess[v] += delta\n                    if v not in (s, t) and excess[v] == delta:\n                        active.append(v)\n            if excess[u] &gt; 0:\n                pi[u] += epsilon\n        epsilon //= 2\n\n    total_cost = 0\n    for u in range(V):\n        for v, cap, cost, rev in adj[u]:\n            if cost &gt; 0:\n                total_cost += cost * (adj[v][rev][1])\n    return total_cost\n\n\nWhy It Matters\n\nAvoids expensive shortest-path searches at each step.\nEnsures strong polynomial bounds using ε-optimality.\nSuitable for dense graphs and large integer costs.\nForms the backbone of network simplex and scalable MCMF solvers.\n\n\n\nA Gentle Proof (Why It Works)\n\nε-optimality: ensures approximate optimality at each scaling phase.\nScaling: reduces ε geometrically, converging to exact optimality.\nTermination: when \\(\\varepsilon &lt; 1\\), all reduced costs are nonnegative, the flow is optimal.\n\nEach push respects capacity and admissibility; each relabel decreases ε, guaranteeing progress.\n\n\nTry It Yourself\n\nCompare to Successive Shortest Path algorithm, note fewer path searches.\nTrack how potentials \\(\\pi(u)\\) evolve over phases.\nVisualize ε-layers of admissible edges.\nExperiment with cost magnitudes, larger costs benefit more from scaling.\nObserve convergence as ε halves each phase.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nMin Cost\nNotes\n\n\n\n\nSimple 3-node\n5\n10\nsmall example\n\n\nClassic network\n23\n42\ncost-scaled\n\n\nDense graph\nvaries\nefficient\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(E \\log C_{\\max} (V + E))\\) (depends on cost range)\nSpace: \\(O(V + E)\\)\nPhases: \\(O(\\log C_{\\max})\\)\n\nCost scaling showcases the precision-refinement paradigm, start coarse, end exact. It’s a masterclass in combining scaling, potential functions, and local admissibility to achieve globally optimal flows.\n\n\n\n357 Min-Cost Max-Flow (Bellman-Ford)\nThe Min-Cost Max-Flow (MCMF) algorithm with Bellman-Ford is a cornerstone method for solving flow problems where both capacity and cost constraints matter. It repeatedly augments flow along the shortest-cost paths, ensuring every unit of flow moves as cheaply as possible.\nThis version uses Bellman-Ford to handle negative edge costs, ensuring correctness even when cycles reduce total cost.\n\nWhat Problem Are We Solving?\nGiven a directed graph \\(G = (V, E)\\) with:\n\nCapacity \\(c(u, v)\\)\nCost per unit flow \\(w(u, v)\\)\nSource \\(s\\) and sink \\(t\\)\n\nFind a flow \\(f(u, v)\\) that:\n\nRespects capacities: \\(0 \\le f(u, v) \\le c(u, v)\\)\nConserves flow: \\(\\sum_v f(u, v) = \\sum_v f(v, u)\\) for all \\(u \\neq s,t\\)\nMaximizes total flow, while\nMinimizing total cost: \\[\n\\text{Cost}(f) = \\sum_{(u,v)\\in E} f(u,v),w(u,v)\n\\]\n\n\n\nHow Does It Work (Plain Language)?\nThe algorithm iteratively finds augmenting paths from \\(s\\) to \\(t\\) using shortest path search by cost (not distance).\nAt each iteration:\n\nRun Bellman-Ford on the residual graph to find the shortest-cost path.\nDetermine the bottleneck capacity \\(\\delta\\) on that path.\nAugment flow along that path by \\(\\delta\\).\nUpdate residual capacities and reverse edges.\nRepeat until no more augmenting paths exist.\n\nBecause Bellman-Ford supports negative costs, this algorithm correctly handles graphs where cost reductions occur via cycles.\n\n\nTiny Code (C-like Pseudocode)\nstruct Edge { int v, cap, cost, rev; };\nvector&lt;Edge&gt; adj[V];\nint dist[V], parent[V], parent_edge[V];\n\nbool bellman_ford(int s, int t, int V) {\n    fill(dist, dist+V, INF);\n    dist[s] = 0;\n    bool updated = true;\n    for (int i = 0; i &lt; V-1 && updated; i++) {\n        updated = false;\n        for (int u = 0; u &lt; V; u++) {\n            if (dist[u] == INF) continue;\n            for (int k = 0; k &lt; adj[u].size(); k++) {\n                auto &e = adj[u][k];\n                if (e.cap &gt; 0 && dist[e.v] &gt; dist[u] + e.cost) {\n                    dist[e.v] = dist[u] + e.cost;\n                    parent[e.v] = u;\n                    parent_edge[e.v] = k;\n                    updated = true;\n                }\n            }\n        }\n    }\n    return dist[t] &lt; INF;\n}\n\npair&lt;int,int&gt; min_cost_max_flow(int s, int t, int V) {\n    int flow = 0, cost = 0;\n    while (bellman_ford(s, t, V)) {\n        int f = INF;\n        for (int v = t; v != s; v = parent[v]) {\n            int u = parent[v];\n            auto &e = adj[u][parent_edge[v]];\n            f = min(f, e.cap);\n        }\n        for (int v = t; v != s; v = parent[v]) {\n            int u = parent[v];\n            auto &e = adj[u][parent_edge[v]];\n            e.cap -= f;\n            adj[v][e.rev].cap += f;\n            cost += f * e.cost;\n        }\n        flow += f;\n    }\n    return {flow, cost};\n}\n\n\nWhy It Matters\n\nHandles negative edge costs safely.\nGuaranteed optimality if no negative cycles exist.\nWorks well for small to medium graphs.\nA foundation for more efficient variants (e.g. SPFA, Dijkstra with potentials).\n\nIt’s the go-to teaching implementation for min-cost flow, balancing clarity and correctness.\n\n\nA Gentle Proof (Why It Works)\nEach iteration finds a shortest-cost augmenting path using Bellman-Ford. Because edge costs are non-decreasing over augmentations, the algorithm converges to a globally optimal flow.\nEach augmentation:\n\nIncreases flow.\nDoes not introduce cheaper paths later (monotonicity).\nTerminates when no more augmenting paths exist.\n\nThus, the resulting flow is both maximal and minimum cost.\n\n\nTry It Yourself\n\nDraw a simple graph with 4 nodes and a negative edge.\nTrace residual updates after each augmentation.\nCompare results with a naive greedy path selection.\nReplace Bellman-Ford with Dijkstra + potentials to improve speed.\nVisualize residual capacity evolution.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nMax Flow\nMin Cost\nNotes\n\n\n\n\nChain \\(s \\to a \\to b \\to t\\)\n5\n10\nSimple path\n\n\nGraph with negative cycle\nInvalid\n-\nMust detect\n\n\nGrid-like network\n12\n24\nMultiple augmentations\n\n\n\n\n\nComplexity\n\nTime: \\(O(F \\cdot V \\cdot E)\\), where \\(F\\) is total flow sent.\nSpace: \\(O(V + E)\\)\n\nFor small graphs or graphs with negative weights, Bellman-Ford MCMF is the most robust and straightforward method, clear, reliable, and foundational.\n\n\n\n358 Min-Cost Max-Flow (SPFA)\nThe Min-Cost Max-Flow algorithm using SPFA (Shortest Path Faster Algorithm) is an optimization of the Bellman–Ford approach. It leverages a queue-based relaxation method to find the shortest-cost augmenting paths more efficiently in practice, especially on sparse graphs or when negative edges are rare.\n\nWhat Problem Are We Solving?\nWe are solving the minimum-cost maximum-flow problem: Find a flow \\(f\\) in a directed graph \\(G = (V, E)\\) with:\n\nCapacity \\(c(u, v) \\ge 0\\)\nCost per unit flow \\(w(u, v)\\)\nSource \\(s\\), Sink \\(t\\)\n\nSubject to:\n\nCapacity constraint: \\(0 \\le f(u, v) \\le c(u, v)\\)\nConservation of flow: \\(\\sum_v f(u,v) = \\sum_v f(v,u)\\) for all \\(u \\neq s,t\\)\nObjective: \\[\n\\min \\text{Cost}(f) = \\sum_{(u,v)\\in E} f(u,v),w(u,v)\n\\]\n\nWe seek a maximum flow from \\(s\\) to \\(t\\) that incurs the minimum total cost.\n\n\nHow Does It Work (Plain Language)?\nSPFA finds shortest paths by cost more efficiently than Bellman–Ford, using a queue to relax only the vertices that can still improve distances.\nIn each iteration:\n\nRun SPFA on the residual graph to find the shortest-cost path.\nCompute the bottleneck flow \\(\\delta\\) on that path.\nAugment \\(\\delta\\) units of flow along the path.\nUpdate residual capacities and reverse edges.\nRepeat until no augmenting path remains.\n\nSPFA dynamically manages which nodes are in the queue, making it faster on average than full Bellman–Ford (though worst-case still similar).\n\n\nTiny Code (C-like Pseudocode)\nstruct Edge { int v, cap, cost, rev; };\nvector&lt;Edge&gt; adj[V];\nint dist[V], parent[V], parent_edge[V];\nbool in_queue[V];\n\nbool spfa(int s, int t, int V) {\n    fill(dist, dist+V, INF);\n    fill(in_queue, in_queue+V, false);\n    queue&lt;int&gt; q;\n    dist[s] = 0;\n    q.push(s);\n    in_queue[s] = true;\n\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        in_queue[u] = false;\n        for (int k = 0; k &lt; adj[u].size(); k++) {\n            auto &e = adj[u][k];\n            if (e.cap &gt; 0 && dist[e.v] &gt; dist[u] + e.cost) {\n                dist[e.v] = dist[u] + e.cost;\n                parent[e.v] = u;\n                parent_edge[e.v] = k;\n                if (!in_queue[e.v]) {\n                    q.push(e.v);\n                    in_queue[e.v] = true;\n                }\n            }\n        }\n    }\n    return dist[t] &lt; INF;\n}\n\npair&lt;int,int&gt; min_cost_max_flow(int s, int t, int V) {\n    int flow = 0, cost = 0;\n    while (spfa(s, t, V)) {\n        int f = INF;\n        for (int v = t; v != s; v = parent[v]) {\n            int u = parent[v];\n            auto &e = adj[u][parent_edge[v]];\n            f = min(f, e.cap);\n        }\n        for (int v = t; v != s; v = parent[v]) {\n            int u = parent[v];\n            auto &e = adj[u][parent_edge[v]];\n            e.cap -= f;\n            adj[v][e.rev].cap += f;\n            cost += f * e.cost;\n        }\n        flow += f;\n    }\n    return {flow, cost};\n}\n\n\nWhy It Matters\n\nPractical speedup over Bellman–Ford, especially in sparse networks.\nHandles negative edges safely (no negative cycles).\nWidely used in competitive programming and network optimization.\nEasier to implement than Dijkstra-based variants.\n\nSPFA combines the simplicity of Bellman–Ford with real-world efficiency, often reducing redundant relaxations dramatically.\n\n\nA Gentle Proof (Why It Works)\nEach SPFA run computes a shortest-cost augmenting path. Augmenting along that path ensures:\n\nThe flow remains feasible,\nThe cost strictly decreases, and\nThe process terminates after a finite number of augmentations (bounded by total flow).\n\nBecause SPFA always respects shortest-path distances, the final flow is cost-optimal.\n\n\nTry It Yourself\n\nReplace Bellman–Ford in your MCMF with SPFA.\nCompare runtime on sparse vs dense graphs.\nAdd an edge with negative cost and verify correct behavior.\nVisualize queue contents after each relaxation.\nMeasure how many times each vertex is processed.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nMin Cost\nNotes\n\n\n\n\nChain graph\n4\n12\nSimple\n\n\nSparse DAG\n10\n25\nSPFA efficient\n\n\nDense graph\n15\n40\nSimilar to Bellman–Ford\n\n\n\n\n\nComplexity\n\nTime: \\(O(F \\cdot E)\\) on average, \\(O(F \\cdot V \\cdot E)\\) worst-case\nSpace: \\(O(V + E)\\)\n\nSPFA-based MCMF blends clarity, practical efficiency, and negative-cost support, making it a favorite for real-world min-cost flow implementations.\n\n\n\n359 Circulation with Demands\nThe Circulation with Demands problem is a generalization of the classic flow and min-cost flow problems. Instead of having just a single source and sink, every node can demand or supply a certain amount of flow. Your goal is to find a feasible circulation—a flow that satisfies all node demands and capacity limits.\nThis formulation unifies network flow, feasibility checking, and optimization under one elegant model.\n\nWhat Problem Are We Solving?\nGiven a directed graph \\(G = (V, E)\\), each edge \\((u,v)\\) has:\n\nLower bound \\(l(u,v)\\)\nUpper bound (capacity) \\(c(u,v)\\)\nCost \\(w(u,v)\\)\n\nEach vertex \\(v\\) may have a demand \\(b(v)\\) (positive means it needs inflow, negative means it provides outflow).\nWe want to find a circulation (a flow satisfying \\(f(u,v)\\) for all edges) such that:\n\nCapacity constraints: \\[\nl(u,v) \\le f(u,v) \\le c(u,v)\n\\]\nFlow conservation: \\[\n\\sum_{(u,v)\\in E} f(u,v) - \\sum_{(v,u)\\in E} f(v,u) = b(v)\n\\]\nOptional cost minimization: \\[\n\\min \\sum_{(u,v)\\in E} f(u,v),w(u,v)\n\\]\n\nA feasible circulation exists if all demands can be met simultaneously.\n\n\nHow Does It Work (Plain Language)?\nThe trick is to transform this into a standard min-cost flow problem with a super-source and super-sink.\nStep-by-step transformation:\n\nFor each edge \\((u,v)\\) with lower bound \\(l(u,v)\\):\n\nReduce the capacity to \\((c(u,v) - l(u,v))\\).\nSubtract \\(l(u,v)\\) from the demands: \\[\nb(u) \\mathrel{{-}{=}} l(u,v), \\quad b(v) \\mathrel{{+}{=}} l(u,v)\n\\]\n\nAdd a super-source \\(S\\) and super-sink \\(T\\):\n\nFor each node \\(v\\):\n\nIf \\(b(v) &gt; 0\\), add edge \\((S,v)\\) with capacity \\(b(v)\\).\nIf \\(b(v) &lt; 0\\), add edge \\((v,T)\\) with capacity \\(-b(v)\\).\n\n\nSolve Max Flow (or Min-Cost Flow) from \\(S\\) to \\(T\\).\nIf the max flow = total demand, a feasible circulation exists. Otherwise, no circulation satisfies all constraints.\n\n\n\nTiny Code (Simplified Example)\ndef circulation_with_demands(V, edges, demand):\n    # edges: (u, v, lower, cap, cost)\n    # demand: list of node demands b(v)\n\n    adj = [[] for _ in range(V + 2)]\n    S, T = V, V + 1\n    b = demand[:]\n\n    for u, v, low, cap, cost in edges:\n        cap -= low\n        b[u] -= low\n        b[v] += low\n        adj[u].append((v, cap, cost))\n        adj[v].append((u, 0, -cost))\n\n    total_demand = 0\n    for i in range(V):\n        if b[i] &gt; 0:\n            adj[S].append((i, b[i], 0))\n            total_demand += b[i]\n        elif b[i] &lt; 0:\n            adj[i].append((T, -b[i], 0))\n\n    flow, cost = min_cost_max_flow(adj, S, T)\n    if flow == total_demand:\n        print(\"Feasible circulation found\")\n    else:\n        print(\"No feasible circulation\")\n\n\nWhy It Matters\n\nUnifies multiple problems: Many flow formulations—like assignment, transportation, and scheduling—can be expressed as circulations with demands.\nHandles lower bounds elegantly: Standard flows can’t directly enforce \\(f(u,v) \\ge l(u,v)\\). Circulations fix that.\nFoundation for advanced models: Cost constraints, multi-commodity flows, and balance equations all build on this.\n\n\n\nA Gentle Proof (Why It Works)\nBy enforcing lower bounds, we adjust the net balance of each vertex. After normalization, the system becomes equivalent to finding a flow from super-source to super-sink satisfying all balances.\nIf such a flow exists, we can reconstruct the original flow: \\[\nf'(u,v) = f(u,v) + l(u,v)\n\\] which satisfies all original constraints.\n\n\nTry It Yourself\n\nModel a supply-demand network (e.g., factories → warehouses → stores).\nAdd lower bounds to enforce minimum delivery.\nIntroduce node demands to balance supply/consumption.\nSolve with min-cost flow and verify circulation.\nRemove cost to just check feasibility.\n\n\n\nTest Cases\n\n\n\nNetwork\nResult\nNotes\n\n\n\n\nBalanced 3-node\nFeasible\nSimple supply-demand\n\n\nLower bounds exceed total\nInfeasible\nNo solution\n\n\nMixed positive/negative demands\nFeasible\nRequires adjustment\n\n\n\n\n\nComplexity\n\nTime: depends on flow solver used (\\(O(E^2V)\\) for Bellman–Ford variant)\nSpace: \\(O(V + E)\\)\n\nCirculation with demands gives a universal framework: any linear flow constraint can be encoded, checked, and optimized—turning balance equations into solvable graph problems.\n\n\n\n360 Successive Shortest Path\nThe Successive Shortest Path (SSP) algorithm is a clean and intuitive way to solve the Minimum-Cost Maximum-Flow (MCMF) problem. It builds the optimal flow incrementally, one shortest path at a time, always sending flow along the cheapest available route until capacity or balance constraints stop it.\n\nWhat Problem Are We Solving?\nGiven a directed graph \\(G = (V, E)\\) with:\n\nCapacity: \\(c(u,v)\\)\nCost per unit flow: \\(w(u,v)\\)\nSource: \\(s\\) and sink: \\(t\\)\n\nWe want to send the maximum flow from \\(s\\) to \\(t\\) with minimum total cost:\n\\[\n\\min \\sum_{(u,v)\\in E} f(u,v), w(u,v)\n\\]\nsubject to:\n\n\\(0 \\le f(u,v) \\le c(u,v)\\)\nFlow conservation at each vertex except \\(s,t\\)\n\n\n\nHow Does It Work (Plain Language)?\nSSP proceeds by repeatedly finding the shortest-cost augmenting path from \\(s\\) to \\(t\\) in the residual graph (cost as weight). It then pushes as much flow as possible along that path. Residual edges track available capacity (forward and backward).\nAlgorithm Steps:\n\nInitialize flow \\(f(u,v) = 0\\) for all edges.\nBuild residual graph with edge costs \\(w(u,v)\\).\nWhile there exists a path \\(P\\) from \\(s\\) to \\(t\\):\n\nFind shortest path \\(P\\) by cost (using Dijkstra or Bellman–Ford).\nCompute bottleneck capacity \\(\\delta = \\min_{(u,v)\\in P} c_f(u,v)\\).\nAugment flow along \\(P\\): \\[\nf(u,v) \\mathrel{{+}{=}} \\delta,\\quad f(v,u) \\mathrel{{-}{=}} \\delta\n\\]\nUpdate residual capacities and costs.\n\nRepeat until no augmenting path remains.\nResulting \\(f\\) is the min-cost max-flow.\n\nIf negative edges exist, use Bellman–Ford; otherwise Dijkstra with potentials for efficiency.\n\n\nTiny Code (Simplified Python)\nfrom heapq import heappush, heappop\n\ndef successive_shortest_path(V, edges, s, t):\n    adj = [[] for _ in range(V)]\n    for u, v, cap, cost in edges:\n        adj[u].append([v, cap, cost, len(adj[v])])\n        adj[v].append([u, 0, -cost, len(adj[u]) - 1])\n\n    INF = 109\n    pi = [0]*V  # potentials for reduced cost\n    flow = cost = 0\n\n    while True:\n        dist = [INF]*V\n        parent = [-1]*V\n        parent_edge = [-1]*V\n        dist[s] = 0\n        pq = [(0, s)]\n        while pq:\n            d, u = heappop(pq)\n            if d &gt; dist[u]: continue\n            for i, (v, cap, w, rev) in enumerate(adj[u]):\n                if cap &gt; 0 and dist[v] &gt; dist[u] + w + pi[u] - pi[v]:\n                    dist[v] = dist[u] + w + pi[u] - pi[v]\n                    parent[v] = u\n                    parent_edge[v] = i\n                    heappush(pq, (dist[v], v))\n        if dist[t] == INF:\n            break\n        for v in range(V):\n            if dist[v] &lt; INF:\n                pi[v] += dist[v]\n        f = INF\n        v = t\n        while v != s:\n            u = parent[v]\n            e = adj[u][parent_edge[v]]\n            f = min(f, e[1])\n            v = u\n        v = t\n        while v != s:\n            u = parent[v]\n            i = parent_edge[v]\n            e = adj[u][i]\n            e[1] -= f\n            adj[v][e[3]][1] += f\n            cost += f * e[2]\n            v = u\n        flow += f\n    return flow, cost\n\n\nWhy It Matters\n\nSimple and clear logic, augment along cheapest path each time.\nOptimal solution for min-cost max-flow when no negative cycles exist.\nCan handle large graphs efficiently with reduced costs and potentials.\nForms the basis for cost-scaling and network simplex methods.\n\n\n\nA Gentle Proof (Why It Works)\nEach augmentation moves flow along a shortest path (minimum reduced cost). Reduced costs ensure no negative cycles arise, so each augmentation preserves optimality. Once no augmenting path exists, all reduced costs are nonnegative and \\(f\\) is optimal.\nPotentials \\(\\pi(v)\\) guarantee Dijkstra finds true shortest paths in transformed graph: \\[\nw'(u,v) = w(u,v) + \\pi(u) - \\pi(v)\n\\] maintaining equivalence and non-negativity.\n\n\nTry It Yourself\n\nUse a simple 4-node network and trace each path’s cost and flow.\nCompare augmentations using Bellman–Ford vs Dijkstra.\nAdd negative edge costs and test with potential adjustments.\nVisualize residual graphs after each iteration.\nMeasure cost convergence per iteration.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nMin Cost\nNotes\n\n\n\n\nSimple 3-node\n5\n10\none path\n\n\nTwo parallel paths\n10\n15\nchooses cheapest\n\n\nWith negative edge\n7\n5\npotentials fix costs\n\n\n\n\n\nComplexity\n\nTime: \\(O(F \\cdot E \\log V)\\) using Dijkstra + potentials\nSpace: \\(O(V + E)\\)\n\nThe successive shortest path algorithm is a workhorse for costed flow problems: easy to code, provably correct, and efficient with the right data structures.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-37.-cuts",
    "href": "books/en-us/list-4.html#section-37.-cuts",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 37. Cuts",
    "text": "Section 37. Cuts\n\n361 Stoer–Wagner Minimum Cut\nThe Stoer–Wagner Minimum Cut algorithm finds the global minimum cut in an undirected weighted graph. A cut is a partition of vertices into two non-empty sets; its weight is the sum of edges crossing the partition. The algorithm efficiently discovers the cut with minimum total edge weight, using repeated maximum adjacency searches.\n\nWhat Problem Are We Solving?\nGiven an undirected graph \\(G = (V, E)\\) with non-negative weights \\(w(u,v)\\), find a cut \\((S, V \\setminus S)\\) such that:\n\\[\n\\text{cut}(S) = \\sum_{u \\in S, v \\in V \\setminus S} w(u,v)\n\\]\nis minimized over all nontrivial partitions \\(S \\subset V\\).\nThis is called the global minimum cut, distinct from the s–t minimum cut, which fixes two endpoints.\n\n\nHow Does It Work (Plain Language)?\nThe algorithm iteratively merges vertices while keeping track of the tightest cut found along the way.\nAt each phase:\n\nPick an arbitrary starting vertex.\nGrow a set \\(A\\) by repeatedly adding the most strongly connected vertex (highest weight to \\(A\\)).\nContinue until all vertices are added.\nThe last added vertex \\(t\\) and the second last \\(s\\) define a cut \\((A \\setminus {t}, {t})\\).\nRecord the cut weight, it’s a candidate for the minimum cut.\nMerge \\(s\\) and \\(t\\) into a single vertex.\nRepeat until only one vertex remains.\n\nThe smallest recorded cut across all phases is the global minimum cut.\n\n\nStep-by-Step Example\nSuppose we have 4 vertices \\(A, B, C, D\\). Each phase:\n\nStart with \\(A\\).\nAdd vertex most connected to \\(A\\).\nContinue until only one vertex remains.\nRecord cut weight each time the last vertex is added.\nMerge last two vertices, repeat.\n\nAfter all merges, the lightest cut weight is the minimum cut value.\n\n\nTiny Code (Python)\ndef stoer_wagner_min_cut(V, weight):\n    n = V\n    best = float('inf')\n    vertices = list(range(n))\n\n    while n &gt; 1:\n        used = [False] * n\n        weights = [0] * n\n        prev = -1\n        for i in range(n):\n            # select most connected vertex not yet in A\n            sel = -1\n            for j in range(n):\n                if not used[j] and (sel == -1 or weights[j] &gt; weights[sel]):\n                    sel = j\n            used[sel] = True\n            if i == n - 1:\n                # last vertex added, record cut\n                best = min(best, weights[sel])\n                # merge prev and sel\n                if prev != -1:\n                    for j in range(n):\n                        weight[prev][j] += weight[sel][j]\n                        weight[j][prev] += weight[j][sel]\n                    vertices.pop(sel)\n                    weight.pop(sel)\n                    for row in weight:\n                        row.pop(sel)\n                n -= 1\n                break\n            prev = sel\n            for j in range(n):\n                if not used[j]:\n                    weights[j] += weight[sel][j]\n    return best\n\n\nWhy It Matters\n\nFinds the global min cut without fixing \\(s,t\\).\nWorks directly on weighted undirected graphs.\nRequires no flow computation.\nSimpler and faster than \\(O(VE \\log V)\\) max-flow min-cut for global cut problems.\n\nIt’s one of the few exact polynomial-time algorithms for global min cut in weighted graphs.\n\n\nA Gentle Proof (Why It Works)\nEach phase finds a minimum \\(s\\)–\\(t\\) cut separating the last vertex \\(t\\) from the rest. The merging process maintains equivalence, merging does not destroy the optimality of remaining cuts. The lightest phase cut corresponds to the global minimum.\nBy induction over merging steps, the algorithm explores all essential cuts.\n\n\nTry It Yourself\n\nCreate a triangle graph with different edge weights.\nTrace \\(A\\)–\\(B\\)–\\(C\\) order additions and record cut weights.\nMerge last two vertices, reduce matrix, repeat.\nVerify cut corresponds to smallest crossing weight.\nCompare with max-flow min-cut for validation.\n\n\n\nTest Cases\n\n\n\nGraph\nMinimum Cut\nNotes\n\n\n\n\nTriangle \\(w=1,2,3\\)\n3\nRemoves edge of weight 3\n\n\nSquare equal weights\n2\nAny two opposite sides\n\n\nWeighted complete\nsmallest edge sum\nDense test\n\n\n\n\n\nComplexity\n\nTime: \\(O(V^3)\\) using adjacency matrix\nSpace: \\(O(V^2)\\)\n\nFaster variants exist using adjacency lists and priority queues (\\(O(VE + V^2 \\log V)\\)).\nThe Stoer–Wagner algorithm shows that global connectivity fragility can be measured efficiently, one cut at a time, through pure merging insight.\n\n\n\n362 Karger’s Randomized Cut\nThe Karger’s Algorithm is a beautifully simple randomized algorithm to find the global minimum cut of an undirected graph. Instead of deterministically exploring all partitions, it contracts edges randomly, shrinking the graph until only two supernodes remain. The sum of edges between them is (with high probability) the minimum cut.\n\nWhat Problem Are We Solving?\nGiven an undirected, unweighted (or weighted) graph \\(G = (V, E)\\), a cut is a partition of \\(V\\) into two disjoint subsets \\((S, V \\setminus S)\\). The cut size is the total number (or weight) of edges crossing the partition.\nWe want to find the global minimum cut: \\[\n\\min_{S \\subset V, S \\neq \\emptyset, S \\neq V} \\text{cut}(S)\n\\]\nKarger’s algorithm gives a probabilistic guarantee of finding the exact minimum cut.\n\n\nHow Does It Work (Plain Language)?\nIt’s almost magical in its simplicity:\n\nWhile there are more than 2 vertices:\n\nPick a random edge \\((u, v)\\).\nContract it, merge \\(u\\) and \\(v\\) into a single vertex.\nRemove self-loops.\n\nWhen only two vertices remain,\n\nThe edges between them form a cut.\n\n\nRepeat the process multiple times to increase success probability.\nEach run finds the true minimum cut with probability at least \\[\n\\frac{2}{n(n-1)}\n\\]\nBy repeating \\(O(n^2 \\log n)\\) times, the probability of missing it becomes negligible.\n\n\nExample\nConsider a triangle graph \\((A, B, C)\\):\n\nRandomly pick one edge, say \\((A, B)\\), contract into supernode \\((AB)\\).\nNow edges: \\((AB, C)\\) with multiplicity 2.\nOnly 2 nodes remain, cut weight = 2, the min cut.\n\n\n\nTiny Code (Python)\nimport random\nimport copy\n\ndef karger_min_cut(graph):\n    # graph: adjacency list {u: [v1, v2, ...]}\n    vertices = list(graph.keys())\n    edges = []\n    for u in graph:\n        for v in graph[u]:\n            if u &lt; v:\n                edges.append((u, v))\n\n    g = copy.deepcopy(graph)\n    while len(g) &gt; 2:\n        u, v = random.choice(edges)\n        # merge v into u\n        g[u].extend(g[v])\n        for w in g[v]:\n            g[w] = [u if x == v else x for x in g[w]]\n        del g[v]\n        # remove self-loops\n        g[u] = [x for x in g[u] if x != u]\n        edges = []\n        for x in g:\n            for y in g[x]:\n                if x &lt; y:\n                    edges.append((x, y))\n    # any remaining edge list gives cut size\n    return len(list(g.values())[0])\n\n\nWhy It Matters\n\nElegantly simple yet provably correct with probability.\nNo flows, no complex data structures.\nExcellent pedagogical algorithm for randomized reasoning.\nForms base for improved variants (Karger–Stein, randomized contraction).\n\nKarger’s approach demonstrates the power of randomization, minimal logic, maximal insight.\n\n\nA Gentle Proof (Why It Works)\nAt each step, contracting a non-min-cut edge does not destroy the min cut. Since there are \\(O(n^2)\\) edges and at least \\(n-2\\) contractions, probability that we never contract a min-cut edge is: \\[\nP = \\prod_{i=0}^{n-3} \\frac{k_i}{m_i} \\ge \\frac{2}{n(n-1)}\n\\] where \\(k_i\\) is min cut size, \\(m_i\\) is edges remaining.\nBy repeating enough times, the failure probability decays exponentially.\n\n\nTry It Yourself\n\nRun the algorithm 1 time on a 4-node graph, note variability.\nRepeat 50 times, collect min cut frequencies.\nVisualize contraction steps on paper.\nAdd weights by expanding weighted edges into parallel copies.\nCompare runtime vs Stoer–Wagner on dense graphs.\n\n\n\nTest Cases\n\n\n\nGraph\nExpected Min Cut\nNotes\n\n\n\n\nTriangle\n2\nalways 2\n\n\nSquare (4-cycle)\n2\nrandom paths converge\n\n\nComplete \\(K_4\\)\n3\ndense, repeat to confirm\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\) per trial\nSpace: \\(O(n^2)\\) (for adjacency)\nRepetitions: \\(O(n^2 \\log n)\\) for high confidence\n\nKarger’s algorithm is a landmark example: a single line, “pick an edge at random and contract it”, unfolds into a full-fledged, provably correct algorithm for global min-cut discovery.\n\n\n\n363 Karger–Stein Minimum Cut\nThe Karger–Stein algorithm is an improved randomized divide-and-conquer version of Karger’s original contraction algorithm. It achieves a higher success probability and better expected runtime, while preserving the same beautifully simple idea: repeatedly contract random edges, but stop early and recurse instead of fully collapsing to two vertices.\n\nWhat Problem Are We Solving?\nGiven an undirected, weighted or unweighted graph \\(G = (V, E)\\), the goal is to find the global minimum cut, that is:\n\\[\n\\min_{S \\subset V,, S \\neq \\emptyset,, S \\neq V} \\text{cut}(S)\n\\]\nwhere:\n\\[\n\\text{cut}(S) = \\sum_{u \\in S,, v \\in V \\setminus S} w(u, v)\n\\]\n\n\nHow Does It Work (Plain Language)?\nLike Karger’s algorithm, we contract edges randomly, but instead of shrinking to 2 vertices immediately, we contract only until the graph has about \\(\\frac{n}{\\sqrt{2}}\\) vertices, then recurse twice independently. The best cut found across recursive calls is our result.\nThis strategy amplifies success probability while maintaining efficiency.\nAlgorithm:\n\nIf \\(|V| \\le 6\\):\n\nCompute min cut directly by brute force or basic contraction.\n\nOtherwise:\n\nLet \\(t = \\lceil n / \\sqrt{2} + 1 \\rceil\\).\nRandomly contract edges until only \\(t\\) vertices remain.\nRun the algorithm recursively twice on two independent contractions.\n\nReturn the smaller of the two cuts.\n\nEach contraction phase preserves the minimum cut with good probability, and recursive repetition compounds success.\n\n\nExample\nFor a graph of 16 vertices:\n\nContract randomly down to \\(\\lceil 16 / \\sqrt{2} \\rceil = 12\\).\nRecurse twice independently.\nEach recursion again halves vertex count until base case.\nReturn smallest cut found.\n\nMultiple recursion branches make the overall probability of keeping all min-cut edges much higher than one-shot contraction.\n\n\nTiny Code (Python)\nimport random\nimport math\nimport copy\n\ndef contract_random_edge(graph):\n    u, v = random.choice([(u, w) for u in graph for w in graph[u] if u &lt; w])\n    # merge v into u\n    graph[u].extend(graph[v])\n    for w in graph[v]:\n        graph[w] = [u if x == v else x for x in graph[w]]\n    del graph[v]\n    # remove self-loops\n    graph[u] = [x for x in graph[u] if x != u]\n\ndef karger_stein(graph):\n    n = len(graph)\n    if n &lt;= 6:\n        # base case: fall back to basic Karger\n        g_copy = copy.deepcopy(graph)\n        while len(g_copy) &gt; 2:\n            contract_random_edge(g_copy)\n        return len(list(g_copy.values())[0])\n\n    t = math.ceil(n / math.sqrt(2)) + 1\n    g1 = copy.deepcopy(graph)\n    g2 = copy.deepcopy(graph)\n    while len(g1) &gt; t:\n        contract_random_edge(g1)\n    while len(g2) &gt; t:\n        contract_random_edge(g2)\n\n    return min(karger_stein(g1), karger_stein(g2))\n\n\nWhy It Matters\n\nImproves success probability to roughly \\(1 / \\log n\\), vs \\(1/n^2\\) for basic Karger.\nDivide-and-conquer structure reduces required repetitions.\nDemonstrates power of probability amplification in randomized algorithms.\nUseful for large dense graphs where deterministic \\(O(V^3)\\) algorithms are slower.\n\nIt’s a near-optimal randomized min-cut algorithm, balancing simplicity and efficiency.\n\n\nA Gentle Proof (Why It Works)\nEach contraction preserves the min cut with probability:\n\\[\np = \\prod_{i=k+1}^n \\left(1 - \\frac{2}{i}\\right)\n\\]\nThe early stopping at \\(t = n / \\sqrt{2}\\) keeps \\(p\\) reasonably high. Since we recurse twice independently, overall success probability becomes:\n\\[\nP = 1 - (1 - p)^2 \\approx 2p\n\\]\nRepeating \\(O(\\log^2 n)\\) times ensures high confidence of finding the true minimum cut.\n\n\nTry It Yourself\n\nCompare runtime and accuracy vs plain Karger.\nRun on small graphs and collect success rate over 100 trials.\nVisualize recursive tree of contractions.\nAdd edge weights (by expanding parallel edges).\nConfirm returned cut matches Stoer–Wagner result.\n\n\n\nTest Cases\n\n\n\nGraph\nExpected Min Cut\nNotes\n\n\n\n\nTriangle\n2\nAlways found\n\n\n4-node cycle\n2\nHigh accuracy\n\n\nDense \\(K_6\\)\n5\nRepeats improve confidence\n\n\n\n\n\nComplexity\n\nExpected Time: \\(O(n^2 \\log^3 n)\\)\nSpace: \\(O(n^2)\\)\nRepetitions: \\(O(\\log^2 n)\\) for high probability\n\nKarger–Stein refines the raw elegance of random contraction into a divide-and-conquer gem, faster, more reliable, and still delightfully simple.\n\n\n\n364 Gomory–Hu Tree\nThe Gomory–Hu Tree is a remarkable data structure that compactly represents all-pairs minimum cuts in an undirected weighted graph. Instead of computing \\(O(n^2)\\) separate cuts, it builds a single tree (with \\(n-1\\) edges) whose edge weights capture every pair’s min-cut value.\nThis structure transforms global connectivity questions into simple tree queries, fast, exact, and elegant.\n\nWhat Problem Are We Solving?\nGiven an undirected weighted graph \\(G = (V, E, w)\\), we want to find, for every pair \\((s, t)\\):\n\\[\n\\lambda(s, t) = \\min_{S \\subset V,, s \\in S,, t \\notin S} \\sum_{u \\in S, v \\notin S} w(u, v)\n\\]\nInstead of running a separate min-cut computation for each pair, we build a Gomory–Hu tree \\(T\\), such that:\n\nFor any pair \\((s, t)\\), the minimum edge weight on the path between \\(s\\) and \\(t\\) in \\(T\\) equals \\(\\lambda(s, t)\\).\n\nThis tree encodes all-pairs min-cuts compactly.\n\n\nHow Does It Work (Plain Language)?\nThe Gomory–Hu tree is constructed iteratively using \\(n-1\\) minimum-cut computations:\n\nChoose a root \\(r\\) (arbitrary).\nMaintain a partition tree \\(T\\) with vertices as nodes.\nWhile there are unprocessed partitions:\n\nPick two vertices \\(s, t\\) in the same partition.\nCompute the minimum \\(s\\)–\\(t\\) cut using any max-flow algorithm.\nPartition vertices into two sets \\((S, V \\setminus S)\\) along that cut.\nAdd an edge \\((s, t)\\) in the Gomory–Hu tree with weight equal to the cut value.\nRecurse on each partition.\n\nAfter \\(n-1\\) cuts, the tree is complete.\n\nEvery edge in the tree represents a distinct partition cut in the original graph.\n\n\nExample\nConsider a graph with vertices \\({A, B, C, D}\\) and weighted edges. We run cuts step by step:\n\nChoose \\(s = A, t = B\\) → find cut weight \\(w(A,B) = 2\\) → Add edge \\((A, B, 2)\\) to tree.\nRecurse within \\(A\\)’s and \\(B\\)’s sides.\nContinue until tree has \\(n-1 = 3\\) edges.\n\nNow for any \\((u,v)\\) pair, the min-cut = minimum edge weight along the path connecting them in \\(T\\).\n\n\nTiny Code (High-Level Skeleton)\ndef gomory_hu_tree(V, edges):\n    # edges: list of (u, v, w)\n    from collections import defaultdict\n    n = V\n    tree = defaultdict(list)\n    parent = [0] * n\n    cut_value = [0] * n\n\n    for s in range(1, n):\n        t = parent[s]\n        # compute s-t min cut via max-flow\n        mincut, partition = min_cut(s, t, edges)\n        cut_value[s] = mincut\n        for v in range(n):\n            if v != s and parent[v] == t and partition[v]:\n                parent[v] = s\n        tree[s].append((t, mincut))\n        tree[t].append((s, mincut))\n        if partition[t]:\n            parent[s], parent[t] = parent[t], s\n    return tree\nIn practice, min_cut(s, t) is computed using Edmonds–Karp or Push–Relabel.\n\n\nWhy It Matters\n\nCaptures all-pairs min-cuts in just \\(n-1\\) max-flow computations.\nTransforms graph cut queries into simple tree queries.\nEnables efficient network reliability analysis, connectivity queries, and redundancy planning.\nWorks for weighted undirected graphs.\n\nThis algorithm compresses rich connectivity information into a single elegant structure.\n\n\nA Gentle Proof (Why It Works)\nEach \\(s\\)–\\(t\\) min-cut partitions vertices into two sides; merging these partitions iteratively preserves all-pairs min-cut relationships.\nBy induction, every pair \\((u, v)\\) is eventually separated by exactly one edge in the tree, with weight equal to \\(\\lambda(u, v)\\). Thus, \\[\n\\lambda(u, v) = \\min_{e \\in \\text{path}(u, v)} w(e)\n\\]\n\n\nTry It Yourself\n\nRun on a small 4-node weighted graph.\nVerify each edge weight equals an actual cut value.\nQuery random pairs \\((u, v)\\) by tree path min.\nCompare with independent flow-based min-cut computations.\nDraw both the original graph and the resulting tree.\n\n\n\nTest Cases\n\n\n\nGraph\nTree Edges\nNotes\n\n\n\n\nTriangle\n2 edges\nUniform weights produce equal cuts\n\n\nSquare\n3 edges\nDistinct cuts form balanced tree\n\n\nComplete \\(K_4\\)\n3 edges\nSymmetric connectivity\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\cdot \\text{MaxFlow}(V,E))\\)\nSpace: \\(O(V + E)\\)\nQueries: \\(O(\\log V)\\) (via path min in tree)\n\nThe Gomory–Hu tree elegantly transforms the cut landscape of a graph into a tree of truths, one structure, all min-cuts.\n\n\n\n365 Max-Flow Min-Cut Theorem\nThe Max-Flow Min-Cut Theorem is one of the foundational results in graph theory and combinatorial optimization. It reveals a duality between maximum flow (what can pass through a network) and minimum cut (what blocks the network). The two are not just related, they are exactly equal.\nThis theorem underpins nearly every algorithm for flows, cuts, and network design.\n\nWhat Problem Are We Solving?\nWe are working with a directed graph \\(G = (V, E)\\), a source \\(s\\), and a sink \\(t\\). Each edge \\((u,v)\\) has a capacity \\(c(u,v) \\ge 0\\).\nA flow assigns values \\(f(u,v)\\) to edges such that:\n\nCapacity constraint: \\(0 \\le f(u,v) \\le c(u,v)\\)\nFlow conservation: \\(\\sum_v f(u,v) = \\sum_v f(v,u)\\) for all \\(u \\ne s, t\\)\n\nThe value of the flow is: \\[\n|f| = \\sum_{v} f(s, v)\n\\]\nWe want the maximum possible flow value from \\(s\\) to \\(t\\).\nA cut \\((S, T)\\) is a partition of vertices such that \\(s \\in S\\) and \\(t \\in T = V \\setminus S\\). The capacity of the cut is: \\[\nc(S, T) = \\sum_{u \\in S, v \\in T} c(u, v)\n\\]\n\n\nThe Theorem\n\nMax-Flow Min-Cut Theorem: In every flow network, the maximum value of a feasible flow equals the minimum capacity of an \\(s\\)–\\(t\\) cut.\n\nFormally, \\[\n\\max_f |f| = \\min_{(S,T)} c(S, T)\n\\]\nThis is a strong duality statement, a maximum over one set equals a minimum over another.\n\n\nHow Does It Work (Plain Language)?\n\nYou try to push as much flow as possible from \\(s\\) to \\(t\\).\nAs you saturate edges, certain parts of the graph become bottlenecks.\nThe final residual graph separates reachable vertices from \\(s\\) (via unsaturated edges) and the rest.\nThis separation \\((S, T)\\) forms a minimum cut, whose capacity equals the total flow sent.\n\nThus, once you can’t push any more flow, you’ve also found the tightest cut blocking you.\n\n\nExample\nConsider a small network:\n\n\n\nEdge\nCapacity\n\n\n\n\ns → a\n3\n\n\ns → b\n2\n\n\na → t\n2\n\n\nb → t\n3\n\n\na → b\n1\n\n\n\nMax flow via augmenting paths:\n\n\\(s \\to a \\to t\\): 2 units\n\\(s \\to b \\to t\\): 2 units\n\\(s \\to a \\to b \\to t\\): 1 unit\n\nTotal flow = 5.\nMin cut: \\(S = {s, a}\\), \\(T = {b, t}\\) → capacity = 5. Flow = Cut = 5 ✔\n\n\nTiny Code (Verification via Edmonds–Karp)\nfrom collections import deque\n\ndef bfs(cap, flow, s, t, parent):\n    n = len(cap)\n    visited = [False]*n\n    q = deque([s])\n    visited[s] = True\n    while q:\n        u = q.popleft()\n        for v in range(n):\n            if not visited[v] and cap[u][v] - flow[u][v] &gt; 0:\n                parent[v] = u\n                visited[v] = True\n                if v == t: return True\n                q.append(v)\n    return False\n\ndef max_flow_min_cut(cap, s, t):\n    n = len(cap)\n    flow = [[0]*n for _ in range(n)]\n    parent = [-1]*n\n    max_flow = 0\n    while bfs(cap, flow, s, t, parent):\n        v = t\n        path_flow = float('inf')\n        while v != s:\n            u = parent[v]\n            path_flow = min(path_flow, cap[u][v] - flow[u][v])\n            v = u\n        v = t\n        while v != s:\n            u = parent[v]\n            flow[u][v] += path_flow\n            flow[v][u] -= path_flow\n            v = u\n        max_flow += path_flow\n    return max_flow\nThe final reachable set from \\(s\\) in the residual graph defines the minimum cut.\n\n\nWhy It Matters\n\nFundamental theorem linking optimization and combinatorics.\nBasis for algorithms like Ford–Fulkerson, Edmonds–Karp, Dinic, Push–Relabel.\nUsed in image segmentation, network reliability, scheduling, bipartite matching, clustering, and transportation.\n\nIt bridges the gap between flow maximization and cut minimization, two faces of the same coin.\n\n\nA Gentle Proof (Why It Works)\nAt termination of Ford–Fulkerson:\n\nNo augmenting path exists in the residual graph.\nLet \\(S\\) be all vertices reachable from \\(s\\).\nThen all edges \\((u,v)\\) with \\(u \\in S, v \\notin S\\) are saturated.\n\nThus: \\[\n|f| = \\sum_{u \\in S, v \\notin S} f(u,v) = \\sum_{u \\in S, v \\notin S} c(u,v) = c(S, T)\n\\] and no cut can have smaller capacity. Therefore, \\(\\max |f| = \\min c(S,T)\\).\n\n\nTry It Yourself\n\nBuild a simple 4-node network and trace augmenting paths.\nIdentify the cut \\((S, T)\\) at termination.\nVerify total flow equals cut capacity.\nTest different max-flow algorithms, result remains identical.\nVisualize cut edges in residual graph.\n\n\n\nTest Cases\n\n\n\nGraph\nMax Flow\nMin Cut\nNotes\n\n\n\n\nSimple chain\n5\n5\nidentical\n\n\nParallel edges\n8\n8\nsame result\n\n\nDiamond graph\n6\n6\nmin cut = bottleneck edges\n\n\n\n\n\nComplexity\n\nDepends on underlying flow algorithm (e.g. \\(O(VE^2)\\) for Edmonds–Karp)\nCut extraction: \\(O(V + E)\\)\n\nThe Max-Flow Min-Cut Theorem is the heartbeat of network optimization, every augmenting path is a step toward equality between two fundamental perspectives: sending and separating.\n\n\n\n366 Stoer–Wagner Repeated Phase\nThe Stoer–Wagner Repeated Phase algorithm is a refinement of the Stoer–Wagner minimum cut algorithm for undirected weighted graphs. It finds the global minimum cut by repeating a maximum adjacency search phase multiple times, progressively merging vertices and tracking cut weights.\nThis algorithm is elegant, deterministic, and runs in polynomial time, often faster than flow-based approaches for undirected graphs.\n\nWhat Problem Are We Solving?\nWe are finding the minimum cut in an undirected, weighted graph \\(G = (V, E)\\), where each edge \\((u, v)\\) has a non-negative weight \\(w(u, v)\\).\nA cut \\((S, V \\setminus S)\\) partitions the vertex set into two disjoint subsets. The weight of a cut is: \\[\nw(S, V \\setminus S) = \\sum_{u \\in S, v \\notin S} w(u, v)\n\\]\nOur goal is to find a cut \\((S, T)\\) that minimizes this sum.\n\n\nCore Idea\nThe algorithm works by repeatedly performing “phases”. Each phase identifies a minimum \\(s\\)–\\(t\\) cut in the current contracted graph and merges the last two added vertices. Over multiple phases, the smallest cut discovered is the global minimum.\nEach phase follows a maximum adjacency search pattern, similar to Prim’s algorithm but in reverse logic.\n\n\nHow It Works (Plain Language)\nEach phase:\n\nPick an arbitrary start vertex.\nMaintain a set \\(A\\) of added vertices.\nAt each step, add the most tightly connected vertex \\(v\\) not in \\(A\\) (the one with maximum total edge weight to \\(A\\)).\nContinue until all vertices are in \\(A\\).\nLet \\(s\\) be the second-to-last added vertex, and \\(t\\) the last added.\nThe cut separating \\(t\\) from the rest is a candidate min cut.\nRecord its weight; then merge \\(s\\) and \\(t\\) into a supervertex and repeat.\n\nAfter \\(|V| - 1\\) phases, the smallest cut seen is the global minimum cut.\n\n\nExample\nGraph:\n\n\n\nEdge\nWeight\n\n\n\n\nA–B\n3\n\n\nA–C\n2\n\n\nB–C\n4\n\n\nB–D\n2\n\n\nC–D\n3\n\n\n\nPhase 1:\n\nStart with \\(A = {A}\\)\nAdd \\(B\\) (max connected to A: 3)\nAdd \\(C\\) (max connected to \\({A,B}\\): total 6)\nAdd \\(D\\) last → Cut weight = sum of edges from D to \\({A,B,C}\\) = 5\n\nRecord min cut = 5. Merge \\((C,D)\\) and continue.\nRepeat phases → global min cut = 5.\n\n\nTiny Code (Simplified Python)\ndef stoer_wagner_min_cut(graph):\n    n = len(graph)\n    vertices = list(range(n))\n    min_cut = float('inf')\n\n    while len(vertices) &gt; 1:\n        added = [False] * n\n        weights = [0] * n\n        prev = -1\n        for _ in range(len(vertices)):\n            u = max(vertices, key=lambda v: weights[v] if not added[v] else -1)\n            added[u] = True\n            if _ == len(vertices) - 1:\n                # Last vertex added, potential cut\n                min_cut = min(min_cut, weights[u])\n                # Merge u into prev\n                if prev != -1:\n                    for v in vertices:\n                        if v != u and v != prev:\n                            graph[prev][v] += graph[u][v]\n                            graph[v][prev] = graph[prev][v]\n                    vertices.remove(u)\n                break\n            prev = u\n            for v in vertices:\n                if not added[v]:\n                    weights[v] += graph[u][v]\n    return min_cut\nGraph is given as an adjacency matrix. Each phase picks the tightest vertex, records the cut, and merges nodes.\n\n\nWhy It Matters\n\nDeterministic and elegant for undirected weighted graphs.\nFaster than running multiple max-flow computations.\nIdeal for network reliability, graph partitioning, clustering, and circuit design.\nEach phase mimics “tightening” the graph until only one supervertex remains.\n\n\n\nA Gentle Proof (Why It Works)\nEach phase finds the minimum \\(s\\)–\\(t\\) cut where \\(t\\) is the last added vertex. By merging \\(s\\) and \\(t\\), we preserve all other possible cuts. The smallest of these phase cuts is the global minimum, as each cut in the merged graph corresponds to one in the original graph.\nFormally: \\[\n\\text{mincut}(G) = \\min_{\\text{phases}} w(S, V \\setminus S)\n\\]\nThis inductive structure ensures optimality.\n\n\nTry It Yourself\n\nRun on a 4-node complete graph with random weights.\nTrace vertex addition order in each phase.\nRecord cut weights per phase.\nCompare with brute-force all cuts, they match.\nVisualize contraction steps.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges\nMin Cut\nNotes\n\n\n\n\nTriangle (equal weights)\n3\n2×weight\nall equal\n\n\nSquare (unit weights)\n4\n2\nopposite sides\n\n\nWeighted grid\n6\nsmallest bridge\ncut at bottleneck\n\n\n\n\n\nComplexity\n\nEach phase: \\(O(V^2)\\)\nTotal: \\(O(V^3)\\) for adjacency matrix\n\nWith Fibonacci heaps, can improve to \\(O(V^2 \\log V + VE)\\).\nThe Stoer–Wagner repeated phase algorithm is a powerful, purely combinatorial tool, no flows, no residual graphs, just tight connectivity and precise merging toward the true global minimum cut.\n\n\n\n367 Dynamic Min Cut\nThe Dynamic Minimum Cut problem extends classical min-cut computation to graphs that change over time, edges can be added, deleted, or updated. Instead of recomputing from scratch after each change, we maintain data structures that update the min cut efficiently.\nDynamic min-cut algorithms are critical in applications like network resilience, incremental optimization, and real-time systems where connectivity evolves.\n\nWhat Problem Are We Solving?\nGiven a graph \\(G = (V, E)\\) with weighted edges and a current minimum cut, how do we efficiently maintain this cut when:\n\nAn edge \\((u, v)\\) is inserted\nAn edge \\((u, v)\\) is deleted\nAn edge \\((u, v)\\) has its weight changed\n\nThe naive approach recomputes min-cut from scratch using an algorithm like Stoer–Wagner (\\(O(V^3)\\)). Dynamic algorithms aim for faster incremental updates.\n\n\nCore Idea\nThe min cut is sensitive only to edges crossing the cut boundary. When the graph changes, only a local region around modified edges can alter the global cut.\nDynamic algorithms use:\n\nDynamic trees (e.g. Link-Cut Trees)\nFully dynamic connectivity structures\nRandomized contraction tracking\nIncremental recomputation of affected regions\n\nto update efficiently instead of re-running full min-cut.\n\n\nHow It Works (Plain Language)\n\nMaintain a representation of cuts, often as trees or partition sets.\nWhen an edge weight changes, update connectivity info:\n\nIf edge lies within one side of the cut, no change.\nIf edge crosses the cut, update cut capacity.\n\nWhen an edge is added or deleted, adjust affected components and recalculate locally.\nOptionally, run periodic global recomputations to correct drift after many updates.\n\nThese methods trade exactness for efficiency, often maintaining approximate min cuts within small error bounds.\n\n\nExample (High-Level)\nGraph has vertices \\({A, B, C, D}\\) with current min cut \\(({A, B}, {C, D})\\), weight \\(5\\).\n\nAdd edge \\((B, C)\\) with weight \\(1\\):\n\nNew crossing edge, cut weight becomes \\(5 + 1 = 6\\).\nCheck if alternate partition gives smaller total, update if needed.\n\nDelete edge \\((A, C)\\):\n\nRemove from cut set.\nIf this edge was essential to connectivity, min cut might increase.\nRecompute local cut if affected.\n\n\n\n\nTiny Code (Sketch, Python)\nclass DynamicMinCut:\n    def __init__(self, graph):\n        self.graph = graph\n        self.min_cut_value = self.compute_min_cut()\n\n    def compute_min_cut(self):\n        # Use Stoer–Wagner or flow-based method\n        return stoer_wagner(self.graph)\n\n    def update_edge(self, u, v, new_weight):\n        self.graph[u][v] = new_weight\n        self.graph[v][u] = new_weight\n        # Locally recompute affected region\n        self.min_cut_value = self.recompute_local(u, v)\n\n    def recompute_local(self, u, v):\n        # Simplified placeholder: recompute fully if small graph\n        return self.compute_min_cut()\nFor large graphs, replace recompute_local with incremental cut update logic.\n\n\nWhy It Matters\n\nReal-time systems need quick responses to network changes.\nStreaming graphs (e.g. traffic, social, or power networks) evolve continuously.\nReliability analysis in dynamic systems relies on up-to-date min-cut values.\n\nDynamic maintenance saves time compared to recomputing from scratch at every step.\n\n\nA Gentle Proof (Why It Works)\nLet \\(C_t\\) be the min cut after \\(t\\) updates. If each update affects only local structure, then: \\[\nC_{t+1} = \\min(C_t, \\text{local adjustment})\n\\] Maintaining a certificate structure (like a Gomory–Hu tree) ensures correctness since all pairwise min-cuts are preserved under local changes, except where updated edges are involved.\nRecomputing only affected cuts guarantees correctness with amortized efficiency.\n\n\nTry It Yourself\n\nBuild a small weighted graph (5–6 nodes).\nCompute initial min cut using Stoer–Wagner.\nAdd or delete edges, one at a time.\nUpdate only affected cuts manually.\nCompare to full recomputation, results should match.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nOperation\nDescription\nNew Min Cut\n\n\n\n\nAdd edge \\((u,v)\\) with small weight\nNew crossing edge\nPossibly smaller cut\n\n\nIncrease edge weight\nStrengthens bridge\nCut may change elsewhere\n\n\nDelete edge across cut\nWeakens connection\nCut may increase\n\n\nDelete edge inside partition\nNo change\n,\n\n\n\n\n\nComplexity\n\n\n\nOperation\nTime\n\n\n\n\nNaive recomputation\n\\(O(V^3)\\)\n\n\nDynamic approach (randomized)\n\\(O(V^2 \\log V)\\) amortized\n\n\nApproximate dynamic cut\n\\(O(E \\log^2 V)\\)\n\n\n\nDynamic min-cut algorithms balance exactness with responsiveness, maintaining near-optimal connectivity insight as graphs evolve in real time.\n\n\n\n368 Minimum s–t Cut (Edmonds–Karp)\nThe Minimum s–t Cut problem seeks the smallest total capacity of edges that must be removed to separate a source vertex \\(s\\) from a sink vertex \\(t\\). It’s the dual counterpart to maximum flow, and the Edmonds–Karp algorithm provides a clear path to compute it using BFS-based augmenting paths.\n\nWhat Problem Are We Solving?\nGiven a directed, weighted graph \\(G=(V,E)\\) with capacities \\(c(u,v)\\), find a partition of \\(V\\) into two disjoint sets \\((S, T)\\) such that:\n\n\\(s \\in S\\), \\(t \\in T\\)\nThe sum of capacities of edges from \\(S\\) to \\(T\\) is minimum\n\nFormally: \\[\n\\text{min-cut}(s, t) = \\min_{(S,T)} \\sum_{u \\in S, v \\in T} c(u, v)\n\\]\nThis cut value equals the maximum flow value from \\(s\\) to \\(t\\) by the Max-Flow Min-Cut Theorem.\n\n\nHow It Works (Plain Language)\nThe algorithm uses Edmonds–Karp, a BFS-based version of Ford–Fulkerson, to find the maximum flow first. After computing max-flow, the reachable vertices from \\(s\\) in the residual graph determine the min-cut.\nSteps:\n\nInitialize flow \\(f(u, v) = 0\\) for all edges.\nWhile there exists a BFS path from \\(s\\) to \\(t\\) in the residual graph:\n\nCompute bottleneck capacity along path.\nAugment flow along path.\n\nAfter no augmenting path exists:\n\nRun BFS one last time from \\(s\\) in residual graph.\nVertices reachable from \\(s\\) form set \\(S\\).\nOthers form \\(T\\).\n\nThe edges crossing from \\(S\\) to \\(T\\) with full capacity are the min-cut edges.\n\n\n\nExample\nGraph:\n\nVertices: \\({s, a, b, t}\\)\nEdges:\n\n\\((s, a) = 3\\)\n\\((s, b) = 2\\)\n\\((a, b) = 1\\)\n\\((a, t) = 2\\)\n\\((b, t) = 3\\)\n\n\n\nRun Edmonds–Karp to find max-flow = 4.\nResidual graph:\n\nReachable set from \\(s\\): \\({s, a}\\)\nUnreachable set: \\({b, t}\\)\n\nMin-cut edges: \\((a, t)\\) and \\((s, b)\\)\nMin-cut value = \\(2 + 2 = 4\\) Matches max-flow value.\n\n\n\nTiny Code (C-like Pseudocode)\nint min_st_cut(Graph *G, int s, int t) {\n    int maxflow = edmonds_karp(G, s, t);\n    bool visited[V];\n    bfs_residual(G, s, visited);\n    int cut_value = 0;\n    for (edge (u,v) in G-&gt;edges)\n        if (visited[u] && !visited[v])\n            cut_value += G-&gt;capacity[u][v];\n    return cut_value;\n}\n\n\nWhy It Matters\n\nReveals bottlenecks in a network.\nKey for reliability and segmentation problems.\nFoundational for image segmentation, network design, and flow decomposition.\nDirectly supports duality proofs between optimization problems.\n\n\n\nA Gentle Proof (Why It Works)\nThe Max-Flow Min-Cut Theorem states:\n\\[\n\\max_{\\text{flow } f} \\sum_{v} f(s, v) = \\min_{(S, T)} \\sum_{u \\in S, v \\in T} c(u, v)\n\\]\nEdmonds–Karp finds the maximum flow by repeatedly augmenting along shortest paths (BFS order). Once no more augmenting paths exist, the residual graph partitions nodes naturally into \\(S\\) and \\(T\\), and the edges from \\(S\\) to \\(T\\) define the min cut.\n\n\nTry It Yourself\n\nBuild a small directed network with capacities.\nRun Edmonds–Karp manually (trace augmenting paths).\nDraw residual graph and find reachable set from \\(s\\).\nMark crossing edges, sum their capacities.\nCompare with max-flow value.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nMax Flow\nMin Cut\nMatches?\n\n\n\n\nSimple 4-node\n4\n4\n✅\n\n\nLinear chain \\(s \\to a \\to b \\to t\\)\nmin edge\nmin edge\n✅\n\n\nParallel paths\nsum of min caps\nsum of min caps\n✅\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\n\n\n\n\nBFS per augmentation\n\\(O(E)\\)\n\n\nAugmentations\n\\(O(VE)\\)\n\n\nTotal\n\\(O(VE^2)\\)\n\n\n\nThe Minimum s–t Cut via Edmonds–Karp is an elegant bridge between flow algorithms and partition reasoning, every edge in the cut tells a story of constraint, capacity, and balance.\n\n\n\n369 Approximate Min Cut\nThe Approximate Minimum Cut algorithm provides a way to estimate the minimum cut of a graph faster than exact algorithms, especially when exact precision is not critical. It’s built on randomization and sampling, using probabilistic reasoning to find small cuts with high confidence in large or dynamic graphs.\n\nWhat Problem Are We Solving?\nGiven a weighted, undirected graph \\(G=(V, E)\\), we want a cut \\((S, T)\\) such that the cut capacity is close to the true minimum:\n\\[\nw(S, T) \\le (1 + \\epsilon) \\cdot \\lambda(G)\n\\]\nwhere \\(\\lambda(G)\\) is the weight of the global min cut, and \\(\\epsilon\\) is a small error tolerance (e.g. \\(0.1\\)).\nThe goal is speed: approximate min-cut algorithms run in near-linear time, much faster than exact ones (\\(O(V^3)\\)).\n\n\nHow It Works (Plain Language)\nApproximate algorithms rely on two key principles:\n\nRandom Sampling: Randomly sample edges with probability proportional to their weight. The smaller the edge capacity, the more likely it’s critical to the min cut.\nGraph Sparsification: Build a smaller “sketch” of the graph that preserves cut weights approximately. Compute the min cut on this sparse graph, it’s close to the true value.\n\nBy repeating sampling several times and taking the minimum found cut, we converge to a near-optimal solution.\n\n\nAlgorithm Sketch (Karger’s Sampling Method)\n\nInput: Graph \\(G(V, E)\\) with \\(n = |V|\\) and \\(m = |E|\\)\nChoose sampling probability \\(p = \\frac{c \\log n}{\\epsilon^2 \\lambda}\\)\nBuild sampled graph \\(G'\\):\n\nInclude each edge \\((u, v)\\) with probability \\(p\\)\nScale included edge weights by \\(\\frac{1}{p}\\)\n\nRun an exact min cut algorithm (Stoer–Wagner) on \\(G'\\)\nRepeat sampling \\(O(\\log n)\\) times; take the best cut found\n\nThe result approximates \\(\\lambda(G)\\) within factor \\((1 + \\epsilon)\\) with high probability.\n\n\nExample\nSuppose \\(G\\) has \\(10^5\\) edges, and exact Stoer–Wagner would be too slow.\n\nChoose \\(\\epsilon = 0.1\\), \\(p = 0.02\\)\nSample \\(2%\\) of edges randomly (2000 edges)\nReweight sampled edges by \\(\\frac{1}{0.02} = 50\\)\nRun exact min cut on this smaller graph\nRepeat 5–10 times; pick smallest cut\n\nResult: A cut within \\(10%\\) of optimal, in a fraction of the time.\n\n\nTiny Code (Python-like Pseudocode)\ndef approximate_min_cut(G, epsilon=0.1, repeats=10):\n    best_cut = float('inf')\n    for _ in range(repeats):\n        p = compute_sampling_probability(G, epsilon)\n        G_sample = sample_graph(G, p)\n        cut_value = stoer_wagner(G_sample)\n        best_cut = min(best_cut, cut_value)\n    return best_cut\n\n\nWhy It Matters\n\nScalability: Handles huge graphs where exact methods are infeasible\nSpeed: Near-linear time using randomization\nApplications:\n\nStreaming graphs\nNetwork reliability\nClustering and partitioning\nGraph sketching and sparsification\n\n\nApproximate min cuts are crucial when you need quick, robust decisions, not perfect answers.\n\n\nA Gentle Proof (Why It Works)\nKarger’s analysis shows that each small cut is preserved with high probability if enough edges are sampled:\n\\[\n\\Pr[\\text{cut weight preserved}] \\ge 1 - \\frac{1}{n^2}\n\\]\nBy repeating the process \\(O(\\log n)\\) times, we amplify confidence, ensuring that with high probability, at least one sampled graph maintains the true min-cut structure.\nUsing Chernoff bounds, the error is bounded by \\((1 \\pm \\epsilon)\\).\n\n\nTry It Yourself\n\nGenerate a random graph with 50 nodes and random weights.\nCompute exact min cut using Stoer–Wagner.\nSample 10%, 5%, and 2% of edges, compute approximate cuts.\nCompare results and runtime.\nAdjust \\(\\epsilon\\) and observe trade-off between speed and accuracy.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nGraph\nExact Min Cut\nApprox. (ε=0.1)\nError\nSpeedup\n\n\n\n\nSmall dense (100 edges)\n12\n13\n8%\n5×\n\n\nMedium sparse (1k edges)\n8\n8\n0%\n10×\n\n\nLarge (100k edges)\n30\n33\n10%\n50×\n\n\n\n\n\nComplexity\n\n\n\n\n\n\n\n\nMethod\nTime\nAccuracy\n\n\n\n\nStoer–Wagner\n\\(O(V^3)\\)\nExact\n\n\nKarger (Randomized Exact)\n\\(O(V^2 \\log^3 V)\\)\nExact (probabilistic)\n\n\nApproximate Sampling\n\\(O(E \\log^2 V / \\epsilon^2)\\)\n\\((1 + \\epsilon)\\)\n\n\n\nApproximate min-cut algorithms show that probability can replace precision when scale demands speed, they slice through massive graphs with surprising efficiency.\n\n\n\n370 Min k-Cut\nThe Minimum k-Cut problem generalizes the classic min-cut idea. Instead of splitting a graph into just two parts, the goal is to partition it into k disjoint subsets while cutting edges of minimum total weight.\nIt’s a key problem in clustering, parallel processing, and network design, where you want multiple disconnected regions with minimal interconnection cost.\n\nWhat Problem Are We Solving?\nGiven a weighted, undirected graph \\(G = (V, E)\\) and an integer \\(k\\), find a partition of \\(V\\) into \\(k\\) subsets \\({V_1, V_2, \\ldots, V_k}\\) such that:\n\n\\(V_i \\cap V_j = \\emptyset\\) for all \\(i \\ne j\\)\n\\(\\bigcup_i V_i = V\\)\nThe sum of edge weights crossing between parts is minimized\n\nFormally:\n\\[\n\\text{min-}k\\text{-cut}(G) = \\min_{V_1, \\ldots, V_k} \\sum_{\\substack{(u,v) \\in E \\ u \\in V_i, v \\in V_j, i \\ne j}} w(u,v)\n\\]\nFor \\(k=2\\), this reduces to the standard min-cut problem.\n\n\nHow It Works (Plain Language)\nThe Min k-Cut problem is NP-hard for general \\(k\\), but several algorithms provide exact solutions for small \\(k\\) and approximations for larger \\(k\\).\nThere are two main approaches:\n\nGreedy Iterative Cutting:\n\nRepeatedly find and remove a global min cut, splitting the graph into components one by one.\nAfter \\(k-1\\) cuts, you have \\(k\\) components.\nWorks well but not always optimal.\n\nDynamic Programming over Trees (Exact for small k):\n\nUse a tree decomposition of the graph.\nCompute optimal partition by exploring edge removals in the minimum spanning tree.\nBased on the Karger–Stein framework.\n\n\n\n\nExample\nGraph with 5 nodes and edges:\n\n\n\nEdge\nWeight\n\n\n\n\n(A, B)\n1\n\n\n(B, C)\n2\n\n\n(C, D)\n3\n\n\n(D, E)\n4\n\n\n(A, E)\n2\n\n\n\nGoal: partition into \\(k=3\\) subsets.\n\nFind minimum cut edges to remove:\n\nCut \\((A, B)\\) (weight 1)\nCut \\((B, C)\\) (weight 2)\n\nTotal cut weight = \\(3\\)\nResulting subsets: \\({A}, {B}, {C, D, E}\\)\n\n\n\nTiny Code (Python-like Pseudocode)\ndef min_k_cut(graph, k):\n    cuts = []\n    G = graph.copy()\n    for _ in range(k - 1):\n        cut_value, (S, T) = stoer_wagner(G)\n        cuts.append(cut_value)\n        G = G.subgraph(S)  # Keep one component, remove crossing edges\n    return sum(cuts)\nFor small \\(k\\), you can use recursive contraction (like Karger’s algorithm) or dynamic programming on tree structures.\n\n\nWhy It Matters\n\nClustering: Group nodes into \\(k\\) balanced communities.\nParallel Computing: Partition workloads while minimizing communication cost.\nImage Segmentation: Divide pixels into \\(k\\) coherent regions.\nGraph Simplification: Split networks into modular subgraphs.\n\nMin k-Cut transforms connectivity into structured modularity.\n\n\nA Gentle Proof (Why It Works)\nEach cut increases the number of connected components by 1. Thus, performing \\(k-1\\) cuts produces exactly \\(k\\) components.\nLet \\(C_1, C_2, \\ldots, C_{k-1}\\) be the successive minimum cuts. The sum of their weights bounds the global optimum:\n\\[\n\\text{min-}k\\text{-cut} \\le \\sum_{i=1}^{k-1} \\lambda_i\n\\]\nwhere \\(\\lambda_i\\) is the \\(i\\)-th smallest cut value. Iterative min-cuts often approximate the optimal \\(k\\)-cut well.\nFor exact solutions, algorithms based on flow decomposition or tree contractions use recursive partitioning to explore combinations of edges.\n\n\nTry It Yourself\n\nConstruct a small weighted graph with 6–8 vertices.\nRun Stoer–Wagner to find the first min cut.\nRemove edges, repeat for next cut.\nCompare total cut weight to brute-force partition for \\(k=3\\).\nObserve approximation quality.\n\n\n\nTest Cases\n\n\n\nGraph\nk\nExact\nGreedy\nError\n\n\n\n\nTriangle\n3\n3\n3\n0%\n\n\nLine of 5\n3\n3\n3\n0%\n\n\nDense 6-node\n3\n12\n13\n8%\n\n\n\n\n\nComplexity\n\n\n\nAlgorithm\nTime\nType\n\n\n\n\nBrute Force\nExponential\nExact\n\n\nGreedy Cuts\n\\(O(k \\cdot V^3)\\)\nApprox\n\n\nTree DP\n\\(O(V^{k-1})\\)\nExact (small \\(k\\))\n\n\nRandomized (Karger–Stein)\n\\(O(V^2 \\log^3 V)\\)\nApprox\n\n\n\nThe Min k-Cut problem generalizes connectivity design — it’s where graph partitioning meets optimization, balancing efficiency and modularity.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-38.-matchings",
    "href": "books/en-us/list-4.html#section-38.-matchings",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 38. Matchings",
    "text": "Section 38. Matchings\n\n371 Bipartite Matching (DFS)\nBipartite Matching is one of the most fundamental problems in graph theory. Given a bipartite graph \\(G=(U,V,E)\\), the goal is to find the maximum number of pairings between nodes in \\(U\\) and \\(V\\) such that no two edges share a vertex. A simple and intuitive way to do this is by using DFS-based augmenting paths.\n\nWhat Problem Are We Solving?\nWe are given a bipartite graph with two disjoint sets \\(U\\) and \\(V\\). We want to find the maximum matching, the largest set of edges where each vertex is matched to at most one partner.\nFormally, find a subset \\(M \\subseteq E\\) such that:\n\nEach vertex in \\(U \\cup V\\) is incident to at most one edge in \\(M\\)\n\\(|M|\\) is maximized\n\n\n\nHow It Works (Plain Language)\nThe core idea is to build matchings incrementally by finding augmenting paths — paths that start from an unmatched vertex in \\(U\\), alternate between unmatched and matched edges, and end at an unmatched vertex in \\(V\\).\nEach time such a path is found, we flip the match status along it (matched edges become unmatched, and vice versa), increasing the total matching size by one.\nSteps:\n\nStart with an empty matching \\(M\\).\nFor each vertex \\(u \\in U\\):\n\nRun DFS to find an augmenting path to a free vertex in \\(V\\).\nIf found, augment the matching along this path.\n\nRepeat until no more augmenting paths exist.\n\n\n\nExample\nLet \\(U = {u_1, u_2, u_3}\\), \\(V = {v_1, v_2, v_3}\\), and edges:\n\n\\((u_1, v_1)\\), \\((u_1, v_2)\\)\n\\((u_2, v_2)\\)\n\\((u_3, v_3)\\)\n\n\nStart with empty matching.\nDFS from \\(u_1\\): finds \\((u_1, v_1)\\) → match.\nDFS from \\(u_2\\): finds \\((u_2, v_2)\\) → match.\nDFS from \\(u_3\\): finds \\((u_3, v_3)\\) → match. All vertices matched, maximum matching size = 3.\n\n\n\nTiny Code (C-like Pseudocode)\n#define MAXV 100\nvector&lt;int&gt; adj[MAXV];\nint matchR[MAXV], visited[MAXV];\n\nbool dfs(int u) {\n    for (int v : adj[u]) {\n        if (visited[v]) continue;\n        visited[v] = 1;\n        if (matchR[v] == -1 || dfs(matchR[v])) {\n            matchR[v] = u;\n            return true;\n        }\n    }\n    return false;\n}\n\nint maxBipartiteMatching(int U) {\n    memset(matchR, -1, sizeof(matchR));\n    int result = 0;\n    for (int u = 0; u &lt; U; ++u) {\n        memset(visited, 0, sizeof(visited));\n        if (dfs(u)) result++;\n    }\n    return result;\n}\n\n\nWhy It Matters\n\nFoundation for Hungarian Algorithm and Hopcroft–Karp\nCore tool in resource allocation, scheduling, pairing problems\nIntroduces concept of augmenting paths, a cornerstone in flow theory\n\nUsed in:\n\nAssigning workers to jobs\nMatching students to projects\nNetwork flow initialization\nGraph theory teaching and visualization\n\n\n\nA Gentle Proof (Why It Works)\nIf there exists an augmenting path, flipping the matching along it always increases the matching size by 1.\nLet \\(M\\) be a current matching. If no augmenting path exists, \\(M\\) is maximum (Berge’s Lemma):\n\nA matching is maximum if and only if there is no augmenting path.\n\nTherefore, repeatedly augmenting ensures convergence to the maximum matching.\n\n\nTry It Yourself\n\nDraw a bipartite graph with 4 nodes on each side.\nUse DFS to find augmenting paths manually.\nTrack which vertices are matched/unmatched after each augmentation.\nStop when no augmenting paths remain.\n\n\n\nTest Cases\n\n\n\nGraph\n\\(|U|\\)\n\\(|V|\\)\nMax Matching\nSteps\n\n\n\n\nComplete \\(K_{3,3}\\)\n3\n3\n3\n3 DFS\n\n\nChain \\(u_1v_1, u_2v_2, u_3v_3\\)\n3\n3\n3\n3 DFS\n\n\nSparse graph\n4\n4\n2\n4 DFS\n\n\n\n\n\nComplexity\n\n\n\nAspect\nCost\n\n\n\n\nDFS per vertex\n\\(O(E)\\)\n\n\nTotal\n\\(O(VE)\\)\n\n\nSpace\n\\(O(V+E)\\)\n\n\n\nThis DFS-based approach gives an intuitive baseline for bipartite matching, later improved by Hopcroft–Karp (\\(O(E\\sqrt{V})\\)) but perfect for learning and small graphs.\n\n\n\n372 Hopcroft–Karp\nThe Hopcroft–Karp algorithm is a classic improvement over DFS-based bipartite matching. It uses layered BFS and DFS to find multiple augmenting paths in parallel, reducing redundant searches and achieving an optimal runtime of \\(O(E\\sqrt{V})\\).\n\nWhat Problem Are We Solving?\nGiven a bipartite graph \\(G = (U, V, E)\\), we want to find a maximum matching, the largest set of vertex-disjoint edges connecting \\(U\\) to \\(V\\).\nA matching is a subset \\(M \\subseteq E\\) such that each vertex is incident to at most one edge in \\(M\\). The algorithm seeks the maximum cardinality matching.\n\n\nHow It Works (Plain Language)\nInstead of finding one augmenting path at a time (like simple DFS), Hopcroft–Karp finds a layer of shortest augmenting paths, then augments all of them together. This drastically reduces the number of BFS-DFS phases.\nKey idea:\n\nEach phase increases the matching by many paths at once.\nThe distance (layer) of unmatched vertices strictly increases after each phase.\n\nSteps:\n\nInitialization: Start with empty matching \\(M = \\emptyset\\).\nRepeat until no augmenting path exists:\n\nBFS phase:\n\nBuild a layered graph (level graph) from unmatched vertices in \\(U\\) to unmatched vertices in \\(V\\).\nEach layer increases by 1 hop.\n\nDFS phase:\n\nFind vertex-disjoint augmenting paths in this layered graph.\nAugment along all of them simultaneously.\n\n\nReturn the total matching size.\n\n\n\nExample\nLet \\(U = {u_1, u_2, u_3}\\), \\(V = {v_1, v_2, v_3}\\), edges:\n\n\\(u_1 \\to v_1, v_2\\)\n\\(u_2 \\to v_2\\)\n\\(u_3 \\to v_3\\)\n\n\nInitial matching: empty\nBFS builds layers:\n\nLevel 0: \\(u_1, u_2, u_3\\)\nLevel 1: \\(v_1, v_2, v_3\\) All are reachable.\n\nDFS finds 3 augmenting paths:\n\n\\(u_1 \\to v_1\\), \\(u_2 \\to v_2\\), \\(u_3 \\to v_3\\)\n\nAugment all → Matching size = 3 No more augmenting paths → Maximum matching = 3\n\n\n\nTiny Code (C-like Pseudocode)\nvector&lt;int&gt; adj[MAXV];\nint pairU[MAXV], pairV[MAXV], dist[MAXV];\nint NIL = 0, INF = 1e9;\n\nbool bfs(int U) {\n    queue&lt;int&gt; q;\n    for (int u = 1; u &lt;= U; u++) {\n        if (pairU[u] == NIL) { dist[u] = 0; q.push(u); }\n        else dist[u] = INF;\n    }\n    dist[NIL] = INF;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        if (dist[u] &lt; dist[NIL]) {\n            for (int v : adj[u]) {\n                if (dist[pairV[v]] == INF) {\n                    dist[pairV[v]] = dist[u] + 1;\n                    q.push(pairV[v]);\n                }\n            }\n        }\n    }\n    return dist[NIL] != INF;\n}\n\nbool dfs(int u) {\n    if (u == NIL) return true;\n    for (int v : adj[u]) {\n        if (dist[pairV[v]] == dist[u] + 1 && dfs(pairV[v])) {\n            pairV[v] = u; pairU[u] = v;\n            return true;\n        }\n    }\n    dist[u] = INF;\n    return false;\n}\n\nint hopcroftKarp(int U) {\n    memset(pairU, 0, sizeof(pairU));\n    memset(pairV, 0, sizeof(pairV));\n    int matching = 0;\n    while (bfs(U)) {\n        for (int u = 1; u &lt;= U; u++)\n            if (pairU[u] == NIL && dfs(u))\n                matching++;\n    }\n    return matching;\n}\n\n\nWhy It Matters\n\nEfficient: \\(O(E\\sqrt{V})\\), ideal for large graphs\nFoundational in:\n\nJob assignment\nResource allocation\nStable match foundations\nNetwork optimization\n\n\nIt’s the standard method for maximum bipartite matching in competitive programming and real systems.\n\n\nA Gentle Proof (Why It Works)\nEach BFS-DFS phase finds a set of shortest augmenting paths. After augmenting, no shorter path remains.\nLet \\(d\\) = distance to the nearest unmatched vertex in BFS. Every phase increases the minimum augmenting path length, and the number of phases is at most \\(O(\\sqrt{V})\\) (Hopcroft–Karp Lemma).\nEach BFS-DFS costs \\(O(E)\\), so total = \\(O(E\\sqrt{V})\\).\n\n\nTry It Yourself\n\nDraw a bipartite graph with 5 nodes on each side.\nRun one BFS layer build.\nUse DFS to find all shortest augmenting paths.\nAugment all, track matching size per phase.\n\n\n\nTest Cases\n\n\n\nGraph\n\\(|U|\\)\n\\(|V|\\)\nMatching Size\nComplexity\n\n\n\n\n\\(K_{3,3}\\)\n3\n3\n3\nFast\n\n\nChain graph\n5\n5\n5\nLinear\n\n\nSparse graph\n10\n10\n6\nSublinear phases\n\n\n\n\n\nComplexity\n\n\n\nOperation\nCost\n\n\n\n\nBFS (build layers)\n\\(O(E)\\)\n\n\nDFS (augment paths)\n\\(O(E)\\)\n\n\nTotal\n\\(O(E\\sqrt{V})\\)\n\n\n\nHopcroft–Karp is the benchmark for bipartite matching, balancing elegance, efficiency, and theoretical depth.\n\n\n\n373 Hungarian Algorithm\nThe Hungarian Algorithm (also known as the Kuhn–Munkres Algorithm) solves the assignment problem, finding the minimum-cost perfect matching in a weighted bipartite graph. It’s a cornerstone in optimization, turning complex allocation tasks into elegant linear-time computations on cost matrices.\n\nWhat Problem Are We Solving?\nGiven a bipartite graph \\(G = (U, V, E)\\) with \\(|U| = |V| = n\\), and a cost function \\(c(u,v)\\) for each edge, we want to find a matching \\(M\\) such that:\n\nEvery \\(u \\in U\\) is matched to exactly one \\(v \\in V\\)\nTotal cost is minimized:\n\n\\[\n\\text{Minimize } \\sum_{(u,v) \\in M} c(u, v)\n\\]\nThis is the assignment problem, a special case of linear programming that can be solved exactly in polynomial time.\n\n\nHow It Works (Plain Language)\nThe Hungarian Algorithm treats the cost matrix like a grid puzzle — you systematically reduce, label, and cover rows and columns to reveal a set of zeros corresponding to the optimal assignment.\nCore idea: Transform the cost matrix so that at least one optimal solution lies among zeros.\nSteps:\n\nRow Reduction Subtract the smallest element in each row from all elements in that row.\nColumn Reduction Subtract the smallest element in each column from all elements in that column.\nCovering Zeros Use the minimum number of lines (horizontal + vertical) to cover all zeros.\nAdjust Matrix If the number of covering lines &lt; \\(n\\):\n\nFind smallest uncovered value \\(m\\)\nSubtract \\(m\\) from all uncovered elements\nAdd \\(m\\) to elements covered twice\nRepeat from step 3\n\nAssignment Once \\(n\\) lines are used, pick one zero per row/column → that’s the optimal matching.\n\n\n\nExample\nCost matrix:\n\n\n\n\nv1\nv2\nv3\n\n\n\n\nu1\n4\n1\n3\n\n\nu2\n2\n0\n5\n\n\nu3\n3\n2\n2\n\n\n\n\nRow Reduction: subtract min in each row | u1 | 3 | 0 | 2 | | u2 | 2 | 0 | 5 | | u3 | 1 | 0 | 0 |\nColumn Reduction: subtract min in each column | u1 | 2 | 0 | 2 | | u2 | 1 | 0 | 5 | | u3 | 0 | 0 | 0 |\nCover zeros with 3 lines → feasible.\nAssignment: pick \\((u1,v2)\\), \\((u2,v1)\\), \\((u3,v3)\\) → total cost = \\(1+2+2=5\\)\n\nOptimal assignment found.\n\n\nTiny Code (Python-like Pseudocode)\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef hungarian(cost_matrix):\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    total_cost = cost_matrix[row_ind, col_ind].sum()\n    return list(zip(row_ind, col_ind)), total_cost\n\n# Example\ncost = np.array([[4,1,3],[2,0,5],[3,2,2]])\nmatch, cost = hungarian(cost)\nprint(match, cost)  # [(0,1), (1,0), (2,2)], 5\n\n\nWhy It Matters\n\nExact and efficient: \\(O(n^3)\\) complexity\nFundamental in operations research and AI (task allocation, scheduling, tracking)\nUsed in:\n\nJob–worker assignment\nOptimal resource allocation\nMatching predictions to ground truth (e.g. Hungarian loss in object detection)\n\n\nThe algorithm balances combinatorics and linear algebra, a rare blend of elegance and utility.\n\n\nA Gentle Proof (Why It Works)\nThe algorithm maintains dual feasibility and complementary slackness at each step. By reducing rows and columns, we ensure at least one zero in each row and column, creating a reduced cost matrix where zeros correspond to feasible assignments.\nEach iteration moves closer to a perfect matching in the equality graph (edges where reduced cost = 0). Once all vertices are matched, the solution satisfies optimality conditions.\n\n\nTry It Yourself\n\nCreate a 3×3 or 4×4 cost matrix.\nPerform row and column reductions manually.\nCover zeros and count lines.\nAdjust and repeat until \\(n\\) lines used.\nAssign one zero per row/column.\n\n\n\nTest Cases\n\n\n\nMatrix Size\nCost Matrix Type\nResult\n\n\n\n\n3×3\nRandom integers\nMinimum cost assignment\n\n\n4×4\nDiagonal dominance\nDiagonal chosen\n\n\n5×5\nSymmetric\nMatching pairs found\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\n\n\n\n\nRow/column reductions\n\\(O(n^2)\\)\n\n\nIterative covering\n\\(O(n^3)\\)\n\n\nTotal\n\\(O(n^3)\\)\n\n\n\nThe Hungarian Algorithm transforms a cost matrix into structure, revealing an optimal matching hidden within zeros, one line at a time.\n\n\n\n374 Kuhn–Munkres (Max-Weight Matching)\nThe Kuhn–Munkres Algorithm, also known as the Hungarian Algorithm for Maximum Weight Matching, solves the maximum-weight bipartite matching problem. While the standard Hungarian method minimizes total cost, this version maximizes total reward or utility, making it ideal for assignment optimization when bigger is better.\n\nWhat Problem Are We Solving?\nGiven a complete bipartite graph \\(G = (U, V, E)\\) with \\(|U| = |V| = n\\), and weights \\(w(u, v)\\) on each edge, find a perfect matching \\(M \\subseteq E\\) such that:\n\\[\n\\text{maximize } \\sum_{(u,v) \\in M} w(u,v)\n\\]\nEach vertex is matched to exactly one partner on the opposite side, and total weight is as large as possible.\n\n\nHow It Works (Plain Language)\nThe algorithm treats the weight matrix like a profit grid. It constructs a labeling on vertices and maintains equality edges (where label sums equal edge weight). By building and augmenting matchings within this equality graph, it converges to the optimal maximum-weight matching.\nKey ideas:\n\nMaintain vertex labels \\(l(u)\\) and \\(l(v)\\) that satisfy \\(l(u) + l(v) \\ge w(u, v)\\) (dual feasibility)\nBuild equality graph where equality holds\nFind augmenting paths in equality graph\nUpdate labels when stuck to reveal new equality edges\n\nSteps:\n\nInitialize labels\n\n\\(l(u) = \\max_{v} w(u, v)\\) for each \\(u \\in U\\)\n\\(l(v) = 0\\) for each \\(v \\in V\\)\n\nRepeat for each \\(u\\):\n\nBuild alternating tree using BFS\nIf no augmenting path, update labels to expose new equality edges\nAugment matching along found path\n\nContinue until all vertices are matched.\n\n\n\nExample\nWeights:\n\n\n\n\nv1\nv2\nv3\n\n\n\n\nu1\n3\n2\n1\n\n\nu2\n2\n4\n6\n\n\nu3\n3\n5\n3\n\n\n\nGoal: maximize total weight\nOptimal matching:\n\n\\(u1 \\to v1\\) (3)\n\\(u2 \\to v3\\) (6)\n\\(u3 \\to v2\\) (5)\n\nTotal weight = \\(3 + 6 + 5 = 14\\)\n\n\nTiny Code (Python-like Pseudocode)\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef kuhn_munkres(weight_matrix):\n    # Convert to cost by negation (Hungarian solves minimization)\n    cost = -weight_matrix\n    row_ind, col_ind = linear_sum_assignment(cost)\n    total = weight_matrix[row_ind, col_ind].sum()\n    return list(zip(row_ind, col_ind)), total\n\n# Example\nW = np.array([[3,2,1],[2,4,6],[3,5,3]])\nmatch, total = kuhn_munkres(W)\nprint(match, total)  # [(0,0),(1,2),(2,1)], 14\n\n\nWhy It Matters\n\nSolves maximum reward assignments in polynomial time\nFundamental in:\n\nJob–task allocation\nOptimal pairing problems\nMachine learning (e.g. Hungarian loss)\nGame theory and economics\n\n\nMany systems use this algorithm to maximize overall efficiency when assigning limited resources.\n\n\nA Gentle Proof (Why It Works)\n\nMaintain dual feasibility: \\(l(u) + l(v) \\ge w(u, v)\\)\nMaintain complementary slackness: matched edges satisfy equality\nBy alternating updates between equality graph expansion and label adjustments, the algorithm ensures eventual feasibility and optimality\nThe final matching satisfies strong duality, achieving maximum weight\n\n\n\nTry It Yourself\n\nBuild a \\(3 \\times 3\\) profit matrix.\nInitialize \\(l(u)\\) as row maxima, \\(l(v)=0\\).\nDraw equality edges (\\(l(u)+l(v)=w(u,v)\\)).\nFind augmenting path → augment → update labels.\nRepeat until matching is perfect.\n\n\n\nTest Cases\n\n\n\nGraph\nSize\nMax Weight\nMatching\n\n\n\n\n3×3\nRandom weights\n14\n[(0,0),(1,2),(2,1)]\n\n\n4×4\nDiagonal high\nsum(diag)\nDiagonal\n\n\n5×5\nUniform\n5×max weight\nany\n\n\n\n\n\nComplexity\n\n\n\nStep\nTime\n\n\n\n\nLabel updates\n\\(O(V^2)\\)\n\n\nMatching phases\n\\(O(V)\\)\n\n\nTotal\n\\(O(V^3)\\)\n\n\n\nThe Kuhn–Munkres algorithm is the ultimate tool for maximum-weight assignments, blending geometry, duality, and combinatorics into one powerful optimization engine.\n\n\n\n375 Blossom Algorithm\nThe Blossom Algorithm, developed by Jack Edmonds, is the foundational algorithm for finding maximum matchings in general graphs, including non-bipartite ones. It introduced the concept of blossoms (odd-length cycles) and showed that maximum matching is solvable in polynomial time, a landmark in combinatorial optimization.\n\nWhat Problem Are We Solving?\nGiven a general (not necessarily bipartite) graph \\(G = (V, E)\\), find a maximum matching, a largest set of edges such that no two share a vertex.\nUnlike bipartite graphs, odd-length cycles can block progress in standard augmenting path searches. The Blossom Algorithm contracts these cycles so that augmenting paths can still be found.\nFormally, find \\(M \\subseteq E\\) maximizing \\(|M|\\) such that each vertex is incident to at most one edge in \\(M\\).\n\n\nHow It Works (Plain Language)\nThe algorithm extends the augmenting path approach (used in bipartite matching) by adding one powerful idea: When you hit an odd-length cycle, treat it as a single vertex, a blossom.\nSteps:\n\nInitialize with an empty matching \\(M = \\emptyset\\).\nSearch for augmenting paths using BFS/DFS in the alternating tree:\n\nAlternate between matched and unmatched edges.\nIf you reach a free vertex, augment (flip matched/unmatched edges).\n\nIf you encounter an odd cycle (a blossom), contract it into a single super-vertex.\n\nContinue the search in the contracted graph.\nOnce an augmenting path is found, expand blossoms and update the matching.\n\nRepeat until no augmenting path exists.\n\nEach augmentation increases \\(|M|\\) by 1. When no augmenting path remains, the matching is maximum.\n\n\nExample\nGraph:\n\nVertices: \\({A, B, C, D, E}\\)\nEdges: \\((A, B), (B, C), (C, A), (B, D), (C, E)\\)\n\n\nStart matching: empty\nBuild alternating tree: \\(A \\to B \\to C \\to A\\) forms an odd cycle\nContract blossom \\((A, B, C)\\) into one node\nContinue search → find augmenting path through blossom\nExpand blossom, adjust matching\nResult: maximum matching includes edges \\((A,B), (C,E), (D,...)\\)\n\n\n\nTiny Code (Python-Like Pseudocode)\n# Using networkx for simplicity\nimport networkx as nx\n\ndef blossom_maximum_matching(G):\n    return nx.max_weight_matching(G, maxcardinality=True)\n\n# Example\nG = nx.Graph()\nG.add_edges_from([(0,1),(1,2),(2,0),(1,3),(2,4)])\nmatch = blossom_maximum_matching(G)\nprint(match)  # {(0,1), (2,4)}\n\n\nWhy It Matters\n\nGeneral graphs (non-bipartite) are common in real-world systems:\n\nSocial networks (friend pairings)\nMolecular structure matching\nScheduling with constraints\nGraph-theoretic proofs and optimization\n\n\nThe Blossom Algorithm proved that matching is polynomial-time solvable, a key milestone in algorithmic theory.\n\n\nA Gentle Proof (Why It Works)\nBy Berge’s Lemma, a matching is maximum iff there’s no augmenting path. The challenge is that augmenting paths can be hidden inside odd cycles.\nBlossom contraction ensures that every augmenting path in the contracted graph corresponds to one in the original. After each augmentation, \\(|M|\\) strictly increases, so the algorithm terminates in polynomial time.\nCorrectness follows from maintaining:\n\nAlternating trees with consistent parity\nContraction invariants (augmenting path preservation)\nBerge’s condition across contractions and expansions\n\n\n\nTry It Yourself\n\nDraw a triangle graph (3-cycle).\nRun augmenting path search, find blossom.\nContract it into one node.\nContinue search and expand after finding path.\nVerify maximum matching.\n\n\n\nTest Cases\n\n\n\nGraph Type\nMatching Size\nMethod\n\n\n\n\nTriangle\n1\nBlossom contraction\n\n\nSquare with diagonal\n2\nAugmentation\n\n\nPentagonal odd cycle\n2\nBlossom\n\n\nBipartite (sanity check)\nAs usual\nMatches Hopcroft–Karp\n\n\n\n\n\nComplexity\n\n\n\nPhase\nTime\n\n\n\n\nSearch (per augmentation)\n\\(O(VE)\\)\n\n\nAugmentations\n\\(O(V)\\)\n\n\nTotal\n\\(O(V^3)\\)\n\n\n\nThe Blossom Algorithm was a revelation, showing how to tame odd cycles and extending matchings to all graphs, bridging combinatorics and optimization theory.\n\n\n\n376 Edmonds’ Blossom Shrinking\nEdmonds’ Blossom Shrinking is the core subroutine that powers the Blossom Algorithm, enabling augmenting-path search in non-bipartite graphs. It provides the crucial mechanism for contracting odd-length cycles (blossoms) so that hidden augmenting paths can be revealed and exploited.\n\nWhat Problem Are We Solving?\nIn non-bipartite graphs, augmenting paths can be obscured by odd-length cycles. Standard matching algorithms fail because they assume bipartite structure.\nWe need a way to handle odd cycles during search so that the algorithm can progress without missing valid augmentations.\nGoal: Detect blossoms, shrink them into single vertices, and continue the search efficiently.\nGiven a matching \\(M\\) in graph \\(G = (V, E)\\), and an alternating tree grown during a search, when a blossom is found:\n\nShrink the blossom into a super-vertex\nContinue the search in the contracted graph\nExpand the blossom when an augmenting path is found\n\n\n\nHow It Works (Plain Language)\nImagine running a BFS/DFS from a free vertex. You alternate between matched and unmatched edges to build an alternating tree.\nIf you find an edge connecting two vertices at the same level (both even depth):\n\nYou’ve detected an odd cycle.\nThat cycle is a blossom.\n\nTo handle it:\n\nIdentify the blossom (odd-length alternating cycle)\nShrink all its vertices into a single super-node\nContinue the augmenting path search on this contracted graph\nOnce an augmenting path is discovered, expand the blossom and adjust the path accordingly\nAugment along the expanded path\n\nThis shrinking maintains all valid augmenting paths and allows the algorithm to operate as if the blossom were one vertex.\n\n\nExample\nGraph: Vertices: \\({A, B, C, D, E}\\) Edges: \\((A,B), (B,C), (C,A), (B,D), (C,E)\\)\nSuppose \\(A, B, C\\) form an odd cycle discovered during search.\n\nDetect blossom: \\((A, B, C)\\)\nContract into a single vertex \\(X\\)\nContinue search on reduced graph\nIf augmenting path passes through \\(X\\), expand back\nAlternate edges within blossom to integrate path correctly\n\nResult: A valid augmenting path is found and matching increases by one.\n\n\nTiny Code (Python-Like)\ndef find_blossom(u, v, parent):\n    # Find common ancestor\n    path_u, path_v = set(), set()\n    while u != -1:\n        path_u.add(u)\n        u = parent[u]\n    while v not in path_u:\n        path_v.add(v)\n        v = parent[v]\n    lca = v\n    # Shrink blossom (conceptually)\n    return lca\nIn practical implementations (like Edmonds’ algorithm), this logic merges all nodes in the blossom into a single node and adjusts parent/child relationships.\n\n\nWhy It Matters\nThis is the heart of general-graph matching. Without shrinking, the search may loop infinitely or fail to detect valid augmentations.\nBlossom shrinking allows:\n\nHandling odd-length cycles\nMaintaining augmenting path invariants\nGuaranteeing polynomial time behavior\n\nIt is also one of the earliest uses of graph contraction in combinatorial optimization.\n\n\nA Gentle Proof (Why It Works)\nBy Berge’s Lemma, a matching is maximum iff there is no augmenting path. If an augmenting path exists in \\(G\\), then one exists in any contracted version of \\(G\\).\nShrinking a blossom preserves augmentability:\n\nEvery augmenting path in the original graph corresponds to one in the contracted graph.\nAfter expansion, the alternating pattern is restored correctly.\n\nTherefore, contraction does not lose information, it simply simplifies the search.\n\n\nTry It Yourself\n\nDraw a triangle \\(A, B, C\\) connected to other vertices \\(D, E\\).\nBuild an alternating tree starting from a free vertex.\nWhen you find an edge connecting two even-level vertices, mark the odd cycle.\nShrink the blossom, continue search, then expand once path is found.\n\nObserve how this allows discovery of an augmenting path that was previously hidden.\n\n\nTest Cases\n\n\n\nGraph\nDescription\nAugmentable After Shrink?\n\n\n\n\nTriangle\nSingle odd cycle\nYes\n\n\nTriangle + tail\nCycle + path\nYes\n\n\nBipartite\nNo odd cycle\nShrink not needed\n\n\nPentagon\nBlossom of length 5\nYes\n\n\n\n\n\nComplexity\nShrinking can be done in linear time relative to blossom size. Total algorithm remains \\(O(V^3)\\) when integrated into full matching search.\nEdmonds’ Blossom Shrinking is the conceptual leap that made maximum matching in general graphs tractable, transforming an intractable maze of cycles into a solvable structure through careful contraction and expansion.\n\n\n\n377 Greedy Matching\nGreedy Matching is the simplest way to approximate a maximum matching in a graph. Instead of exploring augmenting paths, it just keeps picking available edges that don’t conflict, a fast, intuitive baseline for matching problems.\n\nWhat Problem Are We Solving?\nIn many real-world settings, job assignment, pairing users, scheduling, we need a set of edges such that no two share a vertex. This is a matching.\nFinding the maximum matching exactly (especially in general graphs) can be expensive. But sometimes, we only need a good enough answer quickly.\nA Greedy Matching algorithm provides a fast approximation:\n\nIt won’t always find the largest matching\nBut it runs in \\(O(E)\\) and often gives a decent solution\n\nGoal: Quickly build a maximal matching (no edge can be added).\n\n\nHow It Works (Plain Language)\nStart with an empty set \\(M\\) (the matching). Go through the edges one by one:\n\nFor each edge \\((u, v)\\)\nIf neither \\(u\\) nor \\(v\\) is already matched\nAdd \\((u, v)\\) to \\(M\\)\n\nContinue until all edges have been checked.\nThis ensures no vertex appears in more than one edge, a valid matching. The result is maximal: you can’t add any other edge without breaking the matching rule.\n\n\nExample\nGraph edges: \\[\nE = {(A,B), (B,C), (C,D), (D,E)}\n\\]\n\nStart: \\(M = \\emptyset\\)\nPick \\((A,B)\\) → mark \\(A\\), \\(B\\) as matched\nSkip \\((B,C)\\) → \\(B\\) is matched\nPick \\((C,D)\\) → mark \\(C\\), \\(D\\)\nSkip \\((D,E)\\) → \\(D\\) is matched\n\nResult: \\[\nM = {(A,B), (C,D)}\n\\]\n\n\nTiny Code (Python-Like)\ndef greedy_matching(graph):\n    matched = set()\n    matching = []\n    for u, v in graph.edges:\n        if u not in matched and v not in matched:\n            matching.append((u, v))\n            matched.add(u)\n            matched.add(v)\n    return matching\nThis simple loop runs in linear time over edges.\n\n\nWhy It Matters\n\nFast: runs in \\(O(E)\\)\nSimple: easy to implement\nUseful baseline: good starting point for heuristic or hybrid approaches\nGuaranteed maximality: no edge can be added without breaking matching condition\n\nAlthough it’s not optimal, its result is at least half the size of a maximum matching: \\[\n|M_{\\text{greedy}}| \\ge \\frac{1}{2}|M_{\\text{max}}|\n\\]\n\n\nA Gentle Proof (Why It Works)\nEach greedy edge \\((u,v)\\) blocks at most one edge from \\(M_{\\text{max}}\\) (since both \\(u\\) and \\(v\\) are used). So for every chosen edge, we lose at most one from the optimal. Thus: \\[\n2|M_{\\text{greedy}}| \\ge |M_{\\text{max}}|\n\\] ⇒ Greedy achieves a 1/2-approximation.\n\n\nTry It Yourself\n\nCreate a graph with 6 vertices and 7 edges.\nRun greedy matching in different edge orders.\nObserve how results differ, order can affect the final set.\nCompare with an exact maximum matching (e.g., Hopcroft–Karp).\n\nYou’ll see how simple decisions early can influence outcome size.\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nEdges\nGreedy Matching\nMaximum Matching\n\n\n\n\nPath of 4\n\\((A,B),(B,C),(C,D)\\)\n2\n2\n\n\nTriangle\n\\((A,B),(B,C),(C,A)\\)\n1\n1\n\n\nSquare\n\\((A,B),(B,C),(C,D),(D,A)\\)\n2\n2\n\n\nStar (5 leaves)\n\\((C,1)...(C,5)\\)\n1\n1\n\n\n\n\n\nComplexity\n\nTime: \\(O(E)\\)\nSpace: \\(O(V)\\)\n\nWorks for any undirected graph. For directed graphs, edges must be treated as undirected or symmetric.\nGreedy matching is a quick and practical approach when you need speed over perfection, a simple handshake strategy that pairs as many as possible before the clock runs out.\n\n\n\n378 Stable Marriage (Gale–Shapley)\nStable Marriage (or the Stable Matching Problem) is a cornerstone of combinatorial optimization, where two equal-sized sets (e.g. men and women, jobs and applicants, hospitals and residents) must be paired so that no two participants prefer each other over their assigned partners. The Gale–Shapley algorithm finds such a stable matching in \\(O(n^2)\\) time.\n\nWhat Problem Are We Solving?\nGiven two sets:\n\nSet \\(A = {a_1, a_2, ..., a_n}\\)\nSet \\(B = {b_1, b_2, ..., b_n}\\)\n\nEach member ranks all members of the other set in order of preference. We want to find a matching (a one-to-one pairing) such that:\nThere is no pair \\((a_i, b_j)\\) where:\n\n\\(a_i\\) prefers \\(b_j\\) over their current match, and\n\\(b_j\\) prefers \\(a_i\\) over their current match.\n\nSuch a pair would be unstable, since they would rather be matched together.\nOur goal: find a stable configuration, no incentive to switch partners.\n\n\nHow It Works (Plain Language)\nThe Gale–Shapley algorithm uses proposals and rejections:\n\nAll \\(a_i\\) start unmatched.\nWhile some \\(a_i\\) is free:\n\n\\(a_i\\) proposes to the most-preferred \\(b_j\\) not yet proposed to.\nIf \\(b_j\\) is free, accept the proposal.\nIf \\(b_j\\) is matched but prefers \\(a_i\\) over current partner, she “trades up” and rejects the old one.\nOtherwise, she rejects \\(a_i\\).\n\nContinue until everyone is matched.\n\nThe process always terminates with a stable matching.\n\n\nExample\nLet’s say we have:\n\n\n\nA\nPreference List\n\n\n\n\nA1\nB1, B2, B3\n\n\nA2\nB2, B1, B3\n\n\nA3\nB3, B1, B2\n\n\n\n\n\n\nB\nPreference List\n\n\n\n\nB1\nA2, A1, A3\n\n\nB2\nA1, A2, A3\n\n\nB3\nA1, A2, A3\n\n\n\nStep by step:\n\nA1 → B1, B1 free → match (A1, B1)\nA2 → B2, B2 free → match (A2, B2)\nA3 → B3, B3 free → match (A3, B3)\n\nStable matching: {(A1, B1), (A2, B2), (A3, B3)}\nNo two prefer each other over current partners → Stable.\n\n\nTiny Code (Python-Like)\ndef gale_shapley(A_prefs, B_prefs):\n    free_A = list(A_prefs.keys())\n    engaged = {}\n    next_choice = {a: 0 for a in A_prefs}\n\n    while free_A:\n        a = free_A.pop(0)\n        b = A_prefs[a][next_choice[a]]\n        next_choice[a] += 1\n\n        if b not in engaged:\n            engaged[b] = a\n        else:\n            current = engaged[b]\n            if B_prefs[b].index(a) &lt; B_prefs[b].index(current):\n                engaged[b] = a\n                free_A.append(current)\n            else:\n                free_A.append(a)\n    return {v: k for k, v in engaged.items()}\n\n\nWhy It Matters\n\nGuarantees a stable solution.\nAlways terminates after at most \\(n^2\\) proposals.\nIf one side proposes (say \\(A\\)), the result is optimal for \\(A\\) and pessimal for \\(B\\).\nForms the foundation of real-world systems like the National Residency Matching Program (NRMP) and school assignments.\n\n\n\nA Gentle Proof (Why It Works)\nEach proposal is made once per pair → at most \\(n^2\\) proposals.\nNo cycles:\n\n\\(b\\) always stays with the best proposer so far\nOnce rejected, \\(a\\) cannot improve, ensuring termination\n\nStability: Suppose \\((a_i, b_j)\\) is a blocking pair. Then \\(a_i\\) must have proposed to \\(b_j\\) and was rejected. That means \\(b_j\\) preferred her current match. So \\((a_i, b_j)\\) cannot be blocking → contradiction.\nThus, the final matching is stable.\n\n\nTry It Yourself\n\nCreate two sets of 3 elements with ranked lists.\nRun Gale–Shapley twice, once with A proposing, once with B proposing.\nCompare the two matchings, see who benefits.\nExplore what happens if preference lists are incomplete.\n\n\n\nTest Cases\n\n\n\nCase\nResult\nStable?\n\n\n\n\nEqual preferences\nMultiple stable matchings\nYes\n\n\nCyclic preferences\nAlgorithm converges\nYes\n\n\nOne-sided identical\nDeterministic output\nYes\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(n)\\)\n\nScalable for small to medium systems, widely used in practice.\nStable marriage shows how simple local rules (propose, reject, keep best) can produce global stability, a harmony of preference and fairness.\n\n\n\n379 Weighted b-Matching\nWeighted b-Matching generalizes standard matching by allowing each node to be matched to up to \\(b(v)\\) partners instead of just one. When edges carry weights, the goal is to find a maximum-weight subset of edges such that the degree of each vertex does not exceed its capacity \\(b(v)\\).\n\nWhat Problem Are We Solving?\nGiven a graph \\(G = (V, E)\\) with:\n\nEdge weights \\(w(e)\\) for each \\(e \\in E\\)\nCapacity constraint \\(b(v)\\) for each vertex \\(v \\in V\\)\n\nFind a subset \\(M \\subseteq E\\) that:\n\nMaximizes total weight \\[W(M) = \\sum_{e \\in M} w(e)\\]\nSatisfies degree constraints \\[\\forall v \\in V,\\quad \\deg_M(v) \\le b(v)\\]\n\nIf \\(b(v) = 1\\) for all \\(v\\), this becomes the standard maximum-weight matching.\n\n\nHow It Works (Plain Language)\nThink of each vertex \\(v\\) as having b(v) slots available for connections. We want to pick edges to fill these slots to maximize total edge weight, without exceeding any vertex’s limit.\nThe weighted b-matching problem can be solved by:\n\nReduction to flow: Convert matching to a network flow problem:\n\nEach edge becomes a flow connection with capacity 1 and cost = \\(-w(e)\\)\nVertex capacities become flow limits\nSolve via min-cost max-flow\n\nLinear Programming: Relax constraints and solve with LP or primal-dual algorithms\nApproximation: For large sparse graphs, greedy heuristics can achieve near-optimal solutions\n\n\n\nExample\nSuppose:\n\nVertices: \\(V = {A, B, C}\\)\nEdges and weights: \\(w(A,B)=5,; w(A,C)=4,; w(B,C)=3\\)\nCapacities: \\(b(A)=2,; b(B)=1,; b(C)=1\\)\n\nWe can pick:\n\n\\((A,B)\\) weight 5\n\\((A,C)\\) weight 4\n\nTotal weight = \\(9\\) Valid since \\(\\deg(A)=2,\\deg(B)=1,\\deg(C)=1\\)\nIf \\(b(A)=1\\), we’d pick only \\((A,B)\\) → weight \\(5\\)\n\n\nTiny Code (Python-Like Pseudocode)\nimport networkx as nx\n\ndef weighted_b_matching(G, b):\n    # Convert to flow network\n    flow_net = nx.DiGraph()\n    source, sink = 's', 't'\n\n    for v in G.nodes:\n        flow_net.add_edge(source, v, capacity=b[v], weight=0)\n        flow_net.add_edge(v, sink, capacity=b[v], weight=0)\n\n    for (u, v, data) in G.edges(data=True):\n        w = -data['weight']  # negate for min-cost\n        flow_net.add_edge(u, v, capacity=1, weight=w)\n        flow_net.add_edge(v, u, capacity=1, weight=w)\n\n    flow_dict = nx.max_flow_min_cost(flow_net, source, sink)\n    # Extract edges with flow=1 between vertex pairs\n    return [(u, v) for u, nbrs in flow_dict.items() for v, f in nbrs.items() if f &gt; 0 and u in G.nodes and v in G.nodes]\n\n\nWhy It Matters\n\nModels resource allocation with limits (e.g. each worker can handle \\(b\\) tasks)\nExtends classic matchings to capacitated networks\nFoundation for:\n\nTask assignment with quotas\nScheduling with multi-capacity\nClustering with degree constraints\n\n\n\n\nA Gentle Proof (Why It Works)\nEach vertex \\(v\\) has degree constraint \\(b(v)\\): \\[\\sum_{e \\ni v} x_e \\le b(v)\\]\nEach edge is chosen at most once: \\[x_e \\in {0, 1}\\]\nThe optimization: \\[\\max \\sum_{e \\in E} w(e) x_e\\]\nsubject to constraints above, forms an integer linear program. Relaxation via network flow ensures optimal integral solution for bipartite graphs.\n\n\nTry It Yourself\n\nBuild a triangle graph with different edge weights.\nSet \\(b(v)=2\\) for one vertex, 1 for others.\nCompute by hand which edges yield maximum weight.\nImplement with a flow solver and verify.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nGraph\nCapacities\nResult\nWeight\n\n\n\n\nTriangle (5,4,3)\n{2,1,1}\n{(A,B),(A,C)}\n9\n\n\nSquare\n{1,1,1,1}\nStandard matching\nMax weight sum\n\n\nLine (A-B-C)\n{2,1,2}\n{(A,B),(B,C)}\nSum of both edges\n\n\n\n\n\nComplexity\n\nTime: \\(O(V^3)\\) using min-cost max-flow\nSpace: \\(O(V + E)\\)\n\nEfficient for medium graphs; scalable via heuristics for larger ones.\nWeighted b-matching balances optimality and flexibility, allowing richer allocation models than one-to-one pairings.\n\n\n\n380 Maximal Matching\nMaximal Matching is a greedy approach that builds a matching by adding edges one by one until no more can be added without breaking the matching condition. Unlike maximum matchings (which maximize size or weight), a maximal matching simply ensures no edge can be added, it’s locally optimal, not necessarily globally optimal.\n\nWhat Problem Are We Solving?\nWe want to select a set of edges \\(M \\subseteq E\\) such that:\n\nNo two edges in \\(M\\) share a vertex \\[\\forall (u,v), (x,y) \\in M,; {u,v} \\cap {x,y} = \\emptyset\\]\n\\(M\\) is maximal: no additional edge from \\(E\\) can be added without violating (1).\n\nGoal: Find any maximal matching, not necessarily the largest one.\n\n\nHow It Works (Plain Language)\nThe algorithm is greedy and simple:\n\nStart with an empty matching \\(M = \\emptyset\\)\nIterate through edges \\((u,v)\\)\nIf neither \\(u\\) nor \\(v\\) are matched yet, add \\((u,v)\\) to \\(M\\)\nContinue until all edges are processed\n\nAt the end, \\(M\\) is maximal: every unmatched edge touches a vertex already matched.\n\n\nExample\nGraph:\nA -- B -- C\n|         \nD\nEdges: \\((A,B)\\), \\((B,C)\\), \\((A,D)\\)\nStep-by-step:\n\nPick \\((A,B)\\) → mark A, B matched\nSkip \\((B,C)\\) (B already matched)\nSkip \\((A,D)\\) (A already matched)\n\nResult: \\(M = {(A,B)}\\)\nAnother valid maximal matching: \\({(A,D), (B,C)}\\) Not unique, but all maximal sets share the property that no more edges can be added.\n\n\nTiny Code (Python)\ndef maximal_matching(edges):\n    matched = set()\n    M = []\n    for u, v in edges:\n        if u not in matched and v not in matched:\n            M.append((u, v))\n            matched.add(u)\n            matched.add(v)\n    return M\n\nedges = [('A','B'), ('B','C'), ('A','D')]\nprint(maximal_matching(edges))  # [('A','B')]\n\n\nWhy It Matters\n\nFast approximation: Maximal matching is a 2-approximation for the maximum matching (it’s at least half as large).\nBuilding block: Used in distributed and parallel algorithms where global optimization is too expensive.\nPreprocessing step: Reduces problem size in flow, scheduling, and network design.\n\n\n\nA Gentle Proof (Why It Works)\nIf an edge \\((u,v)\\) is not in \\(M\\), at least one of its endpoints is already matched. Thus, adding \\((u,v)\\) would break the matching condition. This ensures maximality, no more edges can be safely added.\nFormally, for every \\((u,v) \\notin M\\): \\[u \\in V_M \\lor v \\in V_M\\] where \\(V_M\\) is the set of matched vertices.\nTherefore, \\(M\\) is maximal.\n\n\nTry It Yourself\n\nDraw a small graph with 4–6 vertices.\nApply the greedy process in different edge orders.\nCompare resulting matchings, note they may differ in size, but all are maximal.\nTry to add an extra edge, see why it violates matching rules.\n\n\n\nTest Cases\n\n\n\nGraph\nEdges\nMaximal Matching\nSize\n\n\n\n\nPath A–B–C\n{(A,B),(B,C)}\n{(A,B)}\n1\n\n\nStar A–B,C,D\n{(A,B),(A,C),(A,D)}\n{(A,B)}\n1\n\n\nSquare A–B–C–D\n{(A,B),(B,C),(C,D),(D,A)}\n{(A,B),(C,D)}\n2\n\n\n\n\n\nComplexity\n\nTime: \\(O(E)\\), single pass through edges\nSpace: \\(O(V)\\) for vertex match tracking\n\nFast, simple, and effective for large graphs where optimal matchings are too expensive.\nMaximal matching gives you a quick and safe pairing, not always perfect, but always done.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-39.-tree-algorithms",
    "href": "books/en-us/list-4.html#section-39.-tree-algorithms",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 39. Tree Algorithms",
    "text": "Section 39. Tree Algorithms\n\n381 Euler Tour LCA\nEuler Tour LCA transforms the problem of finding the Lowest Common Ancestor (LCA) into a Range Minimum Query (RMQ) problem by recording an Euler tour of the tree. It’s elegant, precomputable, and fast, perfect for multiple LCA queries.\n\nWhat Problem Are We Solving?\nGiven a rooted tree and two nodes \\(u\\) and \\(v\\), we want to find their lowest common ancestor, the deepest node that is an ancestor of both.\nFor example, in a tree rooted at \\(1\\):\n      1\n    /   \\\n   2     3\n  / \\   /\n 4  5  6\nThe LCA of \\((4,5)\\) is \\(2\\), and LCA of \\((4,6)\\) is \\(1\\).\nWe want to answer many such queries efficiently.\n\n\nHow It Works (Plain Language)\n\nPerform a DFS traversal (Euler tour):\n\nRecord each node when you enter it.\nWhen you backtrack, record it again.\n\nStore the depth of each node when visited.\nFor each node, remember its first occurrence in the Euler tour.\nFor query \\((u, v)\\):\n\nFind the first occurrences of \\(u\\) and \\(v\\) in the tour.\nTake the range between them.\nThe node with minimum depth in that range is the LCA.\n\n\nSo, LCA reduces to an RMQ over depths.\n\n\nExample\nTree (root = 1):\n1\n├── 2\n│   ├── 4\n│   └── 5\n└── 3\n    └── 6\nEuler tour: \\([1,2,4,2,5,2,1,3,6,3,1]\\)\nDepth array: \\([0,1,2,1,2,1,0,1,2,1,0]\\)\nFirst occurrences: \\(1\\to0,;2\\to1,;3\\to7,;4\\to2,;5\\to4,;6\\to8\\)\nQuery: LCA(4,5)\n\nFirst(4) = 2, First(5) = 4\nDepth range [2..4] = [2,1,2]\nMinimum depth = 1 → node = 2\n\nSo LCA(4,5) = 2.\n\n\nTiny Code (Python)\ndef euler_tour_lca(graph, root):\n    n = len(graph)\n    euler, depth, first = [], [], {}\n    \n    def dfs(u, d):\n        first.setdefault(u, len(euler))\n        euler.append(u)\n        depth.append(d)\n        for v in graph[u]:\n            dfs(v, d + 1)\n            euler.append(u)\n            depth.append(d)\n    \n    dfs(root, 0)\n    return euler, depth, first\n\ndef build_rmq(depth):\n    n = len(depth)\n    log = [0] * (n + 1)\n    for i in range(2, n + 1):\n        log[i] = log[i // 2] + 1\n    k = log[n]\n    st = [[0] * (k + 1) for _ in range(n)]\n    for i in range(n):\n        st[i][0] = i\n    j = 1\n    while (1 &lt;&lt; j) &lt;= n:\n        i = 0\n        while i + (1 &lt;&lt; j) &lt;= n:\n            left = st[i][j - 1]\n            right = st[i + (1 &lt;&lt; (j - 1))][j - 1]\n            st[i][j] = left if depth[left] &lt; depth[right] else right\n            i += 1\n        j += 1\n    return st, log\n\ndef query_lca(u, v, euler, depth, first, st, log):\n    l, r = first[u], first[v]\n    if l &gt; r:\n        l, r = r, l\n    j = log[r - l + 1]\n    left = st[l][j]\n    right = st[r - (1 &lt;&lt; j) + 1][j]\n    return euler[left] if depth[left] &lt; depth[right] else euler[right]\n\n\nWhy It Matters\n\nReduces LCA to RMQ, allowing \\(O(1)\\) query with \\(O(n \\log n)\\) preprocessing\nEasy to combine with Segment Trees, Sparse Tables, or Cartesian Trees\nEssential for:\n\nTree DP with ancestor relationships\nDistance queries: \\[\\text{dist}(u,v) = \\text{depth}(u) + \\text{depth}(v) - 2 \\times \\text{depth}(\\text{LCA}(u,v))\\]\nPath queries in heavy-light decomposition\n\n\n\n\nA Gentle Proof (Why It Works)\nIn DFS traversal:\n\nEvery ancestor appears before and after descendants.\nThe first common ancestor to reappear between two nodes’ first occurrences is their LCA.\n\nTherefore, the node with minimum depth between the first appearances corresponds exactly to the lowest common ancestor.\n\n\nTry It Yourself\n\nConstruct a tree and perform an Euler tour manually.\nWrite down depth and first occurrence arrays.\nPick pairs \\((u,v)\\), find the minimum depth in their range.\nConfirm LCA correctness.\n\n\n\nTest Cases\n\n\n\nQuery\nExpected\nReason\n\n\n\n\nLCA(4,5)\n2\nBoth under 2\n\n\nLCA(4,6)\n1\nRoot is common ancestor\n\n\nLCA(2,3)\n1\nDifferent subtrees\n\n\nLCA(6,3)\n3\nOne is ancestor\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(n \\log n)\\)\nQuery: \\(O(1)\\)\nSpace: \\(O(n \\log n)\\)\n\nEuler Tour LCA shows how tree problems become array problems, a perfect example of structural transformation in algorithms.\n\n\n\n382 Binary Lifting LCA\nBinary Lifting is a fast and elegant technique to find the Lowest Common Ancestor (LCA) of two nodes in a tree using precomputed jumps to ancestors at powers of two. It turns ancestor queries into simple bit operations, giving \\(O(\\log n)\\) per query.\n\nWhat Problem Are We Solving?\nGiven a rooted tree and two nodes \\(u\\) and \\(v\\), we want to find their lowest common ancestor (LCA), the deepest node that is an ancestor of both.\nNaively, we could walk up one node at a time, but that’s \\(O(n)\\) per query. With binary lifting, we jump exponentially up the tree, reducing the cost to \\(O(\\log n)\\).\n\n\nHow It Works (Plain Language)\nBinary lifting precomputes for each node:\n\\[\\text{up}[v][k] = \\text{the } 2^k \\text{-th ancestor of } v\\]\nSo \\(\\text{up}[v][0]\\) is the parent, \\(\\text{up}[v][1]\\) is the grandparent, \\(\\text{up}[v][2]\\) is the ancestor 4 levels up, and so on.\nThe algorithm proceeds in three steps:\n\nPreprocess:\n\nRun DFS from the root.\nRecord each node’s depth.\nFill table up[v][k].\n\nEqualize Depths:\n\nIf one node is deeper, lift it up to match the shallower one.\n\nLift Together:\n\nJump both nodes up together (largest powers first) until their ancestors match.\n\n\nThe meeting point is the LCA.\n\n\nExample\nTree:\n      1\n    /   \\\n   2     3\n  / \\   /\n 4  5  6\nBinary lifting table (up[v][k]):\n\n\n\nv\nup[v][0]\nup[v][1]\nup[v][2]\n\n\n\n\n1\n-\n-\n-\n\n\n2\n1\n-\n-\n\n\n3\n1\n-\n-\n\n\n4\n2\n1\n-\n\n\n5\n2\n1\n-\n\n\n6\n3\n1\n-\n\n\n\nQuery: LCA(4,5)\n\nDepth(4)=2, Depth(5)=2\nLift 4,5 together → parents (2,2) match → LCA = 2\n\nQuery: LCA(4,6)\n\nDepth(4)=2, Depth(6)=2\nLift 4→2, 6→3 → not equal\nLift 2→1, 3→1 → LCA = 1\n\n\n\nTiny Code (Python)\nLOG = 20  # enough for n up to ~1e6\n\ndef preprocess(graph, root):\n    n = len(graph)\n    up = [[-1] * LOG for _ in range(n)]\n    depth = [0] * n\n\n    def dfs(u, p):\n        up[u][0] = p\n        for k in range(1, LOG):\n            if up[u][k-1] != -1:\n                up[u][k] = up[up[u][k-1]][k-1]\n        for v in graph[u]:\n            if v != p:\n                depth[v] = depth[u] + 1\n                dfs(v, u)\n    dfs(root, -1)\n    return up, depth\n\ndef lca(u, v, up, depth):\n    if depth[u] &lt; depth[v]:\n        u, v = v, u\n    diff = depth[u] - depth[v]\n    for k in range(LOG):\n        if diff & (1 &lt;&lt; k):\n            u = up[u][k]\n    if u == v:\n        return u\n    for k in reversed(range(LOG)):\n        if up[u][k] != up[v][k]:\n            u = up[u][k]\n            v = up[v][k]\n    return up[u][0]\n\n\nWhy It Matters\n\nHandles large trees with many queries efficiently\nEnables ancestor jumps, k-th ancestor queries, distance queries, etc.\nFundamental in competitive programming, tree DP, and path queries\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(u\\) and \\(v\\) have different depths, lifting the deeper node by \\(2^k\\) steps ensures we equalize their depth quickly.\nOnce aligned, lifting both simultaneously ensures we never skip the LCA, since jumps are made only when ancestors differ.\nEventually, both meet at their lowest shared ancestor.\n\n\nTry It Yourself\n\nBuild a small tree with 7 nodes.\nCompute \\(\\text{up}[v][k]\\) manually.\nPick pairs and simulate lifting step-by-step.\nVerify with naive path-to-root comparison.\n\n\n\nTest Cases\n\n\n\nQuery\nExpected\nReason\n\n\n\n\nLCA(4,5)\n2\nsame parent\n\n\nLCA(4,6)\n1\nacross subtrees\n\n\nLCA(2,3)\n1\nroot ancestor\n\n\nLCA(6,3)\n3\nancestor\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(n \\log n)\\)\nQuery: \\(O(\\log n)\\)\nSpace: \\(O(n \\log n)\\)\n\nBinary lifting turns tree ancestry into bitwise jumping, offering a clear path from root to ancestor in logarithmic time.\n\n\n\n383 Tarjan’s LCA (Offline DSU)\nTarjan’s LCA algorithm answers multiple LCA queries offline using Disjoint Set Union (DSU) with path compression. It traverses the tree once (DFS) and merges subtrees, answering all queries in \\(O(n + q \\alpha(n))\\) time.\n\nWhat Problem Are We Solving?\nGiven a rooted tree and multiple queries \\((u, v)\\), we want the lowest common ancestor of each pair.\nUnlike binary lifting (which works online), Tarjan’s algorithm works offline:\n\nWe know all queries in advance.\nWe answer them in one DFS traversal using a union-find structure.\n\n\n\nHow It Works (Plain Language)\nWe process the tree bottom-up:\n\nStart a DFS from the root.\nEach node begins as its own set in DSU.\nAfter visiting a child, union it with its parent.\nWhen a node is fully processed, mark it as visited.\nFor each query \\((u,v)\\):\n\nIf the other node has been visited, then \\(\\text{find}(v)\\) (or \\(\\text{find}(u)\\)) gives the LCA.\n\n\nThis works because the DSU structure merges all processed ancestors, so \\(\\text{find}(v)\\) always points to the lowest processed ancestor common to both.\n\n\nExample\nTree (root = 1):\n1\n├── 2\n│   ├── 4\n│   └── 5\n└── 3\n    └── 6\nQueries: \\((4,5), (4,6), (3,6)\\)\nProcess:\n\nDFS(1): visit 2, visit 4\n\nQuery(4,5): 5 not visited → skip\nBacktrack → union(4,2)\n\nDFS(5): mark visited, union(5,2)\n\nQuery(4,5): 4 visited → \\(\\text{find}(4)=2\\) → LCA(4,5)=2\n\nDFS(3), visit 6 → union(6,3)\n\nQuery(4,6): 4 visited, \\(\\text{find}(4)=2\\), \\(\\text{find}(6)=3\\) → no LCA yet\nQuery(3,6): both visited → \\(\\text{find}(3)=3\\) → LCA(3,6)=3\n\nBacktrack 2→1, 3→1 → LCA(4,6)=1\n\n\n\nTiny Code (Python)\nfrom collections import defaultdict\n\nclass DSU:\n    def __init__(self, n):\n        self.parent = list(range(n))\n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n    def union(self, a, b):\n        self.parent[self.find(a)] = self.find(b)\n\ndef tarjan_lca(tree, root, queries):\n    n = len(tree)\n    dsu = DSU(n)\n    visited = [False] * n\n    ancestor = [0] * n\n    ans = {}\n    qmap = defaultdict(list)\n    for u, v in queries:\n        qmap[u].append(v)\n        qmap[v].append(u)\n    def dfs(u):\n        ancestor[u] = u\n        for v in tree[u]:\n            dfs(v)\n            dsu.union(v, u)\n            ancestor[dsu.find(u)] = u\n        visited[u] = True\n        for v in qmap[u]:\n            if visited[v]:\n                ans[(u, v)] = ancestor[dsu.find(v)]\n    dfs(root)\n    return ans\n\n\nWhy It Matters\n\nHandles many LCA queries efficiently in one traversal\nNo need for depth or binary lifting table\nIdeal for offline batch queries\n\nApplications:\n\nTree queries in competitive programming\nOffline graph analysis\nDynamic connectivity in rooted structures\n\n\n\nA Gentle Proof (Why It Works)\nOnce a node \\(u\\) is processed:\n\nAll its descendants are merged into its DSU set.\n\\(\\text{find}(u)\\) always returns the lowest ancestor processed so far.\n\nWhen visiting query \\((u,v)\\):\n\nIf \\(v\\) is already visited, then \\(\\text{find}(v)\\) is the lowest common ancestor of \\(u\\) and \\(v\\).\n\nThe invariant: \\[\n\\text{ancestor}[\\text{find}(v)] = \\text{LCA}(u, v)\n\\] is maintained by post-order traversal and union operations.\n\n\nTry It Yourself\n\nDraw a small tree and list all queries.\nPerform DFS manually.\nTrack unions and ancestor updates.\nRecord LCA when both nodes visited.\n\n\n\nTest Cases\n\n\n\nQuery\nExpected\nReason\n\n\n\n\n(4,5)\n2\nsame subtree\n\n\n(4,6)\n1\nacross subtrees\n\n\n(3,6)\n3\nancestor\n\n\n\n\n\nComplexity\n\nTime: \\(O(n + q \\alpha(n))\\)\nSpace: \\(O(n + q)\\)\n\nEfficient for batch queries; each union and find is nearly constant-time.\nTarjan’s LCA transforms the problem into a union-find dance, answering all ancestor questions in a single elegant DFS sweep.\n\n\n\n384 Heavy-Light Decomposition\nHeavy-Light Decomposition (HLD) splits a tree into heavy paths and light edges so that any path between two nodes can be broken into \\(O(\\log n)\\) segments. It’s the backbone for path queries, sum, max, min, or updates, using segment trees or Fenwick trees.\n\nWhat Problem Are We Solving?\nIn many problems, we need to answer queries like:\n\nWhat’s the sum or max on the path from \\(u\\) to \\(v\\)?\nHow to update values along a path?\nWhat’s the minimum edge weight between two nodes?\n\nNaively, walking the path is \\(O(n)\\) per query. With HLD, we decompose paths into few contiguous segments, reducing queries to \\(O(\\log^2 n)\\) (or better with LCA precomputation).\n\n\nHow It Works (Plain Language)\nEvery node chooses one heavy child (the one with largest subtree). All other edges are light. This guarantees:\n\nEach node is part of exactly one heavy path.\nEvery root-to-leaf path crosses \\(O(\\log n)\\) light edges.\n\nSo we can represent each heavy path as a contiguous segment in an array, and map tree queries → array range queries.\n\nSteps\n\nDFS 1:\n\nCompute subtree size for each node.\nMark heavy child = largest subtree child.\n\nDFS 2:\n\nAssign head of heavy path.\nMap each node to a position in linear order.\n\nPath Query (u, v):\n\nWhile \\(u\\) and \\(v\\) are not in same heavy path:\n\nAlways move the deeper head upward.\nQuery segment tree on that segment.\nJump \\(u\\) to parent of its head.\n\nWhen in same path: query segment from \\(u\\) to \\(v\\) (in order).\n\n\n\n\n\nExample\nTree:\n1\n├── 2\n│   ├── 4\n│   └── 5\n└── 3\n    ├── 6\n    └── 7\n\nSubtree sizes: \\(size(2)=3,; size(3)=3\\) Heavy edges: \\((1,2)\\), \\((2,4)\\), \\((3,6)\\)\nHeavy paths: Path 1: 1 → 2 → 4 Path 2: 3 → 6 Light edges: \\((1,3), (2,5), (3,7)\\)\n\nAny query path \\((4,7)\\) crosses at most \\(\\log n\\) segments:\n4→2→1 | 1→3 | 3→7\n\n\nTiny Code (Python)\ndef dfs1(u, p, g, size, heavy):\n    size[u] = 1\n    max_sub = 0\n    for v in g[u]:\n        if v == p: continue\n        dfs1(v, u, g, size, heavy)\n        size[u] += size[v]\n        if size[v] &gt; max_sub:\n            max_sub = size[v]\n            heavy[u] = v\n\ndef dfs2(u, p, g, head, pos, cur_head, order, heavy):\n    head[u] = cur_head\n    pos[u] = len(order)\n    order.append(u)\n    if heavy[u] != -1:\n        dfs2(heavy[u], u, g, head, pos, cur_head, order, heavy)\n    for v in g[u]:\n        if v == p or v == heavy[u]:\n            continue\n        dfs2(v, u, g, head, pos, v, order, heavy)\n\ndef query_path(u, v, head, pos, depth, segtree, lca):\n    res = 0\n    while head[u] != head[v]:\n        if depth[head[u]] &lt; depth[head[v]]:\n            u, v = v, u\n        res += segtree.query(pos[head[u]], pos[u])\n        u = parent[head[u]]\n    if depth[u] &gt; depth[v]:\n        u, v = v, u\n    res += segtree.query(pos[u], pos[v])\n    return res\n\n\nWhy It Matters\n\nTurns path queries into range queries\nPairs beautifully with segment trees or Fenwick trees\nCore for:\n\nPath sum / max / min\nPath updates\nLowest edge weight\nSubtree queries (with Euler mapping)\n\n\n\n\nA Gentle Proof (Why It Works)\nEvery node moves at most \\(\\log n\\) times up light edges, since subtree size at least halves each time. Within a heavy path, nodes form contiguous segments, so path queries can be broken into \\(O(\\log n)\\) contiguous intervals.\nThus, overall query time is \\(O(\\log^2 n)\\) (or \\(O(\\log n)\\) with segment tree optimization).\n\n\nTry It Yourself\n\nDraw a tree, compute subtree sizes.\nMark heavy edges to largest subtrees.\nAssign head nodes and linear indices.\nTry a query \\((u,v)\\) → decompose into path segments.\nObserve each segment is contiguous in linear order.\n\n\n\nTest Cases\n\n\n\nQuery\nPath\nSegments\n\n\n\n\n(4,5)\n4→2→5\n2\n\n\n(4,7)\n4→2→1→3→7\n3\n\n\n(6,7)\n6→3→7\n2\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(n)\\)\nQuery: \\(O(\\log^2 n)\\) (or \\(O(\\log n)\\) with optimized segment tree)\nSpace: \\(O(n)\\)\n\nHeavy-Light Decomposition bridges tree topology and array queries, enabling logarithmic-time operations on paths.\n\n\n\n385 Centroid Decomposition\nCentroid Decomposition is a divide-and-conquer method that recursively breaks a tree into smaller parts using centroids, nodes that balance the tree when removed. It transforms tree problems into logarithmic-depth recursion, enabling fast queries, updates, and path counting.\n\nWhat Problem Are We Solving?\nFor certain tree problems (distance queries, path sums, subtree coverage, etc.), we need to repeatedly split the tree into manageable pieces.\nA centroid of a tree is a node such that, if removed, no resulting component has more than \\(n/2\\) nodes.\nBy recursively decomposing the tree using centroids, we build a centroid tree, a hierarchical structure capturing the balance of the original tree.\n\n\nHow It Works (Plain Language)\n\nFind the centroid:\n\nCompute subtree sizes.\nPick the node whose largest child subtree ≤ \\(n/2\\).\n\nRecord it as the root of this level’s decomposition.\nRemove the centroid from the tree.\nRecurse on each remaining component (subtree).\n\nEach node appears in \\(O(\\log n)\\) levels, so queries or updates using this structure become logarithmic.\n\n\nExample\nTree (7 nodes):\n      1\n    / | \\\n   2  3  4\n  / \\\n 5   6\n      \\\n       7\nStep 1: \\(n=7\\). Subtree sizes → pick 2 (its largest child subtree ≤ 4). Centroid = 2.\nStep 2: Remove 2 → split into components:\n\n{5}, {6,7}, {1,3,4}\n\nStep 3: Recursively find centroids in each component.\n\n{6,7} → centroid = 6\n{1,3,4} → centroid = 1\n\nCentroid tree hierarchy:\n     2\n   / | \\\n  5  6  1\n       \\\n        3\n\n\nTiny Code (Python)\ndef build_centroid_tree(graph):\n    n = len(graph)\n    size = [0] * n\n    removed = [False] * n\n    parent = [-1] * n\n\n    def dfs_size(u, p):\n        size[u] = 1\n        for v in graph[u]:\n            if v == p or removed[v]:\n                continue\n            dfs_size(v, u)\n            size[u] += size[v]\n\n    def find_centroid(u, p, n):\n        for v in graph[u]:\n            if v != p and not removed[v] and size[v] &gt; n // 2:\n                return find_centroid(v, u, n)\n        return u\n\n    def decompose(u, p):\n        dfs_size(u, -1)\n        c = find_centroid(u, -1, size[u])\n        removed[c] = True\n        parent[c] = p\n        for v in graph[c]:\n            if not removed[v]:\n                decompose(v, c)\n        return c\n\n    root = decompose(0, -1)\n    return parent  # parent[c] is centroid parent\n\n\nWhy It Matters\nCentroid decomposition allows efficient solutions to many tree problems:\n\nPath queries: distance sums, min/max weights\nPoint updates: affect only \\(O(\\log n)\\) centroids\nDistance-based search: find nearest node of a type\nDivide-and-conquer DP on trees\n\nIt’s especially powerful in competitive programming for problems like count pairs within distance k, color-based queries, or centroid tree construction.\n\n\nA Gentle Proof (Why It Works)\nEvery decomposition step removes one centroid. By definition, each remaining subtree ≤ \\(n/2\\). Thus, recursion depth is \\(O(\\log n)\\), and every node participates in at most \\(O(\\log n)\\) subproblems.\nTotal complexity: \\[\nT(n) = T(n_1) + T(n_2) + \\cdots + O(n) \\le O(n \\log n)\n\\]\n\n\nTry It Yourself\n\nDraw a small tree.\nCompute subtree sizes.\nPick centroid (largest child ≤ \\(n/2\\)).\nRemove and recurse.\nBuild centroid tree hierarchy.\n\n\n\nTest Cases\n\n\n\nTree\nCentroid\nComponents\nDepth\n\n\n\n\nLine (1–2–3–4–5)\n3\n{1,2}, {4,5}\n2\n\n\nStar (1 center)\n1\nleaves\n1\n\n\nBalanced tree (7 nodes)\n2\n3 components\n2\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(n \\log n)\\)\nQuery/Update (per node): \\(O(\\log n)\\)\nSpace: \\(O(n)\\)\n\nCentroid Decomposition turns trees into balanced recursive hierarchies, a universal tool for logarithmic-time query and update systems.\n\n\n\n386 Tree Diameter (DFS Twice)\nThe tree diameter is the longest path between any two nodes in a tree. A simple and elegant method to find it is to perform two DFS traversals, one to find the farthest node, and another to measure the longest path length.\n\nWhat Problem Are We Solving?\nIn a tree (an acyclic connected graph), we want to find the diameter, defined as:\n\\[\n\\text{diameter} = \\max_{u,v \\in V} \\text{dist}(u, v)\n\\]\nwhere \\(\\text{dist}(u,v)\\) is the length (in edges or weights) of the shortest path between \\(u\\) and \\(v\\).\nThis path always lies between two leaf nodes, and can be found using two DFS/BFS traversals.\n\n\nHow It Works (Plain Language)\n\nPick any node (say 1).\nRun DFS to find the farthest node from it → call it \\(A\\).\nRun DFS again from \\(A\\) → find the farthest node from \\(A\\) → call it \\(B\\).\nThe distance between \\(A\\) and \\(B\\) is the diameter length.\n\nWhy this works:\n\nThe first DFS ensures you start one end of the diameter.\nThe second DFS stretches to the opposite end.\n\n\n\nExample\nTree:\n1\n├── 2\n│   └── 4\n└── 3\n    ├── 5\n    └── 6\n\nDFS(1): farthest node = 4 (distance 2)\nDFS(4): farthest node = 6 (distance 4)\n\nSo diameter = 4 → path: \\(4 \\to 2 \\to 1 \\to 3 \\to 6\\)\n\n\nTiny Code (Python)\nfrom collections import defaultdict\n\ndef dfs(u, p, dist, graph):\n    farthest = (u, dist)\n    for v, w in graph[u]:\n        if v == p:\n            continue\n        cand = dfs(v, u, dist + w, graph)\n        if cand[1] &gt; farthest[1]:\n            farthest = cand\n    return farthest\n\ndef tree_diameter(graph):\n    start = 0\n    a, _ = dfs(start, -1, 0, graph)\n    b, diameter = dfs(a, -1, 0, graph)\n    return diameter, (a, b)\n\n# Example:\ngraph = defaultdict(list)\nedges = [(0,1,1),(1,3,1),(0,2,1),(2,4,1),(2,5,1)]\nfor u,v,w in edges:\n    graph[u].append((v,w))\n    graph[v].append((u,w))\n\nd, (a,b) = tree_diameter(graph)\nprint(d, (a,b))  # 4, path (3,5)\n\n\nWhy It Matters\n\nSimple and linear time: \\(O(n)\\)\nWorks for weighted and unweighted trees\nCore for:\n\nTree center finding\nDynamic programming on trees\nNetwork analysis (longest delay, max latency path)\n\n\n\n\nA Gentle Proof (Why It Works)\nLet \\((u, v)\\) be the true diameter endpoints. Any DFS from any node \\(x\\) finds a farthest node \\(A\\). By triangle inequality on trees, \\(A\\) must be one endpoint of a diameter path. A second DFS from \\(A\\) finds the other endpoint \\(B\\), ensuring \\(\\text{dist}(A,B)\\) is the maximum possible.\n\n\nTry It Yourself\n\nDraw a small tree.\nPick any starting node, run DFS to find farthest.\nFrom that node, run DFS again.\nTrace the path between these two nodes, it’s always the longest.\n\n\n\nTest Cases\n\n\n\nTree Type\nDiameter\nPath\n\n\n\n\nLine (1–2–3–4)\n3\n(1,4)\n\n\nStar (1 center + leaves)\n2\n(leaf₁, leaf₂)\n\n\nBalanced binary\n4\nleftmost to rightmost leaf\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) recursion stack\n\nTree Diameter via two DFS is one of the cleanest tricks in graph theory, from any point, go far, then farther.\n\n\n\n387 Tree DP (Subtree-Based Optimization)\nTree Dynamic Programming (Tree DP) is a technique that solves problems on trees by combining results from subtrees. Each node’s result depends on its children, following a recursive pattern much like divide and conquer but on a tree.\n\nWhat Problem Are We Solving?\nMany problems on trees ask for optimal values (sum, min, max, count) that depend on child subtrees, for example:\n\nMaximum path sum in a tree\nSize of largest independent set\nNumber of ways to color a tree\nSum of distances to all nodes\n\nTree DP provides a bottom-up approach where we compute results for each subtree and merge them upwards.\n\n\nHow It Works (Plain Language)\n\nRoot the tree at an arbitrary node (often 1 or 0).\nDefine a DP state at each node based on its subtree.\nCombine children’s results recursively.\nReturn the result to the parent.\n\nExample state: \\[\ndp[u] = f(dp[v_1], dp[v_2], \\dots, dp[v_k])\n\\] where \\(v_i\\) are children of \\(u\\).\n\n\nExample Problem\nFind the size of the largest independent set (no two adjacent nodes chosen).\nRecurrence:\n\\[\ndp[u][0] = \\sum_{v \\in children(u)} \\max(dp[v][0], dp[v][1])\n\\]\n\\[\ndp[u][1] = 1 + \\sum_{v \\in children(u)} dp[v][0]\n\\]\n\n\nTiny Code (Python)\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nedges = [(0,1),(0,2),(1,3),(1,4)]\nfor u,v in edges:\n    graph[u].append(v)\n    graph[v].append(u)\n\ndef dfs(u, p):\n    incl = 1  # include u\n    excl = 0  # exclude u\n    for v in graph[u]:\n        if v == p:\n            continue\n        inc_v, exc_v = dfs(v, u)\n        incl += exc_v\n        excl += max(inc_v, exc_v)\n    return incl, excl\n\nincl, excl = dfs(0, -1)\nans = max(incl, excl)\nprint(ans)  # 3\nThis finds the largest independent set size (3 nodes).\n\n\nWhy It Matters\nTree DP is foundational for:\n\nCounting: number of ways to assign labels, colors, or states\nOptimization: maximize or minimize cost over paths or sets\nCombinatorics: solve partition or constraint problems\nGame theory: compute win/loss states recursively\n\nIt turns global tree problems into local merges.\n\n\nA Gentle Proof (Why It Works)\nEach node’s subtree is independent once you know whether its parent is included or not. By processing children before parents (post-order DFS), we ensure all necessary data is ready when merging. This guarantees correctness via structural induction on tree size.\n\n\nTry It Yourself\n\nDefine a simple property (sum, count, max).\nWrite recurrence for a node in terms of its children.\nTraverse via DFS, merge children’s results.\nReturn value to parent, combine at root.\n\n\n\nTemplate (Generic)\ndef dfs(u, p):\n    res = base_case()\n    for v in children(u):\n        if v != p:\n            child = dfs(v, u)\n            res = merge(res, child)\n    return finalize(res)\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) (recursion)\n\nTree DP transforms recursive thinking into structured computation, one subtree at a time, building the answer from the leaves to the root.\n\n\n\n388 Rerooting DP (Compute All Roots’ Answers)\nRerooting DP is an advanced tree dynamic programming technique that computes answers for every possible root efficiently. Instead of recomputing from scratch, we reuse results by propagating information down and up the tree.\n\nWhat Problem Are We Solving?\nSuppose we want the same value for every node as if that node were the root. Examples:\n\nSum of distances from each node to all others\nNumber of nodes in each subtree\nDP value that depends on subtree structure\n\nA naive approach runs \\(O(n)\\) DP per root → \\(O(n^2)\\) total. Rerooting reduces this to \\(O(n)\\) total.\n\n\nHow It Works (Plain Language)\n\nFirst pass (downward DP): Compute answers for each subtree assuming root = 0.\nSecond pass (rerooting): Use parent’s info to update child’s answer — effectively “rerooting” the DP at each node.\n\nThe key is combining results from children excluding the child itself, often using prefix–suffix merges.\n\n\nExample Problem\nCompute sum of distances from each node to all others.\nRecurrence:\n\nsubtree_size[u] = number of nodes in subtree of u\ndp[u] = sum of distances from u to nodes in its subtree\n\nFirst pass: \\[\ndp[u] = \\sum_{v \\in children(u)} (dp[v] + subtree_size[v])\n\\]\nSecond pass: When moving root from u to v: \\[\ndp[v] = dp[u] - subtree_size[v] + (n - subtree_size[v])\n\\]\n\n\nTiny Code (Python)\nfrom collections import defaultdict\n\ngraph = defaultdict(list)\nedges = [(0,1),(0,2),(2,3),(2,4)]\nfor u,v in edges:\n    graph[u].append(v)\n    graph[v].append(u)\n\nn = 5\nsize = [1]*n\ndp = [0]*n\nans = [0]*n\n\ndef dfs1(u, p):\n    for v in graph[u]:\n        if v == p: continue\n        dfs1(v, u)\n        size[u] += size[v]\n        dp[u] += dp[v] + size[v]\n\ndef dfs2(u, p):\n    ans[u] = dp[u]\n    for v in graph[u]:\n        if v == p: continue\n        pu, pv = dp[u], dp[v]\n        su, sv = size[u], size[v]\n\n        dp[u] -= dp[v] + size[v]\n        size[u] -= size[v]\n        dp[v] += dp[u] + size[u]\n        size[v] += size[u]\n\n        dfs2(v, u)\n\n        dp[u], dp[v] = pu, pv\n        size[u], size[v] = su, sv\n\ndfs1(0, -1)\ndfs2(0, -1)\n\nprint(ans)\nThis computes the sum of distances for each node.\n\n\nWhy It Matters\n\nEfficiently derive all-root answers\nCommon in centroid problems, sum of distances, rerooting sums\nReduces redundant recomputation dramatically\n\n\n\nA Gentle Proof (Why It Works)\nEvery DP depends only on local merges of subtrees. By removing a child subtree and adding the rest, we adjust the parent’s value to reflect a new root. Induction over tree structure ensures correctness.\n\n\nTry It Yourself\n\nWrite your DP recurrence for a single root.\nIdentify what changes when rerooting to a child.\nApply push–pull updates using stored subtree info.\nCollect final answers after traversal.\n\n\n\nComplexity\n\nTime: \\(O(n)\\) (two DFS passes)\nSpace: \\(O(n)\\)\n\nRerooting DP turns a global rerooting problem into a pair of local transformations, computing every node’s perspective with elegant reuse.\n\n\n\n389 Binary Search on Tree (Edge Weight Constraints)\nBinary Search on Tree is a versatile strategy used to solve problems where we must find a threshold edge weight or path condition that satisfies a constraint. It combines DFS or BFS traversal with binary search over a numeric property, often edge weights or limits.\n\nWhat Problem Are We Solving?\nGiven a tree with weighted edges, we might want to answer questions like:\n\nWhat is the minimum edge weight so that the path between two nodes satisfies some property?\nWhat is the maximum allowed cost under which the tree remains connected?\nWhat is the smallest threshold that allows at least \\(k\\) nodes to be reachable?\n\nA naive approach checks each possible weight. We can do better by binary searching over sorted edge weights.\n\n\nHow It Works (Plain Language)\n\nSort the edge weights or define a numeric search range.\nBinary search over possible threshold \\(T\\).\nFor each mid value, traverse the tree (via DFS/BFS/Union-Find) including only edges satisfying a condition (e.g., weight ≤ T).\nCheck if the property holds.\nNarrow the search interval.\n\nThe key is monotonicity: if a property holds for \\(T\\), it holds for all larger/smaller values.\n\n\nExample Problem\nFind the minimum edge weight threshold \\(T\\) so that all nodes become connected.\nAlgorithm:\n\nSort edges by weight.\nBinary search \\(T\\) in \\([min_weight, max_weight]\\).\nFor each \\(T\\), include edges with \\(w \\le T\\).\nCheck if resulting graph is connected.\nAdjust bounds accordingly.\n\n\n\nTiny Code (Python)\ndef is_connected(n, edges, limit):\n    parent = list(range(n))\n    def find(x):\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n    def union(a, b):\n        pa, pb = find(a), find(b)\n        if pa != pb:\n            parent[pb] = pa\n\n    for u, v, w in edges:\n        if w &lt;= limit:\n            union(u, v)\n    return len({find(i) for i in range(n)}) == 1\n\ndef binary_search_tree_threshold(n, edges):\n    weights = sorted(set(w for _,_,w in edges))\n    lo, hi = 0, len(weights)-1\n    ans = weights[-1]\n    while lo &lt;= hi:\n        mid = (lo + hi) // 2\n        if is_connected(n, edges, weights[mid]):\n            ans = weights[mid]\n            hi = mid - 1\n        else:\n            lo = mid + 1\n    return ans\n\nedges = [(0,1,4),(1,2,2),(0,2,5)]\nprint(binary_search_tree_threshold(3, edges))  # Output: 4\n\n\nWhy It Matters\n\nReduces complex “threshold” problems to logarithmic searches\nWorks when solution space is monotonic\nCombines structural traversal with decision logic\n\nCommon applications:\n\nPath feasibility with limit\nEdge filtering by cost\nTree queries with weight bounds\n\n\n\nA Gentle Proof (Why It Works)\nLet property \\(P(T)\\) be true if condition holds for threshold \\(T\\). If \\(P(T)\\) is monotonic (either always true beyond some point, or always false before), then binary search correctly converges to the minimal/maximal satisfying \\(T\\).\n\n\nTry It Yourself\n\nDefine a monotonic property on weights (e.g. “connected under limit”).\nImplement a decision function checking \\(P(T)\\).\nWrap it in a binary search loop.\nReturn the minimal/maximal valid \\(T\\).\n\n\n\nComplexity\n\nSorting edges: \\(O(E \\log E)\\)\nBinary search: \\(O(\\log E)\\) checks\nEach check: \\(O(E \\alpha(V))\\) (Union-Find)\nOverall: \\(O(E \\log^2 E)\\) or better\n\nBinary search on trees is not about searching within the tree, but over constraints defined on the tree, a sharp technique for threshold-based reasoning.\n\n\n\n390 Virtual Tree (Query Subset Construction)\nA Virtual Tree is a compressed representation of a tree built from a selected subset of nodes, along with their Lowest Common Ancestors (LCAs), connected in the same hierarchical order as the original tree. It’s used in query problems where we only need to reason about a small subset of nodes rather than the full tree.\n\nWhat Problem Are We Solving?\nSuppose we have a large tree and a query gives us a small set \\(S\\) of nodes. We need to compute some property among nodes in \\(S\\) (like sum of distances, coverage, or DP over their paths). Traversing the entire tree each time is inefficient.\nA Virtual Tree shrinks the full tree down to just the nodes in \\(S\\) plus their LCAs, keeping ancestry relationships intact.\nThis reduces the problem to a much smaller tree, often \\(O(|S|\\log |S|)\\) nodes instead of \\(O(n)\\).\n\n\nHow It Works (Plain Language)\n\nCollect the query nodes \\(S\\).\nSort \\(S\\) by Euler Tour order (entry time in DFS).\nInsert LCAs of consecutive nodes to maintain structure.\nBuild edges between consecutive ancestors to form a tree.\nNow process queries on this mini-tree instead of the full one.\n\n\n\nExample\nGiven tree:\n       1\n     / | \\\n    2  3  4\n      / \\\n     5   6\nQuery nodes \\(S = {2, 5, 6}\\).\nTheir LCAs:\n\n\\(\\text{LCA}(5,6)=3\\)\n\\(\\text{LCA}(2,3)=1\\)\n\nVirtual Tree Nodes = \\({1,2,3,5,6}\\) Connect them according to parent-child relations: 1 → 2 1 → 3 3 → 5 3 → 6\nThis is your Virtual Tree.\n\n\nTiny Code (C++ Style Pseudocode)\nvector&lt;int&gt; build_virtual_tree(vector&lt;int&gt;& S, vector&lt;int&gt;& tin, auto lca) {\n    sort(S.begin(), S.end(), [&](int a, int b) { return tin[a] &lt; tin[b]; });\n    vector&lt;int&gt; nodes = S;\n    for (int i = 0; i + 1 &lt; (int)S.size(); i++)\n        nodes.push_back(lca(S[i], S[i+1]));\n    sort(nodes.begin(), nodes.end(), [&](int a, int b) { return tin[a] &lt; tin[b]; });\n    nodes.erase(unique(nodes.begin(), nodes.end()), nodes.end());\n    stack&lt;int&gt; st;\n    vector&lt;vector&lt;int&gt;&gt; vt_adj(nodes.size());\n    for (int u : nodes) {\n        while (!st.empty() && !is_ancestor(st.top(), u))\n            st.pop();\n        if (!st.empty())\n            vt_adj[st.top()].push_back(u);\n        st.push(u);\n    }\n    return nodes;\n}\nThis builds adjacency for the virtual tree using stack ancestry.\n\n\nWhy It Matters\n\nReduces large-tree queries to small subproblems\nEssential in problems with multiple subset queries\nCommon in offline processing, rerooting DP, distance sum problems\nWorks perfectly with LCA precomputation\n\n\n\nA Gentle Proof (Why It Works)\nEach node in \\(S\\) must connect through its LCAs. By sorting in Euler order and maintaining a stack of ancestors, we ensure that:\n\nParent-child relations are consistent\nNo cycles or duplicates\nThe resulting tree preserves ancestry\n\nSo the virtual tree is a minimal subtree connecting all nodes in \\(S\\).\n\n\nTry It Yourself\n\nImplement Euler Tour (get tin[u]).\nImplement LCA (Binary Lifting or RMQ).\nBuild Virtual Tree for given \\(S\\).\nApply your query logic (sum, count, DP).\n\n\n\nComplexity\n\nSorting \\(S\\): \\(O(|S|\\log|S|)\\)\nLCA calls: \\(O(|S|)\\)\nBuilding structure: \\(O(|S|)\\)\nOverall: \\(O(|S|\\log|S|)\\) per query\n\nA Virtual Tree is your “query-scaled tree”, a precise projection of the big tree onto the small world of your problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-4.html#section-40.-advanced-graph-algorithms-and-tricks",
    "href": "books/en-us/list-4.html#section-40.-advanced-graph-algorithms-and-tricks",
    "title": "Chapter 4. Graph Algorithms",
    "section": "Section 40. Advanced Graph Algorithms and Tricks",
    "text": "Section 40. Advanced Graph Algorithms and Tricks\n\n391 Topological DP (Dynamic Programming on DAG)\nTopological DP is a dynamic programming technique on Directed Acyclic Graphs (DAGs). It computes values in dependency order, ensuring each node’s state is calculated only after all its prerequisites. This method is a core tool for problems involving partial orders, longest paths, counting paths, and propagation across dependencies.\n\nWhat Problem Are We Solving?\nIn DAGs, some nodes depend on others (edges point from dependencies to dependents). We often want to compute a DP value like:\n\nLongest path ending at each node\nNumber of ways to reach a node\nMinimum cost to reach a node\nAccumulated value through dependencies\n\nBecause DAGs have no cycles, a topological order exists, allowing linear-time evaluation.\n\n\nHow It Works (Plain Language)\n\nTopologically sort the DAG.\nInitialize base cases (e.g. sources = 0 or 1).\nIterate in topo order, updating each node based on incoming edges.\nEach node’s value is final once processed.\n\nThis guarantees each dependency is resolved before use.\n\n\nExample: Longest Path in DAG\nGiven DAG with edge weights \\(w(u,v)\\), define DP:\n\\[\ndp[v] = \\max_{(u,v)\\in E}(dp[u] + w(u,v))\n\\]\nBase case: \\(dp[source] = 0\\)\n\n\nExample DAG\n1 → 2 → 4\n ↘︎ 3 ↗︎\nEdges:\n\n1 → 2 (1)\n1 → 3 (2)\n2 → 4 (1)\n3 → 4 (3)\n\nTopological order: [1, 2, 3, 4]\nCompute:\n\ndp[1] = 0\ndp[2] = max(dp[1] + 1) = 1\ndp[3] = max(dp[1] + 2) = 2\ndp[4] = max(dp[2] + 1, dp[3] + 3) = max(2, 5) = 5\n\nResult: Longest path length = 5\n\n\nTiny Code (C++ Style)\nvector&lt;int&gt; topo_sort(int n, vector&lt;vector&lt;int&gt;&gt;& adj) {\n    vector&lt;int&gt; indeg(n, 0), topo;\n    for (auto& u : adj)\n        for (int v : u) indeg[v]++;\n    queue&lt;int&gt; q;\n    for (int i = 0; i &lt; n; i++) if (indeg[i] == 0) q.push(i);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        topo.push_back(u);\n        for (int v : adj[u])\n            if (--indeg[v] == 0) q.push(v);\n    }\n    return topo;\n}\n\nvector&lt;int&gt; topo_dp(int n, vector&lt;vector&lt;pair&lt;int,int&gt;&gt;&gt;& adj) {\n    auto order = topo_sort(n, ...);\n    vector&lt;int&gt; dp(n, INT_MIN);\n    dp[0] = 0;\n    for (int u : order)\n        for (auto [v, w] : adj[u])\n            dp[v] = max(dp[v], dp[u] + w);\n    return dp;\n}\n\n\nWhy It Matters\n\nConverts recursive dependencies into iterative computation\nAvoids redundant work\nEnables linear-time solutions for many DAG problems\nWorks for counting, min/max, aggregation tasks\n\nCommon uses:\n\nLongest path in DAG\nCounting number of paths\nMinimum cost scheduling\nProject dependency planning\n\n\n\nA Gentle Proof (Why It Works)\nA topological order guarantees:\n\nFor every edge \\((u,v)\\), \\(u\\) appears before \\(v\\) Thus when processing \\(v\\), all \\(dp[u]\\) for predecessors \\(u\\) are ready. This ensures correctness for any DP formula of the form:\n\n\\[\ndp[v] = f({dp[u]\\ |\\ (u,v)\\in E})\n\\]\n\n\nTry It Yourself\n\nCompute number of paths from source to each node: \\(dp[v] = \\sum_{(u,v)} dp[u]\\)\nCompute minimum cost if edges have weights\nBuild a longest chain of tasks with dependencies\nApply topological DP on SCC DAG (meta-graph)\n\n\n\nComplexity\n\nTopo sort: \\(O(V+E)\\)\nDP propagation: \\(O(V+E)\\)\nTotal: \\(O(V+E)\\) time, \\(O(V)\\) space\n\nTopological DP is how you bring order to dependency chaos, one layer, one node, one dependency at a time.\n\n\n\n392 SCC Condensed Graph DP (Dynamic Programming on Meta-Graph)\nSCC Condensed Graph DP applies dynamic programming to the condensation of a directed graph into a Directed Acyclic Graph (DAG), where each node represents a strongly connected component (SCC). This transforms a cyclic graph into an acyclic one, enabling topological reasoning, path aggregation, and value propagation across components.\n\nWhat Problem Are We Solving?\nMany problems on directed graphs become complex due to cycles. Within each SCC, every node can reach every other, they are strongly connected. By condensing SCCs into single nodes, we obtain a DAG:\n\\[\nG' = (V', E')\n\\]\nwhere each \\(v' \\in V'\\) is an SCC, and edges \\((u', v')\\) represent transitions between components.\nOnce the graph is acyclic, we can run Topological DP to compute:\n\nMaximum or minimum value reaching each SCC\nNumber of paths between components\nAggregated scores, weights, or costs\n\n\n\nHow It Works (Plain Language)\n\nFind SCCs (using Tarjan or Kosaraju).\nBuild Condensed DAG: each SCC becomes a single node.\nAggregate initial values for each SCC (e.g. sum of weights).\nRun DP over DAG in topological order, combining contributions from incoming edges.\nMap back results to original nodes if needed.\n\nThis isolates cycles (internal SCCs) and manages dependencies cleanly.\n\n\nExample\nOriginal graph \\(G\\):\n1 → 2 → 3\n↑    ↓\n5 ← 4\nSCCs:\n\nC₁ = {1,2,4,5}\nC₂ = {3}\n\nCondensed graph:\nC₁ → C₂\nIf each node has weight \\(w[i]\\), then:\n\\[\ndp[C₂] = \\max_{(C₁,C₂)}(dp[C₁] + \\text{aggregate}(C₁))\n\\]\n\n\nTiny Code (C++ Style)\nint n;\nvector&lt;vector&lt;int&gt;&gt; adj;\nvector&lt;int&gt; comp, order;\nvector&lt;bool&gt; vis;\n\nvoid dfs1(int u) {\n    vis[u] = true;\n    for (int v : adj[u]) if (!vis[v]) dfs1(v);\n    order.push_back(u);\n}\n\nvoid dfs2(int u, int c, vector&lt;vector&lt;int&gt;&gt;& radj) {\n    comp[u] = c;\n    for (int v : radj[u]) if (comp[v] == -1) dfs2(v, c, radj);\n}\n\nvector&lt;int&gt; scc_condense() {\n    vector&lt;vector&lt;int&gt;&gt; radj(n);\n    for (int u=0; u&lt;n; ++u)\n        for (int v : adj[u]) radj[v].push_back(u);\n\n    vis.assign(n,false);\n    for (int i=0; i&lt;n; ++i) if (!vis[i]) dfs1(i);\n\n    comp.assign(n,-1);\n    int cid=0;\n    for (int i=n-1;i&gt;=0;--i){\n        int u = order[i];\n        if (comp[u]==-1) dfs2(u,cid++,radj);\n    }\n\n    return comp;\n}\nThen build condensed graph:\nvector&lt;vector&lt;int&gt;&gt; dag(cid);\nfor (int u=0;u&lt;n;++u)\n  for (int v:adj[u])\n    if (comp[u]!=comp[v])\n      dag[comp[u]].push_back(comp[v]);\nRun Topological DP on dag.\n\n\nWhy It Matters\n\nTurns cyclic graphs into acyclic DAGs\nAllows DP, path counting, and aggregation in cyclic contexts\nSimplifies reasoning about reachability and influence\nForms the basis for:\n\nDynamic condensation\nMeta-graph optimization\nModular graph analysis\n\n\n\n\nA Gentle Proof (Why It Works)\n\nWithin each SCC, all nodes are mutually reachable.\nCondensation merges these into single nodes, ensuring no cycles.\nEdges between SCCs define a partial order, enabling topological sorting.\nAny property defined recursively over dependencies can now be solved via DP on this order.\n\nFormally, for each \\(C_i\\):\n\\[\ndp[C_i] = f({dp[C_j] \\mid (C_j, C_i) \\in E'}, \\text{value}(C_i))\n\\]\n\n\nTry It Yourself\n\nAssign a weight to each node; compute max sum path over SCC DAG.\nCount distinct paths between SCCs.\nCombine SCC detection + DP for weighted reachability.\nSolve problems like:\n\n“Maximum gold in a dungeon with teleport cycles”\n“Dependency graph with feedback loops”\n\n\n\n\nComplexity\n\nSCC computation: \\(O(V+E)\\)\nDAG construction: \\(O(V+E)\\)\nTopo DP: \\(O(V'+E')\\)\nTotal: \\(O(V+E)\\)\n\nSCC Condensed Graph DP is the art of shrinking cycles into certainty, revealing a clean DAG beneath the tangled surface.\n\n\n\n393 Eulerian Path\nAn Eulerian Path is a trail in a graph that visits every edge exactly once. If the path starts and ends at the same vertex, it is called an Eulerian Circuit. This concept lies at the heart of route planning, graph traversals, and network analysis.\n\nWhat Problem Are We Solving?\nWe want to find a path that uses every edge once, no repeats, no omissions.\nIn an undirected graph, an Eulerian Path exists if and only if:\n\nExactly 0 or 2 vertices have odd degree\nThe graph is connected\n\nIn a directed graph, it exists if and only if:\n\nAt most one vertex has outdegree - indegree = 1 (start)\nAt most one vertex has indegree - outdegree = 1 (end)\nAll other vertices have equal in-degree and out-degree\nThe graph is strongly connected (or connected when ignoring direction)\n\n\n\nHow Does It Work (Plain Language)\n\nCheck degree conditions (undirected) or in/out-degree balance (directed).\nChoose a start vertex:\n\nFor undirected: any odd-degree vertex (if exists), otherwise any vertex.\nFor directed: vertex with outdeg = indeg + 1, else any.\n\nApply Hierholzer’s algorithm:\n\nWalk edges greedily until stuck.\nBacktrack and merge cycles into one path.\n\nReverse the constructed order for the final path.\n\n\n\nExample (Undirected)\nGraph edges:\n1—2, 2—3, 3—1, 2—4\nDegrees:\n\ndeg(1)=2, deg(2)=3, deg(3)=2, deg(4)=1 Odd vertices: 2, 4 → path exists (starts at 2, ends at 4)\n\nEulerian Path: 2 → 1 → 3 → 2 → 4\n\n\nExample (Directed)\nEdges:\nA → B, B → C, C → A, C → D\nDegrees:\n\nA: out=1, in=1\nB: out=1, in=1\nC: out=2, in=1\nD: out=0, in=1\n\nStart: C (out=in+1), End: D (in=out+1)\nEulerian Path: C → A → B → C → D\n\n\nTiny Code (C++)\nvector&lt;vector&lt;int&gt;&gt; adj;\nvector&lt;int&gt; path;\n\nvoid dfs(int u) {\n    while (!adj[u].empty()) {\n        int v = adj[u].back();\n        adj[u].pop_back();\n        dfs(v);\n    }\n    path.push_back(u);\n}\nRun from the start vertex, then reverse path.\n\n\nTiny Code (Python)\ndef eulerian_path(graph, start):\n    stack, path = [start], []\n    while stack:\n        u = stack[-1]\n        if graph[u]:\n            v = graph[u].pop()\n            stack.append(v)\n        else:\n            path.append(stack.pop())\n    return path[::-1]\n\n\nWhy It Matters\n\nFoundational for graph traversal problems\nUsed in:\n\nDNA sequencing (De Bruijn graph reconstruction)\nRoute planning (postal delivery, garbage collection)\nNetwork diagnostics (tracing all connections)\n\n\n\n\nA Gentle Proof (Why It Works)\nEach edge must appear exactly once. At every vertex (except possibly start/end), every entry must be matched with an exit. This requires balanced degrees.\nIn an Eulerian Circuit: \\[\n\\text{in}(v) = \\text{out}(v) \\quad \\forall v\n\\]\nIn an Eulerian Path: \\[\n\\exists \\text{start with } \\text{out}(v)=\\text{in}(v)+1 \\\n\\exists \\text{end with } \\text{in}(v)=\\text{out}(v)+1\n\\]\nHierholzer’s algorithm constructs the path by merging cycles and ensuring all edges are consumed.\n\n\nTry It Yourself\n\nBuild a small graph and test parity conditions.\nImplement Hierholzer’s algorithm and trace each step.\nVerify correctness by counting traversed edges.\nExplore both directed and undirected variants.\nModify to detect Eulerian circuit vs path.\n\n\n\nComplexity\n\nTime: \\(O(V+E)\\)\nSpace: \\(O(V+E)\\)\n\nEulerian paths are elegant because they cover every connection exactly once, perfect order in perfect traversal.\n\n\n\n394 Hamiltonian Path\nA Hamiltonian Path is a path in a graph that visits every vertex exactly once. If it starts and ends at the same vertex, it forms a Hamiltonian Cycle. Unlike the Eulerian Path (which focuses on edges), the Hamiltonian Path focuses on vertices.\nFinding one is a classic NP-complete problem, there’s no known polynomial-time algorithm for general graphs.\n\nWhat Problem Are We Solving?\nWe want to determine if there exists a path that visits every vertex exactly once, no repetition, no omission.\nIn formal terms: Given a graph \\(G = (V, E)\\), find a sequence of vertices \\[v_1, v_2, \\ldots, v_n\\] such that \\((v_i, v_{i+1}) \\in E\\) for all \\(i\\), and all vertices are distinct.\nFor a Hamiltonian Cycle, additionally \\((v_n, v_1) \\in E\\).\n\n\nHow Does It Work (Plain Language)\nThere’s no simple parity or degree condition like in Eulerian paths. We usually solve it by backtracking, bitmask DP, or heuristics (for large graphs):\n\nPick a start vertex.\nRecursively explore all unvisited neighbors.\nMark visited vertices.\nIf all vertices are visited → found a Hamiltonian Path.\nOtherwise, backtrack.\n\nFor small graphs, this brute force works; for large graphs, it’s impractical.\n\n\nExample (Undirected Graph)\nGraph:\n1, 2, 3\n|    \\   |\n4 ———— 5\nOne Hamiltonian Path: 1 → 2 → 3 → 5 → 4 One Hamiltonian Cycle: 1 → 2 → 3 → 5 → 4 → 1\n\n\nExample (Directed Graph)\nA → B → C\n↑       ↓\nE ← D ← \nPossible Hamiltonian Path: A → B → C → D → E\n\n\nTiny Code (Backtracking – C++)\nbool hamiltonianPath(int u, vector&lt;vector&lt;int&gt;&gt;& adj, vector&lt;bool&gt;& visited, vector&lt;int&gt;& path, int n) {\n    if (path.size() == n) return true;\n    for (int v : adj[u]) {\n        if (!visited[v]) {\n            visited[v] = true;\n            path.push_back(v);\n            if (hamiltonianPath(v, adj, visited, path, n)) return true;\n            visited[v] = false;\n            path.pop_back();\n        }\n    }\n    return false;\n}\nCall with each vertex as a potential start.\n\n\nTiny Code (DP Bitmask – C++)\nUsed for TSP-style Hamiltonian search:\nint n;\nvector&lt;vector&lt;int&gt;&gt; dp(1 &lt;&lt; n, vector&lt;int&gt;(n, INF));\ndp[1][0] = 0;\nfor (int mask = 1; mask &lt; (1 &lt;&lt; n); ++mask) {\n    for (int u = 0; u &lt; n; ++u) if (mask & (1 &lt;&lt; u)) {\n        for (int v = 0; v &lt; n; ++v) if (!(mask & (1 &lt;&lt; v)) && adj[u][v]) {\n            dp[mask | (1 &lt;&lt; v)][v] = min(dp[mask | (1 &lt;&lt; v)][v], dp[mask][u] + cost[u][v]);\n        }\n    }\n}\n\n\nWhy It Matters\n\nModels ordering problems:\n\nTraveling Salesman (TSP)\nJob sequencing with constraints\nGenome assembly paths\n\nFundamental in theoretical computer science, cornerstone NP-complete problem.\nHelps distinguish easy (Eulerian) vs hard (Hamiltonian) traversal.\n\n\n\nA Gentle Proof (Why It’s Hard)\nHamiltonian Path existence is NP-complete:\n\nVerification is easy (given a path, check in \\(O(V)\\)).\nNo known polynomial algorithm (unless \\(P=NP\\)).\nMany problems reduce to it (like TSP).\n\nThis means it likely requires exponential time in general: \\[\nO(n!)\n\\] in naive form, or \\[\nO(2^n n)\n\\] using bitmask DP.\n\n\nTry It Yourself\n\nBuild small graphs (4–6 vertices) and trace paths.\nCompare with Eulerian path conditions.\nImplement backtracking search.\nExtend to cycle detection (check edge back to start).\nTry bitmask DP for small \\(n \\le 20\\).\n\n\n\nComplexity\n\nTime (backtracking): \\(O(n!)\\)\nTime (DP bitmask): \\(O(2^n \\cdot n^2)\\)\nSpace: \\(O(2^n \\cdot n)\\)\n\nHamiltonian paths capture the essence of combinatorial explosion, simple to state, hard to solve, yet central to understanding computational limits.\n\n\n\n395 Chinese Postman Problem (Route Inspection)\nThe Chinese Postman Problem (CPP), also known as the Route Inspection Problem, asks for the shortest closed path that traverses every edge of a graph at least once. It generalizes the Eulerian Circuit, allowing edge repetitions when necessary.\n\nWhat Problem Are We Solving?\nGiven a weighted graph \\(G = (V, E)\\), we want to find a minimum-cost tour that covers all edges at least once and returns to the start.\nIf \\(G\\) is Eulerian (all vertices have even degree), the answer is simple, the Eulerian Circuit itself. Otherwise, we must duplicate edges strategically to make the graph Eulerian, minimizing total added cost.\n\n\nHow It Works (Plain Language)\n\nCheck vertex degrees:\n\nCount how many have odd degree.\n\nIf all even → just find Eulerian Circuit.\nIf some odd →\n\nPair up odd vertices in such a way that the sum of shortest path distances between paired vertices is minimal.\nDuplicate edges along those shortest paths.\n\nThe resulting graph is Eulerian, so an Eulerian Circuit can be constructed.\nThe cost of this circuit is the sum of all edges + added edges.\n\n\n\nExample\nGraph edges (with weights):\n\n\n\nEdge\nWeight\n\n\n\n\n1–2\n3\n\n\n2–3\n2\n\n\n3–4\n4\n\n\n4–1\n3\n\n\n2–4\n1\n\n\n\nDegrees:\n\ndeg(1)=2, deg(2)=3, deg(3)=2, deg(4)=3 → odd vertices: 2, 4 Shortest path between 2–4: 1 Add that again → now all even\n\nTotal cost = (3 + 2 + 4 + 3 + 1) + 1 = 14\n\n\nAlgorithm (Steps)\n\nIdentify odd-degree vertices\nCompute shortest path matrix (Floyd–Warshall)\nSolve minimum-weight perfect matching among odd vertices\nDuplicate edges in the matching\nPerform Eulerian Circuit traversal (Hierholzer’s algorithm)\n\n\n\nTiny Code (Pseudocode)\ndef chinese_postman(G):\n    odd = [v for v in G if degree(v) % 2 == 1]\n    if not odd:\n        return eulerian_circuit(G)\n    \n    dist = floyd_warshall(G)\n    pairs = minimum_weight_matching(odd, dist)\n    for (u, v) in pairs:\n        add_path(G, u, v, dist)\n    return eulerian_circuit(G)\n\n\nWhy It Matters\n\nCore algorithm in network optimization\nUsed in:\n\nPostal route planning\nGarbage collection routing\nSnow plow scheduling\nStreet sweeping\n\nDemonstrates how graph augmentation can solve traversal problems efficiently\n\n\n\nA Gentle Proof (Why It Works)\nTo traverse every edge, all vertices must have even degree (Eulerian condition). When vertices with odd degree exist, we must pair them up to restore evenness.\nThe minimal duplication set is a minimum-weight perfect matching among odd vertices:\n\\[\n\\text{Extra cost} = \\min_{\\text{pairing } M} \\sum_{(u,v) \\in M} \\text{dist}(u,v)\n\\]\nThus, the optimal path cost:\n\\[\nC = \\sum_{e \\in E} w(e) + \\text{Extra cost}\n\\]\n\n\nTry It Yourself\n\nDraw a graph with odd-degree vertices.\nIdentify odd vertices and shortest pair distances.\nCompute minimal matching manually.\nAdd duplicated edges and find Eulerian Circuit.\nCompare total cost before and after duplication.\n\n\n\nComplexity\n\nFloyd–Warshall: \\(O(V^3)\\)\nMinimum Matching: \\(O(V^3)\\)\nEulerian traversal: \\(O(E)\\)\nTotal: \\(O(V^3)\\)\n\nThe Chinese Postman Problem transforms messy graphs into elegant tours, balancing degrees, minimizing effort, and ensuring every edge gets its due.\n\n\n\n396 Hierholzer’s Algorithm\nHierholzer’s Algorithm is the classical method for finding an Eulerian Path or Eulerian Circuit in a graph. It constructs the path by merging cycles until all edges are used exactly once.\n\nWhat Problem Are We Solving?\nWe want to find an Eulerian trail, a path or circuit that visits every edge exactly once.\n\nFor an Eulerian Circuit (closed trail): All vertices have even degree.\nFor an Eulerian Path (open trail): Exactly two vertices have odd degree.\n\nThe algorithm efficiently constructs the path in linear time relative to the number of edges.\n\n\nHow It Works (Plain Language)\n\nCheck Eulerian conditions:\n\n0 odd-degree vertices → Eulerian Circuit\n2 odd-degree vertices → Eulerian Path (start at one odd)\n\nStart from a valid vertex (odd if path, any if circuit)\nTraverse edges greedily:\n\nFollow edges until you return to the start or cannot continue.\n\nBacktrack and merge:\n\nWhen stuck, backtrack to a vertex with unused edges, start a new cycle, and merge it into the current path.\n\nContinue until all edges are used.\n\nThe final sequence of vertices is the Eulerian trail.\n\n\nExample (Undirected Graph)\nGraph:\n1, 2\n|   |\n4, 3\nAll vertices have even degree, so an Eulerian Circuit exists.\nStart at 1:\n1 → 2 → 3 → 4 → 1\nResult: Eulerian Circuit = [1, 2, 3, 4, 1]\n\n\nExample (With Odd Vertices)\nGraph:\n1, 2, 3\ndeg(1)=1, deg(2)=2, deg(3)=1 → Eulerian Path exists Start at 1:\n1 → 2 → 3\n\n\nTiny Code (C++)\nvector&lt;vector&lt;int&gt;&gt; adj;\nvector&lt;int&gt; path;\n\nvoid dfs(int u) {\n    while (!adj[u].empty()) {\n        int v = adj[u].back();\n        adj[u].pop_back();\n        dfs(v);\n    }\n    path.push_back(u);\n}\nAfter DFS, path will contain vertices in reverse order. Reverse it to get the Eulerian path or circuit.\n\n\nTiny Code (Python)\ndef hierholzer(graph, start):\n    stack, path = [start], []\n    while stack:\n        u = stack[-1]\n        if graph[u]:\n            v = graph[u].pop()\n            graph[v].remove(u)  # remove both directions for undirected\n            stack.append(v)\n        else:\n            path.append(stack.pop())\n    return path[::-1]\n\n\nWhy It Matters\n\nEfficiently constructs Eulerian Paths in \\(O(V + E)\\)\nBackbone for:\n\nChinese Postman Problem\nEulerian Circuit detection\nDNA sequencing (De Bruijn graphs)\nRoute design and network analysis\n\n\n\n\nA Gentle Proof (Why It Works)\n\nEvery time you traverse an edge, it’s removed (used once).\nEach vertex retains balanced degree (entries = exits).\nWhen you get stuck, the subpath formed is a cycle.\nMerging all such cycles yields a single complete traversal.\n\nThus, every edge appears exactly once in the final route.\n\n\nTry It Yourself\n\nDraw small Eulerian graphs.\nManually trace Hierholzer’s algorithm.\nIdentify start vertex (odd-degree or any).\nVerify path covers all edges exactly once.\nApply to both directed and undirected graphs.\n\n\n\nComplexity\n\nTime: \\(O(V + E)\\)\nSpace: \\(O(V + E)\\)\n\nHierholzer’s Algorithm elegantly builds order from connectivity, ensuring each edge finds its place in a perfect traversal.\n\n\n\n397 Johnson’s Cycle Finding Algorithm\nJohnson’s Algorithm is a powerful method for enumerating all simple cycles (elementary circuits) in a directed graph. A simple cycle is one that visits no vertex more than once, except the starting/ending vertex.\nUnlike DFS-based approaches that can miss or duplicate cycles, Johnson’s method systematically lists each cycle exactly once, running in O((V + E)(C + 1)), where C is the number of cycles.\n\nWhat Problem Are We Solving?\nWe want to find all simple cycles in a directed graph \\(G = (V, E)\\).\nThat is, find all vertex sequences \\[v_1 \\to v_2 \\to \\ldots \\to v_k \\to v_1\\] where each \\(v_i\\) is distinct and edges exist between consecutive vertices.\nEnumerating all cycles is fundamental for:\n\nDependency analysis\nFeedback detection\nCircuit design\nGraph motif analysis\n\n\n\nHow It Works (Plain Language)\nJohnson’s algorithm is based on backtracking with smart pruning and SCC decomposition:\n\nProcess vertices in order\n\nConsider vertices \\(1, 2, \\dots, n\\).\n\nFor each vertex \\(s\\):\n\nConsider the subgraph induced by vertices ≥ s.\nFind strongly connected components (SCCs) in this subgraph.\nIf \\(s\\) belongs to a nontrivial SCC, explore all simple cycles starting at \\(s\\).\n\nUse a blocked set to avoid redundant exploration:\n\nOnce a vertex leads to a dead-end, mark it blocked.\nUnblock when a valid cycle is found through it.\n\n\nThis avoids exploring the same path multiple times.\n\n\nExample\nGraph:\nA → B → C\n↑   ↓   |\n└── D ←─┘\nCycles:\n\nA → B → C → D → A\nB → C → D → B\n\nJohnson’s algorithm will find both efficiently, without duplication.\n\n\nPseudocode\ndef johnson(G):\n    result = []\n    blocked = set()\n    B = {v: set() for v in G}\n    stack = []\n\n    def circuit(v, s):\n        f = False\n        stack.append(v)\n        blocked.add(v)\n        for w in G[v]:\n            if w == s:\n                result.append(stack.copy())\n                f = True\n            elif w not in blocked:\n                if circuit(w, s):\n                    f = True\n        if f:\n            unblock(v)\n        else:\n            for w in G[v]:\n                B[w].add(v)\n        stack.pop()\n        return f\n\n    def unblock(u):\n        blocked.discard(u)\n        for w in B[u]:\n            if w in blocked:\n                unblock(w)\n        B[u].clear()\n\n    for s in sorted(G.keys()):\n        # consider subgraph of nodes &gt;= s\n        subG = {v: [w for w in G[v] if w &gt;= s] for v in G if v &gt;= s}\n        SCCs = strongly_connected_components(subG)\n        if not SCCs:\n            continue\n        scc = min(SCCs, key=lambda S: min(S))\n        s_node = min(scc)\n        circuit(s_node, s_node)\n    return result\n\n\nWhy It Matters\n\nEnumerates all simple cycles without duplicates\nWorks for directed graphs (unlike many undirected-only algorithms)\nKey in:\n\nDeadlock detection\nCycle basis computation\nFeedback arc set analysis\nSubgraph pattern mining\n\n\n\n\nA Gentle Proof (Why It Works)\nEach recursive search begins from the smallest vertex in an SCC. By restricting search to vertices \\(\\ge s\\), every cycle is discovered exactly once (at its least-numbered vertex). The blocked set prevents repeated exploration of dead ends. Unblocking ensures vertices re-enter search space when part of a valid cycle.\nThis guarantees:\n\nNo cycle is missed\nNo cycle is duplicated\n\n\n\nTry It Yourself\n\nDraw a small directed graph with 3–5 vertices.\nIdentify SCCs manually.\nApply the algorithm step-by-step, noting blocked updates.\nRecord each cycle when returning to start.\nCompare with naive DFS enumeration.\n\n\n\nComplexity\n\nTime: \\(O((V + E)(C + 1))\\)\nSpace: \\(O(V + E)\\)\n\nJohnson’s algorithm reveals the hidden loops inside directed graphs, one by one, exhaustively and elegantly.\n\n\n\n398 Transitive Closure (Floyd–Warshall)\nThe Transitive Closure of a directed graph captures reachability: it tells us, for every pair of vertices \\((u, v)\\), whether there exists a path from \\(u\\) to \\(v\\).\nIt’s often represented as a boolean matrix \\(R\\), where \\[\nR[u][v] = 1 \\text{ if and only if there is a path from } u \\text{ to } v\n\\]\nThis can be computed efficiently using a modified version of the Floyd–Warshall algorithm.\n\nWhat Problem Are We Solving?\nGiven a directed graph \\(G = (V, E)\\), we want to determine for every pair \\((u, v)\\) whether:\n\\[\nu \\leadsto v\n\\]\nThat is, can we reach \\(v\\) from \\(u\\) through a sequence of directed edges?\nThe output is a reachability matrix, useful in:\n\nDependency analysis\nAccess control and authorization graphs\nProgram call graphs\nDatabase query optimization\n\n\n\nHow It Works (Plain Language)\nWe apply the Floyd–Warshall dynamic programming idea, but instead of distances, we propagate reachability.\nLet \\(R[u][v] = 1\\) if \\(u \\to v\\) (direct edge), otherwise \\(0\\). Then for each vertex \\(k\\) (intermediate node), we update:\n\\[\nR[u][v] = R[u][v] \\lor (R[u][k] \\land R[k][v])\n\\]\nIntuitively: “\\(u\\) can reach \\(v\\) if \\(u\\) can reach \\(k\\) and \\(k\\) can reach \\(v\\).”\n\n\nExample\nGraph:\nA → B → C\n↑         |\n└─────────┘\nInitial reachability (direct edges):\nA B C\nA 0 1 0\nB 0 0 1\nC 1 0 0\nAfter applying transitive closure:\nA B C\nA 1 1 1\nB 1 1 1\nC 1 1 1\nEvery vertex is reachable from every other, the graph is strongly connected.\n\n\nTiny Code (C)\n#define N 100\nint R[N][N];\n\nvoid floyd_warshall_tc(int n) {\n    for (int k = 0; k &lt; n; ++k)\n        for (int i = 0; i &lt; n; ++i)\n            for (int j = 0; j &lt; n; ++j)\n                R[i][j] = R[i][j] || (R[i][k] && R[k][j]);\n}\n\n\nTiny Code (Python)\ndef transitive_closure(R):\n    n = len(R)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                R[i][j] = R[i][j] or (R[i][k] and R[k][j])\n    return R\n\n\nWhy It Matters\n\nConverts a graph into a reachability matrix\nEnables constant-time queries: “Can \\(u\\) reach \\(v\\)?”\nUsed in:\n\nCompilers (call dependencies)\nDatabases (recursive queries)\nSecurity graphs\nNetwork analysis\n\n\n\n\nA Gentle Proof (Why It Works)\nWe extend reachability step by step:\n\nBase: \\(R^{(0)}\\) = direct edges\nStep: \\(R^{(k)}\\) = paths using vertices \\({1, 2, \\dots, k}\\) as intermediates\n\nBy induction: \\[\nR^{(k)}[i][j] = 1 \\iff \\text{there exists a path } i \\to j \\text{ using vertices } \\le k\n\\]\nAt the end (\\(k = n\\)), \\(R^{(n)}\\) contains all possible paths.\n\n\nTry It Yourself\n\nCreate a directed graph with 4–5 nodes.\nBuild its adjacency matrix.\nApply the algorithm by hand.\nObserve how new reachabilities emerge after each \\(k\\).\nCompare with paths you can see visually.\n\n\n\nComplexity\n\nTime: \\(O(V^3)\\)\nSpace: \\(O(V^2)\\)\n\nTransitive closure transforms reachability into certainty, mapping every potential path into a single clear view of what connects to what.\n\n\n\n399 Graph Coloring (Backtracking)\nGraph Coloring is the problem of assigning colors to vertices of a graph so that no two adjacent vertices share the same color. The smallest number of colors required is called the chromatic number of the graph.\nThis classic constraint satisfaction problem lies at the heart of scheduling, register allocation, and pattern assignment.\n\nWhat Problem Are We Solving?\nGiven a graph \\(G = (V, E)\\) and an integer \\(k\\), determine whether it is possible to color all vertices using at most \\(k\\) colors such that:\n\\[\n\\forall (u, v) \\in E, ; \\text{color}(u) \\ne \\text{color}(v)\n\\]\nIf such a coloring exists, \\(G\\) is \\(k\\)-colorable.\n\n\nHow It Works (Plain Language)\nWe solve this using backtracking:\n\nAssign a color to the first vertex.\nMove to the next vertex.\nTry all available colors (from \\(1\\) to \\(k\\)).\nIf a color assignment violates adjacency constraints, skip it.\nIf a vertex cannot be colored, backtrack to previous vertex and change its color.\nContinue until:\n\nall vertices are colored (success), or\nno valid assignment exists (failure).\n\n\n\n\nExample\nGraph:\n1, 2\n|   |\n3, 4\nA square requires at least 2 colors:\n\ncolor(1) = 1\ncolor(2) = 2\ncolor(3) = 2\ncolor(4) = 1\n\nValid 2-coloring.\nTry 1 color → fails (adjacent nodes same color) Try 2 colors → success → chromatic number = 2\n\n\nTiny Code (C++)\nint n, k;\nvector&lt;vector&lt;int&gt;&gt; adj;\nvector&lt;int&gt; color;\n\nbool isSafe(int v, int c) {\n    for (int u : adj[v])\n        if (color[u] == c) return false;\n    return true;\n}\n\nbool solve(int v) {\n    if (v == n) return true;\n    for (int c = 1; c &lt;= k; ++c) {\n        if (isSafe(v, c)) {\n            color[v] = c;\n            if (solve(v + 1)) return true;\n            color[v] = 0;\n        }\n    }\n    return false;\n}\n\n\nTiny Code (Python)\ndef graph_coloring(graph, k):\n    n = len(graph)\n    color = [0] * n\n\n    def safe(v, c):\n        return all(color[u] != c for u in range(n) if graph[v][u])\n\n    def backtrack(v):\n        if v == n:\n            return True\n        for c in range(1, k + 1):\n            if safe(v, c):\n                color[v] = c\n                if backtrack(v + 1):\n                    return True\n                color[v] = 0\n        return False\n\n    return backtrack(0)\n\n\nWhy It Matters\nGraph coloring captures the essence of constraint-based allocation:\n\nScheduling: assign time slots to tasks\nRegister allocation: map variables to CPU registers\nMap coloring: color regions with shared boundaries\nFrequency assignment: allocate channels in wireless networks\n\n\n\nA Gentle Proof (Why It Works)\nWe explore all possible assignments (depth-first search) under the rule: \\[\n\\forall (u, v) \\in E, ; \\text{color}(u) \\ne \\text{color}(v)\n\\]\nBacktracking prunes partial solutions that cannot lead to valid full assignments. When a full coloring is found, constraints are satisfied by construction.\nBy completeness of backtracking, if a valid \\(k\\)-coloring exists, it will be found.\n\n\nTry It Yourself\n\nDraw small graphs (triangle, square, pentagon).\nAttempt coloring with \\(k = 2, 3, 4\\).\nObserve where conflicts force backtracking.\nTry greedy coloring and compare with backtracking.\nIdentify the chromatic number experimentally.\n\n\n\nComplexity\n\nTime: \\(O(k^V)\\) (exponential worst case)\nSpace: \\(O(V)\\)\n\nGraph coloring blends search and logic, a careful dance through the constraints, discovering harmony one color at a time.\n\n\n\n400 Articulation Points & Bridges\nArticulation Points and Bridges identify weak spots in a graph, nodes or edges whose removal increases the number of connected components. They are essential in analyzing network resilience, communication reliability, and biconnected components.\n\nWhat Problem Are We Solving?\nGiven an undirected graph \\(G = (V, E)\\), find:\n\nArticulation Points (Cut Vertices): Vertices whose removal disconnects the graph.\nBridges (Cut Edges): Edges whose removal disconnects the graph.\n\nWe want efficient algorithms to detect these in \\(O(V + E)\\) time.\n\n\nHow It Works (Plain Language)\nWe use a single DFS traversal (Tarjan’s algorithm) with two key arrays:\n\ndisc[v]: discovery time of vertex \\(v\\)\nlow[v]: the lowest discovery time reachable from \\(v\\) (including back edges)\n\nDuring DFS:\n\nA vertex \\(u\\) is an articulation point if:\n\n\\(u\\) is root and has more than one child, or\n\\(\\exists\\) child \\(v\\) such that low[v] ≥ disc[u]\n\nAn edge \\((u, v)\\) is a bridge if:\n\nlow[v] &gt; disc[u]\n\n\nThese conditions detect when no back-edge connects a subtree back to an ancestor.\n\n\nExample\nGraph:\n  1\n / \\\n2   3\n|   |\n4   5\nRemove node 2 → node 4 becomes isolated → 2 is an articulation point. Remove edge (2, 4) → increases components → (2, 4) is a bridge.\n\n\nTiny Code (C++)\nvector&lt;vector&lt;int&gt;&gt; adj;\nvector&lt;int&gt; disc, low, parent;\nvector&lt;bool&gt; ap;\nint timer = 0;\n\nvoid dfs(int u) {\n    disc[u] = low[u] = ++timer;\n    int children = 0;\n\n    for (int v : adj[u]) {\n        if (!disc[v]) {\n            parent[v] = u;\n            ++children;\n            dfs(v);\n            low[u] = min(low[u], low[v]);\n\n            if (parent[u] == -1 && children &gt; 1)\n                ap[u] = true;\n            if (parent[u] != -1 && low[v] &gt;= disc[u])\n                ap[u] = true;\n            if (low[v] &gt; disc[u])\n                cout &lt;&lt; \"Bridge: \" &lt;&lt; u &lt;&lt; \" - \" &lt;&lt; v &lt;&lt; \"\\n\";\n        } else if (v != parent[u]) {\n            low[u] = min(low[u], disc[v]);\n        }\n    }\n}\n\n\nTiny Code (Python)\ndef articulation_points_and_bridges(graph):\n    n = len(graph)\n    disc = [0] * n\n    low = [0] * n\n    parent = [-1] * n\n    ap = [False] * n\n    bridges = []\n    time = 1\n\n    def dfs(u):\n        nonlocal time\n        disc[u] = low[u] = time\n        time += 1\n        children = 0\n        for v in graph[u]:\n            if not disc[v]:\n                parent[v] = u\n                children += 1\n                dfs(v)\n                low[u] = min(low[u], low[v])\n\n                if parent[u] == -1 and children &gt; 1:\n                    ap[u] = True\n                if parent[u] != -1 and low[v] &gt;= disc[u]:\n                    ap[u] = True\n                if low[v] &gt; disc[u]:\n                    bridges.append((u, v))\n            elif v != parent[u]:\n                low[u] = min(low[u], disc[v])\n\n    for i in range(n):\n        if not disc[i]:\n            dfs(i)\n\n    return [i for i, x in enumerate(ap) if x], bridges\n\n\nWhy It Matters\nArticulation points and bridges reveal critical nodes and links in:\n\nNetwork design: identify weak links in infrastructure\nSocial networks: find influencers whose removal splits communities\nCompiler dependency graphs: locate critical connections\nTransport systems: ensure robust routing\n\nUnderstanding where a graph breaks helps us design systems that don’t.\n\n\nA Gentle Proof (Why It Works)\nFor each node \\(u\\), low[u] captures the earliest discovered vertex reachable via DFS or a back edge. If a child subtree cannot reach an ancestor of \\(u\\), then \\(u\\) is a bottleneck, removing it splits the graph.\nThe inequalities: \\[\nlow[v] \\ge disc[u] \\implies u \\text{ is articulation point}\n\\] \\[\nlow[v] &gt; disc[u] \\implies (u,v) \\text{ is bridge}\n\\]\nare derived from whether a subtree is connected back to an ancestor.\n\n\nTry It Yourself\n\nDraw small graphs (triangle, line, star).\nManually run DFS and record disc and low.\nIdentify articulation points and bridges.\nTry adding edges, see how redundancy removes articulation points.\n\n\n\nComplexity\n\nTime: \\(O(V + E)\\)\nSpace: \\(O(V)\\)\n\nFinding articulation points and bridges transforms structure into insight, helping you build networks that stay connected even when parts fail.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 4. Graph Algorithms</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-5.html",
    "href": "books/en-us/list-5.html",
    "title": "Chapter 5. Dynamic Programming",
    "section": "",
    "text": "Section 41. DP basic and state transitions\n\n401 Fibonacci DP\nFibonacci is the hello-world of dynamic programming, a simple sequence that teaches the power of remembering past results. Instead of recomputing subproblems over and over, we store them. The result? A huge leap from exponential to linear time.\n\nWhat Problem Are We Solving?\nThe Fibonacci sequence is defined as:\n\\[\nF(0) = 0, \\quad F(1) = 1, \\quad F(n) = F(n-1) + F(n-2)\n\\]\nA naive recursive version recomputes the same values many times. For example, F(5) calls F(4) and F(3), but F(4) also calls F(3) again, wasteful repetition.\nOur goal is to avoid recomputation by caching results. That’s dynamic programming in a nutshell.\n\n\nHow Does It Work (Plain Language)?\nThink of Fibonacci as a ladder. You can climb to step n only if you know the number of ways to reach n-1 and n-2. Instead of recalculating those steps every time, record them once, then reuse.\nThere are two main flavors of DP:\n\n\n\n\n\n\n\n\nApproach\nDescription\nExample\n\n\n\n\nTop-Down (Memoization)\nRecursion + caching\nStore results in an array\n\n\nBottom-Up (Tabulation)\nIteration from base cases\nBuild array from 0 up\n\n\n\nLet’s visualize the state filling:\n\n\n\nn\nF(n-2)\nF(n-1)\nF(n) = F(n-1) + F(n-2)\n\n\n\n\n0\n-\n-\n0\n\n\n1\n-\n-\n1\n\n\n2\n0\n1\n1\n\n\n3\n1\n1\n2\n\n\n4\n1\n2\n3\n\n\n5\n2\n3\n5\n\n\n6\n3\n5\n8\n\n\n\nEach new value reuses two old ones, no redundant work.\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up Fibonacci)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n\n    if (n == 0) { printf(\"0\\n\"); return 0; }\n    if (n == 1) { printf(\"1\\n\"); return 0; }\n\n    long long dp[n + 1];\n    dp[0] = 0;\n    dp[1] = 1;\n\n    for (int i = 2; i &lt;= n; i++) {\n        dp[i] = dp[i - 1] + dp[i - 2];\n    }\n\n    printf(\"Fibonacci(%d) = %lld\\n\", n, dp[n]);\n    return 0;\n}\nPython (Memoized Fibonacci)\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef fib(n):\n    if n &lt;= 1:\n        return n\n    return fib(n-1) + fib(n-2)\n\nn = int(input(\"Enter n: \"))\nprint(\"Fibonacci(\", n, \") =\", fib(n))\n\n\nWhy It Matters\n\nDemonstrates state definition: F(n) = F(n-1) + F(n-2)\nIntroduces overlapping subproblems and optimal substructure\nFirst step toward mastering DP intuition\nReduces time complexity from exponential O(2ⁿ) to linear O(n)\n\nYou learn that solving once and remembering is better than solving a hundred times.\n\n\nStep-by-Step Example\n\n\n\nStep\nCalculation\nMemo Table (Partial)\n\n\n\n\nBase\nF(0)=0, F(1)=1\n[0, 1, , , , ]\n\n\nF(2)\nF(1)+F(0)=1\n[0, 1, 1, , , _]\n\n\nF(3)\nF(2)+F(1)=2\n[0, 1, 1, 2, , ]\n\n\nF(4)\nF(3)+F(2)=3\n[0, 1, 1, 2, 3, _]\n\n\nF(5)\nF(4)+F(3)=5\n[0, 1, 1, 2, 3, 5]\n\n\n\n\n\nTry It Yourself\n\nWrite Fibonacci recursively without memoization. Measure calls.\nAdd a dictionary or array for memoization, compare speeds.\nConvert recursive to iterative (tabulation).\nOptimize space: store only two variables instead of full array.\nPrint the full table of computed values.\n\n\n\nTest Cases\n\n\n\nn\nExpected Output\nNotes\n\n\n\n\n0\n0\nBase case\n\n\n1\n1\nBase case\n\n\n2\n1\n1 + 0\n\n\n5\n5\nSequence: 0,1,1,2,3,5\n\n\n10\n55\nSmooth growth check\n\n\n\n\n\nComplexity\n\nTime: O(n) for both memoization and tabulation\nSpace: O(n) for table; O(1) if optimized\n\nFibonacci DP is the simplest proof that remembering pays off, it’s where dynamic programming begins, and efficiency is born.\n\n\n\n402 Climbing Stairs\nThe climbing stairs problem is a friendly cousin of Fibonacci, same recurrence, different story. You’re standing at the bottom of a staircase with n steps. You can climb 1 step or 2 steps at a time. How many distinct ways can you reach the top?\nThis is one of the most intuitive gateways into dynamic programming: define states, relate them recursively, and reuse past computations.\n\nWhat Problem Are We Solving?\nWe want the number of distinct ways to reach step n.\nYou can reach step n by:\n\ntaking 1 step from n-1, or\ntaking 2 steps from n-2.\n\nSo the recurrence is: \\[\ndp[n] = dp[n-1] + dp[n-2]\n\\]\nwith base cases: \\[\ndp[0] = 1, \\quad dp[1] = 1\n\\]\nThis is structurally identical to Fibonacci, but with a combinatorial interpretation.\n\n\nHow Does It Work (Plain Language)?\nThink of each step as a checkpoint. To reach step n, you must come from either of the two prior checkpoints. If you already know how many ways there are to reach those, just add them.\nLet’s illustrate with a table:\n\n\n\n\n\n\n\n\nStep (n)\nWays to Reach\nExplanation\n\n\n\n\n0\n1\nStay at ground\n\n\n1\n1\nSingle step\n\n\n2\n2\n(1+1), (2)\n\n\n3\n3\n(1+1+1), (1+2), (2+1)\n\n\n4\n5\n(1+1+1+1), (2+1+1), (1+2+1), (1+1+2), (2+2)\n\n\n5\n8\nPrevious two sums: 5 = 3+2\n\n\n\nEach new value is the sum of the previous two.\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up Climbing Stairs)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n;\n    printf(\"Enter number of steps: \");\n    scanf(\"%d\", &n);\n\n    if (n == 0 || n == 1) {\n        printf(\"Ways: 1\\n\");\n        return 0;\n    }\n\n    long long dp[n + 1];\n    dp[0] = 1;\n    dp[1] = 1;\n\n    for (int i = 2; i &lt;= n; i++) {\n        dp[i] = dp[i - 1] + dp[i - 2];\n    }\n\n    printf(\"Ways to climb %d steps: %lld\\n\", n, dp[n]);\n    return 0;\n}\nPython (Space Optimized)\nn = int(input(\"Enter number of steps: \"))\n\nif n == 0 or n == 1:\n    print(1)\nelse:\n    a, b = 1, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    print(\"Ways to climb\", n, \"steps:\", b)\n\n\nWhy It Matters\n\nDemonstrates state transitions: dp[i] depends on dp[i-1] and dp[i-2]\nTeaches bottom-up thinking and base case setup\nShows how recurrence translates to counting problems\nConnects combinatorics with DP intuition\n\nClimbing stairs is a great mental bridge between pure math (recurrence) and applied reasoning (counting paths).\n\n\nStep-by-Step Example\nLet’s trace n = 5:\n\n\n\ni\ndp[i-2]\ndp[i-1]\ndp[i] = dp[i-1] + dp[i-2]\n\n\n\n\n0\n-\n-\n1\n\n\n1\n-\n-\n1\n\n\n2\n1\n1\n2\n\n\n3\n1\n2\n3\n\n\n4\n2\n3\n5\n\n\n5\n3\n5\n8\n\n\n\nSo, 8 ways to climb 5 steps.\n\n\nTry It Yourself\n\nModify the code to allow 1, 2, or 3 steps at a time.\nPrint the entire dp table for small n.\nCompare recursive vs iterative solutions.\nTry to derive a formula, notice the Fibonacci pattern.\nWhat if each step had a cost? Adapt it to Min Cost Climb.\n\n\n\nTest Cases\n\n\n\nn\nExpected Output\nWays\n\n\n\n\n0\n1\nDo nothing\n\n\n1\n1\n[1]\n\n\n2\n2\n[1+1], [2]\n\n\n3\n3\n[1+1+1], [1+2], [2+1]\n\n\n4\n5\nAll combinations\n\n\n5\n8\nGrows like Fibonacci\n\n\n\n\n\nComplexity\n\nTime: O(n)\nSpace: O(n), reducible to O(1) with two variables\n\nClimbing stairs shows that dynamic programming isn’t just math, it’s about recognizing patterns in movement, growth, and memory.\n\n\n\n403 Grid Paths\nThe grid path problem is a gentle step into 2D dynamic programming, where states depend on neighbors, not just previous elements. Imagine standing in the top-left corner of a grid, moving only right or down, trying to count how many ways lead to the bottom-right corner.\nEach cell’s value is determined by paths reaching it from above or from the left, a perfect metaphor for how DP builds solutions layer by layer.\n\nWhat Problem Are We Solving?\nGiven an m × n grid, find the number of distinct paths from (0, 0) to (m-1, n-1) when you can move only:\n\nRight (x, y+1)\nDown (x+1, y)\n\nThe recurrence: \\[\ndp[i][j] = dp[i-1][j] + dp[i][j-1]\n\\] with base cases: \\[\ndp[0][j] = 1, \\quad dp[i][0] = 1\n\\] (since only one way exists along the first row or column)\n\n\nHow Does It Work (Plain Language)?\nThink of the grid like a city map, every intersection (i, j) can be reached from either the north (i-1, j) or the west (i, j-1). So total routes = routes from north + routes from west.\nLet’s visualize a 3×3 grid (0-indexed):\n\n\n\nCell (i,j)\nWays\nExplanation\n\n\n\n\n(0,0)\n1\nStart\n\n\n(0,1)\n1\nOnly from left\n\n\n(0,2)\n1\nOnly from left\n\n\n(1,0)\n1\nOnly from top\n\n\n(1,1)\n2\n(0,1)+(1,0)=2\n\n\n(1,2)\n3\n(0,2)+(1,1)=3\n\n\n(2,0)\n1\nOnly from top\n\n\n(2,1)\n3\n(1,1)+(2,0)=3\n\n\n(2,2)\n6\n(1,2)+(2,1)=6\n\n\n\nSo dp[2][2] = 6 → 6 distinct paths.\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int m, n;\n    printf(\"Enter rows and cols: \");\n    scanf(\"%d %d\", &m, &n);\n\n    long long dp[m][n];\n\n    for (int i = 0; i &lt; m; i++) dp[i][0] = 1;\n    for (int j = 0; j &lt; n; j++) dp[0][j] = 1;\n\n    for (int i = 1; i &lt; m; i++) {\n        for (int j = 1; j &lt; n; j++) {\n            dp[i][j] = dp[i-1][j] + dp[i][j-1];\n        }\n    }\n\n    printf(\"Unique paths: %lld\\n\", dp[m-1][n-1]);\n    return 0;\n}\nPython (Space Optimized)\nm, n = map(int, input(\"Enter rows and cols: \").split())\ndp = [1] * n\n\nfor _ in range(1, m):\n    for j in range(1, n):\n        dp[j] += dp[j - 1]\n\nprint(\"Unique paths:\", dp[-1])\n\n\nWhy It Matters\n\nTeaches 2D DP grids\nBuilds intuition for problems on lattices, matrices, grids\nFoundation for min-cost path, maze traversal, robot movement\nEncourages space optimization from 2D → 1D\n\nFrom counting paths to optimizing them, this grid is your DP canvas.\n\n\nStep-by-Step Example\nFor a 3×3 grid:\n\n\n\ni\n0\n1\n2\n\n\n\n\n0\n1\n1\n1\n\n\n1\n1\n2\n3\n\n\n2\n1\n3\n6\n\n\n\ndp[2][2] = 6 → six unique routes.\n\n\nTry It Yourself\n\nModify the code to handle obstacles (0 = block, 1 = open).\nPrint the DP table.\nImplement using recursion + memoization.\nAdd a condition for moving right, down, and diagonal.\nCompare with combinatorial formula: \\(\\binom{m+n-2}{m-1}\\).\n\n\n\nTest Cases\n\n\n\nGrid Size\nExpected Paths\nNotes\n\n\n\n\n1×1\n1\nOnly starting cell\n\n\n2×2\n2\nRight→Down, Down→Right\n\n\n3×3\n6\nClassic case\n\n\n3×4\n10\nCombinatorics check\n\n\n4×4\n20\nPascal triangle pattern\n\n\n\n\n\nComplexity\n\nTime: O(m×n)\nSpace: O(m×n), reducible to O(n)\n\nGrid Paths reveal the essence of DP, every position depends on simpler ones. From here, you’ll learn to minimize, maximize, and traverse with purpose.\n\n\n\n404 Min Cost Path\nThe Min Cost Path problem is where counting meets optimization. Instead of asking “How many ways can I reach the end?”, we ask “What’s the cheapest way to get there?”. You’re moving across a grid, cell by cell, each with a cost, and your goal is to reach the bottom-right corner while minimizing the total cost.\nThis is one of the most fundamental path optimization problems in dynamic programming.\n\nWhat Problem Are We Solving?\nGiven a matrix cost[m][n], where each cell represents a non-negative cost, find the minimum total cost path from (0, 0) to (m-1, n-1), moving only right, down, or (optionally) diagonally down-right.\nThe recurrence: \\[\ndp[i][j] = cost[i][j] + \\min(dp[i-1][j], dp[i][j-1])\n\\] If diagonal moves are allowed: \\[\ndp[i][j] = cost[i][j] + \\min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n\\]\nBase case: \\[\ndp[0][0] = cost[0][0]\n\\]\n\n\nHow Does It Work (Plain Language)?\nImagine hiking across a grid of terrain, where each cell has an energy cost. Every move you make adds to your total cost. You always want to choose the path that keeps your running total as small as possible.\nThe DP table records the minimum cost to reach each cell, building from the top-left to the bottom-right.\nLet’s see an example grid:\n\n\n\nCell\nCost\n\n\n\n\n(0,0)\n1\n\n\n(0,1)\n3\n\n\n(0,2)\n5\n\n\n(1,0)\n2\n\n\n(1,1)\n1\n\n\n(1,2)\n2\n\n\n(2,0)\n4\n\n\n(2,1)\n3\n\n\n(2,2)\n1\n\n\n\nWe fill dp[i][j] = cost to reach (i,j):\n\n\n\ni\n0\n1\n2\n\n\n\n\n0\n1\n4\n9\n\n\n1\n3\n2\n4\n\n\n2\n7\n5\n5\n\n\n\nMinimum cost = 5\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\nint main(void) {\n    int m, n;\n    printf(\"Enter rows and cols: \");\n    scanf(\"%d %d\", &m, &n);\n\n    int cost[m][n];\n    printf(\"Enter cost matrix:\\n\");\n    for (int i = 0; i &lt; m; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &cost[i][j]);\n\n    int dp[m][n];\n    dp[0][0] = cost[0][0];\n\n    for (int i = 1; i &lt; m; i++) dp[i][0] = dp[i-1][0] + cost[i][0];\n    for (int j = 1; j &lt; n; j++) dp[0][j] = dp[0][j-1] + cost[0][j];\n\n    for (int i = 1; i &lt; m; i++)\n        for (int j = 1; j &lt; n; j++)\n            dp[i][j] = cost[i][j] + MIN(dp[i-1][j], dp[i][j-1]);\n\n    printf(\"Min cost: %d\\n\", dp[m-1][n-1]);\n    return 0;\n}\nPython (Optional Diagonal Move)\ndef min_cost_path(cost):\n    m, n = len(cost), len(cost[0])\n    dp = [[0]*n for _ in range(m)]\n    dp[0][0] = cost[0][0]\n\n    for i in range(1, m):\n        dp[i][0] = dp[i-1][0] + cost[i][0]\n    for j in range(1, n):\n        dp[0][j] = dp[0][j-1] + cost[0][j]\n\n    for i in range(1, m):\n        for j in range(1, n):\n            dp[i][j] = cost[i][j] + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n\n    return dp[-1][-1]\n\ncost = [\n    [1, 3, 5],\n    [2, 1, 2],\n    [4, 3, 1]\n$$\n\nprint(\"Min cost path:\", min_cost_path(cost))\n\n\nWhy It Matters\n\nTransitions from counting paths to optimizing paths\nIntroduces minimization recurrence\nBasis for many grid problems (e.g. maze solving, image traversal, shortest paths)\nBuilds intuition for weighted DP\n\nThis problem is a stepping stone toward Dijkstra’s and Bellman-Ford in graphs.\n\n\nStep-by-Step Example\n\n\n\nCell\nFrom Top\nFrom Left\nMin\nTotal Cost\n\n\n\n\n(0,0)\n-\n-\n-\n1\n\n\n(0,1)\n-\n1+3\n4\n4\n\n\n(1,0)\n1+2\n-\n3\n3\n\n\n(1,1)\n4+1\n3+1\n4\n4\n\n\n(2,2)\n…\n…\n…\n5\n\n\n\nAnswer: 5\n\n\nTry It Yourself\n\nAdd diagonal moves, compare results.\nAdd blocked cells (infinite cost).\nModify to find maximum cost path.\nReconstruct the path using a parent table.\nUse a priority queue (Dijkstra) for non-grid graphs.\n\n\n\nTest Cases\n\n\n\nGrid\nExpected Min Cost\nNotes\n\n\n\n\n[[1]]\n1\nSingle cell\n\n\n[[1,2],[3,4]]\n7\n1→2→4\n\n\n[[1,3,5],[2,1,2],[4,3,1]]\n5\nOptimal route\n\n\n[[5,9],[4,2]]\n11\n5→4→2\n\n\n\n\n\nComplexity\n\nTime: O(m×n)\nSpace: O(m×n), reducible to O(n)\n\nMin Cost Path turns the grid into a map of decisions, each cell asks, “What’s the cheapest way to reach me?” and the DP table answers with calm precision.\n\n\n\n405 Coin Change (Count Ways)\nYou have coin denominations and an amount. How many distinct ways can you make that amount if you can use unlimited coins of each type? We count combinations where order does not matter. For example, with coins [1, 2, 5] there are 4 ways to make 5: 5, 2+2+1, 2+1+1+1, 1+1+1+1+1.\n\nWhat Problem Are We Solving?\nGiven an array coins[] and an integer amount, compute the number of combinations to form amount using unlimited copies of each coin.\nState and recurrence:\n\\[\ndp[x] = \\text{number of ways to make sum } x\n\\] \\[\ndp[0] = 1\n\\] \\[\n\\text{for each coin } c:\\quad \\text{for } x \\text{ from } c \\text{ to } \\text{amount}:\\quad dp[x] \\mathrel{+=} dp[x - c]\n\\]\nWhy this order: iterating coins on the outside ensures each combination is counted once. If you loop amounts on the outside and coins inside, you would count permutations.\n\n\nHow Does It Work (Plain Language)?\nThink of building the total from left to right. For each coin value c, you ask: if I must use c at least once, how many ways remain to fill x - c? Add those to the ways already known. Move forward increasing x, and repeat for the next coin. The table fills like a rolling tally.\nExample with coins [1, 2, 5] and amount = 5:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter processing\ndp[0]\ndp[1]\ndp[2]\ndp[3]\ndp[4]\ndp[5]\nExplanation\n\n\n\n\nInit\n1\n0\n0\n0\n0\n0\nOne way to make 0: choose nothing\n\n\nCoin 1\n1\n1\n1\n1\n1\n1\nUsing 1s only\n\n\nCoin 2\n1\n1\n2\n2\n3\n3\nAdd ways that end with a 2\n\n\nCoin 5\n1\n1\n2\n2\n3\n4\nAdd ways that end with a 5\n\n\n\nAnswer is dp[5] = 4.\n\n\nTiny Code (Easy Versions)\nC (1D DP, combinations)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n, amount;\n    printf(\"Enter number of coin types and amount: \");\n    scanf(\"%d %d\", &n, &amount);\n\n    int coins[n];\n    printf(\"Enter coin values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &coins[i]);\n\n    // Use long long to avoid overflow for large counts\n    long long dp[amount + 1];\n    for (int x = 0; x &lt;= amount; x++) dp[x] = 0;\n    dp[0] = 1;\n\n    for (int i = 0; i &lt; n; i++) {\n        int c = coins[i];\n        for (int x = c; x &lt;= amount; x++) {\n            dp[x] += dp[x - c];\n        }\n    }\n\n    printf(\"Number of ways: %lld\\n\", dp[amount]);\n    return 0;\n}\nPython (1D DP, combinations)\ncoins = list(map(int, input(\"Enter coin values: \").split()))\namount = int(input(\"Enter amount: \"))\n\ndp = [0] * (amount + 1)\ndp[0] = 1\n\nfor c in coins:\n    for x in range(c, amount + 1):\n        dp[x] += dp[x - c]\n\nprint(\"Number of ways:\", dp[amount])\n\n\nWhy It Matters\n\nIntroduces the idea of unbounded knapsack counting\nShows how loop ordering controls whether you count combinations or permutations\nForms a foundation for many counting DPs such as integer partitions and dice sum counts\nEncourages space optimization with a single dimension\n\n\n\nStep-by-Step Example\nCoins [1, 3, 4], amount 6:\n\n\n\nStep\nUpdate\ndp array snapshot (index is amount)\n\n\n\n\nInit\ndp[0]=1\n[1, 0, 0, 0, 0, 0, 0]\n\n\nCoin 1\nfill x=1..6\n[1, 1, 1, 1, 1, 1, 1]\n\n\nCoin 3\nx=3..6 add dp[x-3]\n[1, 1, 1, 2, 2, 2, 3]\n\n\nCoin 4\nx=4..6 add dp[x-4]\n[1, 1, 1, 2, 3, 3, 4]\n\n\n\nAnswer: dp[6] = 4.\n\n\nTry It Yourself\n\nSwitch to permutations counting: loop x outside and coins inside. Compare results.\nAdd a cap per coin type and convert to a bounded version.\nSort coins and print one valid combination using a parent pointer array.\nUse modulo arithmetic to avoid overflow: for example 10^9+7.\nExtend to count ways for every x from 0 to amount and print the full table.\n\n\n\nTest Cases\n\n\n\nCoins\nAmount\nExpected Ways\nNotes\n\n\n\n\n[]\n0\n1\nOne empty way\n\n\n[]\n5\n0\nNo coins cannot form positive sum\n\n\n[1]\n4\n1\nOnly 1+1+1+1\n\n\n[2]\n3\n0\nOdd cannot be formed\n\n\n[1,2,5]\n5\n4\nClassic example\n\n\n[2,3,7]\n12\n4\nCombinations only\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times \\text{amount})\\) where n is number of coin types\nSpace: \\(O(\\text{amount})\\) with 1D DP\n\nCoin Change counting teaches you how state order and loop order shape the meaning of a DP. Once you feel this pattern, many counting problems become straightforward.\n\n\n\n406 Coin Change (Min Coins)\nNow we shift from counting ways to finding the fewest coins. Given a target amount and coin denominations, how can we form the sum using the minimum number of coins? This version of coin change turns counting into optimization, a small twist with big impact.\n\nWhat Problem Are We Solving?\nGiven coins[] and a total amount, find the minimum number of coins needed to make up that amount. If it’s impossible, return -1.\nWe define: \\[\ndp[x] = \\text{minimum coins to make sum } x\n\\] with base case: \\[\ndp[0] = 0\n\\] Recurrence: \\[\ndp[x] = \\min_{c \\in coins,\\ c \\le x} (dp[x - c] + 1)\n\\]\nEach dp[x] asks: “If I take coin c, what’s the best I can do with the remainder?”\n\n\nHow Does It Work (Plain Language)?\nYou start from 0 and climb up, building the cheapest way to reach every amount. For each x, you try all coins c ≤ x. If you can make x-c, add one coin and see if that’s better than your current best.\nIt’s like choosing the shortest route to a destination using smaller hops.\nExample: coins = [1, 3, 4], amount = 6\n\n\n\nAmount (x)\ndp[x]\nExplanation\n\n\n\n\n0\n0\nBase case\n\n\n1\n1\n1×1\n\n\n2\n2\n1+1\n\n\n3\n1\n3\n\n\n4\n1\n4\n\n\n5\n2\n1+4\n\n\n6\n2\n3+3\n\n\n\nSo minimum = 2.\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n#define INF 1000000\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\nint main(void) {\n    int n, amount;\n    printf(\"Enter number of coins and amount: \");\n    scanf(\"%d %d\", &n, &amount);\n\n    int coins[n];\n    printf(\"Enter coin values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &coins[i]);\n\n    int dp[amount + 1];\n    for (int x = 1; x &lt;= amount; x++) dp[x] = INF;\n    dp[0] = 0;\n\n    for (int x = 1; x &lt;= amount; x++) {\n        for (int i = 0; i &lt; n; i++) {\n            int c = coins[i];\n            if (x - c &gt;= 0) dp[x] = MIN(dp[x], dp[x - c] + 1);\n        }\n    }\n\n    if (dp[amount] == INF)\n        printf(\"Not possible\\n\");\n    else\n        printf(\"Min coins: %d\\n\", dp[amount]);\n\n    return 0;\n}\nPython (Straightforward DP)\ncoins = list(map(int, input(\"Enter coin values: \").split()))\namount = int(input(\"Enter amount: \"))\n\nINF = float('inf')\ndp = [INF] * (amount + 1)\ndp[0] = 0\n\nfor x in range(1, amount + 1):\n    for c in coins:\n        if x - c &gt;= 0:\n            dp[x] = min(dp[x], dp[x - c] + 1)\n\nprint(\"Min coins:\" if dp[amount] != INF else \"Not possible\", end=\" \")\nprint(dp[amount] if dp[amount] != INF else \"\")\n\n\nWhy It Matters\n\nCore unbounded optimization DP\nShows minimization recurrence with base infinity\nIllustrates subproblem dependency: dp[x] depends on smaller sums\nConnects directly to Knapsack, shortest path, and DP + BFS hybrids\n\nThis version teaches you to mix greedy intuition with DP correctness.\n\n\nStep-by-Step Example\nCoins [1, 3, 4], amount = 6\n\n\n\nx\nTry 1\nTry 3\nTry 4\ndp[x]\n\n\n\n\n0\n-\n-\n-\n0\n\n\n1\ndp[0]+1=1\n-\n-\n1\n\n\n2\ndp[1]+1=2\n-\n-\n2\n\n\n3\ndp[2]+1=3\ndp[0]+1=1\n-\n1\n\n\n4\ndp[3]+1=2\ndp[1]+1=2\ndp[0]+1=1\n1\n\n\n5\ndp[4]+1=2\ndp[2]+1=3\ndp[1]+1=2\n2\n\n\n6\ndp[5]+1=3\ndp[3]+1=2\ndp[2]+1=3\n2\n\n\n\nAnswer = dp[6] = 2\n\n\nTry It Yourself\n\nAdd code to reconstruct the actual coin set.\nCompare greedy vs DP for coins [1, 3, 4], amount = 6.\nModify to handle limited coin supply.\nUse recursion + memoization and compare runtime.\nTry edge cases: amount smaller than smallest coin.\n\n\n\nTest Cases\n\n\n\nCoins\nAmount\nExpected Output\nNotes\n\n\n\n\n[1]\n3\n3\nOnly 1s\n\n\n[2]\n3\n-1\nImpossible\n\n\n[1,3,4]\n6\n2\n3+3 or 4+1+1\n\n\n[1,2,5]\n11\n3\n5+5+1\n\n\n[2,5,10]\n0\n0\nNo coins needed\n\n\n\n\n\nComplexity\n\nTime: O(n×amount)\nSpace: O(amount)\n\nCoin Change (Min Coins) is a masterclass in thinking minimally, every subproblem is a small decision toward the most efficient path.\n\n\n\n407 Knapsack 0/1\nThe 0/1 Knapsack problem is one of the crown jewels of dynamic programming. You’re given a backpack with limited capacity and a set of items, each with a weight and a value. You must decide which items to pack so that the total value is maximized without exceeding the weight limit. You can either take an item (1) or leave it (0), hence the name.\n\nWhat Problem Are We Solving?\nGiven:\n\nn items, each with weight[i] and value[i]\ncapacity W\n\nFind the maximum total value you can carry: \\[\ndp[i][w] = \\text{max value using first i items with capacity } w\n\\]\nRecurrence: \\[\ndp[i][w] = \\max(\ndp[i-1][w], \\quad\nvalue[i-1] + dp[i-1][w - weight[i-1]]\n)\n\\] if weight[i-1] &lt;= w, else dp[i][w] = dp[i-1][w].\nBase: \\[\ndp[0][w] = 0, \\quad dp[i][0] = 0\n\\]\n\n\nHow Does It Work (Plain Language)?\nThink of your backpack as a budget of space. Each item is a trade-off:\n\nInclude it → gain its value but lose capacity\nExclude it → keep capacity for others\n\nYou make this decision for every item and every possible capacity.\nWe build a table where each cell dp[i][w] stores the best value you can achieve with the first i items and total capacity w.\nExample: Items = [(w=1,v=1), (w=3,v=4), (w=4,v=5), (w=5,v=7)], W = 7\n\n\n\n\\(i/w\\)\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n1\n1\n1\n1\n1\n1\n1\n\n\n2\n0\n1\n1\n4\n5\n5\n5\n5\n\n\n3\n0\n1\n1\n4\n5\n6\n6\n9\n\n\n4\n0\n1\n1\n4\n5\n7\n8\n9\n\n\n\nAnswer = 9 (items 2 + 3)\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n\n#define MAX(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n, W;\n    printf(\"Enter number of items and capacity: \");\n    scanf(\"%d %d\", &n, &W);\n\n    int wt[n], val[n];\n    printf(\"Enter weights: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &wt[i]);\n    printf(\"Enter values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &val[i]);\n\n    int dp[n + 1][W + 1];\n\n    for (int i = 0; i &lt;= n; i++) {\n        for (int w = 0; w &lt;= W; w++) {\n            if (i == 0 || w == 0) dp[i][w] = 0;\n            else if (wt[i-1] &lt;= w)\n                dp[i][w] = MAX(val[i-1] + dp[i-1][w - wt[i-1]], dp[i-1][w]);\n            else\n                dp[i][w] = dp[i-1][w];\n        }\n    }\n\n    printf(\"Max value: %d\\n\", dp[n][W]);\n    return 0;\n}\nPython (Space Optimized)\nweights = list(map(int, input(\"Enter weights: \").split()))\nvalues = list(map(int, input(\"Enter values: \").split()))\nW = int(input(\"Enter capacity: \"))\nn = len(weights)\n\ndp = [0] * (W + 1)\n\nfor i in range(n):\n    for w in range(W, weights[i] - 1, -1):\n        dp[w] = max(dp[w], values[i] + dp[w - weights[i]])\n\nprint(\"Max value:\", dp[W])\n\n\nWhy It Matters\n\nTeaches choice-based DP: include or exclude\nCore of resource allocation, subset selection, budgeting problems\nFoundation for advanced DPs (subset sum, partition, scheduling)\nIntroduces 2D → 1D space optimization\n\nThis problem embodies the essence of decision-making in DP: to take or not to take.\n\n\nStep-by-Step Example\nItems: (w,v) = (1,1), (3,4), (4,5), (5,7), W=7\n\n\n\nStep\nCapacity\nAction\ndp\n\n\n\n\ni=1\nw≥1\nTake (1,1)\n+1 value\n\n\ni=2\nw≥3\nTake (3,4)\nReplace low combos\n\n\ni=3\nw≥4\nCombine (3+4) = 9\nMax found\n\n\ni=4\nw=7\nCan’t beat 9\nDone\n\n\n\n\n\nTry It Yourself\n\nPrint selected items using a traceback table.\nCompare 2D vs 1D versions.\nAdd constraint for exact weight match.\nTry variants: maximize weight, minimize count, etc.\nModify for fractional weights → Greedy Fractional Knapsack.\n\n\n\nTest Cases\n\n\n\nWeights\nValues\nCapacity\nExpected\nNotes\n\n\n\n\n[1,2,3]\n[10,15,40]\n6\n65\nTake all\n\n\n[2,3,4,5]\n[3,4,5,6]\n5\n7\n(2,3)\n\n\n[1,3,4,5]\n[1,4,5,7]\n7\n9\n(3,4)\n\n\n[2,5]\n[5,10]\n3\n5\nOnly first fits\n\n\n\n\n\nComplexity\n\nTime: O(n×W)\nSpace: O(n×W) → O(W) optimized\n\n0/1 Knapsack is the archetype of dynamic programming, it’s all about balancing choices, constraints, and rewards.\n\n\n\n408 Knapsack Unbounded\nThe Unbounded Knapsack problem is the free refill version of knapsack. You still want to maximize value under a capacity limit, but now each item can be chosen multiple times. It’s like packing snacks, you can grab as many as you want, as long as they fit in the bag.\n\nWhat Problem Are We Solving?\nGiven:\n\nn items with weight[i] and value[i]\ncapacity W\nUnlimited copies of each item\n\nFind the maximum value achievable without exceeding W.\nState: \\[\ndp[w] = \\text{max value for capacity } w\n\\]\nRecurrence: \\[\ndp[w] = \\max_{i: weight[i] \\le w} (dp[w - weight[i]] + value[i])\n\\]\nBase: \\[\ndp[0] = 0\n\\]\nNotice that this is similar to 0/1 Knapsack, but here we reuse items. The difference lies in the order of iteration.\n\n\nHow Does It Work (Plain Language)?\nThink of capacity w as a budget. For each capacity, you check all items, if one fits, you see what happens when you reuse it. Unlike 0/1 Knapsack (where each item can only be used once per combination), Unbounded Knapsack allows multiple selections.\n\n\n\nCapacity (w)\nBest Value\nExplanation\n\n\n\n\n0\n0\nEmpty\n\n\n1\n15\n1 copy of item(1,15)\n\n\n2\n30\n2 copies\n\n\n3\n45\n3 copies\n\n\n4\n60\n4 copies\n\n\n\n(If all items have same ratio, you’ll fill with the best one.)\nExample: Items: (w,v) = (2,4), (3,7), (4,9), W = 7\n\ndp[2] = 4\ndp[3] = 7\ndp[4] = 9\ndp[5] = max(dp[3]+4, dp[2]+7) = 11\ndp[6] = max(dp[4]+4, dp[3]+7, dp[2]+9) = 14\ndp[7] = max(dp[5]+4, dp[4]+7, dp[3]+9) = 16\n\nAnswer = 16\n\n\nTiny Code (Easy Versions)\nC (1D DP, Unbounded)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n, W;\n    printf(\"Enter number of items and capacity: \");\n    scanf(\"%d %d\", &n, &W);\n\n    int wt[n], val[n];\n    printf(\"Enter weights: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &wt[i]);\n    printf(\"Enter values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &val[i]);\n\n    int dp[W + 1];\n    for (int w = 0; w &lt;= W; w++) dp[w] = 0;\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int w = wt[i]; w &lt;= W; w++) {\n            dp[w] = MAX(dp[w], val[i] + dp[w - wt[i]]);\n        }\n    }\n\n    printf(\"Max value: %d\\n\", dp[W]);\n    return 0;\n}\nPython (Simple Bottom-Up)\nweights = list(map(int, input(\"Enter weights: \").split()))\nvalues = list(map(int, input(\"Enter values: \").split()))\nW = int(input(\"Enter capacity: \"))\n\ndp = [0] * (W + 1)\n\nfor i in range(len(weights)):\n    for w in range(weights[i], W + 1):\n        dp[w] = max(dp[w], values[i] + dp[w - weights[i]])\n\nprint(\"Max value:\", dp[W])\n\n\nWhy It Matters\n\nDemonstrates unbounded usage of elements\nBasis for coin change (min), rod cutting, integer break\nHighlights the importance of iteration order in DP\nConnects counting (how many ways) to optimization (best way)\n\nThis is where combinatorial explosion becomes manageable.\n\n\nStep-by-Step Example\nItems: (2,4), (3,7), (4,9), W = 7\n\n\n\nw\ndp[w]\nBest Choice\n\n\n\n\n0\n0\n-\n\n\n1\n0\nnone fits\n\n\n2\n4\n(2)\n\n\n3\n7\n(3)\n\n\n4\n9\n(4)\n\n\n5\n11\n(2+3)\n\n\n6\n14\n(3+3)\n\n\n7\n16\n(3+4)\n\n\n\nAnswer: 16\n\n\nTry It Yourself\n\nPrint the items used (store parent choice).\nCompare 0/1 and Unbounded outputs.\nAdd a limit on copies, hybrid knapsack.\nChange objective: minimize number of items.\nApply to Rod Cutting problem.\n\n\n\nTest Cases\n\n\n\nWeights\nValues\nW\nExpected\nNotes\n\n\n\n\n[2,3,4]\n[4,7,9]\n7\n16\n3+4\n\n\n[1,2,3]\n[10,15,40]\n6\n90\nsix 1s\n\n\n[5,10,20]\n[10,30,50]\n20\n100\nfour 5s or one 20\n\n\n[2,5]\n[5,10]\n3\n5\nonly one 2\n\n\n\n\n\nComplexity\n\nTime: O(n×W)\nSpace: O(W)\n\nUnbounded Knapsack is your first taste of infinite choice under constraint, a powerful idea that flows through many DP designs.\n\n\n\n409 Longest Increasing Subsequence (DP)\nThe Longest Increasing Subsequence (LIS) problem is a classic, it’s all about finding the longest chain of numbers that strictly increases. You don’t have to keep them consecutive, just in order. This is a foundational DP problem that blends state definition, transitions, and comparisons beautifully.\n\nWhat Problem Are We Solving?\nGiven an array arr[] of length n, find the length of the longest increasing subsequence, a sequence of indices i₁ &lt; i₂ &lt; ... &lt; iₖ such that: \\[\narr[i₁] &lt; arr[i₂] &lt; \\cdots &lt; arr[iₖ]\n\\]\nWe want maximum length.\nRecurrence: \\[\ndp[i] = 1 + \\max(dp[j]) \\quad \\text{for all } j &lt; i \\text{ where } arr[j] &lt; arr[i]\n\\] Otherwise: \\[\ndp[i] = 1\n\\]\nBase case: \\[\ndp[0] = 1\n\\]\nAnswer: \\[\n\\max_i dp[i]\n\\]\n\n\nHow Does It Work (Plain Language)?\nYou look at each number and ask: “Can I extend an increasing sequence ending before me?” If yes, take the longest one that fits and extend it by one.\nIt’s like building towers, each number stacks on top of a smaller one, extending the tallest possible stack.\nExample: arr = [10, 22, 9, 33, 21, 50, 41, 60]\n\n\n\ni\narr[i]\ndp[i]\nReason\n\n\n\n\n0\n10\n1\nstart\n\n\n1\n22\n2\n10→22\n\n\n2\n9\n1\nno smaller before\n\n\n3\n33\n3\n10→22→33\n\n\n4\n21\n2\n10→21\n\n\n5\n50\n4\n10→22→33→50\n\n\n6\n41\n4\n10→22→33→41\n\n\n7\n60\n5\n10→22→33→50→60\n\n\n\nAnswer = 5\n\n\nTiny Code (Easy Versions)\nC (O(n²) DP)\n#include &lt;stdio.h&gt;\n\n#define MAX(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n;\n    printf(\"Enter number of elements: \");\n    scanf(\"%d\", &n);\n\n    int arr[n];\n    printf(\"Enter array: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &arr[i]);\n\n    int dp[n];\n    for (int i = 0; i &lt; n; i++) dp[i] = 1;\n\n    int ans = 1;\n    for (int i = 1; i &lt; n; i++) {\n        for (int j = 0; j &lt; i; j++) {\n            if (arr[j] &lt; arr[i])\n                dp[i] = MAX(dp[i], dp[j] + 1);\n        }\n        ans = MAX(ans, dp[i]);\n    }\n\n    printf(\"Length of LIS: %d\\n\", ans);\n    return 0;\n}\nPython (Simple DP)\narr = list(map(int, input(\"Enter array: \").split()))\nn = len(arr)\ndp = [1] * n\n\nfor i in range(n):\n    for j in range(i):\n        if arr[j] &lt; arr[i]:\n            dp[i] = max(dp[i], dp[j] + 1)\n\nprint(\"Length of LIS:\", max(dp))\n\n\nWhy It Matters\n\nCore sequence DP, compares pairs, tracks best chain\nDemonstrates O(n²) DP thinking\nFoundation for LCS, Edit Distance, Patience Sorting (O(n log n))\nApplied in stock analysis, genome sequences, and chain problems\n\nThis problem teaches “look back and extend”, a key DP instinct.\n\n\nStep-by-Step Example\narr = [3, 10, 2, 1, 20]\n\n\n\ni\narr[i]\ndp[i]\nBest Chain\n\n\n\n\n0\n3\n1\n[3]\n\n\n1\n10\n2\n[3,10]\n\n\n2\n2\n1\n[2]\n\n\n3\n1\n1\n[1]\n\n\n4\n20\n3\n[3,10,20]\n\n\n\nAnswer = 3\n\n\nTry It Yourself\n\nPrint the actual LIS using a parent array.\nConvert to non-decreasing LIS (≤ instead of &lt;).\nCompare with O(n log n) binary search version.\nAdapt for longest decreasing subsequence.\nApply to 2D pairs (Russian Doll Envelopes).\n\n\n\nTest Cases\n\n\n\narr\nExpected\nNotes\n\n\n\n\n[1,2,3,4,5]\n5\nAlready increasing\n\n\n[5,4,3,2,1]\n1\nOnly one element\n\n\n[3,10,2,1,20]\n3\n[3,10,20]\n\n\n[10,22,9,33,21,50,41,60]\n5\nClassic example\n\n\n[2,2,2,2]\n1\nStrictly increasing only\n\n\n\n\n\nComplexity\n\nTime: O(n²)\nSpace: O(n)\n\nLIS is the melody of DP, every element listens to its predecessors, finds harmony, and extends the tune to its fullest length.\n\n\n\n410 Edit Distance (Levenshtein)\nThe Edit Distance (or Levenshtein distance) problem measures how different two strings are by counting the minimum number of operations needed to transform one into the other. The allowed operations are:\n\nInsert\nDelete\nReplace\n\nIt’s the foundation of spell checkers, DNA sequence alignment, and fuzzy search, anywhere we need to measure “how close” two sequences are.\n\nWhat Problem Are We Solving?\nGiven two strings A and B, find the minimum number of operations required to convert A → B.\nLet:\n\nA has length m\nB has length n\n\nState: \\[\ndp[i][j] = \\text{min edits to convert } A[0..i-1] \\text{ to } B[0..j-1]\n\\]\nRecurrence:\n\nIf A[i-1] == B[j-1]: \\[\ndp[i][j] = dp[i-1][j-1]\n\\]\nElse, take min of the three operations: \\[\ndp[i][j] = 1 + \\min(\ndp[i-1][j],     \\text{ (delete)}\ndp[i][j-1],     \\text{ (insert)}\ndp[i-1][j-1]    \\text{ (replace)}\n)\n\\]\n\nBase: \\[\ndp[0][j] = j,\\quad dp[i][0] = i\n\\] (empty string conversions)\n\n\nHow Does It Work (Plain Language)?\nImagine editing a word character by character. At each step, compare the current letters:\n\nIf they match → no cost, move diagonally.\nIf they differ → choose the cheapest fix (insert, delete, replace).\n\nThe DP table builds all prefix transformations, from small strings to full ones.\nExample: A = \"kitten\", B = \"sitting\"\n\n\n\nStep\nOperation\nResult\n\n\n\n\nReplace k → s\nsitten\n\n\n\nReplace e → i\nsittin\n\n\n\nInsert g\nsitting\n\n\n\n\nAnswer = 3\n\n\nTiny Code (Easy Versions)\nC (2D DP)\n#include &lt;stdio.h&gt;\n#define MIN3(a,b,c) ((a&lt;b?a:b)&lt;c?(a&lt;b?a:b):c)\n\nint main(void) {\n    char A[100], B[100];\n    printf(\"Enter string A: \");\n    scanf(\"%s\", A);\n    printf(\"Enter string B: \");\n    scanf(\"%s\", B);\n\n    int m = 0, n = 0;\n    while (A[m]) m++;\n    while (B[n]) n++;\n\n    int dp[m + 1][n + 1];\n\n    for (int i = 0; i &lt;= m; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= n; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            if (A[i-1] == B[j-1]) dp[i][j] = dp[i-1][j-1];\n            else dp[i][j] = 1 + MIN3(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]);\n        }\n    }\n\n    printf(\"Edit distance: %d\\n\", dp[m][n]);\n    return 0;\n}\nPython (Simple Version)\nA = input(\"Enter string A: \")\nB = input(\"Enter string B: \")\n\nm, n = len(A), len(B)\ndp = [[0]*(n+1) for _ in range(m+1)]\n\nfor i in range(m+1): dp[i][0] = i\nfor j in range(n+1): dp[0][j] = j\n\nfor i in range(1, m+1):\n    for j in range(1, n+1):\n        if A[i-1] == B[j-1]:\n            dp[i][j] = dp[i-1][j-1]\n        else:\n            dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n\nprint(\"Edit distance:\", dp[m][n])\n\n\nWhy It Matters\n\nIllustrates 2D DP on strings\nIntroduces transformation problems\nForms the backbone of spell correction, DNA alignment, diff tools\nBeautifully captures state = prefix lengths pattern\n\nEdit Distance is the dictionary definition of “step-by-step transformation.”\n\n\nStep-by-Step Example\nA = \"intention\", B = \"execution\"\n\n\n\n\\(A/B\\)\n“”\ne\nx\ne\nc\nu\nt\ni\no\nn\n\n\n\n\n“”\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\ni\n1\n1\n2\n3\n4\n5\n6\n6\n7\n8\n\n\nn\n2\n2\n2\n3\n4\n5\n6\n7\n7\n7\n\n\nt\n3\n3\n3\n3\n4\n5\n5\n6\n7\n8\n\n\ne\n4\n3\n4\n3\n4\n5\n6\n6\n7\n8\n\n\nn\n5\n4\n4\n4\n4\n5\n6\n7\n7\n7\n\n\nt\n6\n5\n5\n5\n5\n5\n5\n6\n7\n8\n\n\ni\n7\n6\n6\n6\n6\n6\n6\n5\n6\n7\n\n\no\n8\n7\n7\n7\n7\n7\n7\n6\n5\n6\n\n\nn\n9\n8\n8\n8\n8\n8\n8\n7\n6\n5\n\n\n\nAnswer = 5 edits\n\n\nTry It Yourself\n\nPrint actual edit sequence (backtrack).\nAdd costs: assign different weights for insert/delete/replace.\nTry case-insensitive variant.\nCompare with Longest Common Subsequence.\nImplement recursive + memoized version.\n\n\n\nTest Cases\n\n\n\nA\nB\nExpected\nNotes\n\n\n\n\n“kitten”\n“sitting”\n3\nclassic\n\n\n“horse”\n“ros”\n3\nleetcode\n\n\n“flaw”\n“lawn”\n2\nreplace + insert\n\n\n“abc”\n“yabd”\n2\ninsert + replace\n\n\n“”\n“abc”\n3\nall inserts\n\n\n\n\n\nComplexity\n\nTime: O(m×n)\nSpace: O(m×n), reducible to O(n)\n\nEdit Distance teaches precision in DP: every cell means “smallest change to fix this prefix”. It’s the language of correction, one letter at a time.\n\n\n\n\nSection 42. Classic Problems\n\n411 0/1 Knapsack\nThe 0/1 Knapsack is one of the most iconic problems in dynamic programming. It’s the perfect example of decision-making under constraints, each item can either be taken or left, but never split or repeated. The goal is to maximize total value within a fixed capacity.\nThis version focuses on understanding choice, capacity, and optimal substructure, the three pillars of DP.\n\nWhat Problem Are We Solving?\nGiven:\n\nn items, each with weight w[i] and value v[i]\na knapsack of capacity W\n\nWe want: \\[\n\\text{maximize total value} \\quad \\sum v[i]\n\\] subject to \\[\n\\sum w[i] \\le W\n\\] and each item can be used at most once.\nState definition: \\[\ndp[i][w] = \\text{max value using first } i \\text{ items with capacity } w\n\\]\nRecurrence: \\[\ndp[i][w] =\n\\begin{cases}\ndp[i-1][w], & \\text{if } w_{i-1} &gt; w,\\\\\n\\max\\big(dp[i-1][w],\\ dp[i-1][w - w_{i-1}] + v_{i-1}\\big), & \\text{otherwise.}\n\\end{cases}\n\\]\nBase case: \\[\ndp[0][w] = 0, \\quad dp[i][0] = 0\n\\]\nAnswer: \\[\ndp[n][W]\n\\]\n\n\nHow Does It Work (Plain Language)?\nAt every step, you ask: “Should I take this item or leave it?” If it fits, compare:\n\nNot taking it → stick with previous best (dp[i-1][w])\nTaking it → add its value plus best value for remaining capacity (dp[i-1][w - weight[i]] + value[i])\n\nThe DP table stores the best possible value at every sub-capacity for each subset of items.\n\n\n\nItem\nWeight\nValue\n\n\n\n\n1\n1\n1\n\n\n2\n3\n4\n\n\n3\n4\n5\n\n\n4\n5\n7\n\n\n\nCapacity = 7 → Answer = 9 (items 2 + 3)\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n, W;\n    printf(\"Enter number of items and capacity: \");\n    scanf(\"%d %d\", &n, &W);\n\n    int wt[n], val[n];\n    printf(\"Enter weights: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &wt[i]);\n    printf(\"Enter values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &val[i]);\n\n    int dp[n + 1][W + 1];\n\n    for (int i = 0; i &lt;= n; i++) {\n        for (int w = 0; w &lt;= W; w++) {\n            if (i == 0 || w == 0) dp[i][w] = 0;\n            else if (wt[i-1] &lt;= w)\n                dp[i][w] = MAX(val[i-1] + dp[i-1][w - wt[i-1]], dp[i-1][w]);\n            else\n                dp[i][w] = dp[i-1][w];\n        }\n    }\n\n    printf(\"Max value: %d\\n\", dp[n][W]);\n    return 0;\n}\nPython (1D Optimized)\nweights = list(map(int, input(\"Enter weights: \").split()))\nvalues = list(map(int, input(\"Enter values: \").split()))\nW = int(input(\"Enter capacity: \"))\nn = len(weights)\n\ndp = [0] * (W + 1)\n\nfor i in range(n):\n    for w in range(W, weights[i] - 1, -1):\n        dp[w] = max(dp[w], dp[w - weights[i]] + values[i])\n\nprint(\"Max value:\", dp[W])\n\n\nWhy It Matters\n\nIntroduces decision-based DP: take or skip\nBuilds on recurrence intuition (state transition)\nForms basis for subset sum, equal partition, and resource allocation\nTeaches capacity-dependent states\n\nIt’s the first time you feel the tension between greedy desire and constrained reality, a DP classic.\n\n\nStep-by-Step Example\nItems: (w,v) = (1,1), (3,4), (4,5), (5,7), W = 7\n\n\n\n\\(i/w\\)\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n1\n1\n1\n1\n1\n1\n1\n\n\n2\n0\n1\n1\n4\n5\n5\n5\n5\n\n\n3\n0\n1\n1\n4\n5\n6\n6\n9\n\n\n4\n0\n1\n1\n4\n5\n7\n8\n9\n\n\n\nAnswer = 9\n\n\nTry It Yourself\n\nAdd code to reconstruct chosen items.\nCompare 2D vs 1D DP outputs.\nModify to minimize weight for a given value.\nVisualize table transitions for small inputs.\nExperiment with large weights, test performance.\n\n\n\nTest Cases\n\n\n\nWeights\nValues\nW\nExpected\nNotes\n\n\n\n\n[1,2,3]\n[10,15,40]\n6\n65\nall items fit\n\n\n[2,3,4,5]\n[3,4,5,6]\n5\n7\n(2,3)\n\n\n[1,3,4,5]\n[1,4,5,7]\n7\n9\n(3,4)\n\n\n[2,5]\n[5,10]\n3\n5\nonly first fits\n\n\n\n\n\nComplexity\n\nTime: O(n×W)\nSpace: O(n×W) → O(W) (optimized)\n\n0/1 Knapsack is the heartbeat of DP, every decision echoes the fundamental trade-off: to take or not to take.\n\n\n\n412 Subset Sum\nThe Subset Sum problem is a fundamental example of boolean dynamic programming. Instead of maximizing or minimizing, we simply ask “Is it possible?”, can we pick a subset of numbers that adds up to a given target?\nThis problem forms the foundation for many combinatorial DP problems such as Equal Partition, Count of Subsets, Target Sum, and even Knapsack itself.\n\nWhat Problem Are We Solving?\nGiven:\n\nAn array arr[] of n positive integers\nA target sum S\n\nDetermine whether there exists a subset of arr[] whose elements sum to exactly S.\nWe define: \\[\ndp[i][s] = \\text{true if subset of first } i \\text{ elements can form sum } s\n\\]\nRecurrence:\n\nIf arr[i-1] &gt; s: [ dp[i][s] = dp[i-1][s]]\nElse: [ dp[i][s] = dp[i-1][s] dp[i-1][s - arr[i-1]]]\n\nBase cases: \\[\ndp[0][0] = \\text{true}, \\quad dp[0][s&gt;0] = \\text{false}\n\\]\nAnswer: \\[\ndp[n][S]\n\\]\n\n\nHow Does It Work (Plain Language)?\nThink of it as a yes/no table:\n\nRows → items\nColumns → sums\n\nEach cell asks: “Can I form sum s using the first i items?” The answer comes from either skipping or including the current item.\nExample: arr = [2, 3, 7, 8, 10], S = 11\n\n\n\n\\(i/Sum\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n0\nT\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\n\n\n1 (2)\nT\nF\nT\nF\nF\nF\nF\nF\nF\nF\nF\nF\n\n\n2 (3)\nT\nF\nT\nT\nF\nT\nF\nF\nF\nF\nF\nF\n\n\n3 (7)\nT\nF\nT\nT\nF\nT\nF\nT\nT\nF\nT\nT\n\n\n\ndp[5][11] = True → [3, 8] is one valid subset.\n\n\nTiny Code (Easy Versions)\nC (2D Boolean Table)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n, S;\n    printf(\"Enter number of elements and target sum: \");\n    scanf(\"%d %d\", &n, &S);\n\n    int arr[n];\n    printf(\"Enter elements: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &arr[i]);\n\n    int dp[n + 1][S + 1];\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = 1;\n    for (int s = 1; s &lt;= S; s++) dp[0][s] = 0;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int s = 1; s &lt;= S; s++) {\n            if (arr[i-1] &gt; s)\n                dp[i][s] = dp[i-1][s];\n            else\n                dp[i][s] = dp[i-1][s] || dp[i-1][s - arr[i-1]];\n        }\n    }\n\n    printf(\"Subset sum %s possible\\n\", dp[n][S] ? \"is\" : \"is not\");\n    return 0;\n}\nPython (1D Optimization)\narr = list(map(int, input(\"Enter elements: \").split()))\nS = int(input(\"Enter target sum: \"))\n\ndp = [False] * (S + 1)\ndp[0] = True\n\nfor num in arr:\n    for s in range(S, num - 1, -1):\n        dp[s] = dp[s] or dp[s - num]\n\nprint(\"Subset sum is possible\" if dp[S] else \"Not possible\")\n\n\nWhy It Matters\n\nIntroduces boolean DP (true/false states)\nFoundation for Equal Partition, Target Sum, and Count Subsets\nClosely related to 0/1 Knapsack but without values\nPerfect exercise for learning state dependency\n\nThis problem captures the logic of feasibility: “If I could make s - arr[i] before, then I can make s now.”\n\n\nStep-by-Step Example\nArray = [2, 3, 7, 8, 10], S = 11\n\n\n\nStep\nConsider\nNew True Sums\n\n\n\n\n2\n[0,2]\n{2}\n\n\n3\n[0,2,3,5]\n{3,5}\n\n\n7\n[0,2,3,5,7,9,10,12]\n{7,9,10}\n\n\n8\n[0,2,3,5,7,8,9,10,11,12,13,15]\n{11}\n\n\n\nFound 11.\n\n\nTry It Yourself\n\nPrint one valid subset using a parent pointer table.\nCount the total number of valid subsets (convert to count DP).\nTry with duplicates, does it change anything?\nModify to check if sum is divisible by k.\nAdd negative numbers (use offset shifting).\n\n\n\nTest Cases\n\n\n\narr\nS\nExpected\nNotes\n\n\n\n\n[2,3,7,8,10]\n11\nTrue\n3+8\n\n\n[1,2,3]\n5\nTrue\n2+3\n\n\n[1,2,5]\n4\nFalse\nno subset\n\n\n[1,1,1,1]\n2\nTrue\n1+1\n\n\n[5,2,6,4]\n13\nTrue\n5+4+4\n\n\n\n\n\nComplexity\n\nTime: O(n×S)\nSpace: O(n×S) → O(S) (optimized)\n\nSubset Sum is a cornerstone of DP, a yes/no version of Knapsack that teaches how logic flows through states, one sum at a time.\n\n\n\n413 Equal Partition\nThe Equal Partition problem asks a natural question: can we divide a set of numbers into two subsets with equal sum? It’s a direct application of Subset Sum, reframed as a partitioning challenge. If the total sum is even, we check if there’s a subset that sums to half, that ensures the other subset sums to the same.\n\nWhat Problem Are We Solving?\nGiven:\n\nAn array arr[] of n positive integers\n\nDetermine whether it can be partitioned into two subsets whose sums are equal.\nLet total sum be S. We need to check:\n\nIf S is odd → impossible\nIf S is even → check if Subset Sum to S/2 is possible\n\nSo the problem reduces to:\n\\[\n\\text{Is there a subset of } arr[] \\text{ with sum } = S/2?\n\\]\nWe use the same recurrence from Subset Sum:\n\\[\ndp[i][s] = dp[i-1][s] \\lor dp[i-1][s - arr[i-1]]\n\\]\nBase: \\[\ndp[0][0] = \\text{true}\n\\]\nAnswer: \\[\ndp[n][S/2]\n\\]\n\n\nHow Does It Work (Plain Language)?\n\nCompute total sum S.\nIf S is odd → cannot split evenly.\nOtherwise, use Subset Sum DP to check if we can reach S/2. If yes, one subset forms S/2, and the remaining numbers automatically form the other half.\n\nExample: arr = [1, 5, 11, 5]\n\nSum = 22\nTarget = 11\nCan we make 11? Yes → [11] and [1,5,5] are two halves.\n\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n;\n    printf(\"Enter number of elements: \");\n    scanf(\"%d\", &n);\n\n    int arr[n];\n    printf(\"Enter elements: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &arr[i]);\n\n    int sum = 0;\n    for (int i = 0; i &lt; n; i++) sum += arr[i];\n\n    if (sum % 2 != 0) {\n        printf(\"Cannot partition into equal sum subsets\\n\");\n        return 0;\n    }\n\n    int target = sum / 2;\n    int dp[n + 1][target + 1];\n\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = 1;\n    for (int s = 1; s &lt;= target; s++) dp[0][s] = 0;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int s = 1; s &lt;= target; s++) {\n            if (arr[i-1] &gt; s)\n                dp[i][s] = dp[i-1][s];\n            else\n                dp[i][s] = dp[i-1][s] || dp[i-1][s - arr[i-1]];\n        }\n    }\n\n    printf(\"Equal partition %s possible\\n\", dp[n][target] ? \"is\" : \"is not\");\n    return 0;\n}\nPython (1D Space Optimization)\narr = list(map(int, input(\"Enter elements: \").split()))\nS = sum(arr)\n\nif S % 2 != 0:\n    print(\"Cannot partition into equal subsets\")\nelse:\n    target = S // 2\n    dp = [False] * (target + 1)\n    dp[0] = True\n\n    for num in arr:\n        for s in range(target, num - 1, -1):\n            dp[s] = dp[s] or dp[s - num]\n\n    print(\"Equal partition is possible\" if dp[target] else \"Not possible\")\n\n\nWhy It Matters\n\nBuilds directly on Subset Sum\nDemonstrates problem reduction in DP\nUseful for balanced partitioning, load balancing, and fair division\nTeaches thinking in terms of state feasibility\n\nEqual Partition shows how yes/no DPs can solve seemingly complex questions with simple logic.\n\n\nStep-by-Step Example\narr = [1, 5, 11, 5]\n\nS = 22 → S/2 = 11\nUse Subset Sum DP to check if 11 can be formed\nTrue → subsets [11] and [1,5,5]\n\nAnother case: arr = [1, 2, 3, 5]\n\nS = 11 (odd) → cannot partition.\n\n\n\nTry It Yourself\n\nPrint the actual subsets (traceback table).\nTry arrays with duplicates.\nCompare with total sum odd case.\nAdd constraint: must use at least one element in each subset.\nVisualize dp table for small arrays.\n\n\n\nTest Cases\n\n\n\narr\nSum\nExpected\nNotes\n\n\n\n\n[1,5,11,5]\n22\nTrue\n11 and 11\n\n\n[1,2,3,5]\n11\nFalse\nodd total\n\n\n[3,3,3,3]\n12\nTrue\nsplit evenly\n\n\n[2,2,2,2,2]\n10\nTrue\n5+5\n\n\n[1,1,3,4,7]\n16\nTrue\n8+8\n\n\n\n\n\nComplexity\n\nTime: O(n × S/2)\nSpace: O(S/2) (optimized)\n\nEqual Partition is the first real taste of reduction in dynamic programming, take a bigger problem, express it as Subset Sum, and solve with the same machinery.\n\n\n\n414 Count of Subsets with Sum\nThe Count of Subsets with Sum problem extends the Subset Sum idea. Instead of asking “Is it possible to form this sum?”, we ask “In how many ways can we form it?”. This transforms a boolean DP into a counting DP, where each state accumulates the number of combinations that yield a given sum.\n\nWhat Problem Are We Solving?\nGiven:\n\nAn array arr[] of n positive integers\nA target sum S\n\nWe want the number of subsets whose elements sum exactly to S.\nWe define the state:\n\\[\ndp[i][s] = \\text{number of ways to form sum } s \\text{ using first } i \\text{ elements}\n\\]\nThe recurrence:\n\\[\ndp[i][s] =\n\\begin{cases}\ndp[i-1][s], & \\text{if } arr[i-1] &gt; s,\\\\\ndp[i-1][s] + dp[i-1][s - arr[i-1]], & \\text{otherwise.}\n\\end{cases}\n\\]\nBase cases:\n\\[\ndp[0][0] = 1, \\quad dp[0][s&gt;0] = 0\n\\]\nFinal answer:\n\\[\ndp[n][S]\n\\]\n\n\nHow Does It Work (Plain Language)\nEach element gives two paths: include or exclude. If you include it, you count all subsets that formed s - arr[i-1] before. If you exclude it, you inherit all subsets that already formed s. So each cell accumulates total combinations from both branches.\nExample: arr = [2, 3, 5, 6, 8, 10], S = 10\nWays to form 10:\n\n{10}\n{2, 8}\n{2, 3, 5}\n\nAnswer = 3\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n, S;\n    printf(\"Enter number of elements and target sum: \");\n    scanf(\"%d %d\", &n, &S);\n\n    int arr[n];\n    printf(\"Enter elements: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &arr[i]);\n\n    int dp[n + 1][S + 1];\n\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = 1;\n    for (int s = 1; s &lt;= S; s++) dp[0][s] = 0;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int s = 0; s &lt;= S; s++) {\n            if (arr[i-1] &gt; s)\n                dp[i][s] = dp[i-1][s];\n            else\n                dp[i][s] = dp[i-1][s] + dp[i-1][s - arr[i-1]];\n        }\n    }\n\n    printf(\"Number of subsets: %d\\n\", dp[n][S]);\n    return 0;\n}\nPython (1D Space Optimized)\narr = list(map(int, input(\"Enter elements: \").split()))\nS = int(input(\"Enter target sum: \"))\n\ndp = [0] * (S + 1)\ndp[0] = 1\n\nfor num in arr:\n    for s in range(S, num - 1, -1):\n        dp[s] += dp[s - num]\n\nprint(\"Number of subsets:\", dp[S])\n\n\nWhy It Matters\n\nExtends Subset Sum from feasibility to counting\nFoundation for Target Sum, Equal Partition Count, and Combinatorics DP\nShows how a small change in recurrence changes meaning\nDemonstrates accumulation instead of boolean OR\n\nThis is where DP transitions from logic to combinatorics, from “can I?” to “how many ways?”\n\n\nStep-by-Step Example\narr = [2, 3, 5, 6, 8, 10], S = 10\n\n\n\ni\narr[i]\nWays to form 10\nExplanation\n\n\n\n\n1\n2\n0\ncannot reach 10 yet\n\n\n2\n3\n0\n2+3=5 only\n\n\n3\n5\n1\n{5}\n\n\n4\n6\n1\n{10}\n\n\n5\n8\n2\n{2,8}, {10}\n\n\n6\n10\n3\n{10}, {2,8}, {2,3,5}\n\n\n\nAnswer = 3\n\n\nA Gentle Proof (Why It Works)\nWe build dp[i][s] using the inclusion-exclusion principle:\nTo form sum s using first i items, two possibilities exist:\n\nExclude arr[i-1]: all subsets that form s remain valid \\[ dp[i-1][s] \\]\nInclude arr[i-1]: each subset that formed s - arr[i-1] now forms s \\[ dp[i-1][s - arr[i-1]] \\]\n\nThus:\n\\[\ndp[i][s] = dp[i-1][s] + dp[i-1][s - arr[i-1]]\n\\]\nNo double counting occurs since each element is processed once, contributing to exactly one branch per subproblem. By building layer by layer, dp[n][S] accumulates all valid subset combinations summing to S.\n\n\nTry It Yourself\n\nPrint all valid subsets using recursive backtracking.\nModify the DP to count subsets with sum ≤ target.\nAdd duplicates and compare results.\nApply modulo \\(10^9 + 7\\) to handle large counts.\nExtend to count subsets with sum difference = D.\n\n\n\nTest Cases\n\n\n\narr\nS\nExpected\nNotes\n\n\n\n\n[2,3,5,6,8,10]\n10\n3\n{10}, {2,8}, {2,3,5}\n\n\n[1,1,1,1]\n2\n6\nchoose any 2\n\n\n[1,2,3]\n3\n2\n{3}, {1,2}\n\n\n[1,2,5]\n4\n0\nno subset\n\n\n[2,4,6,10]\n16\n2\n{6,10}, {2,4,10}\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times S)\\)\nSpace: \\(O(S)\\)\n\nThe Count of Subsets with Sum problem is a perfect illustration of how dynamic programming can evolve from feasibility to enumeration, counting every path that leads to success.\n\n\n\n415 Target Sum\nThe Target Sum problem combines Subset Sum and sign assignment, instead of selecting elements, you assign + or − to each one so that their total equals a target value. It’s a beautiful example of how DP can turn algebraic constraints into combinatorial counting.\n\nWhat Problem Are We Solving?\nGiven:\n\nAn array arr[] of n non-negative integers\nA target value T\n\nCount the number of ways to assign + or − signs to elements so that:\n\\[\na_1 \\pm a_2 \\pm a_3 \\pm \\ldots \\pm a_n = T\n\\]\nEach element must appear once with either sign.\nWe define:\n\nLet total sum be \\(S = \\sum arr[i]\\)\n\nIf we split into two subsets \\(P\\) (positive) and \\(N\\) (negative), we have:\n\\[\n\\begin{cases}\nP + N = S \\\nP - N = T\n\\end{cases}\n\\]\nSolve these equations:\n\\[\nP = \\frac{S + T}{2}\n\\]\nSo the problem becomes count subsets whose sum = (S + T)/2.\nIf \\((S + T)\\) is odd or \\(T &gt; S\\), answer = 0 (impossible).\n\n\nKey Idea\nConvert the sign problem into a subset counting problem:\n\\[\n\\text{Count subsets with sum } P = \\frac{S + T}{2}\n\\]\nThen use the recurrence from Count of Subsets with Sum:\n\\[\ndp[i][p] =\n\\begin{cases}\ndp[i-1][p], & \\text{if } arr[i-1] &gt; p,\\\\\ndp[i-1][p] + dp[i-1][p - arr[i-1]], & \\text{otherwise.}\n\\end{cases}\n\\]\nBase case:\n\\[\ndp[0][0] = 1\n\\]\nAnswer:\n\\[\ndp[n][P]\n\\]\n\n\nHow Does It Work (Plain Language)\nThink of each element as being placed on one of two sides: positive or negative. Instead of directly simulating signs, we compute how many subsets sum to \\((S + T)/2\\). That subset represents all numbers assigned +; the rest implicitly become −.\nExample: arr = [1, 1, 2, 3], T = 1 Sum S = 7 → \\(P = (7 + 1)/2 = 4\\)\nSo we count subsets summing to 4:\n\n{1, 3}\n{1, 1, 2} Answer = 2\n\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n, T;\n    printf(\"Enter number of elements and target: \");\n    scanf(\"%d %d\", &n, &T);\n\n    int arr[n];\n    printf(\"Enter elements: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &arr[i]);\n\n    int S = 0;\n    for (int i = 0; i &lt; n; i++) S += arr[i];\n\n    if ((S + T) % 2 != 0 || T &gt; S) {\n        printf(\"No solutions\\n\");\n        return 0;\n    }\n\n    int P = (S + T) / 2;\n    int dp[n + 1][P + 1];\n\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = 1;\n    for (int j = 1; j &lt;= P; j++) dp[0][j] = 0;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 0; j &lt;= P; j++) {\n            if (arr[i-1] &gt; j)\n                dp[i][j] = dp[i-1][j];\n            else\n                dp[i][j] = dp[i-1][j] + dp[i-1][j - arr[i-1]];\n        }\n    }\n\n    printf(\"Number of ways: %d\\n\", dp[n][P]);\n    return 0;\n}\nPython (1D Space Optimized)\narr = list(map(int, input(\"Enter elements: \").split()))\nT = int(input(\"Enter target: \"))\nS = sum(arr)\n\nif (S + T) % 2 != 0 or T &gt; S:\n    print(\"No solutions\")\nelse:\n    P = (S + T) // 2\n    dp = [0] * (P + 1)\n    dp[0] = 1\n\n    for num in arr:\n        for s in range(P, num - 1, -1):\n            dp[s] += dp[s - num]\n\n    print(\"Number of ways:\", dp[P])\n\n\nWhy It Matters\n\nTransforms sign assignment into subset counting\nReinforces algebraic manipulation in DP\nFoundation for expression evaluation, partition problems, and probabilistic sums\nDemonstrates how mathematical reformulation simplifies state design\n\nIt’s a powerful example of turning a tricky ± sum problem into a familiar counting DP.\n\n\nStep-by-Step Example\narr = [1, 1, 2, 3], T = 1 \\(S = 7\\) → \\(P = 4\\)\nSubsets summing to 4:\n\n{1, 3}\n{1, 1, 2}\n\nAnswer = 2\n\n\nA Gentle Proof (Why It Works)\nLet the positive subset sum be \\(P\\) and negative subset sum be \\(N\\). We have:\n\\[\nP - N = T \\quad \\text{and} \\quad P + N = S\n\\]\nAdding both: \\[\n2P = S + T \\implies P = \\frac{S + T}{2}\n\\]\nThus, any valid assignment of signs corresponds exactly to one subset summing to \\(P\\). Every subset of sum \\(P\\) defines a unique sign configuration:\n\nNumbers in \\(P\\) → positive\nNumbers not in \\(P\\) → negative\n\nSo counting subsets with sum \\(P\\) is equivalent to counting all valid sign assignments.\n\n\nTry It Yourself\n\nHandle zeros (they double the count).\nReturn all possible sign configurations.\nCheck with negative T (same symmetry).\nCompare brute-force enumeration with DP result.\nModify for constraints like “at least k positive numbers”.\n\n\n\nTest Cases\n\n\n\narr\nT\nExpected\nNotes\n\n\n\n\n[1,1,2,3]\n1\n2\n{1,3}, {1,1,2}\n\n\n[1,2,3]\n0\n2\n{1,2,3} and {-1,-2,-3}\n\n\n[2,2,2,2]\n4\n6\nmany ways\n\n\n[1,1,1,1]\n0\n6\nsymmetric partitions\n\n\n[5,3,2,1]\n5\n2\n{2,3}, {5}\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times P)\\)\nSpace: \\(O(P)\\)\n\nThe Target Sum problem shows how algebra and DP meet: by reinterpreting signs as subsets, you turn a puzzle of pluses and minuses into a clean combinatorial count.\n\n\n\n416 Unbounded Knapsack\nThe Unbounded Knapsack problem is the unlimited version of the classic 0/1 Knapsack. Here, each item can be chosen multiple times, as long as total weight stays within capacity. It’s one of the most elegant illustrations of reusable states in dynamic programming.\n\nWhat Problem Are We Solving?\nGiven:\n\nn items, each with weight[i] and value[i]\na knapsack of capacity W\n\nFind the maximum total value achievable without exceeding capacity W. Each item can be used any number of times.\nWe define the state:\n\\[\ndp[w] = \\text{maximum value for capacity } w\n\\]\nRecurrence:\n\\[\ndp[w] = \\max_{i: , weight[i] \\le w} \\big( dp[w - weight[i]] + value[i] \\big)\n\\]\nBase:\n\\[\ndp[0] = 0\n\\]\nFinal answer:\n\\[\ndp[W]\n\\]\nThe key difference from 0/1 Knapsack is order of iteration — for unbounded, we move forward through weights so items can be reused.\n\n\nHow Does It Work (Plain Language)\nThink of the capacity as a ladder. At each rung w, you check every item:\n\nIf it fits, you ask: “If I take this item, what’s the best I can do with the remaining space?”\nSince items are reusable, you can add it again later.\n\nThis way, every dp[w] builds from smaller capacities, each possibly using the same item again.\nExample: Items = (weight, value): (2,4), (3,7), (4,9) W = 7\n\n\n\nCapacity\ndp[w]\nExplanation\n\n\n\n\n0\n0\nbase\n\n\n1\n0\nno item fits\n\n\n2\n4\none (2,4)\n\n\n3\n7\none (3,7)\n\n\n4\n9\none (4,9)\n\n\n5\n11\n(2,4)+(3,7)\n\n\n6\n14\n(3,7)+(3,7)\n\n\n7\n16\n(3,7)+(4,9)\n\n\n\nAnswer = 16\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up 1D DP)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\n\nint main(void) {\n    int n, W;\n    printf(\"Enter number of items and capacity: \");\n    scanf(\"%d %d\", &n, &W);\n\n    int wt[n], val[n];\n    printf(\"Enter weights: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &wt[i]);\n    printf(\"Enter values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &val[i]);\n\n    int dp[W + 1];\n    for (int w = 0; w &lt;= W; w++) dp[w] = 0;\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int w = wt[i]; w &lt;= W; w++) {\n            dp[w] = MAX(dp[w], val[i] + dp[w - wt[i]]);\n        }\n    }\n\n    printf(\"Max value: %d\\n\", dp[W]);\n    return 0;\n}\nPython (Iterative Version)\nweights = list(map(int, input(\"Enter weights: \").split()))\nvalues = list(map(int, input(\"Enter values: \").split()))\nW = int(input(\"Enter capacity: \"))\n\ndp = [0] * (W + 1)\n\nfor i in range(len(weights)):\n    for w in range(weights[i], W + 1):\n        dp[w] = max(dp[w], values[i] + dp[w - weights[i]])\n\nprint(\"Max value:\", dp[W])\n\n\nWhy It Matters\n\nDemonstrates reusable subproblems, items can appear multiple times\nConnects to Coin Change (Min Coins) and Rod Cutting\nFoundation for integer partition, resource allocation, and bounded-unbounded hybrid problems\nTeaches forward iteration logic\n\nUnbounded Knapsack is the perfect showcase of DP with repetition.\n\n\nStep-by-Step Example\nItems: (2,4), (3,7), (4,9), W = 7\n\n\n\nw\ndp[w]\nBest Choice\n\n\n\n\n0\n0\nbase\n\n\n1\n0\nnone fits\n\n\n2\n4\n(2)\n\n\n3\n7\n(3)\n\n\n4\n9\n(4)\n\n\n5\n11\n(2,3)\n\n\n6\n14\n(3,3)\n\n\n7\n16\n(3,4)\n\n\n\nAnswer = 16\n\n\nA Gentle Proof (Why It Works)\nFor each capacity w, we consider every item i such that weight[i] ≤ w. If we choose item i, we gain its value plus the best value achievable for the remaining space w - weight[i]:\n\\[\ndp[w] = \\max_i \\big( value[i] + dp[w - weight[i]] \\big)\n\\]\nUnlike 0/1 Knapsack (which must avoid reuse), this recurrence allows reuse because dp[w - weight[i]] is computed before dp[w] in the same pass, meaning item i can contribute multiple times.\nBy filling the array from 0 to W, every capacity’s best value is derived from optimal substructures of smaller capacities.\n\n\nTry It Yourself\n\nPrint chosen items by tracking predecessors.\nCompare with 0/1 Knapsack results.\nAdd constraint: each item ≤ k copies.\nApply to Rod Cutting: weight = length, value = price.\nExperiment with fractional weights (greedy fails here).\n\n\n\nTest Cases\n\n\n\nWeights\nValues\nW\nExpected\nNotes\n\n\n\n\n[2,3,4]\n[4,7,9]\n7\n16\n(3,4)\n\n\n[5,10,15]\n[10,30,50]\n20\n100\nfour 5s or two 10s\n\n\n[1,2,3]\n[10,15,40]\n6\n90\nsix 1s\n\n\n[2,5]\n[5,10]\n3\n5\none 2\n\n\n[1,3,4,5]\n[10,40,50,70]\n8\n160\nmultiples of 1 and 3\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times W)\\)\nSpace: \\(O(W)\\)\n\nThe Unbounded Knapsack captures the essence of reusable DP states, every step builds on smaller, self-similar subproblems, stacking value piece by piece until the capacity is full.\n\n\n\n417 Fractional Knapsack\nThe Fractional Knapsack problem is a close cousin of the 0/1 Knapsack, but with a twist. Here, you can break items into fractions, taking partial weight to maximize total value. This problem is not solved by DP; it’s a greedy algorithm, serving as a contrast to show where DP is not needed.\n\nWhat Problem Are We Solving?\nGiven:\n\nn items, each with weight[i] and value[i]\na knapsack with capacity W\n\nFind the maximum total value achievable by possibly taking fractions of items.\nWe define:\n\nValue density (ratio): \\[\n\\text{ratio}[i] = \\frac{value[i]}{weight[i]}\n\\]\n\nTo maximize value:\n\nSort items by decreasing ratio.\nTake full items until you can’t.\nTake a fraction of the next one to fill the remaining capacity.\n\nAnswer is the sum of selected (full + partial) values.\n\n\nHow Does It Work (Plain Language)\nIf each item can be split, the best approach is take the most valuable per unit weight first. It’s like filling your bag with gold dust, start with the richest dust, then move to less valuable kinds.\nExample: Items:\n\n\n\nItem\nValue\nWeight\nRatio\n\n\n\n\n1\n60\n10\n6.0\n\n\n2\n100\n20\n5.0\n\n\n3\n120\n30\n4.0\n\n\n\nCapacity = 50\n\nTake all of Item 1 → weight 10, value 60\nTake all of Item 2 → weight 20, value 100\nTake 20/30 = 2/3 of Item 3 → weight 20, value 80\n\nTotal = 60 + 100 + 80 = 240\n\n\nTiny Code (Easy Versions)\nC (Greedy Algorithm)\n#include &lt;stdio.h&gt;\n\nstruct Item {\n    int value, weight;\n};\n\nint compare(const void *a, const void *b) {\n    double r1 = (double)((struct Item *)a)-&gt;value / ((struct Item *)a)-&gt;weight;\n    double r2 = (double)((struct Item *)b)-&gt;value / ((struct Item *)b)-&gt;weight;\n    return (r1 &lt; r2) ? 1 : -1;\n}\n\nint main(void) {\n    int n, W;\n    printf(\"Enter number of items and capacity: \");\n    scanf(\"%d %d\", &n, &W);\n\n    struct Item arr[n];\n    printf(\"Enter value and weight:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        scanf(\"%d %d\", &arr[i].value, &arr[i].weight);\n\n    qsort(arr, n, sizeof(struct Item), compare);\n\n    double totalValue = 0.0;\n    int curWeight = 0;\n\n    for (int i = 0; i &lt; n; i++) {\n        if (curWeight + arr[i].weight &lt;= W) {\n            curWeight += arr[i].weight;\n            totalValue += arr[i].value;\n        } else {\n            int remain = W - curWeight;\n            totalValue += arr[i].value * ((double)remain / arr[i].weight);\n            break;\n        }\n    }\n\n    printf(\"Max value: %.2f\\n\", totalValue);\n    return 0;\n}\nPython (Greedy Implementation)\nitems = []\nn = int(input(\"Enter number of items: \"))\nW = int(input(\"Enter capacity: \"))\n\nfor _ in range(n):\n    v, w = map(int, input(\"Enter value and weight: \").split())\n    items.append((v, w, v / w))\n\nitems.sort(key=lambda x: x[2], reverse=True)\n\ntotal_value = 0.0\ncur_weight = 0\n\nfor v, w, r in items:\n    if cur_weight + w &lt;= W:\n        cur_weight += w\n        total_value += v\n    else:\n        remain = W - cur_weight\n        total_value += v * (remain / w)\n        break\n\nprint(\"Max value:\", round(total_value, 2))\n\n\nWhy It Matters\n\nDemonstrates where DP isn’t needed, a greedy choice property\nContrasts with 0/1 Knapsack (DP needed)\nBuilds intuition for ratio-based optimization\nAppears in resource allocation, finance, optimization\n\nThe Fractional Knapsack is the “continuous” version, you don’t choose yes or no, you pour the best parts until you run out of room.\n\n\nStep-by-Step Example\n\n\n\nItem\nValue\nWeight\nRatio\nTake\n\n\n\n\n1\n60\n10\n6.0\nFull\n\n\n2\n100\n20\n5.0\nFull\n\n\n3\n120\n30\n4.0\n2/3\n\n\n\nTotal = 240\n\n\nA Gentle Proof (Why It Works)\nIf all items can be divided arbitrarily, the optimal strategy is always to take the one with the highest value density first. Proof sketch:\n\nSuppose an optimal solution skips a higher-ratio item to take a lower-ratio one.\nReplacing part of the lower-ratio item with the higher-ratio one strictly increases total value.\nContradiction, thus, sorting by ratio is optimal.\n\nThis property is called the greedy choice property. Because the problem satisfies both optimal substructure and greedy choice, a greedy algorithm suffices.\n\n\nTry It Yourself\n\nCompare results with 0/1 Knapsack for same items.\nAdd more items with identical ratios.\nImplement sorting manually and test correctness.\nCheck behavior when capacity &lt; smallest weight.\nVisualize partial fill using ratio chart.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nValues\nWeights\nW\nExpected\nNotes\n\n\n\n\n[60,100,120]\n[10,20,30]\n50\n240\nclassic\n\n\n[10,5,15,7,6,18,3]\n[2,3,5,7,1,4,1]\n15\n55.33\ngreedy mix\n\n\n[25,50,75]\n[5,10,15]\n10\n50\nfull item\n\n\n[5,10,15]\n[1,2,3]\n3\n15\ntake all\n\n\n[1,2,3]\n[3,2,1]\n3\n5\nhighest ratio first\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log n)\\) (for sorting)\nSpace: \\(O(1)\\)\n\nThe Fractional Knapsack shows the power of greedy reasoning, sometimes, thinking locally optimal truly leads to the global best.\n\n\n\n418 Coin Change (Min Coins)\nThe Coin Change (Min Coins) problem is about finding the fewest number of coins needed to make a given amount. Unlike the counting version, which sums all combinations, this one focuses on minimization, the shortest path to the target sum.\nIt’s a classic unbounded DP problem, where each coin can be used multiple times.\n\nWhat Problem Are We Solving?\nGiven:\n\nA list of coins coins[]\nA target amount A\n\nFind the minimum number of coins needed to make amount A. If it’s impossible, return -1.\nWe define the state:\n\\[\ndp[x] = \\text{minimum coins required to make amount } x\n\\]\nRecurrence:\n\\[\ndp[x] = \\min_{c \\in coins,; c \\le x} \\big( dp[x - c] + 1 \\big)\n\\]\nBase:\n\\[\ndp[0] = 0\n\\]\nFinal answer:\n\\[\ndp[A]\n\\]\nIf no combination is possible, dp[A] will remain at infinity (or a large sentinel value).\n\n\nHow Does It Work (Plain Language)\nThink of building the amount step by step. For each value x, try all coins c that fit, and see which leads to the fewest total coins. Each state dp[x] represents the shortest chain from 0 to x.\nIt’s like climbing stairs to a target floor, each coin is a step size, and you want the path with the fewest steps.\nExample: coins = [1, 3, 4], A = 6\n\n\n\nAmount\ndp[x]\nChoice\n\n\n\n\n0\n0\nbase\n\n\n1\n1\n1\n\n\n2\n2\n1+1\n\n\n3\n1\n3\n\n\n4\n1\n4\n\n\n5\n2\n3+2\n\n\n6\n2\n3+3\n\n\n\nAnswer = 2 (3 + 3)\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n#define INF 1000000\n#define MIN(a,b) ((a) &lt; (b) ? (a) : (b))\n\nint main(void) {\n    int n, A;\n    printf(\"Enter number of coins and amount: \");\n    scanf(\"%d %d\", &n, &A);\n\n    int coins[n];\n    printf(\"Enter coin values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &coins[i]);\n\n    int dp[A + 1];\n    for (int i = 1; i &lt;= A; i++) dp[i] = INF;\n    dp[0] = 0;\n\n    for (int x = 1; x &lt;= A; x++) {\n        for (int i = 0; i &lt; n; i++) {\n            int c = coins[i];\n            if (x - c &gt;= 0)\n                dp[x] = MIN(dp[x], dp[x - c] + 1);\n        }\n    }\n\n    if (dp[A] == INF) printf(\"Not possible\\n\");\n    else printf(\"Min coins: %d\\n\", dp[A]);\n\n    return 0;\n}\nPython (Simple Iterative Version)\ncoins = list(map(int, input(\"Enter coin values: \").split()))\nA = int(input(\"Enter amount: \"))\n\nINF = float('inf')\ndp = [INF] * (A + 1)\ndp[0] = 0\n\nfor x in range(1, A + 1):\n    for c in coins:\n        if x - c &gt;= 0:\n            dp[x] = min(dp[x], dp[x - c] + 1)\n\nprint(\"Min coins:\" if dp[A] != INF else \"Not possible\", end=\" \")\nprint(dp[A] if dp[A] != INF else \"\")\n\n\nWhy It Matters\n\nClassic unbounded minimization DP\nCore of many resource optimization problems\nFoundation for graph shortest paths, minimum steps, edit operations\nContrasts with counting version (same recurrence, different aggregation)\n\nThis problem shows how min replaces sum in DP to shift from “how many” to “how few”.\n\n\nStep-by-Step Example\nCoins = [1, 3, 4], A = 6\n\n\n\nx\nChoices\ndp[x]\n\n\n\n\n0\n-\n0\n\n\n1\ndp[0]+1\n1\n\n\n2\ndp[1]+1\n2\n\n\n3\ndp[0]+1\n1\n\n\n4\ndp[0]+1\n1\n\n\n5\nmin(dp[2]+1, dp[1]+1)\n2\n\n\n6\nmin(dp[3]+1, dp[2]+1)\n2\n\n\n\nAnswer = 2 (3+3)\n\n\nA Gentle Proof (Why It Works)\nThe recurrence builds from smaller amounts upward. For each amount x, every coin c offers a path from x - c → x, adding 1 step.\nBy induction:\n\nBase case: dp[0] = 0 (no coins to make 0).\nInductive step: assume optimal solutions exist for all &lt; x. Then, the minimal value among all dp[x - c] + 1 is the fewest coins to form x.\n\nSince each x reuses optimal subsolutions, dp[A] is globally optimal.\n\n\nTry It Yourself\n\nPrint the chosen coins (trace back dp[x]).\nAdd a coin that never helps (e.g., [1, 3, 10], A=6).\nCompare with greedy for [1,3,4] (fails).\nExtend to limited coins (bounded knapsack).\nTry larger A to see performance.\n\n\n\nTest Cases\n\n\n\nCoins\nAmount\nExpected\nNotes\n\n\n\n\n[1,3,4]\n6\n2\n3+3\n\n\n[2,5]\n3\n-1\nnot possible\n\n\n[1,2,5]\n11\n3\n5+5+1\n\n\n[9,6,5,1]\n11\n2\n6+5\n\n\n[2,3,7]\n12\n3\n3+3+6\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times A)\\)\nSpace: \\(O(A)\\)\n\nThe Coin Change (Min Coins) problem is where unbounded DP meets optimization, building minimal paths to a target through simple, repeated decisions.\n\n\n\n419 Coin Change (Count Ways)\nThe Coin Change (Count Ways) problem is about how many different ways you can make a given amount using available coins. Unlike the minimization version, here every combination matters, order doesn’t.\nThis is a perfect example of unbounded combinatorial DP, where each coin can be used multiple times, but arrangement order is irrelevant.\n\nWhat Problem Are We Solving?\nGiven:\n\nA list of coins coins[]\nA target amount A\n\nFind the number of distinct combinations (unordered) that sum to A.\nWe define the state:\n\\[\ndp[x] = \\text{number of ways to make amount } x\n\\]\nRecurrence:\n\\[\ndp[x] = \\sum_{c \\in coins,; c \\le x} dp[x - c]\n\\]\nBase:\n\\[\ndp[0] = 1\n\\]\nTo avoid counting the same combination multiple times (e.g., [1,2] and [2,1]), we iterate coins first, then amount.\n\n\nHow Does It Work (Plain Language)\nWe count combinations, not permutations. That means {1,2} and {2,1} are considered the same way. So we fix each coin’s order, when processing a coin, we allow it to be reused, but not reordered with future coins.\nExample: coins = [1, 2, 5], A = 5\nWays:\n\n1+1+1+1+1\n1+1+1+2\n1+2+2\n5\n\nAnswer = 4\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n, A;\n    printf(\"Enter number of coins and amount: \");\n    scanf(\"%d %d\", &n, &A);\n\n    int coins[n];\n    printf(\"Enter coin values: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &coins[i]);\n\n    long long dp[A + 1];\n    for (int i = 0; i &lt;= A; i++) dp[i] = 0;\n    dp[0] = 1;\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int x = coins[i]; x &lt;= A; x++) {\n            dp[x] += dp[x - coins[i]];\n        }\n    }\n\n    printf(\"Number of ways: %lld\\n\", dp[A]);\n    return 0;\n}\nPython (Iterative Combinations)\ncoins = list(map(int, input(\"Enter coin values: \").split()))\nA = int(input(\"Enter amount: \"))\n\ndp = [0] * (A + 1)\ndp[0] = 1\n\nfor c in coins:\n    for x in range(c, A + 1):\n        dp[x] += dp[x - c]\n\nprint(\"Number of ways:\", dp[A])\n\n\nWhy It Matters\n\nFoundation of combinatorial DP\nBasis for partition counting, compositional sums, and probability DP\nReinforces loop ordering importance, changing order counts permutations instead\nConnects to integer partition problems in number theory\n\nIt teaches you that what you count (order vs combination) depends on how you iterate.\n\n\nStep-by-Step Example\nCoins = [1, 2, 5], A = 5\nInitialize dp = [1, 0, 0, 0, 0, 0]\n\n\n\nCoin\nState\ndp array (after processing)\n\n\n\n\n1\nall\n[1, 1, 1, 1, 1, 1]\n\n\n2\nadds combos using 2\n[1, 1, 2, 2, 3, 3]\n\n\n5\nadds combos using 5\n[1, 1, 2, 2, 3, 4]\n\n\n\nAnswer = 4\n\n\nA Gentle Proof (Why It Works)\nWe fill dp[x] by summing contributions from each coin c: every time we use coin c, we move from subproblem x - c → x.\n\\[\ndp[x] = \\sum_{c \\in coins} dp[x - c]\n\\]\nBut we must fix coin iteration order to ensure unique combinations. Iterating coins first ensures each combination is formed in a canonical order:\n\n1 before 2 before 5 So {1,2} appears once, not twice.\n\nBy induction:\n\nBase: dp[0] = 1 (one way: use nothing)\nStep: each dp[x] counts valid combos by extending smaller sums.\n\n\n\nTry It Yourself\n\nSwap loop order → count permutations.\nAdd coin 3 and compare growth.\nPrint all combinations via recursion.\nAdd modulo \\(10^9 + 7\\) for large results.\nCompare with Min Coins (same coins, different goal).\n\n\n\nTest Cases\n\n\n\nCoins\nAmount\nExpected\nNotes\n\n\n\n\n[1,2,5]\n5\n4\nclassic\n\n\n[2,3,5,6]\n10\n5\nmultiple combos\n\n\n[1]\n3\n1\nonly one way\n\n\n[2]\n3\n0\nimpossible\n\n\n[1,2,3]\n4\n4\n(1+1+1+1), (1+1+2), (2+2), (1+3)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times A)\\)\nSpace: \\(O(A)\\)\n\nThe Coin Change (Count Ways) problem captures the combinatorial heart of DP, a single recurrence, but with the magic of order-aware iteration, turns counting from chaos into clarity.\n\n\n\n420 Multi-Dimensional Knapsack\nThe Multi-Dimensional Knapsack problem (also called the Multi-Constraint Knapsack) extends the classic 0/1 Knapsack into a richer, more realistic world. Here, each item consumes multiple types of resources (weight, volume, cost, etc.), and we must respect all constraints simultaneously.\nIt’s where the simplicity of one dimension gives way to the complexity of many.\n\nWhat Problem Are We Solving?\nGiven:\n\nn items\nEach item i has:\n\nvalue v[i]\nweights in m dimensions w[i][1..m]\n\nCapacities C[1..m] for each dimension\n\nSelect a subset of items maximizing total value, subject to:\n\\[\n\\forall j \\in [1, m]: \\sum_{i \\in S} w[i][j] \\le C[j]\n\\]\nState definition:\n\\[\ndp[c_1][c_2] \\ldots [c_m] = \\text{maximum value achievable with capacities } (c_1, \\ldots, c_m)\n\\]\nRecurrence:\n\\[\ndp[\\vec{c}] = \\max \\big( dp[\\vec{c}],; dp[\\vec{c} - \\vec{w_i}] + v[i] \\big)\n\\]\nwhere \\(\\vec{c} - \\vec{w_i}\\) means subtracting all weights component-wise.\n\n\nHow Does It Work (Plain Language)\nIt’s like packing a spaceship with multiple limits, weight, volume, fuel usage, and every item drains each dimension differently. You can’t just fill to a single limit; every item’s cost affects all dimensions at once.\nThe DP grid is now multi-dimensional: you must iterate over every combination of capacities and decide to include or exclude each item.\nExample (2D case): Items:\n\n\n\nItem\nValue\nWeight\nVolume\n\n\n\n\n1\n60\n2\n3\n\n\n2\n100\n3\n4\n\n\n3\n120\n4\n5\n\n\n\nCapacity: (W=5, V=7)\nAnswer: 160 (Items 1 + 2)\n\n\nTiny Code (2D DP Example)\nC (2D Capacity, 0/1 Version)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\n\nint main(void) {\n    int n, W, V;\n    printf(\"Enter number of items, weight cap, volume cap: \");\n    scanf(\"%d %d %d\", &n, &W, &V);\n\n    int w[n], vol[n], val[n];\n    printf(\"Enter weight, volume, value:\\n\");\n    for (int i = 0; i &lt; n; i++)\n        scanf(\"%d %d %d\", &w[i], &vol[i], &val[i]);\n\n    int dp[W + 1][V + 1];\n    for (int i = 0; i &lt;= W; i++)\n        for (int j = 0; j &lt;= V; j++)\n            dp[i][j] = 0;\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int wi = W; wi &gt;= w[i]; wi--) {\n            for (int vi = V; vi &gt;= vol[i]; vi--) {\n                dp[wi][vi] = MAX(dp[wi][vi],\n                                 dp[wi - w[i]][vi - vol[i]] + val[i]);\n            }\n        }\n    }\n\n    printf(\"Max value: %d\\n\", dp[W][V]);\n    return 0;\n}\nPython (2D DP)\nitems = [(2,3,60), (3,4,100), (4,5,120)]\nW, V = 5, 7\n\ndp = [[0]*(V+1) for _ in range(W+1)]\n\nfor w, v, val in items:\n    for wi in range(W, w-1, -1):\n        for vi in range(V, v-1, -1):\n            dp[wi][vi] = max(dp[wi][vi], dp[wi-w][vi-v] + val)\n\nprint(\"Max value:\", dp[W][V])\n\n\nWhy It Matters\n\nModels real-world constraints, multiple resources\nCore of operations research, resource allocation, logistics, multi-resource scheduling\nIllustrates how DP dimensionality grows with complexity\nForces careful state design and iteration order\n\nWhen one dimension is not enough, this generalization captures tradeoffs across many.\n\n\nStep-by-Step Example (2D)\nCapacity: W=5, V=7 Items:\n\n(2,3,60)\n(3,4,100)\n(4,5,120)\n\nWe explore subsets:\n\n{1} → (2,3), value=60\n{2} → (3,4), value=100\n{3} → (4,5), value=120\n{1,2} → (5,7), value=160 ✅ optimal\n{1,3} → (6,8) ❌ exceeds\n{2,3} → (7,9) ❌ exceeds\n\nAnswer = 160\n\n\nA Gentle Proof (Why It Works)\nBy induction on item index and capacities:\nLet \\(dp[i][c_1][c_2] \\ldots [c_m]\\) be the best value using first i items and capacity vector \\((c_1, c_2, \\ldots, c_m)\\).\nTwo choices for each item:\n\nExclude → keep \\(dp[i-1][\\vec{c}]\\)\nInclude → \\(dp[i-1][\\vec{c} - \\vec{w_i}] + v[i]\\) (if feasible)\n\nTake the max.\nSince all transitions only depend on smaller capacities, and each subproblem is optimal, overall DP converges to global optimum.\n\n\nTry It Yourself\n\nAdd third dimension (e.g., “time”).\nCompare with greedy (fails).\nVisualize DP table for 2D.\nTrack chosen items with traceback.\nAdd unbounded variation (re-use items).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nItems (W,V,Val)\nCapacity (W,V)\nExpected\nNotes\n\n\n\n\n[(2,3,60),(3,4,100),(4,5,120)]\n(5,7)\n160\n(1+2)\n\n\n[(1,2,10),(2,3,20),(3,3,40)]\n(3,4)\n40\nsingle best\n\n\n[(2,2,8),(2,3,9),(3,4,14)]\n(4,5)\n17\n(1+2)\n\n\n[(3,2,10),(2,4,12),(4,3,18)]\n(5,6)\n22\n(1+2)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times C_1 \\times C_2 \\times \\ldots \\times C_m)\\)\nSpace: \\(O(C_1 \\times C_2 \\times \\ldots \\times C_m)\\)\n\nThe Multi-Dimensional Knapsack is a reminder that every extra resource adds a new axis to your reasoning, and your DP table.\n\n\n\n\nSection 43. Sequence Problems\n\n421 Longest Increasing Subsequence (O(n^2) DP)\nThe Longest Increasing Subsequence (LIS) problem asks for the maximum length of a subsequence that is strictly increasing. Elements do not need to be contiguous, only in order.\n\nWhat Problem Are We Solving?\nGiven an array arr[0..n-1], find the maximum k such that there exist indices 0 ≤ i1 &lt; i2 &lt; ... &lt; ik &lt; n with \\[\narr[i_1] &lt; arr[i_2] &lt; \\cdots &lt; arr[i_k].\n\\]\nDefine the state \\[\ndp[i] = \\text{length of the LIS that ends at index } i.\n\\]\nRecurrence \\[\ndp[i] = 1 + \\max_{;0 \\le j &lt; i,; arr[j] &lt; arr[i]} dp[j], \\quad \\text{with } dp[i] \\leftarrow 1 \\text{ if no such } j.\n\\]\nAnswer \\[\n\\max_{0 \\le i &lt; n} dp[i].\n\\]\n\n\nHow Does It Work (Plain Language)\nFor each position i, look back at all earlier positions j &lt; i with a smaller value. Any increasing subsequence ending at j can be extended by arr[i]. Pick the best among them and add one. If nothing is smaller, start a new subsequence of length 1 at i.\nExample: arr = [10, 22, 9, 33, 21, 50, 41, 60]\n\n\n\ni\narr[i]\ndp[i]\nexplanation\n\n\n\n\n0\n10\n1\nstart\n\n\n1\n22\n2\n10 → 22\n\n\n2\n9\n1\nrestart at 9\n\n\n3\n33\n3\n10 → 22 → 33\n\n\n4\n21\n2\n10 → 21\n\n\n5\n50\n4\n10 → 22 → 33 → 50\n\n\n6\n41\n4\n10 → 22 → 33 → 41\n\n\n7\n60\n5\n10 → 22 → 33 → 50 → 60\n\n\n\nAnswer is 5.\n\n\nTiny Code (Easy Versions)\nC (O(n^2))\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n    int arr[n];\n    printf(\"Enter array: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &arr[i]);\n\n    int dp[n];\n    for (int i = 0; i &lt; n; i++) dp[i] = 1;\n\n    int ans = 0;\n    for (int i = 0; i &lt; n; i++) {\n        for (int j = 0; j &lt; i; j++) {\n            if (arr[j] &lt; arr[i]) dp[i] = MAX(dp[i], dp[j] + 1);\n        }\n        ans = MAX(ans, dp[i]);\n    }\n    printf(\"LIS length: %d\\n\", ans);\n    return 0;\n}\nPython (O(n^2))\narr = list(map(int, input(\"Enter array: \").split()))\nn = len(arr)\ndp = [1] * n\nfor i in range(n):\n    for j in range(i):\n        if arr[j] &lt; arr[i]:\n            dp[i] = max(dp[i], dp[j] + 1)\nprint(\"LIS length:\", max(dp) if dp else 0)\n\n\nWhy It Matters\n\nArchetypal sequence DP: define state on prefixes and extend with a choice.\nFoundation for LCS, Edit Distance, patience sorting LIS in O(n log n), and 2D variants like Russian Doll Envelopes.\nUseful in ranking, time series smoothing, and chain scheduling.\n\n\n\nStep by Step Example\narr = [3, 10, 2, 1, 20]\n\n\n\n\n\n\n\n\n\ni\narr[i]\ncandidates (dp[j]+1) with (arr[j] &lt; arr[i])\ndp[i]\n\n\n\n\n0\n3\n,\n1\n\n\n1\n10\ndp[0]+1 = 2\n2\n\n\n2\n2\n,\n1\n\n\n3\n1\n,\n1\n\n\n4\n20\nmax( dp[0]+1=2, dp[1]+1=3, dp[2]+1=2, dp[3]+1=2 )\n3\n\n\n\nAnswer is 3.\n\n\nA Gentle Proof (Why It Works)\nLet OPT(i) denote the LIS length that ends exactly at index i. Any LIS ending at i must either be just [arr[i]] of length 1, or extend a strictly smaller element at some j &lt; i. Therefore \\[\nOPT(i) = \\max\\left( 1,; 1 + \\max_{j&lt;i,;arr[j]&lt;arr[i]} OPT(j) \\right).\n\\] This depends only on optimal values from smaller indices, hence dynamic programming applies. The overall LIS is the maximum over all end positions: \\[\n\\text{LIS} = \\max_i OPT(i).\n\\] By induction on i, the recurrence computes OPT(i) correctly, so the final maximum is optimal.\n\n\nTry It Yourself\n\nRecover an actual LIS: keep a parent[i] pointing to the j that gave the best transition.\nChange to nondecreasing subsequence: replace &lt; with &lt;=.\nCompare with the O(n log n) patience sorting method and verify both lengths match.\nCompute the number of LIS of maximum length using a parallel cnt[i].\nExtend to 2D pairs (a,b) by sorting on a and running LIS on b with careful tie handling.\n\n\n\nTest Cases\n\n\n\narr\nexpected\n\n\n\n\n[1,2,3,4,5]\n5\n\n\n[5,4,3,2,1]\n1\n\n\n[3,10,2,1,20]\n3\n\n\n[10,22,9,33,21,50,41,60]\n5\n\n\n[2,2,2,2]\n1\n\n\n\n\n\nComplexity\n\nTime: (O(n^2))\nSpace: (O(n))\n\nThis O(n^2) DP is the clearest path to LIS: define the end, look back to smaller ends, and grow the longest chain.\n\n\n\n422 LIS (Patience Sorting) – O(n log n) Optimized\nThe Longest Increasing Subsequence (LIS) can be solved faster than the classic (O(n^2)) DP by using a clever idea inspired by patience sorting. Instead of building all sequences, we maintain a minimal tail array, each element represents the smallest possible tail of an increasing subsequence of a given length.\n\nWhat Problem Are We Solving?\nGiven an array arr[0..n-1], find the length of the longest strictly increasing subsequence in O(n log n) time.\nWe want \\[\n\\text{LIS length} = \\max k \\text{ such that } \\exists i_1 &lt; i_2 &lt; \\cdots &lt; i_k,; arr[i_1] &lt; arr[i_2] &lt; \\cdots &lt; arr[i_k].\n\\]\n\n\nKey Idea\nMaintain an array tails[] where tails[len] = smallest tail value of any increasing subsequence of length len+1.\nFor each element x in the array:\n\nUse binary search in tails to find the first position pos with tails[pos] ≥ x.\nReplace tails[pos] with x (we found a better tail).\nIf x is larger than all tails, append it, subsequence grows.\n\nAt the end, len(tails) = LIS length.\n\n\nHow Does It Work (Plain Language)\nThink of placing numbers onto piles (like patience solitaire):\n\nEach pile’s top is the smallest possible number ending an increasing subsequence of that length.\nWhen a new number comes, place it on the leftmost pile whose top is ≥ the number.\nIf none exists, start a new pile.\n\nThe number of piles equals the LIS length.\nExample: arr = [10, 22, 9, 33, 21, 50, 41, 60]\nProcess:\n\n\n\nx\ntails (after processing x)\n\n\n\n\n10\n[10]\n\n\n22\n[10, 22]\n\n\n9\n[9, 22]\n\n\n33\n[9, 22, 33]\n\n\n21\n[9, 21, 33]\n\n\n50\n[9, 21, 33, 50]\n\n\n41\n[9, 21, 33, 41]\n\n\n60\n[9, 21, 33, 41, 60]\n\n\n\nAnswer = 5\n\n\nTiny Code (Easy Versions)\nC (Using Binary Search)\n#include &lt;stdio.h&gt;\n\nint lower_bound(int arr[], int len, int x) {\n    int l = 0, r = len;\n    while (l &lt; r) {\n        int mid = (l + r) / 2;\n        if (arr[mid] &lt; x) l = mid + 1;\n        else r = mid;\n    }\n    return l;\n}\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n\n    int a[n];\n    printf(\"Enter array: \");\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &a[i]);\n\n    int tails[n], len = 0;\n\n    for (int i = 0; i &lt; n; i++) {\n        int pos = lower_bound(tails, len, a[i]);\n        tails[pos] = a[i];\n        if (pos == len) len++;\n    }\n\n    printf(\"LIS length: %d\\n\", len);\n    return 0;\n}\nPython (Using bisect)\nimport bisect\n\narr = list(map(int, input(\"Enter array: \").split()))\ntails = []\n\nfor x in arr:\n    pos = bisect.bisect_left(tails, x)\n    if pos == len(tails):\n        tails.append(x)\n    else:\n        tails[pos] = x\n\nprint(\"LIS length:\", len(tails))\n\n\nWhy It Matters\n\nReduces LIS from \\(O(n^2)\\) to \\(O(n \\log n)\\)\nIntroduces binary search in DP transitions\nDemonstrates state compression, we track only tails, not all subproblems\nServes as basis for LIS reconstruction, LDS, Longest Bitonic Subsequence, and 2D LIS\n\nThis technique shows how mathematical insight can collapse a DP table into a minimal structure.\n\n\nStep-by-Step Example\narr = [3, 10, 2, 1, 20]\n\n\n\nx\ntails\n\n\n\n\n3\n[3]\n\n\n10\n[3,10]\n\n\n2\n[2,10]\n\n\n1\n[1,10]\n\n\n20\n[1,10,20]\n\n\n\nAnswer = 3\n\n\nA Gentle Proof (Why It Works)\nInvariant:\n\ntails[k] = minimal possible tail of any increasing subsequence of length k+1.\n\nWhen we place x:\n\nReplacing a tail keeps subsequences valid (shorter or equal tail → more chance to extend).\nAppending x grows the length by one.\n\nBy induction:\n\nEach tails[k] is nondecreasing with length.\nFinal size of tails equals the LIS length, because every pile represents a distinct subsequence length.\n\n\n\nTry It Yourself\n\nTrack predecessors to reconstruct one LIS.\nModify to nondecreasing subsequence with bisect_right.\nCompare counts with (O(n^2)) version.\nVisualize piles after each insertion.\nUse on 2D sorted pairs (a,b) for envelope problems.\n\n\n\nTest Cases\n\n\n\narr\nExpected\nNotes\n\n\n\n\n[10,22,9,33,21,50,41,60]\n5\nclassic\n\n\n[3,10,2,1,20]\n3\n{3,10,20}\n\n\n[1,2,3,4,5]\n5\nalready increasing\n\n\n[5,4,3,2,1]\n1\ndecreasing\n\n\n[2,2,2,2]\n1\nconstant\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log n)\\) (binary search per element)\nSpace: \\(O(n)\\)\n\nThe Patience Sorting LIS turns a quadratic DP into a sleek logarithmic method, a masterclass in trading space for insight.\n\n\n\n423 Longest Common Subsequence (LCS)\nThe Longest Common Subsequence (LCS) problem finds the longest sequence that appears in the same relative order (not necessarily contiguous) in both strings. It is one of the most fundamental two-sequence DPs, and the basis of algorithms like diff, edit distance, and DNA alignment.\n\nWhat Problem Are We Solving?\nGiven two sequences \\[\nX = x_1, x_2, \\dots, x_m,\\quad Y = y_1, y_2, \\dots, y_n\n\\] find the length of the longest sequence that is a subsequence of both.\nDefine the state:\n\\[\ndp[i][j] = \\text{LCS length of prefixes } X[0..i-1],, Y[0..j-1]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\n0, & \\text{if } i = 0 \\text{ or } j = 0,\\\\\ndp[i-1][j-1] + 1, & \\text{if } x_{i-1} = y_{j-1},\\\\\n\\max\\big(dp[i-1][j],\\, dp[i][j-1]\\big), & \\text{if } x_{i-1} \\ne y_{j-1}.\n\\end{cases}\n\\]\nAnswer:\n\\[\ndp[m][n]\n\\]\n\n\nHow Does It Work (Plain Language)\nWe build a grid where each cell dp[i][j] represents the LCS of the first i characters of X and the first j characters of Y.\n\nIf the characters match, extend the subsequence diagonally.\nIf not, skip one character (either from X or Y) and take the better result.\n\nThink of it as aligning the two strings, step by step, and keeping the longest matching order.\nExample: X = \"ABCBDAB\", Y = \"BDCAB\"\nThe longest common subsequence is \"BCAB\", length 4.\n\n\nTiny Code (Easy Versions)\nC (Classic 2D DP)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\n\nint main(void) {\n    char X[100], Y[100];\n    printf(\"Enter first string: \");\n    scanf(\"%s\", X);\n    printf(\"Enter second string: \");\n    scanf(\"%s\", Y);\n\n    int m = 0, n = 0;\n    while (X[m]) m++;\n    while (Y[n]) n++;\n\n    int dp[m + 1][n + 1];\n    for (int i = 0; i &lt;= m; i++)\n        for (int j = 0; j &lt;= n; j++)\n            dp[i][j] = 0;\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            if (X[i - 1] == Y[j - 1])\n                dp[i][j] = dp[i - 1][j - 1] + 1;\n            else\n                dp[i][j] = MAX(dp[i - 1][j], dp[i][j - 1]);\n        }\n    }\n\n    printf(\"LCS length: %d\\n\", dp[m][n]);\n    return 0;\n}\nPython (2D DP)\nX = input(\"Enter first string: \")\nY = input(\"Enter second string: \")\n\nm, n = len(X), len(Y)\ndp = [[0]*(n+1) for _ in range(m+1)]\n\nfor i in range(1, m+1):\n    for j in range(1, n+1):\n        if X[i-1] == Y[j-1]:\n            dp[i][j] = dp[i-1][j-1] + 1\n        else:\n            dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n\nprint(\"LCS length:\", dp[m][n])\n\n\nWhy It Matters\n\nClassic two-dimensional DP template\nCore for Edit Distance, Sequence Alignment, Diff tools\nDemonstrates subproblem reuse via overlapping prefixes\nHelps understand table-filling and backtracking reconstruction\n\nLCS is where dynamic programming meets string similarity.\n\n\nStep-by-Step Example\nX = \"ABCBDAB\", Y = \"BDCAB\"\n\n\n\ni\nj\nX[i-1], Y[j-1]\ndp[i][j]\nExplanation\n\n\n\n\n1\n1\nA, B\n0\nmismatch\n\n\n2\n1\nB, B\n1\nmatch\n\n\n3\n2\nC, D\n1\ncarry max\n\n\n4\n3\nB, C\n1\ncarry max\n\n\n5\n4\nD, A\n2\nlater match\n\n\n7\n5\nB, B\n4\nfull subsequence\n\n\n\nAnswer = 4 (\"BCAB\")\n\n\nA Gentle Proof (Why It Works)\nBy induction on i and j:\n\nBase: \\(dp[0][j] = dp[i][0] = 0\\) (empty prefix)\nIf \\(x_{i-1} = y_{j-1}\\), every common subsequence of X[0..i-2] and Y[0..j-2] can be extended by this match.\nIf not equal, longest subsequence must exclude one character, hence max of left and top cells.\n\nSince each subproblem depends only on smaller prefixes, filling the table row by row ensures all dependencies are ready.\n\n\nTry It Yourself\n\nReconstruct the actual LCS (store direction or traceback).\nModify to handle case-insensitive matches.\nCompare with Edit Distance formula.\nVisualize table diagonal matches.\nUse it to find diff between two lines of text.\n\n\n\nTest Cases\n\n\n\nX\nY\nExpected\nNotes\n\n\n\n\n“ABCBDAB”\n“BDCAB”\n4\n“BCAB”\n\n\n“AGGTAB”\n“GXTXAYB”\n4\n“GTAB”\n\n\n“AAAA”\n“AA”\n2\nsubset\n\n\n“ABC”\n“DEF”\n0\nnone\n\n\n“”\n“ABC”\n0\nbase case\n\n\n\n\n\nComplexity\n\nTime: \\(O(m \\times n)\\)\nSpace: \\(O(m \\times n)\\), or \\(O(\\min(m,n))\\) with rolling arrays\n\nThe Longest Common Subsequence teaches you to align two worlds, character by character, building similarity from shared order, not proximity.\n\n\n\n424 Edit Distance (Levenshtein)\nThe Edit Distance (or Levenshtein Distance) problem measures how different two strings are by counting the minimum number of operations needed to transform one into the other. The allowed operations are usually insert, delete, and replace.\nThis is one of the most elegant two-dimensional DPs, it captures transformation cost between sequences step by step.\n\nWhat Problem Are We Solving?\nGiven two strings \\[\nX = x_1, x_2, \\dots, x_m,\\quad Y = y_1, y_2, \\dots, y_n\n\\] find the minimum number of operations to convert X into Y, using:\n\nInsert a character\nDelete a character\nReplace a character\n\nWe define the state:\n\\[\ndp[i][j] = \\text{minimum edits to convert } X[0..i-1] \\text{ into } Y[0..j-1]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\ni, & \\text{if } j = 0,\\\\\nj, & \\text{if } i = 0,\\\\\ndp[i-1][j-1], & \\text{if } x_{i-1} = y_{j-1},\\\\\n1 + \\min\\big(dp[i-1][j],\\ dp[i][j-1],\\ dp[i-1][j-1]\\big), & \\text{if } x_{i-1} \\ne y_{j-1}.\n\\end{cases}\n\\]\n\n\\(dp[i-1][j] + 1\\) → delete\n\n\\(dp[i][j-1] + 1\\) → insert\n\n\\(dp[i-1][j-1] + 1\\) → replace\n\nAnswer:\n\\[\ndp[m][n]\n\\]\n\n\nHow Does It Work (Plain Language)\nWe build a 2D grid comparing prefixes of both strings.\nEach cell answers: “What’s the cheapest way to make X[:i] look like Y[:j]?”\n\nIf characters match, carry over the diagonal value.\nIf they differ, take the smallest cost among inserting, deleting, or replacing.\n\nThink of typing corrections: every operation moves you closer to the target.\nExample: X = \"kitten\", Y = \"sitting\" Operations:\n\nReplace k → s\nReplace e → i\nInsert g\n\nAnswer = 3\n\n\nTiny Code (Easy Versions)\nC (2D DP Table)\n#include &lt;stdio.h&gt;\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\nint min3(int a, int b, int c) {\n    int m = (a &lt; b) ? a : b;\n    return (m &lt; c) ? m : c;\n}\n\nint main(void) {\n    char X[100], Y[100];\n    printf(\"Enter first string: \");\n    scanf(\"%s\", X);\n    printf(\"Enter second string: \");\n    scanf(\"%s\", Y);\n\n    int m = 0, n = 0;\n    while (X[m]) m++;\n    while (Y[n]) n++;\n\n    int dp[m + 1][n + 1];\n\n    for (int i = 0; i &lt;= m; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= n; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            if (X[i-1] == Y[j-1])\n                dp[i][j] = dp[i-1][j-1];\n            else\n                dp[i][j] = 1 + min3(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]);\n        }\n    }\n\n    printf(\"Edit distance: %d\\n\", dp[m][n]);\n    return 0;\n}\nPython (Compact Version)\nX = input(\"Enter first string: \")\nY = input(\"Enter second string: \")\n\nm, n = len(X), len(Y)\ndp = [[0]*(n+1) for _ in range(m+1)]\n\nfor i in range(m+1):\n    dp[i][0] = i\nfor j in range(n+1):\n    dp[0][j] = j\n\nfor i in range(1, m+1):\n    for j in range(1, n+1):\n        if X[i-1] == Y[j-1]:\n            dp[i][j] = dp[i-1][j-1]\n        else:\n            dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n\nprint(\"Edit distance:\", dp[m][n])\n\n\nWhy It Matters\n\nFoundation for spell checking, DNA alignment, fuzzy matching, and diff tools\nDemonstrates multi-option recurrence (3 choices per state)\nBasis for weighted edit distances (cost per operation)\nShows how to encode sequence transformation into DP\n\nIt’s one of the cleanest examples where DP reveals the shortest transformation path.\n\n\nStep-by-Step Example\nX = \"kitten\", Y = \"sitting\"\n\n\n\ni\nj\nX[:i]\nY[:j]\ndp[i][j]\nExplanation\n\n\n\n\n0\n0\n“”\n“”\n0\nbase\n\n\n1\n1\n“k”\n“s”\n1\nreplace\n\n\n2\n2\n“ki”\n“si”\n1\ncarry\n\n\n3\n3\n“kit”\n“sit”\n1\ncarry\n\n\n4\n4\n“kitt”\n“sitt”\n1\ncarry\n\n\n5\n5\n“kitte”\n“sitti”\n2\nreplace\n\n\n6\n6\n“kitten”\n“sittin”\n2\nreplace\n\n\n6\n7\n“kitten”\n“sitting”\n3\ninsert\n\n\n\nAnswer = 3\n\n\nA Gentle Proof (Why It Works)\nFor each prefix pair (i, j):\n\nIf last chars match: no new cost, inherit dp[i-1][j-1].\nElse:\n\nDelete X[i-1] → dp[i-1][j] + 1\nInsert Y[j-1] → dp[i][j-1] + 1\nReplace X[i-1] with Y[j-1] → dp[i-1][j-1] + 1\n\n\nWe choose the minimal option. By induction on (i, j), every dp[i][j] is optimal, since it uses optimal subsolutions from smaller prefixes.\n\n\nTry It Yourself\n\nPrint the sequence of operations (traceback from dp[m][n]).\nChange costs: make replace = 2, others = 1.\nCompare with LCS: EditDistance = m + n - 2 × LCS.\nHandle insert/delete only (turn it into LCS variant).\nTry with words like \"intention\" → \"execution\".\n\n\n\nTest Cases\n\n\n\nX\nY\nExpected\nNotes\n\n\n\n\n“kitten”\n“sitting”\n3\nreplace, replace, insert\n\n\n“flaw”\n“lawn”\n2\nreplace, insert\n\n\n“abc”\n“abc”\n0\nsame\n\n\n“abc”\n“yabd”\n2\nreplace, insert\n\n\n“”\n“abc”\n3\ninserts\n\n\n\n\n\nComplexity\n\nTime: \\(O(m \\times n)\\)\nSpace: \\(O(m \\times n)\\), or \\(O(\\min(m,n))\\) with rolling rows\n\nThe Edit Distance captures the very essence of transformation, how to reshape one structure into another, one careful operation at a time.\n\n\n\n425 Longest Palindromic Subsequence\nThe Longest Palindromic Subsequence (LPS) problem finds the longest sequence that reads the same forward and backward, not necessarily contiguous. It’s a classic two-dimensional DP, and a mirror image of the Longest Common Subsequence (LCS), but here, we compare a string with its reverse.\n\nWhat Problem Are We Solving?\nGiven a string \\[\nS = s_1, s_2, \\dots, s_n\n\\] find the length of the longest subsequence that is a palindrome.\nDefine the state:\n\\[\ndp[i][j] = \\text{LPS length in substring } S[i..j]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\n1, & \\text{if } i = j,\\\\\n2 + dp[i+1][j-1], & \\text{if } s_i = s_j,\\\\\n\\max\\big(dp[i+1][j],\\, dp[i][j-1]\\big), & \\text{if } s_i \\ne s_j.\n\\end{cases}\n\\]\nBase case: \\[\ndp[i][i] = 1\n\\]\nFinal answer: \\[\ndp[0][n-1]\n\\]\n\n\nHow Does It Work (Plain Language)\nWe expand outward between two indices i and j:\n\nIf the characters match, they can wrap a smaller palindrome inside.\nIf not, skip one character (either start or end) and try again.\n\nThink of it as folding the string onto itself, one matching pair at a time.\nExample: S = \"bbbab\"\nLPS = \"bbbb\" (length 4)\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up 2D DP)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\n\nint main(void) {\n    char S[100];\n    printf(\"Enter string: \");\n    scanf(\"%s\", S);\n\n    int n = 0;\n    while (S[n]) n++;\n\n    int dp[n][n];\n    for (int i = 0; i &lt; n; i++) dp[i][i] = 1;\n\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 0; i &lt;= n - len; i++) {\n            int j = i + len - 1;\n            if (S[i] == S[j] && len == 2)\n                dp[i][j] = 2;\n            else if (S[i] == S[j])\n                dp[i][j] = 2 + dp[i+1][j-1];\n            else\n                dp[i][j] = MAX(dp[i+1][j], dp[i][j-1]);\n        }\n    }\n\n    printf(\"LPS length: %d\\n\", dp[0][n-1]);\n    return 0;\n}\nPython (Clean DP Version)\nS = input(\"Enter string: \")\nn = len(S)\ndp = [[0]*n for _ in range(n)]\n\nfor i in range(n):\n    dp[i][i] = 1\n\nfor length in range(2, n+1):\n    for i in range(n - length + 1):\n        j = i + length - 1\n        if S[i] == S[j]:\n            dp[i][j] = 2 + (dp[i+1][j-1] if length &gt; 2 else 0)\n        else:\n            dp[i][j] = max(dp[i+1][j], dp[i][j-1])\n\nprint(\"LPS length:\", dp[0][n-1])\n\n\nWhy It Matters\n\nCore DP on substrings (interval DP)\nConnects to LCS: [ (S) = (S, (S))]\nFoundation for Palindrome Partitioning, String Reconstruction, and DNA symmetry problems\nTeaches two-pointer DP intuition\n\nThe LPS shows how symmetry and substructure intertwine.\n\n\nStep-by-Step Example\nS = \"bbbab\"\n\n\n\ni\nj\nS[i..j]\ndp[i][j]\nReason\n\n\n\n\n0\n0\nb\n1\nsingle char\n\n\n1\n1\nb\n1\nsingle char\n\n\n2\n2\nb\n1\nsingle char\n\n\n3\n3\na\n1\nsingle char\n\n\n4\n4\nb\n1\nsingle char\n\n\n2\n4\n“bab”\n3\nb + a + b\n\n\n1\n4\n“bbab”\n3\nwrap b’s\n\n\n0\n4\n“bbbab”\n4\nb + (bb a b) + b\n\n\n\nAnswer = 4 (\"bbbb\")\n\n\nA Gentle Proof (Why It Works)\nFor substring S[i..j]:\n\nIf s_i == s_j: both ends can contribute to a longer palindrome, add 2 around dp[i+1][j-1].\nIf s_i != s_j: one of them can’t be in the palindrome, skip either i or j and take max.\n\nBy filling increasing substring lengths, every subproblem is solved before it’s needed.\n\n\nTry It Yourself\n\nReconstruct one longest palindrome using backtracking.\nCompare with LCS(S, reverse(S)) result.\nTry on \"cbbd\", \"agbdba\".\nModify to count number of distinct palindromic subsequences.\nVisualize table diagonals (bottom-up growth).\n\n\n\nTest Cases\n\n\n\nS\nExpected\nNotes\n\n\n\n\n“bbbab”\n4\n“bbbb”\n\n\n“cbbd”\n2\n“bb”\n\n\n“agbdba”\n5\n“abdba”\n\n\n“abcd”\n1\nany single char\n\n\n“aaa”\n3\nwhole string\n\n\n\n\n\nComplexity\n\nTime: (O(n^2))\nSpace: (O(n^2)), reducible with rolling arrays\n\nThe Longest Palindromic Subsequence is a mirror held up to your string, revealing the symmetry hidden within.\n\n\n\n426 Shortest Common Supersequence (SCS)\nThe Shortest Common Supersequence (SCS) problem asks for the shortest string that contains both given strings as subsequences. It’s like merging two sequences together without breaking order, balancing overlap and inclusion.\nThis problem is a close companion to LCS, in fact, its length can be directly expressed in terms of the Longest Common Subsequence.\n\nWhat Problem Are We Solving?\nGiven two strings \\[\nX = x_1, x_2, \\dots, x_m,\\quad Y = y_1, y_2, \\dots, y_n\n\\] find the length of the shortest string that contains both as subsequences.\nDefine the state:\n\\[\ndp[i][j] = \\text{length of SCS of prefixes } X[0..i-1] \\text{ and } Y[0..j-1]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\ni, & \\text{if } j = 0,\\\\\nj, & \\text{if } i = 0,\\\\\n1 + dp[i-1][j-1], & \\text{if } x_{i-1} = y_{j-1},\\\\\n1 + \\min\\big(dp[i-1][j],\\ dp[i][j-1]\\big), & \\text{if } x_{i-1} \\ne y_{j-1}.\n\\end{cases}\n\\]\nAnswer: \\[\ndp[m][n]\n\\]\nAlternate formula: \\[\n\\text{SCS length} = m + n - \\text{LCS length}\n\\]\n\n\nHow Does It Work (Plain Language)\nIf two characters match, you include it once and move diagonally. If they differ, include one character and move toward the smaller subproblem (skipping one side). You’re building the shortest merged string preserving both orders.\nThink of it as stitching the two sequences together with minimal redundancy.\nExample: X = \"AGGTAB\", Y = \"GXTXAYB\"\nLCS = \"GTAB\" (length 4)\nSo: \\[\n\\text{SCS length} = 6 + 7 - 4 = 9\n\\]\nSCS = \"AGXGTXAYB\"\n\n\nTiny Code (Easy Versions)\nC (DP Table)\n#include &lt;stdio.h&gt;\n#define MIN(a,b) ((a) &lt; (b) ? (a) : (b))\n\nint main(void) {\n    char X[100], Y[100];\n    printf(\"Enter first string: \");\n    scanf(\"%s\", X);\n    printf(\"Enter second string: \");\n    scanf(\"%s\", Y);\n\n    int m = 0, n = 0;\n    while (X[m]) m++;\n    while (Y[n]) n++;\n\n    int dp[m + 1][n + 1];\n    for (int i = 0; i &lt;= m; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= n; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            if (X[i-1] == Y[j-1])\n                dp[i][j] = 1 + dp[i-1][j-1];\n            else\n                dp[i][j] = 1 + MIN(dp[i-1][j], dp[i][j-1]);\n        }\n    }\n\n    printf(\"SCS length: %d\\n\", dp[m][n]);\n    return 0;\n}\nPython (Straightforward DP)\nX = input(\"Enter first string: \")\nY = input(\"Enter second string: \")\n\nm, n = len(X), len(Y)\ndp = [[0]*(n+1) for _ in range(m+1)]\n\nfor i in range(m+1):\n    dp[i][0] = i\nfor j in range(n+1):\n    dp[0][j] = j\n\nfor i in range(1, m+1):\n    for j in range(1, n+1):\n        if X[i-1] == Y[j-1]:\n            dp[i][j] = 1 + dp[i-1][j-1]\n        else:\n            dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1])\n\nprint(\"SCS length:\", dp[m][n])\n\n\nWhy It Matters\n\nShows merging sequences with order preservation\nTies directly to LCS via the formula\nUseful for file merging, version control, and sequence alignment\nDemonstrates minimal superstructure over two DPs\n\nIt’s the “union” counterpart to the “intersection” of LCS.\n\n\nStep-by-Step Example\nX = \"AGGTAB\", Y = \"GXTXAYB\"\n\n\n\nStep\nAction\nResult\n\n\n\n\nCompare A,G\ndiffer\nchoose A\n\n\nCompare G,G\nmatch\nadd G\n\n\nCompare G,X\ndiffer\nadd X\n\n\nCompare G,T\ndiffer\nadd T\n\n\nCompare T,X\ndiffer\nadd X\n\n\nCompare A,A\nmatch\nadd A\n\n\nCompare B,B\nmatch\nadd B\n\n\n\nSCS = \"AGXGTXAYB\"\nLength = 9\n\n\nA Gentle Proof (Why It Works)\nEvery SCS must include all characters of both strings in order.\n\nIf last chars match: append once → 1 + dp[i-1][j-1]\nElse, shortest option comes from skipping one character from either string.\n\nBy induction on (i, j), since subproblems solve strictly smaller prefixes, we get optimal length.\nThe equivalence \\[\n|SCS| = m + n - |LCS|\n\\] follows because overlapping LCS chars are counted twice when summing lengths, and must be subtracted once.\n\n\nTry It Yourself\n\nReconstruct actual SCS string (traceback from dp[m][n]).\nVerify |SCS| = |X| + |Y| - |LCS|.\nCompare SCS vs concatenation X + Y.\nApply to sequences with no overlap.\nTest with identical strings (SCS = same string).\n\n\n\nTest Cases\n\n\n\nX\nY\nExpected Length\nNotes\n\n\n\n\n“AGGTAB”\n“GXTXAYB”\n9\noverlap GTAB\n\n\n“ABCBDAB”\n“BDCAB”\n9\nshares BCAB\n\n\n“HELLO”\n“GEEK”\n8\nno big overlap\n\n\n“AB”\n“AB”\n2\nidentical\n\n\n“AB”\n“CD”\n4\ndisjoint\n\n\n\n\n\nComplexity\n\nTime: \\(O(m \\times n)\\)\nSpace: \\(O(m \\times n)\\), or \\(O(\\min(m,n))\\) for length only\n\nThe Shortest Common Supersequence weaves two strings into one, the tightest possible thread that holds both stories together.\n\n\n\n427 Longest Repeated Subsequence\nThe Longest Repeated Subsequence (LRS) of a string is the longest subsequence that appears at least twice in the string without reusing the same index position. It is like LCS of a string with itself, with an extra constraint to avoid matching a character with itself.\n\nWhat Problem Are We Solving?\nGiven a string \\[\nS = s_1, s_2, \\dots, s_n\n\\] find the length of the longest subsequence that occurs at least twice in (S) with disjoint index positions.\nDefine the state by comparing the string with itself:\n\\[\ndp[i][j] = \\text{LRS length for } S[1..i] \\text{ and } S[1..j]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\n0, & \\text{if } i = 0 \\text{ or } j = 0,\\\\\n1 + dp[i-1][j-1], & \\text{if } s_i = s_j \\text{ and } i \\ne j,\\\\\n\\max\\big(dp[i-1][j],\\, dp[i][j-1]\\big), & \\text{otherwise.}\n\\end{cases}\n\\]\nAnswer: \\[\ndp[n][n]\n\\]\nThe key difference from LCS is the constraint \\(i \\ne j\\) to prevent matching the same occurrence of a character.\n\n\nHow Does It Work (Plain Language)\nThink of aligning the string with itself. You are looking for common subsequences, but you are not allowed to match a character to its identical position. When characters match at different positions, you extend the repeated subsequence. When they do not match or are at the same position, you take the best from skipping one side.\nExample: S = \"aabebcdd\" One LRS is \"abd\" with length 3.\n\n\nTiny Code (Easy Versions)\nC (2D DP)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\n\nint main(void) {\n    char S[1005];\n    printf(\"Enter string: \");\n    scanf(\"%s\", S);\n\n    int n = 0; while (S[n]) n++;\n\n    int dp[n + 1][n + 1];\n    for (int i = 0; i &lt;= n; i++)\n        for (int j = 0; j &lt;= n; j++)\n            dp[i][j] = 0;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            if (S[i-1] == S[j-1] && i != j)\n                dp[i][j] = 1 + dp[i-1][j-1];\n            else\n                dp[i][j] = MAX(dp[i-1][j], dp[i][j-1]);\n        }\n    }\n\n    printf(\"LRS length: %d\\n\", dp[n][n]);\n    return 0;\n}\nPython (Straightforward DP)\nS = input(\"Enter string: \")\nn = len(S)\ndp = [[0]*(n+1) for _ in range(n+1)]\n\nfor i in range(1, n+1):\n    for j in range(1, n+1):\n        if S[i-1] == S[j-1] and i != j:\n            dp[i][j] = 1 + dp[i-1][j-1]\n        else:\n            dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n\nprint(\"LRS length:\", dp[n][n])\n\n\nWhy It Matters\n\nIllustrates transforming a problem into LCS on the same string with a simple constraint.\nUseful for detecting repeated patterns and compression signals.\nBuilds intuition for self-alignment DPs and index constraints.\n\n\n\nStep by Step Example\nS = \"aabebcdd\" Let us look at the alignment idea:\n\nMatching pairs at different indices:\n\na at positions 1 and 2 can contribute, but not with the same index.\nb at positions 3 and 6.\nd at positions 7 and 8.\n\n\nA valid repeated subsequence is \"abd\" using indices (1,3,7) and (2,6,8). Length (= 3).\n\n\nA Gentle Proof (Why It Works)\nConsider LCS of (S) with itself:\n\nIf you allowed matches at the same indices, you would trivially match every character with itself and get (n).\nBy forbidding matches where (i = j), any character contributes only when there exists another occurrence at a different index.\nThe recurrence mirrors LCS but enforces (i j).\nBy induction on (i, j), the table accumulates exactly the lengths of repeated subsequences, and the maximum at (dp[n][n]) is the LRS length.\n\n\n\nTry It Yourself\n\nReconstruct one LRS by tracing back from (dp[n][n]) while respecting (i j).\nModify to count the number of distinct LRS of maximum length.\nCompare LRS and LPS on the same string to see structural differences.\nHandle ties when reconstructing to get the lexicographically smallest LRS.\nTest behavior on strings with all unique characters.\n\n\n\nTest Cases\n\n\n\nS\nExpected LRS length\nOne LRS\n\n\n\n\n“aabebcdd”\n3\n“abd”\n\n\n“axxxy”\n2\n“xx”\n\n\n“aaaa”\n3\n“aaa”\n\n\n“abc”\n0\n“”\n\n\n“aaba”\n2\n“aa”\n\n\n\n\n\nComplexity\n\nTime: (O(n^2))\nSpace: (O(n^2))\n\nThe Longest Repeated Subsequence is LCS turned inward. Compare the string with itself, forbid identical positions, and the repeated pattern reveals itself.\n\n\n\n428 String Interleaving\nThe String Interleaving problem asks whether a string \\(S\\) can be formed by interleaving (or weaving together) two other strings \\(X\\) and \\(Y\\) while preserving the relative order of characters from each.\nIt’s a dynamic programming problem that elegantly captures sequence merging under order constraints, similar in spirit to merging two sorted lists.\n\nWhat Problem Are We Solving?\nGiven three strings \\(X\\), \\(Y\\), and \\(S\\), determine if \\(S\\) is a valid interleaving of \\(X\\) and \\(Y\\).\nWe define the state:\n\\[\ndp[i][j] = \\text{True if } S[0..i+j-1] \\text{ can be formed by interleaving } X[0..i-1] \\text{ and } Y[0..j-1]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\begin{cases}\ndp[i-1][j], & \\text{if } X[i-1] = S[i+j-1] \\text{ and } dp[i-1][j],\\\\\ndp[i][j-1], & \\text{if } Y[j-1] = S[i+j-1] \\text{ and } dp[i][j-1],\\\\\ndp[i-1][j] \\lor dp[i][j-1], & \\text{if both conditions hold.}\n\\end{cases}\n\\]\nBase conditions:\n\\[\ndp[0][0] = \\text{True}\n\\]\n\\[\ndp[i][0] = dp[i-1][0] \\land (X[i-1] = S[i-1])\n\\]\n\\[\ndp[0][j] = dp[0][j-1] \\land (Y[j-1] = S[j-1])\n\\]\nAnswer:\n\\[\ndp[m][n]\n\\]\nwhere \\(m = |X|\\), \\(n = |Y|\\).\n\n\nHow Does It Work (Plain Language)\nYou have two input strings \\(X\\) and \\(Y\\), and you’re asked whether you can merge them in order to get \\(S\\).\nEach step, decide whether the next character in \\(S\\) should come from \\(X\\) or \\(Y\\), as long as you don’t break the order within either.\nImagine reading from two ribbons of characters, you can switch between them but never rearrange within a ribbon.\nExample: \\(X = \\text{\"abc\"}\\), \\(Y = \\text{\"def\"}\\), \\(S = \\text{\"adbcef\"}\\)\nValid interleaving: \\(a\\) (from \\(X\\)), \\(d\\) (from \\(Y\\)), \\(b\\) (from \\(X\\)), \\(c\\) (from \\(X\\)), \\(e\\) (from \\(Y\\)), \\(f\\) (from \\(Y\\))\n\n\nTiny Code (Easy Versions)\nC (2D Boolean DP)\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;string.h&gt;\n\nbool isInterleave(char *X, char *Y, char *S) {\n    int m = strlen(X), n = strlen(Y);\n    if (m + n != strlen(S)) return false;\n\n    bool dp[m+1][n+1];\n    dp[0][0] = true;\n\n    for (int i = 1; i &lt;= m; i++)\n        dp[i][0] = dp[i-1][0] && X[i-1] == S[i-1];\n\n    for (int j = 1; j &lt;= n; j++)\n        dp[0][j] = dp[0][j-1] && Y[j-1] == S[j-1];\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            dp[i][j] = false;\n            if (X[i-1] == S[i+j-1]) dp[i][j] |= dp[i-1][j];\n            if (Y[j-1] == S[i+j-1]) dp[i][j] |= dp[i][j-1];\n        }\n    }\n\n    return dp[m][n];\n}\n\nint main(void) {\n    char X[100], Y[100], S[200];\n    printf(\"Enter X: \"); scanf(\"%s\", X);\n    printf(\"Enter Y: \"); scanf(\"%s\", Y);\n    printf(\"Enter S: \"); scanf(\"%s\", S);\n\n    if (isInterleave(X, Y, S))\n        printf(\"Yes, S is an interleaving of X and Y.\\n\");\n    else\n        printf(\"No, S cannot be formed.\\n\");\n\n    return 0;\n}\nPython (Simple DP Table)\nX = input(\"Enter X: \")\nY = input(\"Enter Y: \")\nS = input(\"Enter S: \")\n\nm, n = len(X), len(Y)\nif len(S) != m + n:\n    print(\"No\")\nelse:\n    dp = [[False]*(n+1) for _ in range(m+1)]\n    dp[0][0] = True\n\n    for i in range(1, m+1):\n        dp[i][0] = dp[i-1][0] and X[i-1] == S[i-1]\n    for j in range(1, n+1):\n        dp[0][j] = dp[0][j-1] and Y[j-1] == S[j-1]\n\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            dp[i][j] = (X[i-1] == S[i+j-1] and dp[i-1][j]) or \\\n                       (Y[j-1] == S[i+j-1] and dp[i][j-1])\n\n    print(\"Yes\" if dp[m][n] else \"No\")\n\n\nWhy It Matters\n\nDemonstrates two-sequence merging under order constraints\nCore idea behind path interleaving, merge scheduling, and string weaving\nGood stepping stone for problems involving 2D grid DPs and string constraints\n\n\n\nStep-by-Step Example\n\\(X = \\text{\"abc\"}\\), \\(Y = \\text{\"def\"}\\), \\(S = \\text{\"adbcef\"}\\)\n\n\n\ni\nj\nX[:i]\nY[:j]\ndp[i][j]\nExplanation\n\n\n\n\n0\n0\n“”\n“”\nT\nbase\n\n\n1\n0\na\n“”\nT\na from X\n\n\n1\n1\na\nd\nT\nd from Y\n\n\n2\n1\nab\nd\nT\nb from X\n\n\n3\n1\nabc\nd\nF\ncannot match next\n\n\n3\n2\nabc\nde\nT\ne from Y\n\n\n3\n3\nabc\ndef\nT\nf from Y\n\n\n\nAnswer: True\n\n\nA Gentle Proof (Why It Works)\nAt any point, we have used \\(i + j\\) characters from \\(S\\):\n\nIf last came from \\(X\\): \\(X[i-1] = S[i+j-1]\\) and \\(dp[i-1][j]\\) was True\nIf last came from \\(Y\\): \\(Y[j-1] = S[i+j-1]\\) and \\(dp[i][j-1]\\) was True\n\nBy filling the table left-to-right, top-to-bottom, every prefix is validated before combining. Inductive reasoning ensures correctness for all prefixes.\n\n\nTry It Yourself\n\nPrint one valid interleaving path\nModify to count total interleavings\nHandle strings with duplicate characters carefully\nTry on examples where \\(X\\) and \\(Y\\) share common prefixes\nExtend to three strings interleaving\n\n\n\nTest Cases\n\n\n\nX\nY\nS\nExpected\n\n\n\n\n“abc”\n“def”\n“adbcef”\nTrue\n\n\n“ab”\n“cd”\n“abcd”\nTrue\n\n\n“ab”\n“cd”\n“acbd”\nTrue\n\n\n“ab”\n“cd”\n“acdb”\nFalse\n\n\n“aa”\n“ab”\n“aaba”\nTrue\n\n\n\n\n\nComplexity\n\nTime: \\(O(m \\times n)\\)\nSpace: \\(O(m \\times n)\\), can be reduced to \\(O(n)\\)\n\nThe String Interleaving problem is about harmony, weaving two sequences together, letter by letter, in perfect order.\n\n\n\n429 Sequence Alignment (Bioinformatics)\nThe Sequence Alignment problem asks how to best align two sequences (often DNA, RNA, or proteins) to measure their similarity, allowing for gaps and mismatches. It forms the foundation of bioinformatics, string similarity, and edit-based scoring systems.\nUnlike edit distance, sequence alignment assigns scores for matches, mismatches, and gaps, and seeks a maximum score, not a minimal edit count.\n\nWhat Problem Are We Solving?\nGiven two sequences \\[\nX = x_1, x_2, \\dots, x_m, \\quad Y = y_1, y_2, \\dots, y_n\n\\] and scoring rules:\n\n\\(+1\\) for a match\n\\(-1\\) for a mismatch\n\\(-2\\) for a gap (insertion/deletion)\n\nwe want to find an alignment of \\(X\\) and \\(Y\\) that maximizes the total score.\nWe define the state:\n\\[\ndp[i][j] = \\text{maximum alignment score between } X[0..i-1] \\text{ and } Y[0..j-1]\n\\]\nRecurrence:\n\\[\ndp[i][j] =\n\\max\n\\begin{cases}\ndp[i-1][j-1] + \\text{score}(x_{i-1}, y_{j-1}) \\\ndp[i-1][j] + \\text{gap penalty} \\\ndp[i][j-1] + \\text{gap penalty}\n\\end{cases}\n\\]\nBase:\n\\[\ndp[i][0] = i \\times \\text{gap penalty}, \\quad dp[0][j] = j \\times \\text{gap penalty}\n\\]\nAnswer: \\(dp[m][n]\\)\n\n\nHow Does It Work (Plain Language)\nWe fill a grid where each cell \\((i, j)\\) represents the best score to align the first \\(i\\) characters of \\(X\\) with the first \\(j\\) of \\(Y\\).\nAt each step, we decide:\n\nMatch/Mismatch (\\(x_{i-1}\\) with \\(y_{j-1}\\))\nInsert a gap in \\(Y\\) (skip character in \\(X\\))\nInsert a gap in \\(X\\) (skip character in \\(Y\\))\n\nThe final cell holds the optimal alignment score. Tracing back reveals the aligned strings, with dashes representing gaps.\nExample:\n\\(X = \\text{\"GATTACA\"}\\) \\(Y = \\text{\"GCATGCU\"}\\)\nOne alignment:\nG A T T A C A -\n| |   |   | |\nG - C A T G C U\n\n\nTiny Code (Easy Versions)\nC (Global Alignment / Needleman–Wunsch)\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\n\nint score(char a, char b) {\n    return a == b ? 1 : -1;\n}\n\nint main(void) {\n    char X[100], Y[100];\n    printf(\"Enter X: \"); scanf(\"%s\", X);\n    printf(\"Enter Y: \"); scanf(\"%s\", Y);\n\n    int m = 0, n = 0;\n    while (X[m]) m++;\n    while (Y[n]) n++;\n\n    int gap = -2;\n    int dp[m+1][n+1];\n\n    for (int i = 0; i &lt;= m; i++) dp[i][0] = i * gap;\n    for (int j = 0; j &lt;= n; j++) dp[0][j] = j * gap;\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            int match = dp[i-1][j-1] + score(X[i-1], Y[j-1]);\n            int delete = dp[i-1][j] + gap;\n            int insert = dp[i][j-1] + gap;\n            dp[i][j] = MAX(match, MAX(delete, insert));\n        }\n    }\n\n    printf(\"Max alignment score: %d\\n\", dp[m][n]);\n    return 0;\n}\nPython (Clean Version)\nX = input(\"Enter X: \")\nY = input(\"Enter Y: \")\n\nm, n = len(X), len(Y)\nmatch, mismatch, gap = 1, -1, -2\n\ndp = [[0]*(n+1) for _ in range(m+1)]\n\nfor i in range(1, m+1):\n    dp[i][0] = i * gap\nfor j in range(1, n+1):\n    dp[0][j] = j * gap\n\nfor i in range(1, m+1):\n    for j in range(1, n+1):\n        score = match if X[i-1] == Y[j-1] else mismatch\n        dp[i][j] = max(\n            dp[i-1][j-1] + score,\n            dp[i-1][j] + gap,\n            dp[i][j-1] + gap\n        )\n\nprint(\"Max alignment score:\", dp[m][n])\n\n\nWhy It Matters\n\nFoundational in bioinformatics (DNA, RNA, protein comparison)\nUsed in spell correction, plagiarism detection, text similarity\nShows a weighted DP with scores, not just counts\nDemonstrates path reconstruction with multiple decisions\n\nThis is the generalization of edit distance to scored alignment.\n\n\nStep-by-Step Example\nLet \\(X = \\text{\"AGT\"}\\), \\(Y = \\text{\"GTT\"}\\) Match \\(= +1\\), Mismatch \\(= -1\\), Gap \\(= -2\\)\n\n\n\ni\nj\n\\(X[0..i]\\)\n\\(Y[0..j]\\)\n\\(dp[i][j]\\)\nChoice\n\n\n\n\n1\n1\nA, G\nG\n-1\nmismatch\n\n\n2\n2\nAG, GT\nGT\n+0\nalign G\n\n\n3\n3\nAGT, GTT\nGTT\n+1\nalign T\n\n\n\nAnswer = +1\n\n\nA Gentle Proof (Why It Works)\nEach \\(dp[i][j]\\) represents the best possible score achievable aligning \\(X[0..i-1]\\) and \\(Y[0..j-1]\\). Induction ensures correctness:\n\nBase: \\(dp[0][j], dp[i][0]\\) handle leading gaps\nStep: At each \\((i, j)\\), you consider all valid transitions, match/mismatch, insert, delete, and take the max. Thus \\(dp[m][n]\\) is globally optimal.\n\n\n\nTry It Yourself\n\nTrace back to print alignment with gaps\nTry different scoring systems\nCompare global (Needleman–Wunsch) vs local (Smith–Waterman) alignment\nHandle affine gaps (gap opening + extension)\nVisualize grid paths as alignments\n\n\n\nTest Cases\n\n\n\nX\nY\nExpected\nNotes\n\n\n\n\n“AGT”\n“GTT”\n1\none match\n\n\n“GATTACA”\n“GCATGCU”\ndepends on scoring\nclassic\n\n\n“ABC”\n“ABC”\n3\nall match\n\n\n“ABC”\n“DEF”\n-3\nall mismatch\n\n\n“A”\n“AAA”\n-2\ngaps added\n\n\n\n\n\nComplexity\n\nTime: \\(O(m \\times n)\\)\nSpace: \\(O(m \\times n)\\) (can be reduced with row-rolling)\n\nThe Sequence Alignment problem teaches that similarity is not just about matches, it’s about balancing alignments, mismatches, and gaps to find the best correspondence between two sequences.\n\n\n\n430 Diff Algorithm (Myers / DP)\nThe Diff Algorithm compares two sequences and finds their shortest edit script (SES), the minimal sequence of insertions and deletions required to transform one into the other. It’s the heart of tools like git diff and diff, providing human-readable change summaries.\nThe Myers Algorithm is the most famous linear-space implementation, but the DP formulation builds on edit distance and LCS intuition.\n\nWhat Problem Are We Solving?\nGiven two strings \\[\nX = x_1, x_2, \\dots, x_m, \\quad Y = y_1, y_2, \\dots, y_n\n\\] find a minimal sequence of edits (insertions and deletions) to turn \\(X\\) into \\(Y\\).\nEach edit transforms one sequence toward the other, and matching characters are left untouched.\nThe minimal number of edits equals:\n\\[\n\\text{SES length} = m + n - 2 \\times \\text{LCS length}\n\\]\nWe can also explicitly trace the path to recover the diff.\n\n\nRecurrence (DP Formulation)\nLet \\(dp[i][j]\\) be the minimal number of edits to convert \\(X[0..i-1]\\) into \\(Y[0..j-1]\\).\nThen:\n\\[\ndp[i][j] =\n\\begin{cases}\ni, & \\text{if } j = 0,\\\\\nj, & \\text{if } i = 0,\\\\\ndp[i-1][j-1], & \\text{if } x_{i-1} = y_{j-1},\\\\\n1 + \\min\\big(dp[i-1][j],\\, dp[i][j-1]\\big), & \\text{if } x_{i-1} \\ne y_{j-1}.\n\\end{cases}\n\\]\nAnswer: \\[\ndp[m][n]\n\\]\nThe traceback reconstructs the sequence of operations:\nkeeping, deleting, or inserting characters to transform \\(X\\) into \\(Y\\).\n\n\nHow Does It Work (Plain Language)\nImagine aligning two sequences line by line. When characters match, move diagonally (no cost). If they differ, you must either delete from \\(X\\) or insert from \\(Y\\).\nBy walking through a grid of all prefix pairs, you can find the shortest edit path, the same logic as git diff.\nExample: \\(X = \\text{\"ABCABBA\"}\\) \\(Y = \\text{\"CBABAC\"}\\)\nOne minimal diff:\n- A\n  B\n  C\n+ B\n  A\n  B\n- B\n+ A\n  C\n\n\nTiny Code (Easy Versions)\nC (DP Traceback for Diff)\n#include &lt;stdio.h&gt;\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\nint main(void) {\n    char X[100], Y[100];\n    printf(\"Enter X: \"); scanf(\"%s\", X);\n    printf(\"Enter Y: \"); scanf(\"%s\", Y);\n\n    int m = 0, n = 0;\n    while (X[m]) m++;\n    while (Y[n]) n++;\n\n    int dp[m+1][n+1];\n    for (int i = 0; i &lt;= m; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= n; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= m; i++) {\n        for (int j = 1; j &lt;= n; j++) {\n            if (X[i-1] == Y[j-1])\n                dp[i][j] = dp[i-1][j-1];\n            else\n                dp[i][j] = 1 + MIN(dp[i-1][j], dp[i][j-1]);\n        }\n    }\n\n    printf(\"Edit distance: %d\\n\", dp[m][n]);\n\n    // Traceback\n    int i = m, j = n;\n    printf(\"Diff:\\n\");\n    while (i &gt; 0 || j &gt; 0) {\n        if (i &gt; 0 && j &gt; 0 && X[i-1] == Y[j-1]) {\n            printf(\"  %c\\n\", X[i-1]);\n            i--; j--;\n        } else if (i &gt; 0 && dp[i][j] == dp[i-1][j] + 1) {\n            printf(\"- %c\\n\", X[i-1]);\n            i--;\n        } else {\n            printf(\"+ %c\\n\", Y[j-1]);\n            j--;\n        }\n    }\n\n    return 0;\n}\nPython (Simple Diff Reconstruction)\nX = input(\"Enter X: \")\nY = input(\"Enter Y: \")\n\nm, n = len(X), len(Y)\ndp = [[0]*(n+1) for _ in range(m+1)]\n\nfor i in range(m+1):\n    dp[i][0] = i\nfor j in range(n+1):\n    dp[0][j] = j\n\nfor i in range(1, m+1):\n    for j in range(1, n+1):\n        if X[i-1] == Y[j-1]:\n            dp[i][j] = dp[i-1][j-1]\n        else:\n            dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1])\n\ni, j = m, n\nops = []\nwhile i &gt; 0 or j &gt; 0:\n    if i &gt; 0 and j &gt; 0 and X[i-1] == Y[j-1]:\n        ops.append(f\"  {X[i-1]}\")\n        i -= 1; j -= 1\n    elif i &gt; 0 and dp[i][j] == dp[i-1][j] + 1:\n        ops.append(f\"- {X[i-1]}\")\n        i -= 1\n    else:\n        ops.append(f\"+ {Y[j-1]}\")\n        j -= 1\n\nprint(\"Edit distance:\", dp[m][n])\nprint(\"Diff:\")\nfor op in reversed(ops):\n    print(op)\n\n\nWhy It Matters\n\nFoundation of version control systems (git diff, patch)\nMinimizes edit operations for transformation\nSimplifies merge conflict resolution\nBuilds upon LCS and Edit Distance concepts\nDemonstrates traceback-based reconstruction\n\nThe diff is the human-readable face of dynamic programming, turning tables into insight.\n\n\nStep-by-Step Example\n\\(X = \\text{\"ABCABBA\"}\\), \\(Y = \\text{\"CBABAC\"}\\)\n\n\n\nOperation\nExplanation\n\n\n\n\n- A\nX starts with ‘A’ not in Y prefix\n\n\nB\nmatch\n\n\nC\nmatch\n\n\n+ B\ninsert\n\n\nA\nmatch\n\n\nB\nmatch\n\n\n- B\ndelete\n\n\n+ A\ninsert\n\n\nC\nmatch\n\n\n\nTotal edits = 4 (minimal)\n\n\nA Gentle Proof (Why It Works)\n\nIf \\(x_{i-1} = y_{j-1}\\), skip both (no cost).\nOtherwise, best path must add a new operation (insert/delete).\n\\(dp[i][j]\\) stores minimal edits for prefixes. By induction, \\(dp[m][n]\\) is the minimal SES length.\n\n\n\nTry It Yourself\n\nPrint diff symbols (+, -, space) in alignment view\nCompare diff path with LCS path\nTry with real words: \"kitten\" → \"sitting\"\nModify cost: insertion/deletion weights\nUse colors or indentation to visualize output\n\n\n\nTest Cases\n\n\n\nX\nY\nEdits\nNotes\n\n\n\n\n“ABCABBA”\n“CBABAC”\n4\nclassic example\n\n\n“abc”\n“abc”\n0\nidentical\n\n\n“abc”\n“def”\n6\nall replaced\n\n\n“kitten”\n“sitting”\n3\nclassic\n\n\n“”\n“xyz”\n3\nall inserts\n\n\n\n\n\nComplexity\n\nTime: \\(O(m \\times n)\\)\nSpace: \\(O(m \\times n)\\) (traceback needs full table)\n\nThe Diff Algorithm transforms comparison into storytelling, showing precisely how one sequence evolves into another, one edit at a time.\n\n\n\n\nSection 44. Matrix and Chain Problems\n\n431 Matrix Chain Multiplication\nThe Matrix Chain Multiplication problem asks for the most efficient way to parenthesize a product of matrices so that the total number of scalar multiplications is minimized. Matrix multiplication is associative, so the order of multiplication can change the cost dramatically even though the result is the same shape.\n\nWhat Problem Are We Solving?\nGiven a chain of matrices \\[\nA_1A_2\\cdots A_n\n\\] with dimensions \\[\nA_i \\text{ is } p_{i-1}\\times p_i \\quad (i=1..n),\n\\] choose the parenthesization that minimizes scalar multiplications.\nDefine the state:\n\\[\ndp[i][j] = \\text{minimum cost to multiply } A_iA_{i+1}\\cdots A_j\n\\]\nRecurrence:\n\\[\ndp[i][i] = 0\n\\] \\[\ndp[i][j] = \\min_{i\\le k&lt;j},\\bigl(dp[i][k] + dp[k+1][j] + p_{i-1}p_kp_j\\bigr)\n\\]\nThe last term is the cost of multiplying the two resulting matrices from the split at (k).\nAnswer:\n\\[\ndp[1][n]\n\\]\nOptionally keep a split table (split[i][j]) storing the (k) achieving the minimum to reconstruct the optimal parenthesization.\n\n\nHow Does It Work (Plain Language)\nMatrix-chain multiplication DP\nRecurrence \\[\n\\begin{aligned}\nm[i,i] &= 0,\\\\\nm[i,j] &= \\min_{i \\le k &lt; j}\\Big(m[i,k] + m[k+1,j] + p_{i-1}\\,p_k\\,p_j\\Big)\\qquad (i&lt;j),\n\\end{aligned}\n\\] where matrices are \\(A_i\\) of size \\(p_{i-1}\\times p_i\\).\nExample with dimensions \\[\np=[10,\\,30,\\,5,\\,60],\\quad\nA_1:10\\times30,\\ A_2:30\\times5,\\ A_3:5\\times60.\n\\]\nTwo ways to parenthesize:\n\n\\((A_1A_2)A_3\\)\nCost \\[\n(10\\cdot 30\\cdot 5) + (10\\cdot 5\\cdot 60)\n= 1500 + 3000 = 4500.\n\\]\n\\(A_1(A_2A_3)\\)\nCost \\[\n(30\\cdot 5\\cdot 60) + (10\\cdot 30\\cdot 60)\n= 9000 + 18000 = 27000.\n\\]\n\nMinimum cost is \\(4500\\), achieved by \\((A_1A_2)A_3\\) with split \\(k=1\\).\n\n\nTiny Code (Easy Versions)\nC (DP with reconstruction)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\nvoid print_optimal(int i, int j, int split[105][105]) {\n    if (i == j) { printf(\"A%d\", i); return; }\n    printf(\"(\");\n    int k = split[i][j];\n    print_optimal(i, k, split);\n    printf(\" x \");\n    print_optimal(k+1, j, split);\n    printf(\")\");\n}\n\nint main(void) {\n    int n;\n    printf(\"Enter number of matrices: \");\n    scanf(\"%d\", &n);\n    int p[n+1];\n    printf(\"Enter dimensions p0..pn: \");\n    for (int i = 0; i &lt;= n; i++) scanf(\"%d\", &p[i]);\n\n    long long dp[105][105];\n    int split[105][105];\n\n    for (int i = 1; i &lt;= n; i++) dp[i][i] = 0;\n\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 1; i + len - 1 &lt;= n; i++) {\n            int j = i + len - 1;\n            dp[i][j] = LLONG_MAX;\n            for (int k = i; k &lt; j; k++) {\n                long long cost = dp[i][k] + dp[k+1][j] + 1LL*p[i-1]*p[k]*p[j];\n                if (cost &lt; dp[i][j]) {\n                    dp[i][j] = cost;\n                    split[i][j] = k;\n                }\n            }\n        }\n    }\n\n    printf(\"Min scalar multiplications: %lld\\n\", dp[1][n]);\n    printf(\"Optimal parenthesization: \");\n    print_optimal(1, n, split);\n    printf(\"\\n\");\n    return 0;\n}\nPython (DP with reconstruction)\ndef matrix_chain_order(p):\n    n = len(p) - 1\n    dp = [[0]*(n+1) for _ in range(n+1)]\n    split = [[0]*(n+1) for _ in range(n+1)]\n\n    for length in range(2, n+1):\n        for i in range(1, n - length + 2):\n            j = i + length - 1\n            dp[i][j] = float('inf')\n            for k in range(i, j):\n                cost = dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]\n                if cost &lt; dp[i][j]:\n                    dp[i][j] = cost\n                    split[i][j] = k\n    return dp, split\n\ndef build_solution(split, i, j):\n    if i == j:\n        return f\"A{i}\"\n    k = split[i][j]\n    return \"(\" + build_solution(split, i, k) + \" x \" + build_solution(split, k+1, j) + \")\"\n\np = list(map(int, input(\"Enter p0..pn: \").split()))\ndp, split = matrix_chain_order(p)\nn = len(p) - 1\nprint(\"Min scalar multiplications:\", dp[1][n])\nprint(\"Optimal parenthesization:\", build_solution(split, 1, n))\n\n\nWhy It Matters\n\nCanonical example of interval DP and optimal binary partitioning\nShows how associativity allows many evaluation orders with different costs\nAppears in query plan optimization, automatic differentiation scheduling, graphics and compiler optimization\n\n\n\nStep by Step Example\nFor (p = [5, 10, 3, 12, 5, 50, 6]) with (n=6):\n\nTry all splits for each subchain length\nThe DP eventually yields (dp[1][6] = 2010) and an optimal structure like (((A_1(A_2A_3))((A_4A_5)A_6))) Exact parentheses can vary among ties but the minimal cost is unique here.\n\n\n\nA Gentle Proof (Why It Works)\nLet (OPT(i,j)) be the optimal cost for \\(A_i\\cdots A_j\\). In any optimal solution, the last multiplication splits the chain at some (k) with \\(i\\le k&lt;j\\). The two sides must themselves be optimal, otherwise replacing one side by a better solution improves the total, contradicting optimality. Therefore \\[\nOPT(i,j) = \\min_{i\\le k&lt;j}\\bigl(OPT(i,k)+OPT(k+1,j)+p_{i-1}p_kp_j\\bigr),\n\\] with (OPT(i,i)=0). Since each subproblem uses strictly shorter chains, filling by increasing length computes all needed values before they are used.\n\n\nTry It Yourself\n\nPrint not only one but all optimal parenthesizations when multiple (k) tie.\nAdd a second objective like minimizing depth after minimizing cost.\nCompare greedy choices vs DP on random instances.\nExtend to a cost model with addition cost or cache reuse.\nVisualize the DP table and splits along diagonals.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\np (p0..pn)\nn\nExpected min cost\nOne optimal parentheses\n\n\n\n\n[10,30,5,60]\n3\n4500\n(A1 x A2) x A3\n\n\n[5,10,3,12,5,50,6]\n6\n2010\none optimal structure\n\n\n[40,20,30,10,30]\n4\n26000\n((A1 x (A2 x A3)) x A4) or tie variant\n\n\n[10,20,30]\n2\n6000\nA1 x A2\n\n\n[2,3,4,5]\n3\n64\n(A1 x A2) x A3\n\n\n\n\n\nComplexity\n\nTime: (O(n^3)) for the triple loop over (i,j,k)\nSpace: (O(n^2)) for the DP and split tables\n\nMatrix Chain Multiplication is the textbook pattern for interval DP: pick a split, combine optimal subchains, and account for the boundary multiplication cost.\n\n\n\n432 Boolean Parenthesization\nThe Boolean Parenthesization problem (also called the Boolean Expression Evaluation problem) asks: Given a boolean expression consisting of T (true), F (false), and operators (&, |, ^), how many ways can we parenthesize it so that it evaluates to True?\nIt’s a classic DP over intervals problem where we explore all possible splits between operators, combining sub-results based on logic rules.\n\nWhat Problem Are We Solving?\nGiven a boolean expression string like T|F&T^T, count the number of ways to parenthesize it so that it evaluates to True.\nWe must consider both True and False counts for sub-expressions.\nLet:\n\n\\(dpT[i][j]\\) = number of ways \\(expr[i..j]\\) evaluates to True\n\\(dpF[i][j]\\) = number of ways \\(expr[i..j]\\) evaluates to False\n\nIf expression length is \\(n\\), then we only consider operands at even indices and operators at odd indices.\n\n\nRecurrence\nFor every split at operator \\(k\\) between \\(i\\) and \\(j\\):\nLet \\(op = expr[k]\\)\nCompute:\n\\[\n\\text{TotalTrue} =\n\\begin{cases}\ndpT[i][k-1]\\cdot dpT[k+1][j], & \\text{if } op=\\land,\\\\\ndpT[i][k-1]\\cdot dpT[k+1][j]\\;+\\;dpT[i][k-1]\\cdot dpF[k+1][j]\\;+\\;dpF[i][k-1]\\cdot dpT[k+1][j], & \\text{if } op=\\lor,\\\\\ndpT[i][k-1]\\cdot dpF[k+1][j]\\;+\\;dpF[i][k-1]\\cdot dpT[k+1][j], & \\text{if } op=\\oplus.\n\\end{cases}\n\\]\nSimilarly for \\(dpF[i][j]\\) using complementary logic.\nBase cases:\n\\[\ndpT[i][i] = 1 \\text{ if expr[i] = 'T' else } 0\n\\] \\[\ndpF[i][i] = 1 \\text{ if expr[i] = 'F' else } 0\n\\]\nAnswer = \\(dpT[0][n-1]\\)\n\n\nHow Does It Work (Plain Language)\nWe cut the expression at each operator and combine the truth counts of the left and right sides according to boolean logic.\nFor each subexpression, we record:\n\nhow many parenthesizations make it True\nhow many make it False\n\nThen we combine smaller subproblems to get bigger ones, just like Matrix Chain Multiplication, but using logic rules.\nExample: Expression = T|F&T\nWe can group as:\n\n(T|F)&T → True\nT|(F&T) → True\n\nAnswer = 2\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define MAX 105\n\nint dpT[MAX][MAX], dpF[MAX][MAX];\n\nint main(void) {\n    char expr[105];\n    printf(\"Enter expression (T/F with &|^): \");\n    scanf(\"%s\", expr);\n\n    int n = strlen(expr);\n    for (int i = 0; i &lt; n; i += 2) {\n        dpT[i][i] = (expr[i] == 'T');\n        dpF[i][i] = (expr[i] == 'F');\n    }\n\n    for (int len = 3; len &lt;= n; len += 2) {\n        for (int i = 0; i + len - 1 &lt; n; i += 2) {\n            int j = i + len - 1;\n            dpT[i][j] = dpF[i][j] = 0;\n\n            for (int k = i + 1; k &lt; j; k += 2) {\n                char op = expr[k];\n                int lT = dpT[i][k-1], lF = dpF[i][k-1];\n                int rT = dpT[k+1][j], rF = dpF[k+1][j];\n\n                if (op == '&') {\n                    dpT[i][j] += lT * rT;\n                    dpF[i][j] += lT * rF + lF * rT + lF * rF;\n                } else if (op == '|') {\n                    dpT[i][j] += lT * rT + lT * rF + lF * rT;\n                    dpF[i][j] += lF * rF;\n                } else if (op == '^') {\n                    dpT[i][j] += lT * rF + lF * rT;\n                    dpF[i][j] += lT * rT + lF * rF;\n                }\n            }\n        }\n    }\n\n    printf(\"Number of ways to get True: %d\\n\", dpT[0][n-1]);\n    return 0;\n}\nPython (Readable DP)\nexpr = input(\"Enter expression (T/F with &|^): \")\nn = len(expr)\n\ndpT = [[0]*n for _ in range(n)]\ndpF = [[0]*n for _ in range(n)]\n\nfor i in range(0, n, 2):\n    dpT[i][i] = 1 if expr[i] == 'T' else 0\n    dpF[i][i] = 1 if expr[i] == 'F' else 0\n\nfor length in range(3, n+1, 2):\n    for i in range(0, n-length+1, 2):\n        j = i + length - 1\n        for k in range(i+1, j, 2):\n            op = expr[k]\n            lT, lF = dpT[i][k-1], dpF[i][k-1]\n            rT, rF = dpT[k+1][j], dpF[k+1][j]\n\n            if op == '&':\n                dpT[i][j] += lT * rT\n                dpF[i][j] += lT * rF + lF * rT + lF * rF\n            elif op == '|':\n                dpT[i][j] += lT * rT + lT * rF + lF * rT\n                dpF[i][j] += lF * rF\n            else:  # '^'\n                dpT[i][j] += lT * rF + lF * rT\n                dpF[i][j] += lT * rT + lF * rF\n\nprint(\"Ways to evaluate to True:\", dpT[0][n-1])\n\n\nWhy It Matters\n\nClassic interval DP pattern with logical combination\nShows how state splitting applies beyond arithmetic\nFoundation for boolean circuit optimization and expression counting problems\nReinforces divide by operator technique\n\n\n\nStep-by-Step Example\nExpression = T|F&T\nSubproblems:\n\n\n\nSubexpr\nWays True\nWays False\n\n\n\n\n\nT\n1\n0\n\n\n\nF\n0\n1\n\n\n\nT\n1\n0\n\n\n\nT\nF\n1\n0\n\n\nF&T\n0\n1\n\n\n\nT\nF&T\n2\n0\n\n\n\nAnswer = 2\n\n\nA Gentle Proof (Why It Works)\nEach subexpression can be split at an operator \\(op_k\\). The truth count of the whole depends only on the truth counts of its parts and the operator’s truth table. By combining all possible \\(k\\) recursively, we count all valid parenthesizations. Overlapping subproblems arise when evaluating the same substring, so memoization or bottom-up filling ensures efficiency.\n\n\nTry It Yourself\n\nExtend to count False outcomes too.\nAdd modulo \\(10^9+7\\) for large counts.\nPrint one valid parenthesization.\nTry on expressions like T^T^F or T|F&T^T.\nModify rules for custom logic systems.\n\n\n\nTest Cases\n\n\n\nExpression\nExpected True Count\n\n\n\n\n\n\nT\nF&T\n2\n\n\n\nTTF\n0\n\n\n\n\nT^F\nF\n2\n\n\n\nT&F\nT\n2\n\n\n\nT\nT&F\nF\n5\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\) (split for every operator)\nSpace: \\(O(n^2)\\) (2 DP tables)\n\nThe Boolean Parenthesization problem is the logic mirror of Matrix Chain Multiplication, instead of minimizing cost, we’re counting truth through combinatorial structure.\n\n\n\n433 Burst Balloons\nThe Burst Balloons problem is a classic interval DP challenge. You’re given a row of balloons, each with a number representing coins. When you burst a balloon, you gain coins equal to the product of its number and the numbers of its immediate neighbors. The task is to determine the maximum coins you can collect by choosing the optimal order of bursting.\n\nWhat Problem Are We Solving?\nGiven an array nums of length n, when you burst balloon i, you gain \\[\n\\text{coins} = nums[i-1] \\times nums[i] \\times nums[i+1]\n\\] where nums[i-1] and nums[i+1] are the adjacent balloons still unburst.\nAfter bursting i, it is removed from the sequence, changing neighbor relationships.\nWe want to maximize total coins by choosing the best bursting order.\nTo simplify boundary conditions, pad the array with 1s at both ends: \\[\nval = [1] + nums + [1]\n\\]\nDefine DP state: \\[\ndp[i][j] = \\text{maximum coins obtainable by bursting all balloons between } i \\text{ and } j\n\\]\nRecurrence: \\[\ndp[i][j] = \\max_{k \\in (i, j)} \\Big( dp[i][k] + dp[k][j] + val[i] \\cdot val[k] \\cdot val[j] \\Big)\n\\]\nAnswer: \\[\ndp[0][n+1]\n\\]\n\n\nHow Does It Work (Plain Language)\nInstead of thinking “Which balloon to pop next?”, think “Which balloon to pop last?” between two boundaries.\nBy fixing the last balloon k between i and j, its neighbors are guaranteed to be i and j at that moment, so the coins earned are easy to compute: val[i] * val[k] * val[j].\nThen we solve the smaller subproblems:\n\ndp[i][k]: best coins from bursting balloons between i and k\ndp[k][j]: best coins from bursting between k and j\n\nCombine and take the best split.\nThis is the reverse of the intuitive “first burst” approach, making the subproblems independent.\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define MAX 305\n#define max(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n;\n    printf(\"Enter number of balloons: \");\n    scanf(\"%d\", &n);\n\n    int nums[MAX], val[MAX];\n    printf(\"Enter balloon values: \");\n    for (int i = 1; i &lt;= n; i++) scanf(\"%d\", &nums[i]);\n\n    val[0] = val[n+1] = 1;\n    for (int i = 1; i &lt;= n; i++) val[i] = nums[i];\n\n    int dp[MAX][MAX];\n    memset(dp, 0, sizeof(dp));\n\n    for (int len = 2; len &lt;= n + 1; len++) {\n        for (int i = 0; i + len &lt;= n + 1; i++) {\n            int j = i + len;\n            for (int k = i + 1; k &lt; j; k++) {\n                int cost = val[i] * val[k] * val[j] + dp[i][k] + dp[k][j];\n                dp[i][j] = max(dp[i][j], cost);\n            }\n        }\n    }\n\n    printf(\"Max coins: %d\\n\", dp[0][n+1]);\n    return 0;\n}\nPython (Interval DP)\nnums = list(map(int, input(\"Enter balloon values: \").split()))\nval = [1] + nums + [1]\nn = len(nums)\ndp = [[0]*(n+2) for _ in range(n+2)]\n\nfor length in range(2, n+2):\n    for i in range(0, n+2-length):\n        j = i + length\n        for k in range(i+1, j):\n            dp[i][j] = max(dp[i][j], val[i]*val[k]*val[j] + dp[i][k] + dp[k][j])\n\nprint(\"Max coins:\", dp[0][n+1])\n\n\nWhy It Matters\n\nExemplifies interval DP structure: choose a pivot balloon as the “last” action\nShows how reverse reasoning simplifies state independence\nAppears in optimization over chains, trees, brackets, and games\nFoundation for polygon triangulation and matrix multiplication variants\n\n\n\nStep-by-Step Example\nExample: nums = [3, 1, 5, 8]\nPad → val = [1, 3, 1, 5, 8, 1]\nCompute dp[i][j] for increasing intervals:\n\nInterval (1,4): choose k=2 or 3, compare costs\nGradually expand to full (0,5): Optimal = 167 coins\n\nOne optimal order: burst 1 → 5 → 3 → 8 → 1\n\n\nA Gentle Proof (Why It Works)\nEach interval (i,j) can only depend on smaller intervals (i,k) and (k,j) because the last balloon k divides the chain cleanly. By fixing k as last, we ensure both sides are independent, they share no unburst balloons. Since every subproblem is smaller, bottom-up DP fills states without cycles. Thus, optimal substructure and overlapping subproblems guarantee correctness.\n\n\nTry It Yourself\n\nImplement a top-down memoized version with recursion.\nVisualize the DP table as a triangle showing optimal splits.\nAdd reconstruction to print the burst order.\nTry [1,2,3], [1,5], [7,9,8] to check intuition.\nCompare performance for n=300.\n\n\n\nTest Cases\n\n\n\nnums\nMax Coins\n\n\n\n\n[3,1,5,8]\n167\n\n\n[1,5]\n10\n\n\n[2,2,2]\n12\n\n\n[1,2,3]\n12\n\n\n[9]\n9\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\) for all subintervals and splits\nSpace: \\(O(n^2)\\) for DP table\n\nThe Burst Balloons problem captures the essence of interval DP: choose the last action, build subproblems on either side, and let structure guide optimal order.\n\n\n\n434 Optimal BST\nThe Optimal Binary Search Tree problem asks for the BST shape that minimizes the expected search cost given access frequencies. Even though all BSTs hold the same keys in-order, different shapes can have very different average lookup depths.\n\nWhat Problem Are We Solving?\nWe have sorted keys \\[K_1&lt;K_2&lt;\\dots&lt;K_n,\\] with successful-search probabilities (p_1,,p_n). Optionally, we may include probabilities (q_0,,q_n) for unsuccessful searches in the gaps between keys (classical formulation).\nGoal: build a BST over these keys that minimizes expected comparisons.\nTwo standard DP models:\n\nFull model with gaps ((p_i,q_i)). This is the textbook version.\nSimplified model with only (p_i). Useful when you only care about successful hits.\n\n\n\nDP: Full Model With Gaps\nDefine DP over intervals of keys \\((K_i,\\dots,K_j)\\) with gaps on both sides.\nWeight (total probability mass in the interval, including gaps): \\[\nw[i][j]=\n\\begin{cases}\nq_{i-1}, & i&gt;j,\\\\\nw[i][j-1] + p_j + q_j, & i\\le j.\n\\end{cases}\n\\]\nExpected cost (including internal node comparisons): \\[\ne[i][j]=\n\\begin{cases}\nq_{i-1}, & i&gt;j,\\\\\n\\displaystyle \\min_{r=i}^{j}\\big( e[i][r-1] + e[r+1][j] + w[i][j] \\big), & i\\le j.\n\\end{cases}\n\\]\nAnswer: \\(e[1][n]\\).\nIf you also keep \\(root[i][j]\\) that stores the minimizing \\(r\\), you can reconstruct the tree.\n\n\nDP: Simplified Model (success-only)\nSometimes you only have hit frequencies \\(f_i\\). Let the cost count depth with each comparison adding 1. Define \\[\ndp[i][j]\n= \\text{minimum total weighted depth for } K_i,\\ldots,K_j\n\\text{ when the root contributes 1 per key below it.}\n\\]\nA convenient formulation uses prefix sums \\[\nS[k]=\\sum_{t=1}^{k} f_t, \\qquad W(i,j)=S[j]-S[i-1].\n\\]\nRecurrence: \\[\ndp[i][j]=\n\\begin{cases}\n0, & i&gt;j,\\\\[4pt]\n\\displaystyle \\min_{r=i}^{j}\\big(dp[i][r-1]+dp[r+1][j]\\big)+W(i,j), & i\\le j.\n\\end{cases}\n\\]\nAnswer: \\(dp[1][n]\\).\nThe extra \\(W(i,j)\\) accounts for the fact that choosing any root increases the depth of all keys in its subtrees by 1.\n\n\nHow Does It Work (Plain Language)\nPick the root for a range of keys. Every key not chosen as root sits one level deeper, so its cost increases. The DP tries every candidate root and adds:\n\nthe optimal cost of the left subtree\nthe optimal cost of the right subtree\nthe penalty for pushing all nonroot keys one level deeper (their total frequency)\n\nChoose the root that minimizes this sum for every interval.\n\n\nTiny Code (Easy Versions)\nC (full model with gaps (p_i,q_i))\n#include &lt;stdio.h&gt;\n#include &lt;float.h&gt;\n\n#define MAXN 205\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\ndouble e[MAXN][MAXN], w[MAXN][MAXN];\nint rootIdx[MAXN][MAXN];\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n\n    double p[MAXN], q[MAXN];\n    printf(\"Enter p1..pn: \");\n    for (int i = 1; i &lt;= n; i++) scanf(\"%lf\", &p[i]);\n    printf(\"Enter q0..qn: \");\n    for (int i = 0; i &lt;= n; i++) scanf(\"%lf\", &q[i]);\n\n    for (int i = 1; i &lt;= n+1; i++) {\n        e[i][i-1] = q[i-1];\n        w[i][i-1] = q[i-1];\n    }\n\n    for (int len = 1; len &lt;= n; len++) {\n        for (int i = 1; i+len-1 &lt;= n; i++) {\n            int j = i + len - 1;\n            w[i][j] = w[i][j-1] + p[j] + q[j];\n            e[i][j] = DBL_MAX;\n            for (int r = i; r &lt;= j; r++) {\n                double cost = e[i][r-1] + e[r+1][j] + w[i][j];\n                if (cost &lt; e[i][j]) {\n                    e[i][j] = cost;\n                    rootIdx[i][j] = r;\n                }\n            }\n        }\n    }\n\n    printf(\"Optimal expected cost: %.6f\\n\", e[1][n]);\n    // rootIdx[i][j] holds the chosen root for reconstruction\n    return 0;\n}\nPython (simplified model with hit frequencies (f_i))\nf = list(map(float, input(\"Enter f1..fn: \").split()))\nn = len(f)\n# 1-index for convenience\nf = [0.0] + f\nS = [0.0]*(n+1)\nfor i in range(1, n+1):\n    S[i] = S[i-1] + f[i]\n\ndef W(i, j):\n    return 0.0 if i &gt; j else S[j] - S[i-1]\n\ndp = [[0.0]*(n+2) for _ in range(n+2)]\nroot = [[0]*(n+2) for _ in range(n+2)]\n\nfor length in range(1, n+1):\n    for i in range(1, n - length + 2):\n        j = i + length - 1\n        best, arg = float('inf'), -1\n        for r in range(i, j+1):\n            cost = dp[i][r-1] + dp[r+1][j] + W(i, j)\n            if cost &lt; best:\n                best, arg = cost, r\n        dp[i][j], root[i][j] = best, arg\n\nprint(\"Optimal weighted cost:\", round(dp[1][n], 6))\n# root[i][j] gives a root choice for reconstruction\n\n\nWhy It Matters\n\nModels biased queries where some keys are far more popular\nCanonical interval DP with a split and an additive per-interval penalty\nBasis for query plan optimization, autocompletion tries, and decision tree shaping\nLeads to advanced speedups like Knuth optimization when conditions hold\n\n\n\nStep by Step Example\nSimplified model with (f = [0.3, 0.2, 0.5]) for (K_1&lt;K_2&lt;K_3).\n\nFor length 1: (dp[i][i] = f_i)\nFor interval ([1,2]): try roots 1 or 2\n\n(r=1: dp[1][0]+dp[2][2]+(f_1+f_2)=0+0.2+0.5=0.7)\n(r=2: dp[1][1]+dp[3][2]+0.5=0.3+0+0.5=0.8) choose (r=1).\n\nFor ([1,3]): try (r=1,2,3) with penalty (W(1,3)=1.0) compute and pick the minimum. The DP returns the best shape and cost.\n\n\n\nA Gentle Proof (Why It Works)\nConsider an optimal tree for keys ([i,j]) whose root is (r). All keys other than (K_r) move one level deeper, adding exactly (W(i,j)-p_r) to their cumulative cost. Splitting at (r) separates the instance into two independent subproblems ([i,r-1]) and ([r+1,j]). If either subtree were not optimal, replacing it by a better one would reduce total cost, contradicting optimality. Thus the recurrence that scans all roots and adds the interval weight is correct. The gap model adds (q)-probabilities to the interval weight (w[i][j]) and yields the classical formula.\n\n\nTry It Yourself\n\nReconstruct the tree using the stored root table and print it in preorder.\nCompare the full model ((p,q)) versus the simplified model on the same data.\nNormalize frequencies so (p_i + q_i = 1) and interpret (e[1][n]) as expected comparisons.\nExperiment with Knuth optimization when the quadrangle inequality holds to reduce time toward (O(n^2)).\nStress test with skewed distributions where one key dominates.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nKeys\nModel\nParams\nExpected behavior\n\n\n\n\n3 keys\nsimplified\n(f=[0.3,0.2,0.5])\nroot tends to be key 3 or 1 depending on split costs\n\n\n4 keys\nsimplified\n(f=[1,1,1,1])\nmore balanced tree wins\n\n\n3 keys\nfull ((p,q))\n(p=[0.3,0.2,0.4], q=[0.02,0.02,0.03,0.03])\ngaps shift the optimal root\n\n\n1 key\neither\nsingle freq\ncost equals (p_1) or (q_0+ p_1 + q_1)\n\n\n\n\n\nComplexity\n\nTime: (O(n^3)) with the naive triple loop over ((i,j,r))\nSpace: (O(n^2)) for DP and root tables\n\nThe Optimal BST DP captures a universal pattern: choose a split, add a per-interval penalty that reflects depth inflation, and combine optimal subtrees for the minimal expected search cost.\n\n\n\n435 Polygon Triangulation\nThe Polygon Triangulation problem is a foundational geometric DP challenge. Given a convex polygon, the task is to divide it into non-overlapping triangles by drawing non-intersecting diagonals, minimizing the total weight—often the sum of triangle areas or edge lengths. It’s structurally similar to Matrix Chain Multiplication, with intervals, splits, and additive costs.\n\nWhat Problem Are We Solving?\nGiven a convex polygon with vertices (V_0, V_1, , V_{n-1}), we want to triangulate it (partition into triangles using diagonals) such that the total cost is minimized.\nDefine cost as: \\[\n\\text{cost}(i,j,k) = \\text{weight of triangle }(V_i, V_j, V_k)\n\\] where weight could be:\n\nArea\nPerimeter\nSquared edge length sum (for generality)\n\nWe define the DP state: \\[\ndp[i][j] = \\text{minimum triangulation cost between } V_i \\text{ and } V_j\n\\]\nRecurrence: \\[\ndp[i][j] = \\min_{i&lt;k&lt;j}\\Big(dp[i][k] + dp[k][j] + \\text{cost}(i,j,k)\\Big)\n\\]\nBase case: \\[\ndp[i][i+1] = 0\n\\]\nFinal answer: (dp[0][n-1])\n\n\nHow Does It Work (Plain Language)\nYou pick a vertex (k) between (i) and (j) to form a triangle ((V_i, V_k, V_j)). This triangle splits the polygon into two smaller polygons:\n\nOne from (V_i) to (V_k)\nAnother from (V_k) to (V_j)\n\nWe recursively find their optimal triangulations and add the triangle’s cost.\nThis is a divide-and-conquer on geometry. Every choice of diagonal corresponds to a split in the DP.\n\n\nTiny Code (Easy Versions)\nC (Using area as cost)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;float.h&gt;\n\n#define MAX 105\n#define min(a,b) ((a)&lt;(b)?(a):(b))\n\ntypedef struct { double x, y; } Point;\n\ndouble dist(Point a, Point b) {\n    double dx = a.x - b.x, dy = a.y - b.y;\n    return sqrt(dx*dx + dy*dy);\n}\n\ndouble triangle_cost(Point a, Point b, Point c) {\n    double s = (dist(a,b) + dist(b,c) + dist(c,a)) / 2.0;\n    double area = sqrt(s * (s - dist(a,b)) * (s - dist(b,c)) * (s - dist(c,a)));\n    return area;\n}\n\nint main(void) {\n    int n;\n    printf(\"Enter number of vertices: \");\n    scanf(\"%d\", &n);\n    Point v[MAX];\n    for (int i = 0; i &lt; n; i++)\n        scanf(\"%lf %lf\", &v[i].x, &v[i].y);\n\n    double dp[MAX][MAX];\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            dp[i][j] = 0.0;\n\n    for (int len = 2; len &lt; n; len++) {\n        for (int i = 0; i + len &lt; n; i++) {\n            int j = i + len;\n            dp[i][j] = DBL_MAX;\n            for (int k = i + 1; k &lt; j; k++) {\n                double cost = dp[i][k] + dp[k][j] + triangle_cost(v[i], v[j], v[k]);\n                dp[i][j] = min(dp[i][j], cost);\n            }\n        }\n    }\n\n    printf(\"Min triangulation cost: %.4f\\n\", dp[0][n-1]);\n    return 0;\n}\nPython (Perimeter cost)\nimport math\n\ndef dist(a, b):\n    return math.hypot(a[0] - b[0], a[1] - b[1])\n\ndef triangle_cost(a, b, c):\n    return dist(a,b) + dist(b,c) + dist(c,a)\n\nn = int(input(\"Enter number of vertices: \"))\nv = [tuple(map(float, input().split())) for _ in range(n)]\ndp = [[0]*n for _ in range(n)]\n\nfor length in range(2, n):\n    for i in range(n - length):\n        j = i + length\n        dp[i][j] = float('inf')\n        for k in range(i+1, j):\n            dp[i][j] = min(dp[i][j],\n                           dp[i][k] + dp[k][j] + triangle_cost(v[i], v[k], v[j]))\n\nprint(\"Min triangulation cost:\", round(dp[0][n-1], 4))\n\n\nWhy It Matters\n\nCanonical geometric DP using intervals and triple splits\nUnderpins graphics, meshing, computational geometry, and 3D modeling\nShows that Matrix Chain Multiplication and Polygon Triangulation share a structural template\nReinforces how spatial reasoning maps to recurrence formulation\n\n\n\nStep-by-Step Example\nConsider a quadrilateral with vertices: \\[\nV_0=(0,0),; V_1=(1,0),; V_2=(1,1),; V_3=(0,1)\n\\]\nTwo triangulations:\n\nDiagonal (V_0V_2): triangles ((V_0,V_1,V_2)) and ((V_0,V_2,V_3))\nDiagonal (V_1V_3): triangles ((V_0,V_1,V_3)) and ((V_1,V_2,V_3))\n\nBoth give same area (=1). DP would compute both and take the minimum.\n\n\nA Gentle Proof (Why It Works)\nEvery triangulation must include exactly (n-3) diagonals. Fixing a triangle ((V_i, V_k, V_j)) that uses diagonal (V_iV_j) partitions the polygon into two smaller convex polygons. Since subproblems do not overlap except at the boundary, their optimal solutions combine to the global optimum. By evaluating all (k) between (i) and (j), we guarantee we find the optimal split. The recurrence enumerates all possible triangulations implicitly.\n\n\nTry It Yourself\n\nChange the cost function to perimeter instead of area.\nPrint the sequence of triangles chosen by storing split points.\nVisualize the triangulation order in 2D.\nCompare complexity vs brute force enumeration.\nImplement in memoized recursion style.\n\n\n\nTest Cases\n\n\n\nVertices\nExpected Min Cost (Area)\n\n\n\n\nSquare (0,0),(1,0),(1,1),(0,1)\n1.0\n\n\nTriangle (0,0),(1,0),(0,1)\n0.5\n\n\nPentagon (0,0),(1,0),(2,1),(1,2),(0,1)\nVaries with shape\n\n\n(0,0),(2,0),(2,2),(0,2)\n4.0\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\) (triply nested loop)\nSpace: \\(O(n^2)\\) (DP table)\n\nPolygon Triangulation is the geometric twin of matrix-chain optimization, same recurrence, new meaning.\n\n\n\n436 Matrix Path Sum\nThe Matrix Path Sum problem asks for a path from the top-left to the bottom-right of a grid that optimizes a score, typically the minimum total sum of visited cells, moving only right or down.\n\nWhat Problem Are We Solving?\nGiven an \\(m\\times n\\) matrix \\(A\\) of integers, find the minimum cost to go from \\((0,0)\\) to \\((m-1,n-1)\\) using moves \\({,\\text{right},\\text{down},}\\).\nState: \\[\ndp[i][j]=\\text{minimum path sum to reach cell }(i,j)\n\\]\nRecurrence: \\[\ndp[i][j]=A[i][j]+\\min\\big(dp[i-1][j],,dp[i][j-1]\\big)\n\\]\nBorders: \\[\ndp[0][0]=A[0][0],\\quad\ndp[i][0]=A[i][0]+dp[i-1][0],\\quad\ndp[0][j]=A[0][j]+dp[0][j-1]\n\\]\nAnswer: \\[\ndp[m-1][n-1]\n\\]\nSpace optimization: keep one row \\[\n\\text{row}[j]=A[i][j]+\\min(\\text{row}[j],,\\text{row}[j-1])\n\\]\n\n\nHow Does It Work (Plain Language)\nEach cell’s best cost equals its value plus the better of the two ways you could have arrived: from above or from the left. Build the table row by row until the destination cell is filled.\nExample: \\[\nA=\n\\begin{bmatrix}\n1&3&1\\\n1&5&1\\\n4&2&1\n\\end{bmatrix}\n\\] Optimal sum is \\(1+1+3+1+1=7\\) via path \\((0,0)\\to(1,0)\\to(1,1)\\to(1,2)\\to(2,2)\\).\n\n\nTiny Code (Easy Versions)\nC (2D DP)\n#include &lt;stdio.h&gt;\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\nint main(void) {\n    int m, n;\n    scanf(\"%d %d\", &m, &n);\n    int A[m][n], dp[m][n];\n    for (int i = 0; i &lt; m; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &A[i][j]);\n\n    dp[0][0] = A[0][0];\n    for (int i = 1; i &lt; m; i++) dp[i][0] = A[i][0] + dp[i-1][0];\n    for (int j = 1; j &lt; n; j++) dp[0][j] = A[0][j] + dp[0][j-1];\n\n    for (int i = 1; i &lt; m; i++)\n        for (int j = 1; j &lt; n; j++)\n            dp[i][j] = A[i][j] + MIN(dp[i-1][j], dp[i][j-1]);\n\n    printf(\"%d\\n\", dp[m-1][n-1]);\n    return 0;\n}\nPython (1D Space Optimized)\nm, n = map(int, input().split())\nA = [list(map(int, input().split())) for _ in range(m)]\n\nrow = [0]*n\nrow[0] = A[0][0]\nfor j in range(1, n):\n    row[j] = row[j-1] + A[0][j]\n\nfor i in range(1, m):\n    row[0] += A[i][0]\n    for j in range(1, n):\n        row[j] = A[i][j] + min(row[j], row[j-1])\n\nprint(row[-1])\n\n\nWhy It Matters\n\nCore template for grid DP and shortest path on DAGs\nBasis for image seam carving, robot motion on grids, spreadsheet cost flows\nDemonstrates classic DP patterns: table fill, border initialization, and space rolling\n\n\n\nStep-by-Step Example\nFor \\[\nA=\n\\begin{bmatrix}\n5&1&3\\\n2&8&1\\\n4&2&1\n\\end{bmatrix}\n\\] compute row by row:\n\nFirst row \\(dp\\): \\([5,6,9]\\)\nFirst column \\(dp\\): \\([5,7,11]\\)\nFill inner:\n\n\\(dp[1][1]=8+\\min(6,7)=14\\)\n\\(dp[1][2]=1+\\min(9,14)=10\\)\n\\(dp[2][1]=2+\\min(14,11)=13\\)\n\\(dp[2][2]=1+\\min(10,13)=11\\)\n\n\nAnswer \\(=11\\).\n\n\nA Gentle Proof (Why It Works)\nAny optimal path to \\((i,j)\\) must come from either \\((i-1,j)\\) or \\((i,j-1)\\) because moves are only right or down. If a cheaper route existed that did not pass through the cheaper of these two, replacing the prefix with the cheaper subpath would reduce the total cost, which is a contradiction. Hence the local recurrence using the minimum of top and left yields the global optimum when filled in topological order.\n\n\nTry It Yourself\n\nReconstruct the path: keep a parent pointer from each \\((i,j)\\) to argmin of top or left.\nMaximize sum instead of minimize by changing \\(\\min\\) to \\(\\max\\).\nAdd obstacles: mark blocked cells with \\(+\\infty\\) and skip them.\nAllow moves right, down, and diagonal down-right. Extend the recurrence to three predecessors.\nUse large matrices and compare 2D DP vs 1D rolling performance.\n\n\n\nTest Cases\n\n\n\nMatrix\nExpected\n\n\n\n\n[[1,3,1],[1,5,1],[4,2,1]]\n7\n\n\n[[5]]\n5\n\n\n[[1,2,3],[4,5,6]]\n12\n\n\n[[1,1,1],[1,1,1],[1,1,1]]\n5\n\n\n[[5,1,3],[2,8,1],[4,2,1]]\n11\n\n\n\n\n\nComplexity\n\nTime: \\(O(mn)\\)\nSpace: \\(O(mn)\\) with full table or \\(O(n)\\) with row rolling\n\nMatrix Path Sum is the go-to pattern for grid costs: initialize borders, sweep the table, and each cell is its value plus the best way to arrive.\n\n\n\n437 Largest Square Submatrix\nThe Largest Square Submatrix problem asks for the size of the biggest square of 1s in a binary matrix. It’s a staple 2D DP problem, each cell’s value tells how large a square can end at that position.\n\nWhat Problem Are We Solving?\nGiven a binary matrix \\(A\\) of size \\(m\\times n\\), find the side length of the largest all-1s square.\nWe define \\[\ndp[i][j] = \\text{side length of the largest square whose bottom right corner is at } (i,j).\n\\]\nRecurrence: \\[\ndp[i][j]=\n\\begin{cases}\nA[i][j], & i=0 \\text{ or } j=0,\\\\\n0, & A[i][j]=0,\\\\\n\\min\\{dp[i-1][j],\\, dp[i][j-1],\\, dp[i-1][j-1]\\}+1, & A[i][j]=1.\n\\end{cases}\n\\]\nAnswer: \\[\n\\max_{i,j} dp[i][j].\n\\]\n\n\nHow Does It Work (Plain Language)\nEach cell says: “How big a square of 1s can I end?” If it’s a 1, we look up, left, and up-left, the smallest of those tells how big a square we can extend.\nIntuition:\n\nA square can grow only if all its borders can.\nSo a 1 at ((i,j)) grows a square of size 1 + min(three neighbors).\n\nExample:\n1 0 1 0 0\n1 0 1 1 1\n1 1 1 1 1\n1 0 0 1 0\nThe largest square has side length 3.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#define MAX(a,b) ((a)&gt;(b)?(a):(b))\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n\nint main(void) {\n    int m, n;\n    scanf(\"%d %d\", &m, &n);\n    int A[m][n], dp[m][n];\n    for (int i=0; i&lt;m; i++)\n        for (int j=0; j&lt;n; j++)\n            scanf(\"%d\", &A[i][j]);\n\n    int max_side = 0;\n    for (int i=0; i&lt;m; i++) {\n        for (int j=0; j&lt;n; j++) {\n            if (A[i][j] == 0) dp[i][j] = 0;\n            else if (i==0 || j==0) dp[i][j] = 1;\n            else dp[i][j] = MIN(MIN(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1]) + 1;\n            if (dp[i][j] &gt; max_side) max_side = dp[i][j];\n        }\n    }\n    printf(\"%d\\n\", max_side);\n    return 0;\n}\nPython\nm, n = map(int, input().split())\nA = [list(map(int, input().split())) for _ in range(m)]\n\ndp = [[0]*n for _ in range(m)]\nmax_side = 0\n\nfor i in range(m):\n    for j in range(n):\n        if A[i][j] == 1:\n            if i == 0 or j == 0:\n                dp[i][j] = 1\n            else:\n                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n            max_side = max(max_side, dp[i][j])\n\nprint(max_side)\n\n\nWhy It Matters\n\nFoundation of 2D DP problems (local recurrence on neighbors).\nDirectly extends to rectangles, histograms, obstacles, or weighted cells.\nUsed in image processing, pattern detection, bioinformatics grids.\nDemonstrates how spatial structure can be captured by overlapping subproblems.\n\n\n\nStep-by-Step Example\nMatrix: \\[\nA =\n\\begin{bmatrix}\n1 & 1 & 1 \\\n1 & 1 & 1 \\\n0 & 1 & 1\n\\end{bmatrix}\n\\]\nFill DP:\n\n\n\nA\nDP\n\n\n\n\n1\n1\n\n\n1\n1\n\n\n1\n1\n\n\n\nRow 2:\n\n\\(dp[1][1] = \\min(1,1,1) + 1 = 2\\)\n\\(dp[1][2] = \\min(1,1,1) + 1 = 2\\)\n\nRow 3:\n\n\\(dp[2][2] = \\min(2,2,1) + 1 = 2\\)\n\n\\(\\text{Max} = 2 \\Rightarrow \\text{square side} = 2\\)\n\n\nA Gentle Proof (Why It Works)\nA cell ((i,j)) can be the bottom-right corner of a square of side (k) iff:\n\nThe cells directly above, left, and diagonal up-left can each form a square of side (k-1). So (dp[i][j]) is the largest (k) satisfying these. Filling top-down ensures each needed neighbor is ready, and taking the min keeps the square aligned.\n\n\n\nTry It Yourself\n\nModify to return area instead of side length.\nHandle obstacles (cells with -1) as blocked.\nAdapt to maximum rectangle of 1s (hint: histogram DP per row).\nOutput coordinates of top-left cell of the largest square.\nCompare time between full and 1D-rolled DP.\n\n\n\nTest Cases\n\n\n\nMatrix\nResult\n\n\n\n\n[[1,1,1],[1,1,1],[1,1,1]]\n3\n\n\n[[0,1],[1,1]]\n2\n\n\n[[1,0,1],[1,1,1],[1,1,0]]\n2\n\n\n[[1]]\n1\n\n\n[[0,0],[0,0]]\n0\n\n\n\n\n\nComplexity\n\nTime: (O(mn))\nSpace: (O(mn)) or (O(n)) with rolling rows\n\nThis problem is a clean showcase of local DP propagation, each cell grows the memory of its best square from three neighbors.\n\n\n\n438 Max Rectangle in Binary Matrix\nThe Max Rectangle in Binary Matrix problem asks for the area of the largest rectangle containing only 1s in a binary matrix. It’s a powerful combination of 2D DP and stack-based histogram algorithms, every row is treated as the base of a histogram, and we compute the largest rectangle there.\n\nWhat Problem Are We Solving?\nGiven a binary matrix \\(A\\) of size \\(m \\times n\\), find the maximum rectangular area consisting entirely of 1s.\nInterpret each row as a histogram of heights and update per row: \\[\n\\text{height}[j]=\n\\begin{cases}\n\\text{height}[j]+1, & A[i][j]=1,\\\\\n0, & A[i][j]=0.\n\\end{cases}\n\\] At each row, compute the Largest Rectangle in Histogram on \\(\\text{height}\\).\nEquivalent 2D recurrence for heights: \\[\n\\text{height}[i][j]=\n\\begin{cases}\nA[i][j]+\\text{height}[i-1][j], & A[i][j]=1,\\\\\n0, & A[i][j]=0.\n\\end{cases}\n\\]\nAnswer: \\[\n\\max_i \\operatorname{LargestRectangle}\\big(\\text{height}[i]\\big).\n\\]\n\n\nHow Does It Work (Plain Language)\nThink of the matrix as stacked histograms:\n\nEach row builds on top of the one above.\nA 1 extends the height; a 0 resets it.\nFor each row, we ask: “What’s the largest rectangle if this row were the bottom?”\n\nThis converts a 2D problem into (m) histogram problems.\nExample:\n1 0 1 0 0\n1 0 1 1 1\n1 1 1 1 1\n1 0 0 1 0\nThe largest rectangle of 1s has area 6.\n\n\nTiny Code (Easy Versions)\nC (Using Stack for Histogram)\n#include &lt;stdio.h&gt;\n#define MAX 205\n#define MAX2(a,b) ((a)&gt;(b)?(a):(b))\n\nint largest_histogram(int *h, int n) {\n    int stack[MAX], top = -1, maxA = 0;\n    for (int i = 0; i &lt;= n; i++) {\n        int cur = (i == n ? 0 : h[i]);\n        while (top &gt;= 0 && h[stack[top]] &gt;= cur) {\n            int height = h[stack[top--]];\n            int width = (top &lt; 0 ? i : i - stack[top] - 1);\n            int area = height * width;\n            if (area &gt; maxA) maxA = area;\n        }\n        stack[++top] = i;\n    }\n    return maxA;\n}\n\nint main(void) {\n    int m, n;\n    scanf(\"%d %d\", &m, &n);\n    int A[MAX][MAX];\n    for (int i = 0; i &lt; m; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &A[i][j]);\n\n    int height[MAX] = {0}, maxArea = 0;\n    for (int i = 0; i &lt; m; i++) {\n        for (int j = 0; j &lt; n; j++)\n            height[j] = A[i][j] ? height[j] + 1 : 0;\n        int area = largest_histogram(height, n);\n        if (area &gt; maxArea) maxArea = area;\n    }\n    printf(\"%d\\n\", maxArea);\n    return 0;\n}\nPython (Stack-Based)\ndef largest_histogram(h):\n    stack, maxA = [], 0\n    h.append(0)\n    for i, x in enumerate(h):\n        while stack and h[stack[-1]] &gt;= x:\n            height = h[stack.pop()]\n            width = i if not stack else i - stack[-1] - 1\n            maxA = max(maxA, height * width)\n        stack.append(i)\n    h.pop()\n    return maxA\n\nm, n = map(int, input().split())\nA = [list(map(int, input().split())) for _ in range(m)]\nheight = [0]*n\nmaxA = 0\n\nfor i in range(m):\n    for j in range(n):\n        height[j] = height[j] + 1 if A[i][j] else 0\n    maxA = max(maxA, largest_histogram(height))\n\nprint(maxA)\n\n\nWhy It Matters\n\nCore of 2D maximal area problems\nConnects DP (height propagation) and stack algorithms (histogram)\nUsed in image segmentation, pattern recognition, binary masks\nTemplate for maximal submatrix under constraints\n\n\n\nStep-by-Step Example\nFor: \\[\nA=\n\\begin{bmatrix}\n1 & 1 & 1 \\\n1 & 1 & 1 \\\n1 & 0 & 1\n\\end{bmatrix}\n\\]\nRow 1: [1,1,1] → largest histogram = 3 Row 2: [2,2,2] → largest histogram = 6 Row 3: [3,0,3] → largest histogram = 3 Max = 6\n\n\nA Gentle Proof (Why It Works)\nEach rectangle in the matrix can be identified by its bottom row and column range. The height array at row (i) encodes exactly the number of consecutive 1s above each column, including row (i). Thus every maximal rectangle’s bottom row is considered once, and the largest histogram algorithm ensures that for each height combination, the maximal area is found. Therefore, iterating all rows yields the global optimum.\n\n\nTry It Yourself\n\nModify to return coordinates of top-left and bottom-right corners.\nExtend to max rectangle of 0s by flipping bits.\nCompare to Largest Square Submatrix, same idea, different recurrence.\nUse rolling arrays for memory reduction.\nVisualize histogram growth row by row.\n\n\n\nTest Cases\n\n\n\n\n\n\n\nMatrix\nExpected Max Area\n\n\n\n\n[[1,0,1,0,0],[1,0,1,1,1],[1,1,1,1,1],[1,0,0,1,0]]\n6\n\n\n[[1,1,1],[1,1,1],[1,1,1]]\n9\n\n\n[[0,0],[0,0]]\n0\n\n\n[[1]]\n1\n\n\n[[1,0,1],[1,1,1],[1,1,0]]\n4\n\n\n\n\n\nComplexity\n\nTime: (O(mn)) (each cell pushed/popped once across all rows)\nSpace: (O(n)) for histogram and stack\n\nThis problem elegantly layers row-wise DP and histogram optimization, a universal method for maximal rectangles in 2D grids.\n\n\n\n439 Submatrix Sum Queries\nThe Submatrix Sum Queries problem asks for the sum of all elements inside many rectangular regions of a 2D array. With a 2D prefix sum DP table, each query can be answered in \\(O(1)\\) time after \\(O(mn)\\) preprocessing.\n\nWhat Problem Are We Solving?\nGiven an \\(m\\times n\\) matrix \\(A\\) and many queries of the form \\((r_1,c_1,r_2,c_2)\\) with \\(0\\le r_1\\le r_2&lt;m\\) and \\(0\\le c_1\\le c_2&lt;n\\), compute:\n\\[\n\\text{Sum}(r_1,c_1,r_2,c_2)=\\sum_{i=r_1}^{r_2}\\sum_{j=c_1}^{c_2}A[i][j]\n\\]\nDefine the 2D prefix sum \\(P\\) using 1-based indexing:\n\\[\nP[i][j]=\\sum_{x=1}^{i}\\sum_{y=1}^{j}A[x-1][y-1], \\quad 1\\le i\\le m,\\ 1\\le j\\le n\n\\]\nwith \\(P[0][*]=P[*][0]=0\\).\nThen any submatrix sum is:\n\\[\nS=P[r_2+1][c_2+1]-P[r_1][c_2+1]-P[r_2+1][c_1]+P[r_1][c_1]\n\\]\n\n\nHow Does It Work (Plain Language)\nPrecompute cumulative sums from the top-left corner. The sum of a rectangle is then the big prefix up to its bottom-right, minus the two prefixes above and left, plus back the overlap you subtracted twice. This is just inclusion-exclusion in 2D.\nExample:\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\]\nQuery \\((r_1,c_1,r_2,c_2)=(1,1,2,2)\\) over \\[\n\\begin{bmatrix}\n5 & 6 \\\\\n8 & 9\n\\end{bmatrix}\n\\] gives \\(5+6+8+9=28\\).\n\n\nTiny Code (Easy Versions)\nC (2D Prefix Sum, many queries in O(1) each)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int m, n, q;\n    scanf(\"%d %d\", &m, &n);\n    long long A[m][n];\n    for (int i = 0; i &lt; m; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%lld\", &A[i][j]);\n\n    long long P[m+1][n+1];\n    for (int i = 0; i &lt;= m; i++) P[i][0] = 0;\n    for (int j = 0; j &lt;= n; j++) P[0][j] = 0;\n\n    for (int i = 1; i &lt;= m; i++) {\n        long long rowsum = 0;\n        for (int j = 1; j &lt;= n; j++) {\n            rowsum += A[i-1][j-1];\n            P[i][j] = P[i-1][j] + rowsum;\n        }\n    }\n\n    scanf(\"%d\", &q);\n    while (q--) {\n        int r1, c1, r2, c2;\n        scanf(\"%d %d %d %d\", &r1, &c1, &r2, &c2);\n        long long S = P[r2+1][c2+1] - P[r1][c2+1] - P[r2+1][c1] + P[r1][c1];\n        printf(\"%lld\\n\", S);\n    }\n    return 0;\n}\nPython (2D Prefix Sum)\nm, n = map(int, input().split())\nA = [list(map(int, input().split())) for _ in range(m)]\n\nP = [[0]*(n+1) for _ in range(m+1)]\nfor i in range(1, m+1):\n    rowsum = 0\n    for j in range(1, n+1):\n        rowsum += A[i-1][j-1]\n        P[i][j] = P[i-1][j] + rowsum\n\nq = int(input())\nfor _ in range(q):\n    r1, c1, r2, c2 = map(int, input().split())\n    S = P[r2+1][c2+1] - P[r1][c2+1] - P[r2+1][c1] + P[r1][c1]\n    print(S)\n\n\nWhy It Matters\n\nTurns many 2D range sum queries into \\(O(1)\\) time each after \\(O(mn)\\) preprocessing\nFundamental for integral images, heatmaps, terrain elevation maps, and data analytics\nBuilding block for maximum submatrix sum, range average, and density queries\n\n\n\nStep-by-Step Example\nLet \\[\nA =\n\\begin{bmatrix}\n2 & -1 & 3 \\\\\n0 & 4 & 5 \\\\\n7 & 2 & -6\n\\end{bmatrix}\n\\]\nQuery \\((0,1,2,2)\\) covers the submatrix \\[\n\\begin{bmatrix}\n-1 & 3 \\\\\n4 & 5 \\\\\n2 & -6\n\\end{bmatrix}\n\\] whose sum is \\[\n-1 + 3 + 4 + 5 + 2 - 6 = 7.\n\\]\nCheck formula:\n\\[\nS = P[3][3]-P[0][3]-P[3][1]+P[0][1] = 12-4-7+6 = 7\n\\]\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\(P[i][j]=\\sum_{x\\le i}\\sum_{y\\le j}A[x-1][y-1]\\).\nFor rectangle \\([r_1..r_2]\\times[c_1..c_2]\\):\n\n\\(P[r_2+1][c_2+1]\\): total up to bottom-right\nsubtract \\(P[r_1][c_2+1]\\): remove rows above\nsubtract \\(P[r_2+1][c_1]\\): remove columns left\nadd \\(P[r_1][c_1]\\): restore overlap\n\nThus, the inclusion-exclusion identity holds.\n\n\nTry It Yourself\n\nExtend to 3D prefix sums for cuboid queries\nSupport range average (divide sum by area)\nAdd modulo arithmetic for large sums\nHandle sparse updates with a 2D Fenwick tree\nPrecompute prefix sum for probability maps or heat distributions\n\n\n\nTest Cases\n\n\n\nMatrix\nQuery \\((r_1,c_1,r_2,c_2)\\)\nExpected\n\n\n\n\n[[1,2],[3,4]]\n(0,0,1,1)\n10\n\n\n[[1,2,3],[4,5,6],[7,8,9]]\n(1,1,2,2)\n28\n\n\n[[2,-1,3],[0,4,5],[7,2,-6]]\n(0,1,2,2)\n7\n\n\n[[5]]\n(0,0,0,0)\n5\n\n\n[[1,0,1],[0,1,0],[1,0,1]]\n(0,0,2,2)\n5\n\n\n\n\n\nComplexity\n\nPreprocessing: \\(O(mn)\\)\nQuery: \\(O(1)\\)\nSpace: \\(O(mn)\\)\n\n2D prefix sums are a foundational DP tool: preprocess once, then every submatrix sum is instant.\n\n\n\n440 Palindrome Partitioning\nThe Palindrome Partitioning problem asks you to divide a string into the fewest possible substrings such that each substring is a palindrome. It’s a quintessential interval DP problem where we explore all split points, using precomputed palindrome checks to accelerate the recurrence.\n\nWhat Problem Are We Solving?\nGiven a string \\(s\\) of length \\(n\\), find the minimum number of cuts needed so that every substring is a palindrome.\nFor example: \\(s=\\text{\"aab\"}\\) The best partition is \"aa\" | \"b\", needing 1 cut.\nWe define:\n\n\\(dp[i]\\) = minimum cuts needed for substring \\(s[0..i]\\)\n\\(pal[i][j] = 1\\) if \\(s[i..j]\\) is palindrome, else \\(0\\)\n\nRecurrence: \\[\ndp[i] =\n\\begin{cases}\n0, & \\text{if } pal[0][i] = 1,\\\\\n\\min_{0 \\le j &lt; i,\\ pal[j+1][i] = 1} (dp[j] + 1), & \\text{otherwise.}\n\\end{cases}\n\\]\nPrecompute \\(pal[i][j]\\) using: \\[\npal[i][j] = (s[i]=s[j]) \\land (j-i&lt;2 \\lor pal[i+1][j-1])\n\\]\nAnswer: \\(dp[n-1]\\)\n\n\nHow Does It Work (Plain Language)\nWe want to cut the string at points where the right substring is a palindrome. For each index \\(i\\), we find all \\(j&lt;i\\) such that \\(s[j+1..i]\\) is palindrome and take the minimum over \\(dp[j]+1\\).\nTo avoid \\(O(n^3)\\), we first precompute \\(pal[i][j]\\) in \\(O(n^2)\\).\nExample:\ns = \"aab\"\npal = \na a b\n1 1 0\n  1 0\n    1\nCuts:\n\n\\(dp[0]=0\\) (a)\n\\(dp[1]=0\\) (aa)\n\\(dp[2]=1\\) (aa|b) → answer = 1\n\n\n\nTiny Code (Easy Versions)\nC (Bottom-Up DP)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;limits.h&gt;\n\n#define MIN(a,b) ((a)&lt;(b)?(a):(b))\n#define MAXN 1005\n\nint main(void) {\n    char s[MAXN];\n    scanf(\"%s\", s);\n    int n = strlen(s);\n    int pal[MAXN][MAXN] = {0};\n    int dp[MAXN];\n\n    for (int i = 0; i &lt; n; i++) pal[i][i] = 1;\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 0; i + len - 1 &lt; n; i++) {\n            int j = i + len - 1;\n            if (s[i] == s[j]) {\n                if (len == 2 || pal[i+1][j-1]) pal[i][j] = 1;\n            }\n        }\n    }\n\n    for (int i = 0; i &lt; n; i++) {\n        if (pal[0][i]) { dp[i] = 0; continue; }\n        dp[i] = INT_MAX;\n        for (int j = 0; j &lt; i; j++) {\n            if (pal[j+1][i] && dp[j] + 1 &lt; dp[i])\n                dp[i] = dp[j] + 1;\n        }\n    }\n\n    printf(\"%d\\n\", dp[n-1]);\n    return 0;\n}\nPython (Readable)\ns = input().strip()\nn = len(s)\npal = [[False]*n for _ in range(n)]\ndp = [0]*n\n\nfor i in range(n):\n    pal[i][i] = True\nfor length in range(2, n+1):\n    for i in range(n-length+1):\n        j = i + length - 1\n        if s[i] == s[j] and (length == 2 or pal[i+1][j-1]):\n            pal[i][j] = True\n\nfor i in range(n):\n    if pal[0][i]:\n        dp[i] = 0\n    else:\n        dp[i] = min(dp[j]+1 for j in range(i) if pal[j+1][i])\n\nprint(dp[-1])\n\n\nWhy It Matters\n\nTeaches interval DP with string-based state\nUsed in text segmentation, DNA sequence analysis, code parsing\nBuilds intuition for partitioning problems and precomputation synergy\n\n\n\nStep-by-Step Example\n\\(s = \\text{\"banana\"}\\)\nPalindromic substrings:\n\nSingle letters\n\"ana\" at positions (1–3), (3–5)\n\"anana\" at (1–5)\n\nCompute \\(dp\\):\n\n\\(dp[0]=0\\)\n\\(dp[1]=1\\) (b|a)\n\\(dp[2]=1\\) (ba|n)\n\\(dp[3]=1\\) (b|ana)\n\\(dp[4]=2\\) (b|an|an)\n\\(dp[5]=1\\) (b|anana) → Answer = 1\n\n\n\nA Gentle Proof (Why It Works)\nA valid partition ends at some position \\(i\\). If \\(s[j+1..i]\\) is palindrome, then the cost to partition up to \\(i\\) is \\(dp[j]+1\\). The optimal must choose the best such \\(j\\). By precomputing all palindrome substrings, each \\(dp[i]\\) depends only on smaller indices, satisfying the principle of optimality.\n\n\nTry It Yourself\n\nReturn the actual partitions using a parent array.\nModify to count all partitions instead of minimizing.\nAdapt to palindromic subsequences (different structure).\nVisualize DP and palindrome tables side by side.\nBenchmark naive vs precomputed palindrome approaches.\n\n\n\nTest Cases\n\n\n\ns\nExpected Cuts\nExample Partition\n\n\n\n\n\n\n“aab”\n1\n“aa”\n“b”\n\n\n\n“racecar”\n0\n“racecar”\n\n\n\n\n“banana”\n1\n“b”\n“anana”\n\n\n\n“abc”\n2\n“a”\n“b”\n“c”\n\n\n“a”\n0\n“a”\n\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(n^2)\\) (can reduce palindrome table)\n\nPalindrome Partitioning is a model example of DP with precomputation, revealing how structure (palindromes) enables efficient segmentation.\n\n\n\n\nSection 45. Bitmask DP and Traveling Salesman\n\n441 Traveling Salesman Problem (TSP), Bitmask DP (Held–Karp)\nThe Traveling Salesman Problem asks for the shortest tour that visits every city exactly once and returns to the start. With dynamic programming over subsets, we can solve it in \\(O(n^2 2^n)\\), which is optimal up to polynomial factors for exact exponential algorithms.\n\nWhat Problem Are We Solving?\nGiven \\(n\\) cities and a distance matrix \\(dist[i][j]\\), find the minimum tour length that starts at city \\(0\\), visits all cities once, and returns to \\(0\\).\nState (Held–Karp):\n\nLet \\(dp[mask][i]\\) be the minimum cost to start at \\(0\\), visit exactly the set of cities in bitmask \\(mask\\) (where bit \\(k\\) set means city \\(k\\) is visited), and end at city \\(i\\).\nBase: \\(dp[1\\ll 0][0]=0\\)\n\nTransition:\n\\[\ndp[mask][i] = \\min_{j \\in mask,, j\\ne i}; (dp[mask\\setminus{i}][j] + dist[j][i])\n\\]\nAnswer:\n\\[\n\\min_{i\\ne 0}; dp[(1\\ll n)-1][i] + dist[i][0]\n\\]\nPath reconstruction: store the predecessor for each \\((mask,i)\\).\n\n\nHow Does It Work (Plain Language)\nWe build tours by growing the set of visited cities. For every subset and last city \\(i\\), we ask:\n\n“What was the best way to visit this subset without \\(i\\), then hop from \\(j\\) to \\(i\\)?”\n\nThis reuses smaller subproblems to solve larger ones, until all cities are visited.\n\n\nTiny Code (Easy Versions)\nC (Bitmask DP with Reconstruction)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAXN 20\n#define INF  1000000000\n\nint dist[MAXN][MAXN];\nint prevCity[1&lt;&lt;MAXN][MAXN];\nint dp[1&lt;&lt;MAXN][MAXN];\n\nint main(void) {\n    int n;\n    scanf(\"%d\", &n);\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &dist[i][j]);\n\n    int N = 1 &lt;&lt; n;\n    for (int m = 0; m &lt; N; m++)\n        for (int i = 0; i &lt; n; i++)\n            dp[m][i] = INF, prevCity[m][i] = -1;\n\n    dp[1&lt;&lt;0][0] = 0;\n\n    for (int mask = 0; mask &lt; N; mask++) {\n        for (int i = 0; i &lt; n; i++) if (mask & (1&lt;&lt;i)) {\n            int pmask = mask ^ (1&lt;&lt;i);\n            if (pmask == 0) continue;\n            for (int j = 0; j &lt; n; j++) if (pmask & (1&lt;&lt;j)) {\n                int val = dp[pmask][j] + dist[j][i];\n                if (val &lt; dp[mask][i]) {\n                    dp[mask][i] = val;\n                    prevCity[mask][i] = j;\n                }\n            }\n        }\n    }\n\n    int all = N - 1, best = INF, last = -1;\n    for (int i = 1; i &lt; n; i++) {\n        int val = dp[all][i] + dist[i][0];\n        if (val &lt; best) best = val, last = i;\n    }\n    printf(\"Min tour cost: %d\\n\", best);\n\n    int path[MAXN+1], cnt = n;\n    int mask = all, cur = last;\n    path[--cnt] = cur;\n    while (cur != 0) {\n        int p = prevCity[mask][cur];\n        mask ^= (1&lt;&lt;cur);\n        cur = p;\n        path[--cnt] = cur;\n    }\n    printf(\"Tour: \");\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", path[i]);\n    printf(\"0\\n\");\n    return 0;\n}\nPython (Compact)\nimport sys\nINF = 1015\n\nn = int(sys.stdin.readline())\ndist = [list(map(int, sys.stdin.readline().split())) for _ in range(n)]\n\nN = 1 &lt;&lt; n\ndp = [[INF]*n for _ in range(N)]\ndp[1][0] = 0\n\nfor mask in range(N):\n    if (mask & 1) == 0:\n        continue\n    for i in range(n):\n        if (mask & (1&lt;&lt;i)) == 0:\n            continue\n        pmask = mask ^ (1&lt;&lt;i)\n        if pmask == 0:\n            continue\n        for j in range(n):\n            if (pmask & (1&lt;&lt;j)) == 0:\n                continue\n            dp[mask][i] = min(dp[mask][i], dp[pmask][j] + dist[j][i])\n\nallmask = N - 1\nans = min(dp[allmask][i] + dist[i][0] for i in range(1, n))\nprint(ans)\n\n\nWhy It Matters\n\nCanonical bitmask DP example\nExact solution with best-known time complexity for general TSP\nTemplate for subset-state DP problems: assignment, routing, path cover, Steiner tree\n\n\n\nStep-by-Step Example\nSuppose\n\\[\ndist =\n\\begin{bmatrix}\n0 & 10 & 15 & 20 \\\\\n10 & 0 & 35 & 25 \\\\\n15 & 35 & 0 & 30 \\\\\n20 & 25 & 30 & 0\n\\end{bmatrix}\n\\]\nThe optimal tour is \\(0 \\to 1 \\to 3 \\to 2 \\to 0\\) with cost \\(10+25+30+15=80\\).\n\n\nA Gentle Proof (Why It Works)\nFor any subset \\(S\\) containing \\(0\\) and endpoint \\(i\\in S\\), the optimal path visiting \\(S\\) and ending at \\(i\\) must come from some \\(j\\in S\\setminus{i}\\) visiting \\(S\\setminus{i}\\) optimally and then edge \\(j\\to i\\).\nThus:\n\\[\ndp[S][i] = \\min_{j\\in S\\setminus{i}}(dp[S\\setminus{i}][j] + dist[j][i])\n\\]\nBy processing subsets in increasing size, dependencies are always ready before use.\n\n\nTry It Yourself\n\nReconstruct the tour path\nAdd must-visit or forbidden cities\nRun on \\(n=15\\) and observe scaling\nAdapt to asymmetric TSP (\\(dist[i][j]\\ne dist[j][i]\\))\nCompare with brute-force \\(O(n!)\\)\n\n\n\nTest Cases\n\n\n\nn\ndist\nExpected\n\n\n\n\n2\n[[0,5],[5,0]]\n10\n\n\n3\n[[0,1,10],[1,0,2],[10,2,0]]\n13\n\n\n4\n[[0,10,15,20],[10,0,35,25],[15,35,0,30],[20,25,30,0]]\n80\n\n\n4\n[[0,3,4,2],[3,0,1,5],[4,1,0,6],[2,5,6,0]]\n12\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2 2^n)\\)\nSpace: \\(O(n 2^n)\\)\n\nHeld–Karp DP is the foundation for exponential-time optimization over subsets, bridging combinatorial search and dynamic programming.\n\n\n\n442 Subset DP (Over Subsets of States)\nSubset DP is a powerful pattern where each DP state represents a subset of elements. It’s used when problems depend on combinations of items, masks, or visited sets. You define transitions based on smaller subsets, building up to larger ones.\n\nWhat Problem Are We Solving?\nWe want to compute some function \\(dp[S]\\) over all subsets \\(S\\) of a universe of size \\(n\\) (where \\(S\\) is represented as a bitmask).\nEach \\(dp[S]\\) depends on smaller subsets of \\(S\\), typically by adding or removing one element at a time.\nCommon forms:\n\nSubset sums: \\(dp[S] = \\sum_{T \\subset S} f[T]\\)\nMaximums over subsets: \\(dp[S] = \\max_{T \\subset S} f[T]\\)\nCounting configurations: \\(dp[S] = \\sum_{i \\in S} dp[S \\setminus {i}]\\)\n\nThe key idea: use bit operations and iterate through submasks efficiently.\n\n\nHow Does It Work (Plain Language)\nThink of each subset as a state. For example, if \\(n=3\\), the subsets are:\n\n\n\nMask\nBinary\nSubset\n\n\n\n\n0\n000\n∅\n\n\n1\n001\n{0}\n\n\n2\n010\n{1}\n\n\n3\n011\n{0,1}\n\n\n4\n100\n{2}\n\n\n5\n101\n{0,2}\n\n\n6\n110\n{1,2}\n\n\n7\n111\n{0,1,2}\n\n\n\nTransitions depend on the structure of the problem:\n\nAdditive (sum over submasks)\nCombinational (merge results)\nStepwise (add/remove one bit)\n\n\n\nExample 1: Sum Over Subsets (SOS DP)\nWe want \\(F[S] = \\sum_{T \\subseteq S} A[T]\\). Naively \\(O(3^n)\\), but SOS DP does it in \\(O(n2^n)\\).\nAlgorithm:\nfor (int i = 0; i &lt; n; i++)\n  for (int mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n    if (mask & (1&lt;&lt;i))\n      F[mask] += F[mask ^ (1&lt;&lt;i)];\nEach iteration adds contributions from subsets missing bit \\(i\\).\n\n\nExample 2: Counting Paths on Subsets\nSuppose we count Hamiltonian paths on subsets: \\[\ndp[S][i] = \\sum_{j \\in S, j \\ne i} dp[S\\setminus{i}][j]\n\\] with base \\(dp[{i}][i]=1\\).\nIterate all subsets, and for each subset and endpoint, sum over possible predecessors.\n\n\nTiny Code (Easy Versions)\nC (Sum Over Subsets Example)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    int n = 3;\n    int A[8] = {1, 2, 3, 4, 5, 6, 7, 8}; // arbitrary base values\n    int F[8];\n    for (int mask = 0; mask &lt; (1&lt;&lt;n); mask++) F[mask] = A[mask];\n\n    for (int i = 0; i &lt; n; i++)\n        for (int mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n            if (mask & (1&lt;&lt;i))\n                F[mask] += F[mask ^ (1&lt;&lt;i)];\n\n    for (int mask = 0; mask &lt; (1&lt;&lt;n); mask++)\n        printf(\"F[%d] = %d\\n\", mask, F[mask]);\n\n    return 0;\n}\nPython (Sum Over Subsets)\nn = 3\nA = [1,2,3,4,5,6,7,8]\nF = A[:]\nfor i in range(n):\n    for mask in range(1&lt;&lt;n):\n        if mask & (1&lt;&lt;i):\n            F[mask] += F[mask ^ (1&lt;&lt;i)]\nprint(F)\n\n\nWhy It Matters\n\nFoundation for bitmask combinatorics\nSpeeds up subset convolutions, inclusion-exclusion, and fast zeta transforms\nEssential in Steiner Tree DP, bitmask knapsack, and TSP variants\n\n\n\nStep-by-Step Example\nLet \\(A = [1,2,3,4,5,6,7,8]\\), \\(n=3\\).\nWe want \\(F[S]=\\sum_{T\\subseteq S}A[T]\\).\n\n\\(F[0]=A[0]=1\\)\n\\(F[1]=A[1]+A[0]=3\\)\n\\(F[3]=A[3]+A[2]+A[1]+A[0]=10\\)\n\nAt the end: \\(F = [1,3,4,10,5,12,13,36]\\)\n\n\nA Gentle Proof (Why It Works)\nEach mask represents a subset \\(S\\). When we iterate bit \\(i\\), we add \\(F[S\\setminus{i}]\\) to \\(F[S]\\) if \\(i\\in S\\). This propagates values from smaller subsets to larger ones, accumulating all submask contributions. The loop order ensures every submask is processed before supersets containing it.\n\n\nTry It Yourself\n\nImplement Sum Over Supersets (SOS Superset) by flipping the condition.\nCompute \\(\\max_{T\\subseteq S} A[T]\\) instead of sum.\nUse subset DP to count number of ways to cover a set with given subsets.\nCombine subset DP with bitcount(mask) to handle per-size transitions.\nVisualize subset lattice as a hypercube traversal.\n\n\n\nTest Cases\n\n\n\nA\nExpected F (Sum Over Subsets)\n\n\n\n\n[1,2,3,4,5,6,7,8]\n[1,3,4,10,5,12,13,36]\n\n\n[0,1,0,1,0,1,0,1]\n[0,1,0,2,0,2,0,4]\n\n\n[1,0,0,0,0,0,0,0]\n[1,1,1,1,1,1,1,1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n2^n)\\)\nSpace: \\(O(2^n)\\)\n\nSubset DP is a unifying pattern for problems on sets, once you see the bitmask, think “DP over subsets”.\n\n\n\n443 Hamiltonian Path DP (State Compression)\nThe Hamiltonian Path DP problem asks for the shortest path that visits every vertex exactly once, without needing to return to the start. It’s a close sibling of the Traveling Salesman Problem (TSP) but without the final return edge. Using bitmask DP, we can solve it in \\(O(n^2 2^n)\\).\n\nWhat Problem Are We Solving?\nGiven a complete or weighted directed graph with \\(n\\) vertices and a cost matrix \\(dist[i][j]\\), find the minimum-cost path that visits all vertices exactly once.\nWe don’t need to return to the starting node (unlike TSP).\nDefine the DP state:\n\n\\(dp[mask][i]\\): the minimum cost to visit exactly the set of vertices in \\(mask\\) and end at vertex \\(i\\).\n\nBase case:\n\\[\ndp[1&lt;&lt;i][i] = 0 \\quad \\forall i\n\\]\nTransition:\n\\[\ndp[mask][i] = \\min_{j \\in mask,\\ j \\ne i}\\big(dp[mask \\setminus {i}][j] + dist[j][i]\\big)\n\\]\nAnswer:\n\\[\n\\min_{i} dp[(1&lt;&lt;n)-1][i]\n\\]\n\n\nHow Does It Work (Plain Language)\nImagine we’re constructing paths step by step:\n\nEach mask represents which vertices we’ve already visited.\nEach endpoint \\(i\\) means we finish the path at vertex \\(i\\).\nWe build \\(dp[mask][i]\\) from smaller subsets by adding one last vertex \\(i\\).\n\nAt each step, we check all \\(j\\) that could be the previous vertex in the path.\nNo need to add \\(dist[i][start]\\) because we don’t return, it’s a path, not a cycle.\n\n\nTiny Code (Easy Versions)\nC (Hamiltonian Path DP)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAXN 20\n#define INF 1000000000\n\nint dist[MAXN][MAXN];\nint dp[1&lt;&lt;MAXN][MAXN];\n\nint main(void) {\n    int n;\n    scanf(\"%d\", &n);\n    for (int i = 0; i &lt; n; i++)\n        for (int j = 0; j &lt; n; j++)\n            scanf(\"%d\", &dist[i][j]);\n\n    int N = 1 &lt;&lt; n;\n    for (int mask = 0; mask &lt; N; mask++)\n        for (int i = 0; i &lt; n; i++)\n            dp[mask][i] = INF;\n\n    for (int i = 0; i &lt; n; i++)\n        dp[1&lt;&lt;i][i] = 0;\n\n    for (int mask = 0; mask &lt; N; mask++) {\n        for (int i = 0; i &lt; n; i++) {\n            if (!(mask & (1&lt;&lt;i))) continue;\n            int pmask = mask ^ (1&lt;&lt;i);\n            if (pmask == 0) continue;\n            for (int j = 0; j &lt; n; j++) {\n                if (!(pmask & (1&lt;&lt;j))) continue;\n                int cost = dp[pmask][j] + dist[j][i];\n                if (cost &lt; dp[mask][i]) dp[mask][i] = cost;\n            }\n        }\n    }\n\n    int all = N - 1;\n    int best = INF;\n    for (int i = 0; i &lt; n; i++)\n        if (dp[all][i] &lt; best)\n            best = dp[all][i];\n\n    printf(\"Minimum Hamiltonian path cost: %d\\n\", best);\n    return 0;\n}\nPython (Compact Version)\nINF = 1015\nn = int(input())\ndist = [list(map(int, input().split())) for _ in range(n)]\n\nN = 1 &lt;&lt; n\ndp = [[INF]*n for _ in range(N)]\nfor i in range(n):\n    dp[1&lt;&lt;i][i] = 0\n\nfor mask in range(N):\n    for i in range(n):\n        if not (mask & (1&lt;&lt;i)): continue\n        pmask = mask ^ (1&lt;&lt;i)\n        if pmask == 0: continue\n        for j in range(n):\n            if not (pmask & (1&lt;&lt;j)): continue\n            dp[mask][i] = min(dp[mask][i], dp[pmask][j] + dist[j][i])\n\nans = min(dp[N-1][i] for i in range(n))\nprint(ans)\n\n\nWhy It Matters\n\nFundamental state compression DP pattern\nUseful when the problem involves visiting all nodes exactly once\nCore for path planning, ordering constraints, and bitmask search\n\n\n\nStep-by-Step Example\nLet\n\\[\ndist =\n\\begin{bmatrix}\n0 & 1 & 4 \\\n1 & 0 & 2 \\\n4 & 2 & 0\n\\end{bmatrix}\n\\]\nPaths:\n\n\\(0 \\to 1 \\to 2\\): \\(1 + 2 = 3\\)\n\\(0 \\to 2 \\to 1\\): \\(4 + 2 = 6\\)\n\\(1 \\to 0 \\to 2\\): \\(1 + 4 = 5\\)\n\nSo minimum path = 3. \\(dp[(1&lt;&lt;3)-1] = dp[7] = [5,3,5]\\), \\(\\min=3\\).\n\n\nA Gentle Proof (Why It Works)\nWe apply optimal substructure:\nFor each subset \\(S\\) and endpoint \\(i\\), the optimal Hamiltonian path to \\((S,i)\\) must extend an optimal path to \\((S\\setminus{i}, j)\\) by one edge \\(j\\to i\\). This recurrence ensures no city is revisited and all are included once.\nEach \\(dp[mask][i]\\) depends only on smaller masks, so it can be built bottom-up.\n\n\nTry It Yourself\n\nAdd start node constraint (fix path must start at 0).\nRecover the actual path using a parent array.\nModify for maximum path (replace min with max).\nAdapt for directed graphs with asymmetric costs.\nUse bit tricks like mask & -mask to iterate bits efficiently.\n\n\n\nTest Cases\n\n\n\nn\ndist\nExpected\n\n\n\n\n3\n[[0,1,4],[1,0,2],[4,2,0]]\n3\n\n\n4\n[[0,3,1,5],[3,0,6,7],[1,6,0,2],[5,7,2,0]]\n6\n\n\n2\n[[0,5],[5,0]]\n5\n\n\n3\n[[0,9,9],[9,0,1],[9,1,0]]\n10\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2 2^n)\\)\nSpace: \\(O(n 2^n)\\)\n\nHamiltonian Path DP is the core structure for problems involving traversal of all nodes exactly once, simple, powerful, and a template for countless variants.\n\n\n\n444 Assignment Problem DP (Mask over Tasks)\nThe Assignment Problem asks for the minimum total cost to assign \\(n\\) workers to \\(n\\) tasks with each worker doing exactly one task and each task done by exactly one worker. Besides the Hungarian algorithm, a clean solution for small \\(n\\) is bitmask DP over subsets of tasks.\n\nWhat Problem Are We Solving?\nGiven a cost matrix \\(C\\) where \\(C[i][j]\\) is the cost for worker \\(i\\) to do task \\(j\\), find the minimum cost perfect matching between workers and tasks.\nState definition:\n\nLet \\(mask\\) encode which tasks have been taken.\nLet \\(i=\\text{popcount}(mask)\\) be the number of already assigned workers, meaning we are about to assign worker \\(i\\).\n\nDP state:\n\\[\ndp[mask]=\\text{minimum total cost to assign the first } \\text{popcount}(mask)\\text{ workers to the set of tasks in }mask\n\\]\nTransition:\n\\[\ndp[mask\\cup{j}]=\\min\\big(dp[mask]+C[i][j]\\big)\\quad\\text{for all tasks }j\\notin mask,\\ i=\\text{popcount}(mask)\n\\]\nBase and answer:\n\\[\ndp[0]=0,\\qquad \\text{Answer}=dp[(1\\ll n)-1]\n\\]\n\n\nHow Does It Work (Plain Language)\nWe process workers in order \\(0,1,\\dots,n-1\\). The bitmask tells which tasks are already used. For the next worker \\(i\\), try assigning any free task \\(j\\), add its cost, and carry the best. This builds up all partial matchings until all tasks are taken.\n\n\nTiny Code (Easy Versions)\nC (Bitmask DP)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAXN 20\n#define INF  1000000000\n\nint C[MAXN][MAXN];\nint dp[1&lt;&lt;MAXN];\n\nint main(void) {\n    int n;\n    scanf(\"%d\", &n);\n    for (int i=0;i&lt;n;i++)\n        for (int j=0;j&lt;n;j++)\n            scanf(\"%d\",&C[i][j]);\n\n    int N=1&lt;&lt;n;\n    for (int m=0;m&lt;N;m++) dp[m]=INF;\n    dp[0]=0;\n\n    for (int mask=0;mask&lt;N;mask++) {\n        int i = __builtin_popcount(mask);\n        if (i&gt;=n || dp[mask]==INF) continue;\n        for (int j=0;j&lt;n;j++) if (!(mask&(1&lt;&lt;j))) {\n            int nmask = mask|(1&lt;&lt;j);\n            if (dp[mask]+C[i][j] &lt; dp[nmask])\n                dp[nmask] = dp[mask]+C[i][j];\n        }\n    }\n    printf(\"%d\\n\", dp[N-1]);\n    return 0;\n}\nPython (Compact)\nimport sys\nINF = 1015\n\nn = int(sys.stdin.readline())\nC = [list(map(int, sys.stdin.readline().split())) for _ in range(n)]\n\nN = 1 &lt;&lt; n\ndp = [INF]*N\ndp[0] = 0\n\nfor mask in range(N):\n    i = bin(mask).count(\"1\")\n    if i &gt;= n or dp[mask] == INF:\n        continue\n    for j in range(n):\n        if (mask &gt;&gt; j) & 1: \n            continue\n        dp[mask | (1 &lt;&lt; j)] = min(dp[mask | (1 &lt;&lt; j)], dp[mask] + C[i][j])\n\nprint(dp[N-1])\n\n\nWhy It Matters\n\nCanonical example of state compression DP on subsets\nSimple and reliable when \\(n\\le 20\\) or so\nBaseline to compare with Hungarian algorithm and min cost flow\nTemplate for richer constraints: forbidden pairs, bonuses, prerequisites\n\n\n\nStep-by-Step Example\nLet\n\\[\nC=\n\\begin{bmatrix}\n9&2&7\\\n6&4&3\\\n5&8&1\n\\end{bmatrix}\n\\]\nOne optimal assignment is worker \\(0\\to1\\) (2), \\(1\\to2\\) (3), \\(2\\to0\\) (5) for total \\(2+3+5=10\\). The DP explores all masks, always extending by one free task for the next worker.\n\n\nA Gentle Proof (Why It Works)\nLet \\(S\\) be a set of tasks and \\(i=|S|\\). Any optimal partial assignment of the first \\(i\\) workers to \\(S\\) must end by assigning worker \\(i-1\\) to some \\(j\\in S\\). Removing task \\(j\\) yields an optimal solution to \\((S\\setminus{j}, i-1)\\) plus \\(C[i-1][j]\\). Reversing this gives the forward transition from \\(dp[mask]\\) to \\(dp[mask\\cup{j}]\\) with \\(i=\\text{popcount}(mask)\\). Since each transition increases the mask, filling masks in increasing popcount topologically respects dependencies.\n\n\nTry It Yourself\n\nReconstruct the assignment by storing a parent task for each \\(mask\\).\nAdd forbidden pairs by skipping those \\((i,j)\\).\nAdd a bonus matrix \\(B\\) and minimize \\(\\sum(C[i][j]-B[i][j])\\).\nHandle rectangular \\(n\\times m\\) by padding the smaller side with zero dummy costs.\nCompare runtime against Hungarian on random instances.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\\(C\\)\nExpected min\n\n\n\n\n\\(\\begin{bmatrix}1 & 2 \\\\ 2 & 1\\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}9 & 2 & 7 \\\\ 6 & 4 & 3 \\\\ 5 & 8 & 1\\end{bmatrix}\\)\n\\(10\\)\n\n\n\\(\\begin{bmatrix}4 & 1 & 3 \\\\ 2 & 0 & 5 \\\\ 3 & 2 & 2\\end{bmatrix}\\)\n\\(5\\)\n\n\n\\(\\begin{bmatrix}10\\end{bmatrix}\\)\n\\(10\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2 2^n)\\)\nSpace: \\(O(2^n)\\)\n\nBitmask DP for the assignment problem is a tidy blueprint: iterate masks, assign the next worker to a free task, and keep the cheapest extension until all tasks are taken.\n\n\n\n445 Partition into Two Sets (Balanced Load)\nThe Partition Problem asks whether a given set of numbers can be split into two subsets with equal sum. In its optimization form, we aim to minimize the difference between subset sums. It’s a classic subset DP example that models balanced workloads, resource allocation, and load balancing.\n\nWhat Problem Are We Solving?\nGiven an array \\(A[0..n-1]\\), partition it into two subsets \\(S_1\\) and \\(S_2\\) such that the difference of sums is minimized:\n\\[\n\\text{minimize } |sum(S_1) - sum(S_2)|\n\\]\nEquivalently, find a subset with sum as close as possible to half the total:\n\\[\n\\text{target} = \\left\\lfloor \\frac{\\sum A}{2} \\right\\rfloor\n\\]\nWe use DP to find all achievable sums up to target.\nDefine boolean DP:\n\\[\ndp[i][s] = 1 \\text{ if some subset of the first } i \\text{ elements has sum } s\n\\]\nTransition:\n\\[\ndp[i][s] = dp[i-1][s] \\lor dp[i-1][s-A[i-1]] \\quad \\text{if } s\\ge A[i-1]\n\\]\nAnswer:\n\\[\n\\text{Find largest } s\\le target \\text{ with } dp[n][s]=1,\\ \\text{difference} = total - 2s\n\\]\n\n\nHow Does It Work (Plain Language)\nThink of it as filling a knapsack of capacity target. Each item can either go into the subset or stay out. We try all combinations of sums up to half the total, beyond that, the second subset mirrors it. The closest sum to target yields the minimal difference.\n\n\nTiny Code (Easy Versions)\nC (Tabulation)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#define MAXN 100\n#define MAXSUM 10000\n\nint main(void) {\n    int n, A[MAXN];\n    scanf(\"%d\", &n);\n    int total = 0;\n    for (int i = 0; i &lt; n; i++) {\n        scanf(\"%d\", &A[i]);\n        total += A[i];\n    }\n\n    int target = total / 2;\n    int dp[MAXSUM + 1] = {0};\n    dp[0] = 1;\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int s = target; s &gt;= A[i]; s--) {\n            if (dp[s - A[i]]) dp[s] = 1;\n        }\n    }\n\n    int best = 0;\n    for (int s = target; s &gt;= 0; s--) {\n        if (dp[s]) { best = s; break; }\n    }\n\n    int diff = total - 2 * best;\n    printf(\"Minimal difference: %d\\n\", diff);\n    return 0;\n}\nPython (Concise Version)\nA = list(map(int, input().split()))\ntotal = sum(A)\ntarget = total // 2\ndp = [False]*(target+1)\ndp[0] = True\n\nfor x in A:\n    for s in range(target, x-1, -1):\n        if dp[s-x]:\n            dp[s] = True\n\nfor s in range(target, -1, -1):\n    if dp[s]:\n        print(\"Minimal difference:\", total - 2*s)\n        break\n\n\nWhy It Matters\n\nModels balanced partitioning of workloads, memory, or resources\nFoundation for Subset Sum and Knapsack problems\nIntroduces boolean DP over sums, a crucial building block for combinatorial search\n\n\n\nStep-by-Step Example\nLet \\(A=[1,6,11,5]\\). Total \\(=23\\), target \\(=11\\).\nFeasible sums: \\({0,1,5,6,7,11,12,16,17,18,23}\\)\nBest \\(s=11\\), minimal difference \\(23-2\\cdot11=1\\).\nPartition: \\([11]\\) and \\([1,5,6]\\).\n\n\nA Gentle Proof (Why It Works)\nEach element either belongs to subset \\(S_1\\) or \\(S_2\\). Let \\(s_1\\) be sum of \\(S_1\\), \\(s_2=total-s_1\\). We want \\(|s_1-s_2|\\) minimized → \\(|total-2s_1|\\). By exploring all achievable sums \\(s_1\\le total/2\\), we find the \\(s_1\\) closest to half. Boolean DP tracks reachability using inclusion-exclusion transitions.\n\n\nTry It Yourself\n\nCount number of balanced partitions (replace boolean with integer DP).\nAdd constraint “each subset must have at least \\(k\\) elements.”\nExtend to multi-set partitions (3 or more subsets).\nVisualize reachable sums as boolean array transitions.\nCompare with brute-force subset enumeration.\n\n\n\nTest Cases\n\n\n\nA\nMinimal Difference\nPartition\n\n\n\n\n[1,6,11,5]\n1\n[11] [1,5,6]\n\n\n[3,1,4,2,2]\n0\n[3,2] [4,2]\n\n\n[1,2,7]\n4\n[7] [1,2]\n\n\n[2,2,2,2]\n0\n[2,2] [2,2]\n\n\n[10,20,15,5]\n0\n[10,15] [20,5]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\cdot sum)\\)\nSpace: \\(O(sum)\\)\n\nThe Partition DP is a gentle bridge from Subset Sum to balanced optimization, teaching how combinatorial structure guides numerical state transitions.\n\n\n\n446 Count Hamiltonian Cycles (Bitmask Enumeration)\nThe Hamiltonian Cycle Counting problem asks: given a graph, how many distinct Hamiltonian cycles (closed tours visiting each vertex exactly once) exist? Unlike the shortest-path variants, this version focuses on counting all possible cycles using bitmask DP.\n\nWhat Problem Are We Solving?\nGiven a graph \\(G=(V,E)\\) with \\(|V|=n\\), count the number of distinct Hamiltonian cycles starting and ending at vertex \\(0\\) that visit every vertex exactly once.\nWe define:\n\n\\(dp[mask][i]\\) = number of Hamiltonian paths starting at \\(0\\), visiting all vertices in \\(mask\\), and ending at \\(i\\)\nBase: \\(dp[1&lt;&lt;0][0]=1\\)\n\nTransition:\n\\[\ndp[mask][i] = \\sum_{j\\in mask,, j\\ne i,,(j,i)\\in E} dp[mask\\setminus{i}][j]\n\\]\nAnswer (number of cycles):\n\\[\n\\sum_{i=1}^{n-1} dp[(1&lt;&lt;n)-1][i] \\text{ if } (i,0)\\in E\n\\]\nEach valid end vertex \\(i\\) must connect back to \\(0\\) to complete the cycle.\n\n\nHow Does It Work (Plain Language)\nWe build all possible paths from vertex \\(0\\) that cover a subset of vertices and end at some \\(i\\). For each step, we extend smaller paths by adding a new endpoint \\(i\\). When all vertices are visited (\\(mask=(1&lt;&lt;n)-1\\)), we check which endpoints connect back to \\(0\\). Summing these gives the total number of Hamiltonian cycles.\nThis is similar to the Held–Karp DP, but the operation is addition (counting) instead of minimization.\n\n\nTiny Code (Easy Versions)\nC (Bitmask Counting DP)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define MAXN 20\n\nlong long dp[1&lt;&lt;MAXN][MAXN];\nint adj[MAXN][MAXN];\n\nint main(void) {\n    int n, m;\n    scanf(\"%d %d\", &n, &m);\n    memset(adj, 0, sizeof(adj));\n    for (int e = 0; e &lt; m; e++) {\n        int u, v;\n        scanf(\"%d %d\", &u, &v);\n        adj[u][v] = adj[v][u] = 1;\n    }\n\n    dp[1][0] = 1; // start at vertex 0\n\n    for (int mask = 1; mask &lt; (1 &lt;&lt; n); mask++) {\n        for (int i = 0; i &lt; n; i++) {\n            if (!(mask & (1 &lt;&lt; i))) continue;\n            for (int j = 0; j &lt; n; j++) {\n                if (i == j || !(mask & (1 &lt;&lt; j))) continue;\n                if (adj[j][i])\n                    dp[mask][i] += dp[mask ^ (1 &lt;&lt; i)][j];\n            }\n        }\n    }\n\n    long long total = 0;\n    int full = (1 &lt;&lt; n) - 1;\n    for (int i = 1; i &lt; n; i++) {\n        if (adj[i][0]) total += dp[full][i];\n    }\n\n    printf(\"%lld\\n\", total / 2); // divide by 2 for undirected graphs\n    return 0;\n}\nPython (Compact Version)\nn, m = map(int, input().split())\nadj = [[0]*n for _ in range(n)]\nfor _ in range(m):\n    u, v = map(int, input().split())\n    adj[u][v] = adj[v][u] = 1\n\nN = 1 &lt;&lt; n\ndp = [[0]*n for _ in range(N)]\ndp[1][0] = 1\n\nfor mask in range(N):\n    for i in range(n):\n        if not (mask & (1&lt;&lt;i)): continue\n        for j in range(n):\n            if i==j or not (mask & (1&lt;&lt;j)): continue\n            if adj[j][i]:\n                dp[mask][i] += dp[mask ^ (1&lt;&lt;i)][j]\n\nfull = N-1\ntotal = sum(dp[full][i] for i in range(1,n) if adj[i][0])\nprint(total // 2)\n\n\nWhy It Matters\n\nDemonstrates DP counting over subsets\nFoundation for counting Hamiltonian paths, cycle covers, and tours\nAppears in graph enumeration, combinatorial design, and traveling salesman counting\n\n\n\nStep-by-Step Example\nLet the graph be a square (4-cycle):\nVertices: \\(0,1,2,3\\)\nEdges: \\((0,1),(1,2),(2,3),(3,0)\\) and \\((0,3),(3,2),(2,1),(1,0)\\)\nAll Hamiltonian cycles:\n\n\\(0\\to1\\to2\\to3\\to0\\)\n\\(0\\to3\\to2\\to1\\to0\\)\n\nSo total = 2.\nThe DP constructs these paths incrementally by adding one vertex at a time.\n\n\nA Gentle Proof (Why It Works)\nEach DP state \\((mask,i)\\) corresponds to unique partial paths visiting \\(mask\\) and ending at \\(i\\). To form a path ending at \\(i\\), we must come from \\(j\\in mask\\setminus{i}\\) with edge \\((j,i)\\). This ensures each path is counted exactly once.\nAt the full mask, we have all Hamiltonian paths starting at \\(0\\) and ending at \\(i\\); connecting \\(i\\to0\\) closes the cycle.\nDivide by 2 for undirected graphs since each cycle is counted twice (once clockwise, once counterclockwise).\n\n\nTry It Yourself\n\nRemove division by 2 for directed graphs.\nCount Hamiltonian paths (no return edge).\nModify to track path sequences using parent arrays.\nFor large \\(n\\), compare with inclusion-exclusion counting.\nImplement memoized recursion (top-down version).\n\n\n\nTest Cases\n\n\n\nGraph\nExpected\n\n\n\n\nTriangle \\((0-1-2-0)\\)\n1\n\n\nSquare \\((0-1-2-3-0)\\)\n2\n\n\nLine \\((0-1-2)\\)\n0\n\n\nComplete graph \\(K_4\\)\n\\((4-1)!/2 = 3\\)\n\n\n\\(K_5\\)\n\\((5-1)!/2 = 12\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2 2^n)\\)\nSpace: \\(O(n 2^n)\\)\n\nCounting Hamiltonian cycles via DP elegantly blends subset enumeration with path counting, offering an exact combinatorial count for small graphs.\n\n\n\n447 Steiner Tree DP\nThe Steiner Tree problem asks for the minimum-cost subgraph that connects a given set of terminals in a weighted graph. You may use extra non-terminal vertices (Steiner nodes) if that reduces total cost. The classic exact DP for small numbers of terminals is the Dreyfus–Wagner subset DP.\n\nWhat Problem Are We Solving?\nInput: an undirected connected graph with nonnegative edge weights, and a terminal set \\(T={t_1,\\dots,t_k}\\).\nGoal: find a minimum-weight tree that connects all vertices in \\(T\\).\nWe use a subset DP over terminals and a root vertex:\n\nPrecompute all-pairs shortest paths \\(dist[u][v]\\).\nDP state: \\(dp[S][v]=\\) minimum cost of a tree that connects all terminals in subset \\(S\\subseteq T\\) and whose tree is rooted at vertex \\(v\\).\n\nInitialization: \\[\ndp[{t_i}][v]=dist[v][t_i]\n\\]\nCombine subsets at the same root: \\[\ndp[S][v]=\\min_{\\emptyset\\ne A\\subset S}\\big(dp[A][v]+dp[S\\setminus A][v]\\big)\n\\]\nThen allow the root to move via shortest paths: \\[\ndp[S][v]=\\min_{u}\\big(dp[S][u]+dist[u][v]\\big)\n\\]\nAnswer: \\[\n\\min_{v}dp[T][v]\n\\]\nIn practice we alternate subset-combine at fixed \\(v\\) and a multi-source shortest-path relaxation for each \\(S\\).\n\n\nHow Does It Work (Plain Language)\nThink of building a Steiner tree by gluing together optimal trees for smaller terminal subsets at a meeting vertex \\(v\\). After merging, you are allowed to slide that meeting point anywhere in the graph using shortest paths. Repeat for all subsets until you cover all terminals.\n\n\nTiny Code (Easy Version, Python)\nDreyfus–Wagner with Floyd–Warshall for \\(dist\\) and Dijkstra-based relax per subset. Works for small \\(k\\).\nimport heapq\n\nINF = 1015\n\ndef floyd_warshall(n, w):\n    dist = [row[:] for row in w]\n    for i in range(n):\n        dist[i][i] = min(dist[i][i], 0)\n    for k in range(n):\n        for i in range(n):\n            dik = dist[i][k]\n            if dik == INF: continue\n            rowi = dist[i]\n            rowk = dist[k]\n            for j in range(n):\n                nd = dik + rowk[j]\n                if nd &lt; rowi[j]: rowi[j] = nd\n    return dist\n\ndef steiner_tree(n, edges, terminals):\n    # Build dense weight matrix\n    w = [[INF]*n for _ in range(n)]\n    for u in range(n):\n        w[u][u] = 0\n    for u, v, c in edges:\n        if c &lt; w[u][v]:\n            w[u][v] = w[v][u] = c\n\n    dist = floyd_warshall(n, w)\n\n    k = len(terminals)\n    term_index = {t:i for i,t in enumerate(terminals)}\n\n    # dp[mask][v]\n    dp = [[INF]*n for _ in range(1&lt;&lt;k)]\n    for t in terminals:\n        m = 1 &lt;&lt; term_index[t]\n        for v in range(n):\n            dp[m][v] = dist[v][t]\n\n    # subset DP\n    for mask in range(1, 1&lt;&lt;k):\n        # combine proper nonempty A subset at same root v\n        sub = (mask-1) & mask\n        while sub:\n            other = mask ^ sub\n            if other:\n                for v in range(n):\n                    val = dp[sub][v] + dp[other][v]\n                    if val &lt; dp[mask][v]:\n                        dp[mask][v] = val\n            sub = (sub-1) & mask\n\n        # root-move relaxation by Dijkstra over complete graph with dist metric\n        # This is equivalent to: dp[mask] = metric-closure shortest-path from sources with potentials dp[mask]\n        # Implement 1 run of Dijkstra with initial costs dp[mask][*]\n        pq = [(dp[mask][v], v) for v in range(n)]\n        heapq.heapify(pq)\n        seen = [False]*n\n        while pq:\n            d,v = heapq.heappop(pq)\n            if seen[v]: continue\n            seen[v] = True\n            if d &gt; dp[mask][v]: continue\n            row = dist[v]\n            for u in range(n):\n                nd = d + row[u]\n                if nd &lt; dp[mask][u]:\n                    dp[mask][u] = nd\n                    heapq.heappush(pq, (nd, u))\n\n    full = (1&lt;&lt;k) - 1\n    return min(dp[full])\n\n# Example usage:\n# n = 5\n# edges = [(0,1,1),(1,2,1),(2,3,1),(3,4,1),(0,4,10),(1,4,2)]\n# terminals = [0,3,4]\n# print(steiner_tree(n, edges, terminals))\nNotes:\n\nFor sparse graphs you can skip Floyd–Warshall and run Dijkstra inside the relax step using the original adjacency. Using the metric closure as above is simple and correct for nonnegative weights.\n\n\n\nWhy It Matters\n\nExact algorithm for Steiner trees when the number of terminals \\(k\\) is small\nStandard approach in VLSI routing, network design, and phylogenetics\nTeaches a powerful pattern: subset merge at a root plus shortest-path relaxation\n\n\n\nStep-by-Step Example\nSuppose a 5-vertex path \\(0\\)–\\(1\\)–\\(2\\)–\\(3\\)–\\(4\\) with unit weights and terminals \\(T={0,3,4}\\).\n\nSingletons: \\(dp[{0}][v]=dist[v][0]\\), etc.\nCombine \\({3}\\) and \\({4}\\) at \\(v=3\\) or \\(v=4\\), then relax along the path.\nFinally combine with \\({0}\\); the optimal tree is edges \\((0,1),(1,2),(2,3),(3,4)\\) with total cost \\(4\\).\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(S\\subseteq T\\) and \\(v\\) be a meeting vertex of an optimal Steiner tree for \\(S\\). The tree decomposes into two subtrees whose terminal sets partition \\(S\\), and both subtrees meet at \\(v\\). Hence \\[\ndp[S][v]\\le dp[A][v]+dp[S\\setminus A][v].\n\\] Conversely, any combination at \\(v\\) plus a shortest-path relocation of \\(v\\) to another vertex is no worse than explicitly wiring with edges, due to triangle inequality from the metric closure. Induction over \\(|S|\\) yields optimality.\n\n\nTry It Yourself\n\nReplace the Dijkstra relax with Bellman–Ford to allow zero edges with tight potentials.\nExtract the actual Steiner tree: store the best split and predecessor during relaxation.\nCompare metric-closure method vs relaxing on the original sparse graph.\nBenchmark vs MST over terminals to see the benefit of Steiner vertices.\nAdd a constraint that certain vertices are forbidden as Steiner nodes.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nGraph\nTerminals\nExpected Steiner cost\n\n\n\n\nPath 0–1–2–3–4 with unit edges\n{0,3,4}\n4\n\n\nTriangle 0–1–2 with unit edges\n{0,2}\n1\n\n\nSquare 0–1–2–3 with unit edges, diagonal 1–3 cost 1\n{0,2,3}\n2\n\n\nStar center 0 to 1..4 all cost 1\n{1,2,3,4}\n4\n\n\n\n\n\nComplexity\n\nWith metric closure and subset merge: \\(O(3^k\\cdot n + 2^k\\cdot n\\log n)\\) typical\nMemory: \\(O(2^k\\cdot n)\\)\n\nThe Dreyfus–Wagner DP is the go-to exact method when \\(k\\) is small: combine subsets at a root, then relax through shortest paths to let Steiner nodes emerge automatically.\n\n\n\n448 SOS DP (Sum Over Subsets)\nSum Over Subsets (SOS DP) is a clever bitmask dynamic programming technique for computing values aggregated over all subsets of each mask efficiently, without enumerating all subset pairs explicitly.\n\nWhat Problem Are We Solving?\nSuppose you have an array f[mask] defined for all bitmasks of length n, and you want to compute:\n\\[\ng[mask] = \\sum_{sub \\subseteq mask} f[sub]\n\\]\nNaively, this requires iterating over all subsets of each mask, which takes \\(O(3^n)\\). With SOS DP, we can compute all \\(g[mask]\\) in \\(O(n2^n)\\) time.\n\n\nHow Does It Work (Plain Language)\nThink of each bit in the mask as a “dimension.” For each bit position i, if that bit is set in the mask, we can inherit contributions from the version where the bit was off. We build up sums by iterating over each bit dimension and folding smaller subsets upward.\n\n\nTransition Formula\nLet dp[mask] initially equal f[mask]. Then for each bit i from 0 to n-1:\nif mask has bit i set:\n    dp[mask] += dp[mask ^ (1 &lt;&lt; i)]\nAfter processing all bits, dp[mask] holds the sum over all subsets of mask.\n\n\nExample\nLet \\(n=3\\), masks from 000 to 111.\nIf \\(f[mask]=1\\) for all masks, then:\n\n\n\nmask\nsubsets\ng[mask]\n\n\n\n\n000\n{000}\n1\n\n\n001\n{000,001}\n2\n\n\n010\n{000,010}\n2\n\n\n011\n{000,001,010,011}\n4\n\n\n100\n{000,100}\n2\n\n\n111\nall 8 subsets\n8\n\n\n\nThe DP folds these results bit by bit.\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n#define MAXN (1&lt;&lt;20)\n\nlong long f[MAXN], dp[MAXN];\n\nint main(void) {\n    int n;\n    scanf(\"%d\", &n);\n    int N = 1 &lt;&lt; n;\n    for (int mask = 0; mask &lt; N; mask++) {\n        scanf(\"%lld\", &f[mask]);\n        dp[mask] = f[mask];\n    }\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int mask = 0; mask &lt; N; mask++) {\n            if (mask & (1 &lt;&lt; i)) {\n                dp[mask] += dp[mask ^ (1 &lt;&lt; i)];\n            }\n        }\n    }\n\n    for (int mask = 0; mask &lt; N; mask++)\n        printf(\"%lld \", dp[mask]);\n    printf(\"\\n\");\n}\nPython\nn = int(input(\"n: \"))\nN = 1 &lt;&lt; n\nf = list(map(int, input().split()))\ndp = f[:]\n\nfor i in range(n):\n    for mask in range(N):\n        if mask & (1 &lt;&lt; i):\n            dp[mask] += dp[mask ^ (1 &lt;&lt; i)]\n\nprint(dp)\n\n\nWhy It Matters\n\nCore trick in bitmask convolution, subset transforms, XOR convolution, and mobius inversion.\nReduces \\(O(3^n)\\) subset loops to \\(O(n2^n)\\).\nCommon in problems over subset sums, probabilistic DP, counting states, and game theory.\n\n\n\nA Gentle Proof (Why It Works)\nWe can represent each mask as a binary vector of \\(n\\) bits. Each bit dimension \\(i\\) adds subsets with \\(i\\)th bit unset. Inductively, after processing bit \\(i\\), each mask accumulates contributions from all subsets differing only in bits \\(\\le i\\). By the end, every subset of mask has been included exactly once.\nFormally, for each subset \\(sub\\subseteq mask\\), there exists a sequence of bit additions leading from \\(sub\\) to \\(mask\\), ensuring its inclusion.\n\n\nTry It Yourself\n\nReverse the process to compute Sum Over Supersets instead.\nModify to compute product over subsets.\nApply to count subsets satisfying parity conditions.\nUse SOS DP to precompute subset sums before a subset convolution.\nCombine with inclusion-exclusion for faster combinatorial counting.\n\n\n\nTest Cases\n\n\n\nn\nf (input)\nExpected dp (output)\n\n\n\n\n2\n1 1 1 1\n1 2 2 4\n\n\n2\n1 2 3 4\n1 3 4 10\n\n\n3\nall 1s\n1 2 2 4 2 4 4 8\n\n\n\n\n\nComplexity\n\nTime: \\(O(n2^n)\\)\nSpace: \\(O(2^n)\\)\n\nSOS DP is a cornerstone of bitmask dynamic programming, revealing structure across subsets without enumerating them explicitly.\n\n\n\n449 Bitmask Knapsack (State Compression)\nThe Bitmask Knapsack technique encodes subsets of items using bitmasks, allowing you to represent selections, transitions, and constraints compactly. It’s a bridge between subset enumeration and dynamic programming, especially useful when the number of items is small (e.g. \\(n \\le 20\\)) but the value/weight range is large.\n\nWhat Problem Are We Solving?\nGiven \\(n\\) items, each with weight \\(w_i\\) and value \\(v_i\\), and capacity \\(W\\), choose a subset with total weight ≤ \\(W\\) to maximize total value.\nInstead of indexing DP by capacity, we can enumerate subsets via bitmask:\n\\[\nbest = \\max_{\\text{subset}} \\sum_{i \\in \\text{subset}} v_i \\quad \\text{such that } \\sum_{i \\in \\text{subset}} w_i \\le W\n\\]\nEach subset corresponds to an integer mask, where bit \\(i\\) indicates inclusion of item \\(i\\).\n\n\nHow Does It Work (Plain Language)\nA bitmask is a snapshot of which items are taken. You precompute total weight and total value for each subset. Then simply iterate all masks, filter by capacity, and keep the best.\nIt’s exponential (\\(2^n\\)) but works when \\(n\\) is small and weights are large, where classical DP by weight is infeasible.\n\n\nTransition Formula\nFor each mask:\n\nCompute \\[\ntotal_w = \\sum_{i:mask_i=1} w_i\n\\] \\[\ntotal_v = \\sum_{i:mask_i=1} v_i\n\\]\nIf \\(total_w \\le W\\), update answer: \\[\nbest = \\max(best, total_v)\n\\]\n\nOr incrementally:\n\\[\ndp[mask] = \\sum_{i:mask_i=1} v_i\n\\] \\[\nweight[mask] = \\sum_{i:mask_i=1} w_i\n\\]\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n\n#define MAXN 20\n#define MAXMASK (1&lt;&lt;MAXN)\n\nint main(void) {\n    int n, W;\n    scanf(\"%d %d\", &n, &W);\n    int w[n], v[n];\n    for (int i = 0; i &lt; n; i++) scanf(\"%d %d\", &w[i], &v[i]);\n\n    int N = 1 &lt;&lt; n;\n    int best = 0;\n\n    for (int mask = 0; mask &lt; N; mask++) {\n        int total_w = 0, total_v = 0;\n        for (int i = 0; i &lt; n; i++) {\n            if (mask & (1 &lt;&lt; i)) {\n                total_w += w[i];\n                total_v += v[i];\n            }\n        }\n        if (total_w &lt;= W && total_v &gt; best)\n            best = total_v;\n    }\n\n    printf(\"%d\\n\", best);\n}\nPython\nn, W = map(int, input().split())\nw, v = [], []\nfor _ in range(n):\n    wi, vi = map(int, input().split())\n    w.append(wi)\n    v.append(vi)\n\nbest = 0\nfor mask in range(1 &lt;&lt; n):\n    total_w = total_v = 0\n    for i in range(n):\n        if mask & (1 &lt;&lt; i):\n            total_w += w[i]\n            total_v += v[i]\n    if total_w &lt;= W:\n        best = max(best, total_v)\n\nprint(best)\n\n\nWhy It Matters\n\nWorks well when \\(n\\) is small (e.g. \\(n \\le 20\\)) but weights/values are large\nNatural fit for meet-in-the-middle and subset enumeration\nSimplifies reasoning about combinations, constraints, and transitions\nFound in traveling salesman variants, set packing, and team selection problems\n\n\n\nStep-by-Step Example\nItems:\n\n\n\ni\nw\nv\n\n\n\n\n0\n3\n4\n\n\n1\n4\n5\n\n\n2\n2\n3\n\n\n\nCapacity \\(W=6\\).\nSubsets:\n\n\n\nmask\nitems\nweight\nvalue\nfeasible\n\n\n\n\n000\n{}\n0\n0\n✓\n\n\n001\n{0}\n3\n4\n✓\n\n\n010\n{1}\n4\n5\n✓\n\n\n011\n{0,1}\n7\n9\n✗\n\n\n100\n{2}\n2\n3\n✓\n\n\n101\n{0,2}\n5\n7\n✓\n\n\n110\n{1,2}\n6\n8\n✓\n\n\n111\n{0,1,2}\n9\n12\n✗\n\n\n\nBest feasible = mask 110 → value 8.\n\n\nA Gentle Proof (Why It Works)\nEach subset is a distinct combination of items. Enumerating all \\(2^n\\) subsets guarantees completeness. Feasibility is checked via total weight ≤ \\(W\\), ensuring no invalid subset contributes. Maximization over all feasible subsets returns the global optimum.\nNo overlapping subproblems, so no need for memoization. The entire search space is finite and explored.\n\n\nTry It Yourself\n\nPrint all feasible subsets and their total values.\nCombine with bitcount to restrict subset size.\nUse meet-in-the-middle: split items into halves, enumerate each half, then merge results.\nExtend to multi-dimensional capacity \\((W_1,W_2,...)\\).\nAdapt to minimize weight for a target value instead.\n\n\n\nTest Cases\n\n\n\nItems\nW\nExpected\n\n\n\n\n(3,4),(4,5),(2,3)\n6\n8\n\n\n(2,3),(3,4),(4,5)\n5\n7\n\n\n(1,1),(2,2),(3,3)\n3\n3\n\n\n\n\n\nComplexity\n\nTime: \\(O(n2^n)\\)\nSpace: \\(O(1)\\) (no DP table needed)\n\nBitmask Knapsack is a brute-force DP with compression, a go-to technique for small \\(n\\), offering full flexibility when classical capacity-indexed DP would blow up.\n\n\n\n450 Bitmask Independent Set (Graph Subset Optimization)\nThe Bitmask Independent Set DP enumerates all subsets of vertices in a graph to find the maximum-weight independent set, a set of vertices with no edges between any pair. It’s a classic exponential DP, efficient for small graphs (\\(n \\le 20\\)), using bit operations for adjacency and feasibility.\n\nWhat Problem Are We Solving?\nGiven an undirected graph \\(G=(V,E)\\) with vertex weights \\(w_i\\), find:\n\\[\n\\max \\sum_{i \\in S} w_i \\quad \\text{subject to } \\forall (u,v) \\in E,\\ u,v \\notin S\n\\]\nThat is, choose a subset \\(S\\) of vertices with no adjacent pairs, maximizing total weight.\nWe represent each subset \\(S\\) by a bitmask, where bit \\(i=1\\) means vertex \\(i\\) is included.\n\n\nHow Does It Work (Plain Language)\nWe iterate over all \\(2^n\\) subsets. For each subset, we check whether it forms an independent set by ensuring no edge connects two chosen vertices. If it’s valid, sum its vertex weights and update the best.\nPrecompute adjacency masks to quickly test validity.\n\n\nTransition / Check\nFor each mask:\n\nValidity\nA subset is independent if, for all vertices \\(i\\) included, it contains no neighbor: \\[\n(\\,adj[i] \\mathbin{\\&} mask\\,) = 0\n\\] where \\(adj[i]\\) is the bitmask of neighbors of vertex \\(i\\).\nValue \\[\nvalue(mask) = \\sum_{i:\\,mask_i=1} w_i\n\\]\nBest \\[\nbest = \\max\\!\\bigl(best,\\ value(mask)\\bigr)\n\\]\n\n\n\nTiny Code (Easy Versions)\nC\n#include &lt;stdio.h&gt;\n\n#define MAXN 20\n\nint main(void) {\n    int n, m;\n    scanf(\"%d %d\", &n, &m);\n    int w[n];\n    for (int i = 0; i &lt; n; i++) scanf(\"%d\", &w[i]);\n\n    int adj[n];\n    for (int i = 0; i &lt; n; i++) adj[i] = 0;\n    for (int e = 0; e &lt; m; e++) {\n        int u, v;\n        scanf(\"%d %d\", &u, &v);\n        adj[u] |= 1 &lt;&lt; v;\n        adj[v] |= 1 &lt;&lt; u;\n    }\n\n    int N = 1 &lt;&lt; n;\n    int best = 0;\n\n    for (int mask = 0; mask &lt; N; mask++) {\n        int ok = 1, val = 0;\n        for (int i = 0; i &lt; n; i++) {\n            if (mask & (1 &lt;&lt; i)) {\n                if (adj[i] & mask) { ok = 0; break; }\n                val += w[i];\n            }\n        }\n        if (ok && val &gt; best) best = val;\n    }\n\n    printf(\"%d\\n\", best);\n}\nPython\nn, m = map(int, input().split())\nw = list(map(int, input().split()))\n\nadj = [0]*n\nfor _ in range(m):\n    u, v = map(int, input().split())\n    adj[u] |= 1 &lt;&lt; v\n    adj[v] |= 1 &lt;&lt; u\n\nbest = 0\nfor mask in range(1 &lt;&lt; n):\n    ok = True\n    val = 0\n    for i in range(n):\n        if mask & (1 &lt;&lt; i):\n            if adj[i] & mask:\n                ok = False\n                break\n            val += w[i]\n    if ok:\n        best = max(best, val)\n\nprint(best)\n\n\nWhy It Matters\n\nSolves maximum independent set (MIS) for small graphs\nUseful for exact solutions in constraint problems, treewidth-based DP, or bitmask search\nA building block in graph coloring, maximum clique, dominating set, and subset optimization\nAdaptable for unweighted (count) or weighted (sum) versions\n\n\n\nStep-by-Step Example\nGraph: 4 vertices, edges \\((0,1), (1,2), (2,3)\\) Weights: \\([3, 2, 4, 1]\\)\nIndependent sets:\n\n\n\nMask\nSet\nValid\nValue\n\n\n\n\n0000\n∅\n✓\n0\n\n\n0001\n{0}\n✓\n3\n\n\n0010\n{1}\n✓\n2\n\n\n0100\n{2}\n✓\n4\n\n\n1000\n{3}\n✓\n1\n\n\n0101\n{0,2}\n✗\n,\n\n\n1001\n{0,3}\n✓\n4\n\n\n1100\n{2,3}\n✗\n,\n\n\n1010\n{1,3}\n✓\n3\n\n\n\nBest = 4 (either {2} or {0,3})\n\n\nA Gentle Proof (Why It Works)\nEach subset represents a candidate solution. A subset is feasible iff it contains no adjacent pair, ensured by the adjacency mask check. Since every subset is tested, the algorithm finds the global optimum by enumeration.\nBitmask feasibility check \\((adj[i] \\mathbin{\\&} mask) == 0\\) ensures constant-time validation per vertex, keeping complexity tight.\n\n\nTry It Yourself\n\nModify to count all independent sets.\nRestrict to subsets of exact size \\(k\\).\nAdd memoization to prune invalid masks early.\nCombine with meet-in-the-middle for \\(n=40\\).\nFlip edges to find maximum clique (complement graph).\n\n\n\nTest Cases\n\n\n\nGraph\nWeights\nExpected\n\n\n\n\nChain 0–1–2–3\n[3,2,4,1]\n4\n\n\nStar center 0\n[5,1,1,1,1]\n5\n\n\nTriangle\n[1,2,3]\n3\n\n\nEmpty graph\n[2,2,2]\n6\n\n\n\n\n\nComplexity\n\nTime: \\(O(n2^n)\\)\nSpace: \\(O(n)\\)\n\nBitmask Independent Set DP explores all subsets systematically, perfect when graphs are small but weights or constraints are complex.\n\n\n\n\nSection 46. Digit DP and SOS DP\n\n451 Count Numbers with Property (Digit DP)\nDigit DP is a method for counting numbers within a range that satisfy digit-level constraints, like no leading zeros, digit sum, specific digits, or forbidden patterns. This algorithm builds results by processing digits one by one while maintaining states for prefix constraints and current properties.\n\nWhat Problem Are We Solving?\nGiven an integer \\(N\\) and a property \\(P\\) (like “sum of digits is even”), count how many integers in \\([0, N]\\) satisfy \\(P\\).\nExample: Count numbers \\(\\le 327\\) where the sum of digits is even.\nWe process each digit from most significant to least significant, maintaining:\n\npos: current digit index\nsum: accumulated property (e.g. sum of digits mod 2)\ntight: whether we are bounded by \\(N\\)’s prefix (0 = free, 1 = still tight)\nleading: whether we’ve only seen leading zeros (optional)\n\nState: \\[\ndp[pos][sum][tight]\n\\]\nTransition over next digit d (0..limit):\n\nUpdate next_sum = (sum + d) % 2\nIf tight=1, limit = digit at pos in \\(N\\), else 9\nMove to next position\n\nAnswer is sum over all valid end states satisfying the property.\n\n\nHow Does It Work (Plain Language)\nDigit DP works like counting with awareness: At each step, you choose the next digit, updating what you know (like current sum), while respecting the upper bound. By the time you process all digits, you’ve counted all valid numbers, no need to iterate up to \\(N\\)!\n\n\nTransition Formula\nLet \\(S\\) be the string of digits of \\(N\\). For each state:\n\\[\ndp[pos][sum][tight] = \\sum_{d=0}^{limit} dp[pos+1][(sum+d)\\bmod M][tight \\land (d==limit)]\n\\]\nwith base: \\[\ndp[len][sum][tight] = 1 \\text{ if property holds, else } 0\n\\]\nExample: Property = sum of digits even → \\(sum%2=0\\)\n\n\nTiny Code (Easy Versions)\nPython (Count numbers ≤ N with even digit sum)\nfrom functools import lru_cache\n\ndef count_even_sum(n):\n    digits = list(map(int, str(n)))\n    m = len(digits)\n\n    @lru_cache(None)\n    def dp(pos, sum_mod2, tight):\n        if pos == m:\n            return 1 if sum_mod2 == 0 else 0\n        limit = digits[pos] if tight else 9\n        total = 0\n        for d in range(limit + 1):\n            total += dp(pos + 1, (sum_mod2 + d) % 2, tight and d == limit)\n        return total\n\n    return dp(0, 0, True)\n\nN = int(input(\"Enter N: \"))\nprint(count_even_sum(N))\nC (Recursive Memoized DP)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nint digits[20];\nlong long memo[20][2][2];\nint len;\n\nlong long dp(int pos, int sum_mod2, int tight) {\n    if (pos == len) return sum_mod2 == 0;\n    if (memo[pos][sum_mod2][tight] != -1) return memo[pos][sum_mod2][tight];\n\n    int limit = tight ? digits[pos] : 9;\n    long long res = 0;\n    for (int d = 0; d &lt;= limit; d++) {\n        res += dp(pos + 1, (sum_mod2 + d) % 2, tight && (d == limit));\n    }\n    return memo[pos][sum_mod2][tight] = res;\n}\n\nlong long solve(long long n) {\n    len = 0;\n    while (n &gt; 0) {\n        digits[len++] = n % 10;\n        n /= 10;\n    }\n    for (int i = 0; i &lt; len / 2; i++) {\n        int tmp = digits[i];\n        digits[i] = digits[len - 1 - i];\n        digits[len - 1 - i] = tmp;\n    }\n    memset(memo, -1, sizeof(memo));\n    return dp(0, 0, 1);\n}\n\nint main(void) {\n    long long n;\n    scanf(\"%lld\", &n);\n    printf(\"%lld\\n\", solve(n));\n}\n\n\nWhy It Matters\n\nFundamental to digit-based counting problems\nEfficiently handles constraints on digits, sum, mod, parity, forbidden patterns\nAvoids looping through large ranges (works in \\(O(\\text{len} \\times \\text{state})\\))\nCore idea behind counting numbers with:\n\nEven digit sum\nSpecific digits (e.g. no 4)\nDigits increasing/decreasing\nRemainder mod M conditions\n\n\n\n\nStep-by-Step Example\nCount numbers ≤ 327 with even digit sum.\nWe track sum mod 2:\n\nStart: pos=0, sum=0, tight=1\nFirst digit: choose 0..3\n\nif 3 chosen → sum=1, next tight=1\nelse → free (tight=0)\n\nContinue until last digit\nAt end, count where sum=0 (even)\n\nResult: 164 numbers.\n\n\nA Gentle Proof (Why It Works)\nAt each position, tight ensures we never exceed N, and recursive branching over digits ensures coverage of all valid prefixes. By caching identical subproblems (same pos, sum, tight), we avoid recomputation. Thus, total states = \\(O(len \\times property_space \\times 2)\\).\n\n\nTry It Yourself\n\nCount numbers ≤ N with sum of digits divisible by 3.\nCount numbers with no consecutive equal digits.\nCount numbers with at most k nonzero digits.\nCount numbers with digit product &lt; M.\nAdapt for range [L, R] via solve(R) - solve(L-1).\n\n\n\nTest Cases\n\n\n\nN\nProperty\nExpected\n\n\n\n\n9\nEven digit sum\n5\n\n\n20\nEven digit sum\n10\n\n\n327\nEven digit sum\n164\n\n\n\n\n\nComplexity\n\nTime: \\(O(len \\times M \\times 2)\\)\nSpace: \\(O(len \\times M \\times 2)\\)\n\nDigit DP transforms combinatorial counting into state-driven reasoning, digit by digit, a foundational trick for number-theoretic dynamic programming.\n\n\n\n452 Count Without Adjacent Duplicates\nThis Digit DP problem counts numbers within a range that do not contain adjacent identical digits, a classic example where the state must remember the previous digit to enforce adjacency rules.\n\nWhat Problem Are We Solving?\nGiven an integer \\(N\\), count how many integers in \\([0, N]\\) have no two consecutive equal digits.\nFor example, up to \\(N = 1234\\):\n\nValid: 1203 (no repeats)\nInvalid: 1224 (two 2’s together)\n\nWe’ll use Digit DP to explore all digit sequences up to \\(N\\), ensuring no adjacent duplicates.\n\n\nDP State\nLet the string of digits of \\(N\\) be S, with length len.\nState: \\[\ndp[pos][prev][tight][leading]\n\\]\nWhere:\n\npos: current index (0-based)\nprev: previous digit (0–9, or 10 if none yet)\ntight: whether prefix equals \\(N\\) so far (1 = bound, 0 = free)\nleading: whether we’ve seen only leading zeros (1 = true)\n\nEach state counts valid completions from position pos onward.\n\n\nTransition\nAt position pos:\n\nChoose next digit d from 0 to limit (where limit = S[pos] if tight = 1, else 9)\nSkip if d == prev and not leading (no adjacent duplicates)\nNext state:\n\nnext_prev = d if not leading else 10\nnext_tight = tight and (d == limit)\nnext_leading = leading and (d == 0)\n\n\nSum all valid transitions.\nBase: \\[\ndp[len][prev][tight][leading] = 1\n\\]\nwhen pos == len (end of number).\n\n\nHow Does It Work (Plain Language)\nWe’re building numbers digit by digit:\n\ntight keeps us within bounds.\nprev remembers the last chosen digit, to prevent repeating it.\nleading helps skip leading zeros (which don’t count as duplicates).\n\nBy caching all combinations, we count every valid number exactly once.\n\n\nTiny Code (Easy Versions)\nPython\nfrom functools import lru_cache\n\ndef count_no_adjacent(N):\n    digits = list(map(int, str(N)))\n    m = len(digits)\n\n    @lru_cache(None)\n    def dp(pos, prev, tight, leading):\n        if pos == m:\n            return 1  # valid number\n        limit = digits[pos] if tight else 9\n        total = 0\n        for d in range(limit + 1):\n            if not leading and d == prev:\n                continue  # no adjacent duplicates\n            total += dp(\n                pos + 1,\n                d if not leading else 10,\n                tight and d == limit,\n                leading and d == 0\n            )\n        return total\n\n    return dp(0, 10, True, True)\n\nN = int(input(\"Enter N: \"))\nprint(count_no_adjacent(N))\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nint digits[20];\nlong long memo[20][11][2][2];\nint len;\n\nlong long dp(int pos, int prev, int tight, int leading) {\n    if (pos == len) return 1;\n    if (memo[pos][prev][tight][leading] != -1) return memo[pos][prev][tight][leading];\n\n    int limit = tight ? digits[pos] : 9;\n    long long res = 0;\n    for (int d = 0; d &lt;= limit; d++) {\n        if (!leading && d == prev) continue;\n        int next_prev = leading && d == 0 ? 10 : d;\n        int next_tight = tight && (d == limit);\n        int next_leading = leading && (d == 0);\n        res += dp(pos + 1, next_prev, next_tight, next_leading);\n    }\n    return memo[pos][prev][tight][leading] = res;\n}\n\nlong long solve(long long n) {\n    len = 0;\n    while (n &gt; 0) {\n        digits[len++] = n % 10;\n        n /= 10;\n    }\n    for (int i = 0; i &lt; len / 2; i++) {\n        int tmp = digits[i];\n        digits[i] = digits[len - 1 - i];\n        digits[len - 1 - i] = tmp;\n    }\n    memset(memo, -1, sizeof(memo));\n    return dp(0, 10, 1, 1);\n}\n\nint main(void) {\n    long long n;\n    scanf(\"%lld\", &n);\n    printf(\"%lld\\n\", solve(n));\n}\n\n\nWhy It Matters\n\nClassic Digit DP with “previous digit” state\nEnables constraints like:\n\nNo adjacent repeats\nNo increasing/decreasing sequences\nNo forbidden pairs\n\nUseful in pattern counting, PIN code generation, license plate validation\n\n\n\nStep-by-Step Example\nCount numbers ≤ 120 with no adjacent duplicates:\n\nLeading zeros allowed at first\nE.g. 101 ✓, 110 ✗\nDP checks each digit:\n\n1?0 (pos=0, prev=10)\nFor next digit: skip equal to prev\n\nTotal valid count = 91\n\n\n\nA Gentle Proof (Why It Works)\nFor each position, transitions ensure:\n\nOnly digits ≤ bound if tight=1\nNo consecutive duplicates via d != prev\nLeading zeros treated specially (ignored in duplicate check)\n\nBy iterating through all valid digits and memoizing results, each subproblem (prefix constraint + previous digit) is solved once, ensuring completeness and correctness.\n\n\nTry It Yourself\n\nCount numbers ≤ N with no equal adjacent digits and sum of digits even.\nCount numbers ≤ N with strictly increasing digits.\nModify to disallow adjacent 0’s only.\nCombine with mod constraints (digit sum mod M).\nExtend to handle exactly k equal pairs.\n\n\n\nTest Cases\n\n\n\nN\nExpected\nNotes\n\n\n\n\n9\n10\n0–9 all valid\n\n\n11\n10\n10 invalid\n\n\n100\n91\nOnly 9 invalids\n\n\n1234\n820\nApprox result\n\n\n\n\n\nComplexity\n\nTime: \\(O(len \\times 11 \\times 2 \\times 2 \\times 10)\\)\nSpace: \\(O(len \\times 11 \\times 2 \\times 2)\\)\n\nDigit DP elegantly enforces local digit constraints (like adjacency) through memory of the previous digit, enabling fast counting across massive ranges.\n\n\n\n453 Sum of Digits in Range\nThis Digit DP problem computes the sum of digits of all numbers in a given range \\([0, N]\\). Instead of enumerating numbers, we accumulate digit contributions position by position, respecting upper bounds.\n\nWhat Problem Are We Solving?\nGiven a number \\(N\\), compute:\n\\[\nS(N) = \\sum_{x=0}^{N} \\text{sum\\_of\\_digits}(x)\n\\]\nFor example:\n\n\\(S(13) = 1+0 + 1+1 + 1+2 + 1+3 = 55\\)\n\nThe goal is to compute \\(S(N)\\) efficiently in \\(O(\\text{len} \\times 2 \\times M)\\), not \\(O(N)\\).\nYou can also handle ranges: \\[\nS(L,R) = S(R) - S(L-1)\n\\]\n\n\nDP State\nLet \\(S\\) = list of digits of \\(N\\).\nWe define a recursive function: \\[\ndp[pos][tight][sum]\n\\]\nBut instead of counting numbers, we also accumulate the total digit sum contribution.\nSo the function returns (count, total_sum), a pair:\n\ncount: number of valid numbers\nsum: sum of digits over all valid numbers\n\nState:\n\npos: current position (0..len-1)\ntight: whether we are still bounded by prefix\nleading: whether only leading zeros so far\n\n\n\nTransition\nAt each position, choose digit d in [0, limit] (limit = digit at pos if tight = 1, else 9)\nLet (cnt_next, sum_next) = result from next position.\nWe add current digit’s contribution: \\[\ntotal_sum += sum_next + d \\times cnt_next\n\\]\nIf leading is true and d=0, then we don’t count that as a “real” leading digit.\nBase case: \\[\ndp[len][tight][leading] = (1, 0)\n\\]\n(one valid number, sum = 0)\n\n\nHow Does It Work (Plain Language)\nEach recursive call counts how many numbers are possible from this prefix, and how much total digit sum they produce.\nWhen you pick a digit d:\n\nd contributes d * cnt_next to all numbers in this branch\nPlus whatever the rest of the digits contribute recursively\n\nBy caching (count, sum) per state, we reuse computations for repeated prefixes.\n\n\nTiny Code (Easy Version)\nPython\nfrom functools import lru_cache\n\ndef sum_of_digits_upto(N):\n    digits = list(map(int, str(N)))\n    m = len(digits)\n\n    @lru_cache(None)\n    def dp(pos, tight, leading):\n        if pos == m:\n            return (1, 0)  # (count, sum)\n\n        limit = digits[pos] if tight else 9\n        total_count, total_sum = 0, 0\n\n        for d in range(limit + 1):\n            cnt_next, sum_next = dp(\n                pos + 1,\n                tight and (d == limit),\n                leading and d == 0\n            )\n            total_count += cnt_next\n            total_sum += sum_next + (0 if leading and d == 0 else d * cnt_next)\n\n        return (total_count, total_sum)\n\n    return dp(0, True, True)[1]\n\ndef sum_of_digits_range(L, R):\n    return sum_of_digits_upto(R) - sum_of_digits_upto(L - 1)\n\n# Example\nL, R = map(int, input(\"Enter L R: \").split())\nprint(sum_of_digits_range(L, R))\nC (Recursive Pair Return via struct)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\ntypedef struct { long long count, sum; } Pair;\n\nint digits[20];\nPair memo[20][2][2];\nint vis[20][2][2];\nint len;\n\nPair dp(int pos, int tight, int leading) {\n    if (pos == len) return (Pair){1, 0};\n    if (vis[pos][tight][leading]) return memo[pos][tight][leading];\n    vis[pos][tight][leading] = 1;\n\n    int limit = tight ? digits[pos] : 9;\n    long long total_count = 0, total_sum = 0;\n\n    for (int d = 0; d &lt;= limit; d++) {\n        Pair next = dp(pos + 1, tight && (d == limit), leading && (d == 0));\n        total_count += next.count;\n        total_sum += next.sum + (leading && d == 0 ? 0 : (long long)d * next.count);\n    }\n\n    return memo[pos][tight][leading] = (Pair){total_count, total_sum};\n}\n\nlong long solve(long long n) {\n    if (n &lt; 0) return 0;\n    len = 0;\n    while (n &gt; 0) {\n        digits[len++] = n % 10;\n        n /= 10;\n    }\n    for (int i = 0; i &lt; len / 2; i++) {\n        int tmp = digits[i];\n        digits[i] = digits[len - 1 - i];\n        digits[len - 1 - i] = tmp;\n    }\n    memset(vis, 0, sizeof(vis));\n    return dp(0, 1, 1).sum;\n}\n\nint main(void) {\n    long long L, R;\n    scanf(\"%lld %lld\", &L, &R);\n    printf(\"%lld\\n\", solve(R) - solve(L - 1));\n}\n\n\nWhy It Matters\n\nComputes digit sums over huge ranges in logarithmic time\nBasis for many digit aggregation problems (count of 1’s, digit sum mod M, etc.)\nExtensible to:\n\nCounting even/odd digits\nWeighted digit sums (like \\(d \\times 10^{pos}\\))\nProperty-based aggregation (e.g., sum of squares)\n\n\n\n\nStep-by-Step Example\nCompute sum of digits for all numbers ≤ 13:\n\n\n\nNumber\nSum\n\n\n\n\n0\n0\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n6\n6\n\n\n7\n7\n\n\n8\n8\n\n\n9\n9\n\n\n10\n1\n\n\n11\n2\n\n\n12\n3\n\n\n13\n4\n\n\n\nTotal = 55 ✅\nDP builds this by digit:\n\nTens digit → repeats 10 times\nOnes digit → contributes 0–9 repeatedly\n\n\n\nA Gentle Proof (Why It Works)\nEach position contributes its digit value multiplied by the number of combinations of remaining positions. Digit DP captures this recursively: If a digit d is fixed at position pos, every completion of later positions includes that digit once, hence d * count_of_suffix. Summing over all digits at all positions gives the total sum.\n\n\nTry It Yourself\n\nCount sum of even digits only.\nCompute sum of digits mod M.\nCompute sum of squared digits.\nCount total number of digits written in range.\nCompute weighted sum (like d * 10^pos contributions).\n\n\n\nTest Cases\n\n\n\nRange\nExpected\n\n\n\n\n0–9\n45\n\n\n0–13\n55\n\n\n10–99\n855\n\n\n1–1000\n13501\n\n\n\n\n\nComplexity\n\nTime: \\(O(\\text{len} \\times 2 \\times 2 \\times 10)\\)\nSpace: \\(O(\\text{len} \\times 2 \\times 2)\\)\n\nDigit DP can aggregate digit-level properties over massive intervals, this sum version is its canonical “count + accumulate” template.\n\n\n\n454 Count with Mod Condition (Digit Sum mod M)\nCount numbers in a range whose digit sum satisfies a modular condition. The standard pattern tracks the digit-sum modulo \\(M\\) while respecting the upper bound.\n\nWhat Problem Are We Solving?\nGiven integers \\(N,M,K\\), count how many \\(x\\in[0,N]\\) satisfy: \\[\n\\big(\\text{sum\\_digits}(x)\\big)\\bmod M=K\n\\] For a general range \\([L,R]\\), use \\(f(R)-f(L-1)\\).\n\n\nDP State\nLet the decimal string of \\(N\\) be \\(S\\), length \\(m\\).\nState: \\[\ndp[pos][mod][tight][leading]\n\\]\n\n\\(pos\\): index in \\(S\\) (0-based, left to right)\n\\(mod\\): current value of digit-sum modulo \\(M\\)\n\\(tight\\in{0,1}\\): still equal to \\(S\\) prefix or already below\n\\(leading\\in{0,1}\\): still placing only leading zeros\n\nGoal: count completions with final \\(mod = K\\).\nBase: \\[\ndp[m][mod][tight][leading] =\n\\begin{cases}\n1, & mod = K,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]\nTransition (choose next digit \\(d\\)):\n\n\\(limit = S[pos]\\) if \\(tight = 1\\), else \\(9\\)\nNext states:\n\n\\(next\\_tight = tight \\land (d = limit)\\)\n\n\\(next\\_leading = leading \\land (d = 0)\\)\n\n\n\\[\n  next\\_mod =\n    \\begin{cases}\n    mod, & \\text{if } next\\_leading = 1,\\\\\n    (mod + d) \\bmod M, & \\text{otherwise.}\n    \\end{cases}\n\\]\nThen \\[\ndp[pos][mod][tight][leading]\n= \\sum_{d=0}^{limit}\ndp[pos+1][next\\_mod][next\\_tight][next\\_leading].\n\\]\n\n\nTiny Code (Easy Versions)\nPython\nfrom functools import lru_cache\n\ndef count_mod_sum_upto(N, M, K):\n    S = list(map(int, str(N)))\n    m = len(S)\n\n    @lru_cache(None)\n    def dp(pos, mod, tight, leading):\n        if pos == m:\n            return 1 if mod == K else 0\n        limit = S[pos] if tight else 9\n        total = 0\n        for d in range(limit + 1):\n            ntight = tight and (d == limit)\n            nleading = leading and (d == 0)\n            nmod = mod if nleading else (mod + d) % M\n            total += dp(pos + 1, nmod, ntight, nleading)\n        return total\n\n    return dp(0, 0, True, True)\n\ndef count_mod_sum_range(L, R, M, K):\n    if L &lt;= 0:\n        return count_mod_sum_upto(R, M, K)\n    return count_mod_sum_upto(R, M, K) - count_mod_sum_upto(L - 1, M, K)\n\n# Example:\n# print(count_mod_sum_range(0, 327, 7, 3))\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nlong long memo[20][200][2][2];\nchar vis[20][200][2][2];\nint digits[20], mlen, M, K;\n\nlong long solve_dp(int pos, int mod, int tight, int leading) {\n    if (pos == mlen) return mod == K;\n    if (vis[pos][mod][tight][leading]) return memo[pos][mod][tight][leading];\n    vis[pos][mod][tight][leading] = 1;\n\n    int limit = tight ? digits[pos] : 9;\n    long long total = 0;\n    for (int d = 0; d &lt;= limit; d++) {\n        int ntight = tight && (d == limit);\n        int nleading = leading && (d == 0);\n        int nmod = nleading ? mod : (mod + d) % M;\n        total += solve_dp(pos + 1, nmod, ntight, nleading);\n    }\n    return memo[pos][mod][tight][leading] = total;\n}\n\nlong long count_upto(long long N, int m, int k) {\n    if (N &lt; 0) return 0;\n    M = m; K = k;\n    int tmp[20], len = 0;\n    while (N &gt; 0) { tmp[len++] = (int)(N % 10); N /= 10; }\n    if (len == 0) tmp[len++] = 0;\n    for (int i = 0; i &lt; len; i++) digits[i] = tmp[len - 1 - i];\n    mlen = len;\n    memset(vis, 0, sizeof(vis));\n    return solve_dp(0, 0, 1, 1);\n}\n\nlong long count_range(long long L, long long R, int m, int k) {\n    return count_upto(R, m, k) - count_upto(L - 1, m, k);\n}\n\n// Example usage in main:\n// int main(){ long long L=0,R=327; int M=7,K=3; printf(\"%lld\\n\", count_range(L,R,M,K)); }\nNotes:\n\nThe array bound 200 for mod assumes \\(M\\le 200\\). Increase if needed.\n\n\n\nWhy It Matters\n\nCore Digit DP for number-theory style constraints\nHandles many variants by changing the carried statistic to modulo form\nBuilding block for problems like:\n\nCount with digit sum equal to \\(S\\) (set \\(M\\) large enough and target \\(K=S\\) with careful state)\nCount with digit sum in a set (sum over multiple \\(K\\))\nMulti-condition states (e.g., digit sum mod \\(M\\) and parity)\n\n\n\n\nStep-by-Step Example\nCount \\(x\\in[0,99]\\) with digit sum mod \\(3\\) equal to \\(0\\).\n\nThe DP carries \\(mod\\in{0,1,2}\\).\nAt each position, branch over \\(d\\in[0..9]\\) with tight until you pass the bound 99.\nThe answer is \\(34\\).\n\n(This small case can also be verified by combinatorics: roughly one third of two-digit-with-leading-zero numbers.)\n\n\nA Gentle Proof (Why It Works)\nEvery number corresponds to one path of digit choices. The state \\((pos,mod,tight,leading)\\) uniquely captures all information that influences future feasibility and the final condition \\(mod=K\\). Since the transition only depends on the current state and chosen digit, memoizing these states yields a complete and non-overlapping partition of the search space.\n\n\nTry It Yourself\n\nCount numbers with digit sum mod \\(M\\) in a set \\(S\\) by summing answers over \\(K\\in S\\).\nCount numbers with digit sum exactly \\(S\\) by replacing \\(mod\\) with a bounded sum state and capping at \\(S\\).\nCombine with no adjacent duplicates by adding a prev digit to the state.\nCompute numbers with sum of squares of digits mod \\(M\\) equal to \\(K\\).\nExtend to base-\\(B\\) by changing the limit from 9 to \\(B-1\\).\n\n\n\nTest Cases\n\n\n\n\\(N\\)\n\\(M\\)\n\\(K\\)\nExpected idea\n\n\n\n\n9\n3\n0\n4 numbers (0,3,6,9)\n\n\n20\n2\n0\nabout half of 0..20\n\n\n99\n3\n0\n34\n\n\n327\n7\n3\ncomputed via code\n\n\n\n\n\nComplexity\n\nTime: \\(O(len\\cdot M\\cdot 2\\cdot 2\\cdot 10)\\)\nSpace: \\(O(len\\cdot M\\cdot 2\\cdot 2)\\)\n\nThis is the standard mod-carry Digit DP: thread the property through the digits modulo \\(M\\), respect the bound with tight, and account for leading zeros cleanly.\n\n\n\n455 Count of Increasing Digits\nWe want to count all integers in a range that have strictly increasing digits, every digit is larger than the one before it. For example, 123, 149, and 7 qualify, but 133, 321, or 224 do not.\n\nWhat Problem Are We Solving?\nGiven an upper bound \\(N\\), count integers \\(x\\) in \\([0, N]\\) such that:\n\\[\n\\text{digits}(x) = [d_0, d_1, \\dots, d_k] \\implies d_0 &lt; d_1 &lt; \\dots &lt; d_k\n\\]\nExample: For \\(N=500\\), valid numbers include 1, 2, …, 9, 12, 13, …, 49, 123, 134, …, 489, etc.\nWe can model this with Digit DP tracking the previous digit to ensure the increasing condition.\n\n\nDP State\nLet \\(S\\) = list of digits of \\(N\\).\nState: \\[\ndp[pos][prev][tight][leading]\n\\]\nWhere:\n\npos: index of current digit (0-based)\nprev: last chosen digit (0–9, or 10 for “none yet”)\ntight: whether we’re still prefix-equal to \\(N\\)\nleading: whether we’ve only placed leading zeros (so far no real digits)\n\n\n\nTransition\nAt each position:\n\nDetermine limit = S[pos] if tight=1, else 9.\nLoop d from 0 to limit.\nSkip d &lt;= prev if not leading (must strictly increase).\nUpdate:\n\nnext_tight = tight and (d == limit)\nnext_leading = leading and (d == 0)\nnext_prev = prev if next_leading else d\n\n\nSum results of recursive calls.\nBase: \\[\ndp[len][prev][tight][leading] = 1\n\\] since one valid number is formed.\n\n\nHow Does It Work (Plain Language)\nWe build the number one digit at a time:\n\nIf we’ve started (not leading), each new digit must be greater than the previous one.\nIf we’re still leading, any zero is fine.\ntight ensures we never exceed \\(N\\)’s prefix. By exploring all possible digits under these rules, we count every strictly increasing number ≤ \\(N\\).\n\n\n\nTiny Code (Easy Versions)\nPython\nfrom functools import lru_cache\n\ndef count_increasing_digits(N):\n    S = list(map(int, str(N)))\n    m = len(S)\n\n    @lru_cache(None)\n    def dp(pos, prev, tight, leading):\n        if pos == m:\n            return 1  # reached end, valid number\n        limit = S[pos] if tight else 9\n        total = 0\n        for d in range(limit + 1):\n            if not leading and d &lt;= prev:\n                continue  # must strictly increase\n            ntight = tight and (d == limit)\n            nleading = leading and (d == 0)\n            nprev = prev if nleading else d\n            total += dp(pos + 1, nprev, ntight, nleading)\n        return total\n\n    return dp(0, 10, True, True)\n\n# Example\nN = int(input(\"Enter N: \"))\nprint(count_increasing_digits(N))\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nlong long memo[20][11][2][2];\nchar vis[20][11][2][2];\nint digits[20], len;\n\nlong long dp(int pos, int prev, int tight, int leading) {\n    if (pos == len) return 1;\n    if (vis[pos][prev][tight][leading]) return memo[pos][prev][tight][leading];\n    vis[pos][prev][tight][leading] = 1;\n\n    int limit = tight ? digits[pos] : 9;\n    long long res = 0;\n    for (int d = 0; d &lt;= limit; d++) {\n        if (!leading && d &lt;= prev) continue;\n        int ntight = tight && (d == limit);\n        int nleading = leading && (d == 0);\n        int nprev = nleading ? prev : d;\n        res += dp(pos + 1, nprev, ntight, nleading);\n    }\n\n    return memo[pos][prev][tight][leading] = res;\n}\n\nlong long solve(long long n) {\n    if (n &lt; 0) return 0;\n    len = 0;\n    while (n &gt; 0) {\n        digits[len++] = n % 10;\n        n /= 10;\n    }\n    if (len == 0) digits[len++] = 0;\n    for (int i = 0; i &lt; len / 2; i++) {\n        int t = digits[i];\n        digits[i] = digits[len - 1 - i];\n        digits[len - 1 - i] = t;\n    }\n    memset(vis, 0, sizeof(vis));\n    return dp(0, 10, 1, 1);\n}\n\nint main(void) {\n    long long N;\n    scanf(\"%lld\", &N);\n    printf(\"%lld\\n\", solve(N));\n}\n\n\nWhy It Matters\n\nBuilds understanding of monotonic digit constraints\nTemplate for counting:\n\nStrictly increasing digits\nStrictly decreasing digits\nNon-decreasing digits (just change condition to d &lt; prev)\n\nAppears in combinatorial enumeration and digit ordering problems\n\n\n\nStep-by-Step Example\nFor \\(N=130\\):\nValid numbers include:\n0–9\n12, 13\n23\n...\n123\nInvalid examples:\n\n11 (equal digits)\n21 (decreasing)\n\nDP automatically filters these based on the d &gt; prev rule.\n\n\nA Gentle Proof (Why It Works)\nEach path corresponds to a unique number ≤ \\(N\\). The rule d &gt; prev enforces strictly increasing order. The DP ensures no overcounting because each prefix (pos, prev, tight, leading) fully determines future choices.\n\n\nTry It Yourself\n\nModify to count strictly decreasing numbers (d &lt; prev).\nCount non-decreasing numbers (d &gt;= prev).\nEnforce exact length \\(k\\) via a len_used parameter.\nAdd mod conditions (sum mod M).\nCompute sum of all increasing numbers instead of count.\n\n\n\nTest Cases\n\n\n\nN\nExpected\nNotes\n\n\n\n\n9\n10\n0–9 valid\n\n\n12\n12\n10 numbers 0–9, plus 12 and 13\n\n\n99\n45\nAll 1–2-digit increasing numbers\n\n\n321\n84\nDerived by DP\n\n\n\n\n\nComplexity\n\nTime: \\(O(len \\times 11 \\times 2 \\times 2 \\times 10)\\)\nSpace: \\(O(len \\times 11 \\times 2 \\times 2)\\)\n\nDigit DP with monotonic digit constraints transforms ordering problems into state-space counting, a fundamental technique for combinatorial digit analysis.\n\n\n\n456 Count with Forbidden Digits\nCount how many integers in a range avoid a given set of forbidden digits. This is a basic Digit DP where the state remembers whether we are still tight to the upper bound and whether we have only placed leading zeros.\n\nWhat Problem Are We Solving?\nGiven \\(N\\) and a set \\(F\\subseteq{0,1,\\dots,9}\\) of forbidden digits, count integers \\(x\\in[0,N]\\) whose standard decimal representation contains no digit from \\(F\\).\nConvention: leading zeros are allowed during the DP but do not count as real digits, so a leading zero is always permitted even if \\(0\\in F\\).\nFor a general range \\([L,R]\\) return \\(f(R)-f(L-1)\\).\n\n\nDP State\nLet \\(S\\) be the digit list of \\(N\\), length \\(m\\).\nState: \\[\ndp[pos][tight][leading]\n\\]\n\n\\(pos\\): index in \\([0,m)\\)\n\\(tight\\in{0,1}\\): 1 if the prefix equals \\(N\\) so far\n\\(leading\\in{0,1}\\): 1 if all chosen digits are leading zeros\n\n\n\nTransition\nAt position \\(pos\\) choose digit \\(d\\in[0,\\text{limit}]\\), where \\(\\text{limit}=S[pos]\\) if \\(tight=1\\) else \\(9\\).\nReject \\(d\\) if it is a real digit that is forbidden:\n\nIf \\(leading=1\\) and \\(d=0\\), accept regardless of \\(F\\).\nOtherwise require \\(d\\notin F\\).\n\nNext state:\n\n\\(next_tight = tight\\land(d=\\text{limit})\\)\n\\(next_leading = leading\\land(d=0)\\)\n\nRecurrence: \\[\ndp[pos][tight][leading] = \\sum_{d=0}^{\\text{limit}} \\mathbf{1}\\big(\\text{allowed}(d,leading)\\big)\\cdot dp[pos+1][next_tight][next_leading]\n\\]\nBase: \\[\ndp[m][tight][leading]=1\n\\]\nAnswer is \\(dp[0][1][1]\\).\n\n\nHow Does It Work (Plain Language)\nWe build the number left to right. If we have not surpassed \\(N\\) yet, the next digit is restricted by \\(N\\) at that position. Leading zeros are virtual padding and do not trigger the forbidden check. The DP counts all valid completions from each prefix.\n\n\nTiny Code (Easy Versions)\nPython\nfrom functools import lru_cache\n\ndef count_without_forbidden(N, forbidden):\n    S = list(map(int, str(N)))\n    m = len(S)\n    F = set(forbidden)\n\n    @lru_cache(None)\n    def dp(pos, tight, leading):\n        if pos == m:\n            return 1\n        limit = S[pos] if tight else 9\n        total = 0\n        for d in range(limit + 1):\n            ntight = tight and (d == limit)\n            nleading = leading and (d == 0)\n            # allow leading zero regardless of F\n            if not nleading and d in F:\n                continue\n            total += dp(pos + 1, ntight, nleading)\n        return total\n\n    return dp(0, True, True)\n\ndef count_range_without_forbidden(L, R, forbidden):\n    if L &lt;= 0:\n        return count_without_forbidden(R, forbidden)\n    return count_without_forbidden(R, forbidden) - count_without_forbidden(L - 1, forbidden)\n\n# Example:\n# print(count_range_without_forbidden(0, 327, {3,4}))\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nlong long memo[20][2][2];\nchar vis[20][2][2];\nint digits[20], len;\nint forbid[10];\n\nlong long dp(int pos, int tight, int leading) {\n    if (pos == len) return 1;\n    if (vis[pos][tight][leading]) return memo[pos][tight][leading];\n    vis[pos][tight][leading] = 1;\n\n    int limit = tight ? digits[pos] : 9;\n    long long total = 0;\n\n    for (int d = 0; d &lt;= limit; d++) {\n        int ntight = tight && (d == limit);\n        int nleading = leading && (d == 0);\n        if (!nleading && forbid[d]) continue; // real digit must not be forbidden\n        total += dp(pos + 1, ntight, nleading);\n    }\n    return memo[pos][tight][leading] = total;\n}\n\nlong long solve_upto(long long N) {\n    if (N &lt; 0) return 0;\n    len = 0;\n    if (N == 0) digits[len++] = 0;\n    while (N &gt; 0) { digits[len++] = (int)(N % 10); N /= 10; }\n    for (int i = 0; i &lt; len/2; i++) {\n        int t = digits[i]; digits[i] = digits[len-1-i]; digits[len-1-i] = t;\n    }\n    memset(vis, 0, sizeof(vis));\n    return dp(0, 1, 1);\n}\n\n// Example main\n// int main(void){\n//     long long L,R; int k,x;\n//     scanf(\"%lld %lld %d\",&L,&R,&k);\n//     memset(forbid,0,sizeof(forbid));\n//     for(int i=0;i&lt;k;i++){ scanf(\"%d\",&x); forbid[x]=1; }\n//     long long ans = solve_upto(R) - solve_upto(L-1);\n//     printf(\"%lld\\n\", ans);\n// }\n\n\nWhy It Matters\n\nCanonical Digit DP that filters digits by a local constraint\nModels problems with digit blacklists, keypad rules, license formats, or numeral-system restrictions\nServes as a base to combine with additional states like digit sum or adjacency constraints\n\n\n\nStep-by-Step Example\nLet \\(F={3,4}\\) and \\(N=327\\).\n\nAt each position, digits \\({3,4}\\) are disallowed unless we are still in leading zeros.\nThe DP explores all prefixes bounded by \\(327\\) and sums valid completions.\nUse the Python snippet to compute the exact count.\n\n\n\nA Gentle Proof (Why It Works)\nEvery number in \\([0,N]\\) corresponds to a unique path of digit choices. The predicate allowed\\((d,leading)\\) ensures that once a real digit is placed, it is not forbidden. The pair \\((pos,tight)\\) ensures we do not exceed \\(N\\). Since subproblems depend only on these three parameters, memoization counts each equivalence class of prefixes exactly once.\n\n\nTry It Yourself\n\nForbid multiple digits, e.g. \\(F={1,3,7}\\).\nForbid a set that includes \\(0\\) and verify that leading zeros still pass.\nCombine with a sum modulo condition by adding a \\(mod\\) state.\nCombine with no adjacent duplicates by adding a \\(prev\\) state.\nSwitch to base \\(B\\) by changing the limit from \\(9\\) to \\(B-1\\).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\\(N\\)\n\\(F\\)\nExpected idea\n\n\n\n\n99\n\\({9}\\)\nCount of two-digit-with-leading-zero numbers using digits 0..8\n\n\n327\n\\({3,4}\\)\nComputed by code\n\n\n1000\n\\({1}\\)\nAll numbers without digit 1\n\n\n0\nany \\(F\\)\n1 (the number 0)\n\n\n\n\n\nComplexity\n\nTime: \\(O(len\\cdot 2\\cdot 2\\cdot 10)\\)\nSpace: \\(O(len\\cdot 2\\cdot 2)\\)\n\nThis pattern is the simplest Digit DP guard: screen digits against a blacklist while handling bounds and leading zeros correctly.\n\n\n\n457 SOS DP Subset Sum\nSum Over Subsets (SOS) DP is a powerful bitmask technique used to precompute values over all subsets of a given mask. One of its core applications is the Subset Sum over bitmasks, efficiently computing \\(f(S) = \\sum_{T \\subseteq S} g(T)\\) for all \\(S\\).\n\nWhat Problem Are We Solving?\nGiven an array g of size \\(2^n\\) indexed by bitmasks, compute a new array f such that:\n\\[\nf[S] = \\sum_{T \\subseteq S} g[T]\n\\]\nA naive approach iterates through all subsets for each \\(S\\), which takes \\(O(3^n)\\). SOS DP reduces this to \\(O(n \\cdot 2^n)\\), making it feasible for \\(n \\le 20\\).\n\n\nHow Does It Work (Plain Language)\nWe treat each bit as a dimension. For each bit position \\(i\\) (from 0 to \\(n-1\\)):\n\nFor each mask \\(S\\):\n\nIf bit \\(i\\) is set in \\(S\\), add contribution from \\(S\\) with bit \\(i\\) cleared.\n\n\nThis accumulates sums over all subsets, one bit at a time.\n\n\nRecurrence\nLet f initially equal g. Then:\n\\[\n\\begin{aligned}\n&\\text{for } i = 0,\\dots,n-1:\\\\\n&\\quad \\text{for } S = 0,\\dots,\\ \\texttt{(1&lt;&lt;n)}-1:\\\\\n&\\quad\\quad \\text{if } \\bigl(S \\mathbin{\\&} \\texttt{(1&lt;&lt;i)}\\bigr) \\ne 0:\\quad\nf[S] \\mathrel{+}= f\\!\\left[S^{\\text{without } i}\\right]\n\\end{aligned}\n\\]\nwhere \\(S^{\\text{without } i} = S \\oplus \\texttt{(1&lt;&lt;i)}\\) removes bit \\(i\\).\nAfter processing all bits, f[S] holds the sum over all subsets of \\(S\\).\n\n\nTiny Code (Easy Versions)\nPython\ndef sos_subset_sum(g, n):\n    f = g[:]  # copy\n    for i in range(n):\n        for S in range(1 &lt;&lt; n):\n            if S & (1 &lt;&lt; i):\n                f[S] += f[S ^ (1 &lt;&lt; i)]\n    return f\n\n# Example\nn = 3\ng = [1,2,3,4,5,6,7,8]  # g[mask]\nf = sos_subset_sum(g, n)\nprint(f)\nC\n#include &lt;stdio.h&gt;\n\nvoid sos_subset_sum(long long f[], int n) {\n    for (int i = 0; i &lt; n; i++) {\n        for (int S = 0; S &lt; (1 &lt;&lt; n); S++) {\n            if (S & (1 &lt;&lt; i)) {\n                f[S] += f[S ^ (1 &lt;&lt; i)];\n            }\n        }\n    }\n}\n\nint main() {\n    int n = 3;\n    long long f[1 &lt;&lt; 3] = {1,2,3,4,5,6,7,8};\n    sos_subset_sum(f, n);\n    for (int i = 0; i &lt; (1 &lt;&lt; n); i++) printf(\"%lld \", f[i]);\n    return 0;\n}\n\n\nWhy It Matters\n\nFoundation for bitmask DP transforms (e.g. subset convolution, inclusion-exclusion).\nEnables fast enumeration of subset properties (sums, counts, etc.).\nReusable building block in probabilistic DP, polynomial transforms, and game DP.\n\n\n\nStep-by-Step Example\nLet \\(n=2\\), masks \\(00,01,10,11\\) and \\(g=[1,2,3,4]\\):\nInitialize \\(f=g\\).\n\nBit \\(i=0\\):\n\n\\(S=01\\): \\(f[01]+=f[00] \\implies 2+1=3\\)\n\\(S=11\\): \\(f[11]+=f[10] \\implies 4+3=7\\)\n\n\n\\(f=[1,3,3,7]\\)\n\nBit \\(i=1\\):\n\n\\(S=10\\): \\(f[10]+=f[00] \\implies 3+1=4\\)\n\\(S=11\\): \\(f[11]+=f[01] \\implies 7+3=10\\)\n\n\nFinal \\(f=[1,3,4,10]\\)\nCheck:\n\n\\(f[11] = g[00]+g[01]+g[10]+g[11] = 1+2+3+4=10\\) ✓\n\n\n\nA Gentle Proof (Why It Works)\nEach bit is processed independently. At iteration \\(i\\), each mask \\(S\\) accumulates contributions from all subsets differing only at bit \\(i\\). After processing all bits, every subset \\(T\\subseteq S\\) is visited exactly once.\nBy induction:\n\nBase: \\(i=0\\), \\(f[S]\\) contains \\(g[S]\\).\nStep: adding \\(f[S^{\\text{without }i}]\\) ensures inclusion of subsets missing bit \\(i\\).\n\nThus, after \\(n\\) passes, \\(f[S]\\) sums over all subsets.\n\n\nTry It Yourself\n\nChange sum to product (if \\(f[S]*=f[S^{\\text{without }i}]\\)).\nCompute \\(f[S]=\\sum_{T\\supseteq S} g[T]\\) (see Superset DP).\nCombine SOS DP with inclusion-exclusion to count valid subsets.\nApply to subset convolution problems.\nUse modulo arithmetic for large values.\n\n\n\nTest Cases\n\n\n\nn\ng (input)\nf (output)\n\n\n\n\n2\n[1,2,3,4]\n[1,3,4,10]\n\n\n3\n[1,1,1,1,1,1,1,1]\n[1,2,2,4,2,4,4,8]\n\n\n3\n[0,1,2,3,4,5,6,7]\ncomputed by code\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\cdot 2^n)\\)\nSpace: \\(O(2^n)\\)\n\nSOS DP transforms the exponential subset-sum enumeration into a structured linear pass across dimensions, making subset-based computation tractable for small \\(n\\).\n\n\n\n458 SOS DP Superset Sum\nSum Over Supersets computes for every mask the aggregate over all of its supersets. It complements the usual SOS DP over subsets.\n\nWhat Problem Are We Solving?\nGiven an array g of size \\(2^n\\) indexed by bitmasks, compute h such that \\[\nh[S]=\\sum_{T\\supseteq S}g[T].\n\\] Naively this is \\(O(3^n)\\). With SOS Superset DP it is \\(O(n\\cdot 2^n)\\).\n\n\nHow Does It Work (Plain Language)\nProcess bits one by one. For each bit \\(i\\), if a mask \\(S\\) has bit \\(i\\) unset, then every superset that turns this bit on is of the form \\(S\\cup{i}=S\\oplus(1&lt;&lt;i)\\). So we can accumulate from that neighbor upward.\n\n\nRecurrence\nInitialize \\(h = g\\). For each bit \\(i = 0..n-1\\):\nFor every mask \\(S\\):\nIf \\((S \\mathbin{\\&} (1 &lt;&lt; i)) == 0\\), then \\[\n    h[S] += h[S \\oplus (1 &lt;&lt; i)].\n    \\] After all bits, \\(h[S]\\) equals the sum over all supersets of \\(S\\).\n\n\nTiny Code\nPython\ndef sos_superset_sum(g, n):\n    h = g[:]  # copy\n    for i in range(n):\n        for S in range(1 &lt;&lt; n):\n            if (S & (1 &lt;&lt; i)) == 0:\n                h[S] += h[S | (1 &lt;&lt; i)]\n    return h\n\n# Example\nn = 3\ng = [1,2,3,4,5,6,7,8]  # g[mask]\nh = sos_superset_sum(g, n)\nprint(h)\nC\n#include &lt;stdio.h&gt;\n\nvoid sos_superset_sum(long long h[], int n){\n    for(int i=0;i&lt;n;i++){\n        for(int S=0;S&lt;(1&lt;&lt;n);S++){\n            if((S&(1&lt;&lt;i))==0){\n                h[S]+=h[S|(1&lt;&lt;i)];\n            }\n        }\n    }\n}\n\nint main(){\n    int n=3;\n    long long h[1&lt;&lt;3]={1,2,3,4,5,6,7,8};\n    sos_superset_sum(h,n);\n    for(int i=0;i&lt;(1&lt;&lt;n);i++) printf(\"%lld \", h[i]);\n    return 0;\n}\n\n\nWhy It Matters\n\nDual of subset SOS DP.\nCore for transforms like zeta and Möbius on the subset lattice.\nUseful for queries like: for each feature set \\(S\\), aggregate values over all supersets that contain \\(S\\).\n\n\n\nStep-by-Step Example\nLet \\(n=2\\), masks \\(00,01,10,11\\) and \\(g=[1,2,3,4]\\).\nProcess bit \\(i=0\\):\n\n\\(S=00\\): \\(h[00]+=h[01]\\Rightarrow 1+2=3\\)\n\\(S=10\\): \\(h[10]+=h[11]\\Rightarrow 3+4=7\\)\n\nNow \\(h=[3,2,7,4]\\).\nProcess bit \\(i=1\\):\n\n\\(S=00\\): \\(h[00]+=h[10]\\Rightarrow 3+7=10\\)\n\\(S=01\\): \\(h[01]+=h[11]\\Rightarrow 2+4=6\\)\n\nFinal \\(h=[10,6,7,4]\\), which matches\n\n\\(h[00]=g[00]+g[01]+g[10]+g[11]=10\\)\n\\(h[01]=g[01]+g[11]=6\\)\n\\(h[10]=g[10]+g[11]=7\\)\n\\(h[11]=g[11]=4\\).\n\n\n\nA Gentle Proof (Why It Works)\nFix a bit order. When processing bit \\(i\\), for any \\(S\\) with bit \\(i\\) unset, every superset of \\(S\\) either keeps bit \\(i\\) off or turns it on. Before processing \\(i\\), \\(h[S]\\) accumulates supersets with bit \\(i\\) off. Adding \\(h[S\\cup{i}]\\) brings in all supersets with bit \\(i\\) on. Induct over bits to conclude all supersets are included exactly once.\n\n\nTry It Yourself\n\nConvert this to compute maximum over supersets by replacing plus with max.\nCombine with subset SOS to precompute both directions for fast subset-superset queries.\nApply modulo arithmetic to prevent overflow.\nImplement the Möbius inversion on supersets to invert the transform.\nExtend to bitwise operations where aggregation depends on bit counts.\n\n\n\nTest Cases\n\n\n\nn\ng input\nh output\n\n\n\n\n2\n[1,2,3,4]\n[10,6,7,4]\n\n\n3\nall 1s\n[8,4,4,2,4,2,2,1]\n\n\n3\n[0,1,2,3,4,5,6,7]\ncomputed by code\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\cdot 2^n)\\)\nSpace: \\(O(2^n)\\)\n\nSOS Superset DP is the natural mirror of subset SOS. Use it whenever queries demand aggregating over all sets that contain a given mask.\n\n\n\n459 XOR Basis DP\nThe XOR Basis DP technique helps count or generate all possible XOR values from a set of numbers efficiently. It constructs a linear basis over GF(2) and enables solving problems like counting distinct XORs, finding minimum/maximum XOR, and combining with digit DP or bitmask states.\n\nWhat Problem Are We Solving?\nGiven a list of numbers \\(A = [a_1, a_2, \\dots, a_n]\\), we want to:\n\nFind how many distinct XOR values can be formed from subsets of \\(A\\).\nOr find maximum/minimum possible XOR.\nOr answer queries on possible XOR combinations.\n\nThe XOR operation forms a vector space over \\(\\mathbb{F}_2\\), and each number contributes a vector. The XOR basis provides a compact representation of all subset XORs.\n\n\nCore Idea\nMaintain an array basis representing independent bit vectors. Insert each number into the basis (Gaussian elimination over GF(2)):\n\nFor each bit from high to low, if that bit is set and not represented, store the number.\nIf it is already represented, XOR with the current basis vector to reduce it.\n\nAt the end, the number of independent vectors is the rank \\(r\\), and the number of distinct XORs is \\(2^r\\).\n\n\nDP Perspective\nThe state represents a basis built from a prefix of the array. You can define:\n\\[\ndp[i] = \\text{XOR basis after processing first } i \\text{ elements}\n\\]\nTo count distinct XORs after all elements:\n\\[\n\\text{count} = 2^{\\text{rank}}\n\\]\nIf you need to build combinations (e.g. count of XOR &lt; M), combine basis construction with digit DP constraints.\n\n\nTiny Code (Easy Versions)\nPython\ndef xor_basis(arr):\n    basis = []\n    for x in arr:\n        for b in basis:\n            x = min(x, x ^ b)\n        if x:\n            basis.append(x)\n    return basis\n\ndef count_distinct_xors(arr):\n    basis = xor_basis(arr)\n    return 1 &lt;&lt; len(basis)  # 2^rank\n\n# Example\nA = [3, 10, 5]\nbasis = xor_basis(A)\nprint(\"Basis:\", basis)\nprint(\"Distinct XORs:\", count_distinct_xors(A))\nC\n#include &lt;stdio.h&gt;\n\nint insert_basis(int basis[], int *sz, int x) {\n    for (int i = 0; i &lt; *sz; i++) {\n        if ((x ^ basis[i]) &lt; x) x ^= basis[i];\n    }\n    if (x == 0) return 0;\n    basis[(*sz)++] = x;\n    return 1;\n}\n\nint main() {\n    int arr[] = {3, 10, 5};\n    int n = 3, basis[32], sz = 0;\n\n    for (int i = 0; i &lt; n; i++)\n        insert_basis(basis, &sz, arr[i]);\n\n    printf(\"Rank: %d\\nDistinct XORs: %d\\n\", sz, 1 &lt;&lt; sz);\n}\n\n\nWhy It Matters\n\nForms the foundation for subset XOR problems.\nUsed in:\n\nCounting distinct XORs\nMaximum XOR subset\nXOR-constrained digit DP\nGaussian elimination in \\(\\mathbb{F}_2\\)\n\n\nIt’s the bitwise analog of linear algebra, solving over GF(2).\n\n\nStep-by-Step Example\nLet \\(A = [3, 10, 5]\\):\n\nBinary: \\(3=011\\), \\(10=1010\\), \\(5=0101\\)\nInsert 3 → basis = {3}\nInsert 10 → independent, basis = {3,10}\nInsert 5 → can be reduced: \\(5⊕3=6\\), \\(6⊕10=12\\) → independent, basis = {3,10,5}\n\nRank \\(r=3\\), number of distinct XORs = \\(2^3=8\\).\nAll subset XORs:\n0, 3, 5, 6, 10, 11, 12, 15\n\n\nA Gentle Proof (Why It Works)\nEach basis vector represents a new independent bit dimension. Every subset XOR corresponds to a linear combination over \\(\\mathbb{F}_2\\) of the basis. If there are \\(r\\) independent vectors, there are \\(2^r\\) possible linear combinations (subsets), hence \\(2^r\\) distinct XORs.\n\n\nTry It Yourself\n\nModify to find maximum XOR subset (XOR greedily from MSB down).\nCombine with digit DP to count numbers with XOR constraints (\\(&lt; M\\)).\nTrack reconstruction: which subset forms a target XOR.\nApply to path XOR queries in trees (via prefix basis merging).\nExtend to multiset bases or online updates.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,10,5]\n8\n3 independent vectors\n\n\n[1,2,3]\n4\nRank = 2\n\n\n[1,1,1]\n2\nRank = 1\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\cdot \\text{bitwidth})\\)\nSpace: \\(O(\\text{bitwidth})\\)\n\nXOR Basis DP is the digital geometry of sets under XOR, every number a vector, every subset a linear combination, every question a path through binary space.\n\n\n\n460 Digit DP for Palindromes\nDigit DP for Palindromes counts all numbers within a given range that are palindromic, numbers that read the same forward and backward. It’s a symmetric DP that constructs digits from both ends simultaneously, respecting tight bounds from the original number.\n\nWhat Problem Are We Solving?\nGiven two integers \\(L\\) and \\(R\\), count the palindromes in \\([L, R]\\).\nExample: in \\([1, 200]\\), palindromes are \\(1,2,\\dots,9,11,22,\\dots,99,101,111,\\dots,191\\), total 28.\nNaively iterating and checking each number is \\(O(N)\\), which is too slow for large \\(N\\). We want an \\(O(d \\times 10^{d/2})\\) approach using Digit DP with symmetry.\n\n\nCore Idea\nWe can build a number digit by digit from the outside in, ensuring the number remains a palindrome at each step.\nFor a given length \\(len\\), we only need to choose digits for the first half; the second half is determined.\nThe tight constraints ensure we stay \\(\\le R\\) (or \\(\\le L-1\\) for inclusive ranges).\n\n\nDP Definition\nLet \\(S\\) = digits of \\(N\\), length \\(len\\).\nState: \\[\ndp[pos][tight][leading]\n\\]\nWhere:\n\npos is the current index from the left half (\\(0 \\le pos &lt; \\frac{len}{2}\\))\ntight means prefix equals \\(N\\) so far\nleading means we’ve placed only leading zeros\n\nThe recursion places one digit at pos, mirrors it at len-1-pos, and recurses inward.\n\n\nTransition\nFor each digit in \\([0, limit]\\):\n\nIf leading and digit==0, we can skip counting it as a real digit.\nMirror digit into the symmetric position.\nUpdate tightness if digit == limit.\nRecurse inward until halfway.\n\nWhen reaching middle, count 1 valid palindrome.\n\n\nAlgorithm Outline\nTo count palindromes ≤ \\(N\\):\n\nConvert \\(N\\) to string S\nRun dfs(pos=0, tight=True, leading=True)\nIf building full palindrome (not half-only), check mirrored structure\n\nCount in \\([L, R]\\) as: \\[\nf(R) - f(L-1)\n\\]\n\n\nTiny Code (Easy Versions)\nPython\nfrom functools import lru_cache\n\ndef count_palindromes_upto(N):\n    S = list(map(int, str(N)))\n    n = len(S)\n\n    @lru_cache(None)\n    def dfs(pos, tight, leading, half):\n        if pos == (n + 1) // 2:\n            return 1  # one palindrome formed\n        limit = S[pos] if tight else 9\n        total = 0\n        for d in range(limit + 1):\n            if leading and d == 0:\n                total += dfs(pos + 1, tight and d == limit, True, half + [0])\n            else:\n                total += dfs(pos + 1, tight and d == limit, False, half + [d])\n        return total\n\n    return dfs(0, True, True, ())\n\ndef count_palindromes(L, R):\n    return count_palindromes_upto(R) - count_palindromes_upto(L - 1)\n\n# Example\nprint(count_palindromes(1, 200))\nC (half-construction)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nlong long count_palindromes_upto(long long N) {\n    if (N &lt; 0) return 0;\n    char s[20];\n    sprintf(s, \"%lld\", N);\n    int len = strlen(s);\n    int half = (len + 1) / 2;\n    long long count = 0;\n\n    // Build prefix half and mirror\n    for (int mask = 0; mask &lt; (1 &lt;&lt; (half * 4)); mask++) {\n        // Conceptual only, use recursion or base-10 enumeration for actual code\n    }\n\n    // Simpler approach: iterate half, build full palindrome, check ≤ N\n    long long start = 1;\n    for (int i = 1; i &lt;= len; i++) {\n        int half_len = (i + 1) / 2;\n        long long base = 1;\n        for (int j = 1; j &lt; half_len; j++) base *= 10;\n        for (long long x = base; x &lt; base * 10; x++) {\n            long long y = x;\n            if (i % 2) y /= 10;\n            long long z = x;\n            while (y &gt; 0) {\n                z = z * 10 + (y % 10);\n                y /= 10;\n            }\n            if (z &lt;= N) count++;\n        }\n    }\n    return count;\n}\n\nint main(void) {\n    long long L = 1, R = 200;\n    printf(\"%lld\\n\", count_palindromes_upto(R) - count_palindromes_upto(L - 1));\n}\n\n\nWhy It Matters\n\nPalindromic counting appears in:\n\nDigit constraints (e.g. special number sets)\nSymmetric number combinatorics\nPatterned sequence generation\n\nBuilds intuition for bidirectional DP where digits mirror.\n\n\n\nStep-by-Step Example\nLet \\(N = 200\\):\n\nLength 3 → half = 2\nChoose first half (00–19), mirror → 00–99 → 0,11,22,...,191\nApply bounds: only ≤200 → count = 28\n\n\n\nA Gentle Proof (Why It Works)\nEvery palindrome is uniquely determined by its first half. For each valid half respecting the upper bound, exactly one mirrored number exists. Tightness ensures no overflow beyond \\(N\\). Leading-zero handling ensures numbers like 00100 are excluded.\n\n\nTry It Yourself\n\nCount even-length palindromes only.\nCount palindromes with a fixed digit sum.\nModify to count palindromic primes by adding primality check.\nCombine with digit constraints (e.g. no 3s).\nCount numbers that become palindromes after reversal operations.\n\n\n\nTest Cases\n\n\n\nL\nR\nOutput\nNotes\n\n\n\n\n1\n9\n9\nSingle-digit palindromes\n\n\n10\n99\n9\n11,22,…,99\n\n\n1\n200\n28\nUp to 191\n\n\n100\n999\n90\nAll 3-digit palindromes\n\n\n\n\n\nComplexity\n\nTime: \\(O(d \\times 10^{d/2})\\)\nSpace: \\(O(d)\\)\n\nDigit DP for Palindromes bridges arithmetic and symmetry, constructing mirrored structures digit by digit under bound constraints.\n\n\n\n\nSection 47. DP Optimizations\n\n461 Divide & Conquer DP (Monotone Optimization)\nWhen a DP transition is a 1D convolution of the form \\(dp[i][j]=\\min\\limits_{k&lt;j}\\big(dp[i-1][k]+C(k,j)\\big)\\), and the argmin is monotone (\\(opt[i][j]\\le opt[i][j+1]\\)), you can compute a whole layer \\(dp[i][*]\\) in \\(O(n)\\) splits with divide and conquer instead of \\(O(n^2)\\). Typical total is \\(O(K,n\\log n)\\) or \\(O(K,n)\\) depending on implementation.\n\nWhat Problem Are We Solving?\nSpeed up DP layers with transitions \\[\ndp[j]=\\min_{k&lt;j}\\big(prev[k]+C(k,j)\\big),\n\\] for \\(j\\) in an interval, where the optimal index \\(opt[j]\\in[\\text{optL},\\text{optR}]\\) and satisfies \\[\nopt[j]\\le opt[j+1]\\quad\\text{(monotone decision property).}\n\\]\nThis structure appears in:\n\n\\(K\\)-partitioning of arrays with convex segment cost\n1D facility placement and line breaking with convex penalties\nSome shortest path on DAG layers with convex arc costs\n\n\n\nHow Does It Work (Plain Language)\nCompute the current DP layer on a segment \\([L,R]\\) by solving the midpoint \\(M\\), searching its best \\(k\\) only in \\([optL,optR]\\). The best index \\(opt[M]\\) splits the problem:\n\nLeft half \\([L,M-1]\\) only needs candidates in \\([optL,opt[M]]\\)\nRight half \\([M+1,R]\\) only needs \\([opt[M],optR]\\)\n\nRecursively repeat until intervals are size 1. Monotonicity guarantees these candidate ranges.\n\n\nPreconditions checklist\nYou can use divide and conquer DP if:\n\nTransition is \\(dp[j]=\\min_{k&lt;j}(prev[k]+C(k,j))\\).\nThe optimal index is monotone in \\(j\\). A sufficient condition is quadrangle inequality or Monge property of \\(C\\): \\[\nC(a,c)+C(b,d)\\le C(a,d)+C(b,c)\\quad\\text{for }a\\le b\\le c\\le d.\n\\]\n\n\n\nTiny Code (Template)\nC++ style pseudocode (drop in C with minor edits)\n// Compute one layer dp_cur[lo..hi], given dp_prev and cost C(k,j).\n// Assumes optimal indices are monotone.\nvoid compute(int lo, int hi, int optL, int optR,\n             const vector&lt;long long&gt;& dp_prev,\n             vector&lt;long long&gt;& dp_cur,\n             auto&& cost) {\n    if (lo &gt; hi) return;\n    int mid = (lo + hi) &gt;&gt; 1;\n\n    long long best = LLONG_MAX;\n    int best_k = -1;\n    int start = optL, end = min(optR, mid - 1);\n    for (int k = start; k &lt;= end; ++k) {\n        long long val = dp_prev[k] + cost(k, mid);\n        if (val &lt; best) {\n            best = val;\n            best_k = k;\n        }\n    }\n    dp_cur[mid] = best;\n\n    // Recurse with narrowed opt ranges\n    compute(lo, mid - 1, optL, best_k, dp_prev, dp_cur, cost);\n    compute(mid + 1, hi, best_k, optR, dp_prev, dp_cur, cost);\n}\nPython (clear and compact)\nINF = 1018\n\ndef compute(lo, hi, optL, optR, dp_prev, dp_cur, cost):\n    if lo &gt; hi:\n        return\n    mid = (lo + hi) // 2\n    best_val, best_k = INF, -1\n    end = min(optR, mid - 1)\n    for k in range(optL, end + 1):\n        v = dp_prev[k] + cost(k, mid)\n        if v &lt; best_val:\n            best_val, best_k = v, k\n    dp_cur[mid] = best_val\n    compute(lo, mid - 1, optL, best_k, dp_prev, dp_cur, cost)\n    compute(mid + 1, hi, best_k, optR, dp_prev, dp_cur, cost)\nHow to use For \\(i=1..K\\): call compute(1,n,0,n-1, dp_prev, dp_cur, cost) then swap layers. Index ranges depend on your base cases.\n\n\nExample: K partitions with convex segment cost\nGiven array \\(a[1..n]\\), let prefix sums \\(S[j]=\\sum_{t=1}^j a[t]\\). Suppose segment cost is \\[\nC(k,j)=\\big(S[j]-S[k]\\big)^2,\n\\] which is convex and satisfies quadrangle inequality. The DP \\[\ndp[i][j]=\\min_{k&lt;j}\\big(dp[i-1][k]+C(k,j)\\big)\n\\] has monotone argmins, so one layer can be computed with the template. Total roughly \\(O(K,n\\log n)\\).\n\n\nWhy It Matters\n\nCuts a quadratic DP layer down to near linear\nSimple to implement compared to more advanced tricks\nPairs well with prefix sums for \\(C(k,j)\\) evaluation\nCore technique in editors line breaking, clustering in 1D, histogram smoothing\n\n\n\nStep by Step on a small instance\nLet \\(a=[1,3,2,4]\\), \\(K=2\\), \\(C(k,j)=(S[j]-S[k])^2\\).\n\nInitialize \\(dp[0][0]=0\\), \\(dp[0][j&gt;0]=+\\infty\\).\nLayer \\(i=1\\): compute \\(dp[1][j]\\) by scanning \\(k&lt;j\\). Argmins are nondecreasing.\nLayer \\(i=2\\): call compute(1,n,0,n-1, ...). The recursion halves the target interval and narrows candidate \\(k\\) ranges by monotonicity.\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(opt[j]\\) be a minimizer for \\(dp[j]\\). If \\(C\\) is Monge, then for \\(x&lt;y\\) and \\(u&lt;v\\): \\[\nC(u,x)+C(v,y)\\le C(u,y)+C(v,x).\n\\] Assume by contradiction \\(opt[x]&gt;opt[y]\\). Using optimality of those indices and the inequality above yields a contradiction. Hence \\(opt\\) is nondecreasing.\nThe recursion evaluates \\(dp[mid]\\) using candidates in \\([optL,optR]\\). The found \\(opt[mid]\\) splits feasible candidates:\n\nAny optimal index for \\(j&lt;mid\\) is in \\([optL,opt[mid]]\\)\nAny optimal index for \\(j&gt;mid\\) is in \\([opt[mid],optR]\\) Induct over segments to show each \\(dp[j]\\) is computed with exactly the needed candidate set and no essential candidate is excluded.\n\n\n\nTry It Yourself\n\nReplace \\(C(k,j)\\) by \\(\\alpha,(S[j]-S[k])^2+\\beta,(j-k)\\) and verify monotonicity still holds.\nUse the template to speed up line breaking with raggedness penalty.\nBenchmark naive \\(O(n^2)\\) vs divide and conquer on random convex costs.\nCombine with space optimization by keeping only two layers.\nContrast with Knuth optimization and Convex Hull Trick and decide which applies for your cost.\n\n\n\nTest Cases\n\nSmall convex cost:\n\n\\(a=[2,1,3], K=2\\), \\(C(k,j)=(S[j]-S[k])^2\\)\nCompare naive and D&C outputs, they must match.\n\nLinear plus convex mix:\n\n\\(a=[1,1,1,1], K=3\\), \\(C(k,j)=(S[j]-S[k])^2+(j-k)\\)\n\nEdge cases:\n\n\\(n=1\\) any \\(K\\ge1\\)\nAll zeros array, any \\(K\\)\n\n\n\n\nComplexity\n\nOne layer with divide and conquer: \\(O(n\\log n)\\) evaluations of \\(C\\) in the simple form, often written as \\(O(n)\\) splits with logarithmic recursion depth.\nFull DP: \\(O(K,n\\log n)\\) time, \\(O(n)\\) or \\(O(K,n)\\) space depending on whether you store all layers.\n\nDivide and conquer DP is your go to when the argmin slides to the right as \\(j\\) grows. It is small code, big speedup.\n\n\n\n462 Knuth Optimization\nKnuth Optimization is a special-case speedup for certain DP transitions where quadrangle inequality and monotonicity of optimal decisions hold. It improves \\(O(n^2)\\) dynamic programs to \\(O(n)\\) per layer, often used in optimal binary search tree, matrix chain, and interval partitioning problems.\n\nWhat Problem Are We Solving?\nWe want to optimize DP recurrences of the form: \\[\ndp[i][j] = \\min_{k \\in [i, j-1]} \\big(dp[i][k] + dp[k+1][j] + C(i, j)\\big)\n\\] for \\(1 \\le i \\le j \\le n\\), where \\(C(i,j)\\) is a cost function satisfying quadrangle inequality.\nNaively, this is \\(O(n^3)\\). With Knuth optimization, we cut it to \\(O(n^2)\\) by restricting the search range using a monotonic property of the optimal split point.\n\n\nKey Condition\nKnuth optimization applies when:\n\nQuadrangle Inequality: \\[\nC(a, c) + C(b, d) \\le C(a, d) + C(b, c), \\quad \\forall a \\le b \\le c \\le d\n\\]\nMonotonicity of Argmin: \\[\nopt[i][j-1] \\le opt[i][j] \\le opt[i+1][j]\n\\] These ensure that the optimal split \\(k\\) for \\([i,j]\\) moves rightward as intervals slide.\n\n\n\nHow Does It Work (Plain Language)\nWhen computing \\(dp[i][j]\\), the best partition point \\(k\\) lies between \\(opt[i][j-1]\\) and \\(opt[i+1][j]\\). So instead of scanning the full \\([i, j-1]\\), we limit to a narrow window. This reduces work from \\(O(n^3)\\) to \\(O(n^2)\\).\nWe fill intervals in increasing length order, maintaining and reusing opt[i][j].\n\n\nStep-by-Step Recurrence\nFor all \\(i\\): \\[\ndp[i][i] = 0\n\\]\nThen for lengths \\(len = 2..n\\):\nfor i in 1..n-len+1:\n    j = i + len - 1\n    dp[i][j] = ∞\n    for k in opt[i][j-1]..opt[i+1][j]:\n        val = dp[i][k] + dp[k+1][j] + C(i,j)\n        if val &lt; dp[i][j]:\n            dp[i][j] = val\n            opt[i][j] = k\n\n\nTiny Code (Easy Versions)\nPython\nINF = 1018\n\ndef knuth_optimization(n, cost):\n    dp = [[0]*n for _ in range(n)]\n    opt = [[0]*n for _ in range(n)]\n\n    for i in range(n):\n        opt[i][i] = i\n\n    for length in range(2, n+1):\n        for i in range(0, n-length+1):\n            j = i + length - 1\n            dp[i][j] = INF\n            start = opt[i][j-1]\n            end = opt[i+1][j] if i+1 &lt;= j else j-1\n            for k in range(start, end+1):\n                val = dp[i][k] + dp[k+1][j] + cost(i, j)\n                if val &lt; dp[i][j]:\n                    dp[i][j] = val\n                    opt[i][j] = k\n    return dp[0][n-1]\nC\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define INF 1000000000000000LL\n#define N 505\n\nlong long dp[N][N], opt[N][N];\n\nlong long cost(int i, int j); // user-defined cost function\n\nlong long knuth(int n) {\n    for (int i = 0; i &lt; n; i++) {\n        dp[i][i] = 0;\n        opt[i][i] = i;\n    }\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 0; i + len - 1 &lt; n; i++) {\n            int j = i + len - 1;\n            dp[i][j] = INF;\n            int start = opt[i][j-1];\n            int end = opt[i+1][j];\n            if (end == 0) end = j - 1;\n            if (start &gt; end) { int tmp = start; start = end; end = tmp; }\n            for (int k = start; k &lt;= end; k++) {\n                long long val = dp[i][k] + dp[k+1][j] + cost(i, j);\n                if (val &lt; dp[i][j]) {\n                    dp[i][j] = val;\n                    opt[i][j] = k;\n                }\n            }\n        }\n    }\n    return dp[0][n-1];\n}\n\n\nWhy It Matters\nKnuth Optimization turns a cubic DP into quadratic without approximations. It’s especially useful for:\n\nOptimal Binary Search Trees (OBST)\nMatrix Chain Multiplication\nMerging Stones / File Merging\nBracket Parsing / Partitioning\n\nIt’s a precise algebraic optimization based on Monge arrays and convexity.\n\n\nStep-by-Step Example\nConsider merging files with sizes \\([10,20,30]\\). \\(C(i,j)=\\text{sum of file sizes from i to j}\\). We fill \\(dp[i][j]\\) with minimal total cost of merging segment \\([i..j]\\). Argmins move monotonically, so Knuth optimization applies.\n\n\nA Gentle Proof (Why It Works)\nIf \\(C\\) satisfies quadrangle inequality: \\[\nC(a, c) + C(b, d) \\le C(a, d) + C(b, c),\n\\] then combining two adjacent subproblems will never cause the optimal cut to move left. Hence, \\(opt[i][j-1] \\le opt[i][j] \\le opt[i+1][j]\\). Thus, restricting the search range preserves correctness while cutting redundant checks.\n\n\nTry It Yourself\n\nApply Knuth optimization to Optimal BST: \\[\ndp[i][j]=\\min_{k\\in[i,j]}(dp[i][k-1]+dp[k+1][j]+w[i][j])\n\\]\nUse it in Merging Stones (sum-cost merge).\nCompare with Divide & Conquer DP, both need monotonicity, but Knuth’s has fixed quadratic structure.\nVerify monotonicity by printing \\(opt[i][j]\\).\nProve \\(C(i,j)=\\text{prefix}[j]-\\text{prefix}[i-1]\\) satisfies the condition.\n\n\n\nTest Cases\n\n\n\nCase\nDescription\nExpected Complexity\n\n\n\n\nFile merging\n[10, 20, 30]\n\\(O(n^2)\\)\n\n\nOptimal BST\nSorted keys, frequencies\n\\(O(n^2)\\)\n\n\nMerging Stones\nEqual weights\nMonotone \\(opt\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(n^2)\\)\n\nKnuth Optimization is the elegant midpoint between full DP and convexity tricks, precise, predictable, and optimal whenever cost satisfies Monge structure.\n\n\n\n463 Convex Hull Trick (CHT)\nThe Convex Hull Trick speeds up DP transitions of the form \\[\ndp[i] = \\min_{k&lt;i}\\big(dp[k] + m_k \\cdot x_i + b_k\\big)\n\\] when the slopes \\(m_k\\) are monotonic (increasing or decreasing) and the \\(x_i\\) queries are also monotonic. It replaces \\(O(n^2)\\) scanning with \\(O(n)\\) amortized or \\(O(\\log n)\\) query time using a dynamic hull.\n\nWhat Problem Are We Solving?\nWe have a DP recurrence like: \\[\ndp[i] = \\min_{k &lt; i}\\big(dp[k] + m_k \\cdot x_i + b_k\\big)\n\\] where:\n\n\\(m_k\\) (slope) and \\(b_k\\) (intercept) define lines,\n\\(x_i\\) is the query coordinate,\nwe want the minimum (or maximum) value over all \\(k\\).\n\nThis appears in:\n\nLine DP (e.g. segmented linear costs),\nDivide and Conquer DP (convex variant),\nKnuth-like DPs with linear penalties,\nAliens trick and Li Chao Trees for non-monotone cases.\n\n\n\nWhen It Applies\n\nTransition fits \\(dp[i] = \\min_k(dp[k] + m_k x_i + b_k)\\)\n\\(m_k\\)’s are monotonic (non-decreasing or non-increasing)\n\\(x_i\\) queries are sorted (non-decreasing)\n\nThen you can use a deque-based CHT for \\(O(1)\\) amortized per insertion/query.\nIf slopes or queries are not monotonic, use Li Chao Tree (next algorithm).\n\n\nHow Does It Work (Plain Language)\nEach \\(k\\) defines a line \\(y = m_k x + b_k\\). The DP asks: for current \\(x_i\\), which previous line gives the smallest value? All lines together form a lower envelope, a piecewise minimum curve. We maintain this hull incrementally and query the minimum efficiently.\nIf slopes are sorted, each new line intersects the previous hull at one point, and old lines become useless after their intersection point.\n\n\nTiny Code (Easy Versions)\nPython (Monotone CHT)\nclass CHT:\n    def __init__(self):\n        self.lines = []  # (m, b)\n    \n    def bad(self, l1, l2, l3):\n        # Check if l2 is unnecessary between l1 and l3\n        return (l3[1] - l1[1]) * (l1[0] - l2[0]) &lt;= (l2[1] - l1[1]) * (l1[0] - l3[0])\n\n    def add(self, m, b):\n        self.lines.append((m, b))\n        while len(self.lines) &gt;= 3 and self.bad(self.lines[-3], self.lines[-2], self.lines[-1]):\n            self.lines.pop(-2)\n\n    def query(self, x):\n        # queries x in increasing order\n        while len(self.lines) &gt;= 2 and \\\n              self.lines[0][0]*x + self.lines[0][1] &gt;= self.lines[1][0]*x + self.lines[1][1]:\n            self.lines.pop(0)\n        m, b = self.lines[0]\n        return m*x + b\n\n# Example: dp[i] = min(dp[k] + m[k]*x[i] + b[k])\ndp = [0]*5\nx = [1, 2, 3, 4, 5]\nm = [2, 1, 3, 4, 5]\nb = [5, 4, 3, 2, 1]\n\ncht = CHT()\ncht.add(m[0], b[0])\nfor i in range(1, 5):\n    dp[i] = cht.query(x[i-1])\n    cht.add(m[i], dp[i] + b[i])\nC (Deque Implementation)\n#include &lt;stdio.h&gt;\n\ntypedef struct { long long m, b; } Line;\nLine hull[100005];\nint sz = 0, ptr = 0;\n\ndouble intersect(Line a, Line b) {\n    return (double)(b.b - a.b) / (a.m - b.m);\n}\n\nint bad(Line a, Line b, Line c) {\n    return (c.b - a.b)*(a.m - b.m) &lt;= (b.b - a.b)*(a.m - c.m);\n}\n\nvoid add_line(long long m, long long b) {\n    Line L = {m, b};\n    while (sz &gt;= 2 && bad(hull[sz-2], hull[sz-1], L)) sz--;\n    hull[sz++] = L;\n}\n\nlong long query(long long x) {\n    while (ptr + 1 &lt; sz && hull[ptr+1].m * x + hull[ptr+1].b &lt;= hull[ptr].m * x + hull[ptr].b)\n        ptr++;\n    return hull[ptr].m * x + hull[ptr].b;\n}\n\n\nWhy It Matters\n\nReduces \\(O(n^2)\\) DP with linear transition to \\(O(n)\\) or \\(O(n\\log n)\\).\nExtremely common in optimization tasks:\n\nConvex cost partitioning\nSlope trick extensions\nDynamic programming with linear penalties\n\nFoundation for Li Chao Trees and Slope Trick.\n\n\n\nStep-by-Step Example\nSuppose \\[\ndp[i] = \\min_{k&lt;i}\\big(dp[k] + a_k \\cdot b_i + c_k\\big)\n\\] Given:\n\n\\(a = [2,4,6]\\)\n\\(b = [1,2,3]\\)\n\\(c = [5,4,2]\\)\n\nEach step:\n\nAdd line \\(y = m_k x + b_k = a_k x + (dp[k] + c_k)\\)\nQuery at \\(x_i = b[i]\\) to get min value.\n\nCHT keeps only useful lines forming lower envelope.\n\n\nA Gentle Proof (Why It Works)\nFor monotonic slopes, intersection points are sorted. Once a line becomes worse than the next at a certain \\(x\\), it will never be optimal again for larger \\(x\\). Therefore, we can pop it from the deque — each line enters and leaves once → \\(O(n)\\) amortized.\n\n\nTry It Yourself\n\nAdapt for maximum query (flip signs).\nCombine with DP: \\(dp[i] = \\min_k(dp[k] + m_k x_i + b_k)\\).\nAdd Li Chao Tree for unsorted slopes/queries.\nVisualize lower envelope intersection points.\nCompare with Slope Trick (piecewise-linear potentials).\n\n\n\nTest Cases\n\n\n\nm\nb\nx\nExpected min\n\n\n\n\n[1,2,3]\n[0,1,3]\n1\n1\n\n\n[2,1]\n[5,4]\n2\n6\n\n\n[1,3,5]\n[2,2,2]\n4\n14\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) amortized (monotone queries) or \\(O(n\\log n)\\) (Li Chao)\nSpace: \\(O(n)\\)\n\nConvex Hull Trick is the bridge between geometry and DP, every line a subproblem, every envelope a minimization frontier.\n\n\n\n464 Li Chao Tree\nThe Li Chao Tree is a dynamic data structure for maintaining a set of lines and efficiently querying the minimum (or maximum) value at any given \\(x\\). Unlike the Convex Hull Trick, it works even when slopes and query points are arbitrary and unordered.\n\nWhat Problem Are We Solving?\nWe want to handle DP recurrences (or cost functions) of the form:\n\\[\ndp[i] = \\min_{j &lt; i} (m_j \\cdot x_i + b_j)\n\\]\nEach \\(j\\) contributes a line \\(y = m_jx + b_j\\). We must find the minimum value among all added lines at a given \\(x_i\\).\nUnlike the Convex Hull Trick (which needs monotonic \\(x\\) or \\(m\\)), Li Chao Tree handles any insertion or query order, no sorting required.\n\n\nWhen It Applies\nLi Chao Tree applies when:\n\nYou need to add arbitrary lines (\\(m_j, b_j\\)) over time\nYou must query arbitrary \\(x\\) in any order\nYou want min or max queries efficiently\nSlopes and queries are not monotonic\n\nThis makes it ideal for:\n\nDP with arbitrary slopes\nOnline queries\nGeometry problems involving lower envelopes\nLine container queries in computational geometry\n\n\n\nHow Does It Work (Plain Language)\nThe Li Chao Tree divides the \\(x\\)-axis into segments. Each node represents an interval, storing one line that’s currently optimal over part (or all) of that range.\nWhen a new line is added:\n\nCompare it with the current line in the interval\nSwap if it’s better at the midpoint\nRecursively insert into one child (left/right), narrowing the range\n\nWhen querying:\n\nDescend the tree using \\(x\\)\nCombine values of lines encountered\nReturn the minimum (or maximum)\n\nThis yields \\(O(\\log X)\\) time per insertion and query, where \\(X\\) is the range of \\(x\\) values (discretized if necessary).\n\n\nTiny Code (Easy Version)\nPython (Min Version)\nINF = 1018\n\nclass Line:\n    def __init__(self, m, b):\n        self.m = m\n        self.b = b\n    def value(self, x):\n        return self.m * x + self.b\n\nclass Node:\n    def __init__(self, l, r):\n        self.l = l\n        self.r = r\n        self.line = None\n        self.left = None\n        self.right = None\n\nclass LiChaoTree:\n    def __init__(self, l, r):\n        self.root = Node(l, r)\n    \n    def _add(self, node, new_line):\n        l, r = node.l, node.r\n        m = (l + r) // 2\n        if node.line is None:\n            node.line = new_line\n            return\n        \n        left_better = new_line.value(l) &lt; node.line.value(l)\n        mid_better = new_line.value(m) &lt; node.line.value(m)\n        \n        if mid_better:\n            node.line, new_line = new_line, node.line\n        \n        if r - l == 0:\n            return\n        if left_better != mid_better:\n            if not node.left:\n                node.left = Node(l, m)\n            self._add(node.left, new_line)\n        else:\n            if not node.right:\n                node.right = Node(m + 1, r)\n            self._add(node.right, new_line)\n    \n    def add_line(self, m, b):\n        self._add(self.root, Line(m, b))\n    \n    def _query(self, node, x):\n        if node is None:\n            return INF\n        res = node.line.value(x) if node.line else INF\n        m = (node.l + node.r) // 2\n        if x &lt;= m:\n            return min(res, self._query(node.left, x))\n        else:\n            return min(res, self._query(node.right, x))\n    \n    def query(self, x):\n        return self._query(self.root, x)\n\n# Example usage\ntree = LiChaoTree(0, 100)\ntree.add_line(2, 3)\ntree.add_line(-1, 10)\nprint(tree.query(5))  # minimum value among all lines at x=5\n\n\nWhy It Matters\n\nHandles arbitrary slopes and queries\nEfficient for online DP and geometry optimization\nGeneralizes CHT (works without monotonic constraints)\nCan be used for both min and max queries (just flip inequalities)\n\n\n\nStep-by-Step Example\nSuppose we insert lines:\n\n\\(y = 2x + 3\\)\n\\(y = -x + 10\\)\n\nThen query at \\(x = 5\\):\n\nFirst line: \\(2(5) + 3 = 13\\)\nSecond line: \\(-5 + 10 = 5\\) → answer = 5\n\nThe tree ensures each query returns the best line without brute force.\n\n\nA Gentle Proof (Why It Works)\nEach interval stores a line that’s locally optimal at some midpoint. If a new line is better at one endpoint, it must eventually overtake the existing line, so the intersection lies in that half.\nBy recursing on halves, we ensure the correct line is chosen for every \\(x\\).\nThe tree height is \\(\\log X\\), and each insertion affects at most \\(\\log X\\) nodes.\nHence: \\[\nT(n) = O(n \\log X)\n\\]\n\n\nTry It Yourself\n\nImplement Li Chao Tree for max queries (invert comparisons).\nAdd \\(n\\) random lines and query random \\(x\\).\nApply it to DP with linear cost: \\[\ndp[i] = \\min_{j &lt; i}(dp[j] + a_jx_i + b_j)\n\\]\nVisualize segment splits and stored lines.\nCompare with Convex Hull Trick on monotonic test cases.\n\n\n\nTest Cases\n\n\n\nLines\nQueries\nRange\nTime\nWorks\n\n\n\n\n10\n10\n[0, 100]\nO(n log X)\n✓\n\n\n1e5\n1e5\n[0, 1e9]\nO(n log X)\n✓\n\n\n\n\n\nComplexity\n\nTime: \\(O(\\log X)\\) per insert/query\nSpace: \\(O(\\log X)\\) per line (tree nodes)\n\nThe Li Chao Tree is your line oracle, always ready to give the best line, no matter how chaotic your slopes and queries become.\n\n\n\n465 Slope Trick\nThe Slope Trick is a dynamic programming optimization technique for problems involving piecewise-linear convex functions. It lets you maintain and update the shape of a convex cost function efficiently, especially when transitions involve operations like adding absolute values, shifting minima, or combining convex shapes.\n\nWhat Problem Are We Solving?\nMany DP problems involve minimizing a cost function that changes shape over time, such as:\n\\[\ndp[i] = \\min_x (dp[i-1](x) + |x - a_i|)\n\\]\nHere, \\(dp[i]\\) is not a single value but a function of \\(x\\). The Slope Trick is how we maintain this function efficiently as a sequence of linear segments.\nThis comes up when:\n\nYou need to add |x - a| terms\nYou need to shift the whole function left or right\nYou need to add constants or merge minima\n\nRather than storing the full function, we store key “breakpoints” and update in logarithmic time.\n\n\nWhen It Applies\nSlope Trick applies when:\n\nThe cost function is convex and piecewise linear\nEach transition is of the form:\n\n\\(f(x) + |x - a|\\)\n\\(f(x + c)\\) or \\(f(x - c)\\)\n\\(\\min_x(f(x)) + c\\)\n\nYou need to track minimal cost across shifting choices\n\nCommon in:\n\nMedian DP\nPath alignment\nConvex smoothing\nMinimizing sum of absolute differences\nCost balancing problems\n\n\n\nHow Does It Work (Plain Language)\nInstead of recomputing the entire cost function each time, we maintain two priority queues (heaps) that track where the slope changes.\nThink of the cost function as a mountain made of straight lines:\n\nAdding \\(|x - a|\\) means putting a “tent” centered at \\(a\\)\nMoving \\(x\\) left/right shifts the mountain\nThe minimum point can move but remains easy to find\n\nWe track the slope’s left and right breakpoints using heaps:\n\nLeft heap (max-heap): stores slopes to the left of minimum\nRight heap (min-heap): stores slopes to the right\n\nEach operation updates these heaps in \\(O(\\log n)\\).\n\n\nExample Problem\nMinimize: \\[\ndp[i] = \\min_x(dp[i-1](x) + |x - a_i|)\n\\]\nWe want the minimal total distance to all \\(a_1, a_2, ..., a_i\\).\nThe optimal \\(x\\) is the median of all \\(a\\)’s seen so far.\nSlope Trick maintains this function efficiently.\n\n\nTiny Code (Easy Version)\nPython\nimport heapq\n\nclass SlopeTrick:\n    def __init__(self):\n        self.left = []   # max-heap (store negative values)\n        self.right = []  # min-heap\n        self.min_cost = 0\n\n    def add_abs(self, a):\n        if not self.left:\n            heapq.heappush(self.left, -a)\n            heapq.heappush(self.right, a)\n            return\n        if a &lt; -self.left[0]:\n            heapq.heappush(self.left, -a)\n            val = -heapq.heappop(self.left)\n            heapq.heappush(self.right, val)\n            self.min_cost += -self.left[0] - a\n        else:\n            heapq.heappush(self.right, a)\n            val = heapq.heappop(self.right)\n            heapq.heappush(self.left, -val)\n            self.min_cost += a - val\n\n    def get_min(self):\n        return self.min_cost\n\n# Example\nst = SlopeTrick()\nfor a in [3, 1, 4, 1, 5]:\n    st.add_abs(a)\nprint(\"Minimum cost:\", st.get_min())\nThis structure efficiently tracks the cost of minimizing the sum of absolute differences.\n\n\nWhy It Matters\n\nReduces function-based DPs into heap updates\nElegant solution for convex minimization\nHandles |x - a|, shift, and constant add in \\(O(\\log n)\\)\nAvoids discretization of continuous \\(x\\)\n\n\n\nStep-by-Step Example\nSuppose we add points sequentially: \\(a = [3, 1, 4]\\)\n\nAdd \\(|x - 3|\\): min at \\(x = 3\\)\nAdd \\(|x - 1|\\): min moves to \\(x = 2\\)\nAdd \\(|x - 4|\\): min moves to \\(x = 3\\)\n\nHeaps track these balance points dynamically. Total cost is sum of minimal shifts.\n\n\nA Gentle Proof (Why It Works)\nAdding \\(|x - a|\\) modifies slope:\n\nFor \\(x &lt; a\\), slope increases by \\(-1\\)\nFor \\(x &gt; a\\), slope increases by \\(+1\\)\n\nThus, the function stays convex. Heaps store where slope crosses zero (the minimum).\nBalancing heaps keeps slopes equalized, ensuring minimum at the median. Each operation maintains convexity and updates cost correctly.\n\n\nTry It Yourself\n\nImplement add_shift(c) to shift function horizontally.\nSolve: \\[\ndp[i] = \\min_x(dp[i-1](x) + |x - a_i|)\n\\] for a list of \\(a_i\\)\nAdd add_constant(c) for vertical shifts.\nTrack the running median using heaps.\nVisualize slope evolution, it should always form a “V” shape.\n\n\n\nTest Cases\n\n\n\nInput\nExpected Minimum\n\n\n\n\n[3]\n0\n\n\n[3, 1]\n2\n\n\n[3, 1, 4]\n3\n\n\n[3, 1, 4, 1]\n5\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe Slope Trick is like origami for DP, you fold and shift convex functions to shape the minimal path, one segment at a time.\n\n\n\n466 Monotonic Queue Optimization\nMonotonic Queue Optimization is a dynamic programming acceleration technique for recurrences involving sliding windows or range-limited minima. It replaces naive scanning (\\(O(nk)\\)) with a monotonic deque that finds optimal states in \\(O(n)\\).\n\nWhat Problem Are We Solving?\nWe want to optimize DP of the form:\n\\[\ndp[i] = \\min_{j \\in [i-k,, i-1]} (dp[j] + cost(j, i))\n\\]\nor simpler, when \\(cost(j, i)\\) is monotonic or separable, like \\(w_i\\) or \\(c(i-j)\\), we can maintain a window of candidate \\(j\\)’s.\nThis pattern appears in:\n\nSliding window DPs\nShortest path in DAGs with window constraints\nQueue scheduling problems\nConstrained subsequence or segment DPs\n\n\n\nWhen It Applies\nYou can apply Monotonic Queue Optimization when:\n\nThe transition uses contiguous ranges of \\(j\\) (like a window)\nThe cost function is monotonic, allowing pruning of bad states\nYou want to find \\(\\min\\) or \\(\\max\\) over a sliding window efficiently\n\nCommon forms:\n\n\\(dp[i] = \\min_{j \\in [i-k, i]} (dp[j] + c[j])\\)\n\\(dp[i] = \\max_{j \\in [i-k, i]} (dp[j] + w[i])\\)\n\nThis trick does not require convexity, only monotonic ordering in the transition range.\n\n\nHow Does It Work (Plain Language)\nInstead of checking all \\(k\\) previous states for each \\(i\\), we maintain a deque of indices that are still potentially optimal.\nAt each step:\n\nRemove old indices (outside window)\nPop worse states (whose value is greater than the new one)\nFront of deque gives the best \\(j\\) for current \\(i\\)\n\nThis ensures the deque is monotonic (increasing or decreasing depending on min/max).\n\n\nExample Recurrence\n\\[\ndp[i] = \\min_{j \\in [i-k, i-1]} (dp[j] + w_i)\n\\]\nSince \\(w_i\\) doesn’t depend on \\(j\\), we just need \\(\\min dp[j]\\) over the last \\(k\\) indices.\n\n\nTiny Code (Easy Version)\nPython\nfrom collections import deque\n\ndef min_sliding_window_dp(arr, k):\n    n = len(arr)\n    dp = [0] * n\n    dq = deque()\n    \n    for i in range(n):\n        # Remove out-of-window\n        while dq and dq[0] &lt; i - k:\n            dq.popleft()\n        # Pop worse elements\n        while dq and dp[dq[-1]] &gt;= dp[i - 1] if i &gt; 0 else False:\n            dq.pop()\n        # Push current\n        dq.append(i)\n        # Compute dp\n        dp[i] = arr[i] + (dp[dq[0]] if dq else 0)\n    return dp\nC\n#include &lt;stdio.h&gt;\n#define N 100000\n#define INF 1000000000\n\nint dp[N], a[N], q[N];\n\nint main() {\n    int n = 5, k = 2;\n    int front = 0, back = 0;\n\n    int arr[5] = {3, 1, 4, 1, 5};\n    dp[0] = arr[0];\n    q[back++] = 0;\n\n    for (int i = 1; i &lt; n; i++) {\n        while (front &lt; back && q[front] &lt; i - k) front++;\n        dp[i] = dp[q[front]] + arr[i];\n        while (front &lt; back && dp[q[back - 1]] &gt;= dp[i]) back--;\n        q[back++] = i;\n    }\n\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", dp[i]);\n}\n\n\nWhy It Matters\n\nConverts range-min DPs from \\(O(nk)\\) → \\(O(n)\\)\nEssential for problems with window constraints\nAvoids heap overhead (constant-time updates)\nExtremely simple and robust\n\n\n\nStep-by-Step Example\nLet \\(arr = [3,1,4,1,5]\\), \\(k = 2\\)\nAt \\(i = 2\\):\n\nCandidates: \\(j \\in [0, 1]\\)\ndp[0]=3, dp[1]=4\ndq = [1] after pruning worse values\ndp[2] = arr[2] + dp[1] = 4 + 4 = 8\n\nDeque moves as window slides, always holding potential minima.\n\n\nA Gentle Proof (Why It Works)\nAt each \\(i\\):\n\nRemove indices \\(&lt; i-k\\) (out of range)\nMaintain monotonic order of dp-values in deque\nThe front always gives the smallest \\(dp[j]\\) in window\nBecause each element is pushed and popped once, total operations = \\(O(n)\\)\n\nThus, overall complexity is linear.\n\n\nTry It Yourself\n\nImplement max version by reversing comparisons.\nApply to \\(dp[i] = \\min_{j \\in [i-k, i]} (dp[j] + c_i)\\)\nVisualize deque evolution per step.\nSolve constrained path problems with limited jump size.\nCompare runtime with naive \\(O(nk)\\) approach.\n\n\n\nTest Cases\n\n\n\nInput\nk\nExpected\n\n\n\n\n[3,1,4,1,5]\n2\nfast min DP\n\n\n[10,9,8,7,6]\n3\ndecreasing\n\n\n[1,2,3,4,5]\n1\nsimple\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(k)\\)\n\nMonotonic Queue Optimization is your sliding window oracle, keeping just the right candidates, and tossing the rest without looking back.\n\n\n\n467 Bitset DP\nBitset DP is a performance optimization technique that uses bit-level parallelism to speed up dynamic programming, especially when state transitions involve Boolean operations over large ranges. By representing states as bits, multiple transitions can be processed simultaneously using fast bitwise operators.\n\nWhat Problem Are We Solving?\nWe want to optimize DPs like:\n\\[\ndp[i] = \\text{reachable states after considering first } i \\text{ elements}\n\\]\nOften in subset sum, knapsack, path existence, or mask propagation, we deal with states where:\n\nEach state is true/false\nTransition is shifting or combining bits\n\nFor example, in Subset Sum: \\[\ndp[i][s] = dp[i-1][s] \\lor dp[i-1][s - a_i]\n\\] We can compress this into a bitset shift.\n\n\nWhen It Applies\nYou can use Bitset DP when:\n\nStates are Boolean (true/false)\nTransition is shift-based or additive\nState space is dense and bounded\n\nCommon use cases:\n\nSubset Sum (\\(O(nS / w)\\))\nBounded Knapsack\nGraph reachability\nPalindromic substrings DP\nCounting with bit masks\n\nHere, \\(w\\) is word size (e.g. 64), giving up to 64x speedup.\n\n\nHow Does It Work (Plain Language)\nRepresent each DP layer as a bitset, each bit indicates whether a state is reachable.\nFor Subset Sum:\n\nInitially: dp[0] = 1 (sum = 0 reachable)\nFor each number a:\n\nShift left by a → new reachable sums\nCombine: dp |= dp &lt;&lt; a\n\n\nExample: Adding 3 to {0, 2, 5} means shift left by 3 → {3, 5, 8}.\nAll in one CPU instruction!\n\n\nExample Recurrence\n\\[\ndp[s] = dp[s] \\lor dp[s - a]\n\\]\nBitset form:\n\\[\ndp = dp \\lor (dp \\ll a)\n\\]\n\n\nTiny Code (Easy Version)\nPython (using int bitset)\ndef subset_sum(nums, target):\n    dp = 1  # bit 0 = reachable sum 0\n    for a in nums:\n        dp |= dp &lt;&lt; a\n    return (dp &gt;&gt; target) & 1  # check if bit target is set\n\nprint(subset_sum([3, 2, 7], 5))  # True (3+2)\nprint(subset_sum([3, 2, 7], 6))  # False\nC (using bitset)\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define MAXS 10000\n#define W 64\nunsigned long long dp[MAXS / W + 1];\n\nvoid setbit(int i) { dp[i / W] |= 1ULL &lt;&lt; (i % W); }\nint getbit(int i) { return (dp[i / W] &gt;&gt; (i % W)) & 1; }\n\nint main() {\n    memset(dp, 0, sizeof(dp));\n    setbit(0); // sum 0 reachable\n    int a[] = {3, 2, 7}, n = 3;\n    for (int i = 0; i &lt; n; i++) {\n        int v = a[i];\n        for (int j = MAXS / W; j &gt;= 0; j--) {\n            unsigned long long shifted = dp[j] &lt;&lt; v;\n            if (j + v / W + 1 &lt;= MAXS / W)\n                dp[j + v / W + 1] |= dp[j] &gt;&gt; (W - (v % W));\n            dp[j] |= shifted;\n        }\n    }\n    printf(\"Sum 5 reachable? %d\\n\", getbit(5));\n}\n\n\nWhy It Matters\n\nExploits hardware parallelism\nIdeal for dense Boolean DP\nWorks for subset sums, range transitions, graph masks\nAchieves massive speedups with simple operations\n\n\n\nStep-by-Step Example\nSuppose nums = [2, 3], target = 5\nStart: dp = 1 → {0}\nAfter 2: dp &lt;&lt; 2 = {2} dp |= dp &lt;&lt; 2 = {0, 2}\nAfter 3: dp &lt;&lt; 3 = {3, 5} dp |= dp &lt;&lt; 3 = {0, 2, 3, 5}\nBit 5 is set → sum 5 reachable ✅\n\n\nA Gentle Proof (Why It Works)\nEach shift corresponds to adding an element to a subset sum. Bitwise OR merges reachable sums. No overlap conflict, each bit is unique to a sum. After all shifts, all sums formed by subsets are represented.\nEach shift-OR runs in \\(O(S / w)\\) time, where \\(S\\) = target sum, \\(w\\) = word size.\n\n\nTry It Yourself\n\nImplement bounded knapsack via repeated shift-and-OR.\nCount distinct subset sums (popcount of dp).\nApply to palindrome DP: \\(dp[i][j] = s[i] == s[j] \\land dp[i+1][j-1]\\).\nVisualize bit patterns after each step.\nBenchmark vs normal DP on large \\(S\\).\n\n\n\nTest Cases\n\n\n\nnums\ntarget\nResult\n\n\n\n\n[3, 2, 7]\n5\nTrue\n\n\n[3, 2, 7]\n6\nFalse\n\n\n[1, 2, 3, 4]\n10\nTrue\n\n\n\n\n\nComplexity\n\nTime: \\(O(nS / w)\\)\nSpace: \\(O(S / w)\\)\n\nBitset DP is your Boolean supercharger, turning slow loops into blinding-fast bitwise moves.\n\n\n\n468 Offline DP Queries\nOffline DP Queries are a strategy to handle queries on a dynamic programming state space by reordering or batching them for efficient computation. Instead of answering queries as they arrive (online), we process them after sorting or grouping, enabling faster transitions or range updates.\n\nWhat Problem Are We Solving?\nYou may have a DP or recurrence that evolves over time, and a set of queries asking for values at specific states or intervals, like:\n\n“What is \\(dp[x]\\) after all updates?”\n“What is the min cost among indices in [L, R]?”\n“How many reachable states satisfy condition C?”\n\nNaively answering queries as they appear leads to repeated recomputation. By processing them offline, we exploit sorting, prefix accumulation, or data structure reuse.\n\n\nWhen It Applies\nOffline DP Query methods apply when:\n\nQueries can be sorted (by time, index, or key)\nTransitions or states evolve monotonically\nYou can batch updates and reuse results\n\nCommon cases:\n\nRange DP queries: \\(dp[i]\\) over [L, R]\nMonotonic state DPs (like convex hull or segment DP)\nMo’s algorithm on DP states\nIncremental DPs where \\(dp[i]\\) is finalized before querying\n\n\n\nHow Does It Work (Plain Language)\nInstead of answering as we go, we:\n\nCollect all queries\nSort them by a relevant dimension (like time or index)\nProcess DP transitions incrementally\nAnswer queries once the needed states are available\n\nThink of it as “moving forward once” and answering everything you pass.\nBy decoupling query order from input order, you avoid recomputation and exploit monotonic progression of DP.\n\n\nExample Problem\nYou’re asked \\(q\\) queries:\n\nFor each \\(x_i\\), what is the minimum \\(dp[j] + cost(j, x_i)\\) over all \\(j \\le x_i\\)?\n\nNaively, \\(O(nq)\\). Offline, sort queries by \\(x_i\\), process \\(j = 1 \\ldots n\\), and maintain current DP structure (like a segment tree or convex hull).\n\n\nTiny Code (Easy Version)\nPython (sorted queries with running DP)\ndef offline_dp(arr, queries):\n    # arr defines dp transitions\n    # queries = [(x, idx)]\n    n = len(arr)\n    dp = [0] * (n + 1)\n    res = [0] * len(queries)\n\n    queries.sort()  # sort by x\n    ptr = 0\n    for i in range(1, n + 1):\n        dp[i] = dp[i-1] + arr[i-1]\n        # process queries with x == i\n        while ptr &lt; len(queries) and queries[ptr][0] == i:\n            _, idx = queries[ptr]\n            res[idx] = dp[i]\n            ptr += 1\n    return res\n\narr = [3, 1, 4, 1, 5]\nqueries = [(3, 0), (5, 1)]\nprint(offline_dp(arr, queries))  # [sum of first 3, sum of first 5]\nC (sorted queries)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct { int x, idx; } Query;\n\nint cmp(const void *a, const void *b) {\n    return ((Query*)a)-&gt;x - ((Query*)b)-&gt;x;\n}\n\nint main() {\n    int arr[] = {3, 1, 4, 1, 5}, n = 5;\n    Query q[] = {{3,0},{5,1}};\n    int dp[6] = {0}, res[2];\n\n    qsort(q, 2, sizeof(Query), cmp);\n    int ptr = 0;\n    for (int i = 1; i &lt;= n; i++) {\n        dp[i] = dp[i-1] + arr[i-1];\n        while (ptr &lt; 2 && q[ptr].x == i) {\n            res[q[ptr].idx] = dp[i];\n            ptr++;\n        }\n    }\n\n    printf(\"%d %d\\n\", res[0], res[1]); // dp[3], dp[5]\n}\n\n\nWhy It Matters\n\nConverts repeated query updates into one forward pass\nEnables range optimizations (segment trees, CHT, etc.)\nReduces complexity from \\(O(nq)\\) to \\(O(n + q \\log n)\\) or better\nEssential for problems mixing queries + DP updates\n\n\n\nStep-by-Step Example\nSuppose we compute cumulative \\(dp[i] = dp[i-1] + a[i]\\) and queries ask \\(dp[x]\\) for random \\(x\\):\nNaive:\n\nRecompute each query: \\(O(qn)\\)\n\nOffline:\n\nSort queries by \\(x\\)\nSingle pass \\(O(n + q)\\)\n\nSame principle applies to complex DPs if queries depend on monotone indices.\n\n\nA Gentle Proof (Why It Works)\nIf DP states evolve monotonically in one dimension (index or time), then after computing \\(dp[1]\\) to \\(dp[x]\\), the answer to all queries with bound ≤ \\(x\\) is final.\nSorting ensures we never recompute older states, and every query sees exactly what it needs, no more, no less.\nThus, each DP transition and query is processed once, yielding \\(O(n + q)\\) total complexity.\n\n\nTry It Yourself\n\nImplement offline queries for prefix sums\nCombine with Convex Hull Trick for sorted \\(x_i\\)\nUse segment tree for range min DP queries\nImplement offline Subset Sum queries (by sum ≤ X)\nCompare performance with online queries\n\n\n\nTest Cases\n\n\n\narr\nqueries\nOutput\n\n\n\n\n[3,1,4,1,5]\n[3,5]\n[8,14]\n\n\n[2,2,2]\n[1,2,3]\n[2,4,6]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n + q \\log q)\\)\nSpace: \\(O(n + q)\\)\n\nOffline DP Queries are your time travelers, answer questions from the future by rearranging them into a single efficient sweep through the past.\n\n\n\n469 DP + Segment Tree\nDP + Segment Tree is a hybrid optimization pattern that combines dynamic programming with a segment tree (or Fenwick tree) to handle transitions involving range queries (min, max, sum) efficiently. It’s especially useful when each DP state depends on a range of previous states, rather than a single index.\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\ndp[i] = \\min_{l_i \\le j \\le r_i}(dp[j] + cost[j])\n\\]\nor more generally,\n\\[\ndp[i] = \\text{aggregate over range [L(i), R(i)] of some function of } dp[j]\n\\]\nWhen transitions span intervals, naive iteration over each range is \\(O(n^2)\\). A segment tree reduces this to \\(O(n \\log n)\\) by supporting range queries and point updates.\n\n\nWhen It Applies\nUse DP + Segment Tree when:\n\nTransitions depend on intervals or ranges\nThe DP recurrence is monotonic in index order\nYou need fast min, max, or sum over subsets\n\nTypical problems:\n\nRange-based knapsack variants\nSequence partitioning with range cost\nInterval scheduling DP\nIncreasing subsequences with weight\nPathfinding with segment bounds\n\n\n\nHow Does It Work (Plain Language)\nInstead of looping over all \\(j\\) to find the best previous state:\n\nStore \\(dp[j]\\) values in a segment tree\nQuery for the minimum/maximum over \\([L(i), R(i)]\\)\nAdd transition cost and store \\(dp[i]\\) back\n\nThis way, every step:\n\nQuery = \\(O(\\log n)\\)\nUpdate = \\(O(\\log n)\\) Total = \\(O(n \\log n)\\)\n\n\n\nExample Recurrence\n\\[\ndp[i] = \\min_{j &lt; i,, a_j &lt; a_i}(dp[j]) + cost(i)\n\\]\nIf \\(a_i\\) values can be ordered or compressed, we can query the segment tree for all \\(a_j &lt; a_i\\) efficiently.\n\n\nTiny Code (Easy Version)\nPython\nINF = 1018\n\nclass SegmentTree:\n    def __init__(self, n):\n        self.N = 1\n        while self.N &lt; n:\n            self.N *= 2\n        self.data = [INF] * (2 * self.N)\n    def update(self, i, val):\n        i += self.N\n        self.data[i] = val\n        while i &gt; 1:\n            i //= 2\n            self.data[i] = min(self.data[2*i], self.data[2*i+1])\n    def query(self, l, r):\n        l += self.N\n        r += self.N\n        res = INF\n        while l &lt; r:\n            if l % 2:\n                res = min(res, self.data[l])\n                l += 1\n            if r % 2:\n                r -= 1\n                res = min(res, self.data[r])\n            l //= 2\n            r //= 2\n        return res\n\ndef dp_segment_tree(arr):\n    n = len(arr)\n    dp = [INF] * n\n    seg = SegmentTree(n)\n    dp[0] = arr[0]\n    seg.update(0, dp[0])\n    for i in range(1, n):\n        best = seg.query(max(0, i - 2), i)  # e.g., range [i-2, i-1]\n        dp[i] = arr[i] + best\n        seg.update(i, dp[i])\n    return dp\n\narr = [3, 1, 4, 1, 5]\nprint(dp_segment_tree(arr))\nC\n#include &lt;stdio.h&gt;\n#define INF 1000000000\n#define N 100005\n\nint seg[4*N], dp[N], arr[N];\n\nint min(int a, int b) { return a &lt; b ? a : b; }\n\nvoid update(int idx, int val, int id, int l, int r) {\n    if (l == r) { seg[id] = val; return; }\n    int mid = (l + r) / 2;\n    if (idx &lt;= mid) update(idx, val, 2*id, l, mid);\n    else update(idx, val, 2*id+1, mid+1, r);\n    seg[id] = min(seg[2*id], seg[2*id+1]);\n}\n\nint query(int ql, int qr, int id, int l, int r) {\n    if (qr &lt; l || r &lt; ql) return INF;\n    if (ql &lt;= l && r &lt;= qr) return seg[id];\n    int mid = (l + r) / 2;\n    return min(query(ql, qr, 2*id, l, mid),\n               query(ql, qr, 2*id+1, mid+1, r));\n}\n\nint main() {\n    int arr[] = {3,1,4,1,5}, n = 5;\n    for (int i = 0; i &lt; 4*N; i++) seg[i] = INF;\n    dp[0] = arr[0];\n    update(0, dp[0], 1, 0, n-1);\n    for (int i = 1; i &lt; n; i++) {\n        int best = query(i-2 &gt;= 0 ? i-2 : 0, i-1, 1, 0, n-1);\n        dp[i] = arr[i] + best;\n        update(i, dp[i], 1, 0, n-1);\n    }\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", dp[i]);\n}\n\n\nWhy It Matters\n\nHandles range transitions efficiently\nReduces quadratic DPs to \\(O(n \\log n)\\)\nWorks for both min and max recurrences\nCombines with coordinate compression for complex ranges\n\n\n\nStep-by-Step Example\nLet \\(arr = [3, 1, 4, 1, 5]\\), and \\[\ndp[i] = arr[i] + \\min_{j \\in [i-2, i-1]} dp[j]\n\\]\n\n\\(i=0\\): \\(dp[0]=3\\)\n\\(i=1\\): query \\([0,0]\\), \\(dp[1]=1+3=4\\)\n\\(i=2\\): query \\([0,1]\\), \\(dp[2]=4+1=5\\)\n\\(i=3\\): query \\([1,2]\\), \\(dp[3]=1+4=5\\)\n\\(i=4\\): query \\([2,3]\\), \\(dp[4]=5+4=9\\)\n\n\n\nA Gentle Proof (Why It Works)\nSegment trees store range minima. Each DP state only depends on previously finalized values. As you move \\(i\\) forward, you query and update disjoint ranges. Hence total complexity:\n\\[\nO(n \\log n) \\text{ (n queries + n updates)}\n\\]\nNo recomputation, each transition is resolved via the tree in logarithmic time.\n\n\nTry It Yourself\n\nChange recurrence to max and adjust segment tree.\nSolve weighted LIS: \\(dp[i] = w_i + \\max_{a_j &lt; a_i} dp[j]\\).\nCombine with coordinate compression for arbitrary \\(a_i\\).\nVisualize segment tree contents over iterations.\nApply to interval scheduling with overlapping windows.\n\n\n\nTest Cases\n\n\n\narr\nRange\nOutput\n\n\n\n\n[3,1,4,1,5]\n[i-2,i-1]\n[3,4,5,5,9]\n\n\n[2,2,2,2]\n[i-1,i-1]\n[2,4,6,8]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log n)\\)\nSpace: \\(O(n)\\)\n\nSegment Tree + DP is your range oracle, answering every interval dependency without scanning the whole past.\n\n\n\n470 Divide & Conquer Knapsack\nDivide & Conquer Knapsack is an optimization method that accelerates dynamic programming for large-capacity knapsack problems by recursively splitting the item set and combining results, rather than building a full \\(O(nW)\\) DP table. It is especially powerful when you need to reconstruct solutions or handle queries across subsets.\n\nWhat Problem Are We Solving?\nThe classic 0/1 Knapsack Problem is:\n\\[\ndp[i][w] = \\max(dp[i-1][w],, dp[i-1][w - w_i] + v_i)\n\\]\nwhere \\(w_i\\) is the item’s weight and \\(v_i\\) its value.\nThis standard DP costs \\(O(nW)\\) in time and space, which becomes infeasible when \\(n\\) or \\(W\\) is large.\nDivide & Conquer Knapsack tackles this by splitting items into halves and solving subproblems recursively, a strategy similar to meet-in-the-middle, but adapted to DP.\n\n\nWhen It Applies\nUse Divide & Conquer Knapsack when:\n\nYou have many items (\\(n &gt; 1000\\))\nCapacity \\(W\\) is large, but manageable via combinations\nYou need partial solution reconstruction\nYou want to handle batch queries (e.g., best value for each capacity range)\n\nCommon contexts:\n\nLarge \\(n\\), moderate \\(W\\) (split across subsets)\nEnumerating feasible states\nOffline processing of item sets\nRecursive solution generation (for decision trees or subset enumeration)\n\n\n\nHow Does It Work (Plain Language)\nInstead of building one giant DP table, we split the item list into halves:\n\nSolve left half → get all achievable \\((weight, value)\\) pairs\nSolve right half → same\nMerge results efficiently (like convolution or sweep)\n\nBy recursively combining subproblems, you reduce total recomputation and enable parallel merging of feasible subsets.\nIf \\(n = 2^k\\), recursion depth = \\(O(\\log n)\\), and each merge costs \\(O(2^{n/2})\\), much faster than \\(O(nW)\\) when \\(W\\) is large.\n\n\nExample Recurrence\nLet solve(l, r) compute all feasible pairs for items \\([l, r)\\):\nIf r - l == 1:\n    return {(0,0), (w_l, v_l)}\nElse:\n    mid = (l + r) / 2\n    left = solve(l, mid)\n    right = solve(mid, r)\n    return combine(left, right)\ncombine merges pairs from left and right (like merging sorted lists, keeping only Pareto-optimal pairs).\n\n\nTiny Code (Easy Version)\nPython\ndef combine(left, right, W):\n    res = []\n    for w1, v1 in left:\n        for w2, v2 in right:\n            w = w1 + w2\n            if w &lt;= W:\n                res.append((w, v1 + v2))\n    # Keep only best value per weight (Pareto frontier)\n    res.sort()\n    best = []\n    cur = -1\n    for w, v in res:\n        if v &gt; cur:\n            best.append((w, v))\n            cur = v\n    return best\n\ndef solve(items, W):\n    n = len(items)\n    if n == 1:\n        w, v = items[0]\n        return [(0, 0), (w, v)] if w &lt;= W else [(0, 0)]\n    mid = n // 2\n    left = solve(items[:mid], W)\n    right = solve(items[mid:], W)\n    return combine(left, right, W)\n\nitems = [(3, 4), (4, 5), (7, 10), (8, 11)]\nW = 10\nprint(solve(items, W))  # [(0,0),(3,4),(4,5),(7,10),(8,11),(10,14)]\n\n\nWhy It Matters\n\nAvoids full \\(O(nW)\\) DP when \\(W\\) is large\nEnables offline merging and solution reconstruction\nUseful in meet-in-the-middle optimization\nCan handle dynamic constraints by recombining subsets\n\n\n\nStep-by-Step Example\nItems: \\((3,4), (4,5), (7,10), (8,11)\\), \\(W = 10\\)\nSplit:\n\nLeft half: \\((3,4), (4,5)\\) → feasible = {(0,0),(3,4),(4,5),(7,9)}\nRight half: \\((7,10), (8,11)\\) → {(0,0),(7,10),(8,11),(15,21)}\n\nCombine all \\((w_L + w_R, v_L + v_R)\\) ≤ 10:\n\n(0,0), (3,4), (4,5), (7,9), (7,10), (8,11), (10,14)\n\nPareto-optimal:\n\n(0,0), (3,4), (4,5), (7,10), (10,14)\n\nMax value for \\(W=10\\): 14\n\n\nA Gentle Proof (Why It Works)\nBy recursively splitting:\n\nEach subset’s combinations are enumerated in \\(O(2^{n/2})\\)\nMerge step ensures only non-dominated states are carried forward\nRecursion covers all subsets exactly once\n\nThus, total cost ≈ \\(O(2^{n/2})\\) instead of \\(O(nW)\\).\nFor moderate \\(n\\) (≤40), this is dramatically faster.\nFor large \\(n\\) with constraints (bounded weights), merges reduce to \\(O(n \\log n)\\) per layer.\n\n\nTry It Yourself\n\nImplement value-only knapsack (max value ≤ W)\nVisualize Pareto frontier after each combine\nUse recursion tree to print intermediate DP states\nCompare against standard \\(O(nW)\\) DP results\nExtend to multi-dimensional weights\n\n\n\nTest Cases\n\n\n\nItems\nW\nResult\n\n\n\n\n[(3,4),(4,5),(7,10),(8,11)]\n10\n14\n\n\n[(1,1),(2,2),(3,3)]\n3\n3\n\n\n[(2,3),(3,4),(4,5)]\n5\n7\n\n\n\n\n\nComplexity\n\nTime: \\(O(2^{n/2} \\cdot n)\\) (meet-in-the-middle)\nSpace: \\(O(2^{n/2})\\)\n\nDivide & Conquer Knapsack is your recursive craftsman, building optimal subsets by combining halves, not filling tables.\n\n\n\n\nSection 48. Tree DP and Rerooting\n\n471 Subtree Sum DP\nSubtree Sum DP is one of the most fundamental patterns in tree dynamic programming. It computes the sum of values in every node’s subtree using a simple post-order traversal. Once you know how to aggregate over subtrees, you can extend the same idea to handle sizes, depths, counts, or any associative property.\n\nWhat Problem Are We Solving?\nGiven a rooted tree where each node has a value, compute for every node the sum of values in its subtree (including itself).\nFor a node \\(u\\) with children \\(v_1, v_2, \\dots, v_k\\), the subtree sum is:\n\\[\ndp[u] = value[u] + \\sum_{v \\in children(u)} dp[v]\n\\]\nThis idea generalizes to many forms of aggregation, such as counting nodes, finding subtree size, or computing subtree products.\n\n\nHow Does It Work (Plain Language)\nThink of each node as a small calculator. When a node finishes computing its children’s sums, it adds them all up, plus its own value. This is post-order traversal, compute from leaves upward.\n\n\nStep-by-Step Example\nConsider the tree:\n       1(5)\n      /   \\\n   2(3)   3(2)\n   / \\\n4(1) 5(4)\nValues:\n\nNode 1 → 5\nNode 2 → 3\nNode 3 → 2\nNode 4 → 1\nNode 5 → 4\n\nWe compute bottom-up:\n\n\\(dp[4] = 1\\)\n\\(dp[5] = 4\\)\n\\(dp[2] = 3 + 1 + 4 = 8\\)\n\\(dp[3] = 2\\)\n\\(dp[1] = 5 + 8 + 2 = 15\\)\n\nSo the subtree sums are:\n\n\n\nNode\nSubtree Sum\n\n\n\n\n1\n15\n\n\n2\n8\n\n\n3\n2\n\n\n4\n1\n\n\n5\n4\n\n\n\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint value[MAXN];\nint adj[MAXN][MAXN], deg[MAXN];\nint dp[MAXN];\n\nint dfs(int u, int parent) {\n    dp[u] = value[u];\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == parent) continue;\n        dp[u] += dfs(v, u);\n    }\n    return dp[u];\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    int vals[] = {0,5,3,2,1,4};\n    for (int i = 1; i &lt;= n; i++) value[i] = vals[i];\n    dfs(1, -1);\n    for (int i = 1; i &lt;= n; i++) printf(\"dp[%d] = %d\\n\", i, dp[i]);\n}\nPython\nfrom collections import defaultdict\nn = 5\nedges = [(1,2),(1,3),(2,4),(2,5)]\nvalue = {1:5, 2:3, 3:2, 4:1, 5:4}\n\ng = defaultdict(list)\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\ndp = {}\n\ndef dfs(u, p):\n    dp[u] = value[u]\n    for v in g[u]:\n        if v == p: continue\n        dfs(v, u)\n        dp[u] += dp[v]\n\ndfs(1, -1)\nprint(dp)\n\n\nWhy It Matters\n\nA core tree DP pattern: many problems reduce to aggregating over subtrees\nForms the basis of rerooting DP, tree diameter, and centroid decomposition\nUsed in computing subtree sizes, subtree XOR, sum of depths, subtree counts\n\nOnce you master subtree DP, you can generalize to:\n\nMax/min subtree values\nCounting paths through nodes\nDynamic rerooting transitions\n\n\n\nA Gentle Proof (Why It Works)\nBy induction on tree depth:\n\nBase Case: For a leaf node \\(u\\), \\(dp[u] = value[u]\\), correct by definition.\nInductive Step: Assume all children \\(v\\) have correct \\(dp[v]\\). Then \\(dp[u] = value[u] + \\sum dp[v]\\) correctly accumulates all values in \\(u\\)’s subtree.\n\nSince every node is visited once and every edge twice, total cost is \\(O(n)\\).\n\n\nTry It Yourself\n\nModify the code to compute subtree size instead of sum\nTrack maximum value in each subtree\nExtend to compute sum of depths per subtree\nAdd rerooting to compute subtree sum for every root\nUse input parser to build arbitrary trees\n\n\n\nTest Cases\n\n\n\nTree\nValues\nSubtree Sums\n\n\n\n\n1–2–3\n{1:1,2:2,3:3}\n{1:6, 2:5, 3:3}\n\n\n1–2, 1–3\n{1:5,2:2,3:1}\n{1:8, 2:2, 3:1}\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) (each node visited once)\nSpace: \\(O(n)\\) recursion + adjacency\n\nSubtree Sum DP is your first brush with tree dynamics, one traversal, full insight.\n\n\n\n472 Diameter DP\nDiameter DP computes the longest path in a tree, the diameter. Unlike shortest paths, the diameter is measured by the greatest distance between any two nodes, not necessarily passing through the root. Using dynamic programming, we can derive this in a single DFS traversal by combining the two deepest child paths at every node.\n\nWhat Problem Are We Solving?\nGiven a tree with \\(n\\) nodes (unweighted or weighted), find the diameter, i.e. the length of the longest simple path between any two nodes.\nFor an unweighted tree, this is measured in edges or nodes; for a weighted tree, in sum of edge weights.\nWe define a DP recurrence:\n\\[\ndp[u] = \\text{length of longest downward path from } u\n\\]\nAt each node, the diameter candidate is the sum of the two longest child paths:\n\\[\ndiameter = \\max(diameter, top1 + top2)\n\\]\n\n\nHow Does It Work (Plain Language)\nThink of every node as a hub connecting paths from its children. The longest path passing through a node is formed by picking its two deepest child paths and joining them. We collect this as we perform a post-order DFS.\nIn the end, the global maximum across all nodes is the tree’s diameter.\n\n\nStep-by-Step Example\nTree:\n      1\n     / \\\n    2   3\n   / \\\n  4   5\nEach edge has weight 1.\nCompute longest downward paths:\n\nLeaves (4, 5, 3): \\(dp=0\\)\nNode 2: \\(dp[2] = 1 + \\max(dp[4], dp[5]) = 1 + 0 = 1\\)\nNode 1: \\(dp[1] = 1 + \\max(dp[2], dp[3]) = 1 + 1 = 2\\)\n\nNow compute diameter:\n\nAt node 2: top1=0, top2=0 → local diameter=0\nAt node 1: top1=1 (from 2), top2=1 (from 3) → local diameter=2\n\nSo the tree diameter = 2 edges (path 4–2–1–3)\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint adj[MAXN][MAXN], deg[MAXN];\nint diameter = 0;\n\nint dfs(int u, int p) {\n    int top1 = 0, top2 = 0;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        int depth = 1 + dfs(v, u);\n        if (depth &gt; top1) {\n            top2 = top1;\n            top1 = depth;\n        } else if (depth &gt; top2) {\n            top2 = depth;\n        }\n    }\n    if (top1 + top2 &gt; diameter) diameter = top1 + top2;\n    return top1;\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs(1, -1);\n    printf(\"Tree diameter: %d edges\\n\", diameter);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4),(2,5)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\ndiameter = 0\n\ndef dfs(u, p):\n    global diameter\n    top1 = top2 = 0\n    for v in g[u]:\n        if v == p: continue\n        depth = 1 + dfs(v, u)\n        if depth &gt; top1:\n            top2 = top1\n            top1 = depth\n        elif depth &gt; top2:\n            top2 = depth\n    diameter = max(diameter, top1 + top2)\n    return top1\n\ndfs(1, -1)\nprint(\"Tree diameter:\", diameter)\n\n\nWhy It Matters\n\nCentral building block for tree analysis, network radius, center finding\nUsed in problems involving longest paths, tree heights, centroid decomposition\nServes as a key step in rerooting or centroid algorithms\n\n\n\nA Gentle Proof (Why It Works)\nLet’s prove correctness by induction:\n\nBase Case: A leaf node has \\(dp[u] = 0\\), no contribution beyond itself.\nInductive Step: For each internal node \\(u\\), if all children \\(v\\) correctly compute their longest downward paths \\(dp[v]\\), then combining the two largest gives the longest path through \\(u\\). Since every path in a tree passes through some lowest common ancestor \\(u\\), our DFS finds the true maximum globally.\n\n\n\nTry It Yourself\n\nModify the code for weighted edges\nReturn both endpoints of the diameter path\nCompare with two-pass BFS method (pick farthest node twice)\nExtend to compute height of each subtree alongside\nVisualize recursion tree with local diameters\n\n\n\nTest Cases\n\n\n\nTree\nDiameter\n\n\n\n\nLine: 1–2–3–4\n3 edges\n\n\nStar: 1–{2,3,4,5}\n2 edges\n\n\nBalanced binary tree (depth 2)\n4 edges\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) recursion stack\n\nDiameter DP is the lens that reveals the tree’s longest breath, one sweep, full span.\n\n\n\n473 Independent Set DP\nIndependent Set DP finds the largest set of nodes in a tree such that no two chosen nodes are adjacent. This is a classic tree dynamic programming problem, showcasing the fundamental trade-off between inclusion and exclusion at each node.\n\nWhat Problem Are We Solving?\nGiven a tree with \\(n\\) nodes (each possibly having a weight), find the maximum-weight independent set, a subset of nodes such that no two connected nodes are selected.\nFor each node \\(u\\), we maintain two DP states:\n\n\\(dp[u][0]\\): maximum value in \\(u\\)’s subtree when \\(u\\) is not chosen\n\\(dp[u][1]\\): maximum value in \\(u\\)’s subtree when \\(u\\) is chosen\n\nThe recurrence:\n\\[\ndp[u][0] = \\sum_{v \\in children(u)} \\max(dp[v][0], dp[v][1])\n\\]\n\\[\ndp[u][1] = value[u] + \\sum_{v \\in children(u)} dp[v][0]\n\\]\nThe answer is \\(\\max(dp[root][0], dp[root][1])\\).\n\n\nHow Does It Work (Plain Language)\nEach node decides:\n\nIf it includes itself, it excludes its children\nIf it excludes itself, it can take the best of each child\n\nThis “take-or-skip” strategy flows bottom-up from leaves to root.\n\n\nStep-by-Step Example\nTree:\n      1(3)\n     /   \\\n   2(2)  3(1)\n   /\n 4(4)\nValues in parentheses.\nCompute DP from bottom:\n\nNode 4: \\(dp[4][0]=0\\), \\(dp[4][1]=4\\)\nNode 2: \\(dp[2][0]=\\max(dp[4][0],dp[4][1])=4\\) \\(dp[2][1]=2+dp[4][0]=2+0=2\\)\nNode 3: \\(dp[3][0]=0\\), \\(dp[3][1]=1\\)\nNode 1: \\(dp[1][0]=\\max(dp[2][0],dp[2][1])+\\max(dp[3][0],dp[3][1])=4+1=5\\) \\(dp[1][1]=3+dp[2][0]+dp[3][0]=3+4+0=7\\)\n\nAnswer: \\(\\max(5,7)=7\\)\nBest set = {1,4}\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint value[MAXN];\nint adj[MAXN][MAXN], deg[MAXN];\nint dp[MAXN][2];\nint visited[MAXN];\n\nvoid dfs(int u, int p) {\n    dp[u][0] = 0;\n    dp[u][1] = value[u];\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u][0] += (dp[v][0] &gt; dp[v][1] ? dp[v][0] : dp[v][1]);\n        dp[u][1] += dp[v][0];\n    }\n}\n\nint main() {\n    n = 4;\n    int edges[][2] = {{1,2},{1,3},{2,4}};\n    for (int i = 0; i &lt; 3; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    int vals[] = {0,3,2,1,4};\n    for (int i = 1; i &lt;= n; i++) value[i] = vals[i];\n    dfs(1, -1);\n    int ans = dp[1][0] &gt; dp[1][1] ? dp[1][0] : dp[1][1];\n    printf(\"Max independent set sum: %d\\n\", ans);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\nvalue = {1:3, 2:2, 3:1, 4:4}\ndp = {}\n\ndef dfs(u, p):\n    include = value[u]\n    exclude = 0\n    for v in g[u]:\n        if v == p: continue\n        dfs(v, u)\n        include += dp[v][0]\n        exclude += max(dp[v][0], dp[v][1])\n    dp[u] = (exclude, include)\n\ndfs(1, -1)\nprint(\"Max independent set sum:\", max(dp[1]))\n\n\nWhy It Matters\n\nFoundation for tree-based constraint problems\nUsed in network stability, resource allocation, scheduling\nExtensible to weighted graphs, forests, ranged constraints\n\nPatterns derived from this:\n\nVertex cover (complement)\nHouse robber on trees\nDynamic inclusion-exclusion states\n\n\n\nA Gentle Proof (Why It Works)\nWe prove by induction:\n\nBase case: Leaf node \\(u\\) has \\(dp[u][1]=value[u]\\), \\(dp[u][0]=0\\), correct.\nInductive step: For any node \\(u\\), if all subtrees compute optimal values, including \\(u\\) adds its value and excludes children (\\(dp[v][0]\\)), excluding \\(u\\) allows best child choice (\\(\\max(dp[v][0], dp[v][1])\\)). Thus each subtree is optimal, ensuring global optimality.\n\n\n\nTry It Yourself\n\nExtend to weighted edges (where cost is per edge)\nModify to reconstruct chosen nodes\nImplement for forest (multiple trees)\nCompare with vertex cover DP\nApply to House Robber III (Leetcode 337)\n\n\n\nTest Cases\n\n\n\nTree\nValues\nAnswer\n\n\n\n\n1–2–3\n{1:3,2:2,3:1}\n{1,3} sum=4\n\n\nStar 1–{2,3,4}\n{1:5, others:3}\n{2,3,4} sum=9\n\n\nChain 1–2–3–4\n{1:1,2:4,3:5,4:4}\n{2,4} sum=8\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) recursion\n\nIndependent Set DP captures the tree’s quiet balance, every node’s choice echoes through its branches.\n\n\n\n474 Vertex Cover DP\nVertex Cover DP solves a classic tree optimization problem: choose the smallest set of nodes such that every edge in the tree has at least one endpoint selected. This complements the Independent Set DP, together they form a dual pair in combinatorial optimization.\n\nWhat Problem Are We Solving?\nGiven a tree with \\(n\\) nodes, find the minimum vertex cover, i.e. a smallest subset of nodes such that every edge \\((u,v)\\) has \\(u\\) or \\(v\\) in the set.\nWe define two states for each node \\(u\\):\n\n\\(dp[u][0]\\): minimum size of vertex cover in subtree rooted at \\(u\\), when \\(u\\) is not included\n\\(dp[u][1]\\): minimum size of vertex cover in subtree rooted at \\(u\\), when \\(u\\) is included\n\nRecurrence:\n\\[\ndp[u][0] = \\sum_{v \\in children(u)} dp[v][1]\n\\]\n\\[\ndp[u][1] = 1 + \\sum_{v \\in children(u)} \\min(dp[v][0], dp[v][1])\n\\]\nIf \\(u\\) is not in the cover, all its children must be included to cover edges \\((u,v)\\). If \\(u\\) is included, each child can choose whether or not to join the cover.\nAnswer: \\(\\min(dp[root][0], dp[root][1])\\)\n\n\nHow Does It Work (Plain Language)\nEach node decides whether to take responsibility for covering edges or delegate that responsibility to its children. This is a mutual-exclusion constraint:\n\nIf \\(u\\) is excluded, its children must be included.\nIf \\(u\\) is included, each child is free to choose.\n\n\n\nStep-by-Step Example\nTree:\n    1\n   / \\\n  2   3\n / \\\n4   5\nWe compute bottom-up:\n\nLeaves 4,5: \\(dp[4][0]=0\\), \\(dp[4][1]=1\\) \\(dp[5][0]=0\\), \\(dp[5][1]=1\\)\nNode 2: \\(dp[2][0]=dp[4][1]+dp[5][1]=2\\) \\(dp[2][1]=1+\\min(dp[4][0],dp[4][1])+\\min(dp[5][0],dp[5][1])=1+0+0=1\\)\nNode 3: \\(dp[3][0]=0\\), \\(dp[3][1]=1\\)\nNode 1: \\(dp[1][0]=dp[2][1]+dp[3][1]=1+1=2\\) \\(dp[1][1]=1+\\min(dp[2][0],dp[2][1])+\\min(dp[3][0],dp[3][1])=1+1+0=2\\)\n\nResult: \\(\\min(2,2)=2\\)\nMinimum vertex cover size = 2\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint adj[MAXN][MAXN], deg[MAXN];\nint dp[MAXN][2];\nint visited[MAXN];\n\nvoid dfs(int u, int p) {\n    dp[u][0] = 0;\n    dp[u][1] = 1;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dfs(v, u);\n        dp[u][0] += dp[v][1];\n        dp[u][1] += (dp[v][0] &lt; dp[v][1] ? dp[v][0] : dp[v][1]);\n    }\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs(1, -1);\n    int ans = dp[1][0] &lt; dp[1][1] ? dp[1][0] : dp[1][1];\n    printf(\"Minimum vertex cover: %d\\n\", ans);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4),(2,5)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\ndp = {}\n\ndef dfs(u, p):\n    include = 1\n    exclude = 0\n    for v in g[u]:\n        if v == p: continue\n        dfs(v, u)\n        exclude += dp[v][1]\n        include += min(dp[v][0], dp[v][1])\n    dp[u] = (exclude, include)\n\ndfs(1, -1)\nprint(\"Minimum vertex cover:\", min(dp[1]))\n\n\nWhy It Matters\n\nFundamental for constraint satisfaction problems on trees\nDual to Independent Set DP (by complement)\nUsed in network design, task monitoring, sensor placement\n\nMany graph algorithms (on trees) rely on this cover-or-skip dichotomy, including:\n\nDominating sets\nGuard problems\nMinimum cameras in binary tree (Leetcode 968)\n\n\n\nA Gentle Proof (Why It Works)\nBy induction:\n\nBase case: Leaf \\(u\\): \\(dp[u][0]=0\\) (if not covered, edge must be covered by parent), \\(dp[u][1]=1\\) (include self).\nInductive step: If all children have optimal covers,\n\nExcluding \\(u\\) forces inclusion of all \\(v\\) (ensures edges \\((u,v)\\) covered).\nIncluding \\(u\\) allows flexible optimal choices for children. Each node’s local decision yields global minimality since the tree has no cycles.\n\n\n\n\nTry It Yourself\n\nPrint actual cover set (backtrack from DP)\nExtend to weighted vertex cover (replace count with sum of weights)\nCompare with Independent Set DP, show complement sizes\nImplement iterative version using post-order\nApply to Minimum Cameras in Binary Tree\n\n\n\nTest Cases\n\n\n\nTree\nResult\nCover\n\n\n\n\n1–2–3\n1–2–3\n{2}\n\n\nStar 1–{2,3,4,5}\n{1}\n\n\n\nChain 1–2–3–4\n{2,4} or {1,3}\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nVertex Cover DP shows how including a node protects its edges, balancing economy and completeness across the tree.\n\n\n\n475 Path Counting DP\nPath Counting DP is a gentle entry point into tree combinatorics, it helps you count how many distinct paths exist under certain conditions, such as from root to leaves, between pairs, or with specific constraints. It builds intuition for counting with structure rather than brute force.\n\nWhat Problem Are We Solving?\nGiven a tree with \\(n\\) nodes, we want to count paths satisfying some property. The simplest form is counting the number of root-to-leaf paths, but we can generalize to:\n\nTotal number of paths between all pairs\nPaths with weight constraints\nPaths with certain node properties\n\nWe’ll start with the fundamental version, root-to-leaf paths.\nDefine:\n\\[\ndp[u] = \\text{number of paths starting at } u\n\\]\nFor a rooted tree, each node’s paths equal the sum of paths from its children. If \\(u\\) is a leaf, it contributes \\(1\\) path (just itself):\n\\[\ndp[u] =\n\\begin{cases}\n1, & \\text{if } u \\text{ is a leaf},\\\\\n\\displaystyle\\sum_{v \\in \\text{children}(u)} dp[v], & \\text{otherwise.}\n\\end{cases}\n\\]\nThe total number of root-to-leaf paths = \\(dp[root]\\).\n\n\nHow Does It Work (Plain Language)\nStart at the leaves, each leaf is one complete path. Each parent accumulates paths from its children: “Every path from my child forms one from me too.” This propagates upward until the root holds the total count.\n\n\nStep-by-Step Example\nTree:\n      1\n     / \\\n    2   3\n   / \\\n  4   5\nCompute \\(dp\\) from leaves upward:\n\n\\(dp[4] = 1\\), \\(dp[5] = 1\\), \\(dp[3] = 1\\)\n\\(dp[2] = dp[4] + dp[5] = 2\\)\n\\(dp[1] = dp[2] + dp[3] = 3\\)\n\nSo there are 3 root-to-leaf paths:\n\n1–2–4\n1–2–5\n1–3\n\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint adj[MAXN][MAXN], deg[MAXN];\nint dp[MAXN];\n\nint dfs(int u, int p) {\n    int count = 0;\n    int isLeaf = 1;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        isLeaf = 0;\n        count += dfs(v, u);\n    }\n    if (isLeaf) return dp[u] = 1;\n    return dp[u] = count;\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs(1, -1);\n    printf(\"Root-to-leaf paths: %d\\n\", dp[1]);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4),(2,5)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\ndp = {}\n\ndef dfs(u, p):\n    is_leaf = True\n    count = 0\n    for v in g[u]:\n        if v == p: continue\n        is_leaf = False\n        count += dfs(v, u)\n    dp[u] = 1 if is_leaf else count\n    return dp[u]\n\ndfs(1, -1)\nprint(\"Root-to-leaf paths:\", dp[1])\n\n\nWhy It Matters\n\nFoundation for counting problems on trees\nForms the basis of path-sum DP, tree DP rerooting, and combinatorial enumeration\nEssential in probabilistic models and decision trees\nUseful for probability propagation and branching process simulation\n\n\n\nA Gentle Proof (Why It Works)\nWe can prove by induction:\n\nBase case: Leaf node \\(u\\) has exactly one path, itself. So \\(dp[u]=1\\).\nInductive step: Assume all children \\(v\\) compute correct counts \\(dp[v]\\). Then \\(dp[u] = \\sum dp[v]\\) counts all distinct root-to-leaf paths passing through \\(u\\).\n\nSince every path is uniquely identified by its first branching decision, we never double-count.\n\n\nTry It Yourself\n\nModify to count all simple paths (pairs \\((u,v)\\)).\nAdd edge weights and count paths with total sum \\(\\le K\\).\nTrack and print all root-to-leaf paths using recursion stack.\nExtend to directed acyclic graphs (DAGs).\nCombine with rerooting to count paths through each node.\n\n\n\nTest Cases\n\n\n\nTree\nPaths\n\n\n\n\n1–2–3\n1 path\n\n\nStar (1–{2,3,4})\n3 paths\n\n\nBinary tree depth 2\n3 paths\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) (each node visited once)\nSpace: \\(O(n)\\) recursion\n\nPath Counting DP shows how structure transforms into number, one traversal, all paths accounted.\n\n\n\n476 DP on Rooted Tree\nDP on Rooted Tree is the most general pattern of tree dynamic programming, it teaches you to reason about states on hierarchical structures. Every subtree contributes partial answers, and a parent combines them. This is the building block for almost every tree-based DP: sums, counts, distances, constraints, and beyond.\n\nWhat Problem Are We Solving?\nWe want to compute a property for each node based on its subtree, things like:\n\nSubtree sum\nSubtree size\nMaximum depth\nPath counts\nModular products\nCombinatorial counts\n\nGiven a rooted tree, we define a DP function that recursively collects results from each child and aggregates them.\nGeneric form:\n\\[\ndp[u] = f(u, {dp[v] : v \\in children(u)})\n\\]\nYou define:\n\nBase case (usually for leaves)\nTransition function (combine children’s results)\nMerge operation (sum, max, min, multiply, etc.)\n\n\n\nHow Does It Work (Plain Language)\nYou think from the leaves upward. Each node:\n\nCollects results from its children.\nApplies a combining function.\nStores a final value.\n\nThis bottom-up reasoning mirrors post-order traversal, solve children first, then parent.\nThe power is that \\(f\\) can represent any operation: sum, min, max, or even bitmask merge.\n\n\nStep-by-Step Example\nLet’s compute subtree size for every node (number of nodes in its subtree):\nRecurrence:\n\\[\ndp[u] = 1 + \\sum_{v \\in children(u)} dp[v]\n\\]\nTree:\n      1\n     / \\\n    2   3\n   / \\\n  4   5\nCompute bottom-up:\n\n\\(dp[4]=1\\), \\(dp[5]=1\\)\n\\(dp[2]=1+1+1=3\\)\n\\(dp[3]=1\\)\n\\(dp[1]=1+3+1=5\\)\n\nSo:\n\n\\(dp[1]=5\\)\n\\(dp[2]=3\\)\n\\(dp[3]=1\\)\n\\(dp[4]=1\\)\n\\(dp[5]=1\\)\n\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint adj[MAXN][MAXN], deg[MAXN];\nint dp[MAXN];\n\nint dfs(int u, int p) {\n    dp[u] = 1; // count itself\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dp[u] += dfs(v, u);\n    }\n    return dp[u];\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs(1, -1);\n    for (int i = 1; i &lt;= n; i++)\n        printf(\"dp[%d] = %d\\n\", i, dp[i]);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4),(2,5)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\ndp = {}\n\ndef dfs(u, p):\n    dp[u] = 1\n    for v in g[u]:\n        if v == p: continue\n        dp[u] += dfs(v, u)\n    return dp[u]\n\ndfs(1, -1)\nprint(dp)\n\n\nWhy It Matters\n\nCore template for any tree DP problem\nPowers algorithms like subtree sum, depth counting, modular product aggregation, path count, and rerooting\nFoundation for advanced rerooting DP, where answers depend on parent and sibling states\n\nOnce you master this pattern, you can:\n\nChange the recurrence → change the problem\nAdd constraints → introduce multiple DP states\nExtend to graphs with DAG structure\n\n\n\nA Gentle Proof (Why It Works)\nInduction on tree height:\n\nBase case: Leaf \\(u\\) → \\(dp[u]\\) initialized to base (e.g. 1 or value[u]).\nInductive step: Suppose all children \\(v\\) compute correct \\(dp[v]\\). Then \\(f(u, {dp[v]})\\) aggregates subtree results correctly.\n\nSince trees are acyclic, post-order guarantees children are processed first, correctness follows.\n\n\nTry It Yourself\n\nChange recurrence to compute sum of subtree values.\nCompute maximum subtree depth.\nTrack count of leaves in each subtree.\nExtend to two-state DP, e.g. include/exclude logic.\nCombine with rerooting to compute value for every root.\n\n\n\nTest Cases\n\n\n\nTree\nSubtree Sizes\n\n\n\n\n1–2–3\n{1:3, 2:2, 3:1}\n\n\nStar 1–{2,3,4,5}\n{1:5, others:1}\n\n\nChain 1–2–3–4\n{1:4,2:3,3:2,4:1}\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) recursion\n\nDP on Rooted Tree is your canvas, define \\(f\\), and paint the structure’s logic across its branches.\n\n\n\n477 Rerooting Technique\nThe Rerooting Technique is a powerful pattern in tree dynamic programming that allows you to compute results for every node as root, in linear time. Instead of recalculating from scratch for each root, we “reroot” efficiently by reusing already computed subtree results.\n\nWhat Problem Are We Solving?\nSuppose we’ve computed a property (like subtree sum, subtree size, distance sum) rooted at a fixed node, say node \\(1\\). Now we want to compute the same property for all nodes as root, for example:\n\nSum of distances from each node to all others\nSize or cost of each subtree when rooted differently\nCount of paths or contributions that depend on root position\n\nWe define:\n\\[\ndp[u] = f({dp[v]}_{v \\in children(u)})\n\\]\nBut we also want \\(res[u]\\), the answer when the tree is rooted at \\(u\\).\nBy rerooting, we can transfer results along edges, moving the root and updating only local contributions.\n\n\nHow Does It Work (Plain Language)\nWe first perform a post-order DFS to compute \\(dp[u]\\) for every node as if the root were fixed (e.g. node 1).\nThen, a second pre-order DFS “pushes” results outward — when moving root from parent \\(u\\) to child \\(v\\), we adjust contributions:\n\nRemove child’s part from parent\nAdd parent’s part to child\n\nEach rerooting step updates \\(res[v]\\) from \\(res[u]\\) in constant or small time.\nThis two-pass structure is the hallmark of rerooting DP:\n\nDownward pass: gather subtree results\nUpward pass: propagate parent contributions\n\n\n\nStep-by-Step Example: Sum of Distances\nGoal: for each node, compute sum of distances to all other nodes.\nTree:\n      1\n     / \\\n    2   3\n   / \\\n  4   5\nStep 1: Post-order (subtree sums + sizes)\nFor each node:\n\n\\(subtree_size[u]\\) = number of nodes in subtree of \\(u\\)\n\\(dp[u]\\) = sum of distances from \\(u\\) to nodes in its subtree\n\nRecurrence:\n\\[\nsubtree_size[u] = 1 + \\sum subtree_size[v]\n\\]\n\\[\ndp[u] = \\sum (dp[v] + subtree_size[v])\n\\]\nAt root (1), \\(dp[1]=8\\) (sum of distances from 1 to all nodes).\nStep 2: Pre-order (rerooting)\nWhen rerooting from \\(u\\) to \\(v\\):\n\nMoving root away from \\(v\\) adds \\((n - subtree_size[v])\\)\nMoving root toward \\(v\\) subtracts \\(subtree_size[v]\\)\n\nSo: \\[\ndp[v] = dp[u] + (n - 2 \\times subtree_size[v])\n\\]\nNow every node has its distance sum in \\(O(n)\\).\n\n\nTiny Code (Sum of Distances)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint adj[MAXN][MAXN], deg[MAXN];\nint dp[MAXN], subtree[MAXN], res[MAXN];\n\nvoid dfs1(int u, int p) {\n    subtree[u] = 1;\n    dp[u] = 0;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dfs1(v, u);\n        subtree[u] += subtree[v];\n        dp[u] += dp[v] + subtree[v];\n    }\n}\n\nvoid dfs2(int u, int p) {\n    res[u] = dp[u];\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        int pu = dp[u], pv = dp[v];\n        int su = subtree[u], sv = subtree[v];\n\n        // move root u -&gt; v\n        dp[u] -= dp[v] + subtree[v];\n        subtree[u] -= subtree[v];\n        dp[v] += dp[u] + subtree[u];\n        subtree[v] += subtree[u];\n\n        dfs2(v, u);\n\n        // restore\n        dp[v] = pv;\n        dp[u] = pu;\n        subtree[v] = sv;\n        subtree[u] = su;\n    }\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs1(1, -1);\n    dfs2(1, -1);\n    for (int i = 1; i &lt;= n; i++)\n        printf(\"res[%d] = %d\\n\", i, res[i]);\n}\nPython\nfrom collections import defaultdict\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4),(2,5)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\nn = 5\ndp = {i:0 for i in range(1,n+1)}\nsub = {i:1 for i in range(1,n+1)}\nres = {}\n\ndef dfs1(u,p):\n    sub[u]=1\n    dp[u]=0\n    for v in g[u]:\n        if v==p: continue\n        dfs1(v,u)\n        sub[u]+=sub[v]\n        dp[u]+=dp[v]+sub[v]\n\ndef dfs2(u,p):\n    res[u]=dp[u]\n    for v in g[u]:\n        if v==p: continue\n        pu,pv=dp[u],dp[v]\n        su,sv=sub[u],sub[v]\n\n        dp[u]-=dp[v]+sub[v]\n        sub[u]-=sub[v]\n        dp[v]+=dp[u]+sub[u]\n        sub[v]+=sub[u]\n\n        dfs2(v,u)\n\n        dp[u],dp[v]=pu,pv\n        sub[u],sub[v]=su,sv\n\ndfs1(1,-1)\ndfs2(1,-1)\nprint(res)\n\n\nWhy It Matters\n\nCompute answers for all nodes in \\(O(n)\\)\nEssential in distance sums, rerooted subtree queries, centroid-based algorithms\nCore pattern in Tree Rerooting DP problems on AtCoder, Codeforces, Leetcode\n\nRerooting transforms one-root logic into every-root knowledge.\n\n\nA Gentle Proof (Why It Works)\n\nFirst pass ensures each node knows its subtree contribution.\nSecond pass applies a constant-time update to shift the root:\n\nRemove child’s contribution\nAdd parent’s complement\n\n\nSince each edge is traversed twice, total cost is linear.\n\n\nTry It Yourself\n\nCount number of nodes in subtree for every possible root.\nCompute sum of depths for every root.\nModify recurrence for product of subtree values.\nApply to tree balancing: minimize total distance.\nExtend to weighted trees.\n\n\n\nTest Cases\n\n\n\nTree\nRoot\nDistance Sum\n\n\n\n\n1–2–3\n1\n3\n\n\n1–2–3\n2\n2\n\n\n1–2–3\n3\n3\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nRerooting DP is your algorithmic kaleidoscope, spin the root, and watch all perspectives appear.\n\n\n\n478 Distance Sum Rerooting\nDistance Sum Rerooting is a classic and elegant application of the rerooting technique. It computes, for every node, the sum of distances to all other nodes in a tree, in just O(n) time. It’s one of the cleanest examples showing how rerooting transforms local subtree data into global insight.\n\nWhat Problem Are We Solving?\nGiven a tree with \\(n\\) nodes, for every node \\(u\\), compute:\n\\[\nres[u] = \\sum_{v=1}^{n} \\text{dist}(u, v)\n\\]\nA naïve approach (running BFS from every node) takes \\(O(n^2)\\). We’ll do it in two DFS passes using rerooting DP.\n\n\nHow Does It Work (Plain Language)\n\nFirst pass (post-order): Root the tree at an arbitrary node (say, 1). Compute:\n\n\\(subtree[u]\\): size of subtree rooted at \\(u\\)\n\\(dp[u]\\): sum of distances from \\(u\\) to all nodes in its subtree\n\nRecurrence:\n\\[\nsubtree[u] = 1 + \\sum_{v \\in children(u)} subtree[v]\n\\]\n\\[\ndp[u] = \\sum_{v \\in children(u)} (dp[v] + subtree[v])\n\\]\nSecond pass (pre-order): Use rerooting to compute \\(res[v]\\) from \\(res[u]\\):\n\\[\nres[v] = res[u] + (n - 2 \\times subtree[v])\n\\]\n\nMoving root from \\(u\\) to \\(v\\):\n\nNodes inside \\(v\\)’s subtree get 1 closer\nNodes outside get 1 farther\n\nSo net change = \\(-subtree[v] + (n - subtree[v]) = n - 2 \\times subtree[v]\\)\n\n\\(res[1] = dp[1]\\) (initial result for root)\n\nAfter this, each node has its sum of distances, all in linear time.\n\n\nStep-by-Step Example\nTree:\n      1\n     / \\\n    2   3\n   / \\\n  4   5\nStep 1 (Post-order)\n\n\\(dp[4]=0,\\ dp[5]=0,\\ dp[3]=0\\)\n\\(subtree[4]=1,\\ subtree[5]=1,\\ subtree[3]=1\\)\n\\(subtree[2]=1+1+1=3,\\ dp[2]=dp[4]+dp[5]+subtree[4]+subtree[5]=0+0+1+1=2\\)\n\\(subtree[1]=1+3+1=5,\\ dp[1]=dp[2]+dp[3]+subtree[2]+subtree[3]=2+0+3+1=6\\)\n\nSo \\(res[1]=6\\).\nStep 2 (Reroot)\n\n\\(res[2]=res[1]+(5-2\\times3)=6-1=5\\)\n\\(res[3]=res[1]+(5-2\\times1)=6+3=9\\)\n\\(res[4]=res[2]+(5-2\\times1)=5+3=8\\)\n\\(res[5]=res[2]+(5-2\\times1)=5+3=8\\)\n\n✅ Final:\n\n\\(res[1]=6\\), \\(res[2]=5\\), \\(res[3]=9\\), \\(res[4]=8\\), \\(res[5]=8\\)\n\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n\nint n;\nint adj[MAXN][MAXN], deg[MAXN];\nint subtree[MAXN], dp[MAXN], res[MAXN];\n\nvoid dfs1(int u, int p) {\n    subtree[u] = 1;\n    dp[u] = 0;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dfs1(v, u);\n        subtree[u] += subtree[v];\n        dp[u] += dp[v] + subtree[v];\n    }\n}\n\nvoid dfs2(int u, int p) {\n    res[u] = dp[u];\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dp[v] = dp[u] + (n - 2 * subtree[v]);\n        dfs2(v, u);\n    }\n}\n\nint main() {\n    n = 5;\n    int edges[][2] = {{1,2},{1,3},{2,4},{2,5}};\n    for (int i = 0; i &lt; 4; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs1(1, -1);\n    dfs2(1, -1);\n    for (int i = 1; i &lt;= n; i++)\n        printf(\"Sum of distances from %d = %d\\n\", i, res[i]);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2),(1,3),(2,4),(2,5)]\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\nn = 5\ndp = {i:0 for i in range(1,n+1)}\nsub = {i:1 for i in range(1,n+1)}\nres = {}\n\ndef dfs1(u,p):\n    sub[u]=1\n    dp[u]=0\n    for v in g[u]:\n        if v==p: continue\n        dfs1(v,u)\n        sub[u]+=sub[v]\n        dp[u]+=dp[v]+sub[v]\n\ndef dfs2(u,p):\n    res[u]=dp[u]\n    for v in g[u]:\n        if v==p: continue\n        dp[v]=dp[u]+(n-2*sub[v])\n        dfs2(v,u)\n\ndfs1(1,-1)\ndfs2(1,-1)\nprint(res)\n\n\nWhy It Matters\n\nComputes sum of distances for every node in linear time\nA foundational rerooting example, applies to many other metrics (sums, products, min/max)\nExtensible to weighted edges, directed trees, and centroid decomposition\nUseful in graph analysis, network latency, tree balancing, dynamic centers\n\n\n\nA Gentle Proof (Why It Works)\nEach reroot step adjusts the sum of distances by accounting for nodes that become closer or farther:\n\nNodes in the new root’s subtree (\\(subtree[v]\\)): distances decrease by 1\nOthers (\\(n - subtree[v]\\)): distances increase by 1\n\nSo:\n\\[\nres[v] = res[u] + (n - 2 \\times subtree[v])\n\\]\nBy induction across edges, each node gets correct total distance.\n\n\nTry It Yourself\n\nExtend to weighted edges.\nCompute average distance per node.\nCombine with centroid finding (node minimizing \\(res[u]\\)).\nVisualize change in \\(res\\) as root slides.\nAdapt for directed rooted trees.\n\n\n\nTest Cases\n\n\n\nTree\nNode\nResult\n\n\n\n\n1–2–3\n1\n3\n\n\n1–2–3\n2\n2\n\n\n1–2–3\n3\n3\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nDistance Sum Rerooting shows the beauty of symmetry in trees, move the root, update the world, keep it all in balance.\n\n\n\n479 Tree Coloring DP\nTree Coloring DP is a versatile pattern for solving coloring and labeling problems on trees under local constraints, for example, counting how many valid colorings exist when adjacent nodes cannot share a color, or minimizing cost under adjacency restrictions. It combines state definition with child aggregation, forming one of the most common templates in competitive programming.\n\nWhat Problem Are We Solving?\nGiven a tree with \\(n\\) nodes, we want to color each node with one of \\(k\\) colors so that no two adjacent nodes share the same color, and count the number of valid colorings.\nFormally, find the number of ways to assign \\(color[u] \\in {1,2,\\dots,k}\\) such that for every edge \\((u,v)\\), \\(color[u] \\ne color[v]\\).\nWe can also generalize:\n\nWeighted versions (cost per color)\nRestricted versions (pre-colored nodes)\nModular counting (\\(\\bmod\\ 10^9+7\\))\n\nHere, we’ll solve the basic unweighted counting version.\n\n\nHow Does It Work (Plain Language)\nWe do a rooted DP where each node decides its color and multiplies the valid combinations from its children.\nIf a node \\(u\\) is colored \\(c\\), then each child \\(v\\) can take any color except \\(c\\).\nSo for each node:\n\\[\ndp[u][c] = \\prod_{v \\in children(u)} \\sum_{\\substack{c' = 1 \\ c' \\ne c}}^{k} dp[v][c']\n\\]\nFinally, the total count is:\n\\[\n\\text{Answer} = \\sum_{c=1}^{k} dp[root][c]\n\\]\nBecause the tree is acyclic, we can safely combine subtrees without overcounting.\n\n\nStep-by-Step Example\nTree:\n    1\n   / \\\n  2   3\n\\(k=3\\) colors (1,2,3)\nStart from leaves:\n\nFor leaf node \\(v\\), \\(dp[v][c] = 1\\) for all \\(c \\in {1,2,3}\\) (any color works)\n\nNow node 2 and 3:\n\n\\(dp[2] = dp[3] = [1,1,1]\\)\n\nAt node 1:\n\n\\(dp[1][1] = \\prod_{child} \\sum_{c' \\ne 1} dp[child][c'] = (1+1)*(1+1)=4\\)\n\\(dp[1][2] = (1+1)*(1+1)=4\\)\n\\(dp[1][3] = (1+1)*(1+1)=4\\)\n\nTotal = \\(4+4+4 = 12\\) valid colorings\nManual check: each of 3 colors for node 1 × 2 choices per child × 2 children = 12 ✅\n\n\nTiny Code (Easy Version)\nC\n#include &lt;stdio.h&gt;\n#define MAXN 100\n#define MAXK 10\n#define MOD 1000000007\n\nint n, k;\nint adj[MAXN][MAXN], deg[MAXN];\nlong long dp[MAXN][MAXK+1];\n\nvoid dfs(int u, int p) {\n    for (int c = 1; c &lt;= k; c++) dp[u][c] = 1;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p) continue;\n        dfs(v, u);\n        for (int c = 1; c &lt;= k; c++) {\n            long long sum = 0;\n            for (int c2 = 1; c2 &lt;= k; c2++) {\n                if (c2 == c) continue;\n                sum = (sum + dp[v][c2]) % MOD;\n            }\n            dp[u][c] = (dp[u][c] * sum) % MOD;\n        }\n    }\n}\n\nint main() {\n    n = 3; k = 3;\n    int edges[][2] = {{1,2},{1,3}};\n    for (int i = 0; i &lt; 2; i++) {\n        int a = edges[i][0], b = edges[i][1];\n        adj[a][deg[a]++] = b;\n        adj[b][deg[b]++] = a;\n    }\n    dfs(1, -1);\n    long long ans = 0;\n    for (int c = 1; c &lt;= k; c++) ans = (ans + dp[1][c]) % MOD;\n    printf(\"Total colorings: %lld\\n\", ans);\n}\nPython\nfrom collections import defaultdict\n\nMOD = 109 + 7\nn, k = 3, 3\nedges = [(1,2),(1,3)]\n\ng = defaultdict(list)\nfor u,v in edges:\n    g[u].append(v)\n    g[v].append(u)\n\ndp = {}\n\ndef dfs(u, p):\n    dp[u] = [1]*(k+1)\n    for v in g[u]:\n        if v == p: continue\n        dfs(v, u)\n        new = [0]*(k+1)\n        for c in range(1, k+1):\n            total = 0\n            for c2 in range(1, k+1):\n                if c2 == c: continue\n                total = (total + dp[v][c2]) % MOD\n            new[c] = (dp[u][c] * total) % MOD\n        dp[u] = new\n\ndfs(1, -1)\nans = sum(dp[1][1:]) % MOD\nprint(\"Total colorings:\", ans)\n\n\nWhy It Matters\n\nSolves coloring, labeling, and assignment problems on trees\nFoundation for constraint satisfaction DPs\nExtensible to weighted, modular, and partial pre-colored versions\nAppears in graph theory, combinatorics, and tree-structured probabilistic models\n\nWith small tweaks, it becomes:\n\nMinimum-cost coloring (replace + with min)\nConstraint coloring (prune invalid colors)\nModular counting (for combinatorics)\n\n\n\nA Gentle Proof (Why It Works)\nBy induction on tree height:\n\nBase case: Leaf node \\(u\\): \\(dp[u][c]=1\\) (can take any color)\nInductive step: Suppose each child \\(v\\) has computed correct \\(dp[v][c']\\). For node \\(u\\) colored \\(c\\), all children \\(v\\) must choose colors \\(c' \\ne c\\). Summing and multiplying ensures we count all valid combinations.\n\nNo overlap or omission occurs because trees have no cycles.\n\n\nTry It Yourself\n\nAdd modular constraint (e.g. \\(k=10^5\\)).\nExtend to pre-colored nodes: fix certain \\(dp[u][c] = 0/1\\).\nModify recurrence for weighted coloring (cost per color).\nOptimize with prefix-suffix products for large \\(k\\).\nApply to binary tree coloring with parity constraints.\n\n\n\nTest Cases\n\n\n\nTree\nn\nk\nResult\n\n\n\n\n1–2\n2\n2\n2\n\n\n1–2–3\n3\n3\n12\n\n\nStar 1–{2,3,4}\n4\n3\n24\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\times k^2)\\)\nSpace: \\(O(n \\times k)\\)\n\nTree Coloring DP is your combinatorial paintbrush, define local rules, traverse once, and color the whole forest with logic.\n\n\n\n480 Binary Search on Tree DP\nBinary Search on Tree DP is a hybrid strategy combining tree dynamic programming with binary search over an answer space. It’s especially useful when the feasibility of a condition is monotonic, for example, when asking “is there a subtree/path satisfying constraint X under threshold T?” and the answer changes from false → true as T increases.\n\nWhat Problem Are We Solving?\nGiven a tree with weights or values on nodes or edges, we want to find a minimum (or maximum) threshold \\(T\\) such that a property holds, e.g.:\n\nLongest path with all edge weights ≤ \\(T\\)\nSmallest \\(T\\) such that there exists a subtree of sum ≥ \\(S\\)\nMinimal limit where a valid DP state becomes achievable\n\nWe binary search over \\(T\\), and for each guess, we run a DP on the tree to check if the condition is satisfied.\n\n\nHow Does It Work (Plain Language)\n\nIdentify a monotonic property, one that, once true, stays true (or once false, stays false).\nDefine a check(T) function using Tree DP that returns whether the property holds.\nApply binary search over \\(T\\) to find the smallest (or largest) value satisfying the condition.\n\n\n\nExample: Longest Path Under Limit\nWe’re given a weighted tree with edge weights \\(w(u,v)\\). Find the maximum path length such that all edges ≤ T. We want the minimum T for which path length ≥ L.\nSteps:\n\nBinary search over \\(T\\)\nFor each \\(T\\), build a subgraph of edges ≤ \\(T\\)\nRun DP on tree (e.g. diameter DP) to check if a path of length ≥ L exists\n\n\n\nDP Design\nWe use a DFS-based DP that computes, for each node:\n\\[\ndp[u] = \\text{length of longest downward path under } T\n\\]\nand combine two best child paths to check if the diameter ≥ L.\n\n\nTiny Code (Feasibility Check)\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#define MAXN 100\n#define INF 1000000000\n\nint n, L;\nint adj[MAXN][MAXN], w[MAXN][MAXN], deg[MAXN];\nint best;\n\nint dfs(int u, int p, int T) {\n    int top1 = 0, top2 = 0;\n    for (int i = 0; i &lt; deg[u]; i++) {\n        int v = adj[u][i];\n        if (v == p || w[u][v] &gt; T) continue;\n        int len = dfs(v, u, T) + 1;\n        if (len &gt; top1) { top2 = top1; top1 = len; }\n        else if (len &gt; top2) top2 = len;\n    }\n    if (top1 + top2 &gt;= L) best = 1;\n    return top1;\n}\n\nint check(int T) {\n    best = 0;\n    dfs(1, -1, T);\n    return best;\n}\n\nint main() {\n    n = 4; L = 3;\n    int edges[3][3] = {{1,2,3},{2,3,5},{3,4,7}};\n    for (int i = 0; i &lt; 3; i++) {\n        int a=edges[i][0], b=edges[i][1], c=edges[i][2];\n        adj[a][deg[a]] = b; w[a][b] = c; deg[a]++;\n        adj[b][deg[b]] = a; w[b][a] = c; deg[b]++;\n    }\n    int lo = 0, hi = 10, ans = -1;\n    while (lo &lt;= hi) {\n        int mid = (lo + hi)/2;\n        if (check(mid)) { ans = mid; hi = mid - 1; }\n        else lo = mid + 1;\n    }\n    printf(\"Minimum T: %d\\n\", ans);\n}\nPython\nfrom collections import defaultdict\n\ng = defaultdict(list)\nedges = [(1,2,3),(2,3,5),(3,4,7)]\nn, L = 4, 3\nfor u,v,w in edges:\n    g[u].append((v,w))\n    g[v].append((u,w))\n\ndef dfs(u, p, T):\n    top1 = top2 = 0\n    global ok\n    for v,w in g[u]:\n        if v == p or w &gt; T: continue\n        length = dfs(v, u, T) + 1\n        if length &gt; top1:\n            top2 = top1\n            top1 = length\n        elif length &gt; top2:\n            top2 = length\n    if top1 + top2 &gt;= L:\n        ok = True\n    return top1\n\ndef check(T):\n    global ok\n    ok = False\n    dfs(1, -1, T)\n    return ok\n\nlo, hi = 0, 10\nans = -1\nwhile lo &lt;= hi:\n    mid = (lo + hi)//2\n    if check(mid):\n        ans = mid\n        hi = mid - 1\n    else:\n        lo = mid + 1\nprint(\"Minimum T:\", ans)\n\n\nWhy It Matters\n\nMany threshold optimization problems rely on binary search + DP\nIdeal when cost / limit interacts with tree-based structure\nUseful in network design, path constraints, tree queries, game theory\n\nExamples:\n\nSmallest edge weight for connectivity\nMinimal node cost for subtree property\nPath feasibility under resource constraint\n\n\n\nA Gentle Proof (Why It Works)\nIf the property is monotonic, binary search guarantees correctness:\n\nIf a condition holds at \\(T\\), it holds at all \\(T' &gt; T\\)\nSo, we can search for the smallest satisfying \\(T\\)\n\nTree DP correctly checks feasibility because it enumerates all root-to-leaf and child-to-child paths under threshold \\(T\\).\n\n\nTry It Yourself\n\nModify to maximize value (reverse monotonicity).\nReplace edge constraint with node value ≤ T.\nUse DP to count paths, not just check existence.\nApply to maximum subtree sum under bound.\nExtend to k-colored constraints (binary search over cost).\n\n\n\nTest Cases\n\n\n\nn\nL\nEdges\nOutput\n\n\n\n\n4\n3\n(1-2:3, 2-3:5, 3-4:7)\n5\n\n\n3\n2\n(1-2:1, 2-3:2)\n2\n\n\n\n\n\nComplexity\n\nDP per check: \\(O(n)\\)\nBinary search: \\(\\log(\\text{range})\\)\nTotal: \\(O(n\\log C)\\) where \\(C\\) is max edge weight\n\nBinary Search on Tree DP bridges feasibility logic and optimization, use it whenever monotonic thresholds and tree states meet.\n\n\n\n\nSection 49. DP Reconstruction and Traceback\n\n481 Reconstruct LCS\nReconstructing the Longest Common Subsequence (LCS) means not just computing its length, but tracing back the actual sequence that two strings share in order. This step turns abstract DP tables into tangible answers, a common need in bioinformatics, text diffing, and alignment tasks.\n\nWhat Problem Are We Solving?\nGiven two sequences \\(A\\) (length \\(n\\)) and \\(B\\) (length \\(m\\)), find the longest subsequence common to both (not necessarily contiguous).\nWe first build a DP table for LCS length:\n\\[\ndp[i][j] =\n\\begin{cases}\n0, & \\text{if } i = 0 \\text{ or } j = 0,\\\\\ndp[i-1][j-1] + 1, & \\text{if } A[i-1] = B[j-1],\\\\\n\\max(dp[i-1][j],\\ dp[i][j-1]), & \\text{otherwise.}\n\\end{cases}\n\\]\nThen we trace back from \\(dp[n][m]\\) to reconstruct the sequence.\n\n\nHow Does It Work (Plain Language)\n\nCompute the LCS length table using standard DP.\nStart from the bottom-right corner (\\(dp[n][m]\\)).\nTrace back:\n\nIf \\(A[i-1] == B[j-1]\\): add that character and move diagonally (\\(i-1, j-1\\))\nElse move to the direction with larger dp value\n\nReverse the collected sequence.\n\n\n\nExample\nLet \\(A = \\text{\"ABCBDAB\"}\\), \\(B = \\text{\"BDCABA\"}\\)\nDP length table leads to result “BCBA”.\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define MAX 100\n\nint dp[MAX][MAX];\nchar A[MAX], B[MAX];\nchar lcs[MAX];\n\nint main() {\n    scanf(\"%s %s\", A, B);\n    int n = strlen(A), m = strlen(B);\n    for (int i = 1; i &lt;= n; i++)\n        for (int j = 1; j &lt;= m; j++)\n            if (A[i-1] == B[j-1])\n                dp[i][j] = dp[i-1][j-1] + 1;\n            else\n                dp[i][j] = dp[i-1][j] &gt; dp[i][j-1] ? dp[i-1][j] : dp[i][j-1];\n\n    // Reconstruct\n    int i = n, j = m, k = dp[n][m];\n    lcs[k] = '\\0';\n    while (i &gt; 0 && j &gt; 0) {\n        if (A[i-1] == B[j-1]) {\n            lcs[--k] = A[i-1];\n            i--; j--;\n        } else if (dp[i-1][j] &gt;= dp[i][j-1])\n            i--;\n        else\n            j--;\n    }\n    printf(\"LCS: %s\\n\", lcs);\n}\nPython\ndef reconstruct_lcs(A, B):\n    n, m = len(A), len(B)\n    dp = [[0]*(m+1) for _ in range(n+1)]\n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            if A[i-1] == B[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n\n    i, j = n, m\n    res = []\n    while i &gt; 0 and j &gt; 0:\n        if A[i-1] == B[j-1]:\n            res.append(A[i-1])\n            i -= 1\n            j -= 1\n        elif dp[i-1][j] &gt;= dp[i][j-1]:\n            i -= 1\n        else:\n            j -= 1\n    return ''.join(reversed(res))\n\nA = input(\"A: \")\nB = input(\"B: \")\nprint(\"LCS:\", reconstruct_lcs(A, B))\n\n\nWhy It Matters\n\nCore of diff, merge, and DNA alignment tools\nDemonstrates how DP solutions can reconstruct actual solutions, not just counts\nFoundation for traceback techniques in many DP problems\n\n\n\nA Gentle Proof (Why It Works)\nAt each step:\n\nIf \\(A[i-1] = B[j-1]\\), the character must belong to the LCS, so we include it and move diagonally.\nOtherwise, the longer LCS lies in the direction of the greater dp value, hence we follow that path. By starting from \\(dp[n][m]\\) and moving backward, we guarantee each included character is part of at least one optimal solution.\n\nSince we collect in reverse order, reversing yields the correct sequence.\n\n\nTry It Yourself\n\nTrace the LCS of “ABCBDAB” and “BDCABA” by hand.\nModify to find one of all possible LCSs (handle ties).\nExtend for case-insensitive comparison.\nAdapt code to return indices of matching characters.\nVisualize path arrows in DP table.\n\n\n\nTest Cases\n\n\n\nA\nB\nLCS\n\n\n\n\n“ABCBDAB”\n“BDCABA”\n“BCBA”\n\n\n“AGGTAB”\n“GXTXAYB”\n“GTAB”\n\n\n“AXYT”\n“AYZX”\n“AY”\n\n\n\n\n\nComplexity\n\nTime: \\(O(nm)\\)\nSpace: \\(O(nm)\\) (can optimize to \\(O(\\min(n,m))\\) for length only)\n\nReconstruct LCS is your first step from number tables to actual solutions, bridging reasoning and reality.\n\n\n\n482 Reconstruct LIS\nReconstructing the Longest Increasing Subsequence (LIS) means finding not just the length of the longest increasing sequence, but the actual subsequence. This is a classic step beyond computing DP values, it’s about tracing how we got there.\n\nWhat Problem Are We Solving?\nGiven a sequence of numbers \\(A = [a_1, a_2, \\dots, a_n]\\), we want to find the longest strictly increasing subsequence. The DP version computes LIS length in \\(O(n^2)\\), but here we focus on reconstruction.\nWe define:\n\\[\ndp[i] = \\text{length of LIS ending at } i\n\\]\nand a parent array to track predecessors:\n\\[\nparent[i] = \\text{index of previous element in LIS ending at } i\n\\]\nFinally, we backtrack from the index of the maximum \\(dp[i]\\) to recover the sequence.\n\n\nHow Does It Work (Plain Language)\n\nCompute dp[i]: longest LIS ending at \\(A[i]\\).\nTrack parent[i]: where this sequence came from.\nFind max length index, call it best.\nBacktrack using parent array.\nReverse the reconstructed list.\n\n\n\nExample\nFor \\(A = [10, 22, 9, 33, 21, 50, 41, 60]\\)\nWe get:\n\ndp = [1, 2, 1, 3, 2, 4, 4, 5]\nLIS length = 5\nSequence = [10, 22, 33, 50, 60]\n\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n\nint main() {\n    int A[] = {10, 22, 9, 33, 21, 50, 41, 60};\n    int n = sizeof(A) / sizeof(A[0]);\n    int dp[n], parent[n];\n\n    for (int i = 0; i &lt; n; i++) {\n        dp[i] = 1;\n        parent[i] = -1;\n        for (int j = 0; j &lt; i; j++) {\n            if (A[j] &lt; A[i] && dp[j] + 1 &gt; dp[i]) {\n                dp[i] = dp[j] + 1;\n                parent[i] = j;\n            }\n        }\n    }\n\n    // Find index of LIS\n    int best = 0;\n    for (int i = 1; i &lt; n; i++)\n        if (dp[i] &gt; dp[best]) best = i;\n\n    // Reconstruct LIS\n    int lis[100], len = 0;\n    for (int i = best; i != -1; i = parent[i])\n        lis[len++] = A[i];\n\n    printf(\"LIS: \");\n    for (int i = len - 1; i &gt;= 0; i--)\n        printf(\"%d \", lis[i]);\n    printf(\"\\n\");\n}\nPython\ndef reconstruct_lis(A):\n    n = len(A)\n    dp = [1]*n\n    parent = [-1]*n\n\n    for i in range(n):\n        for j in range(i):\n            if A[j] &lt; A[i] and dp[j] + 1 &gt; dp[i]:\n                dp[i] = dp[j] + 1\n                parent[i] = j\n\n    best = max(range(n), key=lambda i: dp[i])\n\n    res = []\n    while best != -1:\n        res.append(A[best])\n        best = parent[best]\n    return res[::-1]\n\nA = [10, 22, 9, 33, 21, 50, 41, 60]\nprint(\"LIS:\", reconstruct_lis(A))\n\n\nWhy It Matters\n\nTransforms abstract DP into real sequence output\nUseful in scheduling, stock analysis, subsequence pattern recognition\nTeaches traceback technique with parent tracking, reused across many problems\n\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\(dp[i]\\) records the LIS length ending at \\(i\\). Whenever we update \\(dp[i] = dp[j] + 1\\), we’ve extended the best LIS ending at \\(j\\). Recording parent[i] = j lets us reconstruct that path.\nThe element with the maximum \\(dp[i]\\) must end one LIS, and by backtracking through parents, we trace exactly one valid increasing subsequence achieving the max length.\n\n\nTry It Yourself\n\nTrace LIS reconstruction for \\([3, 10, 2, 1, 20]\\)\nModify to return all LIS sequences (handle equal-length ties).\nAdapt code for non-decreasing LIS.\nCombine with binary search LIS for \\(O(n \\log n)\\) + parent tracking.\nVisualize parent links as arrows between indices.\n\n\n\nTest Cases\n\n\n\nInput\nLIS\nLength\n\n\n\n\n[10, 22, 9, 33, 21, 50, 41, 60]\n[10, 22, 33, 50, 60]\n5\n\n\n[3, 10, 2, 1, 20]\n[3, 10, 20]\n3\n\n\n[50, 3, 10, 7, 40, 80]\n[3, 7, 40, 80]\n4\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(n)\\)\n\nReconstruct LIS is a gentle bridge from computing a number to seeing the story behind it, each element tracing its lineage through the DP table.\n\n\n\n483 Reconstruct Knapsack\nReconstructing the Knapsack solution means identifying which items form the optimal value, not just knowing the maximum value. This is the difference between understanding what’s possible and what to choose.\n\nWhat Problem Are We Solving?\nGiven:\n\n\\(n\\) items with values \\(v[i]\\) and weights \\(w[i]\\)\nCapacity \\(W\\)\n\nWe want:\n\nMaximize total value without exceeding \\(W\\)\nRecover the chosen items\n\nThe 0/1 knapsack DP table is defined as:\n\\[\ndp[i][w] =\n\\begin{cases}\n0, & \\text{if } i = 0 \\text{ or } w = 0,\\\\\ndp[i-1][w], & \\text{if } w_i &gt; w,\\\\\n\\max(dp[i-1][w],\\ dp[i-1][w - w_i] + v_i), & \\text{otherwise.}\n\\end{cases}\n\\]\nTo reconstruct, we backtrack from \\(dp[n][W]\\):\n\nIf \\(dp[i][w] \\neq dp[i-1][w]\\), then item \\(i\\) was included\nSubtract its weight, move to \\(i-1\\)\n\n\n\nHow Does It Work (Plain Language)\n\nBuild the standard 0/1 knapsack DP table.\nStart from bottom-right corner \\((n, W)\\).\nCompare \\(dp[i][w]\\) vs \\(dp[i-1][w]\\):\n\nIf different, include item \\(i\\), update \\(w -= w_i\\).\n\nContinue until \\(i=0\\).\nReverse selected items for correct order.\n\n\n\nExample\nLet:\n\n\n\nItem\nValue\nWeight\n\n\n\n\n1\n60\n10\n\n\n2\n100\n20\n\n\n3\n120\n30\n\n\n\nCapacity \\(W = 50\\)\nOptimal value = 220 Chosen items = {2, 3}\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n\n#define N 4\n#define W 50\n\nint main() {\n    int val[] = {0, 60, 100, 120};\n    int wt[] = {0, 10, 20, 30};\n    int dp[N][W+1];\n\n    for (int i = 0; i &lt; N; i++)\n        for (int w = 0; w &lt;= W; w++)\n            dp[i][w] = 0;\n\n    for (int i = 1; i &lt; N; i++) {\n        for (int w = 1; w &lt;= W; w++) {\n            if (wt[i] &lt;= w) {\n                int include = val[i] + dp[i-1][w-wt[i]];\n                int exclude = dp[i-1][w];\n                dp[i][w] = include &gt; exclude ? include : exclude;\n            } else dp[i][w] = dp[i-1][w];\n        }\n    }\n\n    printf(\"Max Value: %d\\n\", dp[N-1][W]);\n    printf(\"Items Taken: \");\n\n    int w = W;\n    for (int i = N-1; i &gt; 0; i--) {\n        if (dp[i][w] != dp[i-1][w]) {\n            printf(\"%d \", i);\n            w -= wt[i];\n        }\n    }\n    printf(\"\\n\");\n}\nPython\ndef reconstruct_knapsack(values, weights, W):\n    n = len(values)\n    dp = [[0]*(W+1) for _ in range(n+1)]\n\n    for i in range(1, n+1):\n        for w in range(W+1):\n            if weights[i-1] &lt;= w:\n                dp[i][w] = max(dp[i-1][w],\n                               values[i-1] + dp[i-1][w - weights[i-1]])\n            else:\n                dp[i][w] = dp[i-1][w]\n\n    # Reconstruction\n    w = W\n    chosen = []\n    for i in range(n, 0, -1):\n        if dp[i][w] != dp[i-1][w]:\n            chosen.append(i-1)\n            w -= weights[i-1]\n\n    chosen.reverse()\n    return dp[n][W], chosen\n\nvalues = [60, 100, 120]\nweights = [10, 20, 30]\nW = 50\nvalue, items = reconstruct_knapsack(values, weights, W)\nprint(\"Max value:\", value)\nprint(\"Items:\", items)\n\n\nWhy It Matters\n\nTurns value tables into actionable decisions\nEssential in optimization problems (resource allocation, budgeting)\nDemonstrates traceback logic from DP matrix\n\n\n\nA Gentle Proof (Why It Works)\nEvery \\(dp[i][w]\\) represents the best value using the first \\(i\\) items under capacity \\(w\\). If \\(dp[i][w] \\neq dp[i-1][w]\\), item \\(i\\) was critical in improving value, so it must be included. Reducing \\(w\\) by its weight and moving up repeats the same logic, tracing one optimal solution.\n\n\nTry It Yourself\n\nModify for multiple optimal solutions (store parent paths).\nImplement space-optimized DP and reconstruct with backtracking info.\nAdapt for unbounded knapsack (reuse items).\nAdd total weight output.\nVisualize reconstruction arrows from \\(dp[n][W]\\).\n\n\n\nTest Cases\n\n\n\nValues\nWeights\nW\nMax Value\nItems\n\n\n\n\n[60, 100, 120]\n[10, 20, 30]\n50\n220\n[1, 2]\n\n\n[10, 20, 30]\n[1, 1, 1]\n2\n50\n[1, 2]\n\n\n[5, 4, 6, 3]\n[2, 3, 4, 5]\n5\n7\n[0, 1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(nW)\\)\nSpace: \\(O(nW)\\) (can reduce to \\(O(W)\\) for value-only)\n\nReconstruction transforms knapsack from a math result to a real-world selection list, revealing which items make the optimum possible.\n\n\n\n484 Edit Distance Alignment\nEdit Distance tells us how different two strings are, alignment reconstruction shows exactly where they differ. By tracing the path of operations (insert, delete, substitute), we can visualize the full transformation.\n\nWhat Problem Are We Solving?\nGiven two strings \\(A\\) (length \\(n\\)) and \\(B\\) (length \\(m\\)), compute not only the edit distance but also the alignment that transforms \\(A\\) into \\(B\\) using the minimum number of operations.\nWe define:\n\\[\ndp[i][j] =\n\\begin{cases}\n0, & \\text{if } i = 0 \\text{ and } j = 0,\\\\\ni, & \\text{if } j = 0,\\\\\nj, & \\text{if } i = 0,\\\\[6pt]\n\\displaystyle\n\\min\\!\\begin{cases}\ndp[i-1][j] + 1, & \\text{deletion},\\\\\ndp[i][j-1] + 1, & \\text{insertion},\\\\\ndp[i-1][j-1] + \\text{cost}(A[i-1], B[j-1]), & \\text{replace or match.}\n\\end{cases}\n\\end{cases}\n\\]\nThen, we trace back from \\(dp[n][m]\\) to list operations in reverse.\n\n\nHow Does It Work (Plain Language)\n\nBuild standard Levenshtein DP table.\nStart from \\(dp[n][m]\\).\nMove:\n\nDiagonal: match or replace\nUp: delete\nLeft: insert\n\nRecord operation at each move.\nReverse the sequence for final alignment.\n\n\n\nExample\nLet \\(A=\\text{\"kitten\"}\\), \\(B=\\text{\"sitting\"}\\).\nOperations:\n\n\n\nStep\nAction\nResult\n\n\n\n\nk → k\nmatch\nkitten / sitting\n\n\ni → i\nmatch\nkitten / sitting\n\n\nt → t\nmatch\nkitten / sitting\n\n\nt → t\nmatch\nkitten / sitting\n\n\ne → i\nreplace\nkitti n\n\n\ninsert g\ninsertion\nkitting\n\n\n\nEdit distance = 3 (replace e→i, insert g, insert n)\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\n#define MAX 100\n\nint dp[MAX][MAX];\n\nint min3(int a, int b, int c) {\n    return a &lt; b ? (a &lt; c ? a : c) : (b &lt; c ? b : c);\n}\n\nint main() {\n    char A[MAX], B[MAX];\n    scanf(\"%s %s\", A, B);\n    int n = strlen(A), m = strlen(B);\n\n    for (int i = 0; i &lt;= n; i++) dp[i][0] = i;\n    for (int j = 0; j &lt;= m; j++) dp[0][j] = j;\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int j = 1; j &lt;= m; j++) {\n            int cost = (A[i-1] == B[j-1]) ? 0 : 1;\n            dp[i][j] = min3(dp[i-1][j] + 1,\n                            dp[i][j-1] + 1,\n                            dp[i-1][j-1] + cost);\n        }\n    }\n\n    printf(\"Edit Distance: %d\\n\", dp[n][m]);\n    printf(\"Operations:\\n\");\n\n    int i = n, j = m;\n    while (i &gt; 0 || j &gt; 0) {\n        if (i &gt; 0 && j &gt; 0 && dp[i][j] == dp[i-1][j-1] && A[i-1] == B[j-1]) {\n            printf(\"Match %c\\n\", A[i-1]);\n            i--; j--;\n        }\n        else if (i &gt; 0 && j &gt; 0 && dp[i][j] == dp[i-1][j-1] + 1) {\n            printf(\"Replace %c -&gt; %c\\n\", A[i-1], B[j-1]);\n            i--; j--;\n        }\n        else if (i &gt; 0 && dp[i][j] == dp[i-1][j] + 1) {\n            printf(\"Delete %c\\n\", A[i-1]);\n            i--;\n        }\n        else {\n            printf(\"Insert %c\\n\", B[j-1]);\n            j--;\n        }\n    }\n}\nPython\ndef edit_distance_alignment(A, B):\n    n, m = len(A), len(B)\n    dp = [[0]*(m+1) for _ in range(n+1)]\n\n    for i in range(n+1):\n        dp[i][0] = i\n    for j in range(m+1):\n        dp[0][j] = j\n\n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            cost = 0 if A[i-1] == B[j-1] else 1\n            dp[i][j] = min(dp[i-1][j] + 1,\n                           dp[i][j-1] + 1,\n                           dp[i-1][j-1] + cost)\n\n    i, j = n, m\n    ops = []\n    while i &gt; 0 or j &gt; 0:\n        if i &gt; 0 and j &gt; 0 and A[i-1] == B[j-1]:\n            ops.append(f\"Match {A[i-1]}\")\n            i -= 1; j -= 1\n        elif i &gt; 0 and j &gt; 0 and dp[i][j] == dp[i-1][j-1] + 1:\n            ops.append(f\"Replace {A[i-1]} -&gt; {B[j-1]}\")\n            i -= 1; j -= 1\n        elif i &gt; 0 and dp[i][j] == dp[i-1][j] + 1:\n            ops.append(f\"Delete {A[i-1]}\")\n            i -= 1\n        else:\n            ops.append(f\"Insert {B[j-1]}\")\n            j -= 1\n\n    return dp[n][m], list(reversed(ops))\n\ndist, ops = edit_distance_alignment(\"kitten\", \"sitting\")\nprint(\"Edit Distance:\", dist)\nprint(\"Alignment:\")\nfor op in ops:\n    print(op)\n\n\nWhy It Matters\n\nPowers diff, spell checkers, and DNA sequence alignment\nEssential for text transformation visualization\nReinforces traceback logic from DP tables\n\n\n\nA Gentle Proof (Why It Works)\nEach \\(dp[i][j]\\) is minimal over three possibilities:\n\nInsertion: adding \\(B[j-1]\\)\nDeletion: removing \\(A[i-1]\\)\nReplacement: changing \\(A[i-1]\\) to \\(B[j-1]\\)\n\nThe optimal path through the DP grid, via moves left, up, or diagonal, exactly records the sequence of minimal edits. Reversing the trace yields the transformation.\n\n\nTry It Yourself\n\nCompute alignment for “intention” → “execution”\nAdd operation count summary (insertions, deletions, replacements)\nVisualize grid with arrows (↑, ←, ↖)\nModify cost: substitution = 2, insertion/deletion = 1\nReturn both alignment string and operation list\n\n\n\nTest Cases\n\n\n\nA\nB\nDistance\nAlignment (Ops)\n\n\n\n\nkitten\nsitting\n3\nReplace e→i, Insert n, Insert g\n\n\nsunday\nsaturday\n3\nInsert a, Insert t, Replace n→r\n\n\nhorse\nros\n3\nDelete h, Replace o→r, Delete e\n\n\n\n\n\nComplexity\n\nTime: \\(O(nm)\\)\nSpace: \\(O(nm)\\) (can reduce to \\(O(\\min(n,m))\\) without reconstruction)\n\nEdit Distance Alignment transforms a distance metric into a step-by-step story, showing exactly how one word becomes another.\n\n\n\n485 Matrix Chain Parentheses\nMatrix Chain Multiplication gives us the minimum number of multiplications, but reconstruction tells us how to parenthesize, the order of multiplication that achieves that cost. Without this step, we know the cost, but not the recipe.\n\nWhat Problem Are We Solving?\nGiven a sequence of matrices \\(A_1, A_2, \\dots, A_n\\) with dimensions \\(p_0 \\times p_1, p_1 \\times p_2, \\dots, p_{n-1} \\times p_n\\), we want to determine the optimal parenthesization that minimizes scalar multiplications.\nThe cost DP is:\n\\[\ndp[i][j] =\n\\begin{cases}\n0 & \\text{if } i = j \\\n\\min_{i \\le k &lt; j} (dp[i][k] + dp[k+1][j] + p_{i-1} \\cdot p_k \\cdot p_j)\n\\end{cases}\n\\]\nTo reconstruct the solution, we maintain a split table \\(split[i][j]\\) indicating the index \\(k\\) where the optimal split occurs.\n\n\nHow Does It Work (Plain Language)\n\nCompute cost table using bottom-up DP.\nTrack split point \\(k\\) at each subproblem.\nRecurse:\n\nBase: if \\(i==j\\), return \\(A_i\\)\nOtherwise: ( + solve(\\(i\\), \\(k\\)) + solve(\\(k+1\\), \\(j\\)) + )\n\n\nThis yields the exact parenthesization.\n\n\nExample\nMatrix dimensions: \\([40, 20, 30, 10, 30]\\)\nThere are 4 matrices:\n\n\\(A_1: 40\\times20\\)\n\\(A_2: 20\\times30\\)\n\\(A_3: 30\\times10\\)\n\\(A_4: 10\\times30\\)\n\nOptimal order: \\(((A_1(A_2A_3))A_4)\\) Minimal cost: 26000\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define N 5\n\nint dp[N][N];\nint split[N][N];\n\nint min(int a, int b) { return a &lt; b ? a : b; }\n\nvoid print_paren(int i, int j) {\n    if (i == j) {\n        printf(\"A%d\", i);\n        return;\n    }\n    printf(\"(\");\n    print_paren(i, split[i][j]);\n    print_paren(split[i][j] + 1, j);\n    printf(\")\");\n}\n\nint main() {\n    int p[] = {40, 20, 30, 10, 30};\n    int n = 4;\n\n    for (int i = 1; i &lt;= n; i++) dp[i][i] = 0;\n\n    for (int len = 2; len &lt;= n; len++) {\n        for (int i = 1; i &lt;= n - len + 1; i++) {\n            int j = i + len - 1;\n            dp[i][j] = INT_MAX;\n            for (int k = i; k &lt; j; k++) {\n                int cost = dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j];\n                if (cost &lt; dp[i][j]) {\n                    dp[i][j] = cost;\n                    split[i][j] = k;\n                }\n            }\n        }\n    }\n\n    printf(\"Minimum cost: %d\\n\", dp[1][n]);\n    printf(\"Optimal order: \");\n    print_paren(1, n);\n    printf(\"\\n\");\n}\nPython\ndef matrix_chain_order(p):\n    n = len(p) - 1\n    dp = [[0]* (n+1) for _ in range(n+1)]\n    split = [[0]* (n+1) for _ in range(n+1)]\n\n    for l in range(2, n+1):\n        for i in range(1, n-l+2):\n            j = i + l - 1\n            dp[i][j] = float('inf')\n            for k in range(i, j):\n                cost = dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]\n                if cost &lt; dp[i][j]:\n                    dp[i][j] = cost\n                    split[i][j] = k\n\n    def build(i, j):\n        if i == j: return f\"A{i}\"\n        k = split[i][j]\n        return f\"({build(i, k)}{build(k+1, j)})\"\n\n    return dp[1][n], build(1, n)\n\np = [40, 20, 30, 10, 30]\ncost, order = matrix_chain_order(p)\nprint(\"Min Cost:\", cost)\nprint(\"Order:\", order)\n\n\nWhy It Matters\n\nConverts abstract cost table into concrete plan\nFoundation of query optimization, compiler expression parsing\nShows how split tracking yields human-readable structure\n\n\n\nA Gentle Proof (Why It Works)\nAt each subchain \\((i, j)\\), DP tries all \\(k\\) splits. The chosen \\(k\\) minimizing cost is stored in split[i][j]. By recursively applying these stored splits, we follow the same decision tree that generated the minimal cost. Thus reconstruction yields the exact sequence of multiplications.\n\n\nTry It Yourself\n\nTry \\(p=[10, 20, 30, 40, 30]\\) and verify order.\nAdd printing of subproblem cost for each pair \\((i,j)\\).\nModify to return tree structure instead of string.\nVisualize with nested parentheses tree.\nExtend to show intermediate matrix dimensions at each step.\n\n\n\nTest Cases\n\n\n\nDimensions\nCost\nParenthesization\n\n\n\n\n[40,20,30,10,30]\n26000\n((A1(A2A3))A4)\n\n\n[10,20,30]\n6000\n(A1A2A3)\n\n\n[10,20,30,40,30]\n30000\n((A1A2)(A3A4))\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\)\nSpace: \\(O(n^2)\\)\n\nMatrix Chain Parentheses turns cost minimization into concrete strategy, showing not just how much, but how exactly.\n\n\n\n486 Coin Change Reconstruction\nIn the Coin Change problem, we usually count the minimum coins or total ways. Reconstruction, however, asks: which exact coins make up the solution? This bridges the gap between number answers and actual combinations.\n\nWhat Problem Are We Solving?\nGiven:\n\nA set of coin denominations \\(coins = [c_1, c_2, \\dots, c_n]\\)\nA target sum \\(S\\)\n\nWe want to:\n\nCompute the minimum number of coins needed (classic DP)\nReconstruct one optimal combination of coins that achieves \\(S\\)\n\nWe define:\n\\[\ndp[x] =\n\\begin{cases}\n0, & \\text{if } x = 0,\\\\\n1 + \\displaystyle\\min_{c \\le x}\\bigl(dp[x - c]\\bigr), & \\text{if } x &gt; 0.\n\\end{cases}\n\\]\nAnd record which coin gave the best solution:\n\\[\nchoice[x] = c \\text{ that minimizes } dp[x-c]\n\\]\n\n\nHow Does It Work (Plain Language)\n\nBuild DP array from \\(0\\) to \\(S\\).\nFor each amount \\(x\\), try every coin \\(c\\).\nKeep track of:\n\nThe minimum coin count (dp[x])\nThe coin used (choice[x])\n\nAfter filling, trace back from \\(S\\): repeatedly subtract choice[x] until reaching 0.\n\n\n\nExample\nCoins = [1, 3, 4], Target \\(S = 6\\)\nDP steps:\n\n\n\nx\ndp[x]\nchoice[x]\n\n\n\n\n0\n0\n-\n\n\n1\n1\n1\n\n\n2\n2\n1\n\n\n3\n1\n3\n\n\n4\n1\n4\n\n\n5\n2\n1\n\n\n6\n2\n3\n\n\n\nOptimal combination: [3, 3]\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define MAX 100\n\nint main() {\n    int coins[] = {1, 3, 4};\n    int n = 3;\n    int S = 6;\n    int dp[MAX], choice[MAX];\n\n    dp[0] = 0;\n    choice[0] = -1;\n\n    for (int i = 1; i &lt;= S; i++) {\n        dp[i] = INT_MAX;\n        choice[i] = -1;\n        for (int j = 0; j &lt; n; j++) {\n            int c = coins[j];\n            if (c &lt;= i && dp[i-c] + 1 &lt; dp[i]) {\n                dp[i] = dp[i-c] + 1;\n                choice[i] = c;\n            }\n        }\n    }\n\n    printf(\"Min coins: %d\\n\", dp[S]);\n    printf(\"Combination: \");\n    int x = S;\n    while (x &gt; 0) {\n        printf(\"%d \", choice[x]);\n        x -= choice[x];\n    }\n    printf(\"\\n\");\n}\nPython\ndef coin_change_reconstruct(coins, S):\n    dp = [float('inf')] * (S + 1)\n    choice = [-1] * (S + 1)\n    dp[0] = 0\n\n    for x in range(1, S + 1):\n        for c in coins:\n            if c &lt;= x and dp[x - c] + 1 &lt; dp[x]:\n                dp[x] = dp[x - c] + 1\n                choice[x] = c\n\n    if dp[S] == float('inf'):\n        return None, []\n\n    comb = []\n    while S &gt; 0:\n        comb.append(choice[S])\n        S -= choice[S]\n\n    return dp[-1], comb\n\ncoins = [1, 3, 4]\nS = 6\ncount, comb = coin_change_reconstruct(coins, S)\nprint(\"Min coins:\", count)\nprint(\"Combination:\", comb)\n\n\nWhy It Matters\n\nConverts abstract DP result into practical plan\nCritical in finance, vending systems, resource allocation\nReinforces traceback technique for linear DP problems\n\n\n\nA Gentle Proof (Why It Works)\nBy definition, \\(dp[x] = 1 + dp[x-c]\\) for optimal \\(c\\). Thus the optimal last step for \\(x\\) must use coin \\(choice[x] = c\\). Repeatedly subtracting this \\(c\\) gives a valid sequence ending at \\(0\\). Each step reduces the problem size while preserving optimality (greedy by DP).\n\n\nTry It Yourself\n\nTry \\(coins=[1,3,4]\\), \\(S=10\\)\nModify to return all optimal combinations (if multiple)\nExtend for limited coin counts\nVisualize table \\((x, dp[x], choice[x])\\)\nAdapt for non-canonical systems (like [1, 3, 5, 7])\n\n\n\nTest Cases\n\n\n\nCoins\nS\nMin Coins\nCombination\n\n\n\n\n[1, 3, 4]\n6\n2\n[3, 3]\n\n\n[1, 2, 5]\n11\n3\n[5, 5, 1]\n\n\n[2, 5, 10]\n7\n∞\n[]\n\n\n\n\n\nComplexity\n\nTime: \\(O(S \\times n)\\)\nSpace: \\(O(S)\\)\n\nCoin Change Reconstruction transforms “how many” into “which ones”, building not just an answer, but a clear path to it.\n\n\n\n487 Path Reconstruction DP\nPath reconstruction in DP is the art of retracing your steps through a cost or distance table to find the exact route that led to the optimal answer. It’s not enough to know how far, you want to know how you got there.\n\nWhat Problem Are We Solving?\nGiven a grid (or graph) where each cell has a cost, we compute the minimum path cost from a start cell \\((0,0)\\) to a destination \\((n-1, m-1)\\) using only right or down moves. Now, instead of just reporting the minimal cost, we’ll reconstruct the path.\nWe define:\n\\[\ndp[i][j] =\n\\begin{cases}\ngrid[0][0], & \\text{if } i = 0 \\text{ and } j = 0,\\\\\ngrid[i][j] + \\min\\bigl(dp[i-1][j],\\ dp[i][j-1]\\bigr), & \\text{otherwise.}\n\\end{cases}\n\\]\nWe also maintain a parent table parent[i][j] to remember whether we came from top or left.\n\n\nHow Does It Work (Plain Language)\n\nFill dp[i][j] with the minimum cost to reach each cell.\nTrack the move that led to this cost:\n\nIf \\(dp[i][j]\\) came from \\(dp[i-1][j]\\), parent = “up”\nElse parent = “left”\n\nStart from destination \\((n-1,m-1)\\) and backtrack using parent.\nReverse the reconstructed list for the correct order.\n\n\n\nExample\nGrid:\n\n\n\n1\n3\n1\n\n\n\n\n1\n5\n1\n\n\n4\n2\n1\n\n\n\nMinimal path sum: 7 Path: \\((0,0)\\rightarrow(0,1)\\rightarrow(0,2)\\rightarrow(1,2)\\rightarrow(2,2)\\)\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define N 3\n#define M 3\n\nint grid[N][M] = {\n    {1, 3, 1},\n    {1, 5, 1},\n    {4, 2, 1}\n};\n\nint dp[N][M];\nchar parent[N][M]; // 'U' = up, 'L' = left\n\nint min(int a, int b) { return a &lt; b ? a : b; }\n\nint main() {\n    dp[0][0] = grid[0][0];\n\n    // First row\n    for (int j = 1; j &lt; M; j++) {\n        dp[0][j] = dp[0][j-1] + grid[0][j];\n        parent[0][j] = 'L';\n    }\n    // First column\n    for (int i = 1; i &lt; N; i++) {\n        dp[i][0] = dp[i-1][0] + grid[i][0];\n        parent[i][0] = 'U';\n    }\n\n    // Fill rest\n    for (int i = 1; i &lt; N; i++) {\n        for (int j = 1; j &lt; M; j++) {\n            if (dp[i-1][j] &lt; dp[i][j-1]) {\n                dp[i][j] = dp[i-1][j] + grid[i][j];\n                parent[i][j] = 'U';\n            } else {\n                dp[i][j] = dp[i][j-1] + grid[i][j];\n                parent[i][j] = 'L';\n            }\n        }\n    }\n\n    printf(\"Min path sum: %d\\n\", dp[N-1][M-1]);\n\n    // Backtrack\n    int i = N - 1, j = M - 1;\n    int path[100][2], len = 0;\n    while (!(i == 0 && j == 0)) {\n        path[len][0] = i;\n        path[len][1] = j;\n        len++;\n        if (parent[i][j] == 'U') i--;\n        else j--;\n    }\n    path[len][0] = 0; path[len][1] = 0;\n    len++;\n\n    printf(\"Path: \");\n    for (int k = len - 1; k &gt;= 0; k--)\n        printf(\"(%d,%d) \", path[k][0], path[k][1]);\n    printf(\"\\n\");\n}\nPython\ndef min_path_sum_path(grid):\n    n, m = len(grid), len(grid[0])\n    dp = [[0]*m for _ in range(n)]\n    parent = [['']*m for _ in range(n)]\n\n    dp[0][0] = grid[0][0]\n\n    for j in range(1, m):\n        dp[0][j] = dp[0][j-1] + grid[0][j]\n        parent[0][j] = 'L'\n    for i in range(1, n):\n        dp[i][0] = dp[i-1][0] + grid[i][0]\n        parent[i][0] = 'U'\n\n    for i in range(1, n):\n        for j in range(1, m):\n            if dp[i-1][j] &lt; dp[i][j-1]:\n                dp[i][j] = dp[i-1][j] + grid[i][j]\n                parent[i][j] = 'U'\n            else:\n                dp[i][j] = dp[i][j-1] + grid[i][j]\n                parent[i][j] = 'L'\n\n    # Backtrack\n    path = []\n    i, j = n-1, m-1\n    while not (i == 0 and j == 0):\n        path.append((i, j))\n        if parent[i][j] == 'U':\n            i -= 1\n        else:\n            j -= 1\n    path.append((0, 0))\n    path.reverse()\n\n    return dp[-1][-1], path\n\ngrid = [[1,3,1],[1,5,1],[4,2,1]]\ncost, path = min_path_sum_path(grid)\nprint(\"Min cost:\", cost)\nprint(\"Path:\", path)\n\n\nWhy It Matters\n\nTranslates numerical DP into navigable routes\nKey in pathfinding, robot navigation, route planning\nDemonstrates parent-pointer technique for 2D grids\n\n\n\nA Gentle Proof (Why It Works)\nBy construction, \\(dp[i][j]\\) stores the minimal cost to reach \\((i,j)\\). Since each cell depends only on top and left, storing the better source as parent[i][j] ensures each step back leads to a valid prefix of an optimal path. Following parents reconstructs one such optimal path.\n\n\nTry It Yourself\n\nTry on a \\(4\\times4\\) grid with random costs.\nModify to allow diagonal moves.\nExtend for maximum path sum (change min→max).\nVisualize path arrows (↑, ←).\nAdapt for graph shortest path with adjacency matrix.\n\n\n\nTest Cases\n\n\n\nGrid\nResult\nPath\n\n\n\n\n[[1,3,1],[1,5,1],[4,2,1]]\n7\n[(0,0),(0,1),(0,2),(1,2),(2,2)]\n\n\n[[1,2,3],[4,5,6]]\n12\n[(0,0),(0,1),(0,2),(1,2)]\n\n\n\n\n\nComplexity\n\nTime: \\(O(nm)\\)\nSpace: \\(O(nm)\\)\n\nPath Reconstruction DP turns shortest paths into visible journeys, showing every choice that built the optimum.\n\n\n\n488 Sequence Reconstruction\nSequence Reconstruction is the process of recovering an entire sequence from partial or implicit information, typically from DP tables, prefix relations, or pairwise constraints. It is a bridge between solving a problem and interpreting its answer as a sequence.\n\nWhat Problem Are We Solving?\nYou often solve DP problems that count or score possible sequences, but what if you need to recover one valid sequence (or even all)? For example:\n\nGiven the LIS length, reconstruct one LIS.\nGiven partial orders, reconstruct a sequence that satisfies them.\nGiven prefix sums, rebuild the original array.\n\nHere, we’ll explore a general pattern: rebuild the sequence using parent or predecessor states tracked during DP.\n\n\nExample: Reconstruct Longest Increasing Subsequence\nGiven an array arr, we first compute dp[i] = length of LIS ending at i. We then track predecessors using parent[i] to rebuild the actual subsequence.\n\n\nRecurrence\n\\[\ndp[i] = 1 + \\max_{j&lt;i,\\ arr[j]&lt;arr[i]} dp[j]\n\\]\nwith \\[\nparent[i] = \\arg\\max_{j&lt;i,\\ arr[j]&lt;arr[i]} dp[j]\n\\]\nAfter computing dp, we find the index of max(dp), then backtrack using parent.\n\n\nHow Does It Work (Plain Language)\n\nRun the LIS DP as usual.\nWhenever we update dp[i], store which previous index gave that improvement.\nAfter finishing, find the end index of the best LIS.\nWalk backward using parent until -1.\nReverse the collected indices, that’s your LIS.\n\n\n\nTiny Code\nPython\ndef reconstruct_lis(arr):\n    n = len(arr)\n    dp = [1] * n\n    parent = [-1] * n\n\n    for i in range(n):\n        for j in range(i):\n            if arr[j] &lt; arr[i] and dp[j] + 1 &gt; dp[i]:\n                dp[i] = dp[j] + 1\n                parent[i] = j\n\n    length = max(dp)\n    idx = dp.index(length)\n\n    # Backtrack\n    lis = []\n    while idx != -1:\n        lis.append(arr[idx])\n        idx = parent[idx]\n    lis.reverse()\n    return lis\n\narr = [10, 9, 2, 5, 3, 7, 101, 18]\nprint(reconstruct_lis(arr))  # [2, 3, 7, 18]\n\n\nC Version\n#include &lt;stdio.h&gt;\n\nint main() {\n    int arr[] = {10, 9, 2, 5, 3, 7, 101, 18};\n    int n = sizeof(arr)/sizeof(arr[0]);\n    int dp[n], parent[n];\n    for (int i = 0; i &lt; n; i++) {\n        dp[i] = 1;\n        parent[i] = -1;\n    }\n\n    for (int i = 0; i &lt; n; i++) {\n        for (int j = 0; j &lt; i; j++) {\n            if (arr[j] &lt; arr[i] && dp[j] + 1 &gt; dp[i]) {\n                dp[i] = dp[j] + 1;\n                parent[i] = j;\n            }\n        }\n    }\n\n    // find max index\n    int max_len = 0, idx = 0;\n    for (int i = 0; i &lt; n; i++) {\n        if (dp[i] &gt; max_len) {\n            max_len = dp[i];\n            idx = i;\n        }\n    }\n\n    // reconstruct\n    int lis[n], len = 0;\n    while (idx != -1) {\n        lis[len++] = arr[idx];\n        idx = parent[idx];\n    }\n\n    printf(\"LIS: \");\n    for (int i = len - 1; i &gt;= 0; i--) printf(\"%d \", lis[i]);\n    printf(\"\\n\");\n}\n\n\nWhy It Matters\n\nShows how DP tables contain full structure, not just values\nUseful in bioinformatics, diff tools, edit tracing, sequence alignment\nForms the foundation for traceback algorithms\n\n\n\nA Gentle Proof (Why It Works)\nBy induction on index i:\n\nBase case: first element, LIS = [arr[i]]\nInductive step: each parent[i] points to the previous LIS endpoint giving max length Thus, following parent pointers from the max element recreates a valid LIS, and reversing yields forward order.\n\n\n\nTry It Yourself\n\nChange condition to arr[j] &gt; arr[i] → Longest Decreasing Subsequence.\nModify to track all LIS sequences.\nPrint indices instead of values.\nExtend to two dimensions (nested envelopes).\nCombine with binary search LIS to get \\(O(n \\log n)\\) reconstruction.\n\n\n\nTest Cases\n\n\n\nInput\nLIS\n\n\n\n\n[10,9,2,5,3,7,101,18]\n[2,3,7,18]\n\n\n[3,10,2,1,20]\n[3,10,20]\n\n\n[50,3,10,7,40,80]\n[3,7,40,80]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(n)\\)\n\nSequence Reconstruction turns numerical answers into narrative sequences, revealing how each element fits into the optimal story.\n\n\n\n489 Multi-Choice Reconstruction\nMulti-Choice Reconstruction is about retracing selections when a DP problem allows multiple choices per state, such as picking from categories, groups, or configurations. It extends simple parent tracking into multi-dimensional or multi-decision DP, reconstructing a full combination of choices that led to the optimal answer.\n\nWhat Problem Are We Solving?\nSome DP problems involve choosing one option from several categories, such as:\n\nMulti-choice Knapsack, each group has several items; you can pick at most one.\nCourse Scheduling, pick one time slot per subject to maximize free time.\nMachine Assignment, choose one machine per job for minimal cost.\n\nWe need to not only compute the optimal value, but also reconstruct which choices were made across categories.\n\n\nExample: Multi-Choice Knapsack\nGiven G groups, each containing several items (weight, value), select one item per group such that the total weight ≤ W and value is maximized.\n\n\nState Definition\nLet \\(dp[g][w]\\) = max value using first \\(g\\) groups with total weight \\(w\\). We will track which item in each group contributed to this value.\n\n\nRecurrence\n\\[\ndp[g][w] = \\max_{(w_i, v_i) \\in group[g]} \\big(dp[g-1][w - w_i] + v_i\\big)\n\\]\nTo reconstruct, we store:\n\\[\nchoice[g][w] = i \\text{ such that } dp[g][w] \\text{ achieved by item } i\n\\]\n\n\nHow Does It Work (Plain Language)\n\nFor each group, for each capacity, try every item in the group.\nPick the one that gives the highest value.\nStore which item index gave that best value in choice.\nAfter filling the table, backtrack from (G, W) using choice to rebuild selected items.\n\n\n\nTiny Code\nPython\ndef multi_choice_knapsack(groups, W):\n    G = len(groups)\n    dp = [[0] * (W + 1) for _ in range(G + 1)]\n    choice = [[-1] * (W + 1) for _ in range(G + 1)]\n\n    for g in range(1, G + 1):\n        for w in range(W + 1):\n            for idx, (wt, val) in enumerate(groups[g - 1]):\n                if wt &lt;= w and dp[g - 1][w - wt] + val &gt; dp[g][w]:\n                    dp[g][w] = dp[g - 1][w - wt] + val\n                    choice[g][w] = idx\n\n    # Backtrack\n    w = W\n    selected = []\n    for g in range(G, 0, -1):\n        idx = choice[g][w]\n        if idx != -1:\n            wt, val = groups[g - 1][idx]\n            selected.append((g - 1, idx, wt, val))\n            w -= wt\n    selected.reverse()\n    return dp[G][W], selected\n\ngroups = [\n    [(3, 5), (2, 3)], \n    [(4, 6), (1, 2), (3, 4)], \n    [(2, 4), (1, 1)]\n$$\nprint(multi_choice_knapsack(groups, 7))\nOutput:\n(13, [(0, 0, 3, 5), (1, 1, 1, 2), (2, 0, 2, 4)])\n\n\nWhy It Matters\n\nMany optimization problems involve multiple nested decisions.\nUseful in resource allocation, scheduling, and multi-constraint planning.\nReconstruction helps explain why the DP made each choice, crucial for debugging and interpretation.\n\n\n\nA Gentle Proof (Why It Works)\nWe proceed by induction on g (group count):\n\nBase Case: \\(g=1\\), choose the best item under capacity \\(w\\).\nInductive Step: assume all optimal choices up to group \\(g-1\\) are correct. For group \\(g\\), each dp[g][w] is built from dp[g-1][w-w_i] + v_i, and storing the index i ensures reconstructing one valid optimal chain backward from \\((G,W)\\).\n\nThus, each backtracked choice sequence corresponds to one optimal solution.\n\n\nTry It Yourself\n\nAdd a limit on total number of groups selected.\nModify for multiple item selections per group.\nPrint group name instead of index.\nExtend to 3D DP (group × capacity × budget).\nReconstruct second-best solution by skipping one choice.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nGroups\nW\nOutput\n\n\n\n\n[[(3,5),(2,3)], [(4,6),(1,2),(3,4)], [(2,4),(1,1)]]\n7\nValue=13, picks=(3,5),(1,2),(2,4)\n\n\n[[(2,3)], [(2,2),(3,5)]]\n5\nValue=8\n\n\n[[(1,1),(2,4)], [(2,2),(3,5)]]\n4\nValue=6\n\n\n\n\n\nComplexity\n\nTime: \\(O(G \\cdot W \\cdot K)\\) where \\(K\\) = max group size\nSpace: \\(O(G \\cdot W)\\)\n\nMulti-Choice Reconstruction turns layered decision DPs into understandable sequences, revealing exactly what was chosen and why.\n\n\n\n490 Traceback Visualization\nTraceback Visualization is about seeing how a DP algorithm reconstructs its answer, turning invisible state transitions into a clear path of decisions. It converts a DP table into a narrative of moves, showing how each optimal solution is formed step by step.\n\nWhat Problem Are We Solving?\nMost DP problems compute optimal values but hide how those values were reached. Traceback visualization helps us answer:\n\nWhich transitions were taken?\nHow do we get from the base case to the solution?\nWhat pattern does the DP follow through its table?\n\nYou’re not changing the algorithm, you’re revealing its story.\nCommon examples:\n\nLongest Common Subsequence (LCS): arrows tracing matches.\nEdit Distance: diagonal for match, up for delete, left for insert.\nMatrix Path Problems: arrows showing minimal path sum.\nKnapsack: table highlights selected cells.\n\n\n\nHow Does It Work (Plain Language)\nWe reconstruct the DP solution visually:\n\nCompute dp table as usual.\nStart from the final state (e.g. dp[n][m]).\nMove backward following transitions that created the optimal value.\nRecord each step (arrow, direction, or explanation).\nDraw path or print trace.\n\nEach cell’s transition reveals why it was chosen, minimal, maximal, or matching condition.\n\n\nExample: Edit Distance Visualization\nGiven strings A = \"kitten\", B = \"sitting\", we compute \\(dp[i][j]\\) = min edit distance between prefixes \\(A[0..i)\\) and \\(B[0..j)\\).\nWe then trace back:\n\nIf \\(A[i-1] = B[j-1]\\): diagonal (match)\nElse:\n\nif \\(dp[i][j] = dp[i-1][j-1] + 1\\): substitution\nif \\(dp[i][j] = dp[i-1][j] + 1\\): deletion\nif \\(dp[i][j] = dp[i][j-1] + 1\\): insertion\n\n\nTrace path: bottom-right → top-left\n\n\nTiny Code\nPython\ndef edit_distance_trace(a, b):\n    n, m = len(a), len(b)\n    dp = [[0]*(m+1) for _ in range(n+1)]\n\n    for i in range(n+1):\n        dp[i][0] = i\n    for j in range(m+1):\n        dp[0][j] = j\n\n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            if a[i-1] == b[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1])\n\n    # Traceback\n    i, j = n, m\n    trace = []\n    while i &gt; 0 or j &gt; 0:\n        if i &gt; 0 and j &gt; 0 and a[i-1] == b[j-1]:\n            trace.append(f\"Match {a[i-1]}\")\n            i -= 1; j -= 1\n        elif i &gt; 0 and j &gt; 0 and dp[i][j] == dp[i-1][j-1] + 1:\n            trace.append(f\"Substitute {a[i-1]} -&gt; {b[j-1]}\")\n            i -= 1; j -= 1\n        elif i &gt; 0 and dp[i][j] == dp[i-1][j] + 1:\n            trace.append(f\"Delete {a[i-1]}\")\n            i -= 1\n        else:\n            trace.append(f\"Insert {b[j-1]}\")\n            j -= 1\n    trace.reverse()\n    return dp[n][m], trace\n\ndist, steps = edit_distance_trace(\"kitten\", \"sitting\")\nprint(\"Distance:\", dist)\nprint(\"\\n\".join(steps))\nOutput:\nDistance: 3\nSubstitute k -&gt; s\nMatch i\nMatch t\nMatch t\nSubstitute e -&gt; i\nMatch n\nInsert g\n\n\nWhy It Matters\n\nTurns abstract numbers into concrete reasoning.\nGreat for teaching and debugging DPs.\nBridges computation and human understanding.\nHelps verify optimal path or detect incorrect transitions.\n\n\n\nA Gentle Proof (Why It Works)\nBy construction, each \\(dp[i][j]\\) is the minimal cost achievable. At reconstruction, we select any predecessor \\((i', j')\\) satisfying the recurrence equality:\n\\[\ndp[i][j] = f(dp[i'][j']) + cost\n\\]\nThus, following these equalities in reverse forms a valid optimal path. Since all \\(dp\\) states are filled bottom-up, each predecessor was computed earlier, ensuring consistency.\n\n\nTry It Yourself\n\nVisualize LCS: arrows for match and direction.\nVisualize min path sum in a grid: print arrows or coordinates.\nAdd ASCII arrows (↖ ↑ ←) to the traceback.\nGenerate a heatmap with colored path.\nShow decision explanation (e.g. “Insert g at pos 6”).\n\n\n\nTest Cases\n\n\n\nInput\nOutput Trace\n\n\n\n\n(“abc”, “abc”)\nMatches only\n\n\n(“kitten”, “sitting”)\nSubstitute, Insert steps\n\n\n(“intention”, “execution”)\nMixed edits\n\n\n(“horse”, “ros”)\nDelete + Substitute path\n\n\n\n\n\nComplexity\n\nTime: same as DP, \\(O(nm)\\)\nSpace: \\(O(nm)\\) for table + path\n\nTraceback visualization transforms the final DP value into a living path, so learners see how the algorithm’s reasoning unfolds across the grid.\n\n\n\n\nSection 50. Meta-DP and Optimization Templates\n\n491 State Compression Template\nState Compression is a powerful technique for reducing the dimensionality of dynamic programming problems by encoding multiple substate variables into a single integer mask. This allows us to handle problems that depend on subsets, visited states, or boolean configurations efficiently using bitwise operations.\n\nWhat Problem Are We Solving?\nMany DP problems depend on a combination of states:\n\nWhich elements have been used?\nWhich nodes have been visited?\nWhat subset is currently active?\n\nRepresenting these directly can explode in complexity. Instead, we compress the state into an integer bitmask, where each bit indicates the inclusion or status of an element.\nFor example:\n\nTraveling Salesman: mask tracks visited cities.\nSubset Sum: mask tracks sums possible with subsets.\nGraph DP: mask encodes chosen vertices.\n\n\n\nHow Does It Work (Plain Language)\nWe represent subsets or multi-variable states with bitmasks:\n\n\\(1\\) bit → included, \\(0\\) bit → excluded.\nFor \\(n\\) elements, we have \\(2^n\\) possible masks.\n\nEach DP state looks like:\n\\[\ndp[mask] = \\text{best result for subset represented by } mask\n\\]\nTransitions iterate over bits set/unset in the mask, updating dependent states.\nKey operations:\n\nmask | (1 &lt;&lt; i) → include element i\nmask & (1 &lt;&lt; i) → check if i included\nmask ^ (1 &lt;&lt; i) → toggle inclusion\nmask & -mask → extract lowest set bit\n\n\n\nExample: Subset DP Template\n\\[\ndp[mask] = \\min_{i \\in mask} \\big( dp[mask \\setminus {i}] + cost[i] \\big)\n\\]\nHere, each mask represents a combination of items, and we build solutions incrementally by adding one element at a time.\n\n\nTiny Code\nC\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\nint min(int a, int b) { return a &lt; b ? a : b; }\n\nint main(void) {\n    int n = 3;\n    int cost[] = {3, 2, 5};\n    int dp[1 &lt;&lt; 3];\n\n    for (int mask = 0; mask &lt; (1 &lt;&lt; n); mask++)\n        dp[mask] = INT_MAX / 2;\n\n    dp[0] = 0;\n\n    for (int mask = 1; mask &lt; (1 &lt;&lt; n); mask++) {\n        for (int i = 0; i &lt; n; i++) {\n            if (mask & (1 &lt;&lt; i)) {\n                int prev = mask ^ (1 &lt;&lt; i);\n                dp[mask] = min(dp[mask], dp[prev] + cost[i]);\n            }\n        }\n    }\n\n    printf(\"Minimum total cost: %d\\n\", dp[(1 &lt;&lt; n) - 1]);\n}\nPython\nfrom math import inf\n\nn = 3\ncost = [3, 2, 5]\ndp = [inf] * (1 &lt;&lt; n)\ndp[0] = 0\n\nfor mask in range(1, 1 &lt;&lt; n):\n    for i in range(n):\n        if mask & (1 &lt;&lt; i):\n            dp[mask] = min(dp[mask], dp[mask ^ (1 &lt;&lt; i)] + cost[i])\n\nprint(\"Minimum total cost:\", dp[(1 &lt;&lt; n) - 1])\n\n\nWhy It Matters\n\nCompresses exponential states into manageable integer masks.\nEnables elegant solutions for combinatorial problems.\nEssential for TSP, Assignment, Subset DP, and Bitmask Knapsack.\nFits perfectly with iterative DP loops.\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(dp[S]\\) stores the optimal result for subset \\(S\\), and every transition moves from smaller to larger subsets via one addition:\n\\[\ndp[S] = \\min_{i \\in S} \\big( dp[S \\setminus {i}] + cost[i] \\big)\n\\]\nThen by induction:\n\nBase case: \\(dp[\\emptyset]\\) is known (often 0).\nInductive step: each subset \\(S\\) builds on smaller subsets. All subsets are processed in increasing order of size, ensuring correctness.\n\n\n\nTry It Yourself\n\nImplement Subset DP for Sum Over Subsets (SOS DP).\nSolve Traveling Salesman using state compression.\nAdapt to Assignment Problem (\\(n!\\) → \\(2^n n\\) states).\nUse mask parity (even/odd bits) for combinatorial constraints.\nPrint masks in binary to visualize transitions.\n\n\n\nTest Cases\n\n\n\nInput\nDescription\nOutput\n\n\n\n\ncost = [3, 2, 5]\nchoose all 3 elements\n10\n\n\ncost = [1, 2]\n2 elements\n3\n\n\ncost = [5]\nsingle item\n5\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\cdot 2^n)\\)\nSpace: \\(O(2^n)\\)\n\nState Compression DP is your gateway to subset reasoning, compact, powerful, and fundamental for solving exponential combinatorial spaces with structure.\n\n\n\n492 Transition Optimization Template\nTransition Optimization is a core technique for improving the efficiency of DP transitions by precomputing or structuring recurrence updates. Many DP recurrences involve nested loops or repeated evaluations that can be simplified through mathematical properties, monotonicity, or auxiliary data structures.\n\nWhat Problem Are We Solving?\nIn many DPs, each state depends on a range or set of previous states:\n\\[\ndp[i] = \\min_{j &lt; i} \\big( dp[j] + cost(j, i) \\big)\n\\]\nNaively, this takes \\(O(n^2)\\) time. But if \\(cost(j, i)\\) has special structure (monotonicity, convexity, quadrangle inequality), we can reduce it to \\(O(n \\log n)\\) or even \\(O(n)\\) using optimized transitions.\nTransition optimization finds patterns or data structures to accelerate these computations.\n\n\nHow Does It Work (Plain Language)\nWhen you notice repeated transitions like:\nfor (int i = 1; i &lt;= n; i++)\n    for (int j = 0; j &lt; i; j++)\n        dp[i] = min(dp[i], dp[j] + cost(j, i));\n…you’re paying an \\(O(n^2)\\) cost. But often, cost(j, i) follows a pattern (e.g. linear, convex, or monotonic), so we can optimize:\n\nMonotonic Queue Optimization: for sliding window minimums.\nDivide & Conquer DP: when optimal j’s move monotonically.\nConvex Hull Trick: when \\(cost(j, i) = m_j \\cdot x_i + b_j\\) is linear.\nKnuth Optimization: when quadrangle inequality holds.\n\nEach approach precomputes or narrows transitions.\n\n\nExample Transition (Generic)\n\\[\ndp[i] = \\min_{j &lt; i} \\big( dp[j] + f(j, i) \\big)\n\\]\nIf \\(f\\) satisfies the Monge property or quadrangle inequality, we can determine that the optimal \\(j\\) moves in one direction only (monotonic). That means we can use divide & conquer or pointer tricks to find it efficiently.\n\n\nTiny Code\nC (Naive Transition)\nfor (int i = 1; i &lt;= n; i++) {\n    dp[i] = INF;\n    for (int j = 0; j &lt; i; j++) {\n        int candidate = dp[j] + cost(j, i);\n        if (candidate &lt; dp[i])\n            dp[i] = candidate;\n    }\n}\nC (Optimized with Monotonic Pointer)\nint ptr = 0;\nfor (int i = 1; i &lt;= n; i++) {\n    while (ptr + 1 &lt; i && better(ptr + 1, ptr, i))\n        ptr++;\n    dp[i] = dp[ptr] + cost(ptr, i);\n}\nHere better(a, b, i) checks whether a gives a smaller cost than b for dp[i].\n\n\nPython (Sliding Window Optimization)\nfrom collections import deque\n\ndp = [0] * (n + 1)\nq = deque([0])\n\nfor i in range(1, n + 1):\n    while len(q) &gt;= 2 and better(q[1], q[0], i):\n        q.popleft()\n    j = q[0]\n    dp[i] = dp[j] + cost(j, i)\n    while len(q) &gt;= 2 and cross(q[-2], q[-1], i):\n        q.pop()\n    q.append(i)\nThis structure appears in Convex Hull Trick and Monotonic Queue Optimization.\n\n\nWhy It Matters\n\nReduces \\(O(n^2)\\) → \\(O(n \\log n)\\) or \\(O(n)\\) transitions.\nExploits structure (monotonicity, convexity) in DP cost functions.\nPowers major optimizations:\n\nKnuth Optimization\nDivide & Conquer DP\nConvex Hull Trick\nSlope Trick\nMonotone Queue DP\n\n\n\n\nA Gentle Proof (Why It Works)\nIf the recurrence satisfies Monotonicity of the Argmin, i.e.:\n\\[\nopt[i] \\le opt[i+1]\n\\]\nthen the best transition index \\(j\\) moves non-decreasingly. This means we can find optimal \\(j\\) for all \\(i\\) in one sweep, using either:\n\nTwo-pointer traversal (Monotone Queue)\nDivide & Conquer recursion (Knuth or D&C DP)\nLine container (Convex Hull Trick)\n\nBy exploiting this structure, we avoid recomputation.\n\n\nTry It Yourself\n\nIdentify a DP where each state depends on a range of previous states.\nCheck if cost(j, i) satisfies monotonic or convex properties.\nApply divide & conquer optimization to reduce \\(O(n^2)\\).\nImplement Convex Hull Trick for linear cost forms.\nUse deque-based Monotonic Queue for sliding range DP.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nCase\nRecurrence\nOptimization\n\n\n\n\n\\(dp[i] = \\min_{j&lt;i}(dp[j]+c(i-j))\\)\n\\(c\\) convex\nConvex Hull Trick\n\n\n\\(dp[i] = \\min_{j&lt;i}(dp[j]+w(j,i))\\)\nMonotone argmin\nDivide & Conquer\n\n\n\\(dp[i] = \\min_{j&lt;i}(dp[j]) + a_i\\)\nsliding window\nMonotonic Queue\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) to \\(O(n \\log n)\\) (depends on method)\nSpace: \\(O(n)\\)\n\nTransition Optimization is the art of seeing structure in cost, once you spot monotonicity or convexity, your DP becomes faster, cleaner, and smarter.\n\n\n\n493 Space Optimization Template\nSpace Optimization is the art of trimming away unused dimensions in a DP table by realizing that only a limited subset of previous states is needed at each step. Many classic DPs that start with large \\(O(n^2)\\) or \\(O(nm)\\) tables can be reduced to rolling arrays or single-row updates, cutting memory usage drastically.\n\nWhat Problem Are We Solving?\nDynamic Programming often uses multi-dimensional arrays:\n\\[\ndp[i][j] = \\text{answer using first } i \\text{ items with capacity } j\n\\]\nBut not all dimensions are necessary. If each state \\(dp[i]\\) only depends on previous row \\(dp[i-1]\\), we can reuse memory, keeping just two rows (or even one).\nSpace Optimization lets us move from \\(O(nm)\\) to \\(O(m)\\), or from 2D → 1D, or 3D → 2D, without changing logic.\n\n\nHow Does It Work (Plain Language)\nDP updates come from previous states, not all states.\nFor example, in 0/1 Knapsack:\n\\[\ndp[i][w] = \\max(dp[i-1][w], dp[i-1][w - wt[i]] + val[i])\n\\]\nOnly dp[i-1][*] is needed when computing dp[i][*]. So we can collapse the DP table into a single array dp[w], updating it in reverse (to avoid overwriting states we still need).\nIf transitions depend on current or previous row, choose direction carefully:\n\n0/1 Knapsack → reverse loop\nUnbounded Knapsack → forward loop\n\n\n\nExample Transformation\nBefore (2D DP)\nint dp[n+1][W+1];\nfor (int i = 1; i &lt;= n; i++)\n  for (int w = 0; w &lt;= W; w++)\n    dp[i][w] = max(dp[i-1][w], dp[i-1][w-wt[i]] + val[i]);\nAfter (1D DP)\nint dp[W+1] = {0};\nfor (int i = 1; i &lt;= n; i++)\n  for (int w = W; w &gt;= wt[i]; w--)\n    dp[w] = max(dp[w], dp[w-wt[i]] + val[i]);\n\n\nTiny Code\nC (Rolling Array Example)\n#include &lt;stdio.h&gt;\n#define max(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n = 3, W = 5;\n    int wt[] = {0, 2, 3, 4};\n    int val[] = {0, 4, 5, 7};\n    int dp[6] = {0};\n\n    for (int i = 1; i &lt;= n; i++)\n        for (int w = W; w &gt;= wt[i]; w--)\n            dp[w] = max(dp[w], dp[w - wt[i]] + val[i]);\n\n    printf(\"Max value: %d\\n\", dp[W]);\n}\nPython (1D Rolling)\nn, W = 3, 5\nwt = [2, 3, 4]\nval = [4, 5, 7]\n\ndp = [0] * (W + 1)\n\nfor i in range(n):\n    for w in range(W, wt[i] - 1, -1):\n        dp[w] = max(dp[w], dp[w - wt[i]] + val[i])\n\nprint(\"Max value:\", dp[W])\n\n\nWhy It Matters\n\nReduces memory from \\(O(nm)\\) → \\(O(m)\\).\nMakes large DP problems feasible under memory limits.\nReveals dependency structure in transitions.\nForms the backbone of iterative bottom-up optimization.\n\nSpace optimization is vital for:\n\nKnapsack, LCS, LIS\nGrid path counting\nPartition problems\nDigit DP (carry compression)\n\n\n\nA Gentle Proof (Why It Works)\nLet’s define \\(dp[i][j]\\) depending only on \\(dp[i-1][*]\\). Since each new row is computed solely from the previous one:\n\\[\ndp[i][j] = f(dp[i-1][j], dp[i-1][j-w_i])\n\\]\nSo at iteration \\(i\\), once dp[i][*] is complete, dp[i-1][*] is never used again. By updating in reverse (to preserve dependencies), the 2D table can be rolled into one.\nFormally, space can be reduced from \\(O(nm)\\) to \\(O(m)\\) if and only if:\n\nEach \\(dp[i]\\) depends on \\(dp[i-1]\\), not \\(dp[i]\\) itself.\nTransition direction ensures previous states remain unmodified.\n\n\n\nTry It Yourself\n\nConvert your 0/1 Knapsack to 1D DP.\nSpace-optimize the LCS table (2D → 2 rows).\nApply to “Climbing Stairs” (\\(dp[i]\\) only needs last 2 values).\nFor Unbounded Knapsack, try forward updates.\nCompare memory usage before and after.\n\n\n\nTest Cases\n\n\n\nProblem\nOriginal Space\nOptimized Space\n\n\n\n\n0/1 Knapsack\n\\(O(nW)\\)\n\\(O(W)\\)\n\n\nLCS\n\\(O(nm)\\)\n\\(O(2m)\\)\n\n\nFibonacci\n\\(O(n)\\)\n\\(O(1)\\)\n\n\n\n\n\nComplexity\n\nTime: unchanged\nSpace: reduced by 1 dimension\nTradeoff: direction of iteration matters\n\nSpace Optimization is a quiet revolution: by recognizing independence between layers, we free our algorithms from unnecessary memory, one dimension at a time.\n\n\n\n494 Multi-Dimensional DP Template\nMulti-Dimensional DP extends classic one- or two-dimensional formulations into higher-dimensional state spaces, capturing problems where multiple independent variables evolve together. Each dimension corresponds to a decision axis, time, position, capacity, or some discrete property, making it possible to express rich combinatorial or structural relationships.\n\nWhat Problem Are We Solving?\nSome problems require tracking more than one evolving parameter:\n\nKnapsack with two capacities → \\(dp[i][w_1][w_2]\\)\nString interleaving → \\(dp[i][j][k]\\)\nDice sum counting → \\(dp[i][sum][count]\\)\nGrid with keys → \\(dp[x][y][mask]\\)\n\nWhen multiple independent factors drive state transitions, a single index DP cannot capture them. Multi-Dimensional DP encodes joint state evolution explicitly.\n\n\nHow Does It Work (Plain Language)\nWe define a DP table where each axis tracks a property:\n\\[\ndp[a][b][c] = \\text{best result with parameters } (a, b, c)\n\\]\nTransitions update along one or more dimensions:\n\\[\ndp[a][b][c] = \\min/\\max(\\text{transitions from neighbors})\n\\]\nThink of this as traversing a grid of states, where each move modifies several parameters. The key idea is to fill the table systematically based on topological or nested loops that respect dependency order.\n\n\nExample Recurrence\nMulti-dimensional structure often looks like:\n\\[\ndp[i][j][k] = f(dp[i-1][j'][k'], \\text{cost}(i, j, k))\n\\]\nExample (2D Knapsack):\n\\[\ndp[i][w_1][w_2] = \\max(dp[i-1][w_1][w_2],\\ dp[i-1][w_1-wt_1[i]][w_2-wt_2[i]] + val[i])\n\\]\n\n\nTiny Code\nC (2D Knapsack)\n#include &lt;stdio.h&gt;\n#define max(a,b) ((a)&gt;(b)?(a):(b))\n\nint main(void) {\n    int n = 3, W1 = 5, W2 = 5;\n    int wt1[] = {0, 2, 3, 4};\n    int wt2[] = {0, 1, 2, 3};\n    int val[] = {0, 4, 5, 6};\n    int dp[4][6][6] = {0};\n\n    for (int i = 1; i &lt;= n; i++) {\n        for (int w1 = 0; w1 &lt;= W1; w1++) {\n            for (int w2 = 0; w2 &lt;= W2; w2++) {\n                dp[i][w1][w2] = dp[i-1][w1][w2];\n                if (w1 &gt;= wt1[i] && w2 &gt;= wt2[i]) {\n                    dp[i][w1][w2] = max(dp[i][w1][w2],\n                        dp[i-1][w1 - wt1[i]][w2 - wt2[i]] + val[i]);\n                }\n            }\n        }\n    }\n\n    printf(\"Max value: %d\\n\", dp[n][W1][W2]);\n}\nPython (3D Example: String Interleaving)\ns1, s2, s3 = \"ab\", \"cd\", \"acbd\"\nn1, n2, n3 = len(s1), len(s2), len(s3)\n\ndp = [[[False]*(n3+1) for _ in range(n2+1)] for _ in range(n1+1)]\ndp[0][0][0] = True\n\nfor i in range(n1+1):\n    for j in range(n2+1):\n        for k in range(n3+1):\n            if k == 0: continue\n            if i &gt; 0 and s1[i-1] == s3[k-1] and dp[i-1][j][k-1]:\n                dp[i][j][k] = True\n            if j &gt; 0 and s2[j-1] == s3[k-1] and dp[i][j-1][k-1]:\n                dp[i][j][k] = True\n\nprint(\"Interleaving possible:\", dp[n1][n2][n3])\n\n\nWhy It Matters\n\nCaptures multi-factor problems elegantly\nHandles constraints coupling (capacity, index, sum)\nEnables state compression when reduced\nCommon in:\n\nMulti-resource allocation\nInterleaving / sequence merging\nMulti-knapsack / bounded subset\nGrid navigation with additional properties\n\n\nMulti-dimensional DPs form the foundation of generalized search spaces, where each variable adds a dimension of reasoning.\n\n\nA Gentle Proof (Why It Works)\nBy induction over the outermost dimension:\nIf \\(dp[i][*][*]\\) depends only on \\(dp[i-1][*][*]\\), and each transition moves from smaller to larger indices, then the DP fills in topological order, ensuring correctness.\nEach additional dimension multiplies the state space but does not alter dependency direction. Thus, correctness holds as long as we respect dimension order and initialize base cases properly.\n\n\nTry It Yourself\n\nSolve 2D Knapsack with dual capacity.\nImplement string interleaving check with 3D DP.\nModel shortest path in 3D grid using \\(dp[x][y][z]\\).\nAdd bitmask dimension for subset tracking.\nOptimize memory using rolling or compression.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nProblem\nDimensions\nExample State\nOutput\n\n\n\n\n2D Knapsack\n3D (item, w1, w2)\n\\(dp[i][w1][w2]\\)\nMax value\n\n\nString Interleaving\n3D\n\\(dp[i][j][k]\\)\nTrue/False\n\n\nGrid with Keys\n3D\n\\(dp[x][y][mask]\\)\nMin steps\n\n\n\n\n\nComplexity\n\nTime: \\(O(\\text{product of dimensions})\\)\nSpace: same order; compressible via rolling\nTradeoff: richer state space vs feasibility\n\nMulti-Dimensional DP is your tool for multi-constraint reasoning, when life refuses to fit in one dimension, let your DP grow an extra axis.\n\n\n\n495 Decision Monotonicity\nDecision Monotonicity is a structural property in DP recurrences that allows us to optimize transition search. When the optimal decision index for \\(dp[i]\\) moves in one direction (non-decreasing) as \\(i\\) increases, we can reduce a naive \\(O(n^2)\\) DP to \\(O(n \\log n)\\) or even \\(O(n)\\) using divide-and-conquer or two-pointer techniques.\n\nWhat Problem Are We Solving?\nIn many DPs, each state \\(dp[i]\\) is computed by choosing a best transition point \\(j &lt; i\\):\n\\[\ndp[i] = \\min_{0 \\le j &lt; i} \\big( dp[j] + cost(j, i) \\big)\n\\]\nThis naive recurrence requires trying all previous states for every \\(i\\), leading to \\(O(n^2)\\) time. But if the index of the optimal \\(j\\) (called \\(opt[i]\\)) satisfies:\n\\[\nopt[i] \\le opt[i+1]\n\\]\nthen the decision index moves monotonically, and we can search efficiently, either by divide & conquer DP or sliding pointer optimization.\n\n\nHow Does It Work (Plain Language)\nIf as \\(i\\) increases, the best \\(j\\) never moves backward, we can reuse or narrow the search for each next state.\nIn other words:\n\nThe “best split point” for \\(i=10\\) will be at or after the best split for \\(i=9\\).\nNo need to re-check smaller \\(j\\) again.\nYou can sweep \\(j\\) forward or recursively restrict the range.\n\nThis property appears when \\(cost(j, i)\\) satisfies certain quadrangle inequalities or convexity conditions.\n\n\nExample Recurrence\n\\[\ndp[i] = \\min_{j &lt; i} \\big( dp[j] + (i-j)^2 \\big)\n\\]\nHere, as \\(i\\) grows, larger \\(j\\) become more favorable because \\((i-j)^2\\) penalizes small gaps. Thus, \\(opt[i]\\) increases monotonically.\nAnother example: \\[\ndp[i] = \\min_{j &lt; i} \\big( dp[j] + c[j] \\cdot a[i] \\big)\n\\] where \\(a[i]\\) is increasing, the convex hull trick applies, and optimal lines appear in increasing order.\n\n\nTiny Code (Two-Pointer Monotonic Search)\nC\nint n = ...;\nint dp[MAXN];\nint opt[MAXN];\nfor (int i = 1; i &lt;= n; i++) {\n    dp[i] = INF;\n    int start = opt[i-1];\n    for (int j = start; j &lt;= i; j++) {\n        int val = dp[j] + cost(j, i);\n        if (val &lt; dp[i]) {\n            dp[i] = val;\n            opt[i] = j;\n        }\n    }\n}\nEach \\(opt[i]\\) begins searching from \\(opt[i-1]\\), cutting redundant checks.\n\n\nPython (Divide & Conquer Optimization)\ndef solve(l, r, optL, optR):\n    if l &gt; r: return\n    mid = (l + r) // 2\n    best = (float('inf'), -1)\n    for j in range(optL, min(optR, mid) + 1):\n        val = dp[j] + cost(j, mid)\n        if val &lt; best[0]:\n            best = (val, j)\n    dp[mid] = best[0]\n    opt = best[1]\n    solve(l, mid - 1, optL, opt)\n    solve(mid + 1, r, opt, optR)\n\nsolve(1, n, 0, n-1)\n\n\nWhy It Matters\n\nReduces complexity from \\(O(n^2)\\) → \\(O(n \\log n)\\) or \\(O(n)\\)\nEnables Divide & Conquer DP, Knuth Optimization, and Convex Hull Trick\nBuilds foundation for structured cost functions\nHelps identify monotonic transitions in scheduling, partitioning, or chain DPs\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(opt[i] \\le opt[i+1]\\), then \\(dp[i]\\)’s optimal transition comes from no earlier than \\(opt[i-1]\\). Thus, we can safely restrict search intervals:\n\\[\ndp[i] = \\min_{j \\in [opt[i-1], i-1]} f(j, i)\n\\]\nThe proof follows from quadrangle inequality:\n\\[\nf(a, c) + f(b, d) \\le f(a, d) + f(b, c)\n\\]\nwhich ensures convex-like structure and monotone decisions.\nBy induction:\n\nBase: \\(opt[1]\\) known.\nStep: if \\(opt[i] \\le opt[i+1]\\), then the recurrence preserves order.\n\n\n\nTry It Yourself\n\nImplement a divide & conquer DP with \\(opt\\) tracking.\nVerify monotonicity of \\(opt[i]\\) experimentally for a sample cost.\nApply to partitioning problems like Divide Array into K Segments.\nCompare \\(O(n^2)\\) vs optimized \\(O(n \\log n)\\) performance.\nCheck if your cost satisfies quadrangle inequality.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nRecurrence\nProperty\nOptimization\n\n\n\n\n\\(dp[i] = \\min_{j&lt;i}(dp[j]+(i-j)^2)\\)\nconvex\nmonotone opt\n\n\n\\(dp[i] = \\min_{j&lt;i}(dp[j]+a[i]\\cdot b[j])\\)\nincreasing \\(a[i]\\)\nconvex hull\n\n\nSegment DP\n\\(cost(l,r)\\) Monge\ndivide & conquer\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log n)\\) (divide & conquer) or \\(O(n)\\) (two-pointer)\nSpace: \\(O(n)\\)\n\nDecision Monotonicity is the hidden geometry of DP, once you spot that the “best index” moves only forward, your algorithm speeds up dramatically.\n\n\n\n496 Monge Array Optimization\nMonge Array Optimization is a powerful tool for accelerating dynamic programming when the cost matrix satisfies a special inequality known as the Monge property. It guarantees that the argmin of each row moves monotonically across columns, allowing us to use Divide & Conquer DP or SMAWK algorithm for subquadratic optimization.\n\nWhat Problem Are We Solving?\nConsider a DP of the form:\n\\[\ndp[i][j] = \\min_{k &lt; j} \\big(dp[i-1][k] + cost[k][j]\\big)\n\\]\nIf the cost matrix \\(cost[k][j]\\) satisfies the Monge property, we can compute all \\(dp[i][j]\\) in \\(O(n \\log n)\\) or \\(O(n)\\) per layer, instead of the naive \\(O(n^2)\\).\nThis pattern appears in:\n\nPartition DP (divide sequence into segments)\nMatrix Chain / Knuth DP\nOptimal merge / segmentation\n\n\n\nHow Does It Work (Plain Language)\nThe Monge property states that for all \\(a &lt; b\\) and \\(c &lt; d\\):\n\\[\ncost[a][c] + cost[b][d] \\le cost[a][d] + cost[b][c]\n\\]\nThis means the difference in cost is consistent across diagonals, implying convexity in two dimensions. As a result, the optimal split point moves monotonically:\n\\[\nopt[i][j] \\le opt[i][j+1]\n\\]\nWe can therefore restrict our search range for \\(dp[i][j]\\) using Divide & Conquer optimization.\n\n\nExample Recurrence\nFor segment partitioning:\n\\[\ndp[i][j] = \\min_{k &lt; j} \\big( dp[i-1][k] + cost[k][j] \\big)\n\\]\nIf \\(cost[k][j]\\) is Monge, then \\(opt[i][j] \\le opt[i][j+1]\\). Thus, when computing \\(dp[i][j]\\), we only need to search \\(k\\) in \\([opt[i][j-1], opt[i][j+1]]\\).\n\n\nTiny Code (Divide & Conquer over Monge Matrix)\nC (Template)\nvoid compute(int i, int l, int r, int optL, int optR) {\n    if (l &gt; r) return;\n    int mid = (l + r) / 2;\n    int best_k = -1;\n    long long best_val = LLONG_MAX;\n    for (int k = optL; k &lt;= optR && k &lt; mid; k++) {\n        long long val = dp_prev[k] + cost[k][mid];\n        if (val &lt; best_val) {\n            best_val = val;\n            best_k = k;\n        }\n    }\n    dp[mid] = best_val;\n    compute(i, l, mid - 1, optL, best_k);\n    compute(i, mid + 1, r, best_k, optR);\n}\nEach recursive call computes a segment’s midpoint and recursively narrows the search range based on monotonicity.\nPython (Monge DP Skeleton)\ndef compute(i, l, r, optL, optR):\n    if l &gt; r:\n        return\n    mid = (l + r) // 2\n    best = (float('inf'), -1)\n    for k in range(optL, min(optR, mid) + 1):\n        val = dp_prev[k] + cost[k][mid]\n        if val &lt; best[0]:\n            best = (val, k)\n    dp[mid] = best[0]\n    opt[mid] = best[1]\n    compute(i, l, mid - 1, optL, best[1])\n    compute(i, mid + 1, r, best[1], optR)\n\n\nWhy It Matters\n\nExploits Monge property to skip redundant transitions\nReduces 2D DP to \\(O(n \\log n)\\) or even \\(O(n)\\) per layer\nPowers optimizations like:\n\nDivide & Conquer DP\nKnuth Optimization (special Monge case)\nSMAWK algorithm (row minima in Monge arrays)\n\n\nUsed in:\n\nSequence segmentation\nMatrix chain multiplication\nOptimal BST\nInventory / scheduling models\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(cost\\) satisfies Monge inequality:\n\\[\ncost[a][c] + cost[b][d] \\le cost[a][d] + cost[b][c]\n\\]\nthen:\n\\[\nopt[j] \\le opt[j+1]\n\\]\nThat is, as \\(j\\) increases, the best \\(k\\) (split point) cannot move backward. Hence, when computing \\(dp[j]\\), we can reuse or narrow the search interval using the previous opt index.\nThis monotonicity of argmin is the key to divide-and-conquer speedups.\n\n\nTry It Yourself\n\nVerify Monge property for your cost function.\nImplement the Divide & Conquer DP template.\nTest on partition DP with convex segment cost.\nCompare \\(O(n^2)\\) vs optimized \\(O(n \\log n)\\) runtime.\nExplore SMAWK for row minima in Monge matrices.\n\n\n\nTest Cases\n\n\n\nCost Function\nMonge?\nOptimization\n\n\n\n\n\n\n\\(cost[a][b] = (sum[b]-sum[a])^2\\)\n✅\nYes\n\n\n\n\n\\(cost[a][b] = (b-a)^2\\)\n✅\nYes\n\n\n\n\n$cost[a][b] =\nb-a\n$\n❌\nNo\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log n)\\) per layer\nSpace: \\(O(n)\\)\nLayers: multiply by \\(k\\) if multi-stage DP\n\nMonge Array Optimization transforms a naive DP table into a structured landscape, once your costs align, transitions fall neatly into place with logarithmic grace.\n\n\n\n497 Divide & Conquer Template\nDivide & Conquer DP is a technique for optimizing DP transitions when the optimal transition index exhibits monotonicity. By recursively dividing the problem and searching only within a limited range for each midpoint, we reduce complexity from \\(O(n^2)\\) to \\(O(n \\log n)\\) or even \\(O(n)\\) per layer.\n\nWhat Problem Are We Solving?\nMany DP formulations involve transitions like:\n\\[\ndp[i] = \\min_{j &lt; i} \\big( dp[j] + cost(j, i) \\big)\n\\]\nIf \\(opt[i] \\le opt[i+1]\\), meaning the best transition index moves monotonically forward, we can use divide and conquer to find optimal \\(j\\) efficiently instead of scanning all \\(j &lt; i\\).\nThis structure is common in:\n\nPartition DP (divide array into \\(k\\) segments)\nMonge or Convex cost problems\nSegment-based recurrence with monotone argmin\n\n\n\nHow Does It Work (Plain Language)\nWe recursively divide the range \\([L, R]\\), compute \\(dp[mid]\\) using the best transition from a restricted interval \\([optL, optR]\\), then:\n\nLeft half \\([L, mid-1]\\) searches \\([optL, opt[mid]]\\)\nRight half \\([mid+1, R]\\) searches \\([opt[mid], optR]\\)\n\nBy maintaining monotone search boundaries, we ensure correctness and avoid redundant checks.\nThink of it as a guided binary search over DP indices, powered by structural guarantees.\n\n\nExample Recurrence\n\\[\ndp[i] = \\min_{j &lt; i} \\big( dp[j] + cost(j, i) \\big)\n\\]\nIf \\(cost\\) satisfies quadrangle inequality or Monge property, then:\n\\[\nopt[i] \\le opt[i+1]\n\\]\nThus, we can recursively compute \\(dp\\) over subranges.\n\n\nTiny Code (C)\n#include &lt;stdio.h&gt;\n#include &lt;limits.h&gt;\n\n#define INF 1000000000\n#define min(a,b) ((a)&lt;(b)?(a):(b))\n\nint n;\nint dp[10005], prev_dp[10005];\n\n// Example cost function (prefix sums)\nint prefix[10005];\nint cost(int j, int i) {\n    int sum = prefix[i] - prefix[j];\n    return sum * sum;\n}\n\nvoid compute(int l, int r, int optL, int optR) {\n    if (l &gt; r) return;\n    int mid = (l + r) / 2;\n    int best_k = -1;\n    int best_val = INF;\n\n    for (int k = optL; k &lt;= optR && k &lt; mid; k++) {\n        int val = prev_dp[k] + cost(k, mid);\n        if (val &lt; best_val) {\n            best_val = val;\n            best_k = k;\n        }\n    }\n\n    dp[mid] = best_val;\n\n    // Recurse left and right halves\n    compute(l, mid - 1, optL, best_k);\n    compute(mid + 1, r, best_k, optR);\n}\n\nint main(void) {\n    n = 5;\n    int arr[] = {0, 1, 2, 3, 4, 5};\n    for (int i = 1; i &lt;= n; i++) prefix[i] = prefix[i-1] + arr[i];\n    for (int i = 0; i &lt;= n; i++) prev_dp[i] = i*i;\n\n    compute(1, n, 0, n-1);\n\n    for (int i = 1; i &lt;= n; i++) printf(\"dp[%d] = %d\\n\", i, dp[i]);\n}\nPython\ndef cost(j, i):\n    s = prefix[i] - prefix[j]\n    return s * s\n\ndef compute(l, r, optL, optR):\n    if l &gt; r:\n        return\n    mid = (l + r) // 2\n    best = (float('inf'), -1)\n    for k in range(optL, min(optR, mid) + 1):\n        val = prev_dp[k] + cost(k, mid)\n        if val &lt; best[0]:\n            best = (val, k)\n    dp[mid], opt[mid] = best\n    compute(l, mid - 1, optL, best[1])\n    compute(mid + 1, r, best[1], optR)\n\nn = 5\narr = [0, 1, 2, 3, 4, 5]\nprefix = [0]\nfor x in arr: prefix.append(prefix[-1] + x)\nprev_dp = [i*i for i in range(len(arr))]\ndp = [0]*(n+1)\nopt = [0]*(n+1)\ncompute(1, n, 0, n-1)\nprint(dp[1:])\n\n\nWhy It Matters\n\nReduces complexity dramatically: \\(O(n \\log n)\\) per layer\nWorks on structured recurrences with monotonic \\(opt[i]\\)\nForms backbone for:\n\nKnuth Optimization\nMonge Array DP\nSegment Partition DP\n\n\nYou can think of it as “binary search for DP transitions.”\n\n\nA Gentle Proof (Why It Works)\nIf \\(opt[i] \\le opt[i+1]\\), then each \\(dp[mid]\\)’s optimal index \\(k^*\\) lies between \\(optL\\) and \\(optR\\). When dividing the range:\n\nLeft child (\\(L, mid-1\\)) searches \\([optL, k^*]\\)\nRight child (\\(mid+1, R\\)) searches \\([k^*, optR]\\)\n\nBy induction, every segment explores only valid transitions. Since each \\(k\\) is visited \\(O(\\log n)\\) times, total time is \\(O(n \\log n)\\).\n\n\nTry It Yourself\n\nImplement partition DP with convex segment cost.\nVerify monotonicity of \\(opt[i]\\) numerically.\nCompare \\(O(n^2)\\) vs optimized \\(O(n \\log n)\\).\nCombine with space optimization (roll arrays).\nExtend to multi-layer DP (e.g., k-partition).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nRecurrence\nProperty\nOptimization\n\n\n\n\n\n\n\\(dp[i]=\\min_{j&lt;i}(dp[j]+(sum[i]-sum[j])^2)\\)\nconvex\nyes\n\n\n\n\n\\(dp[i]=\\min_{j&lt;i}(dp[j]+                    | i-j      | )\\)\nlinear\nyes\n\n\n\n\n\\(dp[i]=\\min_{j&lt;i}(dp[j]+cost[j][i])\\)\nMonge\nyes\n\n\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log n)\\) per layer\nSpace: \\(O(n)\\)\nLayers: multiply by \\(k\\) for multi-stage DP\n\nDivide & Conquer DP is your scalpel for quadratic DPs, once you find monotonicity, you slice complexity cleanly in half at every level.\n\n\n\n498 Rerooting Template\nRerooting DP is a powerful tree dynamic programming pattern that lets you compute results for every node as the root, efficiently reusing computations from parent-to-child transitions. It’s like rotating the tree root through all nodes without recomputing everything from scratch.\n\nWhat Problem Are We Solving?\nGiven a tree, we often want to compute a property for each node as if it were the root. For example:\n\nSum of distances to all nodes\nSize of subtree or value based on children\nNumber of valid colorings rooted at each node\n\nNaively, you could rerun DP for every node, \\(O(n^2)\\), but rerooting reduces this to \\(O(n)\\) or \\(O(n \\log n)\\) by cleverly reusing partial results.\n\n\nHow Does It Work (Plain Language)\n\nFirst pass (postorder): compute DP values bottom-up for a fixed root (usually node 1).\nSecond pass (preorder): propagate results top-down, rerooting along each edge and combining parent contributions.\n\nWhen moving the root from u to v:\n\nRemove v’s contribution from u’s DP.\nAdd u’s contribution (excluding v) into v’s DP.\n\nThis way, each node inherits a correct rerooted DP in one traversal.\n\n\nExample Problem\nCompute sum of distances from every node to all others.\nLet:\n\n\\(dp[u]\\) = sum of distances from \\(u\\) to all nodes in its subtree\n\\(sz[u]\\) = size of subtree of \\(u\\)\n\nWe can reroot using: \\[\ndp[v] = dp[u] - sz[v] + (n - sz[v])\n\\] when moving root from \\(u\\) to child \\(v\\).\n\n\nTiny Code (C)\n#include &lt;stdio.h&gt;\n#include &lt;vector&gt;\n\n#define MAXN 100005\nusing namespace std;\n\nvector&lt;int&gt; adj[MAXN];\nint n;\nint sz[MAXN];\nlong long dp[MAXN];\nlong long ans[MAXN];\n\nvoid dfs1(int u, int p) {\n    sz[u] = 1;\n    dp[u] = 0;\n    for (int v : adj[u]) if (v != p) {\n        dfs1(v, u);\n        sz[u] += sz[v];\n        dp[u] += dp[v] + sz[v];\n    }\n}\n\nvoid dfs2(int u, int p) {\n    ans[u] = dp[u];\n    for (int v : adj[u]) if (v != p) {\n        long long dp_u = dp[u], dp_v = dp[v];\n        int sz_u = sz[u], sz_v = sz[v];\n\n        // Move root from u to v\n        dp[u] -= dp[v] + sz[v];\n        sz[u] -= sz[v];\n        dp[v] += dp[u] + sz[u];\n        sz[v] += sz[u];\n\n        dfs2(v, u);\n\n        // Restore\n        dp[u] = dp_u; dp[v] = dp_v;\n        sz[u] = sz_u; sz[v] = sz_v;\n    }\n}\n\nint main(void) {\n    scanf(\"%d\", &n);\n    for (int i = 0; i &lt; n-1; i++) {\n        int u, v;\n        scanf(\"%d%d\", &u, &v);\n        adj[u].push_back(v);\n        adj[v].push_back(u);\n    }\n    dfs1(1, -1);\n    dfs2(1, -1);\n    for (int i = 1; i &lt;= n; i++)\n        printf(\"Sum of distances from %d: %lld\\n\", i, ans[i]);\n}\nPython\nfrom collections import defaultdict\n\nn = 5\nadj = defaultdict(list)\nedges = [(1,2),(1,3),(3,4),(3,5)]\nfor u,v in edges:\n    adj[u].append(v)\n    adj[v].append(u)\n\nsz = [0]*(n+1)\ndp = [0]*(n+1)\nans = [0]*(n+1)\n\ndef dfs1(u,p):\n    sz[u] = 1\n    dp[u] = 0\n    for v in adj[u]:\n        if v == p: continue\n        dfs1(v,u)\n        sz[u] += sz[v]\n        dp[u] += dp[v] + sz[v]\n\ndef dfs2(u,p):\n    ans[u] = dp[u]\n    for v in adj[u]:\n        if v == p: continue\n        dp_u, dp_v = dp[u], dp[v]\n        sz_u, sz_v = sz[u], sz[v]\n\n        dp[u] -= dp[v] + sz[v]\n        sz[u] -= sz[v]\n        dp[v] += dp[u] + sz[u]\n        sz[v] += sz[u]\n\n        dfs2(v,u)\n\n        dp[u], dp[v] = dp_u, dp_v\n        sz[u], sz[v] = sz_u, sz_v\n\ndfs1(1,-1)\ndfs2(1,-1)\n\nfor i in range(1,n+1):\n    print(f\"Sum of distances from {i}: {ans[i]}\")\n\n\nWhy It Matters\n\nEnables \\(O(n)\\) computation of per-node DP values.\nReuses child and parent information via reversible transitions.\nCrucial for:\n\nDistance sums\nSubtree aggregations\nColoring and constraint propagation\n\n\nYou can reroot any tree once you know how to move contributions.\n\n\nA Gentle Proof (Why It Works)\nThe rerooting relation ensures:\n\n\\(dp[u]\\) stores full-tree values when rooted at \\(u\\).\nWhen rerooting to \\(v\\), subtract \\(v\\)’s contribution from \\(u\\), then add \\(u\\)’s contribution to \\(v\\).\n\nBecause each edge is traversed twice, total complexity is \\(O(n)\\).\nThis is a direct application of DP reusability under tree decomposition.\n\n\nTry It Yourself\n\nCompute subtree sums and reroot to get sum of values at distance ≤ k.\nApply rerooting to count paths passing through each node.\nModify transitions for tree coloring or centroid scoring.\nVisualize contribution flow parent↔︎child.\n\n\n\nTest Cases\n\n\n\nTree\nQuery\nOutput\n\n\n\n\nLine (1–2–3–4)\nDistance sums\n6, 4, 4, 6\n\n\nStar (1–2,1–3,1–4)\nDistance sums\n3, 5, 5, 5\n\n\nBalanced tree\nAggregation\nsymmetric\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nRerooting DP is your “walk the tree” trick, one bottom-up pass, one top-down pass, and you know what every node would see if it stood at the root.\n\n\n\n499 Iterative DP Pattern\nIterative (bottom-up) dynamic programming is the most systematic and efficient way to compute state-based solutions. Instead of recursion and memoization, we explicitly build tables in increasing order of dependency, turning recurrence relations into simple loops.\n\nWhat Problem Are We Solving?\nWhen you have a recurrence like:\n\\[\ndp[i] = f(dp[i-1], dp[i-2], \\ldots)\n\\]\nyou don’t need recursion, you can iterate from base to target. This approach avoids call stack overhead, ensures predictable memory access, and simplifies debugging.\nIterative DP is ideal for:\n\nCounting problems (e.g. Fibonacci, climbing stairs)\nPath minimization (e.g. shortest path, knapsack)\nSequence alignment (e.g. LCS, edit distance)\n\n\n\nHow Does It Work (Plain Language)\n\nDefine the state \\(dp[i]\\): what does it represent?\nIdentify base cases (e.g. \\(dp[0]\\), \\(dp[1]\\)).\nEstablish transition using smaller states.\nIterate from smallest to largest index, ensuring dependencies are filled before use.\nExtract result (e.g. \\(dp[n]\\) or \\(\\max_i dp[i]\\)).\n\nThe iteration order must match dependency direction.\n\n\nExample: Climbing Stairs\nYou can climb either 1 or 2 steps at a time. Number of ways to reach step \\(n\\):\n\\[\ndp[i] = dp[i-1] + dp[i-2]\n\\]\nwith base cases \\(dp[0] = 1\\), \\(dp[1] = 1\\).\n\n\nTiny Code (C)\n#include &lt;stdio.h&gt;\n\nint main() {\n    int n = 5;\n    int dp[6];\n    dp[0] = 1;\n    dp[1] = 1;\n    for (int i = 2; i &lt;= n; i++)\n        dp[i] = dp[i-1] + dp[i-2];\n    printf(\"Ways to climb %d stairs: %d\\n\", n, dp[n]);\n}\nPython\nn = 5\ndp = [0]*(n+1)\ndp[0] = dp[1] = 1\nfor i in range(2, n+1):\n    dp[i] = dp[i-1] + dp[i-2]\nprint(f\"Ways to climb {n} stairs: {dp[n]}\")\n\n\nWhy It Matters\n\nPerformance: Iteration eliminates recursion overhead.\nClarity: Each state is computed once, in a known order.\nMemory Optimization: You can reduce space when only recent states are needed (rolling array).\nFoundation: All advanced DPs (knapsack, edit distance, LIS) can be written iteratively.\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(dp[i]\\) depends only on smaller indices, then filling \\(dp[0 \\ldots n]\\) in order guarantees correctness.\nBy induction:\n\nBase cases true by definition.\nAssuming \\(dp[0..i-1]\\) correct, then \\(dp[i] = f(dp[0..i-1])\\) produces correct result.\n\nNo state is used before it’s computed.\n\n\nTry It Yourself\n\nImplement iterative Fibonacci with constant space.\nConvert recursive knapsack into iterative table form.\nWrite bottom-up LCS for two strings.\nTry 2D iterative DP for grid paths.\n\n\n\nTest Cases\n\n\n\nInput\nExpected Output\n\n\n\n\n\\(n=0\\)\n1\n\n\n\\(n=1\\)\n1\n\n\n\\(n=5\\)\n8\n\n\n\\(n=10\\)\n89\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) (or \\(O(1)\\) with rolling array)\n\nIterative DP is the canonical form, the simplest, most direct way to think about recursion unrolled into loops.\n\n\n\n500 Memoization Template\nMemoization is the top-down form of dynamic programming, you solve the problem recursively, but store answers so you never recompute the same state twice. It’s the natural bridge between pure recursion and iterative DP.\n\nWhat Problem Are We Solving?\nMany recursive problems revisit the same subproblems multiple times. For example, Fibonacci recursion:\n\\[\nF(n) = F(n-1) + F(n-2)\n\\]\nrecomputes \\(F(k)\\) many times. Memoization avoids this by caching results after the first computation.\nWhenever your recursion tree overlaps, memoization converts exponential time into polynomial time.\n\n\nHow Does It Work (Plain Language)\n\nDefine the state: what parameters describe your subproblem?\nCheck if cached: if already solved, return memoized value.\nRecurse: compute using smaller states.\nStore result before returning.\nReturn the cached value next time it’s needed.\n\nMemoization is ideal for:\n\nRecursive definitions (Fibonacci, Knapsack, LCS)\nCombinatorial counting with overlapping subproblems\nTree/graph traversal with repeated subpaths\n\n\n\nExample: Fibonacci with Memoization\n\\[\nF(n) =\n\\begin{cases}\n1, & n \\le 1,\\\\\nF(n-1) + F(n-2), & \\text{otherwise.}\n\\end{cases}\n\\]\nWe store each \\(F(k)\\) the first time it’s computed.\n\n\nTiny Code (C)\n#include &lt;stdio.h&gt;\n\nint memo[100];\n\nint fib(int n) {\n    if (n &lt;= 1) return 1;\n    if (memo[n] != 0) return memo[n];\n    return memo[n] = fib(n-1) + fib(n-2);\n}\n\nint main() {\n    int n = 10;\n    printf(\"Fib(%d) = %d\\n\", n, fib(n));\n}\nPython\nmemo = {}\ndef fib(n):\n    if n &lt;= 1:\n        return 1\n    if n in memo:\n        return memo[n]\n    memo[n] = fib(n-1) + fib(n-2)\n    return memo[n]\n\nprint(fib(10))\n\n\nWhy It Matters\n\nBridges recursion and iteration: You keep the elegance of recursion with the performance of DP.\nFaster prototypes: Great for quickly building correct solutions.\nEasier to reason: You only define recurrence, not filling order.\nTransition step: Helps derive bottom-up equivalents later.\n\n\n\nA Gentle Proof (Why It Works)\nWe prove correctness by induction:\n\nBase case: \\(dp[0]\\) and \\(dp[1]\\) defined directly.\nInductive step: Each call to \\(f(n)\\) only uses smaller arguments \\(f(k)\\), which are correct by the inductive hypothesis.\nCaching: Ensures each \\(f(k)\\) computed exactly once, guaranteeing \\(O(n)\\) total calls.\n\nThus, memoization preserves recursion semantics while achieving optimal time.\n\n\nTry It Yourself\n\nWrite memoized knapsack with signature solve(i, w)\nMemoize subset sum (solve(i, sum))\nBuild LCS recursively with (i, j) as state\nCompare memoized and bottom-up versions for runtime\n\n\n\nTest Cases\n\n\n\nInput\nExpected Output\n\n\n\n\n\\(fib(0)\\)\n1\n\n\n\\(fib(1)\\)\n1\n\n\n\\(fib(5)\\)\n8\n\n\n\\(fib(10)\\)\n89\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) (each state computed once)\nSpace: \\(O(n)\\) recursion + cache\n\nMemoization is the conceptual core of DP, it reveals how subproblems overlap and prepares you for crafting iterative solutions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 5. Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "books/en-us/list-6.html",
    "href": "books/en-us/list-6.html",
    "title": "Chapter 6. Mathematics for Algorithms",
    "section": "",
    "text": "Section 51. Number Theory\n\n501 Euclidean Algorithm\nThe Euclidean Algorithm is one of the oldest and most elegant algorithms in mathematics. It finds the greatest common divisor (gcd) of two integers, the largest number that divides both without leaving a remainder.\nIt’s fast, simple, and forms the basis of modern number theory and cryptography.\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\n\\text{gcd}(a, b)\n\\]\nThat is, the largest integer\\(d\\) such that\\(d \\mid a\\) and\\(d \\mid b\\).\nInstead of checking every possible divisor, Euclid discovered a beautiful shortcut:\n\\[\n\\gcd(a, b) = \\gcd(b, a \\bmod b)\n\\]\nKeep replacing the larger number by its remainder until one becomes zero. The last non-zero number is the gcd.\nExample:\n\\[\n\\gcd(48, 18) = \\gcd(18, 48 \\bmod 18) = \\gcd(18, 12) = \\gcd(12, 6) = \\gcd(6, 0) = 6\n\\]\nSo \\(gcd(48, 18) = 6\\).\n\n\nHow Does It Work (Plain Language)?\nThink of gcd like peeling layers of remainders. Each step removes a chunk until nothing’s left. The number that “survives” all remainders is the gcd.\nLet’s see it step by step:\n\n\n\nStep\na\nb\na mod b\n\n\n\n\n1\n48\n18\n12\n\n\n2\n18\n12\n6\n\n\n3\n12\n6\n0\n\n\n\nWhen remainder becomes 0, stop. The other number (6) is the gcd.\nEvery step reduces the numbers quickly, so it runs in O(log min(a, b)) time, much faster than trying all divisors.\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nint gcd(int a, int b) {\n    while (b != 0) {\n        int r = a % b;\n        a = b;\n        b = r;\n    }\n    return a;\n}\n\nint main(void) {\n    int a, b;\n    printf(\"Enter a and b: \");\n    scanf(\"%d %d\", &a, &b);\n    printf(\"gcd(%d, %d) = %d\\n\", a, b, gcd(a, b));\n}\nPython Version\ndef gcd(a, b):\n    while b != 0:\n        a, b = b, a % b\n    return a\n\na, b = map(int, input(\"Enter a and b: \").split())\nprint(\"gcd(\", a, \",\", b, \") =\", gcd(a, b))\n\n\nWhy It Matters\n\nCore of modular arithmetic, number theory, and cryptography.\nFoundation for Extended Euclidean Algorithm (solving ax + by = gcd).\nUsed in modular inverses and Chinese Remainder Theorem.\nDemonstrates algorithmic thinking: divide problem into smaller remainders.\n\n\n\nA Gentle Proof (Why It Works)\nIf\\(a = bq + r\\), then any divisor of\\(a\\) and\\(b\\) must also divide\\(r\\). So the set of common divisors of\\((a, b)\\) equals that of\\((b, r)\\). Thus:\n\\[\n\\gcd(a, b) = \\gcd(b, r)\n\\]\nRepeatedly applying this equality until\\(r = 0\\) reveals the gcd.\nBecause remainders shrink each step, the algorithm halts in at most\\(O(\\log n)\\) steps.\n\n\nTry It Yourself\n\nCompute gcd(84, 30) by hand.\nTrace the steps of gcd(210, 45).\nModify code to count number of steps.\nTry with large numbers (e.g. 123456, 789012).\nCompare runtime with a naive divisor-checking method.\n\n\n\nTest Cases\n\n\n\na\nb\nSteps\ngcd\n\n\n\n\n48\n18\n3\n6\n\n\n84\n30\n3\n6\n\n\n210\n45\n3\n15\n\n\n101\n10\n2\n1\n\n\n270\n192\n5\n6\n\n\n\n\n\nComplexity\n\nTime: O(log min(a, b))\nSpace: O(1) (iterative) or O(log n) (recursive)\n\nThe Euclidean Algorithm shows the power of simplicity: divide, reduce, and repeat, a timeless idea that still beats inside modern computation.\n\n\n\n502 Extended Euclidean Algorithm\nThe Extended Euclidean Algorithm goes one step beyond the classic gcd: it not only finds the gcd(a, b) but also gives you x and y such that\n\\[\na \\cdot x + b \\cdot y = \\gcd(a, b)\n\\]\nThese coefficients (x, y) are the key to solving Diophantine equations and finding modular inverses, which are essential in cryptography, modular arithmetic, and algorithm design.\n\nWhat Problem Are We Solving?\nWe want to solve the linear equation:\n\\[\na x + b y = \\gcd(a, b)\n\\]\nGiven integers a and b, we need integers x and y that satisfy this equation. The Euclidean algorithm gives the gcd, but we can trace back its steps to express the gcd as a combination of a and b.\nExample: Find x, y such that \\[\n240x + 46y = \\gcd(240, 46)\n\\]\nWe know gcd(240, 46) = 2. The Extended Euclidean Algorithm gives: \\[\n2 = 240(-9) + 46(47)\n\\] So (x = -9, y = 47).\n\n\nHow Does It Work (Plain Language)?\nThink of it as “remembering” how each remainder was made during the gcd process.\nIn each step: \\[\na = bq + r \\quad \\Rightarrow \\quad r = a - bq\n\\]\nWe recursively compute gcd(b, r), and when unwinding, we rewrite each remainder in terms of a and b.\nStep-by-step for (240, 46):\n\n\n\nStep\na\nb\na % b\nEquation\n\n\n\n\n1\n240\n46\n10\n240 = 46×5 + 10\n\n\n2\n46\n10\n6\n46 = 10×4 + 6\n\n\n3\n10\n6\n4\n10 = 6×1 + 4\n\n\n4\n6\n4\n2\n6 = 4×1 + 2\n\n\n5\n4\n2\n0\nStop, gcd = 2\n\n\n\nNow backtrack:\n\n2 = 6 − 4×1\n4 = 10 − 6×1\n6 = 46 − 10×4\n10 = 240 − 46×5\n\nSubstitute upward until gcd is in the form 240x + 46y.\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nint extended_gcd(int a, int b, int *x, int *y) {\n    if (b == 0) {\n        *x = 1;\n        *y = 0;\n        return a;\n    }\n    int x1, y1;\n    int g = extended_gcd(b, a % b, &x1, &y1);\n    *x = y1;\n    *y = x1 - (a / b) * y1;\n    return g;\n}\n\nint main(void) {\n    int a, b, x, y;\n    printf(\"Enter a and b: \");\n    scanf(\"%d %d\", &a, &b);\n    int g = extended_gcd(a, b, &x, &y);\n    printf(\"gcd(%d, %d) = %d\\n\", a, b, g);\n    printf(\"Coefficients: x = %d, y = %d\\n\", x, y);\n}\nPython Version\ndef extended_gcd(a, b):\n    if b == 0:\n        return a, 1, 0\n    g, x1, y1 = extended_gcd(b, a % b)\n    x = y1\n    y = x1 - (a // b) * y1\n    return g, x, y\n\na, b = map(int, input(\"Enter a and b: \").split())\ng, x, y = extended_gcd(a, b)\nprint(f\"gcd({a}, {b}) = {g}\")\nprint(f\"x = {x}, y = {y}\")\n\n\nWhy It Matters\n\nBuilds the modular inverse:\\(a^{-1} \\equiv x \\pmod{m}\\) if gcd(a, m) = 1\nSolves linear Diophantine equations\nUsed in RSA cryptography, CRT, and modular arithmetic\nConverts gcd into a linear combination of inputs\n\n\n\nA Gentle Proof (Why It Works)\nAt each step: \\[\n\\gcd(a, b) = \\gcd(b, a \\bmod b)\n\\] If we know \\(b x' + (a \\bmod b) y' = \\gcd(a, b)\\), and since \\(a \\bmod b = a - b\\lfloor a/b \\rfloor\\), we can write: \\[\n\\gcd(a, b) = a y' + b(x' - \\lfloor a/b \\rfloor y')\n\\]\nSet \\(x = y'\\), \\(y = x' - \\lfloor a/b \\rfloor y'\\).\nThis gives the recurrence for coefficients (x, y).\n\n\nTry It Yourself\n\nSolve (240x + 46y = 2) by hand.\nVerify gcd(99, 78) = 3 and find x, y.\nFind modular inverse of 3 mod 11 using extended gcd.\nModify the code to return only modular inverse.\nTrace recursive calls for (99, 78).\n\n\n\nTest Cases\n\n\n\na\nb\ngcd\nx\ny\nCheck\n\n\n\n\n240\n46\n2\n-9\n47\n240(-9)+46(47)=2\n\n\n99\n78\n3\n-11\n14\n99(-11)+78(14)=3\n\n\n35\n15\n5\n1\n-2\n35(1)+15(-2)=5\n\n\n7\n5\n1\n-2\n3\n7(-2)+5(3)=1\n\n\n\n\n\nComplexity\n\nTime: O(log min(a, b))\nSpace: O(log n) (recursion depth)\n\nThe Extended Euclidean Algorithm is the “memory” of gcd, not just finding the divisor, but showing how it’s made.\n\n\n\n503 Modular Addition\nModular addition is like wrapping numbers around a circle. Once you reach the end, you loop back to the start. This operation lies at the heart of modular arithmetic, the arithmetic of clocks, cryptography, and hashing.\nWhen we add numbers under a modulus M, we always stay within the range [0, M−1].\n\nWhat Problem Are We Solving?\nWe want to compute the sum of two numbers under modulo M:\n\\[\n(a + b) \\bmod M\n\\]\nExample: Let\\(a = 17, b = 12, M = 10\\)\n\\[\n(17 + 12) \\bmod 10 = 29 \\bmod 10 = 9\n\\]\nSo we “wrap around” after every 10.\n\n\nHow Does It Work (Plain Language)?\nImagine a clock with M hours. If you move forward a hours, then b hours, where do you land?\nYou just add and take the remainder by M.\n\n\n\na\nb\nM\na + b\n(a + b) mod M\nResult\n\n\n\n\n17\n12\n10\n29\n9\n9\n\n\n8\n7\n5\n15\n0\n0\n\n\n25\n25\n7\n50\n1\n1\n\n\n\nIt’s addition with “wrap-around” when crossing multiples of M.\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nint mod_add(int a, int b, int M) {\n    int res = (a % M + b % M) % M;\n    if (res &lt; 0) res += M; // handle negative values\n    return res;\n}\n\nint main(void) {\n    int a, b, M;\n    printf(\"Enter a, b, M: \");\n    scanf(\"%d %d %d\", &a, &b, &M);\n    printf(\"(a + b) mod M = %d\\n\", mod_add(a, b, M));\n}\nPython Version\ndef mod_add(a, b, M):\n    return (a % M + b % M) % M\n\na, b, M = map(int, input(\"Enter a, b, M: \").split())\nprint(\"(a + b) mod M =\", mod_add(a, b, M))\n\n\nWhy It Matters\n\nForms the base operation in modular arithmetic\nUsed in cryptographic algorithms (RSA, Diffie-Hellman)\nEnsures no overflow in fixed-size computations\nSupports hashing, ring arithmetic, and checksum calculations\n\n\n\nA Gentle Proof (Why It Works)\nLet\\(a = Mq_1 + r_1\\) and\\(b = Mq_2 + r_2\\). Then: \\[\na + b = M(q_1 + q_2) + (r_1 + r_2)\n\\] So, \\[\n(a + b) \\bmod M = (r_1 + r_2) \\bmod M = (a \\bmod M + b \\bmod M) \\bmod M\n\\] This is the modular addition property.\n\n\nTry It Yourself\n\nCompute (25 + 37) mod 12.\nCompute (−3 + 5) mod 7.\nModify code to handle 3 numbers: (a + b + c) mod M.\nWrite modular subtraction: (a − b) mod M.\nExplore pattern table for M = 5.\n\n\n\nTest Cases\n\n\n\na\nb\nM\n(a + b) mod M\nResult\n\n\n\n\n17\n12\n10\n9\nOk\n\n\n25\n25\n7\n1\nOk\n\n\n-3\n5\n7\n2\nOk\n\n\n8\n8\n5\n1\nOk\n\n\n0\n9\n4\n1\nOk\n\n\n\n\n\nComplexity\n\nTime: O(1)\nSpace: O(1)\n\nModular addition is arithmetic on a loop, every sum bends back into a finite world, keeping numbers tidy and elegant.\n\n\n\n504 Modular Multiplication\nModular multiplication is arithmetic in a wrapped world, just like modular addition, but with repeated addition. When you multiply two numbers under a modulus M, you keep only the remainder, ensuring the result stays in [0, M−1].\nIt’s the backbone of fast exponentiation, hashing, cryptography, and number-theoretic transforms.\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\n(a \\times b) \\bmod M\n\\]\nExample: Let\\(a = 7, b = 8, M = 5\\)\n\\[\n(7 \\times 8) \\bmod 5 = 56 \\bmod 5 = 1\n\\]\nSo 7×8 “wraps” around the circle of size 5 and lands at 1.\n\n\nHow Does It Work (Plain Language)?\nThink of modular multiplication as repeated modular addition:\n\\[\n(a \\times b) \\bmod M = (a \\bmod M + a \\bmod M + \\dots) \\bmod M\n\\] (repeat b times)\nWe can simplify it with this identity: \\[\n(a \\times b) \\bmod M = ((a \\bmod M) \\times (b \\bmod M)) \\bmod M\n\\]\n\n\n\na\nb\nM\na×b\n(a×b) mod M\nResult\n\n\n\n\n7\n8\n5\n56\n1\n1\n\n\n25\n4\n7\n100\n2\n2\n\n\n12\n13\n10\n156\n6\n6\n\n\n\nFor large values, we avoid overflow by applying modular reduction at each step.\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nlong long mod_mul(long long a, long long b, long long M) {\n    a %= M;\n    b %= M;\n    long long res = (a * b) % M;\n    if (res &lt; 0) res += M; // handle negatives\n    return res;\n}\n\nint main(void) {\n    long long a, b, M;\n    printf(\"Enter a, b, M: \");\n    scanf(\"%lld %lld %lld\", &a, &b, &M);\n    printf(\"(a * b) mod M = %lld\\n\", mod_mul(a, b, M));\n}\nPython Version\ndef mod_mul(a, b, M):\n    return (a % M * b % M) % M\n\na, b, M = map(int, input(\"Enter a, b, M: \").split())\nprint(\"(a * b) mod M =\", mod_mul(a, b, M))\nFor very large numbers, use modular multiplication by addition (to avoid overflow):\ndef mod_mul_safe(a, b, M):\n    res = 0\n    a %= M\n    while b &gt; 0:\n        if b % 2 == 1:\n            res = (res + a) % M\n        a = (2 * a) % M\n        b //= 2\n    return res\n\n\nWhy It Matters\n\nEssential in modular exponentiation and cryptography (RSA, Diffie-Hellman)\nAvoids overflow in arithmetic under modulus\nCore operation for Number Theoretic Transform (NTT) and hash functions\nSupports building modular inverses, power functions, and polynomial arithmetic\n\n\n\nA Gentle Proof (Why It Works)\nLet\\(a = q_1 M + r_1\\),\\(b = q_2 M + r_2\\).\nThen: \\[\na \\times b = M(q_1 b + q_2 a - q_1 q_2 M) + r_1 r_2\n\\] So modulo M, all multiples of M vanish: \\[\n(a \\times b) \\bmod M = (r_1 \\times r_2) \\bmod M\n\\] Hence, \\[\n(a \\times b) \\bmod M = ((a \\bmod M) \\times (b \\bmod M)) \\bmod M\n\\]\n\n\nTry It Yourself\n\nCompute (25 × 13) mod 7.\nTry (−3 × 8) mod 5.\nModify code to handle negative inputs.\nWrite modular multiplication using repeated doubling (like binary exponentiation).\nCreate a table of (a×b) mod 6 for a, b ∈ [0,5].\n\n\n\nTest Cases\n\n\n\na\nb\nM\n(a×b) mod M\nResult\n\n\n\n\n7\n8\n5\n1\nOk\n\n\n25\n4\n7\n2\nOk\n\n\n-3\n8\n5\n1\nOk\n\n\n12\n13\n10\n6\nOk\n\n\n1000000000\n1000000000\n1000000007\n49\nOk\n\n\n\n\n\nComplexity\n\nTime: O(1) (with built-in multiplication), O(log b) (with safe doubling)\nSpace: O(1)\n\nModular multiplication keeps arithmetic stable and predictable, no matter how large the numbers, everything folds back neatly into the modular ring.\n\n\n\n505 Modular Exponentiation\nModular exponentiation is the art of raising a number to a power under a modulus, efficiently. Instead of multiplying (a) by itself (b) times (which would be far too slow), we square and reduce step by step. This technique powers cryptography, hashing, number theory, and fast algorithms for huge exponents.\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\na^b \\bmod M\n\\]\ndirectly, without overflow and without looping (b) times.\nExample: Let\\(a = 3, b = 13, M = 7\\)\nNaively: \\[\n3^{13} = 1594323 \\quad \\Rightarrow \\quad 1594323 \\bmod 7 = 5\n\\]\nEfficiently, we can do it with squaring: \\[\n3^{13} \\bmod 7 = ((3^8 \\cdot 3^4 \\cdot 3^1) \\bmod 7) = 5\n\\]\n\n\nHow Does It Work (Plain Language)?\nWe use exponentiation by squaring. Break the exponent into binary. For each bit, either square or square and multiply.\nIf (b) is even: \\[\na^b = (a^{b/2})^2\n\\] If (b) is odd: \\[\na^b = a \\cdot a^{b-1}\n\\]\nAlways take modulo after every multiplication to keep numbers small.\nExample: \\((a=3,\\ b=13,\\ M=7)\\)\n\n\n\n\n\n\n\n\n\n\nStep\nb (binary)\na\nb\nResult\n\n\n\n\n1\n1101\n3\n13\nres = 1\n\n\n2\nodd\nres = \\((1\\times3)\\bmod7=3\\), \\(a=(3\\times3)\\bmod7=2\\), \\(b=6\\)\n\n\n\n\n3\neven\nres = 3, \\(a=(2\\times2)\\bmod7=4\\), \\(b=3\\)\n\n\n\n\n4\nodd\nres = \\((3\\times4)\\bmod7=5\\), \\(a=(4\\times4)\\bmod7=2\\), \\(b=1\\)\n\n\n\n\n5\nodd\nres = \\((5\\times2)\\bmod7=3\\), \\(a=(2\\times2)\\bmod7=4\\), \\(b=0\\)\n\n\n\n\n\nResult = 3, which is \\(3^{13} \\bmod 7\\).\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nlong long mod_pow(long long a, long long b, long long M) {\n    long long res = 1;\n    a %= M;\n    while (b &gt; 0) {\n        if (b % 2 == 1)\n            res = (res * a) % M;\n        a = (a * a) % M;\n        b /= 2;\n    }\n    return res;\n}\n\nint main(void) {\n    long long a, b, M;\n    printf(\"Enter a, b, M: \");\n    scanf(\"%lld %lld %lld\", &a, &b, &M);\n    printf(\"%lld^%lld mod %lld = %lld\\n\", a, b, M, mod_pow(a, b, M));\n}\nPython Version\ndef mod_pow(a, b, M):\n    res = 1\n    a %= M\n    while b &gt; 0:\n        if b % 2 == 1:\n            res = (res * a) % M\n        a = (a * a) % M\n        b //= 2\n    return res\n\na, b, M = map(int, input(\"Enter a, b, M: \").split())\nprint(f\"{a}^{b} mod {M} =\", mod_pow(a, b, M))\nOr simply:\npow(a, b, M)\n(Python’s built-in pow handles this efficiently.)\n\n\nWhy It Matters\n\nCore operation in RSA, Diffie-Hellman, and ElGamal\nNeeded for Fermat’s Little Theorem and modular inverses\nEnables fast exponentiation without overflow\nTurns exponentiation from O(b) to O(log b)\n\n\n\nA Gentle Proof (Why It Works)\nEvery exponent can be written in binary: \\[\nb = \\sum_{i=0}^{k} b_i \\cdot 2^i\n\\]\nThen: \\[\na^b = \\prod_{i=0}^{k} (a^{2^i})^{b_i}\n\\]\nWe precompute \\(a^{2^i}\\) by squaring, and multiply only where (b_i = 1). Each squaring and multiplication is followed by modulo reduction, so numbers stay small.\n\n\nTry It Yourself\n\nCompute \\(2^{10} \\bmod 1000\\).\nCompute \\(5^{117} \\bmod 19\\).\nModify the code to print each step (trace exponentiation).\nCompare runtime with naive power.\nUse pow(a, b, M) and verify same result.\n\n\n\nTest Cases\n\n\n\na\nb\nM\nResult\nCheck\n\n\n\n\n3\n13\n7\n5\nOk\n\n\n2\n10\n1000\n24\nOk\n\n\n5\n117\n19\n1\nOk\n\n\n10\n9\n6\n4\nOk\n\n\n7\n222\n13\n9\nOk\n\n\n\n\n\nComplexity\n\nTime: O(log b)\nSpace: O(1) (iterative) or O(log b) (recursive)\n\nModular exponentiation is how we tame huge powers, turning exponential growth into a fast, logarithmic dance under the modulus.\n\n\n\n506 Modular Inverse\nThe modular inverse is the number that undoes multiplication under a modulus. If \\(a \\cdot x \\equiv 1 \\pmod{M}\\), then (x) is called the modular inverse of (a) modulo (M).\nIt’s the key to dividing in modular arithmetic, since division isn’t directly defined, we multiply by an inverse instead.\n\nWhat Problem Are We Solving?\nWe want to solve:\n\\[\na \\cdot x \\equiv 1 \\pmod{M}\n\\]\nThat means find \\(x\\) such that:\n\\[\n(a \\times x) \\bmod M = 1\n\\]\nThis \\(x\\) is called the modular multiplicative inverse of \\(a\\) modulo \\(M\\).\nExample:\nFind the inverse of \\(3 \\pmod{11}\\).\nWe need:\n\\[\n3 \\cdot x \\equiv 1 \\pmod{11}\n\\]\nTry \\(x = 4\\):\n\\[\n3 \\cdot 4 = 12 \\equiv 1 \\pmod{11}\n\\]\nTherefore:\n\\[\n3^{-1} \\equiv 4 \\pmod{11}\n\\]\n\n\nHow Does It Work (Plain Language)?\nThere are two main ways to find the modular inverse:\n\nExtended Euclidean Algorithm (works for all when gcd(a, M) = 1)\nFermat’s Little Theorem (works if M is prime)\n\n\n1. Extended Euclidean Method\nWe solve:\n\\[\na x + M y = 1\n\\]\nThe coefficient \\(x \\bmod M\\) is the modular inverse.\nExample:\nFind the inverse of \\(3 \\bmod 11\\).\nUse the extended Euclidean algorithm:\n\\[\n\\begin{aligned}\n11 &= 3 \\times 3 + 2 \\\\\n3  &= 2 \\times 1 + 1 \\\\\n2  &= 1 \\times 2 + 0\n\\end{aligned}\n\\]\nBacktrack:\n\\[\n\\begin{aligned}\n1 &= 3 - 2 \\times 1 \\\\\n  &= 3 - (11 - 3 \\times 3) \\\\\n  &= 4 \\times 3 - 1 \\times 11\n\\end{aligned}\n\\]\nSo \\(x = 4\\).\nHence,\n\\[\n3^{-1} \\equiv 4 \\pmod{11}\n\\]\n\n\n2. Fermat’s Little Theorem (Prime M)\nIf \\(M\\) is prime and \\(a \\not\\equiv 0 \\pmod{M}\\), then:\n\\[\na^{M-1} \\equiv 1 \\pmod{M}\n\\]\nMultiply both sides by \\(a^{-1}\\):\n\\[\na^{M-2} \\equiv a^{-1} \\pmod{M}\n\\]\nSo the modular inverse is:\n\\[\na^{-1} = a^{M-2} \\bmod M\n\\]\nExample:\n\\[\n3^{-1} \\pmod{11} = 3^{9} \\bmod 11 = 4\n\\]\n\n\n\nTiny Code (Easy Versions)\nC Version (Extended GCD)\n#include &lt;stdio.h&gt;\n\nlong long extended_gcd(long long a, long long b, long long *x, long long *y) {\n    if (b == 0) {\n        *x = 1;\n        *y = 0;\n        return a;\n    }\n    long long x1, y1;\n    long long g = extended_gcd(b, a % b, &x1, &y1);\n    *x = y1;\n    *y = x1 - (a / b) * y1;\n    return g;\n}\n\nlong long mod_inverse(long long a, long long M) {\n    long long x, y;\n    long long g = extended_gcd(a, M, &x, &y);\n    if (g != 1) return -1; // no inverse if gcd ≠ 1\n    x = (x % M + M) % M;\n    return x;\n}\n\nint main(void) {\n    long long a, M;\n    printf(\"Enter a, M: \");\n    scanf(\"%lld %lld\", &a, &M);\n    long long inv = mod_inverse(a, M);\n    if (inv == -1)\n        printf(\"No inverse exists.\\n\");\n    else\n        printf(\"Inverse of %lld mod %lld = %lld\\n\", a, M, inv);\n}\nPython Version\ndef mod_inverse(a, M):\n    def extended_gcd(a, b):\n        if b == 0:\n            return a, 1, 0\n        g, x1, y1 = extended_gcd(b, a % b)\n        x = y1\n        y = x1 - (a // b) * y1\n        return g, x, y\n\n    g, x, y = extended_gcd(a, M)\n    if g != 1:\n        return None\n    return x % M\n\na, M = map(int, input(\"Enter a, M: \").split())\ninv = mod_inverse(a, M)\nprint(\"Inverse:\", inv if inv is not None else \"None\")\nPython (Prime Modulus with Fermat)\ndef mod_inverse_prime(a, M):\n    return pow(a, M - 2, M)\n\n\nWhy It Matters\n\nEnables division in modular arithmetic\nCritical in RSA, CRT, Elliptic Curves, and hashing\nUsed to solve equations like \\(a x \\equiv b \\pmod{M}\\)\nBasis for solving linear congruences and modular systems\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(\\gcd(a, M) = 1\\), By Bézout’s identity: \\[\na x + M y = 1\n\\] Taking modulo M: \\[\na x \\equiv 1 \\pmod{M}\n\\] Thus, (x) is the modular inverse of (a).\n\n\nTry It Yourself\n\nFind \\(5^{-1} \\pmod{7}\\).\nFind \\(10^{-1} \\pmod{17}\\).\nCheck which numbers have no inverse mod \\(8\\).\nImplement both Extended GCD and Fermat versions.\nSolve \\(7x \\equiv 3 \\pmod{13}\\) using an inverse.\n\n\n\nTest Cases\n\n\n\na\nM\nInverse\nCheck\n\n\n\n\n3\n11\n4\n\\(3 \\times 4 = 12 \\equiv 1\\)\n\n\n5\n7\n3\n\\(5 \\times 3 = 15 \\equiv 1\\)\n\n\n10\n17\n12\n\\(10 \\times 12 = 120 \\equiv 1\\)\n\n\n2\n4\nNone\n\\(\\gcd(2,4) \\ne 1\\)\n\n\n7\n13\n2\n\\(7 \\times 2 = 14 \\equiv 1\\)\n\n\n\n\n\nComplexity\n\nExtended GCD: O(log M)\nFermat’s (prime M): O(log M) (via modular exponentiation)\nSpace: O(1) iterative\n\nThe modular inverse is the key that unlocks division in the modular world, where every valid number has its own mirror multiplier that brings you back to 1.\n\n\n\n507 Chinese Remainder Theorem\nThe Chinese Remainder Theorem (CRT) is a beautiful bridge between congruences. It lets you solve systems of modular equations, combining many modular worlds into one consistent solution. Originally described over two thousand years ago, it remains a cornerstone of modern number theory and cryptography.\n\nWhat Problem Are We Solving?\nWe want an integer \\(x\\) that satisfies a system of congruences:\n\\[\n\\begin{cases}\nx \\equiv a_1 \\pmod{m_1} \\\\\nx \\equiv a_2 \\pmod{m_2} \\\\\n\\vdots \\\\\nx \\equiv a_k \\pmod{m_k}\n\\end{cases}\n\\]\nIf the moduli \\(m_1,m_2,\\dots,m_k\\) are pairwise coprime, there is a unique solution modulo \\[\nM = m_1 m_2 \\cdots m_k.\n\\]\nExample\nFind \\(x\\) such that \\[\n\\begin{cases}\nx \\equiv 2 \\pmod{3} \\\\\nx \\equiv 3 \\pmod{4} \\\\\nx \\equiv 2 \\pmod{5}\n\\end{cases}\n\\]\nCompute \\[\nM = 3 \\cdot 4 \\cdot 5 = 60,\\quad\nM_1 = \\frac{M}{3}=20,\\quad\nM_2 = \\frac{M}{4}=15,\\quad\nM_3 = \\frac{M}{5}=12.\n\\]\nFind inverses \\[\n20^{-1} \\pmod{3} = 2,\\quad\n15^{-1} \\pmod{4} = 3,\\quad\n12^{-1} \\pmod{5} = 3.\n\\]\nCombine \\[\nx = 2\\cdot 20\\cdot 2 \\;+\\; 3\\cdot 15\\cdot 3 \\;+\\; 2\\cdot 12\\cdot 3\n  = 80 + 135 + 72 = 287.\n\\]\nReduce modulo \\(60\\) \\[\nx \\equiv 287 \\bmod 60 = 47.\n\\]\nSo the solution is \\[\nx \\equiv 47 \\pmod{60}.\n\\]\n\n\nHow Does It Work (Plain Language)?\nEach congruence gives a “lane” that repeats every \\(m_i\\).\nCRT finds the intersection point, the smallest \\(x\\) where all lanes line up.\nThink of modular worlds like clocks with different tick lengths. CRT finds the time when all clocks show the specified hands simultaneously.\n\n\n\nModulus\nRemainder\nPeriod\nAligns At\n\n\n\n\n3\n2\n3\n2, 5, 8, 11, …\n\n\n4\n3\n4\n3, 7, 11, 15, …\n\n\n5\n2\n5\n2, 7, 12, 17, …\n\n\n\nThey first align at 47, and then repeat every 60.\n\n\nTiny Code (Easy Version)\nPython Version\ndef extended_gcd(a, b):\n    if b == 0:\n        return a, 1, 0\n    g, x1, y1 = extended_gcd(b, a % b)\n    return g, y1, x1 - (a // b) * y1\n\ndef mod_inverse(a, m):\n    g, x, y = extended_gcd(a, m)\n    if g != 1:\n        return None\n    return x % m\n\ndef crt(a, m):\n    M = 1\n    for mod in m:\n        M *= mod\n    x = 0\n    for ai, mi in zip(a, m):\n        Mi = M // mi\n        inv = mod_inverse(Mi, mi)\n        x = (x + ai * Mi * inv) % M\n    return x\n\na = [2, 3, 2]\nm = [3, 4, 5]\nprint(\"x =\", crt(a, m))  # Output: 47\nC Version (Simplified)\n#include &lt;stdio.h&gt;\n\nlong long extended_gcd(long long a, long long b, long long *x, long long *y) {\n    if (b == 0) { *x = 1; *y = 0; return a; }\n    long long x1, y1;\n    long long g = extended_gcd(b, a % b, &x1, &y1);\n    *x = y1;\n    *y = x1 - (a / b) * y1;\n    return g;\n}\n\nlong long mod_inverse(long long a, long long m) {\n    long long x, y;\n    long long g = extended_gcd(a, m, &x, &y);\n    if (g != 1) return -1;\n    return (x % m + m) % m;\n}\n\nlong long crt(int a[], int m[], int n) {\n    long long M = 1, x = 0;\n    for (int i = 0; i &lt; n; i++) M *= m[i];\n    for (int i = 0; i &lt; n; i++) {\n        long long Mi = M / m[i];\n        long long inv = mod_inverse(Mi, m[i]);\n        x = (x + (long long)a[i] * Mi * inv) % M;\n    }\n    return x;\n}\n\nint main(void) {\n    int a[] = {2, 3, 2};\n    int m[] = {3, 4, 5};\n    int n = 3;\n    printf(\"x = %lld\\n\", crt(a, m, n)); // Output: 47\n}\n\n\nWhy It Matters\n\nCombines multiple modular systems into one unified solution\nEssential in RSA (CRT optimization)\nUsed in Chinese calendar, hashing, polynomial moduli, FFTs\nFoundation for multi-modular arithmetic in big integer math\n\n\n\nA Gentle Proof (Why It Works)\nIf the moduli \\(m_i\\) are pairwise coprime, then \\(M_i = M/m_i\\) is coprime with \\(m_i\\). So each \\(M_i\\) has an inverse \\(n_i\\) such that \\[\nM_i \\cdot n_i \\equiv 1 \\pmod{m_i}.\n\\]\nThe combination \\[\nx = \\sum_{i=1}^{k} a_i \\, M_i \\, n_i\n\\] satisfies \\(x \\equiv a_i \\pmod{m_i}\\) for all \\(i\\). Reducing \\(x\\) modulo \\(M\\) gives the smallest non-negative solution.\n\n\nTry It Yourself\n\nSolve \\(x \\equiv 1 \\pmod{2}\\), \\(x \\equiv 2 \\pmod{3}\\), \\(x \\equiv 3 \\pmod{5}\\).\nChange one modulus to not be coprime (e.g., \\(4, 6\\)) and observe what happens.\nImplement CRT with non-coprime moduli using Garner’s algorithm.\nTest large primes (use Python big integers).\nUse CRT to reconstruct \\(x\\) from residues modulo \\(10^{9}+7\\) and \\(998244353\\).\n\n\n\nTest Cases\n\n\n\nSystem\nSolution\nModulus\nCheck\n\n\n\n\n(2 mod 3, 3 mod 4, 2 mod 5)\n47\n60\nOk\n\n\n(1 mod 2, 2 mod 3, 3 mod 5)\n23\n30\nOk\n\n\n(3 mod 5, 1 mod 7)\n31\n35\nOk\n\n\n(0 mod 3, 1 mod 4)\n4\n12\nOk\n\n\n\n\n\nComplexity\n\nTime: O(k log M) (each step uses Extended GCD)\nSpace: O(k)\n\nCRT is the harmony of modular worlds, it unifies many congruences into one elegant answer, echoing ancient arithmetic and modern encryption alike.\n\n\n\n508 Binary GCD (Stein’s Algorithm)\nThe Binary GCD algorithm, also known as Stein’s algorithm, computes the greatest common divisor using bit operations instead of division. It’s often faster than the classical Euclidean algorithm, especially on binary hardware, perfect for low-level, performance-sensitive code.\n\nWhat Problem Are We Solving?\nWe want to compute\n\\[\n\\gcd(a,b)\n\\]\nwithout division, using only shifts, subtraction, and parity checks. This is the binary GCD (Stein’s) algorithm.\nAlgorithm\n\nIf \\(a=0\\) return \\(b\\). If \\(b=0\\) return \\(a\\).\nLet \\(k=\\min(v_2(a),\\,v_2(b))\\) where \\(v_2(x)\\) is the number of trailing zero bits in \\(x\\).\nSet \\(a \\gets a/2^{v_2(a)}\\) and \\(b \\gets b/2^{v_2(b)}\\) (both become odd).\nWhile \\(a \\ne b\\):\n\nIf \\(a&gt;b\\), set \\(a \\gets a-b\\); then remove factors of two: \\(a \\gets a/2^{v_2(a)}\\).\nElse set \\(b \\gets b-a\\); then \\(b \\gets b/2^{v_2(b)}\\).\n\nReturn \\(a \\cdot 2^{k}\\).\n\nWorked example\nFind \\(\\gcd(48,18)\\).\n\nTrailing zeros: \\(v_2(48)=4\\), \\(v_2(18)=1\\), so \\(k=\\min(4,1)=1\\).\nMake odd:\n\n\\(a \\gets 48/2^{4}=3\\)\n\\(b \\gets 18/2^{1}=9\\)\n\nLoop:\n\n\\(b \\gets 9-3=6 \\Rightarrow b \\gets 6/2^{1}=3\\)\nNow \\(a=b=3\\)\n\nAnswer: \\(3 \\cdot 2^{1}=6\\)\n\nSo \\(\\gcd(48,18)=6\\).\nNotes\n\nOnly uses subtraction and bit shifts.\nComplexity is \\(O(\\log(\\max(a,b)))\\) with very simple operations.\n\n\n\nHow Does It Work (Plain Language)?\nStein’s insight:\n\nIf both numbers are even → \\(\\gcd(a,b) = 2 \\times \\gcd(a/2,\\, b/2)\\)\nIf one is even → divide it by 2\nIf both are odd → replace the larger by (larger − smaller)\nRepeat until they become equal\n\n\n\n\n\n\n\n\n\n\n\nStep\na\nb\nOperation\nNote\n\n\n\n\n1\n48\n18\nboth even → divide by 2\n\\(\\gcd = 2 \\times \\gcd(24,9)\\)\n\n\n2\n24\n9\none even → divide a\n\\(\\gcd = 2 \\times \\gcd(12,9)\\)\n\n\n3\n12\n9\none even → divide a\n\\(\\gcd = 2 \\times \\gcd(6,9)\\)\n\n\n4\n6\n9\none even → divide a\n\\(\\gcd = 2 \\times \\gcd(3,9)\\)\n\n\n5\n3\n9\nboth odd → \\(b-a=6\\)\n\\(\\gcd = 2 \\times \\gcd(3,6)\\)\n\n\n6\n3\n6\none even → divide b\n\\(\\gcd = 2 \\times \\gcd(3,3)\\)\n\n\n7\n3\n3\nequal → return 3\n\\(\\gcd = 2 \\times 3 = 6\\)\n\n\n\nFinal result: \\(\\gcd(48,18)=6\\).\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nint gcd_binary(int a, int b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    // find power of 2 factor\n    int shift = 0;\n    while (((a | b) & 1) == 0) {\n        a &gt;&gt;= 1;\n        b &gt;&gt;= 1;\n        shift++;\n    }\n\n    // make 'a' odd\n    while ((a & 1) == 0) a &gt;&gt;= 1;\n\n    while (b != 0) {\n        while ((b & 1) == 0) b &gt;&gt;= 1;\n        if (a &gt; b) {\n            int temp = a;\n            a = b;\n            b = temp;\n        }\n        b = b - a;\n    }\n\n    return a &lt;&lt; shift;\n}\n\nint main(void) {\n    int a, b;\n    printf(\"Enter a and b: \");\n    scanf(\"%d %d\", &a, &b);\n    printf(\"gcd(%d, %d) = %d\\n\", a, b, gcd_binary(a, b));\n}\nPython Version\ndef gcd_binary(a, b):\n    if a == 0: return b\n    if b == 0: return a\n    shift = 0\n    while ((a | b) & 1) == 0:\n        a &gt;&gt;= 1\n        b &gt;&gt;= 1\n        shift += 1\n    while (a & 1) == 0:\n        a &gt;&gt;= 1\n    while b != 0:\n        while (b & 1) == 0:\n            b &gt;&gt;= 1\n        if a &gt; b:\n            a, b = b, a\n        b -= a\n    return a &lt;&lt; shift\n\na, b = map(int, input(\"Enter a, b: \").split())\nprint(\"gcd(\", a, \",\", b, \") =\", gcd_binary(a, b))\n\n\nWhy It Matters\n\nAvoids division, uses only shifts, subtraction, and comparisons\nFast on binary processors (hardware-friendly)\nWorks for unsigned integers, useful in embedded systems\nDemonstrates the power of bitwise math for classic problems\n\n\n\nA Gentle Proof (Why It Works)\nThe gcd rules remain the same:\n\ngcd(2a, 2b) = 2 × gcd(a, b)\ngcd(a, 2b) = gcd(a, b) if a is odd\ngcd(a, b) = gcd(a, b − a) if both odd and a &lt; b\n\nEach step preserves gcd properties while removing factors of 2 efficiently.\nBy induction, when a = b, that value is the gcd.\n\n\nTry It Yourself\n\nCompute gcd(48, 18) step by step.\nTry gcd(56, 98).\nModify code to count operations.\nCompare runtime with Euclidean version.\nUse on 64-bit integers and test large inputs.\n\n\n\nTest Cases\n\n\n\na\nb\ngcd(a, b)\nSteps\n\n\n\n\n48\n18\n6\nOk\n\n\n56\n98\n14\nOk\n\n\n101\n10\n1\nOk\n\n\n270\n192\n6\nOk\n\n\n0\n8\n8\nOk\n\n\n\n\n\nComplexity\n\nTime: O(log min(a, b))\nSpace: O(1)\nBitwise operations make it faster in practice than division-based GCD\n\nBinary GCD is Euclid’s spirit in binary form, subtraction, shifting, and symmetry in the dance of bits.\n\n\n\n509 Modular Reduction\nModular reduction is the process of bringing a number back into range by taking its remainder modulo M. It’s a small but essential step in modular arithmetic, every modular algorithm uses it to keep numbers bounded and stable.\nThink of it as folding an infinitely long number line into a circle of length M, and asking, “Where do we land?”\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\nx \\bmod M\n\\]\nThat is, the remainder when (x) is divided by (M), always mapped into the canonical range [0, M−1].\nExample: If\\(M = 10\\):\n\n\n\nx\nx mod 10\n\n\n\n\n23\n3\n\n\n17\n7\n\n\n0\n0\n\n\n-3\n7\n\n\n\nNegative numbers wrap around to a positive residue.\nSo we want a normalized result, never negative.\n\n\nHow Does It Work (Plain Language)?\nThe remainder operator % in most languages can produce negative results for negative inputs. To ensure a proper modular residue, we fix it by adding M back if needed.\nRule of thumb:\n\\[\n\\text{mod}(x, M) = ((x % M) + M) % M\n\\]\n\n\n\nx\nM\nx % M\nNormalized\n\n\n\n\n23\n10\n3\n3\n\n\n-3\n10\n-3\n7\n\n\n15\n6\n3\n3\n\n\n-8\n5\n-3\n2\n\n\n\nThis ensures x mod M ∈ [0, M−1].\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n\nint mod_reduce(int x, int M) {\n    int r = x % M;\n    if (r &lt; 0) r += M;\n    return r;\n}\n\nint main(void) {\n    int x, M;\n    printf(\"Enter x and M: \");\n    scanf(\"%d %d\", &x, &M);\n    printf(\"%d mod %d = %d\\n\", x, M, mod_reduce(x, M));\n}\nPython Version\ndef mod_reduce(x, M):\n    return (x % M + M) % M\n\nx, M = map(int, input(\"Enter x, M: \").split())\nprint(f\"{x} mod {M} =\", mod_reduce(x, M))\n\n\nWhy It Matters\n\nEnsures correct residues even with negative numbers\nKeeps arithmetic consistent:\\((a + b) \\bmod M = ((a \\bmod M) + (b \\bmod M)) \\bmod M\\)\nCritical in cryptography, hashing, polynomial mod arithmetic\nPrevents overflow and sign bugs in modular computations\n\n\n\nA Gentle Proof (Why It Works)\nEvery integer (x) can be written as:\n\\[\nx = qM + r\n\\]\nwhere (q) is the quotient and (r) is the remainder. We want (r ).\nIf % gives negative (r), then (r + M) is the positive residue: \\[\n(x \\bmod M) = (x % M + M) % M\n\\]\nThis satisfies modular congruence: \\[\nx \\equiv r \\pmod{M}\n\\]\n\n\nTry It Yourself\n\nCompute \\(-7 \\bmod 5\\) by hand.\nTest code with negative inputs.\nBuild mod_add, mod_sub, mod_mul using normalized reduction.\nUse reduction inside loops to prevent overflow.\nCompare % vs proper mod with negative numbers in C or Python.\n\n\n\nTest Cases\n\n\n\nx\nM\nExpected\nCheck\n\n\n\n\n23\n10\n3\nOk\n\n\n-3\n10\n7\nOk\n\n\n15\n6\n3\nOk\n\n\n-8\n5\n2\nOk\n\n\n0\n9\n0\nOk\n\n\n\n\n\nComplexity\n\nTime: O(1)\nSpace: O(1)\n\nModular reduction is the heartbeat of modular arithmetic, every operation folds back into a circle, keeping numbers small, positive, and predictable.\n\n\n\n510 Modular Linear Equation Solver\nA modular linear equation is an equation of the form \\[\na x \\equiv b \\pmod{m}\n\\] We want to find all integers \\(x\\) that satisfy this congruence. This is the modular version of solving \\(ax = b\\), but in a world that wraps around at multiples of (m).\n\nWhat Problem Are We Solving?\nGiven integers \\(a,b,m\\), solve the linear congruence \\[\na x \\equiv b \\pmod{m}.\n\\]\nKey facts\n\nLet \\(g=\\gcd(a,m)\\).\n\nA solution exists iff \\(g \\mid b\\).\n\nIf a solution exists, there are exactly \\(g\\) solutions modulo \\(m\\), spaced by \\(m/g\\).\n\nProcedure\n\nCompute \\(g=\\gcd(a,m)\\). If \\(g \\nmid b\\), no solution.\n\nReduce: \\[\na'=\\frac{a}{g},\\quad b'=\\frac{b}{g},\\quad m'=\\frac{m}{g}.\n\\] Then solve the coprime congruence \\[\na' x \\equiv b' \\pmod{m'}.\n\\]\nFind the inverse \\(a'^{-1} \\pmod{m'}\\) and set \\[\nx_0 \\equiv a'^{-1} b' \\pmod{m'}.\n\\]\nAll solutions modulo \\(m\\) are \\[\nx \\equiv x_0 + t\\cdot \\frac{m}{g} \\pmod{m},\\quad t=0,1,\\dots,g-1.\n\\]\n\nWorked example\nSolve \\(6x \\equiv 8 \\pmod{14}\\).\n\n\\(g=\\gcd(6,14)=2\\), and \\(2 \\mid 8\\) so solutions exist.\n\nReduce: \\(a'=6/2=3\\), \\(b'=8/2=4\\), \\(m'=14/2=7\\). Solve \\[\n3x \\equiv 4 \\pmod{7}.\n\\]\nInverse: \\(3^{-1}\\equiv 5 \\pmod{7}\\), so \\[\nx_0 \\equiv 5\\cdot 4 \\equiv 20 \\equiv 6 \\pmod{7}.\n\\]\nLift to modulo 14. Since \\(m/g=7\\), solutions are \\[\nx \\equiv 6 + t\\cdot 7 \\pmod{14},\\quad t=0,1.\n\\] Thus \\(x \\in \\{6,\\,13\\} \\pmod{14}\\).\n\n\n\nHow Does It Work (Plain Language)?\n\nCheck solvability\nA solution exists iff \\(g=\\gcd(a,m)\\) divides \\(b\\).\nReduce the congruence by \\(g\\) \\[\n\\frac{a}{g}\\,x \\equiv \\frac{b}{g} \\pmod{\\frac{m}{g}}\n\\]\nFind the modular inverse\nCompute \\(\\left(\\frac{a}{g}\\right)^{-1} \\pmod{\\frac{m}{g}}\\).\nSolve for one solution, then enumerate all \\(g\\) solutions\n\\[\nx_0 \\equiv \\left(\\frac{a}{g}\\right)^{-1}\\!\\left(\\frac{b}{g}\\right) \\pmod{\\frac{m}{g}}\n\\] \\[\nx \\equiv x_0 + k\\cdot\\frac{m}{g} \\pmod{m},\\quad k=0,1,\\ldots,g-1\n\\]\n\n\n\nStep-by-Step Table (Example)\n\n\n\n\n\n\n\n\n\nStep\nEquation\nAction\nResult\n\n\n\n\n1\n\\(6x \\equiv 8 \\pmod{14}\\)\n\\(\\gcd(6,14)=2 \\mid 8\\)\nsolvable\n\n\n2\ndivide by \\(2\\)\n\\(3x \\equiv 4 \\pmod{7}\\)\nsimplified\n\n\n3\ninverse of \\(3 \\bmod 7\\)\n\\(5\\)\nsince \\(3\\cdot5\\equiv1\\)\n\n\n4\nmultiply both sides\n\\(x \\equiv 4\\cdot5 \\equiv 20 \\equiv 6 \\pmod{7}\\)\none solution\n\n\n5\nlift to \\(\\pmod{14}\\)\n\\(x \\in \\{6,\\,13\\}\\)\nok\n\n\n\n\n\nTiny Code (Easy Versions)\nPython Version\ndef extended_gcd(a, b):\n    if b == 0:\n        return a, 1, 0\n    g, x1, y1 = extended_gcd(b, a % b)\n    return g, y1, x1 - (a // b) * y1\n\ndef solve_modular_linear(a, b, m):\n    g, x, y = extended_gcd(a, m)\n    if b % g != 0:\n        return []  # No solution\n    a1, b1, m1 = a // g, b // g, m // g\n    x0 = (x * b1) % m1\n    return [(x0 + i * m1) % m for i in range(g)]\n\na, b, m = map(int, input(\"Enter a, b, m: \").split())\nsolutions = solve_modular_linear(a, b, m)\nif solutions:\n    print(\"Solutions:\", solutions)\nelse:\n    print(\"No solution\")\nC Version (Simplified)\n#include &lt;stdio.h&gt;\n\nlong long extended_gcd(long long a, long long b, long long *x, long long *y) {\n    if (b == 0) { *x = 1; *y = 0; return a; }\n    long long x1, y1;\n    long long g = extended_gcd(b, a % b, &x1, &y1);\n    *x = y1;\n    *y = x1 - (a / b) * y1;\n    return g;\n}\n\nint solve_modular(long long a, long long b, long long m, long long sol[]) {\n    long long x, y;\n    long long g = extended_gcd(a, m, &x, &y);\n    if (b % g != 0) return 0;\n    a /= g; b /= g; m /= g;\n    long long x0 = ((x * b) % m + m) % m;\n    for (int i = 0; i &lt; g; i++)\n        sol[i] = (x0 + i * m) % (m * g);\n    return g;\n}\n\nint main(void) {\n    long long a, b, m, sol[10];\n    printf(\"Enter a, b, m: \");\n    scanf(\"%lld %lld %lld\", &a, &b, &m);\n    int n = solve_modular(a, b, m, sol);\n    if (n == 0) printf(\"No solution\\n\");\n    else {\n        printf(\"Solutions:\");\n        for (int i = 0; i &lt; n; i++) printf(\" %lld\", sol[i]);\n        printf(\"\\n\");\n    }\n}\n\n\nWhy It Matters\n\nSolves modular equations, the building block for CRT, RSA, and Diophantine systems\nGeneralizes modular inverses (when b=1)\nBasis for solving linear congruence systems\nEnables modular division in non-prime moduli\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(a x \\equiv b \\pmod{m}\\), then \\(m \\mid (a x - b)\\).\nLet \\(g=\\gcd(a,m)\\). Divide the congruence by \\(g\\): \\[\n\\frac{a}{g}\\,x \\equiv \\frac{b}{g} \\pmod{\\frac{m}{g}}.\n\\] Now \\(\\gcd\\!\\left(\\frac{a}{g},\\frac{m}{g}\\right)=1\\), so \\(\\left(\\frac{a}{g}\\right)^{-1} \\pmod{\\frac{m}{g}}\\) exists. Multiplying both sides by this inverse gives one solution modulo \\(\\frac{m}{g}\\). Adding multiples of \\(\\frac{m}{g}\\) generates all \\(g\\) solutions modulo \\(m\\): \\[\nx \\equiv x_0 + k\\cdot \\frac{m}{g} \\pmod{m}, \\quad k=0,1,\\dots,g-1.\n\\]\n\n\nTry It Yourself\n\nSolve \\(6x \\equiv 8 \\pmod{14}\\)\nSolve \\(4x \\equiv 2 \\pmod{6}\\)\nSolve \\(3x \\equiv 2 \\pmod{7}\\)\nTry an unsolvable case: \\(4x \\equiv 3 \\pmod{6}\\)\nModify code to print \\(\\gcd\\) and the inverse at each step\n\n\n\nTest Cases\n\n\n\na\nb\nm\nSolutions\nCheck\n\n\n\n\n6\n8\n14\n6, 13\nOk\n\n\n4\n2\n6\n2, 5\nOk\n\n\n3\n2\n7\n3\nOk\n\n\n4\n3\n6\nNone\nOk\n\n\n\n\n\nComplexity\n\nTime: O(log m)\nSpace: O(1)\n\nThe modular linear solver turns arithmetic into algebra on a circle, finding where linear lines cross modular grids.\n\n\n\n\nSection 52. Primality and Factorization\n\n511 Trial Division\nTrial Division is the simplest way to test if a number is prime, by checking whether any smaller number divides it evenly. It’s slow for large\\(n\\), but perfect for building intuition, small primes, and as a subroutine inside more advanced factorization or primality tests.\n\nWhat Problem Are We Solving?\nWe want to decide if an integer\\(n &gt; 1\\) is prime or composite.\nA prime has exactly two divisors (1 and itself). A composite has additional divisors.\nTrial division checks all possible divisors up to\\(\\sqrt{n}\\).\nExample Is\\(n = 37\\) prime?\nCheck divisors: \\[\n\\begin{aligned}\n37 \\bmod 2 &= 1 \\\n37 \\bmod 3 &= 1 \\\n37 \\bmod 4 &= 1 \\\n37 \\bmod 5 &= 2 \\\n37 \\bmod 6 &= 1\n\\end{aligned}\n\\]\nNo divisors found up to\\(\\sqrt{37}\\). Therefore, prime.\n\n\nHow Does It Work (Plain Language)\nIf\\(n = a \\times b\\), then one of\\(a\\) or\\(b\\) must satisfy\\(a, b \\leq \\sqrt{n}\\). So if\\(n\\) has a divisor, it will appear before\\(\\sqrt{n}\\). We check each integer in that range.\nTo optimize:\n\nFirst check\\(2\\)\nThen test only odd numbers\n\n\n\n\nStep\nDivisor\n\\(n \\bmod \\text{Divisor}\\)\nResult\n\n\n\n\n1\n2\n1\nskip\n\n\n2\n3\n1\nskip\n\n\n3\n4\n1\nskip\n\n\n4\n5\n2\nskip\n\n\n5\n6\n1\nskip\n\n\n,\nNo divisor found\nPrime\n\n\n\n\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n\nbool is_prime(int n) {\n    if (n &lt; 2) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n    int limit = (int)sqrt(n);\n    for (int i = 3; i &lt;= limit; i += 2)\n        if (n % i == 0)\n            return false;\n    return true;\n}\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n    printf(\"%d is %s\\n\", n, is_prime(n) ? \"prime\" : \"composite\");\n}\nPython Version\nimport math\n\ndef is_prime(n):\n    if n &lt; 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    limit = int(math.sqrt(n)) + 1\n    for i in range(3, limit, 2):\n        if n % i == 0:\n            return False\n    return True\n\nn = int(input(\"Enter n: \"))\nprint(n, \"is\", \"prime\" if is_prime(n) else \"composite\")\n\n\nWhy It Matters\n\nFoundation for all primality tests\nUsed for small\\(n\\) and initial sieving\nGreat for factorization of small numbers\nBuilds intuition for\\(\\sqrt{n}\\) boundary and divisor pairs\n\n\n\nA Gentle Proof (Why It Works)\nIf\\(n = a \\times b\\), then one of\\(a, b \\leq \\sqrt{n}\\).\nIf both were greater than\\(\\sqrt{n}\\), then: \\[\na \\times b &gt; \\sqrt{n} \\times \\sqrt{n} = n\n\\] which is impossible. Thus, any factorization must include a number ≤\\(\\sqrt{n}\\).\nTherefore, checking up to\\(\\sqrt{n}\\) is sufficient to confirm primality.\n\n\nTry It Yourself\n\nCheck if\\(37, 49, 51\\) are prime.\nModify code to print the first divisor found.\nExtend code to list all divisors of\\(n\\).\nCompare runtime for\\(n = 10^6\\) vs\\(n = 10^9\\).\nCombine with a sieve to skip non-prime divisors.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected\nFirst Divisor\n\n\n\n\n2\nPrime\n,\n\n\n3\nPrime\n,\n\n\n4\nComposite\n2\n\n\n9\nComposite\n3\n\n\n37\nPrime\n,\n\n\n49\nComposite\n7\n\n\n\n\n\nComplexity\n\nTime:\\(O(\\sqrt{n})\\)\nSpace:\\(O(1)\\)\n\nTrial Division is the “hello world” of primality testing, simple, certain, and fundamental to number theory.\n\n\n\n512 Sieve of Eratosthenes\nThe Sieve of Eratosthenes is a classic and efficient algorithm for finding all prime numbers up to a given limit \\(n\\). Instead of testing each number individually, it eliminates multiples of known primes, leaving only primes behind.\nThis sieve is one of the oldest known algorithms (over 2000 years old) and remains a cornerstone of computational number theory.\n\nWhat Problem Are We Solving?\nWe want to generate all primes up to \\(n\\).\nExample Find all primes \\(\\leq 30\\):\n\\[\n{2, 3, 5, 7, 11, 13, 17, 19, 23, 29}\n\\]\n\n\nHow Does It Work (Plain Language)\nThink of a list of integers \\(2, 3, \\ldots, n\\). We will repeatedly “cross out” multiples of each prime.\nSteps:\n\nStart with the first prime \\(p = 2\\).\nCross out all multiples of \\(p\\) starting from \\(p^2\\).\nMove to the next number not yet crossed out, that’s the next prime.\nRepeat until \\(p^2 &gt; n\\).\n\nWhat remains unmarked are all the primes.\nExample: \\(n = 30\\)\n\n\n\n\n\n\n\n\n\nStep\nPrime \\(p\\)\nRemove Multiples\nRemaining Primes\n\n\n\n\n1\n2\n\\(4, 6, 8, 10, 12, \\ldots\\)\n\\(2, 3, 5, 7, 9, 11, 13, 15, \\ldots\\)\n\n\n2\n3\n\\(9, 12, 15, 18, 21, 24, 27, 30\\)\n\\(2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\)\n\n\n3\n5\n\\(25, 30\\)\nno change\n\n\n4\nStop\n\\(5^2 = 25 &gt; \\sqrt{30}\\)\nDone\n\n\n\nFinal primes up to 30:\n\\[\n{2, 3, 5, 7, 11, 13, 17, 19, 23, 29}\n\\]\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;math.h&gt;\n\nvoid sieve(int n) {\n    bool is_prime[n + 1];\n    for (int i = 0; i &lt;= n; i++) is_prime[i] = true;\n    is_prime[0] = is_prime[1] = false;\n\n    for (int p = 2; p * p &lt;= n; p++) {\n        if (is_prime[p]) {\n            for (int multiple = p * p; multiple &lt;= n; multiple += p)\n                is_prime[multiple] = false;\n        }\n    }\n\n    printf(\"Primes up to %d:\\n\", n);\n    for (int i = 2; i &lt;= n; i++)\n        if (is_prime[i]) printf(\"%d \", i);\n    printf(\"\\n\");\n}\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n    sieve(n);\n}\nPython Version\ndef sieve(n):\n    is_prime = [True] * (n + 1)\n    is_prime[0] = is_prime[1] = False\n    p = 2\n    while p * p &lt;= n:\n        if is_prime[p]:\n            for multiple in range(p * p, n + 1, p):\n                is_prime[multiple] = False\n        p += 1\n    return [i for i in range(2, n + 1) if is_prime[i]]\n\nn = int(input(\"Enter n: \"))\nprint(\"Primes:\", sieve(n))\n\n\nWhy It Matters\n\nEfficiently generates all primes up to \\(n\\)\nUsed in number theory, cryptography, factorization, and prime sieving precomputation\nAvoids repeated division\nFoundation for advanced sieves (e.g. Linear Sieve, Segmented Sieve)\n\n\n\nA Gentle Proof (Why It Works)\nEvery composite number \\(n\\) has a smallest prime factor \\(p\\). That factor \\(p\\) will mark the composite when the algorithm reaches \\(p\\).\nThus, by crossing out all multiples of each prime, all composites are removed, and only primes remain.\nBecause any composite \\(n = a \\times b\\) has at least one \\(a \\leq \\sqrt{n}\\), we can stop sieving when \\(p^2 &gt; n\\).\n\n\nTry It Yourself\n\nGenerate primes \\(\\leq 50\\).\nModify code to count how many primes are found.\nPrint primes in rows of 10.\nCompare runtime for \\(n = 10^4\\), \\(10^5\\), \\(10^6\\).\nOptimize memory: sieve only odd numbers.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected Output\n\n\n\n\n10\n\\(2, 3, 5, 7\\)\n\n\n20\n\\(2, 3, 5, 7, 11, 13, 17, 19\\)\n\n\n30\n\\(2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log \\log n)\\)\nSpace: \\(O(n)\\)\n\nThe Sieve of Eratosthenes blends clarity and efficiency, by striking out every composite, it leaves behind the primes that shape number theory.\n\n\n\n513 Sieve of Atkin\nThe Sieve of Atkin is a modern improvement on the Sieve of Eratosthenes. Instead of crossing out multiples, it uses quadratic forms and modular arithmetic to determine prime candidates, then eliminates non-primes via square multiples. It’s asymptotically faster and beautifully mathematical, though more complex to implement.\n\nWhat Problem Are We Solving?\nWe want to find all prime numbers up to a given limit \\(n\\), but faster than the classic sieve.\nThe Sieve of Atkin uses congruence conditions based on quadratic residues to detect potential primes.\n\n\nHow Does It Work (Plain Language)\nFor a given integer \\(n\\), we determine whether it is a prime candidate by checking specific modular equations:\n\nInitialize an array is_prime[0..n] to false.\nFor every integer pair \\((x, y)\\) with \\(x, y \\ge 1\\), compute:\n\n\\(n_1 = 4x^2 + y^2\\)\n\nIf \\(n_1 \\le N\\) and \\(n_1 \\bmod 12 \\in {1, 5}\\), flip is_prime[n1]\n\n\\(n_2 = 3x^2 + y^2\\)\n\nIf \\(n_2 \\le N\\) and \\(n_2 \\bmod 12 = 7\\), flip is_prime[n2]\n\n\\(n_3 = 3x^2 - y^2\\)\n\nIf \\(x &gt; y\\), \\(n_3 \\le N\\), and \\(n_3 \\bmod 12 = 11\\), flip is_prime[n3]\n\n\nEliminate multiples of squares:\n\nFor each \\(k\\) such that \\(k^2 \\le N\\), mark all multiples of \\(k^2\\) as composite.\n\nFinally, add 2 and 3 as primes.\n\nAll remaining numbers marked true are primes.\nExample (small N = 50):\nStart with 2, 3, and 5. Apply modular conditions to detect others:\n\n\n\nCondition\nFormula\nMod Class\nCandidates\n\n\n\n\n\\(4x^2 + y^2\\)\n\\(1, 5\\)\n\\(12\\)\n\\(5, 13, 17, 29, 37, 41, 49\\)\n\n\n\\(3x^2 + y^2\\)\n\\(7\\)\n\\(12\\)\n\\(7, 19, 31, 43\\)\n\n\n\\(3x^2 - y^2\\)\n\\(11\\)\n\\(12\\)\n\\(11, 23, 47\\)\n\n\n\nThen remove multiples of squares (e.g. \\(25, 49\\)). Remaining primes up to 50:\n\\[\n{2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47}\n\\]\n\n\nTiny Code (Easy Version)\nPython Version\nimport math\n\ndef sieve_atkin(limit):\n    is_prime = [False] * (limit + 1)\n    sqrt_limit = int(math.sqrt(limit)) + 1\n\n    for x in range(1, sqrt_limit):\n        for y in range(1, sqrt_limit):\n            n = 4 * x * x + y * y\n            if n &lt;= limit and n % 12 in (1, 5):\n                is_prime[n] = not is_prime[n]\n\n            n = 3 * x * x + y * y\n            if n &lt;= limit and n % 12 == 7:\n                is_prime[n] = not is_prime[n]\n\n            n = 3 * x * x - y * y\n            if x &gt; y and n &lt;= limit and n % 12 == 11:\n                is_prime[n] = not is_prime[n]\n\n    for n in range(5, sqrt_limit):\n        if is_prime[n]:\n            for k in range(n * n, limit + 1, n * n):\n                is_prime[k] = False\n\n    primes = [2, 3]\n    primes.extend([i for i in range(5, limit + 1) if is_prime[i]])\n    return primes\n\nn = int(input(\"Enter n: \"))\nprint(\"Primes:\", sieve_atkin(n))\n\n\nWhy It Matters\n\nFaster than the Sieve of Eratosthenes for very large \\(n\\)\nDemonstrates deep connections between number theory and computation\nUses modular patterns of quadratic residues to detect primes\nFoundation for optimized sieving algorithms\n\n\n\nA Gentle Proof (Why It Works)\nEvery integer can be represented in quadratic forms. Primes occur in specific modular classes:\n\nPrimes of form \\(4x + 1\\) satisfy \\(4x^2 + y^2\\)\nPrimes of form \\(6x + 1\\) or \\(6x + 5\\) satisfy \\(3x^2 + y^2\\) or \\(3x^2 - y^2\\)\n\nBy counting solutions to these congruences mod 12, one can distinguish primes from composites. Flipping ensures each candidate is toggled odd number of times only if it meets prime conditions.\nMultiples of squares are eliminated since no square factor can belong to a prime.\n\n\nTry It Yourself\n\nGenerate all primes \\(\\le 100\\).\nCompare output with Eratosthenes.\nMeasure performance for \\(n = 10^6\\).\nModify code to count primes only.\nPrint candidate flips to see how toggling works.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected Output (Primes)\n\n\n\n\n10\n\\(2, 3, 5, 7\\)\n\n\n30\n\\(2, 3, 5, 7, 11, 13, 17, 19, 23, 29\\)\n\n\n50\n\\(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) (with modular arithmetic precomputation)\nSpace: \\(O(n)\\)\n\nThe Sieve of Atkin turns number theory into code, using the elegance of quadratic forms to filter primes from the infinite sea of integers.\n\n\n\n514 Miller–Rabin Primality Test\nThe Miller–Rabin test is a fast probabilistic primality test. Given an odd integer \\(n &gt; 2\\), it decides whether \\(n\\) is composite or probably prime by checking whether \\(a\\) behaves like a witness to compositeness modulo \\(n\\).\n\nWhat Problem Are We Solving?\nDecide primality of a large integer \\(n\\) much faster than trial division or a full sieve, especially when \\(n\\) can be hundreds or thousands of bits.\nInput: odd \\(n &gt; 2\\), accuracy parameter \\(k\\) (number of bases). Output: composite or probably prime.\n\n\nHow Does It Work (Plain Language)\nWrite \\[\nn - 1 = 2^s \\cdot d \\quad \\text{with } d \\text{ odd}.\n\\]\nRepeat for \\(k\\) random bases \\(a \\in {2, 3, \\ldots, n-2}\\):\n\nCompute \\[\nx \\equiv a^{,d} \\bmod n.\n\\]\nIf \\(x = 1\\) or \\(x = n-1\\), this base passes.\nOtherwise, square up to \\(s-1\\) times: \\[\nx \\leftarrow x^2 \\bmod n.\n\\] If at any step \\(x = n-1\\), the base passes. If none hit \\(n-1\\), declare composite.\n\nIf all \\(k\\) bases pass, declare probably prime. For composite \\(n\\), a random base exposes compositeness with probability at least \\(1/4\\), so error is at most \\((1/4)^k\\).\n\n\nTiny Code (Easy Versions)\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n\nstatic uint64_t mul_mod(uint64_t a, uint64_t b, uint64_t m) {\n    __uint128_t z =$__uint128_t$a * b % m;\n    return (uint64_t)z;\n}\n\nstatic uint64_t pow_mod(uint64_t a, uint64_t e, uint64_t m) {\n    uint64_t r = 1 % m;\n    a %= m;\n    while (e &gt; 0) {\n        if (e & 1) r = mul_mod(r, a, m);\n        a = mul_mod(a, a, m);\n        e &gt;&gt;= 1;\n    }\n    return r;\n}\n\nstatic int miller_rabin_base(uint64_t n, uint64_t a, uint64_t d, int s) {\n    uint64_t x = pow_mod(a, d, n);\n    if (x == 1 || x == n - 1) return 1;\n    for (int i = 1; i &lt; s; i++) {\n        x = mul_mod(x, x, n);\n        if (x == n - 1) return 1;\n    }\n    return 0; // composite for this base\n}\n\nint is_probable_prime(uint64_t n) {\n    if (n &lt; 2) return 0;\n    for (uint64_t p : (uint64_t[]){2,3,5,7,11,13,17,19,23,0}) {\n        if (p == 0) break;\n        if (n % p == 0) return n == p;\n    }\n    // write n-1 = 2^s * d\n    uint64_t d = n - 1;\n    int s = 0;\n    while ((d & 1) == 0) { d &gt;&gt;= 1; s++; }\n\n    // Deterministic set for 64-bit integers\n    uint64_t bases[] = {2, 3, 5, 7, 11, 13, 17};\n    int nb = sizeof(bases) / sizeof(bases[0]);\n    for (int i = 0; i &lt; nb; i++) {\n        uint64_t a = bases[i] % n;\n        if (a &lt;= 1) continue;\n        if (!miller_rabin_base(n, a, d, s)) return 0;\n    }\n    return 1; // probably prime\n}\n\nint main(void) {\n    uint64_t n;\n    if (scanf(\"%llu\", &n) != 1) return 0;\n    printf(\"%llu is %s\\n\", n, is_probable_prime(n) ? \"probably prime\" : \"composite\");\n    return 0;\n}\nPython Version\ndef pow_mod(a, e, m):\n    r = 1\n    a %= m\n    while e &gt; 0:\n        if e & 1:\n            r = (r * a) % m\n        a = (a * a) % m\n        e &gt;&gt;= 1\n    return r\n\ndef miller_rabin(n, bases=None):\n    if n &lt; 2:\n        return False\n    small_primes = [2,3,5,7,11,13,17,19,23]\n    for p in small_primes:\n        if n % p == 0:\n            return n == p\n    # write n-1 = 2^s * d\n    d = n - 1\n    s = 0\n    while d % 2 == 0:\n        d //= 2\n        s += 1\n    if bases is None:\n        # Deterministic for 64-bit range\n        bases = [2, 3, 5, 7, 11, 13, 17]\n    for a in bases:\n        a %= n\n        if a &lt;= 1:\n            continue\n        x = pow_mod(a, d, n)\n        if x == 1 or x == n - 1:\n            continue\n        for _ in range(s - 1):\n            x = (x * x) % n\n            if x == n - 1:\n                break\n        else:\n            return False\n    return True\n\nn = int(input().strip())\nprint(\"probably prime\" if miller_rabin(n) else \"composite\")\n\n\nWhy It Matters\n\nVery fast screening test for large integers.\nStandard in cryptographic key generation pipelines.\nWith a fixed base set, becomes deterministic for bounded ranges (for 64 bit integers, the given bases suffice).\n\n\n\nA Gentle Proof (Why It Works)\nFermat style motivation: if \\(n\\) is prime and \\(\\gcd(a,n)=1\\), then by Euler \\[\na^{n-1} \\equiv 1 \\pmod n.\n\\] Stronger, by writing \\(n-1 = 2^s d\\), the sequence \\[\na^{d},\\ a^{2d},\\ a^{4d},\\ \\ldots,\\ a^{2^{s-1}d} \\pmod n\n\\] must land at \\(1\\) through a chain that can only introduce the value \\(-1 \\equiv n-1\\) immediately before \\(1\\). If the chain misses \\(n-1\\), \\(a\\) certifies compositeness. For composite \\(n\\), at least three quarters of \\(a\\) are witnesses, giving error at most \\((1/4)^k\\) after \\(k\\) independent bases.\n\n\nTry It Yourself\n\nFactor \\(n-1\\) as \\(2^s d\\) for \\(n = 561\\) and test bases \\(a = 2, 3, 5\\).\nGenerate random 64 bit odd \\(n\\) and compare Miller–Rabin against trial division up to \\(10^6\\).\nReplace bases with random choices and measure error frequency on Carmichael numbers.\nExtend to a deterministic set that covers your target range.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\\(n\\)\nResult\nNotes\n\n\n\n\n\\(37\\)\nprobably prime\nprime\n\n\n\\(221 = 13 \\cdot 17\\)\ncomposite\nsmall factor\n\n\n\\(561\\)\ncomposite\nCarmichael number\n\n\n\\(2^{61}-1\\)\nprobably prime\nMersenne candidate\n\n\n\\(10^{18}+3\\)\ncomposite or probably prime\ndepends on actual value\n\n\n\n\n\nComplexity\n\nTime: \\(O(k \\log^3 n)\\) with schoolbook modular multiplication, \\(k\\) bases.\nSpace: \\(O(1)\\) iterative state.\n\nMiller–Rabin gives a fast and reliable primality screen: decompose \\(n-1\\), test a few bases, and either certify compositeness or return a very strong probably prime.\n\n\n\n515 Fermat Primality Test\nThe Fermat primality test is one of the simplest probabilistic tests for determining whether a number is likely prime. It’s based on Fermat’s Little Theorem, which states that if\\(n\\) is prime and\\(a\\) is not divisible by\\(n\\), then\n\\[\na^{n-1} \\equiv 1 \\pmod{n}.\n\\]\nIf this congruence fails for some base\\(a\\), then\\(n\\) is definitely composite. If it holds for several randomly chosen bases,\\(n\\) is probably prime.\n\nWhat Problem Are We Solving?\nWe want a fast check for whether\\(n\\) is prime, especially for large\\(n\\), where trial division or sieves are too slow.\nWe test whether numbers satisfy Fermat’s congruence condition:\n\\[\na^{n-1} \\bmod n = 1\n\\]\nfor random bases\\(a \\in [2, n-2]\\).\n\n\nHow Does It Work (Plain Language)\n\nChoose a random integer\\(a\\),\\(2 \\le a \\le n-2\\).\nCompute\\(x = a^{n-1} \\bmod n\\).\nIf\\(x \\ne 1\\),\\(n\\) is composite.\nIf\\(x = 1\\),\\(n\\) might be prime.\nRepeat several times with different\\(a\\) for higher confidence.\n\nIf\\(n\\) passes for all chosen bases, we say probably prime. If it fails for any base, composite.\nExample: Test\\(n = 561\\) (a Carmichael number).\n\nPick\\(a = 2\\):\\(2^{560} \\bmod 561 = 1\\)\nPick\\(a = 3\\):\\(3^{560} \\bmod 561 = 1\\)\nPick\\(a = 5\\):\\(5^{560} \\bmod 561 = 1\\)\n\nAll pass, but\\(561 = 3 \\cdot 11 \\cdot 17\\) is composite. Hence, Fermat test can be fooled by Carmichael numbers.\n\n\nTiny Code (Easy Versions)\nPython Version\nimport random\n\ndef pow_mod(a, e, m):\n    r = 1\n    a %= m\n    while e &gt; 0:\n        if e & 1:\n            r = (r * a) % m\n        a = (a * a) % m\n        e &gt;&gt;= 1\n    return r\n\ndef fermat_test(n, k=5):\n    if n &lt; 2:\n        return False\n    if n in (2, 3):\n        return True\n    if n % 2 == 0:\n        return False\n    for _ in range(k):\n        a = random.randint(2, n - 2)\n        if pow_mod(a, n - 1, n) != 1:\n            return False\n    return True\n\nn = int(input(\"Enter n: \"))\nprint(\"Probably prime\" if fermat_test(n) else \"Composite\")\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nlong long pow_mod(long long a, long long e, long long m) {\n    long long r = 1;\n    a %= m;\n    while (e &gt; 0) {\n        if (e & 1) r = (r * a) % m;\n        a = (a * a) % m;\n        e &gt;&gt;= 1;\n    }\n    return r;\n}\n\nint fermat_test(long long n, int k) {\n    if (n &lt; 2) return 0;\n    if (n == 2 || n == 3) return 1;\n    if (n % 2 == 0) return 0;\n    srand(time(NULL));\n    for (int i = 0; i &lt; k; i++) {\n        long long a = 2 + rand() % (n - 3);\n        if (pow_mod(a, n - 1, n) != 1)\n            return 0;\n    }\n    return 1;\n}\n\nint main(void) {\n    long long n;\n    printf(\"Enter n: \");\n    scanf(\"%lld\", &n);\n    printf(\"%lld is %s\\n\", n, fermat_test(n, 5) ? \"probably prime\" : \"composite\");\n}\n\n\nWhy It Matters\n\nExtremely simple to implement\nUseful as a first filter before stronger tests (e.g. Miller–Rabin)\nFoundation for many probabilistic algorithms in number theory\nHelps illustrate Fermat’s Little Theorem in computational form\n\n\n\nA Gentle Proof (Why It Works)\nIf\\(n\\) is prime and\\(\\gcd(a, n) = 1\\), Fermat’s Little Theorem guarantees:\n\\[\na^{n-1} \\equiv 1 \\pmod{n}.\n\\]\nIf this fails,\\(n\\) cannot be prime. However, some composite numbers (Carmichael numbers) satisfy this condition for all\\(a\\) coprime to\\(n\\). Thus, the test is probabilistic, not deterministic.\n\n\nTry It Yourself\n\nTest\\(n = 37\\) with bases 2, 3, 5.\nTry\\(n = 561\\) and see the failure.\nIncrease\\(k\\) (number of trials) to see stability.\nCompare runtime with Miller–Rabin.\nBuild a composite that passes one base but fails another.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nBases\nResult\n\n\n\n\n37\n2, 3, 5\nPass → probably prime\n\n\n15\n2\nFail → composite\n\n\n561\n2, 3, 5\nPass → false positive\n\n\n97\n2, 3\nPass → probably prime\n\n\n\n\n\nComplexity\n\nTime: \\(O(k \\log^3 n)\\) (due to modular exponentiation)\nSpace: \\(O(1)\\)\n\nThe Fermat test is primality at lightning speed, but with a trickster’s flaw: it can be fooled by clever composites called Carmichael numbers.\n\n\n\n516 Pollard’s Rho Algorithm\nThe Pollard’s Rho algorithm is a clever randomized method for integer factorization. It uses a simple iterative function and the birthday paradox to find nontrivial factors quickly, without trial division. Though probabilistic, it’s extremely effective for finding small factors of large numbers.\n\nWhat Problem Are We Solving?\nGiven a composite number\\(n\\), find a nontrivial factor\\(d\\) such that ( 1 &lt; d &lt; n$.\nInstead of checking divisibility exhaustively, Pollard’s Rho uses a pseudorandom sequence modulo\\(n\\) and detects when two values become congruent modulo a hidden factor.\n\n\nHow Does It Work (Plain Language)\nWe define an iteration:\n\\[\nx_{i+1} = f(x_i) \\bmod n, \\quad \\text{commonly } f(x) = (x^2 + c) \\bmod n\n\\]\nTwo sequences running at different speeds (like a “tortoise and hare”) eventually collide modulo a factor of\\(n\\). When they do, the gcd of their difference with\\(n\\) gives a factor.\nAlgorithm Outline\n\nPick a random function\\(f(x) = (x^2 + c) \\bmod n\\) with random\\(x_0\\) and\\(c\\).\nSet\\(x = y = x_0\\),\\(d = 1\\).\nWhile\\(d = 1\\):\n\\(x = f(x)\\) \\(y = f(f(y))\\) *\\(d = \\gcd(|x - y|, n)\\)\nIf\\(d = n\\), restart with a new function.\nIf\\(1 &lt; d &lt; n\\), output\\(d\\).\n\nExample: Let\\(n = 8051 = 83 \\times 97\\). Choose\\(f(x) = (x^2 + 1) \\bmod 8051\\),\\(x_0 = 2\\).\n\n\n\nStep\n(x)\n(y)\n(\nx-y\n\n\n\n\n1\n5\n26\n21\n1\n\n\n2\n26\n7474\n7448\n83\n\n\n\nOk Found factor\\(83\\)\n\n\nTiny Code (Easy Versions)\nPython Version\nimport math\nimport random\n\ndef f(x, c, n):\n    return (x * x + c) % n\n\ndef pollard_rho(n):\n    if n % 2 == 0:\n        return 2\n    x = random.randint(2, n - 1)\n    y = x\n    c = random.randint(1, n - 1)\n    d = 1\n    while d == 1:\n        x = f(x, c, n)\n        y = f(f(y, c, n), c, n)\n        d = math.gcd(abs(x - y), n)\n        if d == n:\n            return pollard_rho(n)\n    return d\n\nn = int(input(\"Enter n: \"))\nfactor = pollard_rho(n)\nprint(f\"Nontrivial factor of {n}: {factor}\")\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nlong long gcd(long long a, long long b) {\n    while (b != 0) {\n        long long t = b;\n        b = a % b;\n        a = t;\n    }\n    return a;\n}\n\nlong long f(long long x, long long c, long long n) {\n    return (x * x + c) % n;\n}\n\nlong long pollard_rho(long long n) {\n    if (n % 2 == 0) return 2;\n    long long x = rand() % (n - 2) + 2;\n    long long y = x;\n    long long c = rand() % (n - 1) + 1;\n    long long d = 1;\n\n    while (d == 1) {\n        x = f(x, c, n);\n        y = f(f(y, c, n), c, n);\n        long long diff = x &gt; y ? x - y : y - x;\n        d = gcd(diff, n);\n        if (d == n) return pollard_rho(n);\n    }\n    return d;\n}\n\nint main(void) {\n    srand(time(NULL));\n    long long n;\n    printf(\"Enter n: \");\n    scanf(\"%lld\", &n);\n    long long factor = pollard_rho(n);\n    printf(\"Nontrivial factor: %lld\\n\", factor);\n}\n\n\nWhy It Matters\n\nFinds factors much faster than trial division\nCrucial in integer factorization, cryptanalysis, and RSA key testing\nBasis for advanced algorithms (e.g. Pollard’s p–1, Brent’s Rho)\nProbabilistic yet efficient for small factors\n\n\n\nA Gentle Proof (Why It Works)\nIf two values become congruent modulo a factor\\(p\\) of\\(n\\): \\[\nx_i \\equiv x_j \\pmod{p}, \\quad i \\ne j\n\\] then\\(p \\mid (x_i - x_j)\\), and thus \\[\n\\gcd(|x_i - x_j|, n) = p.\n\\] Because\\(p\\) divides\\(n\\), but not all of\\(n\\), it reveals a nontrivial factor.\nThe “Rho” shape refers to the cycle formed by repeated squaring modulo\\(p\\).\n\n\nTry It Yourself\n\nFactor\\(n = 8051\\).\nTry different functions\\(f(x) = x^2 + c\\).\nTest on \\(n = 91, 187, 589\\).\nCompare runtime to trial division.\nCombine with recursion to fully factor \\(n\\).\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nFactors\nFound\n\n\n\n\n91\n7 × 13\n7\n\n\n187\n11 × 17\n17\n\n\n8051\n83 × 97\n83\n\n\n2047\n23 × 89\n23\n\n\n\n\n\nComplexity\n\nExpected Time: \\(O(n^{1/4})\\)\nSpace: \\(O(1)\\)\n\nPollard’s Rho is like chasing your own tail, but eventually, the loop gives up a hidden factor.\n\n\n\n517 Pollard’s p−1 Method\nThe Pollard’s p−1 algorithm is a specialized factorization method that works best when a prime factor\\(p\\) of\\(n\\) has a smooth value of\\(p - 1\\) (that is,\\(p-1\\) factors completely into small primes). It’s one of the earliest practical improvements over trial division, simple, elegant, and effective for numbers with smooth prime factors.\n\nWhat Problem Are We Solving?\nGiven a composite number\\(n\\), we want to find a nontrivial factor\\(d\\) such that\\(1 &lt; d &lt; n\\).\nThis method exploits Fermat’s Little Theorem:\n\\[\na^{p-1} \\equiv 1 \\pmod{p}\n\\]\nIf\\(p \\mid n\\), then\\(p\\) divides\\(a^{p-1} - 1\\), even if\\(p\\) is unknown.\nBy computing\\(\\gcd(a^M - 1, n)\\) for a suitable\\(M\\), we may find such\\(p\\).\n\n\nHow Does It Work (Plain Language)\nIf\\(p\\) is a prime factor of\\(n\\) and\\(p-1\\) divides\\(M\\), then\\(a^{M} \\equiv 1 \\pmod{p}\\). So\\(p\\) divides\\(a^{M} - 1\\). Taking the gcd with\\(n\\) reveals\\(p\\).\nAlgorithm:\n\nChoose a base\\(a\\) (commonly 2).\nChoose a smoothness bound\\(B\\).\nCompute: \\[\nM = \\text{lcm}(1, 2, 3, \\ldots, B)\n\\] and \\[\ng = \\gcd(a^M - 1, n)\n\\]\nIf\\(1 &lt; g &lt; n\\), return\\(g\\) (a nontrivial factor). If\\(g = 1\\), increase\\(B\\). If\\(g = n\\), choose a different\\(a\\).\n\nExample: Let\\(n = 91 = 7 \\times 13\\).\nTake\\(a = 2\\),\\(B = 5\\). Then\\(M = \\text{lcm}(1, 2, 3, 4, 5) = 60\\).\nCompute\\(g = \\gcd(2^{60} - 1, 91)\\).\n\\[\n2^{60} - 1 \\bmod 91 = 0 \\implies g = 7\n\\]\nOk Found factor\\(7\\)\n\n\nTiny Code (Easy Versions)\nPython Version\nimport math\nfrom math import gcd\n\ndef pow_mod(a, e, n):\n    r = 1\n    a %= n\n    while e &gt; 0:\n        if e & 1:\n            r = (r * a) % n\n        a = (a * a) % n\n        e &gt;&gt;= 1\n    return r\n\ndef pollard_p_minus_1(n, B=10, a=2):\n    M = 1\n    for i in range(2, B + 1):\n        M *= i // math.gcd(M, i)\n    x = pow_mod(a, M, n)\n    g = gcd(x - 1, n)\n    if 1 &lt; g &lt; n:\n        return g\n    return None\n\nn = int(input(\"Enter n: \"))\nfactor = pollard_p_minus_1(n, B=10, a=2)\nif factor:\n    print(f\"Nontrivial factor of {n}: {factor}\")\nelse:\n    print(\"No factor found, try larger B\")\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nlong long gcd(long long a, long long b) {\n    while (b != 0) {\n        long long t = b;\n        b = a % b;\n        a = t;\n    }\n    return a;\n}\n\nlong long pow_mod(long long a, long long e, long long n) {\n    long long r = 1 % n;\n    a %= n;\n    while (e &gt; 0) {\n        if (e & 1) r = (r * a) % n;\n        a = (a * a) % n;\n        e &gt;&gt;= 1;\n    }\n    return r;\n}\n\nlong long pollard_p_minus_1(long long n, int B, long long a) {\n    long long M = 1;\n    for (int i = 2; i &lt;= B; i++) {\n        long long g = gcd(M, i);\n        M = M / g * i;\n    }\n    long long x = pow_mod(a, M, n);\n    long long g = gcd(x - 1, n);\n    if (g &gt; 1 && g &lt; n) return g;\n    return 0;\n}\n\nint main(void) {\n    long long n;\n    printf(\"Enter n: \");\n    scanf(\"%lld\", &n);\n    long long factor = pollard_p_minus_1(n, 10, 2);\n    if (factor)\n        printf(\"Nontrivial factor: %lld\\n\", factor);\n    else\n        printf(\"No factor found. Try larger B.\\n\");\n}\n\n\nWhy It Matters\n\nExcellent for factoring numbers with smooth prime factors\nSimple to implement\nFast compared to trial division\nBuilds intuition for group order and Fermat’s Little Theorem\n\nUsed in:\n\nRSA key validation\nElliptic curve methods (ECM) as conceptual base\nEducational number theory and cryptography\n\n\n\nA Gentle Proof (Why It Works)\nIf\\(p \\mid n\\) and\\(p - 1 \\mid M\\), then by Fermat’s Little Theorem: \\[\na^{p-1} \\equiv 1 \\pmod{p}\n\\] so \\[\na^{M} \\equiv 1 \\pmod{p}.\n\\]\nHence\\(p \\mid a^M - 1\\), so \\[\n\\gcd(a^M - 1, n) \\ge p.\n\\]\nIf\\(p \\ne n\\), this gcd gives a nontrivial factor.\nIf\\(p-1\\) is not smooth,\\(M\\) must be larger to capture its factors.\n\n\nTry It Yourself\n\nFactor\\(91 = 7 \\times 13\\) with\\(B = 5\\).\nTry\\(8051 = 83 \\times 97\\); increase\\(B\\) gradually.\nExperiment with different bases\\(a\\).\nCompare runtime with Pollard’s Rho.\nObserve failure when\\(p-1\\) has large prime factors.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nFactors\nBound \\(B\\)\nFound\n\n\n\n\n91\n7 × 13\n5\n7\n\n\n187\n11 × 17\n10\n11\n\n\n589\n19 × 31\n15\n19\n\n\n8051\n83 × 97\n20\n83\n\n\n\n\n\nComplexity\n\nTime:\\(O(B \\log^2 n)\\), depends on smoothness of\\(p-1\\)\nSpace:\\(O(1)\\)\n\nPollard’s p−1 method is a mathematical keyhole, it opens composite locks when one factor’s order is built from small primes.\n\n\n\n518 Wheel Factorization\nWheel factorization is a deterministic optimization for trial division. It systematically skips obvious composites by constructing a repeating pattern (the “wheel”) of candidate offsets that are coprime to small primes. This reduces the number of divisibility checks, making basic primality and factorization tests much faster.\n\nWhat Problem Are We Solving?\nWe want to test whether a number \\(n\\) is prime or factor it, but without checking every integer up to \\(\\sqrt{n}\\).\nInstead of testing all numbers, we skip those clearly divisible by small primes such as \\(2,3,5,7,\\ldots\\). The wheel pattern encodes these skips.\n\n\nHow Does It Work (Plain Language)\n\nChoose a set of small primes (the basis), for example \\(\\{2,3,5\\}\\).\nCompute the wheel size as their product: \\[\nW = 2 \\times 3 \\times 5 = 30\n\\]\nDetermine the residues modulo \\(W\\) that are coprime to \\(W\\): \\[\n\\{1,7,11,13,17,19,23,29\\}\n\\]\nTo test numbers up to \\(n\\), only check candidates of the form \\[\nkW + r \\quad \\text{for } r \\in \\text{residues}.\n\\]\nFor each candidate \\(m\\), test divisibility up to \\(\\sqrt{m}\\).\n\nThis skips about 73% of integers when using the \\(2\\times3\\times5\\) wheel.\nExample\nFind primes \\(\\le 50\\) using the wheel with \\(\\{2,3,5\\}\\).\nWheel residues mod 30: \\[\n1, 7, 11, 13, 17, 19, 23, 29\n\\]\nCandidates: \\[\n1, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 49\n\\]\nFilter by divisibility up to \\(\\sqrt{50}\\):\nPrimes: \\[\n7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47\n\\]\n\n\nTiny Code (Easy Versions)\nPython Version\nimport math\n\ndef wheel_candidates(limit, base_primes=[2, 3, 5]):\n    W = 1\n    for p in base_primes:\n        W *= p\n    residues = [r for r in range(1, W) if all(r % p != 0 for p in base_primes)]\n\n    candidates = []\n    k = 0\n    while k * W &lt;= limit:\n        for r in residues:\n            num = k * W + r\n            if num &lt;= limit:\n                candidates.append(num)\n        k += 1\n    return candidates\n\ndef is_prime(n):\n    if n &lt; 2: return False\n    if n in (2, 3, 5): return True\n    for p in [2, 3, 5]:\n        if n % p == 0:\n            return False\n    for candidate in wheel_candidates(int(math.sqrt(n)) + 1):\n        if n % candidate == 0:\n            return False\n    return True\n\nn = int(input(\"Enter n: \"))\nprint(n, \"is\", \"prime\" if is_prime(n) else \"composite\")\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n\nbool is_prime(int n) {\n    if (n &lt; 2) return false;\n    if (n == 2 || n == 3 || n == 5) return true;\n    if (n % 2 == 0 || n % 3 == 0 || n % 5 == 0) return false;\n\n    int residues[] = {1, 7, 11, 13, 17, 19, 23, 29};\n    int wheel = 30;\n    int limit = (int)sqrt(n);\n\n    for (int k = 0; k * wheel &lt;= limit; k++) {\n        for (int i = 0; i &lt; 8; i++) {\n            int d = k * wheel + residues[i];\n            if (d &gt; 1 && d &lt;= limit && n % d == 0)\n                return false;\n        }\n    }\n    return true;\n}\n\nint main(void) {\n    int n;\n    printf(\"Enter n: \");\n    scanf(\"%d\", &n);\n    printf(\"%d is %s\\n\", n, is_prime(n) ? \"prime\" : \"composite\");\n}\n\n\nWhy It Matters\n\nReduces trial checks by skipping obvious composites\nSpeeds up trial division and sieve methods\nReusable in wheel sieves (e.g. 2×3×5×7 wheel)\nConceptual link between modular arithmetic and primality testing\n\nCommonly used in:\n\nOptimized primality tests\nPrime sieves (wheel sieve of Eratosthenes)\nHybrid factorization routines\n\n\n\nA Gentle Proof (Why It Works)\nAny number (n) that shares a factor with any base prime (p) will appear in a non-coprime residue class modulo (W). By checking only residues coprime to (W), we remove all such composites.\nThus, every remaining candidate is coprime to all base primes, and we need only check divisibility by larger primes.\n\n\nTry It Yourself\n\nBuild wheels for ({2, 3}), ({2, 3, 5}), and ({2, 3, 5, 7}).\nCount how many integers each skips up to 100.\nCompare runtime with plain trial division.\nUse wheel to speed up a sieve implementation.\nPrint residues and visualize the pattern.\n\n\n\nTest Cases\n\n\n\nBase Primes\nWheel Size\nResidues\nSkip %\n\n\n\n\n{2, 3}\n6\n1, 5\n66%\n\n\n{2, 3, 5}\n30\n1, 7, 11, 13, 17, 19, 23, 29\n73%\n\n\n{2, 3, 5, 7}\n210\n48 residues\n77%\n\n\n\n\n\n\n\\(n\\)\nResult\n\n\n\n\n7\nprime\n\n\n49\ncomposite\n\n\n97\nprime\n\n\n121\ncomposite\n\n\n\n\n\nComplexity\n\nTime:\\(O(\\frac{\\sqrt{n}}{\\phi(W)})\\), fewer checks than\\(O(\\sqrt{n})\\)\nSpace:\\(O(1)\\)\n\nWheel factorization is like building a gear that only touches promising numbers, a simple modular pattern to roll through candidates efficiently.\n\n\n\n519 AKS Primality Test\nThe AKS primality test is the first deterministic, polynomial-time algorithm for primality testing that does not rely on unproven hypotheses. It answers one of the oldest questions in computational number theory:\n\nCan we check if a number is prime in polynomial time, for sure?\n\nUnlike probabilistic tests (Fermat, Miller–Rabin), AKS gives a definite answer, no randomness, no false positives.\n\nWhat Problem Are We Solving?\nWe want to deterministically decide whether a number\\(n\\) is prime or composite in polynomial time, without relying on assumptions like the Riemann Hypothesis.\n\n\nThe Core Idea\nA number \\(n\\) is prime if and only if it satisfies\n\\[\n(x + a)^n \\equiv x^n + a \\pmod{n}\n\\]\nfor all integers \\(a\\).\nThis congruence captures the binomial property of primes: in a prime modulus, all binomial coefficients \\(\\binom{n}{k}\\) with \\(0 &lt; k &lt; n\\) vanish modulo \\(n\\).\nThe AKS algorithm refines this condition into a computable primality test.\n\n\nThe Algorithm (Simplified)\nStep 1. Check if \\(n\\) is a perfect power.\nIf \\(n = a^b\\) for some integers \\(a, b &gt; 1\\), then \\(n\\) is composite.\nStep 2. Find the smallest integer \\(r\\) such that\n\\[\n\\text{ord}_r(n) &gt; (\\log_2 n)^2\n\\]\nwhere \\(\\text{ord}_r(n)\\) is the multiplicative order of \\(n\\) modulo \\(r\\).\nStep 3. For each \\(a = 2, 3, \\ldots, r\\):\nIf \\(1 &lt; \\gcd(a, n) &lt; n\\), then \\(n\\) is composite.\nStep 4. If \\(n \\le r\\), then \\(n\\) is prime.\nStep 5. For all integers \\(a = 1, 2, \\ldots, \\lfloor \\sqrt{\\phi(r)} \\log n \\rfloor\\), check whether\n\\[\n(x + a)^n \\equiv x^n + a \\pmod{(x^r - 1, n)}.\n\\]\nIf any test fails, \\(n\\) is composite.\nOtherwise, \\(n\\) is prime.\n\n\nHow Does It Work (Plain Language)\n\nPerfect powers fail primality.\nThe order condition ensures\\(r\\) is large enough to distinguish non-primes.\nSmall gcds catch trivial factors.\nThe polynomial congruence ensures that\\(n\\) behaves like a prime under binomial expansion.\n\nTogether, these steps eliminate all composites and confirm all primes.\n\n\nExample (Conceptual)\nLet \\(n = 7\\).\n\n\\(7\\) is not a perfect power.\n\nThe smallest \\(r\\) such that \\(\\text{ord}_r(7) &gt; (\\log 7)^2 \\approx 5.3\\) is \\(r = 5\\).\n\nNo small \\(\\gcd\\) values found.\n\n\\(n &gt; r\\).\n\nCheck \\[\n(x + a)^7 \\bmod (x^5 - 1, 7) = x^7 + a.\n\\] All tests pass, so \\(n\\) is prime.\n\n\n\nTiny Code (Illustrative Only)\nThe full AKS test is mathematically involved. Below is a simplified prototype that captures its structure, not optimized for large (n).\nPython Version\nimport math\nfrom math import gcd\n\ndef is_perfect_power(n):\n    for b in range(2, int(math.log2(n)) + 2):\n        a = round(n  (1 / b))\n        if a  b == n:\n            return True\n    return False\n\ndef multiplicative_order(n, r):\n    if gcd(n, r) != 1:\n        return 0\n    order = 1\n    value = n % r\n    while value != 1:\n        value = (value * n) % r\n        order += 1\n        if order &gt; r:\n            return 0\n    return order\n\ndef aks_is_prime(n):\n    if n &lt; 2:\n        return False\n    if is_perfect_power(n):\n        return False\n\n    logn2 = (math.log2(n))  2\n    r = 2\n    while True:\n        if gcd(n, r) == 1 and multiplicative_order(n, r) &gt; logn2:\n            break\n        r += 1\n\n    for a in range(2, r + 1):\n        g = gcd(a, n)\n        if 1 &lt; g &lt; n:\n            return False\n\n    if n &lt;= r:\n        return True\n\n    limit = int(math.sqrt(r) * math.log2(n))\n    for a in range(1, limit + 1):\n        # Simplified placeholder: full polynomial congruence omitted\n        if pow(a, n, n) != a % n:\n            return False\n    return True\n\nn = int(input(\"Enter n: \"))\nprint(\"Prime\" if aks_is_prime(n) else \"Composite\")\n\n\nWhy It Matters\n\nFirst general, deterministic, polynomial-time test\nLandmark in computational number theory (Agrawal–Kayal–Saxena, 2002)\nTheoretical foundation for all modern primality testing\nShows primes can be recognized without randomness\n\n\n\nA Gentle Proof (Why It Works)\nIf\\(n\\) is prime, then by the binomial theorem:\n\\[\n(x + a)^n = \\sum_{k=0}^{n} \\binom{n}{k} x^k a^{n-k} \\equiv x^n + a \\pmod{n}\n\\]\nbecause all middle binomial coefficients are divisible by\\(n\\). For composites, this identity fails for some\\(a\\), unless\\(n\\) has special smooth structure (caught by earlier steps).\nHence, the polynomial test is both necessary and sufficient for primality.\n\n\nTry It Yourself\n\nTest \\(n = 37\\), \\(n = 97\\), \\(n = 121\\).\n\nCompare runtime with Miller–Rabin.\n\nObserve exponential slowdown for large \\(n\\).\n\nVerify perfect power rejection.\n\nExplore small \\(r\\) and the order function.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nResult\nNotes\n\n\n\n\n2\nprime\nbase case\n\n\n7\nprime\npasses polynomial check\n\n\n37\nprime\ncorrect\n\n\n121\ncomposite\nfails binomial identity\n\n\n\n\n\nComplexity\n\nTime: \\(O((\\log n)^6)\\) (original), improved to \\(O((\\log n)^3)\\)\n\nSpace: polynomial in \\(\\log n\\)\n\nThe AKS primality test transformed primality checking from an art of heuristics into a science of certainty, proving that primes are decidable in polynomial time.\n\n\n\n520 Segmented Sieve\nThe segmented sieve is a memory-efficient variant of the Sieve of Eratosthenes, designed to generate primes within a large range \\([L, R]\\) without storing all numbers up to \\(R\\).\nIt is ideal when \\(R\\) is very large (for example \\(10^{12}\\)), but the segment width \\(R-L\\) is small enough to fit in memory.\n\nWhat Problem Are We Solving?\nWe want to find all prime numbers in a range \\([L, R]\\), where \\(R\\) may be extremely large.\nA standard sieve up to \\(R\\) would require \\(O(R)\\) space, which is infeasible for \\(R \\gg 10^8\\).\nThe segmented sieve solves this by dividing the range into smaller blocks and marking composites using base primes up to \\(\\sqrt{R}\\).\n\n\nHow Does It Work (Plain Language)\n\nPrecompute base primes up to \\(\\sqrt{R}\\) using a standard sieve.\nFor each segment \\([L, R]\\):\n\nMark all numbers as potentially prime.\nFor each base prime \\(p\\):\n\nFind the first multiple of \\(p\\) in \\([L, R]\\): \\[\n\\text{start} = \\max\\left(p^2,\\; \\left\\lceil \\frac{L}{p} \\right\\rceil \\cdot p \\right)\n\\]\nMark all multiples of \\(p\\) as composite.\n\n\nRemaining unmarked numbers are primes.\n\nRepeat for each segment if the full range is too large to fit in memory.\nExample: Find primes in [100, 120]\n\nCompute base primes up to \\(\\sqrt{120} = 10.9\\):\n\\(\\{2, 3, 5, 7\\}\\)\nStart marking:\n\nFor \\(p = 2\\): mark 100, 102, 104, …\nFor \\(p = 3\\): mark 102, 105, 108, …\nFor \\(p = 5\\): mark 100, 105, 110, 115, 120\nFor \\(p = 7\\): mark 105, 112, 119\n\n\nUnmarked numbers: \\[\n\\boxed{101, 103, 107, 109, 113}\n\\] (these are the primes in [100, 120])\n\n\nTiny Code (Easy Versions)\nPython Version\nimport math\n\ndef simple_sieve(limit):\n    mark = [True] * (limit + 1)\n    mark[0] = mark[1] = False\n    for i in range(2, int(math.sqrt(limit)) + 1):\n        if mark[i]:\n            for j in range(i * i, limit + 1, i):\n                mark[j] = False\n    return [i for i, is_prime in enumerate(mark) if is_prime]\n\ndef segmented_sieve(L, R):\n    base_primes = simple_sieve(int(math.sqrt(R)) + 1)\n    mark = [True] * (R - L + 1)\n    for p in base_primes:\n        start = max(p * p, ((L + p - 1) // p) * p)\n        for j in range(start, R + 1, p):\n            mark[j - L] = False\n    if L == 1:\n        mark[0] = False\n    return [L + i for i, is_prime in enumerate(mark) if is_prime]\n\nL, R = map(int, input(\"Enter L R: \").split())\nprint(\"Primes:\", segmented_sieve(L, R))\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid simple_sieve(int limit, int primes, int *count) {\n    bool *mark = calloc(limit + 1, sizeof(bool));\n    for (int i = 2; i &lt;= limit; i++) mark[i] = true;\n    for (int i = 2; i * i &lt;= limit; i++)\n        if (mark[i])\n            for (int j = i * i; j &lt;= limit; j += i)\n                mark[j] = false;\n\n    *count = 0;\n    for (int i = 2; i &lt;= limit; i++)\n        if (mark[i]) (*count)++;\n\n    *primes = malloc(*count * sizeof(int));\n    int idx = 0;\n    for (int i = 2; i &lt;= limit; i++)\n        if (mark[i]) (*primes)[idx++] = i;\n\n    free(mark);\n}\n\nvoid segmented_sieve(long long L, long long R) {\n    int limit = sqrt(R) + 1;\n    int *primes, count;\n    simple_sieve(limit, &primes, &count);\n\n    bool *mark = calloc(R - L + 1, sizeof(bool));\n    for (int i = 0; i &lt;= R - L; i++) mark[i] = true;\n\n    for (int i = 0; i &lt; count; i++) {\n        int p = primes[i];\n        long long start = (long long)p * p;\n        if (start &lt; L)\n            start = ((L + p - 1) / p) * p;\n        for (long long j = start; j &lt;= R; j += p)\n            mark[j - L] = false;\n    }\n    if (L == 1) mark[0] = false;\n\n    for (int i = 0; i &lt;= R - L; i++)\n        if (mark[i]) printf(\"%lld \", L + i);\n    printf(\"\\n\");\n\n    free(primes);\n    free(mark);\n}\n\nint main(void) {\n    long long L, R;\n    printf(\"Enter L R: \");\n    scanf(\"%lld %lld\", &L, &R);\n    segmented_sieve(L, R);\n}\n\n\nWhy It Matters\n\nMemory-efficient: handles large ranges without full sieve storage\nEssential for prime generation in big intervals (competitive programming, cryptography)\nUsed in factorization and probabilistic primality tests\nDemonstrates divide-and-conquer sieving\n\n\n\nA Gentle Proof (Why It Works)\nEvery composite number in \\([L, R]\\) must have a prime factor \\(\\le \\sqrt{R}\\). By marking multiples of all such base primes, all composites are removed, leaving only primes. Each segment repeats the same logic, correctness holds per block.\n\n\nTry It Yourself\n\nGenerate primes in \\([100, 200]\\).\n\nTest \\([10^{12}, 10^{12} + 1000]\\).\n\nCompare memory use with a full sieve.\n\nImplement dynamic segmenting for very large ranges.\n\nPrint each segment as it is processed.\n\n\n\nTest Cases\n\n\n\nRange \\([L, R]\\)\nOutput\n\n\n\n\n[10, 30]\n11, 13, 17, 19, 23, 29\n\n\n[100, 120]\n101, 103, 107, 109, 113\n\n\n[1, 10]\n2, 3, 5, 7\n\n\n\n\n\nComplexity\n\nTime: \\(O((R - L + 1)\\log \\log R)\\)\n\nSpace: \\(O(\\sqrt{R})\\) for base primes and \\(O(R - L)\\) for the segment\n\nThe segmented sieve works like panning for gold — processing one tray (segment) at a time to uncover primes hidden in vast numerical ranges.\n\n\n\n\nSection 53. Combinatorics\n\n521 Factorial Precomputation\nFactorial precomputation is one of the most useful techniques in combinatorics and modular arithmetic. It allows you to quickly compute values like\\(n!\\), binomial coefficients\\(\\binom{n}{k}\\), or permutations, especially under a modulus\\(M\\), without recomputing from scratch each time.\n\nWhat Problem Are We Solving?\nWe often need\\(n!\\) (factorial) or combinations like:\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\nDirectly computing factorials every time costs\\(O(n)\\), which is too slow when\\(n\\) is large or when we need many queries. By precomputing all factorials once up to\\(n\\), we can answer queries in O(1) time.\nThis is especially important when working modulo\\(M\\), where division requires modular inverses.\n\n\nHow Does It Work (Plain Language)\nWe build an array fact[] such that:\n\\[\n\\text{fact}[i] = (i!) \\bmod M\n\\]\nusing the recurrence:\n\\[\n\\text{fact}[0] = 1, \\quad \\text{fact}[i] = (i \\times \\text{fact}[i - 1]) \\bmod M\n\\]\nIf we also need inverse factorials:\n\\[\n\\text{invfact}[n] = (\\text{fact}[n])^{-1} \\bmod M\n\\]\nthen we can compute all inverses in reverse:\n\\[\n\\text{invfact}[i - 1] = (i \\times \\text{invfact}[i]) \\bmod M\n\\]\nThis lets us compute binomial coefficients fast:\n\\[\n\\binom{n}{k} = \\text{fact}[n] \\times \\text{invfact}[k] \\times \\text{invfact}[n - k] \\bmod M\n\\]\n\n\nExample\nLet\\(M = 10^9 + 7\\),\\(n = 5\\):\n\n\n\n\\(i\\)\n\\(i!\\)\n\\(i! \\bmod M\\)\n\n\n\n\n0\n1\n1\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n6\n6\n\n\n4\n24\n24\n\n\n5\n120\n120\n\n\n\n\\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot 3!} = \\frac{120}{2 \\cdot 6} = 10\n\\]\nWith modular inverses, the same holds under\\(M\\).\n\n\nTiny Code (Easy Versions)\nPython Version\nM = 109 + 7\n\ndef precompute_factorials(n):\n    fact = [1] * (n + 1)\n    for i in range(1, n + 1):\n        fact[i] = (fact[i - 1] * i) % M\n    return fact\n\ndef modinv(a, m=M):\n    return pow(a, m - 2, m)  # Fermat's Little Theorem\n\ndef precompute_inverses(fact):\n    n = len(fact) - 1\n    invfact = [1] * (n + 1)\n    invfact[n] = modinv(fact[n])\n    for i in range(n, 0, -1):\n        invfact[i - 1] = (invfact[i] * i) % M\n    return invfact\n\nn = 10\nfact = precompute_factorials(n)\ninvfact = precompute_inverses(fact)\n\ndef nCr(n, r):\n    if r &lt; 0 or r &gt; n: return 0\n    return fact[n] * invfact[r] % M * invfact[n - r] % M\n\nprint(\"5C2 =\", nCr(5, 2))\nC Version\n#include &lt;stdio.h&gt;\n#define M 1000000007\n#define MAXN 1000000\n\nlong long fact[MAXN + 1], invfact[MAXN + 1];\n\nlong long modpow(long long a, long long e) {\n    long long r = 1;\n    while (e &gt; 0) {\n        if (e & 1) r = r * a % M;\n        a = a * a % M;\n        e &gt;&gt;= 1;\n    }\n    return r;\n}\n\nvoid precompute_factorials(int n) {\n    fact[0] = 1;\n    for (int i = 1; i &lt;= n; i++)\n        fact[i] = fact[i - 1] * i % M;\n\n    invfact[n] = modpow(fact[n], M - 2);\n    for (int i = n; i &gt;= 1; i--)\n        invfact[i - 1] = invfact[i] * i % M;\n}\n\nlong long nCr(int n, int r) {\n    if (r &lt; 0 || r &gt; n) return 0;\n    return fact[n] * invfact[r] % M * invfact[n - r] % M;\n}\n\nint main(void) {\n    precompute_factorials(1000000);\n    printf(\"5C2 = %lld\\n\", nCr(5, 2));\n}\n\n\nWhy It Matters\n\nConverts\\(O(n)\\) recomputation into O(1) queries\nFundamental for combinatorics, DP, and modular counting\nEnables quick computation of:\n\nBinomial coefficients\nMultiset combinations\nProbability computations\nCatalan numbers\n\n\nUsed in:\n\nCombinatorial DP\nProbability and expectation problems\nModular combinatorics\n\n\n\nA Gentle Proof (Why It Works)\nFactorials grow recursively: \\[\nn! = n \\cdot (n-1)!\n\\] So precomputing stores each step once. Modulo arithmetic preserves multiplication structure: \\[\n(ab) \\bmod M = ((a \\bmod M) \\cdot (b \\bmod M)) \\bmod M\n\\] and modular inverses exist when\\(M\\) is prime, by Fermat’s Little Theorem: \\[\na^{M-1} \\equiv 1 \\pmod{M} \\implies a^{-1} \\equiv a^{M-2} \\pmod{M}\n\\]\n\n\nTry It Yourself\n\nPrecompute up to\\(n = 10^6\\) and print factorials.\nCompute\\(1000! \\bmod 10^9+7\\).\nVerify\\(\\binom{n}{k} = \\binom{n}{n-k}\\).\nAdd memoization for varying\\(M\\).\nExtend to double factorials or multinomial coefficients.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(k\\)\n\\(n! \\bmod M\\)\n\\(\\binom{n}{k} \\bmod M\\)\n\n\n\n\n5\n2\n120\n10\n\n\n10\n3\n3628800\n120\n\n\n100\n50\n,\n538992043\n\n\n\n\n\nComplexity\n\nPrecomputation:\\(O(n)\\)\nQuery:$O(1)$\nSpace:\\(O(n)\\)\n\nFactorial precomputation is your lookup table for combinatorics, prepare once, compute instantly.\n\n\n\n522 nCr Computation\nnCr computation (binomial coefficient calculation) is the backbone of combinatorics, it counts the number of ways to choose\\(r\\) elements from a set of\\(n\\) elements, without regard to order. It shows up in counting, probability, DP, and combinatorial identities.\n\nWhat Problem Are We Solving?\nWe want to compute:\n\\[\n\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\n\\]\ndirectly or modulo a large prime\\(M\\), efficiently.\nFor small\\(n\\), this can be done by direct multiplication and division. For large\\(n\\), we must use modular arithmetic and modular inverses, since division is not defined under modulo.\n\n\nHow Does It Work (Plain Language)\nWe can compute\\(\\binom{n}{r}\\) in several ways:\n\nMultiplicative formula (direct):\n\\[\n\\binom{n}{r} = \\prod_{i=1}^{r} \\frac{n - r + i}{i}\n\\]\nWorks well when\\(n\\) and\\(r\\) are moderate ((&lt; 10^6)).\nDynamic Programming (Pascal’s Triangle):\n\\[\n\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\n\\]\nwith base cases\\(\\binom{n}{0} = 1\\),\\(\\binom{n}{n} = 1\\).\nFactorial precomputation (for modulo):\nUsing precomputed arrays: \\[\n\\binom{n}{r} = \\text{fact}[n] \\cdot \\text{invfact}[r] \\cdot \\text{invfact}[n-r] \\bmod M\n\\]\n\n\n\nExample\nCompute\\(\\binom{5}{2}\\):\n\\[\n\\binom{5}{2} = \\frac{5!}{2! \\cdot 3!} = \\frac{120}{12} = 10\n\\]\nCheck with Pascal’s identity:\n\\[\n\\binom{5}{2} = \\binom{4}{1} + \\binom{4}{2} = 4 + 6 = 10\n\\]\n\n\nTiny Code (Easy Versions)\n\nMultiplicative Formula (No Modulo)\n\ndef nCr(n, r):\n    if r &lt; 0 or r &gt; n:\n        return 0\n    r = min(r, n - r)\n    res = 1\n    for i in range(1, r + 1):\n        res = res * (n - r + i) // i\n    return res\n\nprint(nCr(5, 2))  # 10\n\nFactorial + Modular Inverse\n\nM = 109 + 7\n\ndef modpow(a, e, m=M):\n    r = 1\n    while e &gt; 0:\n        if e & 1:\n            r = (r * a) % m\n        a = (a * a) % m\n        e &gt;&gt;= 1\n    return r\n\ndef nCr_mod(n, r):\n    if r &lt; 0 or r &gt; n:\n        return 0\n    fact = [1] * (n + 1)\n    for i in range(1, n + 1):\n        fact[i] = (fact[i - 1] * i) % M\n    invfact = [1] * (n + 1)\n    invfact[n] = modpow(fact[n], M - 2)\n    for i in range(n, 0, -1):\n        invfact[i - 1] = (invfact[i] * i) % M\n    return fact[n] * invfact[r] % M * invfact[n - r] % M\n\nprint(nCr_mod(5, 2))  # 10\n\nPascal’s Triangle (Dynamic Programming)\n\ndef build_pascal(n):\n    C = [[0]*(n+1) for _ in range(n+1)]\n    for i in range(n+1):\n        C[i][0] = C[i][i] = 1\n        for j in range(1, i):\n            C[i][j] = C[i-1][j-1] + C[i-1][j]\n    return C\n\npascal = build_pascal(10)\nprint(pascal[5][2])  # 10\n\n\nWhy It Matters\n\nFundamental to combinatorics\nUsed in:\n\nBinomial expansions\nProbability (e.g., hypergeometric distributions)\nDynamic programming (e.g., counting paths)\nNumber theory (Lucas theorem)\nModular arithmetic combinatorics\n\nThe backbone of:\n\nCatalan numbers\nPascal’s triangle\nInclusion–exclusion principles\n\n\n\n\nA Gentle Proof (Why It Works)\nCombinatorially:\n\nTo choose\\(r\\) items from\\(n\\), either include a specific item or exclude it.\n\nSo: \\[\n\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\n\\] with boundaries: \\[\n\\binom{n}{0} = \\binom{n}{n} = 1\n\\]\nMultiplicatively: \\[\n\\frac{n!}{r!(n-r)!} = \\frac{n}{1} \\times \\frac{n-1}{2} \\times \\cdots \\times \\frac{n-r+1}{r}\n\\]\n\n\nTry It Yourself\n\nCompute\\(\\binom{10}{3}\\) manually and using code.\nGenerate the 6th row of Pascal’s triangle.\nVerify symmetry\\(\\binom{n}{r} = \\binom{n}{n-r}\\).\nImplement modulo version for\\(n = 10^6\\).\nUse nCr to compute Catalan numbers: \\[\nC_n = \\frac{1}{n+1}\\binom{2n}{n}\n\\]\n\n\n\nTest Cases\n\n\n\n(n)\n(r)\nResult\nMethod\n\n\n\n\n5\n2\n10\nfactorial\n\n\n10\n3\n120\nDP\n\n\n100\n50\n538992043\nmodulo\n\n\n\n\n\nComplexity\n\n\n\nMethod\nTime\nSpace\n\n\n\n\nMultiplicative\n(O(r))\n(O(1))\n\n\nDP (Pascal)\n(O(n^2))\n(O(n^2))\n\n\nPrecomputed factorial\n(O(1)) per query\n(O(n))\n\n\n\nnCr is the counting lens of algorithms, every subset, combination, and selection passes through it.\n\n\n\n523 Pascal’s Triangle\nPascal’s Triangle is the geometric arrangement of binomial coefficients. Each entry represents \\(\\binom{n}{r}\\), and every row builds upon the previous one. It’s not just a pretty triangle, it’s the living structure of combinatorics, binomial expansions, and dynamic programming.\n\nWhat Problem Are We Solving?\nWe want to compute binomial coefficients efficiently and recurrently, without factorials or modular inverses.\nWe use the recursive identity:\n\\[\n\\binom{n}{r}=\\binom{n-1}{r-1}+\\binom{n-1}{r}\n\\]\nwith base cases:\n\\[\n\\binom{n}{0}=\\binom{n}{n}=1\n\\]\nThis recurrence builds all combinations in a simple triangle, each number is the sum of the two above it.\n\n\nHow Does It Work (Plain Language)\nStart with row 0: [1] Each new row begins and ends with 1, and every middle element is the sum of two neighbors above.\nExample:\n\n\n\nRow\nValues\n\n\n\n\n0\n1\n\n\n1\n1 1\n\n\n2\n1 2 1\n\n\n3\n1 3 3 1\n\n\n4\n1 4 6 4 1\n\n\n5\n1 5 10 10 5 1\n\n\n\nThe value at row \\(n\\), column \\(r\\) equals \\(\\binom{n}{r}\\).\n\n\nExample\nCompute \\(\\binom{5}{2}\\) from Pascal’s triangle:\nRow 5: 1 5 10 10 5 1 → \\(\\binom{5}{2}=10\\)\nCheck recurrence:\n\\[\n\\binom{5}{2}=\\binom{4}{1}+\\binom{4}{2}=4+6=10\n\\]\n\n\nTiny Code (Easy Versions)\nPython Version\ndef pascal_triangle(n):\n    C = [[0] * (n + 1) for _ in range(n + 1)]\n    for i in range(n + 1):\n        C[i][0] = C[i][i] = 1\n        for j in range(1, i):\n            C[i][j] = C[i - 1][j - 1] + C[i - 1][j]\n    return C\n\nC = pascal_triangle(6)\nfor i in range(6):\n    print(C[i][:i + 1])\n\nprint(\"C(5,2) =\", C[5][2])\nC Version\n#include &lt;stdio.h&gt;\n\nvoid pascal_triangle(int n) {\n    int C[n + 1][n + 1];\n    for (int i = 0; i &lt;= n; i++) {\n        for (int j = 0; j &lt;= i; j++) {\n            if (j == 0 || j == i)\n                C[i][j] = 1;\n            else\n                C[i][j] = C[i - 1][j - 1] + C[i - 1][j];\n            printf(\"%d \", C[i][j]);\n        }\n        printf(\"\\n\");\n    }\n}\n\nint main(void) {\n    pascal_triangle(6);\n    return 0;\n}\n\n\nWhy It Matters\n\nEfficient \\(O(n^2)\\) construction of all \\(\\binom{n}{r}\\)\nNo factorials, no large-number overflows for small \\(n\\)\nFoundation for:\n\nBinomial expansions: \\((a+b)^n=\\sum_{r=0}^n\\binom{n}{r}a^{n-r}b^r\\)\nCombinatorial DP: counting subsets, paths, partitions\nProbability computations\n\n\nAlso connects to:\n\nFibonacci (diagonal sums)\nPowers of 2 (row sums)\nSierpinski triangles (mod 2 pattern)\n\n\n\nA Gentle Proof (Why It Works)\nEach term counts ways to choose \\(r\\) items from \\(n\\): either include a particular element or exclude it.\nSo:\n\\[\n\\binom{n}{r}=\\binom{n-1}{r-1}+\\binom{n-1}{r}\n\\]\nwhere:\n\n\\(\\binom{n-1}{r-1}\\): choose \\(r-1\\) from remaining after including\n\\(\\binom{n-1}{r}\\): choose \\(r\\) from remaining after excluding\n\nThis recurrence relation constructs the entire triangle layer by layer.\n\n\nTry It Yourself\n\nPrint the first 10 rows of Pascal’s triangle.\nVerify that the sum of row \\(n\\) equals \\(2^n\\).\nUse triangle values to expand \\((a+b)^5\\).\nVisualize pattern mod 2, get the Sierpinski triangle.\nUse diagonals to build Fibonacci sequence.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(r\\)\n\\(\\binom{n}{r}\\)\nTriangle Row\n\n\n\n\n5\n2\n10\n[1, 5, 10, 10, 5, 1]\n\n\n6\n3\n20\n[1, 6, 15, 20, 15, 6, 1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\)\nSpace: \\(O(n^2)\\), or \\(O(n)\\) with row compression\n\nPascal’s Triangle is combinatorics in motion, a growing, recursive landscape where every number remembers its ancestors.\n\n\n\n524 Multiset Combination\nMultiset combination counts selections with repetition allowed, choosing \\(r\\) elements from \\(n\\) types, where each type can appear multiple times. It’s the combinatorial backbone for problems like compositions, integer partitions, stars and bars, and bag combinations.\n\nWhat Problem Are We Solving?\nWe want to count the number of ways to choose \\(r\\) items from \\(n\\) types when duplicates are allowed.\nFor example, with types \\({A,B,C}\\) and \\(r=2\\), valid selections are: \\[\n{AA,AB,AC,BB,BC,CC}\n\\] That’s 6 combinations, not \\(\\binom{3}{2}=3\\).\nSo the formula is:\n\\[\n\\text{MultisetCombination}(n,r)=\\binom{n+r-1}{r}\n\\]\nThis is the “stars and bars” formula.\n\n\nHow Does It Work (Plain Language)\nThink of \\(r\\) identical stars (items) separated into \\(n\\) groups by \\((n-1)\\) dividers (bars).\nExample: \\(n=3\\), \\(r=4\\)\nWe need to arrange 4 stars and 2 bars:\n- * | * | *\nEach arrangement corresponds to one combination.\nTotal arrangements:\n\\[\n\\binom{n+r-1}{r}=\\binom{6}{4}=15\n\\]\nSo there are 15 ways to pick 4 items from 3 types with repetition.\n\n\nExample\nLet \\(n=3\\) (types A, B, C), \\(r=2\\). Formula:\n\\[\n\\binom{3+2-1}{2}=\\binom{4}{2}=6\n\\]\nList all: AA, AB, AC, BB, BC, CC\nOk Matches formula.\n\n\nTiny Code (Easy Versions)\nPython Version\nfrom math import comb\n\ndef multiset_combination(n, r):\n    return comb(n + r - 1, r)\n\nprint(\"Combinations (n=3, r=2):\", multiset_combination(3, 2))\nModulo Version (using factorial precomputation)\nM = 109 + 7\n\ndef modpow(a, e, m=M):\n    r = 1\n    while e &gt; 0:\n        if e & 1:\n            r = (r * a) % m\n        a = (a * a) % m\n        e &gt;&gt;= 1\n    return r\n\ndef modinv(a):\n    return modpow(a, M - 2)\n\ndef nCr_mod(n, r):\n    if r &lt; 0 or r &gt; n:\n        return 0\n    fact = [1] * (n + 1)\n    for i in range(1, n + 1):\n        fact[i] = fact[i - 1] * i % M\n    return fact[n] * modinv(fact[r]) % M * modinv(fact[n - r]) % M\n\ndef multiset_combination_mod(n, r):\n    return nCr_mod(n + r - 1, r)\n\nprint(\"Multiset Combination (n=3, r=2):\", multiset_combination_mod(3, 2))\n\n\nWhy It Matters\n\nModels combinations with replacement\nAppears in:\n\nCounting multisets / bags\nInteger partitioning\nPolynomial coefficient enumeration\nDistributing identical balls into boxes\n\nKey in DP over multisets, generating functions, and probability spaces\n\n\n\nA Gentle Proof (Why It Works)\nRepresent \\(r\\) identical items as stars * and \\(n-1\\) dividers as bars |.\nExample: \\(n=4\\), \\(r=3\\) We have \\(r+(n-1)=6\\) symbols. Choosing positions of \\(r\\) stars:\n\\[\n\\binom{r+n-1}{r}\n\\]\nEach unique arrangement corresponds to a multiset.\nHence, total combinations = \\(\\binom{n+r-1}{r}\\).\n\n\nTry It Yourself\n\nCount ways to choose 3 fruits from {apple, banana, cherry}.\nCompute \\(\\text{MultisetCombination}(5,2)\\) and list a few examples.\nBuild a DP table using Pascal’s triangle recurrence: \\[\nf(n,r)=f(n,r-1)+f(n-1,r)\n\\]\nUse modulo arithmetic for large \\(n\\).\nVisualize “stars and bars” layouts for \\(n=4, r=3\\).\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(r\\)\n\\(\\binom{n+r-1}{r}\\)\nResult\n\n\n\n\n3\n2\n\\(\\binom{4}{2}=6\\)\n6\n\n\n4\n3\n\\(\\binom{6}{3}=20\\)\n20\n\n\n2\n5\n\\(\\binom{6}{5}=6\\)\n6\n\n\n\n\n\nComplexity\n\nTime: \\(O(1)\\) (with precomputed factorials)\nSpace: \\(O(n+r)\\) (for factorial storage)\n\nMultiset combinations open up counting beyond uniqueness, when repetition is a feature, not a flaw.\n\n\n\n525 Permutation Generation\nPermutation generation is the process of listing all possible arrangements of a set of elements, the order now matters. It’s one of the most fundamental operations in combinatorics, recursion, and search algorithms, powering brute-force solvers, lexicographic enumeration, and backtracking frameworks.\n\nWhat Problem Are We Solving?\nWe want to generate all permutations of a collection of size \\(n\\), that is, every possible ordering.\nFor example, for \\({1, 2, 3}\\), the permutations are:\n\\[\n{1,2,3},{1,3,2},{2,1,3},{2,3,1},{3,1,2},{3,2,1}\n\\]\nThere are \\(n!\\) total permutations.\n\n\nHow Does It Work (Plain Language)\nThere are multiple strategies:\n\nRecursive Backtracking\n\nChoose an element\nPermute the remaining\nCombine\n\nLexicographic (Next Permutation)\n\nGenerate in sorted order by finding next lexicographic successor\n\nHeap’s Algorithm\n\nSwap-based iterative generation in \\(O(n!)\\) time, \\(O(n)\\) space\n\n\nRecursive Approach Example\nTo generate all permutations of \\({1,2,3}\\):\n\nFix 1 → permute \\({2,3}\\) → \\({1,2,3},{1,3,2}\\)\nFix 2 → permute \\({1,3}\\) → \\({2,1,3},{2,3,1}\\)\nFix 3 → permute \\({1,2}\\) → \\({3,1,2},{3,2,1}\\)\n\n\n\nTiny Code (Easy Versions)\nPython Version (Recursive Backtracking)\ndef permute(arr, path=[]):\n    if not arr:\n        print(path)\n        return\n    for i in range(len(arr)):\n        permute(arr[:i] + arr[i+1:], path + [arr[i]])\n\npermute([1, 2, 3])\nPython Version (Using Built-in)\nfrom itertools import permutations\n\nfor p in permutations([1, 2, 3]):\n    print(p)\nC Version (Backtracking)\n#include &lt;stdio.h&gt;\n\nvoid swap(int *a, int *b) {\n    int t = *a; *a = *b; *b = t;\n}\n\nvoid permute(int *arr, int l, int r) {\n    if (l == r) {\n        for (int i = 0; i &lt;= r; i++) printf(\"%d \", arr[i]);\n        printf(\"\\n\");\n        return;\n    }\n    for (int i = l; i &lt;= r; i++) {\n        swap(&arr[l], &arr[i]);\n        permute(arr, l + 1, r);\n        swap(&arr[l], &arr[i]); // backtrack\n    }\n}\n\nint main(void) {\n    int arr[] = {1, 2, 3};\n    permute(arr, 0, 2);\n}\n\n\nWhy It Matters\n\nFoundation for brute-force search, backtracking, and enumeration\nPowers:\n\nTraveling Salesman Problem (TSP) brute-force\nPermutation testing in statistics\nOrder-based search in AI / combinatorial optimization\n\nUseful in:\n\nCombinatorics\nString generation\nConstraint solving\n\n\n\n\nA Gentle Proof (Why It Works)\nEach position in the permutation can hold one of the remaining unused elements.\nThe recurrence:\n\\[\nP(n)=n\\cdot P(n-1)\n\\]\nBase case \\(P(1)=1\\), so \\(P(n)=n!\\).\nEvery branch in recursion represents one arrangement, total branches = \\(n!\\).\n\n\nTry It Yourself\n\nGenerate all permutations of [1,2,3].\nPrint count for n=4 (should be \\(24\\)).\nModify code to store, not print, permutations.\nImplement lexicographic next-permutation method.\nUse permutations to test all possible password orders.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\nInput\nOutput (Permutations)\nCount\n\n\n\n\n[1,2]\n[1,2], [2,1]\n2\n\n\n[1,2,3]\n[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]\n6\n\n\n[‘A’,‘B’]\nAB, BA\n2\n\n\n\n\n\nComplexity\n\nTime: \\(O(n!)\\), total permutations\nSpace: \\(O(n)\\) recursion depth\n\nPermutation generation is exhaustive creativity, exploring every possible order, every possible path.\n\n\n\n526 Next Permutation\nThe Next Permutation algorithm finds the next lexicographically greater permutation of a given sequence. It’s a building block for lexicographic enumeration, letting you step through permutations in sorted order without generating all at once.\nIf no larger permutation exists (the sequence is in descending order), it resets to the smallest (ascending order).\n\nWhat Problem Are We Solving?\nGiven a permutation (arranged sequence), find the next lexicographically larger one.\nExample: From [1, 2, 3], the next permutation is [1, 3, 2]. From [3, 2, 1], there’s no larger one, so we reset to [1, 2, 3].\nWe want a method that transforms a sequence in-place, in O(n) time.\n\n\nHow Does It Work (Plain Language)\nImagine your sequence as a number, e.g. [1, 2, 3] represents 123. You want the next bigger number made from the same digits.\nAlgorithm steps:\n\nFind pivot, scan from right to left to find the first index i where \\(a[i] &lt; a[i+1]\\) (This is the point where the next permutation can be increased.)\nFind successor, from the right, find the smallest \\(a[j] &gt; a[i]\\).\nSwap \\(a[i]\\) and \\(a[j]\\).\nReverse the suffix starting at \\(i+1\\) (turn descending tail into ascending).\n\nIf no pivot is found, reverse the whole array (last permutation → first).\nExample\nFrom [1, 2, 3]:\n\nPivot at 2 (a[1]), since 2 &lt; 3.\nSuccessor = 3.\nSwap → [1, 3, 2].\nReverse suffix [2] → unchanged. Ok Next permutation = [1, 3, 2].\n\nFrom [3, 2, 1]: No pivot (entirely descending) → reverse to [1, 2, 3].\n\n\nTiny Code (Easy Versions)\nPython Version\ndef next_permutation(arr):\n    n = len(arr)\n    i = n - 2\n    while i &gt;= 0 and arr[i] &gt;= arr[i + 1]:\n        i -= 1\n    if i == -1:\n        arr.reverse()\n        return False  # last permutation\n    j = n - 1\n    while arr[j] &lt;= arr[i]:\n        j -= 1\n    arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1:] = reversed(arr[i + 1:])\n    return True\n\n# Example\narr = [1, 2, 3]\nnext_permutation(arr)\nprint(arr)  # [1, 3, 2]\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid reverse(int *a, int l, int r) {\n    while (l &lt; r) {\n        int t = a[l];\n        a[l++] = a[r];\n        a[r--] = t;\n    }\n}\n\nbool next_permutation(int *a, int n) {\n    int i = n - 2;\n    while (i &gt;= 0 && a[i] &gt;= a[i + 1]) i--;\n    if (i &lt; 0) {\n        reverse(a, 0, n - 1);\n        return false;\n    }\n    int j = n - 1;\n    while (a[j] &lt;= a[i]) j--;\n    int t = a[i]; a[i] = a[j]; a[j] = t;\n    reverse(a, i + 1, n - 1);\n    return true;\n}\n\nint main(void) {\n    int a[] = {1, 2, 3};\n    int n = 3;\n    next_permutation(a, n);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n}\n\n\nWhy It Matters\n\nCore tool for lexicographic enumeration\nPowers:\n\nCombinatorial iteration\nSearch problems (e.g. TSP brute-force)\nString/sequence next-step generation\n\nUsed in C++ STL (std::next_permutation)\n\n\n\nA Gentle Proof (Why It Works)\nThe suffix after pivot \\(a[i]\\) is strictly decreasing, meaning it’s the largest arrangement of that suffix. To get the next larger permutation:\n\nIncrease \\(a[i]\\) minimally (swap with next larger element \\(a[j]\\))\nThen reorder the suffix to the smallest possible arrangement (ascending)\n\nThus ensuring the next lexicographic order.\n\n\nTry It Yourself\n\nStep through [1, 2, 3] → [1, 3, 2] → [2, 1, 3] → …\nWrite loop to print all permutations in order.\nTry on characters: ['A', 'B', 'C'].\nTest [3, 2, 1], should reset to [1, 2, 3].\nModify to generate previous permutation.\n\n\n\nTest Cases\n\n\n\nInput\nOutput (Next)\nNote\n\n\n\n\n[1,2,3]\n[1,3,2]\nsimple\n\n\n[1,3,2]\n[2,1,3]\nmiddle\n\n\n[3,2,1]\n[1,2,3]\nwrap-around\n\n\n[1,1,5]\n[1,5,1]\nduplicates\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(1)\\)\n\nNext Permutation is your lexicographic stepper, one small swap, one tidy reverse, one giant leap in order.\n\n\n\n527 Subset Generation\nSubset generation is the process of listing every possible subset of a given set, including the empty set and the full set. It’s a cornerstone of combinatorial enumeration, power set construction, and many backtracking and bitmask algorithms.\n\nWhat Problem Are We Solving?\nGiven a set of \\(n\\) elements, generate all its subsets.\nFor example, for \\({1,2,3}\\), the subsets (the power set) are:\n\\[\n\\varnothing,{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3}\n\\]\nTotal number of subsets = \\(2^n\\).\n\n\nHow Does It Work (Plain Language)\nThere are two classic methods:\n\nRecursive / Backtracking Choose whether to include or exclude each element. Build subsets depth-first.\nBitmask Enumeration Represent subsets using a binary mask of length \\(n\\), where bit \\(i\\) indicates whether the \\(i\\)-th element is included.\n\nExample for \\({1,2,3}\\):\n\n\n\nMask\nBinary\nSubset\n\n\n\n\n0\n000\n\\(\\varnothing\\)\n\n\n1\n001\n\\({3}\\)\n\n\n2\n010\n\\({2}\\)\n\n\n3\n011\n\\({2,3}\\)\n\n\n4\n100\n\\({1}\\)\n\n\n5\n101\n\\({1,3}\\)\n\n\n6\n110\n\\({1,2}\\)\n\n\n7\n111\n\\({1,2,3}\\)\n\n\n\n\n\nTiny Code (Easy Versions)\nPython Version (Recursive)\ndef subsets(arr, path=[], i=0):\n    if i == len(arr):\n        print(path)\n        return\n    # Exclude current\n    subsets(arr, path, i + 1)\n    # Include current\n    subsets(arr, path + [arr[i]], i + 1)\n\nsubsets([1, 2, 3])\nPython Version (Bitmask)\ndef subsets_bitmask(arr):\n    n = len(arr)\n    for mask in range(1 &lt;&lt; n):\n        subset = [arr[i] for i in range(n) if mask & (1 &lt;&lt; i)]\n        print(subset)\n\nsubsets_bitmask([1, 2, 3])\nC Version (Bitmask)\n#include &lt;stdio.h&gt;\n\nvoid subsets(int *arr, int n) {\n    int total = 1 &lt;&lt; n;\n    for (int mask = 0; mask &lt; total; mask++) {\n        printf(\"{ \");\n        for (int i = 0; i &lt; n; i++) {\n            if (mask & (1 &lt;&lt; i))\n                printf(\"%d \", arr[i]);\n        }\n        printf(\"}\\n\");\n    }\n}\n\nint main(void) {\n    int arr[] = {1, 2, 3};\n    subsets(arr, 3);\n}\n\n\nWhy It Matters\n\nPowers:\n\nEnumerating possibilities in combinatorial problems\nSubset sum, knapsack, bitmask DP\nSearch and optimization\n\nBasis for:\n\nPower sets\nInclusion–exclusion principle\nState-space exploration\n\n\nEvery subset represents one state, one choice pattern, one branch in the combinatorial tree.\n\n\nA Gentle Proof (Why It Works)\nEach element can be included or excluded independently. That gives \\(2\\) choices per element, so:\n\\[\n\\text{Total subsets} = 2 \\times 2 \\times \\cdots \\times 2 = 2^n\n\\]\nEach binary mask uniquely encodes a subset. Thus both recursive and bitmask methods visit all \\(2^n\\) subsets exactly once.\n\n\nTry It Yourself\n\nGenerate all subsets of [1,2,3,4].\nCount subsets of size 2 only.\nPrint subsets in lexicographic order.\nFilter subsets summing to 5.\nCombine with DP for subset sum.\n\n\n\nTest Cases\n\n\n\nInput\nOutput (Subsets)\nCount\n\n\n\n\n[1,2]\n[], [1], [2], [1,2]\n4\n\n\n[1,2,3]\n8 subsets\n8\n\n\n[]\n[]\n1\n\n\n\n\n\nComplexity\n\nTime: \\(O(2^n \\cdot n)\\) (each subset generation takes \\(O(n)\\))\nSpace: \\(O(n)\\) recursion stack or temporary list\n\nSubset generation is the combinatorial chorus, every yes/no choice joins the melody of all possibilities.\n\n\n\n528 Gray Code Generation\nGray codes list all \\(2^n\\) binary strings of length \\(n\\) so that consecutive codes differ in exactly one bit (Hamming distance \\(1\\)). They are ideal for enumerations where small step changes reduce errors or avoid expensive recomputation.\n\nWhat Problem Are We Solving?\nGenerate an ordering of all \\(n\\)-bit strings \\[\ng_0,g_1,\\dots,g_{2^n-1}\n\\] such that for each \\(k\\ge 1\\), the Hamming distance \\(\\mathrm{dist}(g_{k-1},g_k)=1\\).\n\n\nHow Does It Work (Plain Language)\nThere are two classic constructions:\n\nReflect and prefix (recursive)\n\n\nBase: \\(G_1=[0,1]\\)\nTo build \\(G_{n}\\) from \\(G_{n-1}\\):\n\nTake \\(G_{n-1}\\) and prefix each code with \\(0\\)\nTake the reverse of \\(G_{n-1}\\) and prefix each code with \\(1\\)\n\nConcatenate the two lists\n\n\nBit trick (iterative, index to Gray)\n\n\nThe \\(k\\)-th Gray code is \\[\ng(k)=k\\oplus (k!!\\gg!1)\n\\] where \\(\\oplus\\) is bitwise XOR and \\(\\gg\\) is right shift\n\nBoth yield the same sequence up to renaming.\n\n\nExample\nFor \\(n=3\\), the reflect-and-prefix method gives:\n\nStart with \\(G_1\\): \\(0,1\\)\n\\(G_2\\): prefix \\(0\\) to get \\(00,01\\), prefix \\(1\\) to reversed \\(G_1\\) to get \\(11,10\\)\n\\(G_2=[00,01,11,10]\\)\n\\(G_3\\): prefix \\(0\\) to \\(G_2\\) \\(\\to\\) \\(000,001,011,010\\) prefix \\(1\\) to reversed \\(G_2\\) \\(\\to\\) \\(110,111,101,100\\)\n\nFinal \\(G_3\\): \\[\n000,001,011,010,110,111,101,100\n\\] Each consecutive pair differs in exactly one bit.\n\n\nTiny Code (Easy Versions)\nPython Version: reflect and prefix\ndef gray_reflect(n):\n    codes = [\"0\", \"1\"]\n    if n == 1:\n        return codes\n    for _ in range(2, n + 1):\n        left = [\"0\" + c for c in codes]\n        right = [\"1\" + c for c in reversed(codes)]\n        codes = left + right\n    return codes\n\nprint(gray_reflect(3))  # ['000','001','011','010','110','111','101','100']\nPython Version: bit trick \\(g(k)=k\\oplus(k&gt;&gt;1)\\)\ndef gray_bit(n):\n    return [k ^ (k &gt;&gt; 1) for k in range(1 &lt;&lt; n)]\n\ndef to_bits(x, n):\n    return format(x, f\"0{n}b\")\n\nn = 3\ncodes = [to_bits(v, n) for v in gray_bit(n)]\nprint(codes)  # ['000','001','011','010','110','111','101','100']\nC Version: bit trick\n#include &lt;stdio.h&gt;\n\nunsigned gray(unsigned k) {\n    return k ^ (k &gt;&gt; 1);\n}\n\nvoid print_binary(unsigned x, int n) {\n    for (int i = n - 1; i &gt;= 0; --i)\n        putchar((x & (1u &lt;&lt; i)) ? '1' : '0');\n}\n\nint main(void) {\n    int n = 3;\n    unsigned total = 1u &lt;&lt; n;\n    for (unsigned k = 0; k &lt; total; ++k) {\n        unsigned g = gray(k);\n        print_binary(g, n);\n        putchar('\\n');\n    }\n    return 0;\n}\n\n\nWhy It Matters\n\nConsecutive states differ by one bit, reducing switching errors and glitches in hardware encoders and ADCs\nUseful in gray-code counters, Hamiltonian paths on hypercubes, and search over subsets where incremental updates are cheap\nCommon in combinatorial generation to minimize change between outputs\n\n\n\nA Gentle Proof (Why It Works)\nFor the reflect-and-prefix method:\n\nThe first half is \\(0\\cdot G_{n-1}\\) where neighbors already differ in one bit\nThe second half is \\(1\\cdot \\mathrm{rev}(G_{n-1})\\) where neighbors still differ in one bit\nThe boundary pair differs only in the first bit because we go from \\(0\\cdot g_0\\) to \\(1\\cdot g_0\\) By induction on \\(n\\), consecutive codes differ in exactly one bit.\n\nFor the bit trick:\n\nWrite \\(g(k)=k\\oplus(k&gt;&gt;1)\\) and \\(g(k+1)=(k+1)\\oplus((k+1)&gt;&gt;1)\\)\nOne can check that \\(g(k)\\) and \\(g(k+1)\\) differ only in the lowest bit that changes when incrementing \\(k\\) Hence Hamming distance is \\(1\\).\n\n\n\nTry It Yourself\n\nGenerate Gray codes for \\(n=1\\ldots 5\\) using both methods and compare.\nMap Gray codes back to binary: inverse is \\(b_0=g_0\\), and \\(b_i=b_{i-1}\\oplus g_i\\) for \\(i\\ge 1\\).\nIterate all subsets of \\({0,\\dots,n-1}\\) in Gray order and maintain an incremental sum by flipping one element each step.\nUse Gray order to traverse vertices of the \\(n\\)-cube.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected prefix of sequence\n\n\n\n\n1\n\\(0,1\\)\n\n\n2\n\\(00,01,11,10\\)\n\n\n3\n\\(000,001,011,010,110,111,101,100\\)\n\n\n\nAlso verify that every adjacent pair differs in exactly one bit.\n\n\nComplexity\n\nTime: \\(O(2^n)\\) to output all codes\nSpace: \\(O(n)\\) per code (or \\(O(1)\\) extra using bit trick)\n\nGray codes give a one-bit-at-a-time tour of the hypercube, perfect for smooth transitions in algorithms and hardware.\n\n\n\n529 Catalan Number DP\nCatalan numbers count a wide variety of recursive structures, from valid parentheses strings to binary trees, polygon triangulations, and non-crossing paths. They are the backbone of combinatorial recursion, dynamic programming, and context-free grammars.\n\nWhat Problem Are We Solving?\nWe want to compute the \\(n\\)-th Catalan number, \\(C_n\\), which counts:\n\nValid parentheses sequences of length \\(2n\\)\nBinary search trees with \\(n\\) nodes\nTriangulations of an \\((n+2)\\)-gon\nNon-crossing partitions, Dyck paths, etc.\n\nThe recursive formula is:\n\\[\nC_0 = 1\n\\]\n\\[\nC_n = \\sum_{i=0}^{n-1} C_i \\cdot C_{n-1-i}\n\\]\nAlternatively, the closed-form expression using binomial coefficients is:\n\\[\nC_n = \\frac{1}{n+1}\\binom{2n}{n}\n\\]\n\n\nHow Does It Work (Plain Language)\nThink of \\(C_n\\) as counting ways to split a structure into two balanced parts.\nExample (valid parentheses): Every sequence starts with ( and pairs with a matching ) that divides the sequence into two smaller valid parts. If the left part has \\(C_i\\) ways and the right part has \\(C_{n-1-i}\\) ways, the total is their product. Summing across all possible splits gives the full \\(C_n\\).\n\n\nExample\nLet’s compute the first few:\n\n\n\n\\(n\\)\n\\(C_n\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n2\n2\n\n\n3\n5\n\n\n4\n14\n\n\n5\n42\n\n\n\n\n\nTiny Code (Easy Versions)\nPython (DP approach)\ndef catalan(n):\n    C = [0] * (n + 1)\n    C[0] = 1\n    for i in range(1, n + 1):\n        for j in range(i):\n            C[i] += C[j] * C[i - 1 - j]\n    return C[n]\n\nfor i in range(6):\n    print(f\"C({i}) = {catalan(i)}\")\nPython (Binomial formula)\nfrom math import comb\n\ndef catalan_binom(n):\n    return comb(2 * n, n) // (n + 1)\n\nprint([catalan_binom(i) for i in range(6)])\nC Version (DP approach)\n#include &lt;stdio.h&gt;\n\nunsigned long catalan(int n) {\n    unsigned long C[n + 1];\n    C[0] = 1;\n    for (int i = 1; i &lt;= n; i++) {\n        C[i] = 0;\n        for (int j = 0; j &lt; i; j++)\n            C[i] += C[j] * C[i - 1 - j];\n    }\n    return C[n];\n}\n\nint main(void) {\n    for (int i = 0; i &lt;= 5; i++)\n        printf(\"C(%d) = %lu\\n\", i, catalan(i));\n}\n\n\nWhy It Matters\nCatalan numbers appear in many fundamental combinatorial and algorithmic contexts:\n\nCombinatorics: Dyck paths, lattice paths, stack-sortable permutations\nDynamic programming: counting tree structures\nParsing theory: number of valid parse trees\nGeometry: triangulations of convex polygons\n\nThey are a universal count for balanced recursive structures.\n\n\nA Gentle Proof (Why It Works)\nEach Catalan object splits into two smaller ones around a root:\n\\[\nC_n = \\sum_{i=0}^{n-1} C_i \\cdot C_{n-1-i}\n\\]\nThis recurrence mirrors binary tree formation, left subtree size \\(i\\), right subtree \\(n-1-i\\).\nThe closed form follows from the binomial identity:\n\\[\nC_n = \\frac{1}{n+1}\\binom{2n}{n}\n\\]\nwhich arises from solving the recurrence using generating functions.\n\n\nTry It Yourself\n\nCompute \\(C_3\\) manually using the recurrence.\nVerify \\(C_4 = 14\\).\nPrint all valid parentheses for \\(n=3\\) (should be 5).\nCompare DP and binomial implementations.\nUse Catalan DP to count binary search trees of size \\(n\\).\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nExpected \\(C_n\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n2\n2\n\n\n3\n5\n\n\n4\n14\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^2)\\) (DP), \\(O(n)\\) (binomial)\nSpace: \\(O(n)\\)\n\nCatalan numbers are the backbone of many recursive counting problems, capturing the essence of balance, structure, and symmetry.\n\n\n\n530 Stirling Numbers\nStirling numbers count ways to partition or arrange elements under specific constraints. They connect combinatorics, recurrence relations, and generating functions, bridging counting, algebra, and probability.\nThere are two families:\n\nStirling numbers of the first kind \\(c(n, k)\\), count permutations of \\(n\\) elements with exactly \\(k\\) cycles.\nStirling numbers of the second kind \\(S(n, k)\\), count ways to partition \\(n\\) elements into \\(k\\) non-empty unlabeled subsets.\n\n\nWhat Problem Are We Solving?\nWe want to compute either:\n\nFirst kind (signed or unsigned):\n\n\\[\nc(n,k) = c(n-1,k-1) + (n-1),c(n-1,k)\n\\]\n\nSecond kind:\n\n\\[\nS(n,k) = S(n-1,k-1) + k,S(n-1,k)\n\\]\nBase cases:\n\\[\nc(0,0)=1,\\quad S(0,0)=1\n\\]\n\\[\nc(n,0)=S(n,0)=0 \\text{ for } n&gt;0\n\\]\n\\[\nc(0,k)=S(0,k)=0 \\text{ for } k&gt;0\n\\]\n\n\nHow Does It Work (Plain Language)\n\nFirst kind: building permutations\n\nPlace element \\(n\\) alone as a new cycle (\\(c(n-1,k-1)\\) ways)\nOr insert \\(n\\) into one of \\((n-1)\\) existing cycles ($ (n-1),c(n-1,k)$ ways)\n\nSecond kind: building partitions\n\nPlace element \\(n\\) alone (new subset) → \\(S(n-1,k-1)\\)\nOr place \\(n\\) into one of \\(k\\) existing subsets → \\(k,S(n-1,k)\\)\n\n\nEach recurrence reflects a “new item added” logic, creating new groups or joining old ones.\n\n\nExample\nFor second kind \\(S(3,k)\\):\n\n\n\n\\(n\\)\n\\(k\\)\n\\(S(n,k)\\)\n\n\n\n\n3\n1\n1\n\n\n3\n2\n3\n\n\n3\n3\n1\n\n\n\nSo \\(S(3,2)=3\\) means 3 ways to split {1,2,3} into 2 non-empty sets:\n\n{1,2},{3}\n{1,3},{2}\n{2,3},{1}\n\n\n\nTiny Code (Easy Versions)\nPython (Stirling numbers of the second kind)\ndef stirling2(n, k):\n    if n == k == 0:\n        return 1\n    if n == 0 or k == 0:\n        return 0\n    dp = [[0]*(k+1) for _ in range(n+1)]\n    dp[0][0] = 1\n    for i in range(1, n+1):\n        for j in range(1, k+1):\n            dp[i][j] = dp[i-1][j-1] + j * dp[i-1][j]\n    return dp[n][k]\n\nfor k in range(1,4):\n    print(f\"S(3,{k}) =\", stirling2(3,k))\nPython (Stirling numbers of the first kind)\ndef stirling1(n, k):\n    if n == k == 0:\n        return 1\n    if n == 0 or k == 0:\n        return 0\n    dp = [[0]*(k+1) for _ in range(n+1)]\n    dp[0][0] = 1\n    for i in range(1, n+1):\n        for j in range(1, k+1):\n            dp[i][j] = dp[i-1][j-1] + (i-1) * dp[i-1][j]\n    return dp[n][k]\n\n\nWhy It Matters\nStirling numbers unify combinatorics, algebra, and analysis:\n\n\\(S(n,k)\\): count of set partitions\n\\(c(n,k)\\): count of permutation cycles\nAppear in:\n\nBell numbers: \\(B_n = \\sum_{k=0}^n S(n,k)\\)\nFactorial expansions\nMoments in probability and statistics\nPolynomial bases (falling/rising factorials)\n\n\nThey’re the coefficients when expressing factorials or powers in different bases: \\[\nx^n = \\sum_k S(n,k),(x)_k, \\quad (x)_n = \\sum_k c(n,k),x^k\n\\]\n\n\nA Gentle Proof (Why It Works)\nFor \\(S(n,k)\\), consider adding one element to a partition of size \\(n-1\\):\n\nCreate new subset (choose none → \\(S(n-1,k-1)\\))\nJoin existing subset (choose one of \\(k\\) → \\(kS(n-1,k)\\))\n\nAdd them → recurrence proven.\nFor \\(c(n,k)\\), each new element:\n\nStarts a new cycle (\\(c(n-1,k-1)\\))\nJoins one of existing cycles (\\((n-1)c(n-1,k)\\))\n\n\n\nTry It Yourself\n\nCompute \\(S(4,2)\\) manually.\nVerify \\(S(4,2)=7\\) and \\(S(4,3)=6\\).\nPrint full Stirling triangle.\nCompare \\(S(n,k)\\) vs binomial coefficients.\nDerive Bell numbers: \\(B_n=\\sum_k S(n,k)\\).\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(k\\)\n\\(S(n,k)\\)\n\\(c(n,k)\\)\n\n\n\n\n3\n1\n1\n2\n\n\n3\n2\n3\n3\n\n\n3\n3\n1\n1\n\n\n4\n2\n7\n11\n\n\n\n\n\nComplexity\n\nTime: \\(O(nk)\\)\nSpace: \\(O(nk)\\)\n\nStirling numbers form the grammar of combinatorial counting, expressing partitions, cycles, and transformations between polynomial worlds.\n\n\n\n\nSection 54. Probability and Randomized Algorithms\n\n531 Monte Carlo Simulation\nMonte Carlo simulation is a numerical method that uses random sampling to approximate solutions to deterministic or probabilistic problems. When exact formulas are hard or impossible to compute, Monte Carlo methods estimate results by simulating many random experiments and averaging the outcomes.\n\nWhat Problem Are We Solving?\nWe want to estimate a quantity (like an area, integral, or probability) by random sampling.\nThe core principle:\n\\[\n\\text{Expected Value} \\approx \\frac{1}{N}\\sum_{i=1}^{N} f(X_i)\n\\]\nwhere \\(X_i\\) are random samples and \\(f(X_i)\\) measures the contribution from each trial.\nAs \\(N \\to \\infty\\), the average converges to the true value by the Law of Large Numbers.\n\n\nExample: Estimating π\nImagine a unit square enclosing a quarter-circle of radius 1. The ratio of points falling inside the circle to total points approximates \\(\\pi/4\\).\n\\[\n\\frac{\\text{points inside}}{\\text{total points}} \\approx \\frac{\\pi}{4}\n\\]\nSo:\n\\[\n\\pi \\approx 4 \\times \\frac{\\text{inside}}{\\text{total}}\n\\]\n\n\nHow Does It Work (Plain Language)\n\nDefine a random experiment (e.g., sample \\((x,y)\\) in \\([0,1]\\times[0,1]\\))\nCheck if the sample satisfies a condition (e.g., \\(x^2 + y^2 \\le 1\\))\nRepeat many times\nUse the ratio (hits / total) to estimate the desired probability or value\n\nMore samples → lower error (variance \\(\\propto 1/\\sqrt{N}\\))\n\n\nTiny Code (Easy Versions)\nPython Version (Estimate π)\nimport random\n\ndef monte_carlo_pi(samples=1000000):\n    inside = 0\n    for _ in range(samples):\n        x, y = random.random(), random.random()\n        if x*x + y*y &lt;= 1:\n            inside += 1\n    return 4 * inside / samples\n\nprint(\"Estimated π =\", monte_carlo_pi())\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(void) {\n    int samples = 1000000, inside = 0;\n    for (int i = 0; i &lt; samples; i++) {\n        double x = (double)rand() / RAND_MAX;\n        double y = (double)rand() / RAND_MAX;\n        if (x*x + y*y &lt;= 1.0) inside++;\n    }\n    double pi = 4.0 * inside / samples;\n    printf(\"Estimated π = %f\\n\", pi);\n    return 0;\n}\n\n\nWhy It Matters\nMonte Carlo methods are essential for:\n\nIntegration in high dimensions\nProbabilistic modeling\nOptimization and simulation\nFinancial modeling (e.g., option pricing)\nPhysics and engineering (e.g., particle transport, statistical mechanics)\n\nThey trade accuracy for generality, thriving when deterministic methods fail.\n\n\nA Gentle Proof (Why It Works)\nLet \\(X_1, X_2, \\dots, X_N\\) be i.i.d. random variables representing outcomes. By the Law of Large Numbers:\n\\[\n\\frac{1}{N}\\sum_{i=1}^{N} X_i \\to \\mathbb{E}[X]\n\\]\nSo the sample mean converges to the expected value, giving a consistent estimator. The Central Limit Theorem ensures that error decreases as \\(1/\\sqrt{N}\\).\n\n\nTry It Yourself\n\nEstimate \\(\\pi\\) with different sample sizes.\nUse Monte Carlo to compute \\(\\int_0^1 x^2,dx\\).\nEstimate the probability that sum of two dice is ≥ 10.\nModel coin flips and compare to exact probabilities.\nMeasure convergence as \\(N\\) grows (error vs samples).\n\n\n\nTest Cases\n\n\n\nSamples\nEstimated π\nExpected Error\n\n\n\n\n1000\n~3.1\n±0.05\n\n\n10,000\n~3.14\n±0.02\n\n\n1,000,000\n~3.1415\n±0.001\n\n\n\n\n\nComplexity\n\nTime: \\(O(N)\\)\nSpace: \\(O(1)\\)\n\nMonte Carlo simulation is statistics as computation, randomness revealing structure through repetition and averages.\n\n\n\n532 Las Vegas Algorithm\nA Las Vegas algorithm always returns a correct answer, but its runtime is random, it may take longer or shorter depending on luck. Unlike Monte Carlo methods (which trade accuracy for speed), Las Vegas algorithms never compromise correctness, only time.\n\nWhat Problem Are We Solving?\nWe need randomized algorithms that remain guaranteed correct, but whose execution time depends on random events.\nIn other words:\n\nOutput: always correct\nTime: random variable\n\nWe want to design algorithms where randomness helps avoid worst-case behavior or simplify logic, without breaking correctness.\n\n\nHow Does It Work (Plain Language)\nA Las Vegas algorithm uses randomness to guide the search, pick pivots, or sample data, but verifies correctness before returning.\nIf the random choice is poor, it tries again.\nExample: Randomized QuickSort\nQuickSort chooses a random pivot, splits data around it, and recursively sorts. Random pivot selection ensures expected \\(O(n \\log n)\\) time, even though worst-case \\(O(n^2)\\) still exists.\nBut the output is always sorted, correctness never depends on chance.\nAnother example: Randomized QuickSelect\nFinds the \\(k\\)-th smallest element using a random pivot. If pivot is bad, runtime worsens, but result is still correct.\n\n\nTiny Code (Easy Versions)\nPython (Randomized QuickSort)\nimport random\n\ndef quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = random.choice(arr)\n    left = [x for x in arr if x &lt; pivot]\n    mid = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + mid + quicksort(right)\n\nprint(quicksort([3, 1, 4, 1, 5, 9, 2, 6]))\nPython (QuickSelect)\nimport random\n\ndef quickselect(arr, k):\n    pivot = random.choice(arr)\n    left = [x for x in arr if x &lt; pivot]\n    mid = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    if k &lt; len(left):\n        return quickselect(left, k)\n    elif k &lt; len(left) + len(mid):\n        return pivot\n    else:\n        return quickselect(right, k - len(left) - len(mid))\n\nprint(quickselect([7, 10, 4, 3, 20, 15], 2))  # 3rd smallest element\n\n\nWhy It Matters\nLas Vegas algorithms combine certainty of correctness with expected efficiency:\n\nSorting and selection (QuickSort, QuickSelect)\nComputational geometry (Randomized incremental algorithms)\nGraph algorithms (Randomized MSTs, planar separations)\nData structures (Skip lists, hash tables)\n\nThey often simplify design and avoid adversarial inputs.\n\n\nA Gentle Proof (Why It Works)\nLet \\(T\\) be runtime random variable. Expected time:\n\\[\n\\mathbb{E}[T] = \\sum_{t} t \\cdot P(T = t)\n\\]\nThe algorithm terminates when a “good” random event occurs (e.g. balanced pivot). By bounding expected cost at each step, we can show \\(\\mathbb{E}[T] = O(f(n))\\).\nCorrectness is ensured by deterministic verification after random choices.\n\n\nTry It Yourself\n\nCompare QuickSort with and without random pivot.\nMeasure runtime across many runs, observe variation.\nCount recursive calls distribution for small arrays.\nUse randomization in skip list insertion.\nWrite a retry-based algorithm (e.g., random hash probing).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nInput\nAlgorithm\nOutput\nCorrect?\nRuntime\n\n\n\n\n[3,1,4,1,5,9]\nQuickSort\n[1,1,3,4,5,9]\nYes\nvaries\n\n\n[7,10,4,3,20,15], k=2\nQuickSelect\n7\nYes\nvaries\n\n\n\n\n\nComplexity\n\nExpected Time: \\(O(f(n))\\) (often \\(O(n)\\) or \\(O(n\\log n)\\))\nWorst Time: still possible, but rare\nSpace: depends on recursion or retries\n\nLas Vegas algorithms gamble with time, never with truth, they always find the right answer, only the path differs each run.\n\n\n\n533 Reservoir Sampling\nReservoir sampling is a clever algorithm for selecting a uniform random sample from a data stream of unknown or very large size, using only O(k) space. It’s a cornerstone of streaming algorithms, where you cannot store all the data but still want fair random selection.\n\nWhat Problem Are We Solving?\nGiven a stream of \\(n\\) items (possibly very large or unbounded), select \\(k\\) items uniformly at random, that is, each item has equal probability \\(\\frac{k}{n}\\) of being chosen, without knowing \\(n\\) in advance.\nWe want to process items one by one, keeping only a reservoir of \\(k\\) samples.\n\n\nHow Does It Work (Plain Language)\nThe key idea is incremental fairness:\n\nInitialize: fill the reservoir with the first \\(k\\) elements.\nProcess stream: for each item \\(i\\) (1-indexed) after the \\(k\\)-th,\n\ngenerate a random integer \\(j \\in [1, i]\\)\nif \\(j \\le k\\), replace reservoir[\\(j\\)] with item \\(i\\)\n\n\nThis ensures each element’s chance = \\(\\frac{k}{i}\\) at the \\(i\\)-th step, and finally \\(\\frac{k}{n}\\) overall.\n\n\nExample (k = 2)\nStream: [A, B, C, D]\n\nReservoir = [A, B]\n\\(i=3\\) → random \\(j\\in[1,3]\\) → say \\(j=2\\) → replace B → [A, C]\n\\(i=4\\) → \\(j\\in[1,4]\\) → say \\(j=4\\) → skip (since &gt;2) → [A, C]\n\nAfter full stream, every pair has equal probability.\n\n\nProof of Uniformity\nAt step \\(i\\):\n\nProbability of being selected: \\(\\frac{k}{i}\\)\nProbability of surviving future replacements: \\[\n\\prod_{t=i+1}^{n}\\left(1 - \\frac{1}{t}\\right)=\\frac{i}{n}\n\\] Thus final probability = \\(\\frac{k}{i} \\cdot \\frac{i}{n} = \\frac{k}{n}\\), equal for all.\n\n\n\nTiny Code (Easy Versions)\nPython Version (k = 1)\nimport random\n\ndef reservoir_sample(stream):\n    result = None\n    for i, item in enumerate(stream, start=1):\n        if random.randint(1, i) == 1:\n            result = item\n    return result\n\ndata = [10, 20, 30, 40, 50]\nprint(\"Random pick:\", reservoir_sample(data))\nPython Version (k = 3)\nimport random\n\ndef reservoir_sample_k(stream, k):\n    reservoir = []\n    for i, item in enumerate(stream, start=1):\n        if i &lt;= k:\n            reservoir.append(item)\n        else:\n            j = random.randint(1, i)\n            if j &lt;= k:\n                reservoir[j - 1] = item\n    return reservoir\n\ndata = range(1, 11)\nprint(reservoir_sample_k(data, 3))\nC Version (k = 1)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(void) {\n    int stream[] = {10, 20, 30, 40, 50};\n    int n = 5;\n    int result = stream[0];\n    for (int i = 1; i &lt; n; i++) {\n        int j = rand() % (i + 1);\n        if (j == 0)\n            result = stream[i];\n    }\n    printf(\"Random pick: %d\\n\", result);\n}\n\n\nWhy It Matters\nReservoir sampling is crucial for:\n\nBig data: when \\(n\\) is too large for memory\nStreaming APIs, logs, telemetry\nRandom sampling in databases and distributed systems\nMachine learning: random mini-batches, unbiased selection\n\nIt gives exact uniform probability without prior knowledge of \\(n\\).\n\n\nA Gentle Proof (Why It Works)\nFor any element \\(x_i\\):\n\nChance selected when seen = \\(\\frac{k}{i}\\)\nChance not replaced later = \\(\\frac{i}{i+1}\\cdot \\frac{i+1}{i+2}\\cdots\\frac{n-1}{n}=\\frac{i}{n}\\)\nCombined: \\(\\frac{k}{i} \\cdot \\frac{i}{n} = \\frac{k}{n}\\) Uniform for all \\(i\\).\n\n\n\nTry It Yourself\n\nRun with increasing \\(n\\) and track frequencies.\nVerify approximate uniformity over 10,000 trials.\nTest with \\(k&gt;1\\).\nApply to data streams from a file or API.\nModify for weighted sampling.\n\n\n\nTest Cases\n\n\n\nStream\nk\nPossible Reservoir\nProbability\n\n\n\n\n[A, B, C]\n1\nA, B, or C\n1/3 each\n\n\n[1, 2, 3, 4]\n2\nAny 2-combo\nEqual\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) (one pass)\nSpace: \\(O(k)\\)\n\nReservoir sampling is randomness under constraint, fair selection from endless flow, one element at a time.\n\n\n\n534 Randomized QuickSort\nRandomized QuickSort is the classic divide-and-conquer sorting algorithm, with a twist of randomness. Instead of choosing a fixed pivot (like the first or last element), it picks a random pivot, ensuring that the expected runtime stays \\(O(n \\log n)\\), regardless of input order.\nThis simple randomization elegantly neutralizes worst-case scenarios.\n\nWhat Problem Are We Solving?\nWe need a fast, in-place sorting algorithm that avoids pathological inputs. Standard QuickSort can degrade to \\(O(n^2)\\) on sorted data if the pivot is chosen poorly. By picking pivots uniformly at random, we guarantee expected balance and expected \\(O(n \\log n)\\) behavior.\n\n\nHow Does It Work (Plain Language)\nQuickSort partitions the array around a pivot — elements smaller go left, greater go right — then recursively sorts the two sides.\nRandomization ensures the pivot is, on average, near the median, keeping the recursion tree balanced.\nSteps:\n\nChoose a random pivot index p.\nPartition array so that:\n\nLeft: elements &lt; pivot\nRight: elements &gt; pivot\n\nRecursively sort both partitions.\n\nWhen randomization is fair, each pivot splits roughly evenly, giving height \\(\\approx \\log n\\) and total work \\(O(n \\log n)\\).\n\n\nExample\nSort [3, 6, 2, 1, 4]\n\nPick random pivot → say 3\nPartition → [2,1] [3] [6,4]\nRecurse on [2,1] and [6,4]\nContinue until sorted: [1,2,3,4,6]\n\nDifferent random seeds → different recursion paths, same final result.\n\n\nTiny Code (Easy Versions)\nPython Version\nimport random\n\ndef quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = random.choice(arr)\n    left = [x for x in arr if x &lt; pivot]\n    mid = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + mid + quicksort(right)\n\narr = [3, 6, 2, 1, 4]\nprint(quicksort(arr))\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid swap(int *a, int *b) {\n    int t = *a; *a = *b; *b = t;\n}\n\nint partition(int a[], int low, int high) {\n    int pivot_idx = low + rand() % (high - low + 1);\n    swap(&a[pivot_idx], &a[high]);\n    int pivot = a[high];\n    int i = low - 1;\n    for (int j = low; j &lt; high; j++) {\n        if (a[j] &lt; pivot) swap(&a[++i], &a[j]);\n    }\n    swap(&a[i + 1], &a[high]);\n    return i + 1;\n}\n\nvoid quicksort(int a[], int low, int high) {\n    if (low &lt; high) {\n        int pi = partition(a, low, high);\n        quicksort(a, low, pi - 1);\n        quicksort(a, pi + 1, high);\n    }\n}\n\nint main(void) {\n    int a[] = {3, 6, 2, 1, 4};\n    int n = 5;\n    quicksort(a, 0, n - 1);\n    for (int i = 0; i &lt; n; i++) printf(\"%d \", a[i]);\n}\n\n\nWhy It Matters\nRandomized QuickSort is:\n\nFast in practice (low constants, cache-friendly)\nIn-place (no extra arrays)\nExpected \\(O(n \\log n)\\) independent of input\nImmune to adversarial inputs and pre-sorted traps\n\nUsed widely in:\n\nStandard libraries (e.g. Python’s sort() uses hybrid with randomized pivot)\nDatabase systems and data processing pipelines\nTeaching divide-and-conquer and randomized algorithms\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(T(n)\\) be expected cost. Each partition step costs \\(O(n)\\) comparisons. Pivot splits array into sizes \\(i\\) and \\(n-i-1\\) with equal probability for each \\(i\\).\n\\[\n\\mathbb{E}[T(n)] = \\frac{1}{n}\\sum_{i=0}^{n-1} (\\mathbb{E}[T(i)] + \\mathbb{E}[T(n-i-1)]) + O(n)\n\\]\nSolving gives \\(\\mathbb{E}[T(n)] = O(n \\log n)\\).\nVariance decreases as random choices average out.\n\n\nTry It Yourself\n\nRun algorithm multiple times, note pivot sequence.\nSort already sorted list, no slowdown expected.\nCompare runtime vs MergeSort.\nVisualize recursion tree depth.\nImplement a 3-way partition version for duplicates.\n\n\n\nTest Cases\n\n\n\nInput\nOutput\nNotes\n\n\n\n\n[3,6,2,1,4]\n[1,2,3,4,6]\nBasic case\n\n\n[1,2,3,4,5]\n[1,2,3,4,5]\nPre-sorted\n\n\n[5,4,3,2,1]\n[1,2,3,4,5]\nReverse\n\n\n[2,2,2,2]\n[2,2,2,2]\nDuplicates\n\n\n\n\n\nComplexity\n\nExpected Time: \\(O(n \\log n)\\)\nWorst Case: \\(O(n^2)\\) (rare)\nSpace: \\(O(\\log n)\\) recursion stack\n\nRandomized QuickSort transforms luck into balance, each shuffle a safeguard, each pivot a chance for harmony.\n\n\n\n535 Randomized QuickSelect\nRandomized QuickSelect is a divide-and-conquer algorithm for finding the k-th smallest element in an unsorted array in expected linear time. It’s the selection twin of QuickSort, using the same partition idea, but exploring only one side of the array at each step.\n\nWhat Problem Are We Solving?\nGiven an unsorted array arr and a rank k (1-indexed), we want the element that would appear at position k if the array were sorted — without fully sorting it.\nExample: In [7, 10, 4, 3, 20, 15], the 3rd smallest element is 7.\nWe want to find it in expected O(n), faster than sorting (\\(O(n \\log n)\\)).\n\n\nHow Does It Work (Plain Language)\n\nChoose a random pivot from the array.\nPartition elements into three groups:\n\nLeft: smaller than pivot\nMiddle: equal to pivot\nRight: greater than pivot\n\nCompare k to the sizes:\n\nIf k ≤ len(left): recurse on left\nElse if k ≤ len(left) + len(mid): pivot is the answer\nElse: recurse on right with adjusted rank\n\n\nBy following only the side containing the k-th element, we cut the problem roughly in half each time.\n\n\nExample\nFind the 3rd smallest in [7, 10, 4, 3, 20, 15]\n\nRandom pivot = 10\n\nLeft = [7, 4, 3], Mid = [10], Right = [20, 15]\n\nlen(left) = 3 Since k=3 ≤ 3, recurse into [7, 4, 3]\nRandom pivot = 4\n\nLeft = [3], Mid = [4], Right = [7]\n\nlen(left)=1, len(mid)=1 k=3 &gt; 1+1 → recurse into [7] with k=1 → Answer = 7\n\n\n\nTiny Code (Easy Versions)\nPython Version\nimport random\n\ndef quickselect(arr, k):\n    pivot = random.choice(arr)\n    left = [x for x in arr if x &lt; pivot]\n    mid = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    \n    if k &lt;= len(left):\n        return quickselect(left, k)\n    elif k &lt;= len(left) + len(mid):\n        return pivot\n    else:\n        return quickselect(right, k - len(left) - len(mid))\n\ndata = [7, 10, 4, 3, 20, 15]\nprint(quickselect(data, 3))  # 3rd smallest → 7\nC Version (In-Place Partition)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid swap(int *a, int *b) { int t = *a; *a = *b; *b = t; }\n\nint partition(int a[], int l, int r, int pivot_idx) {\n    int pivot = a[pivot_idx];\n    swap(&a[pivot_idx], &a[r]);\n    int store = l;\n    for (int i = l; i &lt; r; i++)\n        if (a[i] &lt; pivot) swap(&a[i], &a[store++]);\n    swap(&a[store], &a[r]);\n    return store;\n}\n\nint quickselect(int a[], int l, int r, int k) {\n    if (l == r) return a[l];\n    int pivot_idx = l + rand() % (r - l + 1);\n    int idx = partition(a, l, r, pivot_idx);\n    int rank = idx - l + 1;\n    if (k == rank) return a[idx];\n    if (k &lt; rank) return quickselect(a, l, idx - 1, k);\n    return quickselect(a, idx + 1, r, k - rank);\n}\n\nint main(void) {\n    int arr[] = {7, 10, 4, 3, 20, 15};\n    int n = 6, k = 3;\n    printf(\"%d\\n\", quickselect(arr, 0, n - 1, k));\n}\n\n\nWhy It Matters\n\nExpected O(n) time, O(1) space\nFoundation for median selection and order statistics\nUsed in:\n\nMedian-of-medians\nRandomized algorithms\nQuickSort optimization\nData summarization and sampling\n\n\nIn practical terms, QuickSelect is how many libraries (like numpy.partition) find medians and percentiles efficiently.\n\n\nA Gentle Proof (Why It Works)\nEach step partitions the array in \\(O(n)\\) and recurses on one side. Expected split ratio ≈ 1:2 → recurrence:\n\\[\nT(n) = T\\left(\\frac{n}{2}\\right) + O(n)\n\\]\nSolving gives \\(T(n) = O(n)\\). Worst case (\\(O(n^2)\\)) happens only if pivot is repeatedly extreme, probability exponentially small.\n\n\nTry It Yourself\n\nFind the median (k = n//2) of a large random array.\nCompare QuickSelect vs sorting time.\nRun multiple trials; note variation in recursion depth.\nModify to find k-th largest.\nImplement deterministic pivot (median-of-medians).\n\n\n\nTest Cases\n\n\n\nInput\nk\nOutput\nNote\n\n\n\n\n[7,10,4,3,20,15]\n3\n7\n3rd smallest\n\n\n[5,4,3,2,1]\n1\n1\nsmallest\n\n\n[2,2,2,2]\n2\n2\nduplicates\n\n\n[10]\n1\n10\nsingle element\n\n\n\n\n\nComplexity\n\nExpected Time: \\(O(n)\\)\nWorst Case: \\(O(n^2)\\) (rare)\nSpace: \\(O(1)\\) (in-place)\n\nRandomized QuickSelect is precision through chance, a direct, elegant route to the k-th element, powered by probability.\n\n\n\n536 Birthday Paradox Simulation\nThe Birthday Paradox is a famous probability puzzle showing how quickly collisions occur in random samples. Surprisingly, with just 23 people, there’s a &gt;50% chance two share the same birthday, even though there are 365 possible days.\nA simulation helps reveal why intuition often fails.\n\nWhat Problem Are We Solving?\nWe want to estimate the probability that at least two people share the same birthday in a group of size \\(n\\).\nThere are two approaches:\n\nAnalytical formula (exact)\nMonte Carlo simulation (empirical)\n\n\n\nAnalytical Probability\nLet’s compute \\(P(\\text{no collision})\\) first:\n\n1st person: 365 choices\n2nd person: 364\n3rd person: 363\n…\n\\(n\\)-th person: \\((365 - n + 1)\\) choices\n\nSo:\n\\[\nP(\\text{no match}) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\cdots \\times \\frac{365-n+1}{365}\n\\]\nThen:\n\\[\nP(\\text{collision}) = 1 - P(\\text{no match})\n\\]\nFor \\(n=23\\), \\(P(\\text{collision}) \\approx 0.507\\)\n\n\nHow Does It Work (Plain Language)\nEach new person you add increases the chance of a match, not with everyone, but with any of the previous people. The number of pairs grows fast:\n\\[\n\\text{\\# pairs} = \\binom{n}{2}\n\\]\nThat quadratic growth in comparisons explains why collisions happen quickly.\nA Monte Carlo simulation simply runs the experiment many times and counts how often duplicates appear.\n\n\nExample\nTry \\(n = 23\\) people, 10,000 trials:\n\n~50% of trials have at least one shared birthday\n~50% do not\n\nThe randomness converges to the analytical result.\n\n\nTiny Code (Easy Versions)\nPython Version\nimport random\n\ndef birthday_collision_prob(n, trials=10000):\n    count = 0\n    for _ in range(trials):\n        birthdays = [random.randint(1, 365) for _ in range(n)]\n        if len(birthdays) != len(set(birthdays)):\n            count += 1\n    return count / trials\n\nfor n in [10, 20, 23, 30, 40]:\n    print(f\"n={n}, P(collision)≈{birthday_collision_prob(n):.3f}\")\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;time.h&gt;\n\nbool has_collision(int n) {\n    bool seen[366] = {false};\n    for (int i = 0; i &lt; n; i++) {\n        int b = rand() % 365 + 1;\n        if (seen[b]) return true;\n        seen[b] = true;\n    }\n    return false;\n}\n\ndouble birthday_collision_prob(int n, int trials) {\n    int count = 0;\n    for (int i = 0; i &lt; trials; i++)\n        if (has_collision(n)) count++;\n    return (double)count / trials;\n}\n\nint main(void) {\n    srand(time(NULL));\n    int ns[] = {10, 20, 23, 30, 40};\n    for (int i = 0; i &lt; 5; i++) {\n        int n = ns[i];\n        printf(\"n=%d, P≈%.3f\\n\", n, birthday_collision_prob(n, 10000));\n    }\n}\n\n\nWhy It Matters\nThe Birthday Paradox illustrates collision probability, crucial for:\n\nHash functions (collision analysis)\nCryptography (birthday attacks)\nRandom ID generation (UUIDs, fingerprints)\nSimulation and probability reasoning\n\nIt shows that intuition about randomness often underestimates collisions.\n\n\nA Gentle Proof (Why It Works)\nFor small \\(n\\):\n\\[\nP(\\text{no match}) = \\prod_{i=0}^{n-1} \\frac{365 - i}{365}\n\\]\nExpand for \\(n=23\\):\n\\[\nP(\\text{no match}) \\approx 0.493 \\\nP(\\text{match}) = 1 - 0.493 = 0.507\n\\]\nThus, with only 23 people, more likely than not that two share a birthday.\n\n\nTry It Yourself\n\nPlot \\(P(\\text{collision})\\) vs \\(n\\).\nFind smallest \\(n\\) where probability &gt; 0.9.\nTry different “year lengths” (e.g. 500 or 1000).\nTest for non-uniform birthdays.\nSimulate for hash buckets (\\(m=2^{16}\\), \\(n=500\\)).\n\n\n\nTest Cases\n\n\n\nn\nExpected P(collision)\nSimulation (approx)\n\n\n\n\n10\n0.117\n0.12\n\n\n20\n0.411\n0.41\n\n\n23\n0.507\n0.50\n\n\n30\n0.706\n0.71\n\n\n40\n0.891\n0.89\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\cdot \\text{trials})\\)\nSpace: \\(O(n)\\)\n\nThe Birthday Paradox is a collision lens, a window into how randomness piles up faster than intuition expects.\n\n\n\n537 Random Hashing\nRandom hashing uses randomness to minimize collisions and distribute keys uniformly across buckets. It’s a core idea in hash tables, Bloom filters, and probabilistic data structures, where fairness and independence matter more than determinism.\n\nWhat Problem Are We Solving?\nWe need to map arbitrary keys to buckets (like slots in a hash table) in a way that spreads them evenly, even when input keys follow non-uniform or adversarial patterns.\nDeterministic hash functions may cluster keys or leak structure. Adding randomness to the hash function ensures each key behaves like a random variable, so collisions become rare and predictable only in expectation.\n\n\nHow Does It Work (Plain Language)\nA random hash function is drawn from a family of functions \\(H = {h_1, h_2, \\ldots, h_m}\\) before use, we fix one randomly at runtime.\nFormally, a hash family is universal if for all \\(x \\ne y\\):\n\\[\nP(h(x) = h(y)) \\le \\frac{1}{M}\n\\]\nwhere \\(M\\) is the number of buckets. This guarantees expected \\(O(1)\\) lookups.\nKey idea: each new program run uses a different random hash seed, so attackers or pathological datasets can’t force collisions.\n\n\nExample (Universal Hashing)\nLet \\(U = {0, 1, \\dots, p-1}\\) with prime \\(p\\). For parameters \\(a, b\\) chosen uniformly from \\(1, \\dots, p-1\\), define:\n\\[\nh_{a,b}(x) = ((a \\cdot x + b) \\bmod p) \\bmod M\n\\]\nThis gives a universal hash family, low collision probability and simple computation.\n\n\nExample in Action\nSay \\(p = 17\\), \\(M = 10\\), \\(a = 5\\), \\(b = 3\\):\n\n\n\nx\nh(x) = (5x+3) mod 17 mod 10\n\n\n\n\n1\n8\n\n\n2\n3\n\n\n3\n8\n\n\n4\n3\n\n\n\nEach key gets a pseudo-random bucket. Collisions still possible, but not predictable, controlled by probability, not structure.\n\n\nTiny Code (Easy Versions)\nPython Version\nimport random\n\ndef random_hash(a, b, p, M, x):\n    return ((a * x + b) % p) % M\n\ndef make_hash(p, M):\n    a = random.randint(1, p - 1)\n    b = random.randint(0, p - 1)\n    return lambda x: ((a * x + b) % p) % M\n\n# Example\np, M = 17, 10\nh = make_hash(p, M)\ndata = [1, 2, 3, 4, 5]\nprint([h(x) for x in data])\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint random_hash(int a, int b, int p, int M, int x) {\n    return ((a * x + b) % p) % M;\n}\n\nint main(void) {\n    int p = 17, M = 10;\n    int a = rand() % (p - 1) + 1;\n    int b = rand() % p;\n    int data[] = {1, 2, 3, 4, 5};\n    int n = 5;\n    for (int i = 0; i &lt; n; i++) {\n        printf(\"x=%d -&gt; h=%d\\n\", data[i], random_hash(a, b, p, M, data[i]));\n    }\n}\n\n\nWhy It Matters\n\nFairness: keys are spread evenly, lowering collision risk.\nSecurity: prevents adversarial key selection (important in hash-flooding attacks).\nPerformance: ensures expected \\(O(1)\\) lookup even in worst input cases.\nApplications:\n\nHash tables\nCuckoo hashing\nBloom filters\nConsistent hashing\nCryptographic schemes (non-cryptographic randomness)\n\n\nLanguages like Python, Java, and Go randomize hash seeds to defend against input-based attacks.\n\n\nA Gentle Proof (Why It Works)\nFor a universal hash family \\(H\\) over \\(U\\) with \\(|H| = m\\):\n\\[\nP(h(x) = h(y)) = \\frac{1}{M}, \\quad x \\ne y\n\\]\nThus, the expected number of collisions for \\(n\\) keys is:\n\\[\nE[\\text{collisions}] = \\binom{n}{2} \\cdot \\frac{1}{M}\n\\]\nIf \\(M \\approx n\\), expected collisions ≈ constant.\nHence, average lookup and insertion time = \\(O(1)\\).\n\n\nTry It Yourself\n\nCompare random hash vs naive \\((x \\bmod M)\\).\nPlot bucket frequencies over random runs.\nTest collision count for random seeds.\nImplement 2-choice hashing (\\(h_1\\), \\(h_2\\) choose less loaded).\nBuild small universal hash table.\n\n\n\nTest Cases\n\n\n\nKeys\nM\nHash Type\nExpected Collisions\n\n\n\n\n0–99\n10\nnaive \\(x \\bmod M\\)\nclustered\n\n\n0–99\n10\nrandom \\(ax+b\\)\nbalanced\n\n\nadversarial keys\n10\nrandom seed\nunpredictable\n\n\n\n\n\nComplexity\n\nTime: \\(O(1)\\) per hash\nSpace: \\(O(1)\\) for parameters\n\nRandom hashing is structured unpredictability, deterministic in code, but probabilistically fair in behavior.\n\n\n\n538 Random Walk Simulation\nA random walk is a path defined by a sequence of random steps. It models countless natural and computational processes, from molecules drifting in liquid to stock prices fluctuating, and from diffusion in physics to exploration in AI.\nBy simulating random walks, we capture how randomness unfolds over time.\n\nWhat Problem Are We Solving?\nWe want to study how a process evolves when each next step depends on random choice, not deterministic rules.\nRandom walks appear in:\n\nPhysics: Brownian motion\nFinance: stock movement models\nGraph theory: Markov chains, PageRank\nAlgorithms: randomized search and sampling\n\nA simulation lets us see expected displacement, return probability, and spatial spread.\n\n\nHow Does It Work (Plain Language)\nA random walk starts at an origin (e.g., \\((0,0)\\)). At each step, choose a direction randomly and move one unit.\nExamples:\n\n1D walk: step +1 or −1\n2D walk: move north, south, east, or west\n3D walk: move along x, y, or z axes randomly\n\nRepeat for \\(n\\) steps, track the position, and analyze results.\n\n\nExample (1D)\nStart at \\(x=0\\). For each step:\n\nWith probability \\(1/2\\): \\(x \\gets x+1\\)\nWith probability \\(1/2\\): \\(x \\gets x-1\\)\n\nAfter \\(n\\) steps, position is random variable \\(X_n\\). Expected value: \\(E[X_n] = 0\\) Variance: \\(Var[X_n] = n\\) Expected distance from origin ≈ \\(\\sqrt{n}\\)\n\n\nExample (2D)\nStart at \\((0,0)\\) Each step: randomly move up, down, left, or right. Plotting the path produces a meandering trajectory, a picture of diffusion.\n\n\nTiny Code (Easy Versions)\nPython (1D Random Walk)\nimport random\n\ndef random_walk_1d(steps):\n    x = 0\n    path = [x]\n    for _ in range(steps):\n        x += random.choice([-1, 1])\n        path.append(x)\n    return path\n\nwalk = random_walk_1d(100)\nprint(\"Final position:\", walk[-1])\nPython (2D Random Walk)\nimport random\n\ndef random_walk_2d(steps):\n    x, y = 0, 0\n    path = [(x, y)]\n    for _ in range(steps):\n        dx, dy = random.choice([(1,0), (-1,0), (0,1), (0,-1)])\n        x, y = x + dx, y + dy\n        path.append((x, y))\n    return path\n\nwalk = random_walk_2d(50)\nprint(\"Final position:\", walk[-1])\nC Version (1D)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint main(void) {\n    srand(time(NULL));\n    int steps = 100;\n    int x = 0;\n    for (int i = 0; i &lt; steps; i++) {\n        x += (rand() % 2) ? 1 : -1;\n    }\n    printf(\"Final position: %d\\n\", x);\n}\n\n\nWhy It Matters\nRandom walks form the mathematical backbone of many systems:\n\nPhysics: diffusion, Brownian motion\nFinance: random price fluctuations\nAI & RL: exploration strategies\nGraph algorithms: PageRank, cover time\nStatistics: Monte Carlo methods\n\nThey show how order emerges from randomness and why many natural processes diffuse over time.\n\n\nA Gentle Proof (Why It Works)\nEach step \\(S_i\\) is independent with \\(E[S_i]=0\\), \\(Var[S_i]=1\\). After \\(n\\) steps:\n\\[\nX_n = \\sum_{i=1}^n S_i, \\quad E[X_n]=0, \\quad Var[X_n]=n\n\\]\nExpected squared distance: \\(E[X_n^2] = n\\) Expected absolute displacement: \\(\\sqrt{n}\\) This scaling explains diffusion’s \\(\\sqrt{t}\\) behavior.\nIn 2D or 3D, similar logic extends: \\[\nE[|X_n|^2] = n \\cdot \\text{step size}^2\n\\]\n\n\nTry It Yourself\n\nPlot the path for \\(n=1000\\) in 1D and 2D.\nCompare multiple runs, note variability.\nTrack average distance from origin over trials.\nChange step probabilities (biased walk).\nAdd absorbing barriers (e.g. stop at \\(x=10\\)).\n\n\n\nTest Cases\n\n\n\nSteps\nDimension\nExpected \\(E[X_n]\\)\nExpected Distance\n\n\n\n\n10\n1D\n0\n~3.16\n\n\n100\n1D\n0\n~10\n\n\n100\n2D\n(0,0)\n~10\n\n\n1000\n1D\n0\n~31.6\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\) if storing path; \\(O(1)\\) if tracking position only\n\nA random walk is motion without plan, step by step, direction by chance, pattern emerging only in the long run.\n\n\n\n539 Coupon Collector Estimation\nThe Coupon Collector Problem asks: How many random draws (with replacement) are needed to collect all distinct items from a set of size \\(n\\)?\nIt’s a cornerstone of probabilistic analysis, used to model everything from collecting trading cards to testing coverage in randomized algorithms.\n\nWhat Problem Are We Solving?\nSuppose there are \\(n\\) different coupons (or Pokémon, or test cases). Each time, you draw one uniformly at random.\nQuestion: How many draws \\(T\\) do you need on average to collect all \\(n\\)?\n\n\nThe Big Idea\nEach new draw has a smaller chance of revealing something new. At the beginning, new coupons are easy to find; toward the end, you’re mostly drawing duplicates.\nThe expected number of trials is:\n\\[\nE[T] = n \\cdot H_n = n \\left(1 + \\frac{1}{2} + \\frac{1}{3} + \\cdots + \\frac{1}{n}\\right)\n\\]\nAsymptotically:\n\\[\nE[T] \\approx n \\ln n + \\gamma n + \\frac{1}{2}\n\\]\nwhere \\(\\gamma \\approx 0.57721\\) is the Euler–Mascheroni constant.\n\n\nHow Does It Work (Plain Language)\nYou can think of the collection process as phases:\n\nPhase 1: get the 1st new coupon → expected 1 draw\nPhase 2: get the 2nd new coupon → expected \\(\\frac{n}{n-1}\\) draws\nPhase 3: get the 3rd new coupon → expected \\(\\frac{n}{n-2}\\) draws\n…\nPhase \\(n\\): last coupon → expected \\(\\frac{n}{1}\\) draws\n\nSumming them up gives \\(E[T] = n H_n\\).\n\n\nExample\nFor \\(n = 5\\):\n\\[\nE[T] = 5 \\cdot (1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5}) = 5 \\times 2.283 = 11.415\n\\]\nSo you’ll need about 11–12 draws on average to get all 5.\n\n\nTiny Code (Easy Versions)\nPython (Simulation)\nimport random\n\ndef coupon_collector(n, trials=10000):\n    total = 0\n    for _ in range(trials):\n        collected = set()\n        count = 0\n        while len(collected) &lt; n:\n            coupon = random.randint(1, n)\n            collected.add(coupon)\n            count += 1\n        total += count\n    return total / trials\n\nfor n in [5, 10, 20]:\n    print(f\"n={n}, Expected≈{coupon_collector(n):.2f}, Theory≈{n * sum(1/i for i in range(1, n+1)):.2f}\")\nC (Simple Simulation)\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;time.h&gt;\n\ndouble coupon_collector(int n, int trials) {\n    int total = 0;\n    bool seen[n+1];\n    for (int t = 0; t &lt; trials; t++) {\n        for (int i = 1; i &lt;= n; i++) seen[i] = false;\n        int count = 0, collected = 0;\n        while (collected &lt; n) {\n            int c = rand() % n + 1;\n            count++;\n            if (!seen[c]) { seen[c] = true; collected++; }\n        }\n        total += count;\n    }\n    return (double)total / trials;\n}\n\nint main(void) {\n    srand(time(NULL));\n    int ns[] = {5, 10, 20};\n    for (int i = 0; i &lt; 3; i++) {\n        int n = ns[i];\n        printf(\"n=%d, E≈%.2f\\n\", n, coupon_collector(n, 10000));\n    }\n}\n\n\nWhy It Matters\nThe Coupon Collector model shows up everywhere:\n\nAlgorithm coverage (e.g., hashing all buckets)\nTesting & sampling (ensuring all cases appear)\nNetworking (packet collection)\nDistributed systems (gossip protocols)\nCollectible games (expected cost to finish a set)\n\nIt captures the diminishing returns of randomness, the last few items always take longest.\n\n\nA Gentle Proof (Why It Works)\nLet \\(T_i\\) = draws to get a new coupon when you already have \\(i-1\\).\nProbability of success = \\(\\frac{n - i + 1}{n}\\)\nSo expected draws for phase \\(i\\) = \\(\\frac{1}{p_i} = \\frac{n}{n - i + 1}\\)\nSum over all phases:\n\\[\nE[T] = \\sum_{i=1}^n \\frac{n}{n - i + 1} = n \\sum_{k=1}^n \\frac{1}{k} = n H_n\n\\]\n\n\nTry It Yourself\n\nSimulate for different \\(n\\) values; compare with theory.\nPlot \\(E[T]/n\\), should grow like \\(\\ln n\\).\nModify for biased probabilities (non-uniform draws).\nEstimate probability all coupons collected by step \\(t\\).\nApply to randomized load balancing (balls into bins).\n\n\n\nTest Cases\n\n\n\nn\nTheoretical \\(E[T]\\)\nSimulation (approx)\n\n\n\n\n5\n11.42\n11.40\n\n\n10\n29.29\n29.20\n\n\n20\n71.94\n72.10\n\n\n50\n224.96\n225.10\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\cdot \\text{trials})\\)\nSpace: \\(O(n)\\)\n\nThe Coupon Collector problem is a mathematical metaphor for patience, the closer you are to completion, the rarer progress becomes.\n\n\n\n540 Markov Chain Simulation\nA Markov chain models a system that jumps between states with fixed transition probabilities. The future depends only on the current state, not on the full past. Simulation lets us estimate long-run behavior, hitting times, and steady-state distributions when analysis is hard.\n\nWhat Problem Are We Solving?\nGiven a finite state space \\(\\mathcal{S}={1,\\dots,m}\\) and a row-stochastic transition matrix \\(P\\in\\mathbb{R}^{m\\times m}\\) with \\[\nP_{ij}=P(X_{t+1}=j \\mid X_t=i),\\quad \\sum_{j=1}^m P_{ij}=1,\n\\] we want to generate a trajectory \\(X_0,X_1,\\dots,X_T\\) and estimate quantities like\n\nstationary distribution \\(\\pi\\) such that \\(\\pi P=\\pi\\)\nexpected reward \\(\\mathbb{E}[f(X_t)]\\)\nhitting or return times\n\n\n\nHow Does It Work (Plain Language)\n\nChoose an initial state \\(X_0\\) (or an initial distribution \\(\\mu\\)).\nFor \\(t=0,1,\\dots,T-1\\)\n\nFrom the current state \\(i=X_t\\), draw the next state \\(j\\) according to row \\(i\\) of \\(P\\).\nSet \\(X_{t+1}=j\\).\n\nOptionally discard a burn-in prefix, then average statistics over the rest.\n\nIf the chain is irreducible and aperiodic, empirical frequencies converge to the stationary distribution.\n\n\nExample\nTwo-state weather model, states \\({S,R}\\) for Sunny, Rainy: \\[\nP=\\begin{pmatrix}\n0.8 & 0.2\\\n0.4 & 0.6\n\\end{pmatrix}.\n\\] Start at Sunny, simulate 10,000 steps, estimate fraction of sunny days. Theory: solve \\(\\pi=\\pi P\\), \\(\\pi_S=\\tfrac{2}{3}\\), \\(\\pi_R=\\tfrac{1}{3}\\).\n\n\nTiny Code (Easy Versions)\nPython Version\nimport random\n\ndef simulate_markov(P, start, steps):\n    # P: list of lists, each row sums to 1\n    # start: integer state index\n    x = start\n    traj = [x]\n    for _ in range(steps):\n        r = random.random()\n        cdf, nxt = 0.0, 0\n        for j, p in enumerate(P[x]):\n            cdf += p\n            if r &lt;= cdf:\n                nxt = j\n                break\n        x = nxt\n        traj.append(x)\n    return traj\n\n# Example: Sunny=0, Rainy=1\nP = [[0.8, 0.2],\n     [0.4, 0.6]]\ntraj = simulate_markov(P, start=0, steps=10000)\nburn = 1000\npi_hat_S = sum(1 for s in traj[burn:] if s == 0) / (len(traj) - burn)\nprint(\"Estimated pi(Sunny) =\", pi_hat_S)\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\nint next_state(double *row, int m) {\n    double r = (double)rand() / RAND_MAX, cdf = 0.0;\n    for (int j = 0; j &lt; m; j++) {\n        cdf += row[j];\n        if (r &lt;= cdf) return j;\n    }\n    return m - 1;\n}\n\nint main(void) {\n    srand((unsigned)time(NULL));\n    int m = 2, steps = 10000, x = 0;\n    double P[2][2] = {{0.8, 0.2}, {0.4, 0.6}};\n    int sunny = 0, burn = 1000;\n    for (int t = 0; t &lt; steps; t++) {\n        if (t &gt;= burn && x == 0) sunny++;\n        x = next_state(P[x], m);\n    }\n    double pi_hat = (double)sunny / (steps - burn);\n    printf(\"Estimated pi(Sunny) = %.4f\\n\", pi_hat);\n    return 0;\n}\n\n\nWhy It Matters\n\nStatistics and MCMC: estimate integrals by sampling from complex distributions\nOperations research: queueing systems, reliability, inventory\nReinforcement learning: modeling environment dynamics and returns\nNetworks and graphs: PageRank, random walks, mixing time\n\nSimulation provides estimates when closed forms are unavailable or too costly.\n\n\nA Gentle Proof (Why It Works)\nIf the chain is irreducible and aperiodic with stationary distribution \\(\\pi\\), then by the ergodic theorem \\[\n\\frac{1}{T}\\sum_{t=1}^T f(X_t)\\xrightarrow[]{a.s.}\\sum_{i\\in\\mathcal{S}} \\pi_i f(i).\n\\] In particular, empirical state frequencies converge to \\(\\pi\\). Burn-in reduces bias from the initial distribution.\n\n\nTry It Yourself\n\nVerify the two-state example, compare \\(\\pi_{\\text{Sunny}}\\) estimate to \\(2/3\\).\nBuild a 5-state chain with a known \\(\\pi\\) and confirm convergence.\nAdd a reward function \\(r(i)\\), estimate long-run average reward.\nExplore the effect of burn-in and sample size on variance.\nSimulate a random walk on a graph, estimate visit frequencies.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nChain\nProperty\nTheory\nSimulation (approx)\n\n\n\n\n2-state weather\n\\(\\pi_S\\)\n\\(2/3\\)\n\\(0.66\\) to \\(0.67\\)\n\n\n3-state cyclic with damping\nergodic\nunique \\(\\pi\\)\nfrequencies match \\(\\pi\\)\n\n\nRandom walk on line with reflecting ends\nstationary mass higher near middle\nqualitative\nmatches histogram\n\n\n\n\n\nComplexity\n\nTime: \\(O(T \\cdot \\deg)\\) where \\(\\deg\\) is average nonzero entries per row of \\(P\\)\nSpace: \\(O(1)\\) for online statistics, \\(O(T)\\) if storing the full trajectory\n\nMarkov chain simulation turns matrix probabilities into empirical behavior, one step at a time, then averages reveal the long-run truth.\n\n\n\n\nSection 55. Sieve Methods and Modular Math\n\n541 Sieve of Eratosthenes\nThe Sieve of Eratosthenes is one of the oldest and most elegant algorithms in mathematics, designed to generate all prime numbers up to a given limit \\(n\\). It systematically marks out multiples of primes, leaving only the primes unmarked.\n\nWhat Problem Are We Solving?\nWe want an efficient way to find all primes \\(\\le n\\). A naive method tests divisibility for every number, \\(O(n\\sqrt{n})\\). The sieve reduces this dramatically to \\(O(n \\log\\log n)\\) by marking composites in bulk.\n\n\nHow Does It Work (Plain Language)\nImagine writing all numbers from \\(2\\) to \\(n\\). Starting from the first unmarked number \\(2\\), you:\n\nDeclare it prime.\nMark all multiples of \\(2\\) as composite.\nMove to the next unmarked number (which must be prime).\nRepeat until \\(p^2 &gt; n\\).\n\nThe remaining unmarked numbers are primes.\n\n\nExample\nLet \\(n = 30\\) Start with \\(2\\):\n\n\n\nStep\nPrime\nMarked Multiples\n\n\n\n\n1\n2\n4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30\n\n\n2\n3\n6, 9, 12, 15, 18, 21, 24, 27, 30\n\n\n3\n5\n10, 15, 20, 25, 30\n\n\n4\n7\n\\(7^2 = 49 &gt; 30\\) → stop\n\n\n\nPrimes left: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29\n\n\nTiny Code (Easy Versions)\nPython Version\ndef sieve(n):\n    is_prime = [True] * (n + 1)\n    is_prime[0] = is_prime[1] = False\n    p = 2\n    while p * p &lt;= n:\n        if is_prime[p]:\n            for i in range(p * p, n + 1, p):\n                is_prime[i] = False\n        p += 1\n    return [i for i in range(2, n + 1) if is_prime[i]]\n\nprint(sieve(30))\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;math.h&gt;\n\nvoid sieve(int n) {\n    bool is_prime[n+1];\n    for (int i = 0; i &lt;= n; i++) is_prime[i] = true;\n    is_prime[0] = is_prime[1] = false;\n    for (int p = 2; p * p &lt;= n; p++) {\n        if (is_prime[p]) {\n            for (int i = p * p; i &lt;= n; i += p)\n                is_prime[i] = false;\n        }\n    }\n    for (int i = 2; i &lt;= n; i++)\n        if (is_prime[i]) printf(\"%d \", i);\n}\n\nint main(void) {\n    sieve(30);\n}\n\n\nWhy It Matters\n\nEfficient prime generation, fundamental for number theory, cryptography, and factorization.\nCore building block for:\n\nPrime tables in modular arithmetic\nTotient computation\nSieve-based factorization\nPrecomputation in combinatorics (nCr mod p)\n\nIntuitive, a beautiful demonstration of elimination by pattern.\n\n\n\nA Gentle Proof (Why It Works)\nEvery composite number \\(n\\) has a smallest prime factor \\(p \\le \\sqrt{n}\\). When the sieve marks multiples of each prime up to \\(\\sqrt{n}\\), every composite is marked exactly once by its smallest prime factor. Thus, all unmarked numbers are prime.\n\n\nTry It Yourself\n\nRun the sieve for \\(n=50\\), count primes (should be 15).\nModify to return count instead of list.\nPlot prime density vs \\(n\\).\nExtend to segmented sieve for large \\(n\\).\nCompare runtime vs naive primality testing.\n\n\n\nTest Cases\n\n\n\nn\nPrimes Found\nCount\n\n\n\n\n10\n2, 3, 5, 7\n4\n\n\n20\n2, 3, 5, 7, 11, 13, 17, 19\n8\n\n\n30\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29\n10\n\n\n\n\n\nComplexity\n\nTime: \\(O(n \\log\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe Sieve of Eratosthenes is simplicity sharpened by insight, mark the multiples, reveal the primes.\n\n\n\n542 Linear Sieve\nThe Linear Sieve, also known as the Euler Sieve, is an optimized version of the Sieve of Eratosthenes that computes all primes up to \\(n\\) in \\(O(n)\\) time, marking each composite exactly once.\nIt avoids redundant markings by combining primes with their smallest prime factors.\n\nWhat Problem Are We Solving?\nThe classical sieve marks each composite multiple times, once for every prime divisor. In the linear sieve, each composite is generated only by its smallest prime factor (SPF), ensuring total work proportional to \\(n\\).\nWe want:\n\nAll primes \\(\\le n\\)\nOptionally, the smallest prime factor spf[x] for each \\(x\\)\n\n\n\nHow Does It Work (Plain Language)\nMaintain:\n\nA boolean array is_prime[]\nA list primes[]\n\nAlgorithm:\n\nInitialize all numbers as prime (True).\nFor each integer \\(i\\) from 2 to \\(n\\):\n\nIf is_prime[i] is True, add \\(i\\) to the list of primes.\nFor every prime \\(p\\) in primes:\n\nIf \\(i \\cdot p &gt; n\\), break.\nMark is_prime[i*p] = False.\nIf \\(p\\) divides \\(i\\), stop (to ensure each composite is marked only once).\n\n\n\nThis ensures each composite is marked exactly once by its smallest prime factor.\n\n\nExample\nLet \\(n=10\\):\n\n\n\ni\nprimes\nmarked composites\n\n\n\n\n2\n[2]\n4, 6, 8, 10\n\n\n3\n[2,3]\n6, 9\n\n\n4\nskip (not prime)\n\n\n\n5\n[2,3,5]\n10\n\n\n6\nskip\n\n\n\n7\n[2,3,5,7]\n\n\n\n8\nskip\n\n\n\n9\nskip\n\n\n\n10\nskip\n\n\n\n\nPrimes: 2, 3, 5, 7\n\n\nTiny Code (Easy Versions)\nPython Version\ndef linear_sieve(n):\n    is_prime = [True] * (n + 1)\n    primes = []\n    spf = [0] * (n + 1)  # smallest prime factor\n    is_prime[0] = is_prime[1] = False\n\n    for i in range(2, n + 1):\n        if is_prime[i]:\n            primes.append(i)\n            spf[i] = i\n        for p in primes:\n            if i * p &gt; n:\n                break\n            is_prime[i * p] = False\n            spf[i * p] = p\n            if i % p == 0:\n                break\n    return primes, spf\n\npr, spf = linear_sieve(30)\nprint(\"Primes:\", pr)\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid linear_sieve(int n) {\n    bool is_prime[n+1];\n    int primes[n+1], spf[n+1];\n    int count = 0;\n\n    for (int i = 0; i &lt;= n; i++) is_prime[i] = true;\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i &lt;= n; i++) {\n        if (is_prime[i]) {\n            primes[count++] = i;\n            spf[i] = i;\n        }\n        for (int j = 0; j &lt; count; j++) {\n            int p = primes[j];\n            if (i * p &gt; n) break;\n            is_prime[i * p] = false;\n            spf[i * p] = p;\n            if (i % p == 0) break;\n        }\n    }\n\n    printf(\"Primes: \");\n    for (int i = 0; i &lt; count; i++) printf(\"%d \", primes[i]);\n}\n\nint main(void) {\n    linear_sieve(30);\n}\n\n\nWhy It Matters\nThe linear sieve is a powerful improvement:\n\nEach number processed once\nFastest possible prime sieve (tight asymptotic bound)\nUseful byproducts:\n\nSmallest prime factor (SPF) table for factorization\nPrime list for arithmetic functions (e.g. Euler’s Totient)\n\n\nUsed widely in:\n\nCompetitive programming\nPrecomputation for modular arithmetic\nFactorization and divisor enumeration\n\n\n\nA Gentle Proof (Why It Works)\nEach composite \\(n = p \\cdot m\\) is marked exactly once, by its smallest prime factor \\(p\\). When \\(i = m\\), the algorithm pairs \\(i\\) with \\(p\\):\n\nIf \\(p\\) divides \\(i\\), break to prevent further markings.\n\nThus total operations ≈ \\(O(n)\\).\n\n\nTry It Yourself\n\nCompare with classical sieve timings for \\(n=10^6\\).\nPrint spf[x] for \\(x=2\\) to \\(20\\).\nWrite a function to factorize \\(x\\) using spf.\nModify to count number of prime factors.\nExtend to compute totient in \\(O(n)\\).\n\n\n\nTest Cases\n\n\n\nn\nPrimes Found\nCount\n\n\n\n\n10\n2, 3, 5, 7\n4\n\n\n30\n2, 3, 5, 7, 11, 13, 17, 19, 23, 29\n10\n\n\n100\n25 primes\n25\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nThe Linear Sieve refines Eratosthenes’ brilliance, marking each composite once, no more, no less.\n\n\n\n543 Segmented Sieve\nThe Segmented Sieve extends the classic sieve to handle large ranges efficiently, such as generating primes between \\(L\\) and \\(R\\) where \\(R\\) may be very large (e.g. \\(10^{12}\\)), without storing all numbers up to \\(R\\) in memory.\nIt divides the range into segments small enough to fit in memory, sieving each one using precomputed small primes.\n\nWhat Problem Are We Solving?\nThe normal sieve of Eratosthenes needs memory proportional to \\(n\\), making it infeasible for large upper bounds (like \\(10^{12}\\)). The segmented sieve solves this by using a two-phase process:\n\nPrecompute small primes up to \\(\\sqrt{R}\\) using a standard sieve.\nSieve each segment \\([L, R]\\) using those primes.\n\nThis way, memory usage stays \\(O(\\sqrt{R}) + O(R-L+1)\\), even for very large \\(R\\).\n\n\nHow Does It Work (Plain Language)\nTo generate primes in \\([L, R]\\):\n\nPre-sieve small primes: \\[ \\text{small\\_primes} = \\text{sieve}(\\sqrt{R}) \\]\nInitialize a boolean segment array of size \\(R-L+1\\) (all True).\nFor each small prime \\(p\\):\n\nFind the first multiple of \\(p\\) in \\([L, R]\\): \\[\n\\text{start} = \\max(p^2, \\lceil \\frac{L}{p} \\rceil \\cdot p)\n\\]\nMark all multiples of \\(p\\) as composite.\n\nRemaining unmarked numbers are primes in \\([L, R]\\).\n\n\n\nExample\nFind primes in \\([10, 30]\\)\n\nCompute primes up to \\(\\sqrt{30} = 5\\): \\({2, 3, 5}\\)\nSegment = [10..30], mark multiples:\n\n\\(2\\): mark 10,12,14,…,30\n\\(3\\): mark 12,15,18,21,24,27,30\n\\(5\\): mark 10,15,20,25,30\n\nUnmarked → 11, 13, 17, 19, 23, 29\n\n\n\nTiny Code (Easy Versions)\nPython Version\nimport math\n\ndef simple_sieve(limit):\n    is_prime = [True] * (limit + 1)\n    is_prime[0] = is_prime[1] = False\n    for p in range(2, int(math.sqrt(limit)) + 1):\n        if is_prime[p]:\n            for i in range(p * p, limit + 1, p):\n                is_prime[i] = False\n    return [p for p in range(2, limit + 1) if is_prime[p]]\n\ndef segmented_sieve(L, R):\n    limit = int(math.sqrt(R)) + 1\n    primes = simple_sieve(limit)\n    is_prime = [True] * (R - L + 1)\n\n    for p in primes:\n        start = max(p * p, ((L + p - 1) // p) * p)\n        for i in range(start, R + 1, p):\n            is_prime[i - L] = False\n\n    if L == 1:\n        is_prime[0] = False\n\n    return [L + i for i, prime in enumerate(is_prime) if prime]\n\nprint(segmented_sieve(10, 30))\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid simple_sieve(int limit, int primes[], int *count) {\n    bool mark[limit+1];\n    for (int i = 0; i &lt;= limit; i++) mark[i] = true;\n    mark[0] = mark[1] = false;\n    for (int p = 2; p * p &lt;= limit; p++)\n        if (mark[p])\n            for (int i = p * p; i &lt;= limit; i += p)\n                mark[i] = false;\n    *count = 0;\n    for (int i = 2; i &lt;= limit; i++)\n        if (mark[i]) primes[(*count)++] = i;\n}\n\nvoid segmented_sieve(long long L, long long R) {\n    int primes[100000], count;\n    int limit = (int)sqrt(R) + 1;\n    simple_sieve(limit, primes, &count);\n    int size = R - L + 1;\n    bool is_prime[size];\n    for (int i = 0; i &lt; size; i++) is_prime[i] = true;\n\n    for (int i = 0; i &lt; count; i++) {\n        long long p = primes[i];\n        long long start = p * p;\n        if (start &lt; L) start = ((L + p - 1) / p) * p;\n        for (long long j = start; j &lt;= R; j += p)\n            is_prime[j - L] = false;\n    }\n\n    if (L == 1) is_prime[0] = false;\n\n    for (int i = 0; i &lt; size; i++)\n        if (is_prime[i]) printf(\"%lld \", L + i);\n}\n\nint main(void) {\n    segmented_sieve(10, 30);\n}\n\n\nWhy It Matters\nThe segmented sieve is essential when:\n\n\\(R\\) is too large for a full array\nYou need prime generation in ranges, e.g. [10⁹, 10⁹ + 10⁶]\nBuilding prime lists for big-number algorithms (RSA, primality tests)\n\nIt combines space efficiency with speed, leveraging precomputed small primes.\n\n\nA Gentle Proof (Why It Works)\nEvery composite number in \\([L,R]\\) has a smallest prime factor \\(p \\le \\sqrt{R}\\). Thus, by marking all multiples of primes \\(\\le \\sqrt{R}\\), we eliminate all composites. Numbers left unmarked must be prime.\nNo need to store all numbers ≤ \\(R\\), only the current segment.\n\n\nTry It Yourself\n\nGenerate primes between \\(10^6\\) and \\(10^6+1000\\).\nMeasure memory vs full sieve.\nTry non-square segment sizes.\nCompare performance with classical sieve.\nCombine with wheel factorization for extra speed.\n\n\n\nTest Cases\n\n\n\nRange\nOutput Primes\nCount\n\n\n\n\n[10, 30]\n11,13,17,19,23,29\n6\n\n\n[1, 10]\n2,3,5,7\n4\n\n\n[100, 120]\n101,103,107,109,113\n5\n\n\n\n\n\nComplexity\n\nTime: \\(O((R-L+1)\\log\\log R)\\)\nSpace: \\(O(\\sqrt{R}) + O(R-L+1)\\)\n\nThe Segmented Sieve scales Eratosthenes’ idea to infinity, sieving range by range, memory never a barrier.\n\n\n\n544 SPF (Smallest Prime Factor) Table\nThe Smallest Prime Factor (SPF) Table precomputes, for every integer \\(1 \\le n \\le N\\), the smallest prime that divides it. It’s a powerful foundation for fast factorization, divisor functions, and multiplicative arithmetic functions, all in \\(O(N)\\) time.\n\nWhat Problem Are We Solving?\nWe often need to:\n\nFactorize many numbers quickly\nCompute arithmetic functions (e.g. \\(\\varphi(n)\\), \\(\\tau(n)\\), \\(\\sigma(n)\\))\nCheck primality efficiently\n\nTrial division is \\(O(\\sqrt{n})\\) per query, too slow for many queries. With SPF precomputation, any number can be factorized in \\(O(\\log n)\\).\n\n\nHow Does It Work (Plain Language)\nWe build a table spf[i] such that:\n\nIf \\(i\\) is prime → spf[i] = i\nIf \\(i\\) is composite → spf[i] = smallest prime dividing \\(i\\)\n\nWe fill it using a linear sieve:\n\nStart with spf[i] = 0 (unset).\nWhen visiting a prime \\(p\\), set spf[p] = p.\nFor each \\(p\\), mark \\(i \\cdot p\\) with spf[i*p] = p if unset.\n\nEach composite is processed once, by its smallest prime.\n\n\nExample\nCompute spf for \\(1 \\le i \\le 10\\):\n\n\n\ni\nspf[i]\nFactorization\n\n\n\n\n1\n1\n,\n\n\n2\n2\n2\n\n\n3\n3\n3\n\n\n4\n2\n2×2\n\n\n5\n5\n5\n\n\n6\n2\n2×3\n\n\n7\n7\n7\n\n\n8\n2\n2×2×2\n\n\n9\n3\n3×3\n\n\n10\n2\n2×5\n\n\n\nFactorization becomes trivial:\nn = 84\nspf[84] = 2 → 42\nspf[42] = 2 → 21\nspf[21] = 3 → 7\nspf[7] = 7 → 1\n→ 2 × 2 × 3 × 7\n\n\nTiny Code (Easy Versions)\nPython Version\ndef spf_sieve(n):\n    spf = [0] * (n + 1)\n    spf[1] = 1\n    for i in range(2, n + 1):\n        if spf[i] == 0:\n            spf[i] = i\n            for j in range(i * i, n + 1, i):\n                if spf[j] == 0:\n                    spf[j] = i\n    return spf\n\ndef factorize(n, spf):\n    factors = []\n    while n != 1:\n        factors.append(spf[n])\n        n //= spf[n]\n    return factors\n\nspf = spf_sieve(100)\nprint(factorize(84, spf))\nC Version\n#include &lt;stdio.h&gt;\n\nvoid spf_sieve(int n, int spf[]) {\n    for (int i = 0; i &lt;= n; i++) spf[i] = 0;\n    spf[1] = 1;\n    for (int i = 2; i &lt;= n; i++) {\n        if (spf[i] == 0) {\n            spf[i] = i;\n            for (long long j = (long long)i * i; j &lt;= n; j += i)\n                if (spf[j] == 0) spf[j] = i;\n        }\n    }\n}\n\nvoid factorize(int n, int spf[]) {\n    while (n != 1) {\n        printf(\"%d \", spf[n]);\n        n /= spf[n];\n    }\n}\n\nint main(void) {\n    int n = 100, spf[101];\n    spf_sieve(n, spf);\n    factorize(84, spf);\n}\n\n\nWhy It Matters\nThe SPF table is a Swiss-army knife for number-theoretic algorithms:\n\nPrime factorization in \\(O(\\log n)\\)\nCounting divisors: use exponents from SPF\nComputing totient: \\(\\varphi(n)\\) via prime factors\nDetecting square-free numbers\n\nUsed in:\n\nModular arithmetic systems\nFactorization-heavy algorithms\nCompetitive programming precomputations\n\n\n\nA Gentle Proof (Why It Works)\nEach composite \\(n = p \\cdot m\\) has smallest prime \\(p\\). When sieve reaches \\(p\\), it sets spf[n] = p. If a smaller prime existed, it would’ve marked \\(n\\) earlier. Thus spf[n] is indeed the smallest prime dividing \\(n\\).\nEach number is marked once → total time \\(O(n)\\).\n\n\nTry It Yourself\n\nBuild spf[] for \\(n=50\\) and print all factorizations.\nWrite is_prime(i) = (spf[i]==i).\nModify to count distinct prime factors.\nCompute \\(\\varphi(i)\\) using SPF factors.\nVisualize prime factor frequencies.\n\n\n\nTest Cases\n\n\n\nn\nFactorization via SPF\n\n\n\n\n10\n2 × 5\n\n\n12\n2 × 2 × 3\n\n\n36\n2 × 2 × 3 × 3\n\n\n84\n2 × 2 × 3 × 7\n\n\n\n\n\nComplexity\n\nPrecompute: \\(O(n)\\)\nFactorization per query: \\(O(\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe SPF table turns factorization from division into lookup, precalculate once, reuse forever.\n\n\n\n545 Möbius Function Sieve\nThe Möbius function \\(\\mu(n)\\) is a multiplicative arithmetic function central to inversion formulas and detecting square factors. Values:\n\n\\(\\mu(1)=1\\)\n\\(\\mu(n)=0\\) if \\(n\\) has any squared prime factor\n\\(\\mu(n)=(-1)^k\\) if \\(n\\) is a product of \\(k\\) distinct primes\n\nA Möbius sieve computes \\(\\mu(1),\\dots,\\mu(N)\\) efficiently for large \\(N\\).\n\nWhat Problem Are We Solving?\nWe want to compute \\(\\mu(n)\\) for all \\(1 \\le n \\le N\\) faster than factoring each \\(n\\) separately.\nTarget:\n\nAll values of \\(\\mu\\) up to \\(N\\) in near linear time\nOften together with a prime list and the smallest prime factor\n\n\n\nHow Does It Work (Plain Language)\nUse a linear sieve with these invariants:\n\nMaintain a dynamic list of primes primes\nStore mu[i] as we go\nFor each \\(i\\) from \\(2\\) to \\(N\\):\n\nIf \\(i\\) is prime, set mu[i] = -1 and push to primes\nFor each prime \\(p\\) in primes:\n\nIf $i p &gt; N`, stop\nIf \\(p \\mid i\\), then:\n\nmu[i*p] = 0 because \\(p^2 \\mid i p\\)\nbreak to keep linear complexity\n\nElse:\n\nmu[i*p] = -mu[i] because we add a new distinct prime factor\n\n\n\n\nInitialize with mu[1] = 1.\nThis marks each composite exactly once with its smallest prime factor relation.\n\n\nExample\nCompute \\(\\mu(n)\\) for \\(1 \\le n \\le 12\\):\n\n\n\n\\(n\\)\nprime factors\nsquarefree\n\\(\\mu(n)\\)\n\n\n\n\n1\n,\nyes\n1\n\n\n2\n\\(2\\)\nyes\n\\(-1\\)\n\n\n3\n\\(3\\)\nyes\n\\(-1\\)\n\n\n4\n\\(2^2\\)\nno\n0\n\n\n5\n\\(5\\)\nyes\n\\(-1\\)\n\n\n6\n\\(2\\cdot 3\\)\nyes\n\\(+1\\)\n\n\n7\n\\(7\\)\nyes\n\\(-1\\)\n\n\n8\n\\(2^3\\)\nno\n0\n\n\n9\n\\(3^2\\)\nno\n0\n\n\n10\n\\(2\\cdot 5\\)\nyes\n\\(+1\\)\n\n\n11\n\\(11\\)\nyes\n\\(-1\\)\n\n\n12\n\\(2^2\\cdot 3\\)\nno\n0\n\n\n\n\n\nTiny Code (Easy Versions)\nPython Version (Linear Sieve for \\(\\mu\\))\ndef mobius_sieve(n):\n    mu = [0] * (n + 1)\n    mu[1] = 1\n    primes = []\n    is_comp = [False] * (n + 1)\n\n    for i in range(2, n + 1):\n        if not is_comp[i]:\n            primes.append(i)\n            mu[i] = -1\n        for p in primes:\n            ip = i * p\n            if ip &gt; n:\n                break\n            is_comp[ip] = True\n            if i % p == 0:\n                mu[ip] = 0           # p^2 divides ip\n                break\n            else:\n                mu[ip] = -mu[i]      # add new distinct prime\n    return mu\n\n# Example\nmu = mobius_sieve(50)\nprint([ (i, mu[i]) for i in range(1, 13) ])\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid mobius_sieve(int n, int mu[]) {\n    bool comp[n + 1];\n    for (int i = 0; i &lt;= n; i++) { comp[i] = false; mu[i] = 0; }\n    mu[1] = 1;\n\n    int primes[n + 1], pc = 0;\n\n    for (int i = 2; i &lt;= n; i++) {\n        if (!comp[i]) {\n            primes[pc++] = i;\n            mu[i] = -1;\n        }\n        for (int j = 0; j &lt; pc; j++) {\n            long long ip = 1LL * i * primes[j];\n            if (ip &gt; n) break;\n            comp[ip] = true;\n            if (i % primes[j] == 0) {\n                mu[ip] = 0;              // squared factor\n                break;\n            } else {\n                mu[ip] = -mu[i];         // add a new distinct prime\n            }\n        }\n    }\n}\n\nint main(void) {\n    int N = 50;\n    int mu[51];\n    mobius_sieve(N, mu);\n    for (int i = 1; i &lt;= 12; i++)\n        printf(\"mu(%d) = %d\\n\", i, mu[i]);\n    return 0;\n}\n\n\nWhy It Matters\n\nMöbius inversion in number theory: \\[\nf(n)=\\sum_{d\\mid n} g(d)\n\\quad \\Longleftrightarrow \\quad\ng(n)=\\sum_{d\\mid n} \\mu(d) f!\\left(\\frac{n}{d}\\right)\n\\]\nDetects squarefree numbers: \\(\\mu(n)\\ne 0\\) iff \\(n\\) is squarefree\nCentral to:\n\nDirichlet convolutions and multiplicative functions\nInclusion exclusion over divisors\nEvaluating sums like \\(\\sum_{n\\le N}\\mu(n)\\)\nCounting problems with gcd or coprimality constraints\n\n\n\n\nA Gentle Proof (Why It Works)\nInduct on increasing integers while maintaining:\n\nIf \\(i\\) is prime then \\(\\mu(i)=-1\\)\nFor any prime \\(p\\):\n\nIf \\(p \\mid i\\) then \\(\\mu(i p)=0\\) because \\(p^2 \\mid i p\\)\nIf \\(p \\nmid i\\) then \\(\\mu(i p)=-\\mu(i)\\) since \\(i p\\) is squarefree with one more distinct prime\n\n\nThe linear loop stops at the first prime dividing \\(i\\), so each composite is processed exactly once. Thus total work is proportional to \\(N\\) and the recurrence of signs matches the definition of \\(\\mu\\).\n\n\nTry It Yourself\n\nVerify that \\(\\sum_{d\\mid n} \\mu(d) = 0\\) for \\(n&gt;1\\) and equals \\(1\\) for \\(n=1\\).\nCompute count of squarefree integers up to \\(N\\) using \\(\\sum_{n\\le N} [\\mu(n)\\ne 0]\\).\nImplement Dirichlet convolution with precomputed \\(\\mu\\) to invert divisor sums.\nCompare runtime of the linear sieve versus factoring each \\(n\\).\nExtend the sieve to also store the smallest prime factor and reuse it.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(\\mu(n)\\)\n\n\n\n\n1\n1\n\n\n2\n\\(-1\\)\n\n\n3\n\\(-1\\)\n\n\n4\n0\n\n\n6\n\\(+1\\)\n\n\n10\n\\(+1\\)\n\n\n12\n0\n\n\n30\n\\(-1\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(N)\\) using the linear sieve\nSpace: \\(O(N)\\) for arrays\n\nThe Möbius sieve delivers \\(\\mu\\) values for an entire range in one pass, making inversion and squarefree analysis practical at scale.\n\n\n\n546 Euler’s Totient Sieve\nThe Euler Totient Function \\(\\varphi(n)\\) counts how many integers \\(1 \\le k \\le n\\) are coprime to \\(n\\). That is, numbers such that \\(\\gcd(k,n)=1\\).\nWe can compute all \\(\\varphi(1), \\dots, \\varphi(N)\\) in \\(O(N)\\) time using a linear sieve.\n\nWhat Problem Are We Solving?\nNaively, computing \\(\\varphi(n)\\) requires prime factorization: \\[\n\\varphi(n) = n \\prod_{p|n}\\left(1 - \\frac{1}{p}\\right)\n\\] Doing that for each \\(n\\) individually is too slow.\nWe want a fast sieve to compute \\(\\varphi\\) for every \\(n\\) up to \\(N\\) in one pass.\n\n\nHow Does It Work (Plain Language)\nWe use a linear sieve similar to prime generation:\n\nStart with phi[1] = 1.\nFor each number \\(i\\):\n\nIf \\(i\\) is prime, then \\(\\varphi(i)=i-1\\).\nFor each prime \\(p\\):\n\nIf \\(i \\cdot p &gt; N\\), stop.\nIf \\(p \\mid i\\) (i.e., \\(p\\) divides \\(i\\)):\n\n\\(\\varphi(i \\cdot p) = \\varphi(i) \\cdot p\\)\nbreak (to ensure linear time)\n\nElse:\n\n\\(\\varphi(i \\cdot p) = \\varphi(i) \\cdot (p-1)\\)\n\n\n\n\nEach number is processed exactly once, preserving \\(O(N)\\) complexity.\n\n\nExample\nLet’s compute \\(\\varphi(n)\\) for \\(n=1\\) to \\(10\\):\n\n\n\n\n\n\n\n\n\n\\(n\\)\nPrime Factors\nFormula\n\\(\\varphi(n)\\)\n\n\n\n\n1\n,\n1\n1\n\n\n2\n\\(2\\)\n\\(2(1-\\frac{1}{2})\\)\n1\n\n\n3\n\\(3\\)\n\\(3(1-\\frac{1}{3})\\)\n2\n\n\n4\n\\(2^2\\)\n\\(4(1-\\frac{1}{2})\\)\n2\n\n\n5\n\\(5\\)\n\\(5(1-\\frac{1}{5})\\)\n4\n\n\n6\n\\(2\\cdot3\\)\n\\(6(1-\\frac{1}{2})(1-\\frac{1}{3})\\)\n2\n\n\n7\n\\(7\\)\n\\(7(1-\\frac{1}{7})\\)\n6\n\n\n8\n\\(2^3\\)\n\\(8(1-\\frac{1}{2})\\)\n4\n\n\n9\n\\(3^2\\)\n\\(9(1-\\frac{1}{3})\\)\n6\n\n\n10\n\\(2\\cdot5\\)\n\\(10(1-\\frac{1}{2})(1-\\frac{1}{5})\\)\n4\n\n\n\n\n\nTiny Code (Easy Versions)\nPython Version\ndef totient_sieve(n):\n    phi = [0] * (n + 1)\n    primes = []\n    phi[1] = 1\n    is_comp = [False] * (n + 1)\n\n    for i in range(2, n + 1):\n        if not is_comp[i]:\n            primes.append(i)\n            phi[i] = i - 1\n        for p in primes:\n            ip = i * p\n            if ip &gt; n:\n                break\n            is_comp[ip] = True\n            if i % p == 0:\n                phi[ip] = phi[i] * p\n                break\n            else:\n                phi[ip] = phi[i] * (p - 1)\n    return phi\n\n# Example\nphi = totient_sieve(20)\nfor i in range(1, 11):\n    print(f\"phi({i}) = {phi[i]}\")\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nvoid totient_sieve(int n, int phi[]) {\n    bool comp[n + 1];\n    int primes[n + 1], pc = 0;\n    for (int i = 0; i &lt;= n; i++) { comp[i] = false; phi[i] = 0; }\n    phi[1] = 1;\n\n    for (int i = 2; i &lt;= n; i++) {\n        if (!comp[i]) {\n            primes[pc++] = i;\n            phi[i] = i - 1;\n        }\n        for (int j = 0; j &lt; pc; j++) {\n            int p = primes[j];\n            long long ip = 1LL * i * p;\n            if (ip &gt; n) break;\n            comp[ip] = true;\n            if (i % p == 0) {\n                phi[ip] = phi[i] * p;\n                break;\n            } else {\n                phi[ip] = phi[i] * (p - 1);\n            }\n        }\n    }\n}\n\nint main(void) {\n    int n = 20, phi[21];\n    totient_sieve(n, phi);\n    for (int i = 1; i &lt;= 10; i++)\n        printf(\"phi(%d) = %d\\n\", i, phi[i]);\n}\n\n\nWhy It Matters\nThe totient function \\(\\varphi(n)\\) is foundational in:\n\nEuler’s theorem: \\(a^{\\varphi(n)} \\equiv 1 \\pmod n\\) if \\(\\gcd(a,n)=1\\)\nRSA encryption: \\(\\varphi(n)\\) defines modular inverses for keys\nCounting reduced fractions: number of coprime pairs\nGroup theory: size of multiplicative group \\((\\mathbb{Z}/n\\mathbb{Z})^\\times\\)\n\nAnd useful in:\n\nModular arithmetic\nCryptography\nNumber-theoretic combinatorics\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(i\\) be current number, \\(p\\) a prime:\n\nIf \\(p\\nmid i\\), \\(\\varphi(i p) = \\varphi(i) \\cdot (p-1)\\) since we multiply by a new distinct prime.\nIf \\(p\\mid i\\), \\(\\varphi(i p) = \\varphi(i) \\cdot p\\) because we’re extending a power of an existing prime.\n\nEvery number is built from its smallest prime factor in exactly one way → linear time.\n\n\nTry It Yourself\n\nCompute \\(\\varphi(n)\\) for \\(n=1\\) to \\(20\\).\nVerify \\(\\sum_{d|n} \\varphi(d) = n\\).\nPlot \\(\\varphi(n)/n\\) to visualize density of coprime numbers.\nUse \\(\\varphi(n)\\) to compute modular inverses via Euler’s theorem.\nAdapt sieve to also store primes and smallest prime factors.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\n\\(\\varphi(n)\\)\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n2\n\n\n4\n2\n\n\n5\n4\n\n\n6\n2\n\n\n10\n4\n\n\n12\n4\n\n\n15\n8\n\n\n20\n8\n\n\n\n\n\nComplexity\n\nTime: \\(O(N)\\)\nSpace: \\(O(N)\\)\n\nThe Euler Totient Sieve is the backbone for arithmetic, cryptographic, and modular reasoning, one pass, all \\(\\varphi(n)\\) ready.\n\n\n\n547 Divisor Count Sieve\nThe Divisor Count Sieve precomputes the number of positive divisors \\(d(n)\\) (also written \\(\\tau(n)\\)) for all integers \\(1 \\le n \\le N\\). It’s a powerful tool in number theory and combinatorics, perfect for counting factors efficiently in \\(O(N\\log N)\\) time.\n\nWhat Problem Are We Solving?\nWe want to compute the number of divisors for every integer up to \\(N\\): \\[\nd(n) = \\sum_{i \\mid n} 1\n\\] or equivalently, if the prime factorization of \\(n\\) is \\[\nn = p_1^{a_1} p_2^{a_2} \\dots p_k^{a_k},\n\\] then \\[\nd(n) = (a_1 + 1)(a_2 + 1)\\dots(a_k + 1).\n\\]\nNaively factoring each number is too slow. The sieve method does it in one unified pass.\n\n\nHow Does It Work (Plain Language)\nWe use a divisor accumulation sieve:\nFor each \\(i\\) from \\(1\\) to \\(N\\):\n\nAdd \\(1\\) to every multiple of \\(i\\) (because \\(i\\) divides each multiple)\nIn code:\nfor i in range(1, N+1):\n    for j in range(i, N+1, i):\n        div[j] += 1\n\nEach integer \\(n\\) gets incremented once for each divisor \\(i \\mid n\\). Total operations \\(\\sim N\\log N\\).\n\n\nExample\nFor \\(N = 10\\):\n\n\n\n\\(n\\)\nDivisors\n\\(d(n)\\)\n\n\n\n\n1\n1\n1\n\n\n2\n1, 2\n2\n\n\n3\n1, 3\n2\n\n\n4\n1, 2, 4\n3\n\n\n5\n1, 5\n2\n\n\n6\n1, 2, 3, 6\n4\n\n\n7\n1, 7\n2\n\n\n8\n1, 2, 4, 8\n4\n\n\n9\n1, 3, 9\n3\n\n\n10\n1, 2, 5, 10\n4\n\n\n\n\n\nTiny Code (Easy Versions)\nPython Version\ndef divisor_count_sieve(n):\n    div = [0] * (n + 1)\n    for i in range(1, n + 1):\n        for j in range(i, n + 1, i):\n            div[j] += 1\n    return div\n\n# Example\nN = 10\ndiv = divisor_count_sieve(N)\nfor i in range(1, N + 1):\n    print(f\"d({i}) = {div[i]}\")\nC Version\n#include &lt;stdio.h&gt;\n\nvoid divisor_count_sieve(int n, int div[]) {\n    for (int i = 0; i &lt;= n; i++) div[i] = 0;\n    for (int i = 1; i &lt;= n; i++)\n        for (int j = i; j &lt;= n; j += i)\n            div[j]++;\n}\n\nint main(void) {\n    int N = 10, div[11];\n    divisor_count_sieve(N, div);\n    for (int i = 1; i &lt;= N; i++)\n        printf(\"d(%d) = %d\\n\", i, div[i]);\n}\n\n\nWhy It Matters\nThe divisor count function \\(d(n)\\) is essential in:\n\nDivisor-sum problems\nHighly composite numbers\nCounting lattice points\nSummation over divisors (e.g. \\(\\sum_{i=1}^N d(i)\\))\nDynamic programming and combinatorial enumeration\nNumber-theoretic transforms\n\nAlso appears in formulas like: \\[\n\\sigma_0(n) = d(n), \\quad \\sigma_1(n) = \\text{sum of divisors}\n\\]\n\n\nA Gentle Proof (Why It Works)\nEach integer \\(i\\) divides exactly \\(\\lfloor N/i \\rfloor\\) numbers ≤ \\(N\\). So in the nested loop, \\(i\\) contributes \\(+1\\) to \\(\\lfloor N/i \\rfloor\\) entries. Summing over \\(i\\) gives total operations: \\[\n\\sum_{i=1}^{N} \\frac{N}{i} \\approx N \\log N\n\\]\nThis is efficient and straightforward.\n\n\nTry It Yourself\n\nPrint \\(d(n)\\) for \\(n = 1\\) to \\(30\\).\nPlot \\(d(n)\\) to see how divisor counts fluctuate.\nModify code to compute sum of divisors:\ndivsum[j] += i\nCombine with totient sieve to study divisor distributions.\nCount numbers with exactly \\(k\\) divisors.\n\n\n\nTest Cases\n\n\n\n\\(n\\)\nDivisors\n\\(d(n)\\)\n\n\n\n\n1\n1\n1\n\n\n2\n1, 2\n2\n\n\n4\n1, 2, 4\n3\n\n\n6\n1, 2, 3, 6\n4\n\n\n8\n1, 2, 4, 8\n4\n\n\n12\n1, 2, 3, 4, 6, 12\n6\n\n\n30\n1, 2, 3, 5, 6, 10, 15, 30\n8\n\n\n\n\n\nComplexity\n\nTime: \\(O(N\\log N)\\)\nSpace: \\(O(N)\\)\n\nThe Divisor Count Sieve is simple yet mighty, precomputing factor structure for every number with a few nested loops.\n\n\n\n548 Modular Precomputation\nModular precomputation prepares tables like factorials, inverse elements, and powers modulo \\(M\\) so that later queries run in \\(O(1)\\) time after an \\(O(N)\\) or \\(O(N \\log M)\\) setup. This is the backbone for fast combinatorics, DP, and number theoretic transforms under a modulus.\n\nWhat Problem Are We Solving?\nWe often need to compute repeatedly:\n\n\\(a+b\\), \\(a-b\\), \\(a\\cdot b\\), \\(a^k \\bmod M\\)\nmodular inverses \\(a^{-1} \\bmod M\\)\nbinomial coefficients \\(\\binom{n}{r} \\bmod M\\)\n\nDoing these from scratch per query costs \\(O(\\log M)\\) time via exponentiation. With precomputation we answer in \\(O(1)\\) per query after one linear pass.\n\n\nWhat Do We Precompute?\nFor a prime modulus \\(M\\) and a chosen limit \\(N\\):\n\nfact[i] = i! mod M for \\(0 \\le i \\le N\\)\ninv[i] = i^{-1} mod M for \\(1 \\le i \\le N\\)\ninvfact[i] = (i!)^{-1} mod M for \\(0 \\le i \\le N\\)\noptional: powA[i] = A^i mod M for fixed bases\n\nThen \\[\n\\binom{n}{r} \\bmod M = \\text{fact}[n]\\cdot \\text{invfact}[r]\\cdot \\text{invfact}[n-r] \\bmod M\n\\] in \\(O(1)\\) time.\n\n\nHow Does It Work (Plain Language)\n\nFactorials: one forward pass\nInverse factorials: compute \\(\\text{invfact}[N]=\\text{fact}[N]^{M-2}\\bmod M\\) by Fermat then run backward\nElementwise inverses: either from invfact and fact or linearly by the identity \\[\n\\text{inv}[1]=1,\\qquad\n\\text{inv}[i]=M-\\left(\\left\\lfloor \\frac{M}{i}\\right\\rfloor\\cdot \\text{inv}[M\\bmod i]\\right)\\bmod M\n\\] which runs in \\(O(N)\\).\n\nThese rely on \\(M\\) being prime so that every \\(1\\le i&lt;M\\) is invertible.\n\n\nEdge Cases\n\nIf \\(M\\) is not prime: use extended Euclid to invert numbers coprime with \\(M\\), or use factorial tables only for indices not hitting noninvertible factors. For combinatorics with composite \\(M\\), use prime factorization of \\(M\\) plus CRT, or use Lucas or Garner methods when applicable.\nRange limit: choose \\(N\\) at least as large as the maximum \\(n\\) you will query.\n\n\n\nTiny Code (Easy Versions)\nPython Version (prime modulus)\nM = 109 + 7\n\ndef modpow(a, e, m=M):\n    r = 1\n    while e:\n        if e & 1: r = r * a % m\n        a = a * a % m\n        e &gt;&gt;= 1\n    return r\n\ndef build_tables(N, M=109+7):\n    fact = [1] * (N + 1)\n    for i in range(1, N + 1):\n        fact[i] = fact[i - 1] * i % M\n\n    invfact = [1] * (N + 1)\n    invfact[N] = modpow(fact[N], M - 2, M)  # Fermat\n    for i in range(N, 0, -1):\n        invfact[i - 1] = invfact[i] * i % M\n\n    inv = [0] * (N + 1)\n    inv[1] = 1\n    for i in range(2, N + 1):\n        inv[i] = (M - (M // i) * inv[M % i] % M) % M\n\n    return fact, invfact, inv\n\ndef nCr_mod(n, r, fact, invfact, M=109+7):\n    if r &lt; 0 or r &gt; n: return 0\n    return fact[n] * invfact[r] % M * invfact[n - r] % M\n\n# Example\nN = 1_000_000\nfact, invfact, inv = build_tables(N, M)\nprint(nCr_mod(10, 3, fact, invfact, M))  # 120\nC Version (prime modulus)\n#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n\nconst int MOD = 1000000007;\n\nlong long modpow(long long a, long long e) {\n    long long r = 1 % MOD;\n    while (e) {\n        if (e & 1) r = (r * a) % MOD;\n        a = (a * a) % MOD;\n        e &gt;&gt;= 1;\n    }\n    return r;\n}\n\nvoid build_tables(int N, int fact[], int invfact[], int inv[]) {\n    fact[0] = 1;\n    for (int i = 1; i &lt;= N; i++) fact[i] = (long long)fact[i-1] * i % MOD;\n\n    invfact[N] = modpow(fact[N], MOD - 2);\n    for (int i = N; i &gt;= 1; i--) invfact[i-1] = (long long)invfact[i] * i % MOD;\n\n    inv[1] = 1;\n    for (int i = 2; i &lt;= N; i++)\n        inv[i] = (int)((MOD - (long long)(MOD / i) * inv[MOD % i] % MOD) % MOD);\n}\n\nint nCr_mod(int n, int r, int fact[], int invfact[]) {\n    if (r &lt; 0 || r &gt; n) return 0;\n    return (int)((long long)fact[n] * invfact[r] % MOD * invfact[n - r] % MOD);\n}\n\nint main(void) {\n    int N = 1000000;\n    static int fact[1000001], invfact[1000001], inv[1000001];\n    build_tables(N, fact, invfact, inv);\n    printf(\"%d\\n\", nCr_mod(10, 3, fact, invfact)); // 120\n    return 0;\n}\n\n\nWhy It Matters\n\nFast combinatorics: \\(\\binom{n}{r}\\), permutations, multinomials in \\(O(1)\\)\nDP under modulo: convolution like transitions, counting paths\nNumber theory: modular inverses and powers ready on demand\nCompetitive programming and crypto prototypes where repeated modular queries are common\n\n\n\nA Gentle Proof (Why It Works)\nFor prime \\(M\\), \\(\\mathbb{Z}_M^\\times\\) is a field. Fermat gives \\(a^{M-2}\\equiv a^{-1}\\pmod M\\) for \\(a \\not\\equiv 0\\). Backward fill yields \\(\\text{invfact}[i-1]=\\text{invfact}[i]\\cdot i \\bmod M\\), hence \\((i-1)!^{-1}\\). Then \\[\n\\binom{n}{r} = \\frac{n!}{r!,(n-r)!} \\equiv \\text{fact}[n]\\cdot \\text{invfact}[r]\\cdot \\text{invfact}[n-r] \\pmod M.\n\\] The linear inverse identity follows from writing \\(i\\cdot \\text{inv}[i]\\equiv 1\\) and recursing on \\(M\\bmod i\\).\n\n\nTry It Yourself\n\nPrecompute up to \\(N=10^7\\) with memory tuning and check that \\(\\sum_{r=0}^n \\binom{n}{r}\\equiv 2^n \\pmod M\\).\nAdd a table of powers powA[i] for a fixed base \\(A\\) to answer \\(A^k \\bmod M\\) in \\(O(1)\\).\nImplement multinomial: \\(\\frac{n!}{a_1!\\cdots a_k!}\\) via fact and invfact.\nFor composite \\(M\\), factor \\(M=\\prod p_i^{e_i}\\), compute modulo each \\(p_i^{e_i}\\), then combine with CRT.\nBenchmark precompute once vs on-demand exponentiation per query.\n\n\n\nTest Cases\n\n\n\nQuery\nAnswer\n\n\n\n\n\\(\\binom{10}{3}\\bmod 10^9+7\\)\n120\n\n\n\\(\\binom{1000}{500}\\bmod 10^9+7\\)\ncomputed in \\(O(1)\\) from tables\n\n\n\\(a^{-1}\\bmod M\\) for \\(a=123456\\)\ninv[a]\n\n\n\\(A^k\\bmod M\\) for many \\(k\\)\npowA[k] if precomputed\n\n\n\n\n\nComplexity\n\nPrecompute: \\(O(N)\\) time, \\(O(N)\\) space\nPer query: \\(O(1)\\)\nOne exp: a single \\(O(\\log M)\\) exponentiation to seed invfact[N] if you choose the backward method\n\nModular precomputation turns heavy arithmetic into lookups. Pay once up front, answer instantly forever after.\n\n\n\n549 Fermat’s Little Theorem\nFermat’s Little Theorem is the cornerstone of modular arithmetic. It states that if \\(p\\) is a prime and \\(a\\) is not divisible by \\(p\\), then:\n\\[\na^{p-1} \\equiv 1 \\pmod p\n\\]\nThis powerful relationship underpins modular inverses, primality tests, and exponentiation optimizations.\n\nWhat Problem Are We Solving?\nWe often need to simplify or invert large modular expressions:\n\nComputing \\(a^{-1} \\bmod p\\)\nSimplifying huge exponents like \\(a^k \\bmod p\\)\nVerifying primality (Fermat, Miller–Rabin tests)\n\nInstead of performing expensive division, Fermat’s theorem gives us:\n\\[\na^{-1} \\equiv a^{p-2} \\pmod p\n\\]\nSo inversion becomes modular exponentiation, achievable in \\(O(\\log p)\\).\n\n\nHow Does It Work (Plain Language)\nWhen \\(p\\) is prime, multiplication modulo \\(p\\) forms a group of \\(p-1\\) elements (excluding \\(0\\)). By Lagrange’s theorem, every element raised to the group size equals the identity:\n\\[\na^{p-1} \\equiv 1 \\pmod p\n\\]\nRearranging gives the modular inverse:\n\\[\na \\cdot a^{p-2} \\equiv 1 \\pmod p\n\\]\nSo \\(a^{p-2}\\) is the inverse of \\(a\\) under mod \\(p\\).\n\n\nExample\nLet \\(a=3\\), \\(p=7\\) (a prime):\n\\[\n3^{6} = 729 \\equiv 1 \\pmod 7\n\\]\nAnd indeed:\n\\[\n3^{5} = 243 \\equiv 5 \\pmod 7\n\\]\nSince \\(3 \\times 5 = 15 \\equiv 1 \\pmod 7\\), \\(5\\) is the modular inverse of \\(3\\) mod \\(7\\).\n\n\nTiny Code (Easy Versions)\nPython Version\ndef modpow(a, e, m):\n    r = 1\n    a %= m\n    while e:\n        if e & 1:\n            r = r * a % m\n        a = a * a % m\n        e &gt;&gt;= 1\n    return r\n\ndef modinv(a, p):\n    return modpow(a, p - 2, p)  # Fermat's little theorem\n\n# Example\np = 7\na = 3\nprint(modpow(a, p - 1, p))  # should be 1\nprint(modinv(a, p))         # should be 5\nC Version\n#include &lt;stdio.h&gt;\n\nlong long modpow(long long a, long long e, long long m) {\n    long long r = 1 % m;\n    a %= m;\n    while (e) {\n        if (e & 1) r = r * a % m;\n        a = a * a % m;\n        e &gt;&gt;= 1;\n    }\n    return r;\n}\n\nlong long modinv(long long a, long long p) {\n    return modpow(a, p - 2, p); // Fermat's Little Theorem\n}\n\nint main(void) {\n    long long a = 3, p = 7;\n    printf(\"a^(p-1) mod p = %lld\\n\", modpow(a, p - 1, p));\n    printf(\"Inverse = %lld\\n\", modinv(a, p));\n}\n\n\nWhy It Matters\n\nModular Inverses: Key for division under modulus (e.g., in combinatorics \\(\\binom{n}{r}\\) mod \\(p\\)).\nExponent Reduction: For large exponents, use periodicity modulo \\(p-1\\).\nPrimality Tests: Forms the basis of Fermat and Miller–Rabin tests.\nRSA & Cryptography: Central to modular arithmetic with primes.\n\n\n\nA Gentle Proof (Why It Works)\nConsider all residues \\({1,2,\\ldots,p-1}\\) modulo prime \\(p\\). Multiplying each by \\(a\\) (where \\(\\gcd(a,p)=1\\)) permutes them. Thus:\n\\[\n1\\cdot2\\cdots(p-1) \\equiv (a\\cdot1)(a\\cdot2)\\cdots(a\\cdot(p-1)) \\pmod p\n\\]\nCancelling \\((p-1)!\\) (nonzero mod \\(p\\) by Wilson’s theorem) yields:\n\\[\na^{p-1} \\equiv 1 \\pmod p\n\\]\n\n\nTry It Yourself\n\nVerify \\(a^{p-1}\\equiv 1\\) for various primes \\(p\\) and bases \\(a\\).\nUse it to compute modular inverses: test \\(a^{p-2}\\) for different \\(a\\).\nCombine with modular exponentiation to speed up combinatorial formulas.\nExplore what fails when \\(p\\) is composite (Fermat pseudoprimes).\nImplement Fermat primality test using \\(a^{p-1}\\bmod p\\).\n\n\n\nTest Cases\n\n\n\n\\(a\\)\n\\(p\\)\n\\(a^{p-1}\\bmod p\\)\n\\(a^{p-2}\\bmod p\\) (inverse)\n\n\n\n\n2\n5\n1\n3\n\n\n3\n7\n1\n5\n\n\n4\n11\n1\n3\n\n\n10\n13\n1\n4\n\n\n\n\n\nComplexity\n\nModular Exponentiation: \\(O(\\log p)\\)\nSpace: \\(O(1)\\)\n\nFermat’s Little Theorem transforms division into exponentiation, bringing algebraic structure into computational arithmetic.\n\n\n\n550 Wilson’s Theorem\nWilson’s Theorem gives a remarkable characterization of prime numbers using factorials:\n\\[\n(p-1)! \\equiv -1 \\pmod p\n\\]\nThat is, for a prime \\(p\\), the factorial of \\((p-1)\\) leaves a remainder of \\(p-1\\) (or equivalently \\(-1\\)) when divided by \\(p\\).\nConversely, if this congruence holds, \\(p\\) must be prime.\n\nWhat Problem Are We Solving?\nWe want a way to test primality or understand modular inverses through factorials.\nWhile it’s not practical for large primes due to factorial growth, Wilson’s theorem is conceptually elegant and connects factorials, inverses, and modular arithmetic beautifully.\nIt shows how the multiplicative structure modulo \\(p\\) is cyclic and symmetric.\n\n\nHow Does It Work (Plain Language)\nFor a prime \\(p\\), every number \\(1,2,\\dots,p-1\\) has a unique inverse modulo \\(p\\), and only \\(1\\) and \\(p-1\\) are self-inverse.\nWhen we multiply all of them together:\n\nEach pair \\(a \\cdot a^{-1}\\) contributes \\(1\\)\n\\(1\\) and \\((p-1)\\) contribute \\(1\\) and \\((p-1)\\)\n\nSo the whole product becomes:\n\\[\n(p-1)! \\equiv 1 \\cdot (p-1) \\equiv -1 \\pmod p\n\\]\n\n\nExample\nLet’s test small primes:\n\n\n\n\\(p\\)\n\\((p-1)!\\)\n\\((p-1)! \\bmod p\\)\nCheck\n\n\n\n\n2\n1\n1 ≡ -1 mod 2\n✔\n\n\n3\n2\n2 ≡ -1 mod 3\n✔\n\n\n5\n24\n24 ≡ -1 mod 5\n✔\n\n\n7\n720\n720 ≡ -1 mod 7\n✔\n\n\n11\n10! = 3628800\n3628800 ≡ -1 mod 11\n✔\n\n\n\nTry a composite \\(p=6\\): \\(5! = 120\\), and \\(120 \\bmod 6 = 0\\) → fails.\nSo Wilson’s condition is both necessary and sufficient for primality.\n\n\nTiny Code (Easy Versions)\nPython Version\ndef factorial_mod(n, m):\n    res = 1\n    for i in range(2, n + 1):\n        res = (res * i) % m\n    return res\n\ndef is_prime_wilson(p):\n    if p &lt; 2:\n        return False\n    return factorial_mod(p - 1, p) == p - 1\n\n# Example\nfor p in [2, 3, 5, 6, 7, 11]:\n    print(p, is_prime_wilson(p))\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nlong long factorial_mod(int n, int mod) {\n    long long res = 1;\n    for (int i = 2; i &lt;= n; i++)\n        res = (res * i) % mod;\n    return res;\n}\n\nbool is_prime_wilson(int p) {\n    if (p &lt; 2) return false;\n    return factorial_mod(p - 1, p) == p - 1;\n}\n\nint main(void) {\n    int ps[] = {2, 3, 5, 6, 7, 11};\n    for (int i = 0; i &lt; 6; i++)\n        printf(\"%d %s\\n\", ps[i], is_prime_wilson(ps[i]) ? \"prime\" : \"composite\");\n}\n\n\nWhy It Matters\nWilson’s Theorem connects combinatorics, modular arithmetic, and primality:\n\nPrimality characterization: \\(p\\) is prime \\(\\iff (p-1)! \\equiv -1 \\pmod p\\)\nProof of group structure: \\((\\mathbb{Z}/p\\mathbb{Z})^\\times\\) is a multiplicative group\nFactorial inverses: \\((p-1)!\\) acts as \\(-1\\), enabling certain modular proofs\n\nThough inefficient for large \\(p\\), it’s conceptually vital in number theory.\n\n\nA Gentle Proof (Why It Works)\nLet \\(p\\) be prime. The set \\({1, 2, \\dots, p-1}\\) under multiplication mod \\(p\\) forms a group.\nEach element \\(a\\) has an inverse \\(a^{-1}\\). Multiplying all elements:\n\\[\n(p-1)! \\equiv \\prod_{a=1}^{p-1} a \\equiv \\prod_{a=1}^{p-1} a^{-1} \\equiv (p-1)!^{-1} \\pmod p\n\\]\nThus:\n\\[\n((p-1)!)^2 \\equiv 1 \\pmod p\n\\]\nSo \\((p-1)! \\equiv \\pm 1\\). If \\((p-1)! \\equiv 1\\), then all numbers are self-inverse, only possible for \\(p=2\\). For \\(p&gt;2\\), it must be \\(-1\\).\nConversely, if \\((p-1)! \\equiv -1\\), \\(p\\) cannot be composite (since composite factorials are \\(0 \\bmod p\\)).\n\n\nTry It Yourself\n\nVerify \\((p-1)! \\bmod p\\) for small primes.\nCheck why it fails for \\(p=4,6,8,9\\).\nExplore what happens mod a composite number (factorial will include factors of \\(p\\)).\nUse it to show \\((p-1)! + 1\\) is divisible by \\(p\\).\nTry optimizing factorial modulo \\(p\\) for small ranges.\n\n\n\nTest Cases\n\n\n\n\\(p\\)\n\\((p-1)!\\)\n\\((p-1)! \\bmod p\\)\nPrime?\n\n\n\n\n2\n1\n1\n✔\n\n\n3\n2\n2\n✔\n\n\n4\n6\n2\n✖\n\n\n5\n24\n4\n✔\n\n\n6\n120\n0\n✖\n\n\n7\n720\n6\n✔\n\n\n\n\n\nComplexity\n\nTime: \\(O(p)\\) (factorial modulo computation)\nSpace: \\(O(1)\\)\n\nThough inefficient for primality testing, Wilson’s Theorem beautifully bridges factorials, inverses, and primes, a gem of elementary number theory.\n\n\n\n\nSection 56. Linear Algebra\n\n551 Gaussian Elimination\nGaussian Elimination is the fundamental algorithm for solving systems of linear equations, computing determinants, and finding matrix rank. It systematically transforms a given matrix into an upper triangular form using row operations, after which solutions can be found by back-substitution.\n\nWhat Problem Are We Solving?\nWe want to solve a system of \\(n\\) linear equations in \\(n\\) variables:\n\\[\nA\\mathbf{x} = \\mathbf{b}\n\\]\nwhere \\(A\\) is an \\(n \\times n\\) matrix, \\(\\mathbf{x}\\) is the vector of unknowns, \\(\\mathbf{b}\\) is the constant vector.\nInstead of guessing or manually substituting, Gaussian elimination gives a systematic, deterministic, and polynomial-time method.\n\n\nHow Does It Work (Plain Language)\nWe perform elementary row operations to simplify the augmented matrix \\([A | b]\\):\n\nForward Elimination\n\nFor each column, select a pivot (nonzero element).\nSwap rows if needed (partial pivoting).\nEliminate all entries below the pivot to make them zero.\n\nBack Substitution\n\nOnce in upper-triangular form, solve from the last equation upward.\n\n\nThis converts the system into: \\[\nU\\mathbf{x} = \\mathbf{c}\n\\] where \\(U\\) is upper triangular, easily solvable.\n\n\nExample\nSolve: \\[\n\\begin{cases}\n2x + y - z = 8 \\\n-3x - y + 2z = -11 \\\n-2x + y + 2z = -3\n\\end{cases}\n\\]\nStep 1: Write the augmented matrix\n\\[\n\\left[\n\\begin{array}{rrr|r}\n2 & 1 & -1 & 8 \\\\\n-3 & -1 & 2 & -11 \\\\\n-2 & 1 & 2 & -3\n\\end{array}\n\\right]\n\\]\nStep 2: Eliminate below first pivot\nUse pivot = 2 (row 1).\n\\[\nR_2 \\gets R_2 + \\tfrac{3}{2}R_1,\\qquad\nR_3 \\gets R_3 + R_1\n\\]\n\\[\n\\left[\n\\begin{array}{rrr|r}\n2 & 1 & -1 & 8 \\\\\n0 & \\tfrac{1}{2} & \\tfrac{1}{2} & 1 \\\\\n0 & 2 & 1 & 5\n\\end{array}\n\\right]\n\\]\nStep 3: Eliminate below second pivot\nPivot = \\(\\tfrac{1}{2}\\) (row 2).\n\\[\nR_3 \\gets R_3 - 4R_2\n\\]\n\\[\n\\left[\n\\begin{array}{rrr|r}\n2 & 1 & -1 & 8 \\\\\n0 & \\tfrac{1}{2} & \\tfrac{1}{2} & 1 \\\\\n0 & 0 & -1 & 1\n\\end{array}\n\\right]\n\\]\nStep 4: Back Substitution\nFrom bottom up:\n\n\\(-z = 1 \\implies z = -1\\)\n\\(0.5y + 0.5z = 1 \\implies y = 3\\)\n\\(2x + y - z = 8 \\implies 2x + 3 + 1 = 8 \\implies x = 2\\)\n\nSolution: \\((x, y, z) = (2, 3, -1)\\)\n\n\nTiny Code (Easy Versions)\nPython Version\ndef gaussian_elimination(a, b):\n    n = len(a)\n    for i in range(n):\n        # Pivot\n        max_row = max(range(i, n), key=lambda r: abs(a[r][i]))\n        a[i], a[max_row] = a[max_row], a[i]\n        b[i], b[max_row] = b[max_row], b[i]\n\n        # Eliminate below\n        for j in range(i + 1, n):\n            factor = a[j][i] / a[i][i]\n            for k in range(i, n):\n                a[j][k] -= factor * a[i][k]\n            b[j] -= factor * b[i]\n\n    # Back substitution\n    x = [0] * n\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - sum(a[i][j] * x[j] for j in range(i + 1, n))) / a[i][i]\n    return x\n\nA = [[2, 1, -1], [-3, -1, 2], [-2, 1, 2]]\nB = [8, -11, -3]\nprint(gaussian_elimination(A, B))  # [2.0, 3.0, -1.0]\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define N 3\n\nvoid gaussian_elimination(double a[N][N], double b[N], double x[N]) {\n    for (int i = 0; i &lt; N; i++) {\n        // Pivot\n        int max_row = i;\n        for (int r = i + 1; r &lt; N; r++)\n            if (fabs(a[r][i]) &gt; fabs(a[max_row][i]))\n                max_row = r;\n        for (int c = 0; c &lt; N; c++) {\n            double tmp = a[i][c];\n            a[i][c] = a[max_row][c];\n            a[max_row][c] = tmp;\n        }\n        double tmp = b[i]; b[i] = b[max_row]; b[max_row] = tmp;\n\n        // Eliminate\n        for (int j = i + 1; j &lt; N; j++) {\n            double factor = a[j][i] / a[i][i];\n            for (int k = i; k &lt; N; k++)\n                a[j][k] -= factor * a[i][k];\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = N - 1; i &gt;= 0; i--) {\n        double sum = 0;\n        for (int j = i + 1; j &lt; N; j++)\n            sum += a[i][j] * x[j];\n        x[i] = (b[i] - sum) / a[i][i];\n    }\n}\n\nint main() {\n    double A[N][N] = {{2, 1, -1}, {-3, -1, 2}, {-2, 1, 2}};\n    double B[N] = {8, -11, -3}, X[N];\n    gaussian_elimination(A, B, X);\n    printf(\"x = %.2f, y = %.2f, z = %.2f\\n\", X[0], X[1], X[2]);\n}\n\n\nWhy It Matters\nGaussian elimination is foundational in:\n\nSolving \\(A\\mathbf{x}=\\mathbf{b}\\)\nFinding determinants (\\(\\det(A)\\) is product of pivots)\nFinding inverse matrices (by applying elimination to \\([A|I]\\))\nComputing matrix rank (count nonzero rows after elimination)\n\nIt underpins linear algebra libraries (BLAS/LAPACK), numerical solvers, and symbolic computation.\n\n\nA Gentle Proof (Why It Works)\nEach elementary row operation corresponds to multiplication by an invertible matrix \\(E_i\\). After a sequence: \\[\nE_k \\cdots E_1 A = U\n\\] where \\(U\\) is upper triangular. Then: \\[\nA = E_1^{-1}\\cdots E_k^{-1}U\n\\] so the system \\(A\\mathbf{x}=\\mathbf{b}\\) becomes \\(U\\mathbf{x} = (E_k\\cdots E_1)\\mathbf{b}\\), which is solvable by back substitution.\nEach step preserves solution space, ensuring correctness.\n\n\nTry It Yourself\n\nSolve a 3×3 system using Gaussian elimination manually.\nModify the algorithm to return determinant = product of pivots.\nExtend to augmented matrix to compute inverse.\nAdd partial pivoting to handle zero pivots.\nCompare with matrix decomposition (LU).\n\n\n\nTest Cases\n\n\n\nSystem\nSolution\n\n\n\n\n\\(2x+y-z=8,\\ -3x-y+2z=-11,\\ -2x+y+2z=-3\\)\n\\((2,3,-1)\\)\n\n\n\\(x+y=2,\\ 2x-y=0\\)\n\\((2/3,4/3)\\)\n\n\n\\(x-y=1,\\ 2x+y=4\\)\n\\((5/3,2/3)\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\)\nSpace: \\(O(n^2)\\)\n\nGaussian Elimination is the workhorse of linear algebra, every advanced solver builds upon it.\n\n\n\n552 Gauss–Jordan Elimination\nGauss–Jordan Elimination extends Gaussian elimination by continuing the reduction process until the matrix is in reduced row echelon form (RREF), not just upper-triangular. This makes it ideal for finding inverses, testing linear independence, and solving systems directly without back-substitution.\n\nWhat Problem Are We Solving?\nWe want a full, systematic method to:\n\nSolve \\(A\\mathbf{x} = \\mathbf{b}\\)\nFind \\(A^{-1}\\) (inverse matrix)\nIdentify rank, null space, and pivots\n\nInstead of stopping at an upper-triangular system (as in Gaussian elimination), we go further to make every pivot \\(1\\) and clear all entries above and below it.\n\n\nHow Does It Work (Plain Language)\n\nForm the augmented matrix Combine \\(A\\) and \\(\\mathbf{b}\\): \\([A | \\mathbf{b}]\\)\nForward elimination For each pivot column:\n\nChoose pivot (swap if needed)\nScale row so pivot = 1\nEliminate below (make zeros under pivot)\n\nBackward elimination For each pivot column (starting from last):\n\nEliminate above (make zeros above pivot)\n\n\nAt the end, \\(A\\) becomes the identity matrix and the right-hand side gives the solution vector.\nIf augmenting with \\(I\\), the right-hand side becomes \\(A^{-1}\\).\n\n\nExample\nSolve:\n\\[\n\\begin{cases}\nx + y + z = 6 \\\\\n2y + 5z = -4 \\\\\n2x + 5y - z = 27\n\\end{cases}\n\\]\nStep 1: Write augmented matrix\n\\[\n\\left[\n\\begin{array}{rrr|r}\n1 & 1 & 1 & 6 \\\\\n0 & 2 & 5 & -4 \\\\\n2 & 5 & -1 & 27\n\\end{array}\n\\right]\n\\]\nStep 2: Eliminate below first pivot\n\\(R_3 \\gets R_3 - 2R_1\\)\n\\[\n\\left[\n\\begin{array}{rrr|r}\n1 & 1 & 1 & 6 \\\\\n0 & 2 & 5 & -4 \\\\\n0 & 3 & -3 & 15\n\\end{array}\n\\right]\n\\]\nStep 3: Pivot at row 2\n\\(R_2 \\gets \\tfrac{1}{2}R_2\\)\n\\[\n\\left[\n\\begin{array}{rrr|r}\n1 & 1 & 1 & 6 \\\\\n0 & 1 & \\tfrac{5}{2} & -2 \\\\\n0 & 3 & -3 & 15\n\\end{array}\n\\right]\n\\]\nStep 4: Eliminate below and above the second pivot\n\\(R_3 \\gets R_3 - 3R_2,\\quad R_1 \\gets R_1 - R_2\\)\n\\[\n\\left[\n\\begin{array}{rrr|r}\n1 & 0 & -\\tfrac{3}{2} & 8 \\\\\n0 & 1 & \\tfrac{5}{2} & -2 \\\\\n0 & 0 & -\\tfrac{21}{2} & 21\n\\end{array}\n\\right]\n\\]\nNormalize third pivot\n\\(R_3 \\gets -\\tfrac{2}{21} R_3\\)\n\\[\n\\left[\n\\begin{array}{rrr|r}\n1 & 0 & -\\tfrac{3}{2} & 8 \\\\\n0 & 1 & \\tfrac{5}{2} & -2 \\\\\n0 & 0 & 1 & -2\n\\end{array}\n\\right]\n\\]\nEliminate above the third pivot\n\\(R_1 \\gets R_1 + \\tfrac{3}{2}R_3,\\quad R_2 \\gets R_2 - \\tfrac{5}{2}R_3\\)\n\\[\n\\left[\n\\begin{array}{rrr|r}\n1 & 0 & 0 & 5 \\\\\n0 & 1 & 0 & 3 \\\\\n0 & 0 & 1 & -2\n\\end{array}\n\\right]\n\\]\nSolution:\n\\[\nx=5,\\quad y=3,\\quad z=-2\n\\]\n\n\nTiny Code (Easy Versions)\nPython Version\ndef gauss_jordan(a, b):\n    n = len(a)\n    # Augment A with b\n    for i in range(n):\n        a[i].append(b[i])\n\n    for i in range(n):\n        # Pivot selection\n        max_row = max(range(i, n), key=lambda r: abs(a[r][i]))\n        a[i], a[max_row] = a[max_row], a[i]\n\n        # Normalize pivot row\n        pivot = a[i][i]\n        for j in range(i, n + 1):\n            a[i][j] /= pivot\n\n        # Eliminate all other rows\n        for k in range(n):\n            if k != i:\n                factor = a[k][i]\n                for j in range(i, n + 1):\n                    a[k][j] -= factor * a[i][j]\n\n    # Extract solution\n    return [a[i][n] for i in range(n)]\n\nA = [[1, 1, 1],\n     [0, 2, 5],\n     [2, 5, -1]]\nB = [6, -4, 27]\nprint(gauss_jordan(A, B))  # [5.0, 3.0, -2.0]\nC Version\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define N 3\n\nvoid gauss_jordan(double a[N][N+1]) {\n    for (int i = 0; i &lt; N; i++) {\n        // Pivot selection\n        int max_row = i;\n        for (int r = i + 1; r &lt; N; r++)\n            if (fabs(a[r][i]) &gt; fabs(a[max_row][i]))\n                max_row = r;\n        for (int c = 0; c &lt;= N; c++) {\n            double tmp = a[i][c];\n            a[i][c] = a[max_row][c];\n            a[max_row][c] = tmp;\n        }\n\n        // Normalize pivot row\n        double pivot = a[i][i];\n        for (int c = 0; c &lt;= N; c++)\n            a[i][c] /= pivot;\n\n        // Eliminate other rows\n        for (int r = 0; r &lt; N; r++) {\n            if (r != i) {\n                double factor = a[r][i];\n                for (int c = 0; c &lt;= N; c++)\n                    a[r][c] -= factor * a[i][c];\n            }\n        }\n    }\n}\n\nint main() {\n    double A[N][N+1] = {\n        {1, 1, 1, 6},\n        {0, 2, 5, -4},\n        {2, 5, -1, 27}\n    };\n    gauss_jordan(A);\n    for (int i = 0; i &lt; N; i++)\n        printf(\"x%d = %.2f\\n\", i + 1, A[i][N]);\n}\n\n\nWhy It Matters\nGauss–Jordan Elimination is versatile:\n\nDirect solution without back-substitution\nMatrix inversion by applying to \\([A|I]\\)\nRank computation (number of pivots)\nLinear independence testing\n\nIt’s conceptually clear and forms the basis for high-level linear algebra routines.\n\n\nA Gentle Proof (Why It Works)\nEach row operation corresponds to multiplication by an invertible matrix \\(E_i\\). After full reduction: \\[\nE_k \\cdots E_1 [A | I] = [I | A^{-1}]\n\\] Thus, \\(A^{-1} = E_k \\cdots E_1\\). The method transforms \\(A\\) into \\(I\\) through reversible operations, so the right-hand side evolves into \\(A^{-1}\\).\nFor \\(A\\mathbf{x} = \\mathbf{b}\\), augmenting \\(A\\) with \\(\\mathbf{b}\\) gives the unique solution vector.\n\n\nTry It Yourself\n\nSolve \\(A\\mathbf{x}=\\mathbf{b}\\) using full RREF.\nAugment \\(A\\) with \\(I\\) and compute \\(A^{-1}\\).\nCount nonzero rows to find rank.\nImplement with partial pivoting for stability.\nCompare with LU decomposition in performance.\n\n\n\nTest Cases\n\n\n\nSystem\nSolution\n\n\n\n\n\\(x+y+z=6,\\ 2y+5z=-4,\\ 2x+5y-z=27\\)\n\\((5,3,-2)\\)\n\n\n\\(x+y=2,\\ 3x-2y=1\\)\n\\((1,1)\\)\n\n\n\\(2x+y=5,\\ 4x-2y=6\\)\n\\((2,1)\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\)\nSpace: \\(O(n^2)\\)\n\nGauss–Jordan is a complete solver, producing identity on the left, and solution or inverse on the right, all in one pass.\n\n\n\n553 LU Decomposition\nLU Decomposition factors a matrix \\(A\\) into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\):\n\\[\nA = L \\cdot U\n\\]\nThis factorization is a workhorse of numerical linear algebra. Once \\(A\\) is decomposed, we can solve systems \\(A\\mathbf{x}=\\mathbf{b}\\) quickly for multiple right-hand sides by forward and backward substitution.\n\nWhat Problem Are We Solving?\nWe want to solve \\(A\\mathbf{x}=\\mathbf{b}\\) efficiently and repeatedly.\nGaussian elimination works once, but LU decomposition reuses the factorization:\n\nFirst solve \\(L\\mathbf{y}=\\mathbf{b}\\) (forward substitution)\nThen solve \\(U\\mathbf{x}=\\mathbf{y}\\) (back substitution)\n\nAlso useful for:\n\nDeterminant computation (\\(\\det(A)=\\det(L)\\det(U)\\))\nMatrix inversion\nNumerical stability with pivoting (\\(PA=LU\\))\n\n\n\nHow Does It Work (Plain Language)\nLU decomposition performs the same operations as Gaussian elimination but records the multipliers in \\(L\\).\n\nInitialize\n\n\\(L\\) as identity\n\\(U\\) as a copy of \\(A\\)\n\nEliminate below each pivot\n\nFor \\(i\\) from \\(0\\) to \\(n-1\\):\n\nFor \\(j&gt;i\\): \\(L[j][i] = U[j][i] / U[i][i]\\) Subtract \\(L[j][i] \\times\\) (row \\(i\\)) from row \\(j\\) of \\(U\\)\n\n\n\nAt the end:\n\n\\(L\\) has ones on the diagonal and multipliers below.\n\\(U\\) is upper triangular.\n\nIf \\(A\\) requires row swaps for stability, we include a permutation matrix \\(P\\): \\[\nPA = LU\n\\]\n\n\nExample\nDecompose \\[\nA = \\begin{bmatrix}\n2 & 3 & 1\\\n4 & 7 & 7\\\n-2 & 4 & 5\n\\end{bmatrix}\n\\]\nStep 1: Pivot at \\(a_{11}=2\\)\nEliminate below:\n\nRow 2: \\(L_{21}=4/2=2\\) → Row2 = Row2 - 2*Row1\nRow 3: \\(L_{31}=-2/2=-1\\) → Row3 = Row3 + Row1\n\n\\(L = \\begin{bmatrix}\n1 & 0 & 0\\\n2 & 1 & 0\\\n-1 & 0 & 1\n\\end{bmatrix},\\\nU = \\begin{bmatrix}\n2 & 3 & 1\\\n0 & 1 & 5\\\n0 & 7 & 6\n\\end{bmatrix}\\)\nStep 2: Pivot at \\(U_{22}=1\\)\nEliminate below:\n\nRow3: \\(L_{32}=7/1=7\\) → Row3 = Row3 - 7*Row2\n\n\\(L = \\begin{bmatrix}\n1 & 0 & 0\\\n2 & 1 & 0\\\n-1 & 7 & 1\n\\end{bmatrix},\\\nU = \\begin{bmatrix}\n2 & 3 & 1\\\n0 & 1 & 5\\\n0 & 0 & -29\n\\end{bmatrix}\\)\nCheck: \\(A = L \\cdot U\\)\n\n\nTiny Code (Easy Versions)\nPython Version\ndef lu_decompose(A):\n    n = len(A)\n    L = [[0]*n for _ in range(n)]\n    U = [[0]*n for _ in range(n)]\n\n    for i in range(n):\n        L[i][i] = 1\n\n    for i in range(n):\n        # Upper Triangular\n        for k in range(i, n):\n            U[i][k] = A[i][k] - sum(L[i][j] * U[j][k] for j in range(i))\n        # Lower Triangular\n        for k in range(i + 1, n):\n            L[k][i] = (A[k][i] - sum(L[k][j] * U[j][i] for j in range(i))) / U[i][i]\n    return L, U\n\nA = [\n    [2, 3, 1],\n    [4, 7, 7],\n    [-2, 4, 5]\n$$\nL, U = lu_decompose(A)\nprint(\"L =\", L)\nprint(\"U =\", U)\nC Version\n#include &lt;stdio.h&gt;\n\n#define N 3\n\nvoid lu_decompose(double A[N][N], double L[N][N], double U[N][N]) {\n    for (int i = 0; i &lt; N; i++) {\n        for (int j = 0; j &lt; N; j++) {\n            L[i][j] = (i == j) ? 1 : 0;\n            U[i][j] = 0;\n        }\n    }\n\n    for (int i = 0; i &lt; N; i++) {\n        for (int k = i; k &lt; N; k++) {\n            double sum = 0;\n            for (int j = 0; j &lt; i; j++)\n                sum += L[i][j] * U[j][k];\n            U[i][k] = A[i][k] - sum;\n        }\n        for (int k = i + 1; k &lt; N; k++) {\n            double sum = 0;\n            for (int j = 0; j &lt; i; j++)\n                sum += L[k][j] * U[j][i];\n            L[k][i] = (A[k][i] - sum) / U[i][i];\n        }\n    }\n}\n\nint main(void) {\n    double A[N][N] = {{2,3,1},{4,7,7},{-2,4,5}}, L[N][N], U[N][N];\n    lu_decompose(A, L, U);\n\n    printf(\"L:\\n\");\n    for (int i=0;i&lt;N;i++){ for(int j=0;j&lt;N;j++) printf(\"%6.2f \",L[i][j]); printf(\"\\n\"); }\n    printf(\"U:\\n\");\n    for (int i=0;i&lt;N;i++){ for(int j=0;j&lt;N;j++) printf(\"%6.2f \",U[i][j]); printf(\"\\n\"); }\n}\n\n\nWhy It Matters\n\nFast solving: Reuse \\(LU\\) to solve for multiple \\(\\mathbf{b}\\)\nDeterminant: \\(\\det(A)=\\prod_i U_{ii}\\)\nInverse: Solve \\(LU\\mathbf{x}=\\mathbf{e}_i\\) for each \\(i\\)\nFoundation: Basis of Cholesky, Crout, and Doolittle variants\nStability: Combine with pivoting for robustness (\\(PA=LU\\))\n\n\n\nA Gentle Proof (Why It Works)\nEach elimination step corresponds to multiplying \\(A\\) by an elementary lower-triangular matrix \\(E_i\\). After all steps: \\[\nU = E_k E_{k-1} \\cdots E_1 A\n\\] Then \\[\nA = E_1^{-1} E_2^{-1} \\cdots E_k^{-1} U\n\\] Let \\[\nL = E_1^{-1}E_2^{-1}\\cdots E_k^{-1}\n\\] so \\(A = L U\\), with \\(L\\) lower-triangular (unit diagonal) and \\(U\\) upper-triangular.\n\n\nTry It Yourself\n\nPerform LU decomposition on a \\(3\\times3\\) matrix by hand.\nVerify \\(A = L \\cdot U\\) by multiplication.\nSolve \\(A\\mathbf{x}=\\mathbf{b}\\) via forward + backward substitution.\nImplement determinant computation via \\(\\prod U_{ii}\\).\nAdd partial pivoting (compute \\(P,L,U\\)).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(L\\)\n\\(U\\)\n\n\n\n\n\\(\\begin{bmatrix} 2 & 3 \\\\ 4 & 7 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 & 3 \\\\ 0 & 1 \\end{bmatrix}\\)\n\n\n\\(\\begin{bmatrix} 1 & 1 \\\\ 2 & 3 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\) (factorization)\nSolve: \\(O(n^2)\\) per right-hand side\nSpace: \\(O(n^2)\\)\n\nLU decomposition is the backbone of numerical linear algebra, turning Gaussian elimination into a reusable, modular tool.\n\n\n\n554 Cholesky Decomposition\nCholesky Decomposition is a special case of LU decomposition for symmetric positive definite (SPD) matrices. It factors a matrix \\(A\\) into the product of a lower triangular matrix \\(L\\) and its transpose:\n\\[\nA = L \\cdot L^{T}\n\\]\nThis method is twice as efficient as LU decomposition and numerically more stable for SPD matrices, a favorite in optimization, machine learning, and statistics.\n\nWhat Problem Are We Solving?\nWe want to solve \\(A\\mathbf{x}=\\mathbf{b}\\) efficiently when \\(A\\) is symmetric (\\(A=A^T\\)) and positive definite (\\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x}\\neq0\\)).\nInstead of general elimination, we exploit symmetry to reduce work by half.\nOnce we find \\(L\\), we solve:\n\nForward: \\(L\\mathbf{y}=\\mathbf{b}\\)\nBackward: \\(L^{T}\\mathbf{x}=\\mathbf{y}\\)\n\n\n\nHow Does It Work (Plain Language)\nWe build \\(L\\) row by row (or column by column), using the formulas:\nFor diagonal elements: \\[\nL_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1}L_{ik}^2}\n\\]\nFor off-diagonal elements: \\[\nL_{ij} = \\frac{1}{L_{jj}}\\Big(A_{ij} - \\sum_{k=1}^{j-1}L_{ik}L_{jk}\\Big), \\quad i&gt;j\n\\]\nThe upper half is just the transpose of \\(L\\).\n\n\nExample\nGiven \\[\nA =\n\\begin{bmatrix}\n4 & 12 & -16\\\n12 & 37 & -43\\\n-16 & -43 & 98\n\\end{bmatrix}\n\\]\nStep 1: \\(L_{11} = \\sqrt{4} = 2\\)\nStep 2: \\(L_{21} = 12/2 = 6\\), \\(L_{31} = -16/2 = -8\\)\nStep 3: \\(L_{22} = \\sqrt{37 - 6^2} = \\sqrt{1} = 1\\)\nStep 4: \\(L_{32} = \\frac{-43 - (-8)(6)}{1} = 5\\)\nStep 5: \\(L_{33} = \\sqrt{98 - (-8)^2 - 5^2} = \\sqrt{9} = 3\\)\nSo: \\[\nL =\n\\begin{bmatrix}\n2 & 0 & 0\\\n6 & 1 & 0\\\n-8 & 5 & 3\n\\end{bmatrix}\n,\\quad\nA = L \\cdot L^{T}\n\\]\n\n\nTiny Code (Easy Versions)\nPython\nimport math\n\ndef cholesky(A):\n    n = len(A)\n    L = [[0]*n for _ in range(n)]\n    for i in range(n):\n        for j in range(i+1):\n            s = sum(L[i][k]*L[j][k] for k in range(j))\n            if i == j:\n                L[i][j] = math.sqrt(A[i][i] - s)\n            else:\n                L[i][j] = (A[i][j] - s) / L[j][j]\n    return L\n\nA = [\n    [4, 12, -16],\n    [12, 37, -43],\n    [-16, -43, 98]\n$$\n\nL = cholesky(A)\nfor row in L:\n    print(row)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define N 3\n\nvoid cholesky(double A[N][N], double L[N][N]) {\n    for (int i = 0; i &lt; N; i++) {\n        for (int j = 0; j &lt;= i; j++) {\n            double sum = 0;\n            for (int k = 0; k &lt; j; k++)\n                sum += L[i][k] * L[j][k];\n            if (i == j)\n                L[i][j] = sqrt(A[i][i] - sum);\n            else\n                L[i][j] = (A[i][j] - sum) / L[j][j];\n        }\n    }\n}\n\nint main(void) {\n    double A[N][N] = {\n        {4, 12, -16},\n        {12, 37, -43},\n        {-16, -43, 98}\n    }, L[N][N] = {0};\n\n    cholesky(A, L);\n\n    printf(\"L:\\n\");\n    for (int i = 0; i &lt; N; i++) {\n        for (int j = 0; j &lt; N; j++)\n            printf(\"%8.3f \", L[i][j]);\n        printf(\"\\n\");\n    }\n}\n\n\nWhy It Matters\n\nHalf the work of LU decomposition.\nNumerically stable for SPD matrices.\nEssential for:\n\nLeast squares regression\nKalman filters\nGaussian processes\nCovariance matrix decomposition\n\n\n\n\nA Gentle Proof (Why It Works)\nFor an SPD matrix \\(A\\), all leading principal minors are positive. This guarantees every diagonal pivot (\\(A_{ii} - \\sum L_{ik}^2\\)) is positive, so we can take square roots.\nHence \\(L\\) exists and is unique.\nWe can see \\(A = L L^T\\) because every element \\(A_{ij}\\) is reproduced by summing the dot product of row \\(i\\) and row \\(j\\) of \\(L\\).\n\n\nTry It Yourself\n\nDecompose a \\(3\\times3\\) SPD matrix by hand.\nMultiply \\(L L^T\\) to verify.\nUse it to solve \\(A\\mathbf{x}=\\mathbf{b}\\).\nCompare runtime with LU decomposition.\nTest on a covariance matrix (e.g. symmetric, positive).\n\n\n\nTest Case\n\\[\nA =\n\\begin{bmatrix}\n25 & 15 & -5\\\n15 & 18 &  0\\\n-5 &  0 & 11\n\\end{bmatrix}\n\\Rightarrow\nL =\n\\begin{bmatrix}\n5 & 0 & 0\\\n3 & 3 & 0\\\n-1 & 1 & 3\n\\end{bmatrix}\n\\]\nCheck: \\(L L^T = A\\)\n\n\nComplexity\n\nTime: \\(O(n^3/3)\\)\nSolve: \\(O(n^2)\\)\nSpace: \\(O(n^2)\\)\n\nCholesky decomposition is your go-to method for fast, stable, and symmetric systems, a cornerstone in numerical analysis and machine learning.\n\n\n\n555 QR Decomposition\nQR Decomposition breaks a matrix \\(A\\) into two orthogonal factors:\n\\[\nA = Q \\cdot R\n\\]\nwhere \\(Q\\) is orthogonal (\\(Q^T Q = I\\)) and \\(R\\) is upper triangular. This decomposition is key in solving least squares problems, eigenvalue computations, and orthogonalization tasks.\n\nWhat Problem Are We Solving?\nWe often need to solve \\(A\\mathbf{x} = \\mathbf{b}\\) when \\(A\\) is not square (e.g. \\(m &gt; n\\)). Instead of normal equations \\((A^TA)\\mathbf{x}=A^T\\mathbf{b}\\), QR gives a more stable solution:\n\\[\nA = Q R \\implies R \\mathbf{x} = Q^T \\mathbf{b}\n\\]\nNo need to form \\(A^T A\\), which can amplify numerical errors.\n\n\nHow Does It Work (Plain Language)\nQR decomposition orthogonalizes the columns of \\(A\\) step by step:\n\nStart with columns \\(a_1, a_2, \\ldots, a_n\\) of \\(A\\)\nBuild orthogonal basis \\(q_1, q_2, \\ldots, q_n\\) using Gram–Schmidt\nNormalize to get orthonormal columns of \\(Q\\)\nCompute R as projection coefficients\n\nFor each \\(i\\): \\[\nr_{ii} = |a_i - \\sum_{j=1}^{i-1}r_{ji}q_j|, \\quad q_i = \\frac{a_i - \\sum_{j=1}^{i-1}r_{ji}q_j}{r_{ii}}\n\\]\nCompactly:\n\n\\(Q\\): orthonormal basis\n\\(R\\): upper-triangular coefficients\n\nVariants:\n\nClassical Gram–Schmidt (CGS): simple but unstable\nModified Gram–Schmidt (MGS): more stable\nHouseholder reflections: best for numerical accuracy\n\n\n\nExample\nLet \\[\nA =\n\\begin{bmatrix}\n1 & 1 \\\n1 & -1 \\\n1 & 1\n\\end{bmatrix}\n\\]\nStep 1: Take \\(a_1 = (1, 1, 1)^T\\)\n\\[\nq_1 = \\frac{a_1}{|a_1|} = \\frac{1}{\\sqrt{3}}(1, 1, 1)\n\\]\nStep 2: Remove projection from \\(a_2\\)\n\\[\nr_{12} = q_1^T a_2 = \\frac{1}{\\sqrt{3}}(1 + (-1) + 1) = \\frac{1}{\\sqrt{3}}\n\\]\n\\[\nu_2 = a_2 - r_{12} q_1 = (1, -1, 1) - \\frac{1}{\\sqrt{3}}\\cdot\\frac{1}{\\sqrt{3}}(1,1,1) = (1-\\tfrac{1}{3}, -1-\\tfrac{1}{3}, 1-\\tfrac{1}{3})\n\\]\n\\[\nu_2 = \\left(\\frac{2}{3}, -\\frac{4}{3}, \\frac{2}{3}\\right)\n\\]\nNormalize:\n\\[\nq_2 = \\frac{u_2}{|u_2|} = \\frac{1}{\\sqrt{8/3}} \\left(\\frac{2}{3}, -\\frac{4}{3}, \\frac{2}{3}\\right) = \\frac{1}{\\sqrt{6}}(1, -2, 1)\n\\]\nThen \\(Q = [q_1\\ q_2]\\), \\(R = Q^T A\\)\n\n\nTiny Code (Easy Versions)\nPython\nimport numpy as np\n\ndef qr_decompose(A):\n    A = np.array(A, dtype=float)\n    m, n = A.shape\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n\n    for i in range(n):\n        v = A[:, i]\n        for j in range(i):\n            R[j, i] = np.dot(Q[:, j], A[:, i])\n            v = v - R[j, i] * Q[:, j]\n        R[i, i] = np.linalg.norm(v)\n        Q[:, i] = v / R[i, i]\n    return Q, R\n\nA = [[1, 1], [1, -1], [1, 1]]\nQ, R = qr_decompose(A)\nprint(\"Q =\\n\", Q)\nprint(\"R =\\n\", R)\nC (Simplified Gram–Schmidt)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define M 3\n#define N 2\n\nvoid qr_decompose(double A[M][N], double Q[M][N], double R[N][N]) {\n    for (int i = 0; i &lt; N; i++) {\n        for (int k = 0; k &lt; M; k++)\n            Q[k][i] = A[k][i];\n        for (int j = 0; j &lt; i; j++) {\n            R[j][i] = 0;\n            for (int k = 0; k &lt; M; k++)\n                R[j][i] += Q[k][j] * A[k][i];\n            for (int k = 0; k &lt; M; k++)\n                Q[k][i] -= R[j][i] * Q[k][j];\n        }\n        R[i][i] = 0;\n        for (int k = 0; k &lt; M; k++)\n            R[i][i] += Q[k][i] * Q[k][i];\n        R[i][i] = sqrt(R[i][i]);\n        for (int k = 0; k &lt; M; k++)\n            Q[k][i] /= R[i][i];\n    }\n}\n\nint main(void) {\n    double A[M][N] = {{1,1},{1,-1},{1,1}}, Q[M][N], R[N][N];\n    qr_decompose(A, Q, R);\n\n    printf(\"Q:\\n\");\n    for(int i=0;i&lt;M;i++){ for(int j=0;j&lt;N;j++) printf(\"%8.3f \", Q[i][j]); printf(\"\\n\"); }\n\n    printf(\"R:\\n\");\n    for(int i=0;i&lt;N;i++){ for(int j=0;j&lt;N;j++) printf(\"%8.3f \", R[i][j]); printf(\"\\n\"); }\n}\n\n\nWhy It Matters\n\nNumerical stability for least squares\nOrthogonal basis for column space\nEigenvalue algorithms (QR iteration)\nMachine learning: regression, PCA\nSignal processing: orthogonalization, projections\n\n\n\nA Gentle Proof (Why It Works)\nEvery full-rank matrix \\(A\\) with independent columns can be written as: \\[\nA = [a_1, a_2, \\ldots, a_n] = [q_1, q_2, \\ldots, q_n] R\n\\]\nEach \\(a_i\\) is expressed as a linear combination of the orthogonal basis vectors \\(q_j\\): \\[\na_i = \\sum_{j=1}^{i} r_{ji} q_j\n\\]\nCollecting \\(q_j\\) as columns of \\(Q\\) gives \\(A=QR\\).\n\n\nTry It Yourself\n\nOrthogonalize two 3D vectors manually.\nVerify \\(Q^T Q = I\\).\nCompute \\(R = Q^T A\\).\nUse \\(QR\\) to solve an overdetermined system.\nCompare classical vs modified Gram–Schmidt.\n\n\n\nTest Case\n\\[\nA =\n\\begin{bmatrix}\n1 & 1\\\n1 & 0\\\n0 & 1\n\\end{bmatrix}\n,\\quad\nQ =\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}}\\\n0 & \\frac{2}{\\sqrt{6}}\n\\end{bmatrix}\n,\\quad\nR =\n\\begin{bmatrix}\n\\sqrt{2} & \\frac{1}{\\sqrt{2}}\\\n0 & \\sqrt{\\frac{3}{2}}\n\\end{bmatrix}\n\\]\nCheck: \\(A = Q R\\)\n\n\nComplexity\n\nTime: \\(O(mn^2)\\)\nSpace: \\(O(mn)\\)\nStability: High (especially with Householder reflections)\n\nQR decomposition is your orthogonal compass, guiding you to stable, geometric solutions in least squares, PCA, and spectral algorithms.\n\n\n\n556 Matrix Inversion (Gauss–Jordan Method)\nMatrix Inversion finds a matrix \\(A^{-1}\\) such that\n\\[\nA \\cdot A^{-1} = I\n\\]\nThis operation is fundamental for solving systems, transforming spaces, and expressing linear mappings. While direct inversion is rarely used in practice (solving \\(A\\mathbf{x}=\\mathbf{b}\\) is cheaper), learning how to compute it reveals the structure of linear algebra itself.\n\nWhat Problem Are We Solving?\nWe want to find \\(A^{-1}\\), the matrix that undoes \\(A\\). Once we have \\(A^{-1}\\), any system \\(A\\mathbf{x}=\\mathbf{b}\\) can be solved simply by:\n\\[\n\\mathbf{x} = A^{-1} \\mathbf{b}\n\\]\nBut inversion is only defined when \\(A\\) is square and non-singular (i.e. \\(\\det(A)\\neq0\\)).\n\n\nHow Does It Work (Plain Language)\nThe Gauss–Jordan method augments \\(A\\) with the identity matrix \\(I\\), then performs row operations to transform \\(A\\) into \\(I\\). Whatever \\(I\\) becomes on the right-hand side is \\(A^{-1}\\).\nStep-by-step:\n\nForm the augmented matrix \\([A | I]\\)\nApply row operations to make \\(A\\) into \\(I\\)\nThe right half becomes \\(A^{-1}\\)\n\n\n\nExample\nLet \\[\nA=\n\\begin{bmatrix}\n2 & 1\\\\\n5 & 3\n\\end{bmatrix}.\n\\]\nAugment with the identity: \\[\n\\left[\n\\begin{array}{cc|cc}\n2 & 1 & 1 & 0\\\\\n5 & 3 & 0 & 1\n\\end{array}\n\\right].\n\\]\nStep 1: \\(R_1 \\gets \\tfrac{1}{2}R_1\\) \\[\n\\left[\n\\begin{array}{cc|cc}\n1 & \\tfrac{1}{2} & \\tfrac{1}{2} & 0\\\\\n5 & 3 & 0 & 1\n\\end{array}\n\\right].\n\\]\nStep 2: \\(R_2 \\gets R_2 - 5R_1\\) \\[\n\\left[\n\\begin{array}{cc|cc}\n1 & \\tfrac{1}{2} & \\tfrac{1}{2} & 0\\\\\n0 & \\tfrac{1}{2} & -\\tfrac{5}{2} & 1\n\\end{array}\n\\right].\n\\]\nStep 3: \\(R_2 \\gets 2R_2\\) \\[\n\\left[\n\\begin{array}{cc|cc}\n1 & \\tfrac{1}{2} & \\tfrac{1}{2} & 0\\\\\n0 & 1 & -5 & 2\n\\end{array}\n\\right].\n\\]\nStep 4: \\(R_1 \\gets R_1 - \\tfrac{1}{2}R_2\\) \\[\n\\left[\n\\begin{array}{cc|cc}\n1 & 0 & 3 & -1\\\\\n0 & 1 & -5 & 2\n\\end{array}\n\\right].\n\\]\nThus \\[\nA^{-1}=\n\\begin{bmatrix}\n3 & -1\\\\\n-5 & 2\n\\end{bmatrix}.\n\\]\nCheck: \\[\nA\\,A^{-1}=\n\\begin{bmatrix}\n2 & 1\\\\\n5 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n3 & -1\\\\\n-5 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\n\\end{bmatrix}.\n\\]\n\n\nTiny Code (Easy Versions)\nPython\ndef invert_matrix(A):\n    n = len(A)\n    # Augment with identity\n    aug = [row + [int(i == j) for j in range(n)] for i, row in enumerate(A)]\n\n    for i in range(n):\n        # Make pivot 1\n        pivot = aug[i][i]\n        for j in range(2*n):\n            aug[i][j] /= pivot\n\n        # Eliminate other rows\n        for k in range(n):\n            if k != i:\n                factor = aug[k][i]\n                for j in range(2*n):\n                    aug[k][j] -= factor * aug[i][j]\n\n    # Extract inverse\n    return [row[n:] for row in aug]\n\nA = [[2,1],[5,3]]\nA_inv = invert_matrix(A)\nfor row in A_inv:\n    print(row)\nC\n#include &lt;stdio.h&gt;\n\n#define N 2\n\nvoid invert_matrix(double A[N][N], double I[N][N]) {\n    double aug[N][2*N];\n    for (int i = 0; i &lt; N; i++) {\n        for (int j = 0; j &lt; N; j++) {\n            aug[i][j] = A[i][j];\n            aug[i][j+N] = (i == j) ? 1 : 0;\n        }\n    }\n\n    for (int i = 0; i &lt; N; i++) {\n        double pivot = aug[i][i];\n        for (int j = 0; j &lt; 2*N; j++)\n            aug[i][j] /= pivot;\n\n        for (int k = 0; k &lt; N; k++) {\n            if (k == i) continue;\n            double factor = aug[k][i];\n            for (int j = 0; j &lt; 2*N; j++)\n                aug[k][j] -= factor * aug[i][j];\n        }\n    }\n\n    for (int i = 0; i &lt; N; i++)\n        for (int j = 0; j &lt; N; j++)\n            I[i][j] = aug[i][j+N];\n}\n\nint main(void) {\n    double A[N][N] = {{2,1},{5,3}}, Inv[N][N];\n    invert_matrix(A, Inv);\n    for (int i=0;i&lt;N;i++){ for(int j=0;j&lt;N;j++) printf(\"%6.2f \",Inv[i][j]); printf(\"\\n\"); }\n}\n\n\nWhy It Matters\n\nConceptual clarity: defines what “inverse” means\nSolving systems: \\(\\mathbf{x}=A^{-1}\\mathbf{b}\\)\nGeometry: undo linear transformations\nSymbolic algebra: e.g. transformations, coordinate changes\n\nIn practice, you rarely compute \\(A^{-1}\\) explicitly, you factor and solve instead.\n\n\nA Gentle Proof (Why It Works)\nElementary row operations correspond to multiplying by elementary matrices \\(E_i\\). If\n\\[\nE_k \\cdots E_2 E_1 A = I\n\\]\nthen\n\\[\nA^{-1} = E_k \\cdots E_2 E_1\n\\]\nEach \\(E_i\\) is invertible, so their product is invertible.\n\n\nTry It Yourself\n\nInvert a \\(3 \\times 3\\) matrix by hand.\nVerify \\(A \\cdot A^{-1} = I\\).\nObserve what happens if \\(\\det(A)=0\\).\nCompare with LU-based inversion.\nTime it vs solving \\(A\\mathbf{x}=\\mathbf{b}\\).\n\n\n\nTest Case\n\\[\nA =\n\\begin{bmatrix}\n1 & 2 & 3\\\n0 & 1 & 4\\\n5 & 6 & 0\n\\end{bmatrix}\n\\implies\nA^{-1} =\n\\begin{bmatrix}\n-24 & 18 & 5\\\n20 & -15 & -4\\\n-5 & 4 & 1\n\\end{bmatrix}\n\\]\nCheck: \\(A A^{-1} = I\\)\n\n\nComplexity\n\nTime: \\(O(n^3)\\)\nSpace: \\(O(n^2)\\)\n\nMatrix inversion is a mirror: turning transformations back upon themselves, and teaching us that “solving” and “inverting” are two sides of the same operation.\n\n\n\n557 Determinant by Elimination\nThe determinant of a matrix measures its scaling factor and invertibility. Through Gaussian elimination, we can compute it efficiently by converting the matrix to upper triangular form, where the determinant equals the product of the diagonal entries, adjusted for any row swaps.\n\\[\n\\det(A) = (\\text{sign}) \\times \\prod_{i=1}^{n} U_{ii}\n\\]\n\nWhat Problem Are We Solving?\nWe want to compute \\(\\det(A)\\) without recursive expansion (which is \\(O(n!)\\)). Elimination-based methods do it in \\(O(n^3)\\), the same as LU decomposition.\nThe determinant tells us:\n\nIf \\(\\det(A)=0\\): \\(A\\) is singular (non-invertible)\nIf \\(\\det(A)\\ne0\\): \\(A\\) is invertible\nThe volume scaling factor of the linear transformation by \\(A\\)\nThe orientation (positive = preserved, negative = flipped)\n\n\n\nHow Does It Work (Plain Language)\nWe perform elimination to form an upper triangular matrix \\(U\\), keeping track of row swaps and scaling.\nSteps:\n\nStart with \\(A\\)\nFor each pivot row \\(i\\):\n\nSwap rows if pivot is zero (each swap flips determinant sign)\nEliminate below using row operations (adding multiples doesn’t change determinant)\n\nOnce \\(U\\) is upper triangular: \\[\n\\det(A) = (\\pm 1) \\times \\prod_{i=1}^n U_{ii}\n\\]\n\nOnly row swaps affect the sign.\n\n\nExample\nLet \\[\nA =\n\\begin{bmatrix}\n2 & 1 & 3\\\n4 & 2 & 6\\\n1 & -1 & 1\n\\end{bmatrix}\n\\]\nPerform elimination:\n\nRow2 = Row2 - 2×Row1 → \\([0, 0, 0]\\)\nRow3 = Row3 - ½×Row1 → \\([0, -1.5, -0.5]\\)\n\nNow \\(U =\n\\begin{bmatrix}\n2 & 1 & 3\\\n0 & 0 & 0\\\n0 & -1.5 & -0.5\n\\end{bmatrix}\\)\nA zero row appears → \\(\\det(A)=0\\).\nNow let’s test a non-singular case:\n\\[\nB =\n\\begin{bmatrix}\n2 & 1 & 1\\\n1 & 3 & 2\\\n1 & 0 & 0\n\\end{bmatrix}\n\\]\nEliminate:\n\nRow2 = Row2 - ½×Row1 → \\([0, 2.5, 1.5]\\)\nRow3 = Row3 - ½×Row1 → \\([0, -0.5, -0.5]\\)\nRow3 = Row3 + 0.2×Row2 → \\([0, 0, -0.2]\\)\n\nNow \\(U_{11}=2\\), \\(U_{22}=2.5\\), \\(U_{33}=-0.2\\)\n\\[\n\\det(B) = 2 \\times 2.5 \\times (-0.2) = -1\n\\]\n\n\nTiny Code (Easy Versions)\nPython\ndef determinant(A):\n    n = len(A)\n    A = [row[:] for row in A]\n    det = 1\n    sign = 1\n\n    for i in range(n):\n        # Pivoting\n        if A[i][i] == 0:\n            for j in range(i+1, n):\n                if A[j][i] != 0:\n                    A[i], A[j] = A[j], A[i]\n                    sign *= -1\n                    break\n        pivot = A[i][i]\n        if pivot == 0:\n            return 0\n        det *= pivot\n        for j in range(i+1, n):\n            factor = A[j][i] / pivot\n            for k in range(i, n):\n                A[j][k] -= factor * A[i][k]\n    return sign * det\n\nA = [[2,1,1],[1,3,2],[1,0,0]]\nprint(\"det =\", determinant(A))\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define N 3\n\ndouble determinant(double A[N][N]) {\n    double det = 1;\n    int sign = 1;\n    for (int i = 0; i &lt; N; i++) {\n        if (fabs(A[i][i]) &lt; 1e-9) {\n            int swap = -1;\n            for (int j = i+1; j &lt; N; j++) {\n                if (fabs(A[j][i]) &gt; 1e-9) { swap = j; break; }\n            }\n            if (swap == -1) return 0;\n            for (int k = 0; k &lt; N; k++) {\n                double tmp = A[i][k];\n                A[i][k] = A[swap][k];\n                A[swap][k] = tmp;\n            }\n            sign *= -1;\n        }\n        det *= A[i][i];\n        for (int j = i+1; j &lt; N; j++) {\n            double factor = A[j][i] / A[i][i];\n            for (int k = i; k &lt; N; k++)\n                A[j][k] -= factor * A[i][k];\n        }\n    }\n    return det * sign;\n}\n\nint main(void) {\n    double A[N][N] = {{2,1,1},{1,3,2},{1,0,0}};\n    printf(\"det = %.2f\\n\", determinant(A));\n}\n\n\nWhy It Matters\n\nInvertibility check (\\(\\det(A)\\ne0\\) means invertible)\nVolume scaling under linear transform\nOrientation detection (sign of determinant)\nCrucial for:\n\nJacobians in calculus\nChange of variables\nEigenvalue computation\n\n\n\n\nA Gentle Proof (Why It Works)\nGaussian elimination expresses \\(A\\) as:\n\\[\nA = L U\n\\]\nwith \\(L\\) unit-lower-triangular. Then: \\[\n\\det(A) = \\det(L)\\det(U) = 1 \\times \\prod_{i=1}^n U_{ii}\n\\]\nRow swaps multiply \\(\\det(A)\\) by \\(-1\\) each time. Adding multiples of rows doesn’t change \\(\\det(A)\\).\n\n\nTry It Yourself\n\nCompute \\(\\det(A)\\) by cofactor expansion and elimination, compare.\nTrack how sign changes with swaps.\nVerify with a triangular matrix.\nObserve determinant of singular matrices (should be 0).\nCompare LU-based determinant.\n\n\n\nTest Case\n\n\n\n\n\n\n\nMatrix \\(A\\)\nDeterminant\n\n\n\n\n\\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\)\n\\(-2\\)\n\n\n\\(\\begin{bmatrix} 2 & 1 & 1 \\\\ 1 & 3 & 2 \\\\ 1 & 0 & 0 \\end{bmatrix}\\)\n\\(-1\\)\n\n\n\\(\\begin{bmatrix} 2 & 1 & 3 \\\\ 4 & 2 & 6 \\\\ 1 & -1 & 1 \\end{bmatrix}\\)\n\\(0\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\)\nSpace: \\(O(1)\\) (in-place)\n\nDeterminant by elimination turns algebraic chaos into structure, each pivot a volume factor, each swap a flip of orientation.\n\n\n\n558 Rank of a Matrix\nThe rank of a matrix tells us how many independent rows or columns it has, in other words, the dimension of its image (column space). It’s the bridge between linear independence, solvability, and dimension.\nWe can compute it efficiently using Gaussian elimination: transform the matrix to row echelon form (REF) and count the non-zero rows.\n\nWhat Problem Are We Solving?\nWe want to measure how much information is in a matrix. Rank answers questions like:\n\nAre the columns independent?\nDoes \\(A\\mathbf{x}=\\mathbf{b}\\) have a solution?\nWhat is the dimension of the span of rows/columns?\n\nFor \\(m\\times n\\) matrix \\(A\\): \\[\n\\text{rank}(A) = \\text{number of leading pivots in REF}\n\\]\n\n\nHow Does It Work (Plain Language)\n\nApply Gaussian elimination to reduce \\(A\\) to row echelon form (REF)\nEach non-zero row represents one pivot (independent direction)\nCount the pivots → that’s the rank\n\nIf full rank:\n\n\\(\\text{rank}(A)=n\\) → columns independent\n\\(\\text{rank}(A)&lt;n\\) → some columns are dependent\n\nIn reduced row echelon form (RREF), the pivots are explicit 1s with zeros above and below.\n\n\nExample\nLet \\[\nA =\n\\begin{bmatrix}\n2 & 1 & 3\\\n4 & 2 & 6\\\n1 & -1 & 1\n\\end{bmatrix}\n\\]\nPerform elimination:\nRow2 = Row2 - 2×Row1 → \\([0, 0, 0]\\) Row3 = Row3 - ½×Row1 → \\([0, -1.5, -0.5]\\)\nResult: \\[\n\\begin{bmatrix}\n2 & 1 & 3\\\n0 & -1.5 & -0.5\\\n0 & 0 & 0\n\\end{bmatrix}\n\\]\nTwo non-zero rows → rank = 2\nSo the rows span a 2D plane, not all of \\(\\mathbb{R}^3\\).\n\n\nTiny Code (Easy Versions)\nPython\ndef matrix_rank(A):\n    n, m = len(A), len(A[0])\n    A = [row[:] for row in A]\n    rank = 0\n\n    for col in range(m):\n        # Find pivot\n        pivot_row = None\n        for row in range(rank, n):\n            if abs(A[row][col]) &gt; 1e-9:\n                pivot_row = row\n                break\n        if pivot_row is None:\n            continue\n\n        # Swap to current rank row\n        A[rank], A[pivot_row] = A[pivot_row], A[rank]\n\n        # Normalize pivot row\n        pivot = A[rank][col]\n        A[rank] = [x / pivot for x in A[rank]]\n\n        # Eliminate below\n        for r in range(rank+1, n):\n            factor = A[r][col]\n            A[r] = [A[r][c] - factor*A[rank][c] for c in range(m)]\n\n        rank += 1\n    return rank\n\nA = [[2,1,3],[4,2,6],[1,-1,1]]\nprint(\"rank =\", matrix_rank(A))\nC (Gaussian Elimination)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define N 3\n#define M 3\n\nint matrix_rank(double A[N][M]) {\n    int rank = 0;\n    for (int col = 0; col &lt; M; col++) {\n        int pivot = -1;\n        for (int r = rank; r &lt; N; r++) {\n            if (fabs(A[r][col]) &gt; 1e-9) { pivot = r; break; }\n        }\n        if (pivot == -1) continue;\n\n        // Swap\n        if (pivot != rank) {\n            for (int c = 0; c &lt; M; c++) {\n                double tmp = A[rank][c];\n                A[rank][c] = A[pivot][c];\n                A[pivot][c] = tmp;\n            }\n        }\n\n        // Normalize\n        double div = A[rank][col];\n        for (int c = 0; c &lt; M; c++)\n            A[rank][c] /= div;\n\n        // Eliminate\n        for (int r = rank + 1; r &lt; N; r++) {\n            double factor = A[r][col];\n            for (int c = 0; c &lt; M; c++)\n                A[r][c] -= factor * A[rank][c];\n        }\n\n        rank++;\n    }\n    return rank;\n}\n\nint main(void) {\n    double A[N][M] = {{2,1,3},{4,2,6},{1,-1,1}};\n    printf(\"rank = %d\\n\", matrix_rank(A));\n}\n\n\nWhy It Matters\n\nDimension of span: rank = number of independent directions\nSolvability:\n\nIf \\(\\text{rank}(A)=\\text{rank}([A|\\mathbf{b}])\\), system is consistent\nIf \\(\\text{rank}(A)&lt;n\\), infinite or no solutions\n\nColumn space: rank = dimension of image\nRow space: same as column space dimension (rank = rank\\(^T\\))\n\n\n\nA Gentle Proof (Why It Works)\nRow operations don’t change the span of rows, hence don’t change rank. Once in echelon form, each non-zero row adds one linearly independent vector to the row space.\nThus: \\[\n\\text{rank}(A) = \\text{number of pivot positions}\n\\]\n\n\nTry It Yourself\n\nReduce a \\(3\\times3\\) matrix to REF.\nCount pivot rows → rank.\nCompare with column independence check.\nCompute rank(\\(A\\)) and rank(\\(A^T\\)).\nTest with singular matrices (rank &lt; n).\n\n\n\nTest Cases\n\n\n\n\n\n\n\nMatrix\nRank\n\n\n\n\n\\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\\)\n1\n\n\n\\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\)\n2\n\n\n\\(\\begin{bmatrix} 2 & 1 & 3 \\\\ 4 & 2 & 6 \\\\ 1 & -1 & 1 \\end{bmatrix}\\)\n2\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\)\nSpace: \\(O(n^2)\\)\n\nThe rank is the soul of a matrix, telling you how many truly independent voices sing in its rows and columns.\n\n\n\n559 Eigenvalue Power Method\nThe Power Method is a simple iterative algorithm to approximate the dominant eigenvalue (the one with the largest magnitude) and its corresponding eigenvector of a matrix \\(A\\).\nIt’s one of the earliest and most intuitive ways to “listen” to the matrix and find its strongest direction, the one that stays steady under repeated transformation.\n\nWhat Problem Are We Solving?\nWe want to find \\(\\lambda_{\\max}\\) and \\(\\mathbf{v}_{\\max}\\) such that:\n\\[\nA \\mathbf{v}*{\\max} = \\lambda*{\\max} \\mathbf{v}_{\\max}\n\\]\nWhen \\(A\\) is large or sparse, solving the characteristic polynomial is infeasible. The Power Method gives an iterative, low-memory approximation, crucial in numerical linear algebra, PageRank, and PCA.\n\n\nHow Does It Work (Plain Language)\n\nStart with a random vector \\(\\mathbf{x}_0\\)\nRepeatedly apply \\(A\\): \\(\\mathbf{x}_{k+1} = A \\mathbf{x}_k\\)\nNormalize each step to prevent overflow\nWhen \\(\\mathbf{x}_k\\) stabilizes, it aligns with the dominant eigenvector\nThe corresponding eigenvalue is approximated by the Rayleigh quotient:\n\n\\[\n\\lambda_k \\approx \\frac{\\mathbf{x}_k^T A \\mathbf{x}_k}{\\mathbf{x}_k^T \\mathbf{x}_k}\n\\]\nBecause \\(A^k \\mathbf{x}*0\\) amplifies the component in the direction of \\(\\mathbf{v}*{\\max}\\).\n\n\nAlgorithm (Step-by-Step)\nGiven \\(A\\) and tolerance \\(\\varepsilon\\):\n\nChoose \\(\\mathbf{x}_0\\) (non-zero vector)\nRepeat:\n\n\\(\\mathbf{y} = A \\mathbf{x}\\)\n\\(\\lambda = \\max(|y_i|)\\) or \\(\\mathbf{x}^T A \\mathbf{x}\\)\n\\(\\mathbf{x} = \\mathbf{y} / \\lambda\\)\n\nStop when \\(|\\mathbf{x}_{k+1} - \\mathbf{x}_k| &lt; \\varepsilon\\)\n\nReturn \\(\\lambda, \\mathbf{x}\\).\nConvergence requires that \\(A\\) has a unique largest eigenvalue.\n\n\nExample\nLet \\[\nA =\n\\begin{bmatrix}\n2 & 1\\\n1 & 3\n\\end{bmatrix}\n\\]\nStart \\[\n\\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\n\n\\(\\mathbf{y} = A\\mathbf{x}_0 = [3, 4]^T\\), normalize: \\(\\mathbf{x}_1 = [0.6, 0.8]\\)\n\\(\\mathbf{y} = A\\mathbf{x}_1 = [2.0, 3.0]^T\\), normalize: \\(\\mathbf{x}_2 = [0.5547, 0.8321]\\)\nRepeat, \\(\\mathbf{x}_k\\) converges to eigenvector \\([0.447, 0.894]\\)\n\\(\\lambda \\approx 3.618\\) (dominant eigenvalue)\n\n\n\nTiny Code (Easy Versions)\nPython\nimport numpy as np\n\ndef power_method(A, tol=1e-6, max_iter=1000):\n    n = len(A)\n    x = np.ones(n)\n    lambda_old = 0\n\n    for _ in range(max_iter):\n        y = np.dot(A, x)\n        lambda_new = np.max(np.abs(y))\n        x = y / lambda_new\n        if np.linalg.norm(x - y/np.linalg.norm(y)) &lt; tol:\n            break\n        lambda_old = lambda_new\n    return lambda_new, x / np.linalg.norm(x)\n\nA = np.array([[2,1],[1,3]], dtype=float)\nlam, v = power_method(A)\nprint(\"lambda ≈\", lam)\nprint(\"v ≈\", v)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n#define N 2\n#define MAX_ITER 1000\n#define TOL 1e-6\n\nvoid power_method(double A[N][N], double v[N]) {\n    double y[N], lambda = 0, lambda_old;\n    for (int i=0;i&lt;N;i++) v[i]=1.0;\n\n    for (int iter=0;iter&lt;MAX_ITER;iter++) {\n        // y = A * v\n        for (int i=0;i&lt;N;i++) {\n            y[i]=0;\n            for (int j=0;j&lt;N;j++) y[i]+=A[i][j]*v[j];\n        }\n        // estimate eigenvalue\n        lambda_old = lambda;\n        lambda = fabs(y[0]);\n        for (int i=1;i&lt;N;i++) if (fabs(y[i])&gt;lambda) lambda=fabs(y[i]);\n        // normalize\n        for (int i=0;i&lt;N;i++) v[i]=y[i]/lambda;\n\n        // check convergence\n        double diff=0;\n        for (int i=0;i&lt;N;i++) diff+=fabs(y[i]-lambda*v[i]);\n        if (diff&lt;TOL) break;\n    }\n\n    printf(\"lambda ≈ %.6f\\n\", lambda);\n    printf(\"v ≈ [%.3f, %.3f]\\n\", v[0], v[1]);\n}\n\nint main(void) {\n    double A[N][N] = {{2,1},{1,3}}, v[N];\n    power_method(A, v);\n}\n\n\nWhy It Matters\n\nFinds dominant eigenvalue/vector\nWorks with large sparse matrices\nFoundation for:\n\nPageRank (Google)\nPCA (Principal Component Analysis)\nSpectral methods\nMarkov chains steady states\n\n\n\n\nA Gentle Proof (Why It Works)\nIf \\(A\\) has eigen-decomposition \\[\nA = V \\Lambda V^{-1}\n\\] and \\(\\mathbf{x}_0 = c_1\\mathbf{v}_1 + \\dots + c_n\\mathbf{v}_n\\),\nthen \\[\nA^k \\mathbf{x}_0 = c_1\\lambda_1^k\\mathbf{v}_1 + \\cdots + c_n\\lambda_n^k\\mathbf{v}_n\n\\]\nAs \\(k \\to \\infty\\), \\(\\lambda_1^k\\) dominates, so direction \\(\\to \\mathbf{v}_1\\). Normalization removes scale, leaving the dominant eigenvector.\n\n\nTry It Yourself\n\nApply to a \\(3\\times3\\) symmetric matrix.\nCompare result with numpy.linalg.eig.\nTry on a diagonal matrix, verify it finds largest diagonal.\nObserve divergence if eigenvalues are equal in magnitude.\nModify to inverse power method to find smallest eigenvalue.\n\n\n\nTest Case\n\\[\nA =\n\\begin{bmatrix}\n4 & 1\\\n2 & 3\n\\end{bmatrix},\n\\quad\n\\lambda_{\\max} \\approx 4.561,\n\\quad\n\\mathbf{v}_{\\max} \\approx\n\\begin{bmatrix}\n0.788\\\n0.615\n\\end{bmatrix}\n\\]\n\n\nComplexity\n\nTime: \\(O(kn^2)\\) (for \\(k\\) iterations)\nSpace: \\(O(n^2)\\)\n\nThe Power Method is the simplest window into eigenvalues, each iteration aligns your vector more closely with the matrix’s strongest echo.\n\n\n\n560 Singular Value Decomposition (SVD)\nThe Singular Value Decomposition (SVD) is one of the most powerful and universal factorizations in linear algebra. It expresses any matrix\\(A\\) (square or rectangular) as a product of three special matrices:\n\\[\nA = U \\Sigma V^T\n\\]\n-\\(U\\): orthogonal matrix of left singular vectors -\\(\\Sigma\\): diagonal matrix of singular values (non-negative) -\\(V\\): orthogonal matrix of right singular vectors\nSVD generalizes the eigendecomposition, works for any matrix (even non-square), and reveals deep structure, from geometry to data compression.\n\nWhat Problem Are We Solving?\nWe want to decompose\\(A\\) into simpler, interpretable parts:\n\nDirections of stretching (via\\(V\\))\nAmount of stretching (via\\(\\Sigma\\))\nResulting orthogonal directions (via\\(U\\))\n\nSVD is used to:\n\nCompute rank, null space, and range\nPerform dimensionality reduction (PCA)\nSolve least squares problems\nCompute pseudoinverse\nPerform noise reduction in signals and images\n\n\n\nHow Does It Work (Plain Language)\nFor any\\(A \\in \\mathbb{R}^{m \\times n}\\):\n\nCompute\\(A^T A\\) (symmetric and positive semidefinite)\nFind eigenvalues\\(\\lambda_i\\) and eigenvectors\\(v_i\\)\nSingular values\\(\\sigma_i = \\sqrt{\\lambda_i}\\)\nForm\\(V = [v_1, \\ldots, v_n]\\)\nForm\\(U = \\frac{1}{\\sigma_i} A v_i\\) for non-zero\\(\\sigma_i\\)\nAssemble\\(\\Sigma\\) with\\(\\sigma_i\\) on the diagonal\n\nResult: \\[\nA = U \\Sigma V^T\n\\]\nIf\\(A\\) is\\(m \\times n\\):\n-\\(U\\):\\(m \\times m\\) -\\(\\Sigma\\):\\(m \\times n\\) -\\(V\\):\\(n \\times n\\)\n\n\nExample\nLet \\[\nA=\n\\begin{bmatrix}\n3 & 1\\\\\n1 & 3\n\\end{bmatrix}.\n\\]\n\nCompute \\(A^{\\mathsf T}A\\): \\[\nA^{\\mathsf T}A\n=\n\\begin{bmatrix}\n3 & 1\\\\\n1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n3 & 1\\\\\n1 & 3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n10 & 6\\\\\n6 & 10\n\\end{bmatrix}.\n\\]\nEigenvalues: \\(16,\\,4\\)\nSingular values: \\(\\sigma_1=4,\\ \\sigma_2=2\\).\nRight singular vectors: \\[\nv_1=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\end{bmatrix}, \\quad\nv_2=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\-1\\end{bmatrix}, \\quad\nV=[v_1,v_2].\n\\]\nCompute \\(u_i=\\frac{1}{\\sigma_i}A v_i\\): \\[\nu_1=\\frac{1}{4}A v_1\n=\\frac{1}{4}\n\\begin{bmatrix}3 & 1\\\\1 & 3\\end{bmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}1\\\\1\\end{bmatrix}\n=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\end{bmatrix},\n\\] \\[\nu_2=\\frac{1}{2}A v_2\n=\\frac{1}{2}\n\\begin{bmatrix}3 & 1\\\\1 & 3\\end{bmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}1\\\\-1\\end{bmatrix}\n=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\-1\\end{bmatrix}.\n\\]\n\nThus \\[\nU=\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 & 1\\\\\n1 & -1\n\\end{bmatrix}, \\quad\n\\Sigma=\n\\begin{bmatrix}\n4 & 0\\\\\n0 & 2\n\\end{bmatrix}, \\quad\nV=U.\n\\]\nCheck: \\(A = U\\,\\Sigma\\,V^{\\mathsf T}\\).\n\n\nTiny Code (Easy Versions)\nPython (NumPy built-in)\nimport numpy as np\n\nA = np.array([[3, 1], [1, 3]], dtype=float)\nU, S, Vt = np.linalg.svd(A)\n\nprint(\"U =\\n\", U)\nprint(\"S =\\n\", np.diag(S))\nprint(\"V^T =\\n\", Vt)\nPython (Manual Approximation for 2×2)\nimport numpy as np\n\ndef svd_2x2(A):\n    ATA = A.T @ A\n    eigvals, V = np.linalg.eig(ATA)\n    idx = np.argsort(-eigvals)\n    eigvals, V = eigvals[idx], V[:, idx]\n    S = np.sqrt(eigvals)\n    U = (A @ V) / S\n    return U, S, V\n\nA = np.array([[3, 1], [1, 3]], float)\nU, S, V = svd_2x2(A)\nprint(\"U =\", U)\nprint(\"S =\", S)\nprint(\"V =\", V)\nC (Conceptual Skeleton)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\n// For 2x2 SVD demonstration\nvoid svd_2x2(double A[2][2]) {\n    double a=A[0][0], b=A[0][1], c=A[1][0], d=A[1][1];\n    double ATA[2][2] = {\n        {a*a + c*c, a*b + c*d},\n        {a*b + c*d, b*b + d*d}\n    };\n\n    double trace = ATA[0][0] + ATA[1][1];\n    double det = ATA[0][0]*ATA[1][1] - ATA[0][1]*ATA[1][0];\n    double s = sqrt(trace*trace/4 - det);\n\n    double sigma1 = sqrt(trace/2 + s);\n    double sigma2 = sqrt(trace/2 - s);\n\n    printf(\"Singular values: %.3f, %.3f\\n\", sigma1, sigma2);\n}\n\nint main(void) {\n    double A[2][2] = {{3,1},{1,3}};\n    svd_2x2(A);\n}\n\n\nWhy It Matters\n\nUniversal factorization: works for any matrix\nGeometry: describes stretching and rotation\nData science: PCA, low-rank approximation\nNumerical analysis: stable pseudoinverse\nSignal processing: denoising, compression\n\nSVD powers tools like Latent Semantic Analysis (LSA) and Truncated SVD for dimensionality reduction.\n\n\nA Gentle Proof (Why It Works)\nFrom\\(A^T A = V \\Lambda V^T\\), where\\(\\Lambda\\) is diagonal with eigenvalues\\(\\lambda_i \\ge 0\\). Define\\(\\Sigma = \\sqrt{\\Lambda}\\). Then:\n\\[\nA = (A V \\Sigma^{-1}) \\Sigma V^T\n\\]\nSet\\(U = A V \\Sigma^{-1}\\), which is orthogonal. Thus\\(A = U \\Sigma V^T\\).\n\n\nTry It Yourself\n\nDecompose a\\(3\\times2\\) matrix manually.\nVerify\\(U^T U = I\\),\\(V^T V = I\\).\nCompare singular values with\\(\\sqrt{\\text{eigenvalues of } A^T A}\\).\nUse SVD to find rank (count of non-zero singular values).\nReconstruct\\(A\\) from top-1 singular value for compression.\n\n\n\nTest Case\n\\[\nA =\n\\begin{bmatrix}\n3 & 1\\\n1 & 3\n\\end{bmatrix},\n\\quad\nU =\n\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 & 1\\\n1 & -1\n\\end{bmatrix},\n\\quad\n\\Sigma =\n\\begin{bmatrix}\n4 & 0\\\n0 & 2\n\\end{bmatrix},\n\\quad\nV = U\n\\]\nCheck:\\(A = U \\Sigma V^T\\)\n\n\nComplexity\n\nTime:\\(O(mn^2)\\) for dense matrices\nSpace:\\(O(mn)\\)\n\nSVD is the master key of linear algebra, unlocking rank, eigenstructure, geometry, and data all at once.\n\n\n\n\nSection 57. FFT and NTT (Fast Transforms)\n\n561 Discrete Fourier Transform (DFT)\nThe Discrete Fourier Transform (DFT) is the mathematical bridge between time (or spatial) domain and frequency domain. It decomposes a finite sequence of numbers into a sum of complex sinusoids, revealing the hidden frequencies that make up the signal.\nThe DFT is the foundation for signal processing, audio compression, image filtering, and polynomial multiplication. It’s beautiful, exact, and fundamental.\n\nWhat Problem Are We Solving?\nGiven a sequence of\\(n\\) complex numbers:\n\\[\nx = (x_0, x_1, \\ldots, x_{n-1})\n\\]\nwe want to compute a new sequence\\(X = (X_0, X_1, \\ldots, X_{n-1})\\) that describes how much of each frequency is present in\\(x\\).\nThe transform is defined as:\n\\[\nX_k = \\sum_{j=0}^{n-1} x_j \\cdot e^{-2\\pi i \\frac{jk}{n}}\n\\quad \\text{for } k = 0, 1, \\ldots, n-1\n\\]\nEach\\(X_k\\) measures the amplitude of the complex sinusoid with frequency\\(k/n\\).\n\n\nHow Does It Work (Plain Language)\nThink of your input sequence as a chord played on a piano, a mix of multiple frequencies. The DFT “listens” to the signal and tells you which notes (frequencies) are present and how strong they are.\nAt its core, it multiplies the signal by complex sinusoids\\(e^{-2\\pi i jk/n}\\), summing the results to find how strongly each sinusoid contributes.\n\n\nExample\nLet\\(n = 4\\), and\\(x = (1, 2, 3, 4)\\)\nThen:\n\\[\nX_k = \\sum_{j=0}^{3} x_j \\cdot e^{-2\\pi i \\frac{jk}{4}}\n\\]\nCompute each:\n-\\(X_0 = 1 + 2 + 3 + 4 = 10\\) -\\(X_1 = 1 + 2i - 3 - 4i = -2 - 2i\\) -\\(X_2 = 1 - 2 + 3 - 4 = -2\\) -\\(X_3 = 1 - 2i - 3 + 4i = -2 + 2i\\)\nSo the DFT is:\n\\[\nX = [10, -2-2i, -2, -2+2i]\n\\]\n\n\nInverse DFT\nTo reconstruct the original sequence from its frequencies:\n\\[\nx_j = \\frac{1}{n} \\sum_{k=0}^{n-1} X_k \\cdot e^{2\\pi i \\frac{jk}{n}}\n\\]\nThe forward and inverse transform form a perfect pair, no information is lost.\n\n\nTiny Code (Easy Versions)\nPython (Naive DFT)\nimport cmath\n\ndef dft(x):\n    n = len(x)\n    X = []\n    for k in range(n):\n        s = 0\n        for j in range(n):\n            angle = -2j * cmath.pi * j * k / n\n            s += x[j] * cmath.exp(angle)\n        X.append(s)\n    return X\n\n# Example\nx = [1, 2, 3, 4]\nX = dft(x)\nprint(\"DFT:\", X)\nPython (Inverse DFT)\ndef idft(X):\n    n = len(X)\n    x = []\n    for j in range(n):\n        s = 0\n        for k in range(n):\n            angle = 2j * cmath.pi * j * k / n\n            s += X[k] * cmath.exp(angle)\n        x.append(s / n)\n    return x\nC (Naive Implementation)\n#include &lt;stdio.h&gt;\n#include &lt;complex.h&gt;\n#include &lt;math.h&gt;\n\n#define PI 3.14159265358979323846\n\nvoid dft(int n, double complex x[], double complex X[]) {\n    for (int k = 0; k &lt; n; k++) {\n        X[k] = 0;\n        for (int j = 0; j &lt; n; j++) {\n            double angle = -2.0 * PI * j * k / n;\n            X[k] += x[j] * cexp(I * angle);\n        }\n    }\n}\n\nint main(void) {\n    int n = 4;\n    double complex x[4] = {1, 2, 3, 4};\n    double complex X[4];\n    dft(n, x, X);\n\n    for (int k = 0; k &lt; n; k++)\n        printf(\"X[%d] = %.2f + %.2fi\\n\", k, creal(X[k]), cimag(X[k]));\n}\n\n\nWhy It Matters\n\nConverts time-domain signals into frequency-domain insights\nEnables filtering, compression, and pattern detection\nFundamental in signal processing, audio/video, cryptography, machine learning, and FFT-based algorithms\n\n\n\nA Gentle Proof (Why It Works)\nThe DFT uses the orthogonality of complex exponentials:\n\\[\n\\sum_{j=0}^{n-1} e^{-2\\pi i (k-l) j / n} =\n\\begin{cases}\nn, & \\text{if } k = l, \\\\\n0, & \\text{if } k \\ne l.\n\\end{cases}\n\\]\nThis property ensures we can isolate each frequency component uniquely and invert the transform exactly.\n\n\nTry It Yourself\n\nCompute DFT of\\([1, 0, 1, 0]\\).\nVerify that applying IDFT brings back the original sequence.\nPlot real and imaginary parts of\\(X_k\\).\nObserve how\\(X_0\\) represents the average of input values.\nCompare runtime with FFT for large\\(n\\).\n\n\n\nTest Cases\n\n\n\nInput\\(x\\)\nOutput\\(X\\)\n\n\n\n\n[1, 1, 1, 1]\n[4, 0, 0, 0]\n\n\n[1, 0, 1, 0]\n[2, 0, 2, 0]\n\n\n[1, 2, 3, 4]\n[10, -2-2i, -2, -2+2i]\n\n\n\n\n\nComplexity\n\nTime:\\(O(n^2)\\)\nSpace:\\(O(n)\\)\n\nDFT is the mathematical microscope that reveals the hidden harmonies inside data, every signal becomes a song of frequencies.\n\n\n\n562 Fast Fourier Transform (FFT)\nThe Fast Fourier Transform (FFT) is one of the most important algorithms in computational mathematics. It takes the Discrete Fourier Transform (DFT), originally an\\(O(n^2)\\) operation, and reduces it to\\(O(n \\log n)\\) by exploiting symmetry and recursion. It’s the beating heart behind digital signal processing, convolution, polynomial multiplication, and spectral analysis.\n\nWhat Problem Are We Solving?\nWe want to compute the same transformation as the DFT:\n\\[\nX_k = \\sum_{j=0}^{n-1} x_j \\cdot e^{-2\\pi i \\frac{jk}{n}}\n\\]\nbut faster.\nA direct implementation loops over both\\(j\\) and\\(k\\), requiring\\(n^2\\) operations. FFT cleverly reorganizes computation by dividing the sequence into even and odd parts, halving the work each time.\n\n\nHow Does It Work (Plain Language)\nFFT uses a divide-and-conquer idea:\n\nSplit the sequence into even and odd indexed elements.\nCompute the DFT of each half (recursively).\nCombine results using the symmetry of complex roots of unity.\n\nThis works best when\\(n\\) is a power of 2, because we can split evenly each time until reaching single elements.\nMathematically:\nLet\\(n = 2m\\). Then:\n\\[\nX_k = E_k + \\omega_n^k O_k \\\nX_{k+m} = E_k - \\omega_n^k O_k\n\\]\nwhere:\n-\\(E_k\\) is the DFT of even-indexed terms, -\\(O_k\\) is the DFT of odd-indexed terms, -\\(\\omega_n = e^{-2\\pi i / n}\\) is the primitive root of unity.\n\n\nExample\nLet\\(n = 4\\),\\(x = [1, 2, 3, 4]\\)\nSplit:\n\nEvens:\\([1, 3]\\)\nOdds:\\([2, 4]\\)\n\nCompute DFT of each (size 2):\n-\\(E = [4, -2]\\) -\\(O = [6, -2]\\)\nThen combine:\n-\\(X_0 = E_0 + O_0 = 10\\) -\\(X_1 = E_1 + \\omega_4^1 O_1 = -2 + i(-2) = -2 - 2i\\) -\\(X_2 = E_0 - O_0 = -2\\) -\\(X_3 = E_1 - \\omega_4^1 O_1 = -2 + 2i\\)\nSo\\(X = [10, -2-2i, -2, -2+2i]\\), same as DFT but computed faster.\n\n\nTiny Code (Recursive FFT)\nPython (Cooley–Tukey FFT)\nimport cmath\n\ndef fft(x):\n    n = len(x)\n    if n == 1:\n        return x\n    w_n = cmath.exp(-2j * cmath.pi / n)\n    w = 1\n    x_even = fft(x[0::2])\n    x_odd = fft(x[1::2])\n    X = [0] * n\n    for k in range(n // 2):\n        t = w * x_odd[k]\n        X[k] = x_even[k] + t\n        X[k + n // 2] = x_even[k] - t\n        w *= w_n\n    return X\n\n# Example\nx = [1, 2, 3, 4]\nX = fft(x)\nprint(\"FFT:\", X)\nC (Recursive FFT)\n#include &lt;stdio.h&gt;\n#include &lt;complex.h&gt;\n#include &lt;math.h&gt;\n\n#define PI 3.14159265358979323846\n\nvoid fft(int n, double complex *x) {\n    if (n &lt;= 1) return;\n\n    double complex even[n/2], odd[n/2];\n    for (int i = 0; i &lt; n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i + 1];\n    }\n\n    fft(n/2, even);\n    fft(n/2, odd);\n\n    for (int k = 0; k &lt; n/2; k++) {\n        double complex w = cexp(-2.0 * I * PI * k / n);\n        double complex t = w * odd[k];\n        x[k] = even[k] + t;\n        x[k + n/2] = even[k] - t;\n    }\n}\n\nint main() {\n    double complex x[4] = {1, 2, 3, 4};\n    fft(4, x);\n    for (int i = 0; i &lt; 4; i++)\n        printf(\"X[%d] = %.2f + %.2fi\\n\", i, creal(x[i]), cimag(x[i]));\n}\n\n\nWhy It Matters\n\nTransforms signals, images, and time-series to frequency domain in milliseconds.\nFoundation for digital filters, convolution, and compression.\nCore in machine learning (spectral methods) and physics simulations.\nEnables polynomial multiplication in\\(O(n \\log n)\\).\n\n\n\nA Gentle Proof (Why It Works)\nThe trick is recognizing that the DFT matrix has repeated patterns due to powers of\\(\\omega_n\\):\n\\[\nW_n =\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & \\omega_n & \\omega_n^{2} & \\omega_n^{3} \\\\\n1 & \\omega_n^{2} & \\omega_n^{4} & \\omega_n^{6} \\\\\n1 & \\omega_n^{3} & \\omega_n^{6} & \\omega_n^{9}\n\\end{bmatrix}\n\\]\nBy splitting into even and odd columns, we reuse computations recursively. This halves the problem size at each level, leading to\\(\\log n\\) recursion depth and\\(n\\) work per level.\nTotal:\\(O(n \\log n)\\)\n\n\nTry It Yourself\n\nImplement FFT for\\(n = 8\\) with random values.\nCompare runtime with naive DFT.\nPlot runtime vs\\(n\\) (log-log scale).\nApply FFT to a sine wave and visualize peaks in frequency.\nMultiply two polynomials using FFT convolution.\n\n\n\nTest Cases\n\n\n\nInput\\(x\\)\nFFT\\(X\\)\n\n\n\n\n[1, 1, 1, 1]\n[4, 0, 0, 0]\n\n\n[1, 2, 3, 4]\n[10, -2-2i, -2, -2+2i]\n\n\n\n\n\nComplexity\n\nTime:\\(O(n \\log n)\\)\nSpace:\\(O(n)\\) (recursive) or\\(O(1)\\) (iterative)\n\nFFT turned a quadratic computation into a nearly linear one, a leap so profound it reshaped science, engineering, and computing forever.\n\n\n\n563 Cooley–Tukey FFT\nThe Cooley–Tukey algorithm is the most widely used implementation of the Fast Fourier Transform (FFT). It’s the algorithm that made the FFT practical, elegant, and efficient—reducing the \\(O(n^2)\\) Discrete Fourier Transform to \\(O(n\\log n)\\) by recursively decomposing it into smaller transforms.\nThis method leverages the divide-and-conquer principle and the symmetry of complex roots of unity, making it the backbone of nearly all FFT libraries.\n\nWhat Problem Are We Solving?\nWe want to compute the Discrete Fourier Transform efficiently:\n\\[\nX_k=\\sum_{j=0}^{n-1}x_j\\cdot e^{-2\\pi i\\frac{jk}{n}}\n\\]\nInstead of computing all terms directly, we split the sequence into smaller pieces and reuse results—dramatically cutting down redundant computation.\n\n\nHow Does It Work (Plain Language)\nCooley–Tukey works by recursively dividing the DFT into smaller DFTs:\n\nSplit the input sequence into even and odd indexed elements:\n\n\\(x_{\\text{even}}=[x_0,x_2,x_4,\\ldots]\\)\n\\(x_{\\text{odd}}=[x_1,x_3,x_5,\\ldots]\\)\n\nCompute two smaller DFTs of size \\(n/2\\):\n\n\\(E_k=\\text{DFT}(x_{\\text{even}})\\)\n\\(O_k=\\text{DFT}(x_{\\text{odd}})\\)\n\nCombine them using twiddle factors:\n\n\\(\\omega_n=e^{-2\\pi i/n}\\)\n\n\nThe combination step:\n\\[\nX_k=E_k+\\omega_n^kO_k\n\\]\n\\[\nX_{k+n/2}=E_k-\\omega_n^kO_k\n\\]\n\n\nExample (\\(n=8\\))\nSuppose \\(x=[x_0,x_1,\\ldots,x_7]\\)\n\nSplit into evens and odds:\n\n\\([x_0,x_2,x_4,x_6]\\)\n\\([x_1,x_3,x_5,x_7]\\)\n\nRecursively compute 4-point FFTs for each.\nCombine:\n\n\\(X_k=E_k+\\omega_8^kO_k\\)\n\\(X_{k+4}=E_k-\\omega_8^kO_k\\)\n\n\nEach recursion layer halves the problem, and there are \\(\\log_2 n\\) layers total.\n\n\nTiny Code (Recursive Implementation)\nPython\nimport cmath\n\ndef cooley_tukey_fft(x):\n    n = len(x)\n    if n == 1:\n        return x\n    w_n = cmath.exp(-2j * cmath.pi / n)\n    w = 1\n    X_even = cooley_tukey_fft(x[0::2])\n    X_odd = cooley_tukey_fft(x[1::2])\n    X = [0] * n\n    for k in range(n // 2):\n        t = w * X_odd[k]\n        X[k] = X_even[k] + t\n        X[k + n // 2] = X_even[k] - t\n        w *= w_n\n    return X\n\n# Example\nx = [1, 2, 3, 4, 5, 6, 7, 8]\nX = cooley_tukey_fft(x)\nprint(\"FFT:\", X)\nC (Recursive)\n#include &lt;stdio.h&gt;\n#include &lt;complex.h&gt;\n#include &lt;math.h&gt;\n\n#define PI 3.14159265358979323846\n\nvoid fft(int n, double complex *x) {\n    if (n &lt;= 1) return;\n\n    double complex even[n/2], odd[n/2];\n    for (int i = 0; i &lt; n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i + 1];\n    }\n\n    fft(n/2, even);\n    fft(n/2, odd);\n\n    for (int k = 0; k &lt; n/2; k++) {\n        double complex w = cexp(-2.0 * I * PI * k / n);\n        double complex t = w * odd[k];\n        x[k] = even[k] + t;\n        x[k + n/2] = even[k] - t;\n    }\n}\n\nint main() {\n    double complex x[8] = {1,2,3,4,5,6,7,8};\n    fft(8, x);\n    for (int i = 0; i &lt; 8; i++)\n        printf(\"X[%d] = %.2f + %.2fi\\n\", i, creal(x[i]), cimag(x[i]));\n}\n\n\nWhy It Matters\n\nReduces runtime from \\(O(n^2)\\) to \\(O(n\\log n)\\)\nBasis for audio/video processing, image transforms, and DSP\nEssential for fast polynomial multiplication, signal filtering, and spectral analysis\nUniversally adopted in FFT libraries (FFTW, NumPy, cuFFT)\n\n\n\nA Gentle Proof (Why It Works)\nThe DFT matrix \\(W_n\\) is built from complex roots of unity:\n\\[\nW_n =\n\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & \\omega_n & \\omega_n^2 & \\dots & \\omega_n^{n-1} \\\\\n1 & \\omega_n^2 & \\omega_n^4 & \\dots & \\omega_n^{2(n-1)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & \\omega_n^{n-1} & \\omega_n^{2(n-1)} & \\dots & \\omega_n^{(n-1)^2}\n\\end{bmatrix}\n\\]\nwhere \\(\\omega_n = e^{-2\\pi i / n}\\) is the \\(n\\)th root of unity.\nBy reordering and grouping even/odd columns, we get two smaller \\(W_{n/2}\\) blocks, multiplied by twiddle factors. Thus, each recursive step halves the problem size—leading to \\(\\log_2 n\\) levels of computation.\n\n\nTry It Yourself\n\nCompute FFT for \\([1,2,3,4,5,6,7,8]\\) by hand (2 levels).\nVerify symmetry \\(X_{k+n/2}=E_k-\\omega_n^kO_k\\).\nCompare runtime with DFT for \\(n=1024\\).\nVisualize recursion tree.\nTest on sine wave input—check peaks in frequency domain.\n\n\n\nTest Cases\n\n\n\n\n\n\n\nInput\nOutput (Magnitude)\n\n\n\n\n[1,1,1,1,1,1,1,1]\n[8,0,0,0,0,0,0,0]\n\n\n[1,2,3,4,5,6,7,8]\n[36,-4+9.66i,-4+4i,-4+1.66i,-4,-4-1.66i,-4-4i,-4-9.66i]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\) (recursion) or \\(O(1)\\) (iterative version)\n\nThe Cooley–Tukey FFT is more than a clever trick—it’s a profound insight into symmetry and structure, transforming the way we compute, analyze, and understand signals across every field of science and engineering.\n\n\n\n564 Iterative FFT\nThe Iterative FFT is an efficient, non-recursive implementation of the Cooley–Tukey Fast Fourier Transform. Instead of recursive calls, it computes the transform in-place, reordering elements using bit-reversal permutation and iteratively combining results in layers.\nThis approach is widely used in high-performance FFT libraries (like FFTW or cuFFT) because it’s cache-friendly, stack-safe, and parallelizable.\n\nWhat Problem Are We Solving?\nRecursive FFT is elegant, but function calls and memory allocation overheads can slow it down. The iterative FFT eliminates recursion, doing all the same computations directly in loops.\nWe still want to compute:\n\\[\nX_k=\\sum_{j=0}^{n-1}x_j\\cdot e^{-2\\pi i\\frac{jk}{n}}\n\\]\nbut we’ll reuse the divide-and-conquer pattern iteratively.\n\n\nHow Does It Work (Plain Language)\nThe iterative FFT runs in logarithmic stages, each stage doubling the subproblem size:\n\nBit-Reversal Permutation Reorder the input so indices follow bit-reversed order (mirror the binary digits). Example: for \\(n=8\\), indices \\([0,1,2,3,4,5,6,7]\\) become \\([0,4,2,6,1,5,3,7]\\).\nButterfly Computation Combine pairs of elements (like wings of a butterfly) using twiddle factors: \\[\nt=\\omega_n^k\\cdot X_{\\text{odd}}\n\\] Then update: \\[\nX_{\\text{even}}'=X_{\\text{even}}+t\n\\] \\[\nX_{\\text{odd}}'=X_{\\text{even}}-t\n\\]\nIterate over stages Each stage merges smaller DFTs into larger ones, doubling the block size until full length.\n\nBy the end, \\(x\\) is transformed in-place to \\(X\\).\n\n\nExample (\\(n=8\\))\n\nInput: \\(x=[x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7]\\)\nReorder via bit-reversal: \\([x_0,x_4,x_2,x_6,x_1,x_5,x_3,x_7]\\)\nStage 1: combine pairs \\((x_0,x_1),(x_2,x_3),\\ldots\\)\nStage 2: combine blocks of size 4\nStage 3: combine blocks of size 8\n\nEach stage multiplies by twiddle factors \\(\\omega_n^k=e^{-2\\pi i k/n}\\), performing butterflies iteratively.\n\n\nTiny Code (Iterative FFT)\nPython (In-Place Iterative FFT)\nimport cmath\n\ndef bit_reverse(x):\n    n = len(x)\n    j = 0\n    for i in range(1, n):\n        bit = n &gt;&gt; 1\n        while j & bit:\n            j ^= bit\n            bit &gt;&gt;= 1\n        j ^= bit\n        if i &lt; j:\n            x[i], x[j] = x[j], x[i]\n\ndef iterative_fft(x):\n    n = len(x)\n    bit_reverse(x)\n    size = 2\n    while size &lt;= n:\n        w_m = cmath.exp(-2j * cmath.pi / size)\n        for k in range(0, n, size):\n            w = 1\n            for j in range(size // 2):\n                t = w * x[k + j + size // 2]\n                u = x[k + j]\n                x[k + j] = u + t\n                x[k + j + size // 2] = u - t\n                w *= w_m\n        size *= 2\n    return x\n\n# Example\nx = [1, 2, 3, 4, 5, 6, 7, 8]\nX = iterative_fft([complex(a, 0) for a in x])\nprint(\"FFT:\", X)\nC (In-Place Iterative FFT)\n#include &lt;stdio.h&gt;\n#include &lt;complex.h&gt;\n#include &lt;math.h&gt;\n\n#define PI 3.14159265358979323846\n\nvoid bit_reverse(double complex *x, int n) {\n    int j = 0;\n    for (int i = 1; i &lt; n; i++) {\n        int bit = n &gt;&gt; 1;\n        while (j & bit) { j ^= bit; bit &gt;&gt;= 1; }\n        j ^= bit;\n        if (i &lt; j) {\n            double complex temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n    }\n}\n\nvoid iterative_fft(double complex *x, int n) {\n    bit_reverse(x, n);\n    for (int size = 2; size &lt;= n; size &lt;&lt;= 1) {\n        double angle = -2 * PI / size;\n        double complex w_m = cos(angle) + I * sin(angle);\n        for (int k = 0; k &lt; n; k += size) {\n            double complex w = 1;\n            for (int j = 0; j &lt; size/2; j++) {\n                double complex t = w * x[k + j + size/2];\n                double complex u = x[k + j];\n                x[k + j] = u + t;\n                x[k + j + size/2] = u - t;\n                w *= w_m;\n            }\n        }\n    }\n}\n\n\nWhy It Matters\n\nRemoves recursion overhead, faster in practice\nIn-place, requires no extra arrays\nFoundation for GPU FFTs, DSP hardware, and real-time systems\nEnables batch FFTs efficiently (parallelizable loops)\n\n\n\nA Gentle Proof (Why It Works)\nThe iterative FFT simply traverses the same recursion tree bottom-up. The bit-reversal ensures inputs line up as they would in recursive order. Each stage merges DFTs of size \\(2^k\\) into \\(2^{k+1}\\), using identical butterfly equations:\n\\[\nX_k=E_k+\\omega_n^kO_k,\\quad X_{k+n/2}=E_k-\\omega_n^kO_k\n\\]\nBy repeating for \\(\\log_2 n\\) stages, we compute the full FFT.\n\n\nTry It Yourself\n\nApply iterative FFT to \\([1,2,3,4,5,6,7,8]\\).\nPrint array after bit-reversal, confirm order.\nCompare result to recursive FFT.\nPlot runtime for \\(n=2^k\\).\nExtend to inverse FFT (change sign in exponent, divide by \\(n\\)).\n\n\n\nTest Cases\n\n\n\n\n\n\n\nInput\nOutput (Magnitude)\n\n\n\n\n[1,1,1,1,1,1,1,1]\n[8,0,0,0,0,0,0,0]\n\n\n[1,2,3,4,5,6,7,8]\n[36,-4+9.66i,-4+4i,-4+1.66i,-4,-4-1.66i,-4-4i,-4-9.66i]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(1)\\) (in-place)\n\nThe iterative FFT replaces elegant recursion with raw efficiency—bringing the same mathematical beauty into tight, blazing-fast loops.\n\n\n\n565 Inverse FFT (IFFT)\nThe Inverse Fast Fourier Transform (IFFT) is the mirror image of the FFT. It takes a signal from the frequency domain back to the time domain, perfectly reconstructing the original sequence. Where FFT decomposes a signal into frequencies, IFFT reassembles it from those same components, a complete round-trip transformation.\n\nWhat Problem Are We Solving?\nGiven the Fourier coefficients \\(X_0, X_1, \\ldots, X_{n-1}\\), we want to reconstruct the original signal \\(x_0, x_1, \\ldots, x_{n-1}\\).\nThe definition of the inverse DFT is:\n\\[\nx_j=\\frac{1}{n}\\sum_{k=0}^{n-1}X_k\\cdot e^{2\\pi i\\frac{jk}{n}}\n\\]\nIFFT allows us to recover time-domain data after performing operations (like filtering or convolution) in the frequency domain.\n\n\nHow Does It Work (Plain Language)\nIFFT works just like FFT, same butterfly structure, same recursion or iteration, but with conjugated twiddle factors and a final scaling by \\(1/n\\).\nTo compute the IFFT:\n\nTake the complex conjugate of all frequency components.\nRun a forward FFT.\nTake the complex conjugate again.\nDivide every result by \\(n\\).\n\nThis uses the fact that: \\[\n\\text{IFFT}(X)=\\frac{1}{n}\\cdot\\overline{\\text{FFT}(\\overline{X})}\n\\]\n\n\nExample\nLet \\(X=[10,-2-2i,-2,-2+2i]\\)\n\nConjugate: \\([10,-2+2i,-2,-2-2i]\\)\nFFT of conjugated values \\(\\to [4,8,12,16]\\)\nConjugate again: \\([4,8,12,16]\\)\nDivide by \\(n=4\\): \\([1,2,3,4]\\)\n\nRecovered original: \\(x=[1,2,3,4]\\)\nPerfect reconstruction confirmed.\n\n\nTiny Code (Easy Versions)\nPython (IFFT via FFT)\nimport cmath\n\ndef fft(x):\n    n = len(x)\n    if n == 1:\n        return x\n    w_n = cmath.exp(-2j * cmath.pi / n)\n    w = 1\n    X_even = fft(x[0::2])\n    X_odd = fft(x[1::2])\n    X = [0] * n\n    for k in range(n // 2):\n        t = w * X_odd[k]\n        X[k] = X_even[k] + t\n        X[k + n // 2] = X_even[k] - t\n        w *= w_n\n    return X\n\ndef ifft(X):\n    n = len(X)\n    X_conj = [x.conjugate() for x in X]\n    x = fft(X_conj)\n    x = [val.conjugate() / n for val in x]\n    return x\n\n# Example\nX = [10, -2-2j, -2, -2+2j]\nprint(\"IFFT:\", ifft(X))\nC (IFFT using FFT structure)\n#include &lt;stdio.h&gt;\n#include &lt;complex.h&gt;\n#include &lt;math.h&gt;\n\n#define PI 3.14159265358979323846\n\nvoid fft(int n, double complex *x, int inverse) {\n    if (n &lt;= 1) return;\n\n    double complex even[n/2], odd[n/2];\n    for (int i = 0; i &lt; n/2; i++) {\n        even[i] = x[2*i];\n        odd[i] = x[2*i + 1];\n    }\n\n    fft(n/2, even, inverse);\n    fft(n/2, odd, inverse);\n\n    double sign = inverse ? 2.0 * PI : -2.0 * PI;\n    for (int k = 0; k &lt; n/2; k++) {\n        double complex w = cexp(I * sign * k / n);\n        double complex t = w * odd[k];\n        x[k] = even[k] + t;\n        x[k + n/2] = even[k] - t;\n        if (inverse) {\n            x[k] /= 2;\n            x[k + n/2] /= 2;\n        }\n    }\n}\n\nint main() {\n    double complex X[4] = {10, -2-2*I, -2, -2+2*I};\n    fft(4, X, 1); // inverse FFT\n    for (int i = 0; i &lt; 4; i++)\n        printf(\"x[%d] = %.2f + %.2fi\\n\", i, creal(X[i]), cimag(X[i]));\n}\n\n\nWhy It Matters\n\nRestores time-domain data from frequency components\nUsed in signal reconstruction, convolution, and filter design\nGuarantees perfect reversibility with FFT\nCentral to compression algorithms, image restoration, and physics simulations\n\n\n\nA Gentle Proof (Why It Works)\nThe DFT and IFFT matrices are Hermitian inverses:\n\\[\nF_{n}^{-1}=\\frac{1}{n}\\overline{F_n}^T\n\\]\nThus, applying FFT followed by IFFT yields the identity:\n\\[\n\\text{IFFT}(\\text{FFT}(x))=x\n\\]\nConjugation flips the sign of exponents, and scaling by \\(1/n\\) ensures normalization.\n\n\nTry It Yourself\n\nCompute FFT of \\([1,2,3,4]\\), then apply IFFT.\nCompare reconstructed result to original.\nModify \\(X\\) by zeroing high frequencies, watch smoothing effect.\nUse IFFT for polynomial multiplication results.\nVisualize magnitude and phase before and after IFFT.\n\n\n\nTest Cases\n\n\n\nInput \\(X\\)\nOutput \\(x\\)\n\n\n\n\n[10,-2-2i,-2,-2+2i]\n[1,2,3,4]\n\n\n[4,0,0,0]\n[1,1,1,1]\n\n\n[8,0,0,0,0,0,0,0]\n[1,1,1,1,1,1,1,1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nIFFT is the mirror step of FFT, same structure, reversed flow. It completes the cycle: from time to frequency and back, without losing a single bit of truth.\n\n\n\n566 Convolution via FFT\nConvolution is one of the most fundamental operations in mathematics, signal processing, and computer science. It combines two sequences into one, blending information over shifts. But computing convolution directly takes \\(O(n^2)\\) time. The FFT Convolution Theorem gives us a shortcut: by transforming to the frequency domain, we can do it in \\(O(n\\log n)\\) time.\n\nWhat Problem Are We Solving?\nGiven two sequences \\(a\\) and \\(b\\) of lengths \\(n\\) and \\(m\\), we want their convolution \\(c = a * b\\):\n\\[\nc_k=\\sum_{i=0}^{k}a_i\\cdot b_{k-i}\n\\]\nfor \\(k=0,\\ldots,n+m-2\\).\nThis appears in:\n\nSignal processing (filtering, correlation)\nPolynomial multiplication\nPattern matching\nProbability distributions (sum of random variables)\n\nA direct computation is \\(O(nm)\\). Using FFT, we can reduce it to \\(O(n\\log n)\\).\n\n\nHow Does It Work (Plain Language)\nThe Convolution Theorem says:\n\\[\n\\text{DFT}(a*b)=\\text{DFT}(a)\\cdot\\text{DFT}(b)\n\\]\nThat is, convolution in time domain equals pointwise multiplication in frequency domain.\nSo to compute convolution fast:\n\nPad both sequences to size \\(N\\ge n+m-1\\) (power of 2).\nCompute FFT of both: \\(A=\\text{FFT}(a)\\), \\(B=\\text{FFT}(b)\\).\nMultiply pointwise: \\(C_k=A_k\\cdot B_k\\).\nApply inverse FFT: \\(c=\\text{IFFT}(C)\\).\nTake real parts (round small errors).\n\n\n\nExample\nLet \\(a=[1,2,3]\\), \\(b=[4,5,6]\\)\nExpected convolution (by hand):\n\\[\nc=[1\\cdot4,1\\cdot5+2\\cdot4,1\\cdot6+2\\cdot5+3\\cdot4,2\\cdot6+3\\cdot5,3\\cdot6]\n\\]\n\\[\nc=[4,13,28,27,18]\n\\]\nFFT method:\n\nPad \\(a,b\\) to length 8\n\\(A=\\text{FFT}(a)\\), \\(B=\\text{FFT}(b)\\)\n\\(C=A\\cdot B\\)\n\\(c=\\text{IFFT}(C)\\)\nRound to integers \\(\\Rightarrow [4,13,28,27,18]\\)\n\nMatches perfectly.\n\n\nTiny Code (Easy Versions)\nPython (FFT Convolution)\nimport cmath\n\ndef fft(x):\n    n = len(x)\n    if n == 1:\n        return x\n    w_n = cmath.exp(-2j * cmath.pi / n)\n    w = 1\n    X_even = fft(x[0::2])\n    X_odd = fft(x[1::2])\n    X = [0] * n\n    for k in range(n // 2):\n        t = w * X_odd[k]\n        X[k] = X_even[k] + t\n        X[k + n // 2] = X_even[k] - t\n        w *= w_n\n    return X\n\ndef ifft(X):\n    n = len(X)\n    X_conj = [x.conjugate() for x in X]\n    x = fft(X_conj)\n    return [v.conjugate()/n for v in x]\n\ndef convolution(a, b):\n    n = 1\n    while n &lt; len(a) + len(b) - 1:\n        n *= 2\n    a += [0]*(n - len(a))\n    b += [0]*(n - len(b))\n    A = fft(a)\n    B = fft(b)\n    C = [A[i]*B[i] for i in range(n)]\n    c = ifft(C)\n    return [round(v.real) for v in c]\n\n# Example\na = [1, 2, 3]\nb = [4, 5, 6]\nprint(convolution(a, b))  # [4, 13, 28, 27, 18]\nC (FFT Convolution Skeleton)\n// Assume fft() and ifft() functions are implemented\nvoid convolution(int n, double complex *a, double complex *b, double complex *c) {\n    int size = 1;\n    while (size &lt; 2 * n) size &lt;&lt;= 1;\n\n    // pad arrays\n    for (int i = n; i &lt; size; i++) {\n        a[i] = 0;\n        b[i] = 0;\n    }\n\n    fft(size, a, 0);\n    fft(size, b, 0);\n    for (int i = 0; i &lt; size; i++)\n        c[i] = a[i] * b[i];\n    fft(size, c, 1); // inverse FFT (scaled)\n}\n\n\nWhy It Matters\n\nTurns slow \\(O(n^2)\\) convolution into \\(O(n\\log n)\\)\nCore technique in polynomial multiplication, digital filters, signal correlation, neural network layers, probabilistic sums\nUsed in big integer multiplication (Karatsuba, Schönhage–Strassen)\n\n\n\nA Gentle Proof (Why It Works)\nThe DFT converts convolution to multiplication:\n\\[\nC=\\text{DFT}(a*b)=\\text{DFT}(a)\\cdot\\text{DFT}(b)\n\\]\nBecause the DFT matrix diagonalizes cyclic shifts, it transforms convolution (which involves shifts and sums) into independent multiplications of frequencies.\nThen applying \\(\\text{IDFT}\\) recovers the exact result.\n\n\nTry It Yourself\n\nConvolve \\([1,2,3]\\) and \\([4,5,6]\\) manually, then verify via FFT.\nPad inputs to power of 2 and observe intermediate arrays.\nTry longer polynomials (length 1000), measure speed difference.\nPlot inputs, their spectra, and output.\nImplement modulo convolution (e.g. with Number Theoretic Transform).\n\n\n\nTest Cases\n\n\n\n\\(a\\)\n\\(b\\)\n\\(a*b\\)\n\n\n\n\n[1,2,3]\n[4,5,6]\n[4,13,28,27,18]\n\n\n[1,1,1]\n[1,2,3]\n[1,3,6,5,3]\n\n\n[2,0,1]\n[3,4]\n[6,8,3,4]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nFFT Convolution is how computers multiply long numbers, combine signals, and merge patterns, faster than you could ever do by hand.\n\n\n\n567 Number Theoretic Transform (NTT)\nThe Number Theoretic Transform (NTT) is the modular arithmetic version of the FFT. It replaces complex numbers with integers under a finite modulus, allowing all computations to be done exactly (no floating-point error), perfect for cryptography, combinatorics, and competitive programming.\nWhile FFT uses complex roots of unity \\(e^{2\\pi i/n}\\), NTT uses primitive roots of unity modulo a prime.\n\nWhat Problem Are We Solving?\nWe want to perform polynomial multiplication (or convolution) exactly, using modular arithmetic instead of floating-point numbers.\nGiven two polynomials:\n\\[\nA(x)=\\sum_{i=0}^{n-1}a_i x^i,\\quad B(x)=\\sum_{i=0}^{m-1}b_i x^i\n\\]\nTheir product:\n\\[\nC(x)=A(x)\\cdot B(x)=\\sum_{k=0}^{n+m-2}c_kx^k\n\\]\nwith\n\\[\nc_k=\\sum_{i=0}^{k}a_i\\cdot b_{k-i}\n\\]\nWe want to compute all \\(c_k\\) efficiently, under modulo \\(M\\).\n\n\nKey Idea\nThe Convolution Theorem holds in modular arithmetic too, if we can find a primitive \\(n\\)-th root of unity \\(g\\) such that:\n\\[\ng^n\\equiv1\\pmod M\n\\]\nand\n\\[\ng^k\\not\\equiv1\\pmod M,\\ \\text{for } 0&lt;k&lt;n\n\\]\nThen, we can define an NTT just like FFT:\n\\[\nA_k=\\sum_{j=0}^{n-1}a_j\\cdot g^{jk}\\pmod M\n\\]\nand invert it using \\(g^{-1}\\) and \\(n^{-1}\\pmod M\\).\n\n\nHow It Works (Plain Language)\n\nChoose a modulus \\(M\\) where \\(M=k\\cdot 2^m+1\\) (a FFT-friendly prime). Example: \\(M=998244353=119\\cdot2^{23}+1\\)\nFind primitive root \\(g\\) (e.g. \\(g=3\\)).\nPerform FFT-like butterflies with modular multiplications.\nFor inverse, use \\(g^{-1}\\) and divide by \\(n\\) via modular inverse.\n\nThis ensures exact results, no rounding errors.\n\n\nExample\nLet \\(M=17\\), \\(n=4\\), and \\(g=4\\) since \\(4^4\\equiv1\\pmod{17}\\).\nInput: \\(a=[1,2,3,4]\\)\nCompute NTT:\n\\[\nA_k=\\sum_{j=0}^{3}a_j\\cdot g^{jk}\\pmod{17}\n\\]\nResult: \\(A=[10,2,16,15]\\)\nInverse NTT (using \\(g^{-1}=13\\), \\(n^{-1}=13\\) mod 17):\n\\[\na_j=\\frac{1}{n}\\sum_{k=0}^{3}A_k\\cdot g^{-jk}\\pmod{17}\n\\]\nRecovered: \\([1,2,3,4]\\)\n\n\nTiny Code (NTT Template)\nPython\nMOD = 998244353\nROOT = 3  # primitive root\n\ndef modpow(a, e, m):\n    res = 1\n    while e:\n        if e & 1:\n            res = res * a % m\n        a = a * a % m\n        e &gt;&gt;= 1\n    return res\n\ndef ntt(a, invert):\n    n = len(a)\n    j = 0\n    for i in range(1, n):\n        bit = n &gt;&gt; 1\n        while j & bit:\n            j ^= bit\n            bit &gt;&gt;= 1\n        j ^= bit\n        if i &lt; j:\n            a[i], a[j] = a[j], a[i]\n    len_ = 2\n    while len_ &lt;= n:\n        wlen = modpow(ROOT, (MOD - 1) // len_, MOD)\n        if invert:\n            wlen = modpow(wlen, MOD - 2, MOD)\n        for i in range(0, n, len_):\n            w = 1\n            for j in range(len_ // 2):\n                u = a[i + j]\n                v = a[i + j + len_ // 2] * w % MOD\n                a[i + j] = (u + v) % MOD\n                a[i + j + len_ // 2] = (u - v + MOD) % MOD\n                w = w * wlen % MOD\n        len_ &lt;&lt;= 1\n    if invert:\n        inv_n = modpow(n, MOD - 2, MOD)\n        for i in range(n):\n            a[i] = a[i] * inv_n % MOD\n\ndef multiply(a, b):\n    n = 1\n    while n &lt; len(a) + len(b):\n        n &lt;&lt;= 1\n    a += [0]*(n - len(a))\n    b += [0]*(n - len(b))\n    ntt(a, False)\n    ntt(b, False)\n    for i in range(n):\n        a[i] = a[i] * b[i] % MOD\n    ntt(a, True)\n    return a\n\n\nWhy It Matters\n\nExact modular results, no floating-point rounding\nEnables polynomial multiplication, combinatorial transforms, big integer multiplication\nCritical in cryptography, lattice algorithms, and competitive programming\nUsed in modern schemes (like NTT-based homomorphic encryption)\n\n\n\nA Gentle Proof (Why It Works)\nThe NTT matrix \\(W_n\\) with \\(W_{jk}=g^{jk}\\pmod M\\) satisfies:\n\\[\nW_n^{-1}=\\frac{1}{n}\\overline{W_n}\n\\]\nBecause \\(g\\) is a primitive \\(n\\)-th root of unity, columns of \\(W_n\\) are orthogonal modulo \\(M\\), and modular inverses exist (since \\(M\\) is prime). Hence, the forward and inverse transforms perfectly invert each other.\n\n\nTry It Yourself\n\nUse \\(M=17\\), \\(g=4\\), \\(a=[1,2,3,4]\\), compute NTT manually.\nMultiply \\([1,2,3]\\) and \\([4,5,6]\\) under mod \\(17\\) using NTT.\nTest mod \\(998244353\\), length \\(8\\).\nCompare FFT (float) vs NTT (modular) results.\nObserve rounding-free exactness.\n\n\n\nTest Cases\n\n\n\n\\(a\\)\n\\(b\\)\n\\(a*b\\) (mod 17)\n\n\n\n\n[1,2,3]\n[4,5,6]\n[4,13,28,27,18] mod 17 = [4,13,11,10,1]\n\n\n[1,1]\n[1,1]\n[1,2,1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe NTT is the FFT of the integers, same beauty, different universe. It merges algebra, number theory, and computation into one seamless engine of exactness.\n\n\n\n568 Inverse NTT (INTT)\nThe Inverse Number Theoretic Transform (INTT) is the reverse operation of the NTT. It brings data back from the frequency domain to the coefficient domain, fully reconstructing the original sequence. Just like IFFT for FFT, the INTT undoes the modular transform using the inverse root of unity and a modular scaling factor.\n\nWhat Problem Are We Solving?\nGiven an NTT-transformed sequence \\(A=[A_0,A_1,\\ldots,A_{n-1}]\\), we want to recover the original sequence \\(a=[a_0,a_1,\\ldots,a_{n-1}]\\) under modulus \\(M\\).\nThe definition mirrors the inverse DFT:\n\\[\na_j = n^{-1} \\sum_{k=0}^{n-1} A_k \\cdot g^{-jk} \\pmod M\n\\]\nwhere:\n\n\\(g\\) is a primitive \\(n\\)-th root of unity mod \\(M\\)\n\\(n^{-1}\\) is the modular inverse of \\(n\\) under mod \\(M\\)\n\n\n\nHow Does It Work (Plain Language)\nThe INTT follows the same butterfly structure as the NTT, but:\n\nUse inverse twiddle factors \\(g^{-1}\\) instead of \\(g\\)\nAfter all stages, multiply each element by \\(n^{-1}\\) mod \\(M\\)\n\nThis ensures perfect inversion:\n\\[\n\\text{INTT}(\\text{NTT}(a)) = a\n\\]\n\n\nExample\nLet \\(M=17\\), \\(n=4\\), \\(g=4\\) Then \\(g^{-1}=13\\) (since \\(4\\cdot13\\equiv1\\pmod{17}\\)), \\(n^{-1}=13\\) (since \\(4\\cdot13\\equiv1\\pmod{17}\\))\nSuppose \\(A=[10,2,16,15]\\)\nCompute:\n\\[\na_j=13\\cdot\\sum_{k=0}^{3}A_k\\cdot(13)^{jk}\\pmod{17}\n\\]\nAfter calculation: \\(a=[1,2,3,4]\\)\nPerfect reconstruction.\n\n\nTiny Code (Inverse NTT)\nPython (Inverse NTT)\nMOD = 998244353\nROOT = 3  # primitive root\n\ndef modpow(a, e, m):\n    res = 1\n    while e:\n        if e & 1:\n            res = res * a % m\n        a = a * a % m\n        e &gt;&gt;= 1\n    return res\n\ndef ntt(a, invert=False):\n    n = len(a)\n    j = 0\n    for i in range(1, n):\n        bit = n &gt;&gt; 1\n        while j & bit:\n            j ^= bit\n            bit &gt;&gt;= 1\n        j ^= bit\n        if i &lt; j:\n            a[i], a[j] = a[j], a[i]\n\n    len_ = 2\n    while len_ &lt;= n:\n        wlen = modpow(ROOT, (MOD - 1) // len_, MOD)\n        if invert:\n            wlen = modpow(wlen, MOD - 2, MOD)\n        for i in range(0, n, len_):\n            w = 1\n            for j in range(len_ // 2):\n                u = a[i + j]\n                v = a[i + j + len_ // 2] * w % MOD\n                a[i + j] = (u + v) % MOD\n                a[i + j + len_ // 2] = (u - v + MOD) % MOD\n                w = w * wlen % MOD\n        len_ &lt;&lt;= 1\n\n    if invert:\n        inv_n = modpow(n, MOD - 2, MOD)\n        for i in range(n):\n            a[i] = a[i] * inv_n % MOD\n\n# Example\nA = [10, 2, 16, 15]\nntt(A, invert=True)\nprint(\"Inverse NTT:\", A)  # [1,2,3,4]\nC (Inverse NTT Skeleton)\nvoid ntt(double complex *a, int n, int invert) {\n    // ... (bit-reversal same as NTT)\n    for (int len = 2; len &lt;= n; len &lt;&lt;= 1) {\n        long long wlen = modpow(ROOT, (MOD - 1) / len, MOD);\n        if (invert) wlen = modinv(wlen, MOD);\n        for (int i = 0; i &lt; n; i += len) {\n            long long w = 1;\n            for (int j = 0; j &lt; len / 2; j++) {\n                long long u = a[i + j];\n                long long v = a[i + j + len / 2] * w % MOD;\n                a[i + j] = (u + v) % MOD;\n                a[i + j + len / 2] = (u - v + MOD) % MOD;\n                w = w * wlen % MOD;\n            }\n        }\n    }\n    if (invert) {\n        long long inv_n = modinv(n, MOD);\n        for (int i = 0; i &lt; n; i++)\n            a[i] = a[i] * inv_n % MOD;\n    }\n}\n\n\nWhy It Matters\n\nCompletes the modular FFT pipeline\nEnsures exact reconstruction of coefficients\nCore for polynomial multiplication, cryptographic transforms, error-correcting codes\nEnables precise modular computations with no floating-point errors\n\n\n\nA Gentle Proof (Why It Works)\nNTT and INTT are modular inverses of each other:\n\\[\n\\text{NTT}(a)=W_n\\cdot a,\\quad \\text{INTT}(A)=n^{-1}\\cdot W_n^{-1}\\cdot A\n\\]\nwhere \\(W_n\\) is the Vandermonde matrix over \\(g\\). Because \\(W_nW_n^{-1}=I\\), and \\(g\\) is primitive, the two transforms form a bijection.\n\n\nTry It Yourself\n\nCompute NTT of \\([1,2,3,4]\\) under mod \\(17\\), then apply INTT.\nVerify \\(\\text{INTT}(\\text{NTT}(a))=a\\).\nCheck inverse properties for different \\(n=8,16\\).\nMultiply polynomials using NTT + INTT pipeline.\nCompare modular vs floating-point FFT.\n\n\n\nTest Cases\n\n\n\nInput \\(A\\)\nOutput \\(a\\) (mod 17)\n\n\n\n\n[10,2,16,15]\n[1,2,3,4]\n\n\n[4,13,11,10,1,0,0,0]\n[1,2,3,4,5]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe INTT closes the circle: every modular transformation now has a perfect inverse. From number theory to code, it keeps exactness at every step.\n\n\n\n569 Bluestein’s Algorithm\nBluestein’s Algorithm, also known as the chirp z-transform, is a clever method to compute a Discrete Fourier Transform (DFT) of arbitrary length \\(n\\) using a Fast Fourier Transform (FFT) of length \\(m \\ge 2n-1\\). Unlike Cooley–Tukey, it works even when \\(n\\) is not a power of two.\n\nWhat Problem Are We Solving?\nStandard FFTs (like Cooley–Tukey) require \\(n\\) to be a power of two for fast computation. What if we want to compute the DFT for an arbitrary \\(n\\), such as \\(n=12\\), \\(30\\), or a prime number?\nBluestein’s algorithm rewrites the DFT as a convolution, which can then be computed efficiently using any FFT of sufficient size.\nGiven a sequence \\(a=[a_0,a_1,\\ldots,a_{n-1}]\\), we want:\n\\[\nA_k = \\sum_{j=0}^{n-1} a_j \\cdot \\omega_n^{jk}, \\quad 0 \\le k &lt; n\n\\]\nwhere \\(\\omega_n = e^{-2\\pi i / n}\\).\n\n\nHow Does It Work (Plain Language)\nBluestein’s algorithm transforms the DFT into a convolution problem:\n\\[\nA_k = \\sum_{j=0}^{n-1} a_j \\cdot \\omega_n^{j^2/2} \\cdot \\omega_n^{(k-j)^2/2}\n\\]\nDefine:\n\n\\(b_j = a_j \\cdot \\omega_n^{j^2/2}\\)\n\\(c_j = \\omega_n^{-j^2/2}\\)\n\nThen \\(A_k\\) is the convolution of \\(b\\) and \\(c\\), scaled by \\(\\omega_n^{k^2/2}\\).\nWe can compute this convolution using FFT-based polynomial multiplication, even when \\(n\\) is arbitrary.\n\n\nAlgorithm Steps\n\nPrecompute chirp factors \\(\\omega_n^{j^2/2}\\) and their inverses.\nBuild sequences \\(b_j\\) and \\(c_j\\).\nPad both to length \\(m \\ge 2n-1\\).\nPerform FFT-based convolution: \\[\nd = \\text{IFFT}(\\text{FFT}(b) \\cdot \\text{FFT}(c))\n\\]\nExtract \\(A_k = d_k \\cdot \\omega_n^{-k^2/2}\\) for \\(k=0,\\ldots,n-1\\).\n\n\n\nTiny Code (Python)\nimport cmath\nimport math\n\ndef next_power_of_two(x):\n    return 1 &lt;&lt; (x - 1).bit_length()\n\ndef fft(a, invert=False):\n    n = len(a)\n    if n == 1:\n        return a\n    a_even = fft(a[0::2], invert)\n    a_odd = fft(a[1::2], invert)\n    ang = 2 * math.pi / n * (-1 if not invert else 1)\n    w, wn = 1, cmath.exp(1j * ang)\n    y = [0] * n\n    for k in range(n // 2):\n        t = w * a_odd[k]\n        y[k] = a_even[k] + t\n        y[k + n // 2] = a_even[k] - t\n        w *= wn\n    if invert:\n        for i in range(n):\n            y[i] /= 2\n    return y\n\ndef bluestein_dft(a):\n    n = len(a)\n    m = next_power_of_two(2 * n - 1)\n    ang = math.pi / n\n    w = [cmath.exp(-1j * ang * j * j) for j in range(n)]\n    b = [a[j] * w[j] for j in range(n)] + [0] * (m - n)\n    c = [w[j].conjugate() for j in range(n)] + [0] * (m - n)\n\n    B = fft(b)\n    C = fft(c)\n    D = [B[i] * C[i] for i in range(m)]\n    d = fft(D, invert=True)\n\n    return [d[k] * w[k] for k in range(n)]\n\n# Example\na = [1, 2, 3]\nprint(\"DFT via Bluestein:\", bluestein_dft(a))\n\n\nWhy It Matters\n\nHandles arbitrary-length DFTs (prime \\(n\\), non-power-of-two)\nWidely used in signal processing, polynomial arithmetic, NTT generalization\nEnables uniform FFT pipelines without length restriction\nCore idea: Chirp-z transform → Convolution → FFT\n\n\n\nA Gentle Proof (Why It Works)\nWe rewrite DFT terms:\n\\[\nA_k = \\sum_{j=0}^{n-1} a_j \\cdot \\omega_n^{jk}\n\\]\nMultiply and divide by \\(\\omega_n^{(j^2 + k^2)/2}\\):\n\\[\nA_k = \\omega_n^{-k^2/2} \\sum_{j=0}^{n-1} (a_j \\cdot \\omega_n^{j^2/2}) \\cdot \\omega_n^{(k-j)^2/2}\n\\]\nThis is a discrete convolution of two sequences, computable via FFT. Thus, DFT reduces to convolution, enabling \\(O(m\\log m)\\) performance for any \\(n\\).\n\n\nTry It Yourself\n\nCompute DFT for \\(n=6\\), \\(a=[1,2,3,4,5,6]\\).\nCompare results from Bluestein and Cooley–Tukey (if \\(n\\) is power of two).\nTry \\(n=7\\) (prime), only Bluestein works efficiently.\nVerify that applying inverse DFT returns the original sequence.\nExplore zero-padding effect when \\(m&gt;2n-1\\).\n\n\n\nTest Cases\n\n\n\nInput \\(a\\)\n\\(n\\)\nOutput (approx)\n\n\n\n\n[1,2,3]\n3\n[6,(-1.5+0.866i),(-1.5-0.866i)]\n\n\n[1,2,3,4,5]\n5\n[15,-2.5+3.44i,-2.5+0.81i,-2.5-0.81i,-2.5-3.44i]\n\n\n\n\n\nComplexity\n\nTime: \\(O(m\\log m)\\) where \\(m\\ge2n-1\\)\nSpace: \\(O(m)\\)\n\nBluestein’s algorithm bridges the gap between power-of-two FFTs and general-length DFTs, turning every sequence into a convolutional melody computable by fast transforms.\n\n\n\n570 FFT-Based Multiplication\nFFT-Based Multiplication uses the Fast Fourier Transform to multiply large integers or polynomials efficiently by transforming multiplication into pointwise products in the frequency domain.\nInstead of multiplying term by term (which is \\(O(n^2)\\)), we leverage the FFT to compute the convolution in \\(O(n\\log n)\\) time.\n\nWhat Problem Are We Solving?\nWhen multiplying two large polynomials (or integers), the naive approach requires \\(O(n^2)\\) operations. For large \\(n\\), this becomes impractical.\nWe want to compute:\n\\[\nC(x) = A(x) \\cdot B(x)\n\\]\nwhere\n\\[\nA(x) = \\sum_{i=0}^{n-1} a_i x^i,\\quad B(x) = \\sum_{j=0}^{n-1} b_j x^j\n\\]\nWe need coefficients \\(c_k\\) of \\(C(x)\\) such that\n\\[\nc_k = \\sum_{i+j=k} a_i b_j\n\\]\nThis is exactly a convolution. The FFT computes it efficiently.\n\n\nHow Does It Work (Plain Language)\n\nPad the sequences so their lengths are a power of two and large enough to hold the full result.\nFFT-transform both sequences to frequency space.\nMultiply the results elementwise.\nInverse FFT to bring back the coefficients.\nRound to nearest integer (for integer multiplication).\n\nIt’s like tuning two melodies into frequency space, multiplying harmonics, and transforming them back into time.\n\n\nAlgorithm Steps\n\nLet \\(A\\) and \\(B\\) be coefficient arrays.\nChoose \\(n\\) as the smallest power of two \\(\\ge 2 \\cdot \\max(\\text{len}(A), \\text{len}(B))\\).\nPad \\(A\\) and \\(B\\) to length \\(n\\).\nCompute FFT\\((A)\\) and FFT\\((B)\\).\nCompute \\(C'[k] = A'[k] \\cdot B'[k]\\).\nCompute \\(C = \\text{IFFT}(C')\\).\nRound real parts of \\(C\\) to nearest integer.\n\n\n\nTiny Code (Python)\nimport cmath\nimport math\n\ndef fft(a, invert=False):\n    n = len(a)\n    if n == 1:\n        return a\n    a_even = fft(a[0::2], invert)\n    a_odd = fft(a[1::2], invert)\n    ang = 2 * math.pi / n * (-1 if not invert else 1)\n    w, wn = 1, cmath.exp(1j * ang)\n    y = [0] * n\n    for k in range(n // 2):\n        t = w * a_odd[k]\n        y[k] = a_even[k] + t\n        y[k + n // 2] = a_even[k] - t\n        w *= wn\n    if invert:\n        for i in range(n):\n            y[i] /= 2\n    return y\n\ndef multiply(a, b):\n    n = 1\n    while n &lt; len(a) + len(b):\n        n &lt;&lt;= 1\n    fa = a + [0] * (n - len(a))\n    fb = b + [0] * (n - len(b))\n    FA = fft(fa)\n    FB = fft(fb)\n    FC = [FA[i] * FB[i] for i in range(n)]\n    C = fft(FC, invert=True)\n    return [round(c.real) for c in C]\n\n# Example: Multiply (1 + 2x + 3x^2) * (4 + 5x + 6x^2)\nprint(multiply([1, 2, 3], [4, 5, 6]))\n# Output: [4, 13, 28, 27, 18]\n\n\nWhy It Matters\n\nFoundation of big integer arithmetic (used in libraries like GMP).\nEnables fast polynomial multiplication.\nUsed in cryptography, signal processing, and FFT-based convolution.\nScales to millions of terms efficiently.\n\n\n\nA Gentle Proof (Why It Works)\nLet\n\\[\nC(x) = A(x)B(x)\n\\]\nIn the frequency domain (using DFT):\n\\[\n\\text{DFT}(C) = \\text{DFT}(A) \\cdot \\text{DFT}(B)\n\\]\nBy the Convolution Theorem, multiplication in frequency space corresponds to convolution in time:\n\\[\nC = \\text{IFFT}(\\text{FFT}(A) \\cdot \\text{FFT}(B))\n\\]\nThus, we get \\(c_k = \\sum_{i+j=k} a_i b_j\\) automatically after inverse transform.\n\n\nTry It Yourself\n\nMultiply \\((1+2x+3x^2)\\) and \\((4+5x+6x^2)\\) by hand.\nImplement integer multiplication using base-10 digits as coefficients.\nCompare FFT vs naive multiplication for \\(n=1024\\).\nTry with complex or modular arithmetic.\nExplore rounding issues and precision.\n\n\n\nTest Cases\n\n\n\nA(x)\nB(x)\nResult\n\n\n\n\n[1,2,3]\n[4,5,6]\n[4,13,28,27,18]\n\n\n[3,2,1]\n[1,0,1]\n[3,2,4,2,1]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nFFT-based multiplication turns a slow quadratic process into a symphony of frequency-domain operations, fast, elegant, and scalable.\n\n\n\n\nSection 58. Numerical Methods\n\n571 Newton–Raphson\nNewton–Raphson is a fast root finding method that refines a guess \\(x\\) for \\(f(x)=0\\) by following the tangent line of \\(f\\) at \\(x\\). It has quadratic convergence near a simple root if the derivative does not vanish.\n\nWhat Problem Are We Solving?\nGiven a differentiable function \\(f:\\mathbb{R}\\to\\mathbb{R}\\), find \\(x^*\\) such that \\(f(x^*)=0\\). Typical use cases include solving nonlinear equations, optimizing \\(1\\)D functions via \\(f^\\prime(x)=0\\), and as an inner step in larger numerical methods.\n\n\nHow Does It Work (Plain Language)\nAt a current guess \\(x_k\\), approximate \\(f\\) by its tangent: \\[\nf(x)\\approx f(x_k)+f^\\prime(x_k)(x-x_k).\n\\] Set this linear model to zero and solve for the intercept: \\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f^\\prime(x_k)}.\n\\] Repeat until the change is small or \\(|f(x_k)|\\) is small.\nUpdate rule \\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f^\\prime(x_k)}.\n\\]\n\n\nExample\nSolve \\(x^2-2=0\\) (square root of \\(2\\)).\nLet \\(f(x)=x^2-2\\), \\(f^\\prime(x)=2x\\), start \\(x_0=1\\).\n\n\\(x_1=1-\\frac{-1}{2}=1.5\\)\n\\(x_2=1.5-\\frac{1.5^2-2}{3}=1.416\\overline{6}\\)\n\\(x_3\\approx1.4142157\\)\n\nRapid approach to \\(\\sqrt{2}\\approx1.41421356\\).\n\n\nTiny Code\nPython\ndef newton(f, df, x0, tol=1e-10, max_iter=100):\n    x = x0\n    for _ in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        if dfx == 0:\n            raise ZeroDivisionError(\"Derivative became zero\")\n        x_new = x - fx / dfx\n        if abs(x_new - x) &lt;= tol:\n            return x_new\n        x = x_new\n    return x\n\n# Example: sqrt(2)\nroot = newton(lambda x: x*x - 2, lambda x: 2*x, 1.0)\nprint(root)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble newton(double (*f)(double), double (*df)(double),\n              double x0, double tol, int max_iter) {\n    double x = x0;\n    for (int i = 0; i &lt; max_iter; i++) {\n        double fx = f(x), dfx = df(x);\n        if (dfx == 0.0) break;\n        double x_new = x - fx / dfx;\n        if (fabs(x_new - x) &lt;= tol) return x_new;\n        x = x_new;\n    }\n    return x;\n}\n\ndouble f(double x){ return x*x - 2.0; }\ndouble df(double x){ return 2.0*x; }\n\nint main(void){\n    double r = newton(f, df, 1.0, 1e-10, 100);\n    printf(\"%.12f\\n\", r);\n    return 0;\n}\n\n\nWhy It Matters\n\nVery fast near a simple root: error roughly squares each step\nWidely used inside solvers for nonlinear systems, optimization, and implicit ODE steps\nEasy to implement when \\(f^\\prime\\) is available or cheap to approximate\n\n\n\nA Gentle Proof Idea\nIf \\(f\\in C^2\\) and \\(x^*\\) is a simple root with \\(f(x^*)=0\\) and \\(f^\\prime(x^*)\\ne0\\), a Taylor expansion around \\(x^*\\) gives \\[\nf(x)=f^\\prime(x^*)(x-x^*)+\\tfrac12 f^{\\prime\\prime}(\\xi)(x-x^*)^2.\n\\] Plugging the Newton update shows the new error \\(e_{k+1}=x_{k+1}-x^*\\) satisfies \\[\ne_{k+1}\\approx -\\frac{f^{\\prime\\prime}(x^*)}{2f^\\prime(x^*)}e_k^2,\n\\] which is quadratic convergence when \\(e_k\\) is small.\n\n\nPractical Tips\n\nChoose a good initial guess \\(x_0\\) to avoid divergence\nGuard against \\(f^\\prime(x_k)=0\\) or tiny derivatives\nUse damping: \\(x_{k+1}=x_k-\\alpha\\frac{f(x_k)}{f^\\prime(x_k)}\\) with \\(0&lt;\\alpha\\le1\\) if steps overshoot\nStop criteria: \\(|f(x_k)|\\le\\varepsilon\\) or \\(|x_{k+1}-x_k|\\le\\varepsilon\\)\n\n\n\nTry It Yourself\n\nSolve \\(\\cos x - x=0\\) starting at \\(x_0=0.5\\).\nFind cube root of \\(5\\) via \\(f(x)=x^3-5\\).\nMinimize \\(g(x)=(x-3)^2\\) by applying Newton to \\(g^\\prime(x)=0\\).\nExperiment with a poor initial guess for \\(f(x)=\\tan x - x\\) near \\(\\pi/2\\) and observe failure modes.\nAdd line search or damping and compare robustness.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\\(f(x)\\)\n\\(f^\\prime(x)\\)\nRoot\nStart \\(x_0\\)\nIter to \\(10^{-10}\\)\n\n\n\n\n\\(x^2-2\\)\n\\(2x\\)\n\\(\\sqrt{2}\\)\n\\(1.0\\)\n5\n\n\n\\(\\cos x - x\\)\n\\(-,\\sin x - 1\\)\n\\(\\approx0.739085\\)\n\\(0.5\\)\n5–6\n\n\n\\(x^3-5\\)\n\\(3x^2\\)\n\\(\\sqrt[3]{5}\\)\n\\(1.5\\)\n6–7\n\n\n\n(Iterations are indicative and depend on tolerances.)\n\n\nComplexity\n\nPer iteration: one \\(f\\) and one \\(f^\\prime\\) evaluation plus \\(O(1)\\) arithmetic\nConvergence: typically quadratic near a simple root\nOverall cost: small number of iterations for well behaved problems\n\nNewton–Raphson is the standard \\(1\\)D root finder when derivatives are available and the initial guess is reasonable. It is simple, fast, and forms the backbone of many higher dimensional methods.\n\n\n\n572 Bisection Method\nThe Bisection Method is one of the simplest and most reliable ways to find a root of a continuous function. It’s a “divide and narrow” search for \\(x^*\\) such that \\(f(x^*)=0\\) within an interval where the function changes sign.\n\nWhat Problem Are We Solving?\nWe want to solve \\(f(x)=0\\) for a continuous function \\(f\\) when we know two points \\(a\\) and \\(b\\) such that:\n\\[\nf(a)\\cdot f(b) &lt; 0\n\\]\nThat means the function crosses zero between \\(a\\) and \\(b\\) by the Intermediate Value Theorem.\n\n\nHow Does It Work (Plain Language)\nAt each step:\n\nCompute midpoint \\(m=\\frac{a+b}{2}\\)\nCheck sign of \\(f(m)\\)\n\nIf \\(f(a)\\cdot f(m) &lt; 0\\), root is between \\(a\\) and \\(m\\)\nElse root is between \\(m\\) and \\(b\\)\n\nReplace the interval with the new one and repeat until the interval is small enough\n\nIt halves the search space every step, like a binary search for roots.\n\n\nAlgorithm Steps\n\nStart with \\([a,b]\\) such that \\(f(a)\\cdot f(b)&lt;0\\)\nWhile \\(|b-a|&gt;\\varepsilon\\):\n\n\\(m=\\frac{a+b}{2}\\)\nIf \\(f(m)=0\\) (or close enough), stop\nElse, pick the half where sign changes\n\nReturn midpoint as approximate root\n\n\n\nTiny Code\nPython\ndef bisection(f, a, b, tol=1e-10, max_iter=100):\n    fa, fb = f(a), f(b)\n    if fa * fb &gt;= 0:\n        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n    for _ in range(max_iter):\n        m = 0.5 * (a + b)\n        fm = f(m)\n        if abs(fm) &lt; tol or (b - a) / 2 &lt; tol:\n            return m\n        if fa * fm &lt; 0:\n            b, fb = m, fm\n        else:\n            a, fa = m, fm\n    return 0.5 * (a + b)\n\n# Example: root of x^3 - x - 2 = 0\nroot = bisection(lambda x: x3 - x - 2, 1, 2)\nprint(root)  # ~1.5213797\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) { return x*x*x - x - 2; }\n\ndouble bisection(double a, double b, double tol, int max_iter) {\n    double fa = f(a), fb = f(b);\n    if (fa * fb &gt;= 0) return NAN;\n    for (int i = 0; i &lt; max_iter; i++) {\n        double m = 0.5 * (a + b);\n        double fm = f(m);\n        if (fabs(fm) &lt; tol || (b - a) / 2 &lt; tol) return m;\n        if (fa * fm &lt; 0) {\n            b = m; fb = fm;\n        } else {\n            a = m; fa = fm;\n        }\n    }\n    return 0.5 * (a + b);\n}\n\nint main(void) {\n    printf(\"%.10f\\n\", bisection(1, 2, 1e-10, 100));\n    return 0;\n}\n\n\nWhy It Matters\n\nGuaranteed convergence if \\(f\\) is continuous and sign changes\nNo derivative required (unlike Newton–Raphson)\nRobust and simple to implement\nBasis for hybrid methods like Brent’s method\n\n\n\nA Gentle Proof (Why It Works)\nBy the Intermediate Value Theorem, if \\(f(a)\\cdot f(b)&lt;0\\) and \\(f\\) is continuous, there exists \\(x^*\\in[a,b]\\) such that \\(f(x^*)=0\\).\nEach iteration halves the interval: \\[\n|b_{k+1}-a_{k+1}|=\\frac{1}{2}|b_k-a_k|\n\\] After \\(k\\) iterations: \\[\n|b_k-a_k|=\\frac{1}{2^k}|b_0-a_0|\n\\] To achieve tolerance \\(\\varepsilon\\): \\[\nk \\ge \\log_2\\frac{|b_0-a_0|}{\\varepsilon}\n\\]\n\n\nTry It Yourself\n\nFind root of \\(f(x)=x^3-x-2\\) on \\([1,2]\\).\nTry \\(f(x)=\\cos x - x\\) on \\([0,1]\\).\nCompare iterations vs Newton–Raphson.\nTest failure if \\(f(a)\\) and \\(f(b)\\) have same sign.\nObserve how tolerance affects accuracy and iterations.\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\nRoot (approx)\nIterations (ε=1e-6)\n\n\n\n\n\\(x^2-2\\)\n[1,2]\n1.414214\n20\n\n\n\\(x^3-x-2\\)\n[1,2]\n1.521380\n20\n\n\n\\(\\cos x - x\\)\n[0,1]\n0.739085\n20\n\n\n\n\n\nComplexity\n\nTime: \\(O(\\log_2(\\frac{b-a}{\\varepsilon}))\\)\nSpace: \\(O(1)\\)\n\nThe Bisection Method trades speed for certainty, it never fails when the sign condition holds, making it a cornerstone of reliable root finding.\n\n\n\n573 Secant Method\nThe Secant Method is a root-finding algorithm that uses two initial guesses and repeatedly refines them using secant lines, straight lines passing through two recent points of the function. It’s like a derivative-free Newton–Raphson: instead of using \\(f'(x)\\), we estimate it from the slope of the secant.\n\nWhat Problem Are We Solving?\nWe want to solve for \\(x^*\\) such that \\(f(x^*)=0\\), but sometimes we don’t have the derivative \\(f'(x)\\). The secant method approximates it numerically:\n\\[\nf'(x_k)\\approx \\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}\n\\]\nBy replacing the exact derivative with this difference quotient, we still follow the Newton-like update.\n\n\nHow Does It Work (Plain Language)\nThink of drawing a line through two points \\((x_{k-1},f(x_{k-1}))\\) and \\((x_k,f(x_k))\\). That line crosses the \\(x\\)-axis at a new guess \\(x_{k+1}\\). Repeat the process until convergence.\nThe update formula is:\n\\[\nx_{k+1}=x_k-f(x_k)\\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}\n\\]\nEach iteration uses the last two estimates instead of one (like Newton).\n\n\nAlgorithm Steps\n\nStart with two initial guesses \\(x_0\\) and \\(x_1\\) such that \\(f(x_0)\\ne f(x_1)\\).\nRepeat until convergence:\n\\[\nx_{k+1}=x_k-f(x_k)\\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}\n\\]\nStop when \\(|x_{k+1}-x_k|&lt;\\varepsilon\\) or \\(|f(x_{k+1})|&lt;\\varepsilon\\).\n\n\n\nTiny Code\nPython\ndef secant(f, x0, x1, tol=1e-10, max_iter=100):\n    f0, f1 = f(x0), f(x1)\n    for _ in range(max_iter):\n        if f1 == f0:\n            raise ZeroDivisionError(\"Division by zero in secant method\")\n        x2 = x1 - f1 * (x1 - x0) / (f1 - f0)\n        if abs(x2 - x1) &lt; tol:\n            return x2\n        x0, x1 = x1, x2\n        f0, f1 = f1, f(x1)\n    return x1\n\n# Example: root of x^3 - x - 2 = 0\nroot = secant(lambda x: x3 - x - 2, 1, 2)\nprint(root)  # ~1.5213797\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) { return x*x*x - x - 2; }\n\ndouble secant(double x0, double x1, double tol, int max_iter) {\n    double f0 = f(x0), f1 = f(x1);\n    for (int i = 0; i &lt; max_iter; i++) {\n        if (f1 == f0) break;\n        double x2 = x1 - f1 * (x1 - x0) / (f1 - f0);\n        if (fabs(x2 - x1) &lt; tol) return x2;\n        x0 = x1; f0 = f1;\n        x1 = x2; f1 = f(x1);\n    }\n    return x1;\n}\n\nint main(void) {\n    printf(\"%.10f\\n\", secant(1, 2, 1e-10, 100));\n    return 0;\n}\n\n\nWhy It Matters\n\nNo derivative needed (uses finite differences)\nFaster than Bisection (superlinear, ≈1.618 order)\nCommonly used when \\(f'(x)\\) is expensive or unavailable\nA stepping stone to hybrid methods (e.g. Brent’s method)\n\n\n\nA Gentle Proof (Why It Works)\nNewton’s method update:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}\n\\]\nApproximate \\(f'(x_k)\\) via difference quotient:\n\\[\nf'(x_k)\\approx\\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}\n\\]\nSubstitute into Newton’s update:\n\\[\nx_{k+1}=x_k-f(x_k)\\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}\n\\]\nThus, it’s Newton’s method without explicit derivatives, retaining superlinear convergence when close to the root.\n\n\nTry It Yourself\n\nSolve \\(x^3-x-2=0\\) with \\((x_0,x_1)=(1,2)\\).\nCompare iteration count vs Newton (\\(x_0=1\\)).\nTry \\(\\cos x - x=0\\) with \\((0,1)\\).\nObserve failure when \\(f(x_k)=f(x_{k-1})\\).\nAdd fallback to bisection for robustness.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nFunction\nInitial \\((x_0,x_1)\\)\nRoot (approx)\nIterations (ε=1e-10)\n\n\n\n\n\\(x^2-2\\)\n(1,2)\n1.41421356\n6\n\n\n\\(x^3-x-2\\)\n(1,2)\n1.5213797\n7\n\n\n\\(\\cos x - x\\)\n(0,1)\n0.73908513\n6\n\n\n\n\n\nComplexity\n\nTime: \\(O(k)\\) iterations, each \\(O(1)\\) (1 function eval per step)\nConvergence: Superlinear (\\(\\approx1.618\\) order)\nSpace: \\(O(1)\\)\n\nThe Secant Method blends the speed of Newton’s method with the simplicity of bisection, a derivative-free bridge between theory and practicality.\n\n\n\n574 Fixed-Point Iteration\nFixed-Point Iteration is a general method for solving equations of the form \\(x=g(x)\\). Instead of finding where \\(f(x)=0\\), we repeatedly apply a transformation that should converge to a stable point, the fixed point where input equals output.\n\nWhat Problem Are We Solving?\nWe want to find \\(x^*\\) such that\n\\[\nx^* = g(x^*)\n\\]\nIf we can express a problem \\(f(x)=0\\) as \\(x=g(x)\\), then the solution of one is the fixed point of the other. For example, solving \\(x^2-2=0\\) is equivalent to solving\n\\[\nx = \\sqrt{2}\n\\]\nor in iterative form,\n\\[\nx_{k+1} = g(x_k) = \\frac{1}{2}\\left(x_k + \\frac{2}{x_k}\\right)\n\\]\n\n\nHow Does It Work (Plain Language)\nStart with an initial guess \\(x_0\\), then keep applying \\(g\\):\n\\[\nx_{k+1} = g(x_k)\n\\]\nIf \\(g\\) is well-behaved (a contraction near \\(x^*\\)), this sequence will settle closer and closer to the fixed point.\nYou can think of it like repeatedly bouncing toward equilibrium, each bounce gets smaller until you land at \\(x^*\\).\n\n\nConvergence Condition\nFixed-point iteration converges if \\(g\\) is continuous near \\(x^*\\) and\n\\[\n|g'(x^*)| &lt; 1\n\\]\nIf \\(|g'(x^*)| &gt; 1\\), the iteration diverges.\nThis means the slope of \\(g\\) at the fixed point must not be too steep, it should “pull” you in rather than push you away.\n\n\nAlgorithm Steps\n\nChoose initial guess \\(x_0\\).\nCompute \\(x_{k+1} = g(x_k)\\).\nStop if \\(|x_{k+1} - x_k| &lt; \\varepsilon\\) or after max iterations.\nReturn \\(x_{k+1}\\) as the approximate root.\n\n\n\nTiny Code\nPython\ndef fixed_point(g, x0, tol=1e-10, max_iter=100):\n    x = x0\n    for _ in range(max_iter):\n        x_new = g(x)\n        if abs(x_new - x) &lt; tol:\n            return x_new\n        x = x_new\n    return x\n\n# Example: Solve x = cos(x)\nroot = fixed_point(lambda x: math.cos(x), 0.5)\nprint(root)  # ~0.739085\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble g(double x) { return cos(x); }\n\ndouble fixed_point(double x0, double tol, int max_iter) {\n    double x = x0, x_new;\n    for (int i = 0; i &lt; max_iter; i++) {\n        x_new = g(x);\n        if (fabs(x_new - x) &lt; tol) return x_new;\n        x = x_new;\n    }\n    return x;\n}\n\nint main(void) {\n    printf(\"%.10f\\n\", fixed_point(0.5, 1e-10, 100));\n    return 0;\n}\n\n\nWhy It Matters\n\nFoundation for Newton–Raphson, Gauss–Seidel, and many nonlinear solvers.\nConceptually simple and general.\nShows how convergence depends on transformation design, not just the function.\n\n\n\nA Gentle Proof (Why It Works)\nSuppose \\(x^*\\) is a fixed point, \\(g(x^*)=x^*\\). Expand \\(g\\) near \\(x^*\\) by Taylor approximation:\n\\[\ng(x) = g(x^*) + g'(x^*)(x - x^*) + O((x-x^*)^2)\n\\]\nSubtract \\(x^*\\):\n\\[\nx_{k+1}-x^* = g'(x^*)(x_k-x^*) + O((x_k-x^*)^2)\n\\]\nSo for small errors, \\[\n|x_{k+1}-x^*| \\approx |g'(x^*)||x_k-x^*|\n\\]\nIf \\(|g'(x^*)|&lt;1\\), the error shrinks every iteration, geometric convergence.\n\n\nTry It Yourself\n\nSolve \\(x=\\cos(x)\\) starting at \\(x_0=0.5\\).\nSolve \\(x=\\sqrt{2}\\) using \\(x_{k+1}=\\frac{1}{2}(x_k+\\frac{2}{x_k})\\).\nTry \\(x=g(x)=1+\\frac{1}{x}\\), watch it diverge.\nExperiment with transformations of \\(f(x)=x^3-x-2\\) into different \\(g(x)\\).\nObserve sensitivity to initial guesses and slope.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\\(g(x)\\)\nStart \\(x_0\\)\nResult (approx)\nConverges?\n\n\n\n\n\\(\\cos x\\)\n0.5\n0.739085\nYes\n\n\n\\(\\frac{1}{2}(x+\\frac{2}{x})\\)\n1\n1.41421356\nYes\n\n\n\\(1+\\frac{1}{x}\\)\n1\nDivergent\nNo\n\n\n\n\n\nComplexity\n\nPer iteration: \\(O(1)\\) (one function evaluation)\nConvergence: Linear (if \\(|g'(x^*)|&lt;1\\))\nSpace: \\(O(1)\\)\n\nFixed-point iteration is the gentle heartbeat of numerical solving, simple, geometric, and foundational to modern root-finding and optimization algorithms.\n\n\n\n575 Gaussian Quadrature\nGaussian Quadrature is a high-accuracy numerical integration method that approximates \\[\nI=\\int_a^b f(x),dx\n\\] by evaluating \\(f(x)\\) at optimally chosen points (nodes) and weighting them carefully. It achieves maximum precision for a given number of evaluations, far more accurate than trapezoidal or Simpson’s rules for smooth functions.\n\nWhat Problem Are We Solving?\nWe want to approximate an integral numerically, but instead of equally spaced sample points, we’ll choose the best possible points to minimize error.\nTraditional methods sample uniformly, but Gaussian quadrature uses orthogonal polynomials (like Legendre, Chebyshev, Laguerre, or Hermite) to determine ideal nodes \\(x_i\\) and weights \\(w_i\\) that make the rule exact for all polynomials up to degree \\(2n-1\\).\nFormally,\n\\[\n\\int_a^b f(x),dx \\approx \\sum_{i=1}^n w_i,f(x_i)\n\\]\n\n\nHow Does It Work (Plain Language)\n\nChoose a set of orthogonal polynomials on \\([a,b]\\) (often Legendre for standard integrals).\nThe roots of the \\(n\\)-th polynomial become the sample points \\(x_i\\).\nCompute corresponding weights \\(w_i\\) so that the formula integrates all polynomials up to degree \\(2n-1\\) exactly.\nEvaluate \\(f(x_i)\\), multiply by weights, and sum.\n\nEach \\(x_i\\) and \\(w_i\\) is precomputed, you just plug in your function.\n\n\nExample: Gauss–Legendre Quadrature on \\([-1,1]\\)\nFor \\(n=2\\): \\[\nx_1=-\\frac{1}{\\sqrt{3}}, \\quad x_2=\\frac{1}{\\sqrt{3}}, \\quad w_1=w_2=1.\n\\]\nThus, \\[\n\\int_{-1}^{1} f(x),dx \\approx f(-1/\\sqrt{3})+f(1/\\sqrt{3}).\n\\]\nFor arbitrary \\([a,b]\\), map via \\[\nt=\\frac{b-a}{2}x+\\frac{a+b}{2},\n\\] and scale the result by \\(\\frac{b-a}{2}\\).\n\n\nTiny Code\nPython\nimport numpy as np\n\ndef gauss_legendre(f, a, b, n=2):\n    # nodes and weights for n=2, extendable for larger n\n    x = np.array([-1/np.sqrt(3), 1/np.sqrt(3)])\n    w = np.array([1.0, 1.0])\n    # transform to [a,b]\n    t = 0.5*(b - a)*x + 0.5*(b + a)\n    return 0.5*(b - a)*np.sum(w * f(t))\n\n# Example: integrate f(x)=x^2 from 0 to 1\nresult = gauss_legendre(lambda x: x2, 0, 1)\nprint(result)  # ~0.333333\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) { return x*x; }\n\ndouble gauss_legendre(double a, double b) {\n    double x1 = -1.0/sqrt(3.0), x2 = 1.0/sqrt(3.0);\n    double w1 = 1.0, w2 = 1.0;\n    double t1 = 0.5*(b - a)*x1 + 0.5*(b + a);\n    double t2 = 0.5*(b - a)*x2 + 0.5*(b + a);\n    return 0.5*(b - a)*(w1*f(t1) + w2*f(t2));\n}\n\nint main(void) {\n    printf(\"%.6f\\n\", gauss_legendre(0, 1)); // 0.333333\n    return 0;\n}\n\n\nWhy It Matters\n\nHigh precision with few points\nExact for all polynomials up to degree \\(2n-1\\)\nWorks beautifully for smooth integrands\nForms the foundation for spectral methods, finite elements, and probability integration\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(p_n(x)\\) be an orthogonal polynomial of degree \\(n\\) on \\([a,b]\\) with respect to a weight function \\(w(x)\\). Its \\(n\\) roots \\(x_i\\) satisfy orthogonality:\n\\[\n\\int_a^b p_n(x),p_m(x),w(x),dx=0 \\quad (m&lt;n).\n\\]\nIf \\(f\\) is a polynomial of degree \\(\\le2n-1\\), it can be decomposed into terms up to \\(p_{2n-1}(x)\\), and integrating with the quadrature rule using these roots yields the exact value.\nThus, Gaussian quadrature minimizes integration error within polynomial spaces.\n\n\nTry It Yourself\n\nIntegrate \\(\\sin x\\) from \\(0\\) to \\(\\pi/2\\) using 2- and 3-point Gauss–Legendre.\nCompare with Simpson’s rule.\nExtend to \\(n=3\\) using precomputed nodes and weights.\nTry \\(f(x)=e^x\\) over \\([0,1]\\).\nExperiment with scaling to non-standard intervals \\([a,b]\\).\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\n\\(n\\)\nResult (approx)\nTrue Value\n\n\n\n\n\\(x^2\\)\n[0,1]\n2\n0.333333\n1/3\n\n\n\\(\\sin x\\)\n[0,π/2]\n2\n0.99984\n1.00000\n\n\n\\(e^x\\)\n[0,1]\n2\n1.71828\n1.71828\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) evaluations of \\(f\\)\nSpace: \\(O(n)\\) for nodes and weights\nAccuracy: exact for all polynomials of degree ≤ \\(2n-1\\)\n\nGaussian Quadrature shows how pure mathematics and computation intertwine, orthogonal polynomials guiding us to integrate with surgical precision.\n\n\n\n576 Simpson’s Rule\nSimpson’s Rule is a classical numerical integration method that approximates the integral of a smooth function using parabolas rather than straight lines. It combines the simplicity of the trapezoidal rule with higher-order accuracy, making it one of the most practical methods for evenly spaced data.\n\nWhat Problem Are We Solving?\nWe want to approximate the definite integral\n\\[\nI = \\int_a^b f(x),dx\n\\]\nwhen we only know \\(f(x)\\) at discrete points or cannot find an analytic antiderivative.\nInstead of approximating \\(f\\) by a straight line between each pair of points, Simpson’s Rule uses quadratic interpolation through every two subintervals for much higher accuracy.\n\n\nHow Does It Work (Plain Language)\nImagine you take three points \\((x_0,f(x_0))\\), \\((x_1,f(x_1))\\), \\((x_2,f(x_2))\\) equally spaced by \\(h\\). Fit a parabola through them, integrate that parabola, and repeat.\nFor a single parabolic arc:\n\\[\n\\int_{x_0}^{x_2} f(x),dx \\approx \\frac{h}{3}\\big[f(x_0) + 4f(x_1) + f(x_2)\\big]\n\\]\nFor \\(n\\) subintervals (where \\(n\\) is even):\n\\[\nI \\approx \\frac{h}{3}\\Big[f(x_0) + 4\\sum_{i=1,3,5,\\ldots}^{n-1} f(x_i) + 2\\sum_{i=2,4,6,\\ldots}^{n-2} f(x_i) + f(x_n)\\Big]\n\\]\nwhere \\(h = \\frac{b - a}{n}\\).\n\n\nTiny Code\nPython\ndef simpson(f, a, b, n=100):\n    if n % 2 == 1:\n        n += 1  # must be even\n    h = (b - a) / n\n    s = f(a) + f(b)\n    for i in range(1, n):\n        x = a + i * h\n        s += 4 * f(x) if i % 2 else 2 * f(x)\n    return s * h / 3\n\n# Example: integrate sin(x) from 0 to π\nimport math\nres = simpson(math.sin, 0, math.pi, n=100)\nprint(res)  # ~2.000000\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) { return sin(x); }\n\ndouble simpson(double a, double b, int n) {\n    if (n % 2 == 1) n++; // must be even\n    double h = (b - a) / n;\n    double s = f(a) + f(b);\n    for (int i = 1; i &lt; n; i++) {\n        double x = a + i * h;\n        s += (i % 2 ? 4 : 2) * f(x);\n    }\n    return s * h / 3.0;\n}\n\nint main(void) {\n    printf(\"%.6f\\n\", simpson(0, M_PI, 100)); // 2.000000\n    return 0;\n}\n\n\nWhy It Matters\n\nAccurate and simple for smooth functions\nExact for cubic polynomials\nOften the best balance between accuracy and computation for tabulated data\nForms the backbone of adaptive and composite integration schemes\n\n\n\nA Gentle Proof (Why It Works)\nLet \\(f(x)\\) be approximated by a quadratic polynomial \\[\np(x) = ax^2 + bx + c\n\\] that passes through \\((x_0,f_0)\\), \\((x_1,f_1)\\), \\((x_2,f_2)\\). Integrating \\(p(x)\\) over \\([x_0,x_2]\\) yields\n\\[\n\\int_{x_0}^{x_2} p(x),dx = \\frac{h}{3}\\big[f_0 + 4f_1 + f_2\\big].\n\\]\nBecause the error term depends on \\(f^{(4)}(\\xi)\\), Simpson’s Rule is fourth-order accurate, i.e.\n\\[\nE = -\\frac{(b-a)}{180}h^4f^{(4)}(\\xi)\n\\]\nfor some \\(\\xi\\in[a,b]\\).\n\n\nTry It Yourself\n\nIntegrate \\(\\sin x\\) from \\(0\\) to \\(\\pi\\) (should be \\(2\\)).\nIntegrate \\(x^4\\) from \\(0\\) to \\(1\\), check exactness for polynomials ≤ degree 3.\nCompare with trapezoidal rule for the same \\(n\\).\nExperiment with uneven \\(n\\) and verify convergence.\nImplement adaptive Simpson’s rule for automatic refinement.\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\n\\(n\\)\nSimpson Result\nTrue Value\n\n\n\n\n\\(\\sin x\\)\n[0, π]\n100\n1.999999\n2.000000\n\n\n\\(x^2\\)\n[0, 1]\n10\n0.333333\n1/3\n\n\n\\(e^x\\)\n[0, 1]\n20\n1.718282\n1.718282\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) evaluations of \\(f(x)\\)\nAccuracy: \\(O(h^4)\\)\nSpace: \\(O(1)\\)\n\nSimpson’s Rule is the perfect blend of simplicity and precision, a parabolic leap beyond straight-line approximations.\n\n\n\n577 Trapezoidal Rule\nThe Trapezoidal Rule is one of the simplest numerical integration methods. It approximates the area under a curve by dividing it into trapezoids instead of rectangles, providing a linear interpolation between sampled points.\n\nWhat Problem Are We Solving?\nWe want to estimate\n\\[\nI = \\int_a^b f(x),dx\n\\]\nwhen we only know \\(f(x)\\) at discrete points, or when the integral has no simple analytical form. The idea is to approximate \\(f(x)\\) as piecewise linear between each pair of neighboring points.\n\n\nHow Does It Work (Plain Language)\nIf you know \\(f(a)\\) and \\(f(b)\\), the simplest estimate is the area of a trapezoid:\n\\[\nI \\approx \\frac{b-a}{2},[f(a)+f(b)].\n\\]\nFor multiple subintervals of equal width \\(h=(b-a)/n\\):\n\\[\nI \\approx \\frac{h}{2}\\Big[f(x_0)+2\\sum_{i=1}^{n-1}f(x_i)+f(x_n)\\Big].\n\\]\nEach pair of consecutive points defines a trapezoid, we sum their areas to approximate the total.\n\n\nTiny Code\nPython\ndef trapezoidal(f, a, b, n=100):\n    h = (b - a) / n\n    s = 0.5 * (f(a) + f(b))\n    for i in range(1, n):\n        s += f(a + i * h)\n    return s * h\n\n# Example: integrate e^x from 0 to 1\nimport math\nres = trapezoidal(math.exp, 0, 1, 100)\nprint(res)  # ~1.718282\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x) { return exp(x); }\n\ndouble trapezoidal(double a, double b, int n) {\n    double h = (b - a) / n;\n    double s = 0.5 * (f(a) + f(b));\n    for (int i = 1; i &lt; n; i++) {\n        s += f(a + i * h);\n    }\n    return s * h;\n}\n\nint main(void) {\n    printf(\"%.6f\\n\", trapezoidal(0, 1, 100)); // 1.718282\n    return 0;\n}\n\n\nWhy It Matters\n\nSimple and widely used as a first numerical integration method\nRobust for smooth functions\nServes as a building block for Simpson’s rule and Romberg integration\nWorks directly with tabulated data\n\n\n\nA Gentle Proof (Why It Works)\nOver one subinterval \\([x_i,x_{i+1}]\\), approximate \\(f(x)\\) by a straight line:\n\\[\nf(x)\\approx f(x_i)+\\frac{f(x_{i+1})-f(x_i)}{h}(x-x_i).\n\\]\nIntegrating this line exactly gives\n\\[\n\\int_{x_i}^{x_{i+1}} f(x),dx \\approx \\frac{h}{2}[f(x_i)+f(x_{i+1})].\n\\]\nSumming across all intervals yields the composite trapezoidal rule.\nThe error term for one interval is proportional to the curvature of \\(f\\):\n\\[\nE = -\\frac{(b-a)}{12}h^2f''(\\xi)\n\\]\nfor some \\(\\xi\\in[a,b]\\). Thus, it’s second-order accurate.\n\n\nTry It Yourself\n\nIntegrate \\(\\sin x\\) from \\(0\\) to \\(\\pi\\) (result \\(\\approx2\\)).\nIntegrate \\(x^2\\) from \\(0\\) to \\(1\\) and compare with exact \\(1/3\\).\nCompare error with Simpson’s Rule for the same \\(n\\).\nTry \\(f(x)=1/x\\) on \\([1,2]\\).\nExplore how halving \\(h\\) affects accuracy.\n\n\n\nTest Cases\n\n\n\nFunction\nInterval\n\\(n\\)\nTrapezoidal\nTrue Value\n\n\n\n\n\\(\\sin x\\)\n[0, π]\n100\n1.9998\n2.0000\n\n\n\\(x^2\\)\n[0,1]\n100\n0.33335\n1/3\n\n\n\\(e^x\\)\n[0,1]\n100\n1.71828\n1.71828\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) function evaluations\nAccuracy: \\(O(h^2)\\)\nSpace: \\(O(1)\\)\n\nThe Trapezoidal Rule is the entryway to numerical integration, intuitive, reliable, and surprisingly effective when used with fine discretization or smooth functions.\n\n\n\n578 Runge–Kutta (RK4) Method\nThe Runge–Kutta (RK4) method is one of the most widely used techniques for numerically solving ordinary differential equations (ODEs). It provides a beautiful balance between accuracy and computational simplicity, using multiple slope evaluations within each step to achieve fourth-order precision.\n\nWhat Problem Are We Solving?\nWe want to solve an initial value problem (IVP):\n\\[\n\\frac{dy}{dx}=f(x,y), \\quad y(x_0)=y_0\n\\]\nWhen no analytical solution exists (or is hard to find), RK4 approximates \\(y(x)\\) step by step, with high accuracy.\n\n\nHow Does It Work (Plain Language)\nInstead of taking a single slope per step (like Euler’s method), RK4 samples four slopes and combines them:\n\\[\n\\begin{aligned}\nk_1 &= f(x_n, y_n),\\\nk_2 &= f(x_n + h/2,, y_n + h k_1/2),\\\nk_3 &= f(x_n + h/2,, y_n + h k_2/2),\\\nk_4 &= f(x_n + h,, y_n + h k_3),\\\ny_{n+1} &= y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).\n\\end{aligned}\n\\]\nThis weighted average captures both curvature and local behavior with surprising precision.\n\n\nTiny Code\nPython\ndef rk4(f, x0, y0, h, n):\n    x, y = x0, y0\n    for _ in range(n):\n        k1 = f(x, y)\n        k2 = f(x + h/2, y + h*k1/2)\n        k3 = f(x + h/2, y + h*k2/2)\n        k4 = f(x + h, y + h*k3)\n        y += h*(k1 + 2*k2 + 2*k3 + k4)/6\n        x += h\n    return x, y\n\n# Example: dy/dx = y, y(0) = 1 → true solution y = e^x\nimport math\nf = lambda x, y: y\nx, y = rk4(f, 0, 1, 0.1, 10)\nprint(y, \"vs\", math.e)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x, double y) { return y; } // dy/dx = y\n\nvoid rk4(double x0, double y0, double h, int n) {\n    double x = x0, y = y0;\n    for (int i = 0; i &lt; n; i++) {\n        double k1 = f(x, y);\n        double k2 = f(x + h/2, y + h*k1/2);\n        double k3 = f(x + h/2, y + h*k2/2);\n        double k4 = f(x + h, y + h*k3);\n        y += h*(k1 + 2*k2 + 2*k3 + k4)/6.0;\n        x += h;\n    }\n    printf(\"x=%.2f, y=%.6f\\n\", x, y);\n}\n\nint main(void) {\n    rk4(0, 1, 0.1, 10); // y(1) ≈ e ≈ 2.718282\n    return 0;\n}\n\n\nWhy It Matters\n\nFourth-order accuracy without complex derivatives\nUsed in physics, engineering, and machine learning (ODE solvers)\nFoundation for adaptive step-size solvers and neural ODEs\nMuch more stable and accurate than Euler or midpoint methods\n\n\n\nA Gentle Proof (Why It Works)\nExpand the true solution with Taylor series:\n\\[\ny(x+h)=y(x)+h y'(x)+\\frac{h^2}{2}y''(x)+\\frac{h^3}{6}y^{(3)}(x)+O(h^4)\n\\]\nRK4’s combination of \\(k_1,k_2,k_3,k_4\\) reproduces all terms up to \\(h^4\\), giving global error \\(O(h^4)\\).\nEach \\(k_i\\) estimates the derivative at intermediate points, forming a weighted average that captures local curvature.\n\n\nTry It Yourself\n\nSolve \\(dy/dx = y\\) from \\(x=0\\) to \\(x=1\\) with \\(h=0.1\\).\nCompare with Euler’s method.\nTry \\(dy/dx = -2y\\) with \\(y(0)=1\\).\nVisualize \\(k_1,k_2,k_3,k_4\\) on the slope field.\nTest with nonlinear \\(f(x,y)=x^2+y^2\\).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nDifferential Equation\nInterval\nStep h\nResult\nTrue Value\n\n\n\n\n\\(dy/dx=y\\)\n[0,1]\n0.1\n2.71828\n\\(e\\)\n\n\n\\(dy/dx=-2y\\)\n[0,1]\n0.1\n0.13534\n\\(e^{-2}\\)\n\n\n\\(dy/dx=x+y\\)\n[0,1]\n0.1\n2.7183\nanalytic \\(2e-1\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\) evaluations of \\(f\\) (4 per step)\nAccuracy: Global error \\(O(h^4)\\)\nSpace: \\(O(1)\\)\n\nThe Runge–Kutta method is a masterpiece of numerical design, simple enough to code in minutes, powerful enough to drive modern simulations and control systems.\n\n\n\n579 Euler’s Method\nEuler’s Method is the simplest numerical procedure for solving ordinary differential equations (ODEs). It’s the “hello world” of numerical integration, conceptually clear, easy to implement, and forms the foundation for more advanced methods like Runge–Kutta.\n\nWhat Problem Are We Solving?\nWe want to approximate the solution of an initial value problem:\n\\[\n\\frac{dy}{dx}=f(x,y), \\quad y(x_0)=y_0.\n\\]\nIf we can’t solve this analytically, we can approximate \\(y(x)\\) at discrete points using small steps \\(h\\).\n\n\nHow Does It Work (Plain Language)\nThe key idea: use the slope at the current point to predict the next one.\nAt each step:\n\\[\ny_{n+1} = y_n + h f(x_n, y_n),\n\\]\nand\n\\[\nx_{n+1} = x_n + h.\n\\]\nThis is just “take a small step along the tangent.” The smaller \\(h\\), the better the approximation.\n\n\nTiny Code\nPython\ndef euler(f, x0, y0, h, n):\n    x, y = x0, y0\n    for _ in range(n):\n        y += h * f(x, y)\n        x += h\n    return x, y\n\n# Example: dy/dx = y, y(0)=1 -&gt; y=e^x\nimport math\nf = lambda x, y: y\nx, y = euler(f, 0, 1, 0.1, 10)\nprint(y, \"vs\", math.e)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble f(double x, double y) { return y; }\n\nvoid euler(double x0, double y0, double h, int n) {\n    double x = x0, y = y0;\n    for (int i = 0; i &lt; n; i++) {\n        y += h * f(x, y);\n        x += h;\n    }\n    printf(\"x=%.2f, y=%.6f\\n\", x, y);\n}\n\nint main(void) {\n    euler(0, 1, 0.1, 10); // y(1) ≈ 2.5937 vs e ≈ 2.7183\n    return 0;\n}\n\n\nWhy It Matters\n\nThe first step into numerical ODEs\nSimple, intuitive, and educational\nShows the tradeoff between step size and accuracy\nUsed as a building block for Runge–Kutta, Heun’s, and predictor–corrector methods\n\n\n\nA Gentle Proof (Why It Works)\nUsing Taylor expansion:\n\\[\ny(x+h) = y(x) + h y'(x) + \\frac{h^2}{2} y''(\\xi)\n\\]\nSince \\(y'(x)=f(x,y)\\), Euler’s method approximates by ignoring higher-order terms:\n\\[\ny_{n+1} \\approx y_n + h f(x_n, y_n).\n\\]\nThe local error is \\(O(h^2)\\), and the global error is \\(O(h)\\).\n\n\nTry It Yourself\n\nSolve \\(dy/dx=y\\) from \\(0\\) to \\(1\\) with \\(h=0.1\\).\nDecrease \\(h\\) and observe convergence toward \\(e\\).\nTry \\(dy/dx=-2y\\) and plot exponential decay.\nCompare with Runge–Kutta for same step size.\nImplement a version that stores and plots all \\((x,y)\\) pairs.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\nDifferential Equation\nInterval\nStep \\(h\\)\nEuler Result\nTrue Value\n\n\n\n\n\\(dy/dx=y\\)\n[0,1]\n0.1\n2.5937\n2.7183\n\n\n\\(dy/dx=-2y\\)\n[0,1]\n0.1\n0.1615\n0.1353\n\n\n\\(dy/dx=x+y\\)\n[0,1]\n0.1\n2.65\n2.7183\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nAccuracy: Global \\(O(h)\\)\nSpace: \\(O(1)\\)\n\nEuler’s Method is the simplest step into numerical dynamics, a straight line drawn into the curved world of differential equations.\n\n\n\n580 Gradient Descent (1D Numerical Optimization)\nGradient Descent is a simple yet powerful iterative algorithm for finding minima of differentiable functions. In one dimension, it’s an intuitive process: move in the opposite direction of the slope until the function stops decreasing.\n\nWhat Problem Are We Solving?\nWe want to find the local minimum of a real-valued function \\(f(x)\\), that is:\n\\[\n\\min_x f(x)\n\\]\nIf \\(f'(x)\\) exists but solving \\(f'(x)=0\\) analytically is difficult, we can approach the minimum step-by-step using the gradient (slope).\n\n\nHow Does It Work (Plain Language)\nAt each iteration, move opposite to the derivative, scaled by a learning rate \\(\\eta\\):\n\\[\nx_{t+1} = x_t - \\eta f'(x_t)\n\\]\nThe derivative \\(f'(x_t)\\) points uphill; subtracting it moves downhill. The step size \\(\\eta\\) determines how far we move.\n\nIf \\(\\eta\\) is too small, convergence is slow.\nIf \\(\\eta\\) is too large, we may overshoot or diverge.\n\nThe process repeats until \\(|f'(x_t)|\\) becomes very small.\n\n\nTiny Code\nPython\ndef gradient_descent_1d(df, x0, eta=0.1, tol=1e-6, max_iter=1000):\n    x = x0\n    for _ in range(max_iter):\n        grad = df(x)\n        if abs(grad) &lt; tol:\n            break\n        x -= eta * grad\n    return x\n\n# Example: minimize f(x) = x^2 -&gt; df/dx = 2x\nf_prime = lambda x: 2*x\nx_min = gradient_descent_1d(f_prime, x0=5)\nprint(x_min)  # ≈ 0\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\ndouble df(double x) { return 2*x; } // derivative of x^2\n\ndouble gradient_descent(double x0, double eta, double tol, int max_iter) {\n    double x = x0;\n    for (int i = 0; i &lt; max_iter; i++) {\n        double grad = df(x);\n        if (fabs(grad) &lt; tol) break;\n        x -= eta * grad;\n    }\n    return x;\n}\n\nint main(void) {\n    double xmin = gradient_descent(5.0, 0.1, 1e-6, 1000);\n    printf(\"x_min = %.6f\\n\", xmin);\n    return 0;\n}\n\n\nWhy It Matters\n\nFundamental to optimization, machine learning, and deep learning\nScales naturally from 1D to high dimensions\nHelps visualize energy landscapes, convergence, and learning dynamics\n\n\n\nA Gentle Proof (Why It Works)\nFor convex and smooth \\(f(x)\\), the Taylor expansion gives:\n\\[\nf(x-\\eta f'(x)) \\approx f(x) - \\eta (f'(x))^2 + \\frac{\\eta^2}{2} f''(\\xi) (f'(x))^2\n\\]\nFor small \\(\\eta&gt;0\\) and \\(f''(\\xi)&gt;0\\), the term \\(-\\eta (f'(x))^2\\) dominates, ensuring that \\(f(x)\\) decreases with each step.\nThis shows convergence toward the stationary point where \\(f'(x)=0\\).\n\n\nTry It Yourself\n\n\\(f(x)=x^2\\) with \\(\\eta=0.1\\), starting from \\(x_0=5\\).\nTry \\(\\eta=0.01\\) and \\(\\eta=1.0\\), observe convergence or divergence.\nMinimize \\(f(x)=(x-3)^2\\); check how quickly \\(x\\to3\\).\nVisualize each iteration on a plot of \\(f(x)\\).\nModify for stochastic or momentum variants.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDerivative\nStart \\(x_0\\)\n\\(\\eta\\)\nResult\nTrue Minimum\n\n\n\n\n\\(x^2\\)\n\\(2x\\)\n5\n0.1\n0.0000\n0\n\n\n\\((x-3)^2\\)\n\\(2(x-3)\\)\n0\n0.1\n3.0000\n3\n\n\n\\(\\cos x\\)\n\\(-\\sin x\\)\n2\n0.05\n1.57\n\\(\\pi/2\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(k)\\) iterations (depends on step size and tolerance)\nAccuracy: depends on \\(\\eta\\) and smoothness of \\(f\\)\nSpace: \\(O(1)\\)\n\nGradient Descent is the simplest form of learning, a gentle step downhill that embodies the heart of optimization and machine intelligence.\n\n\n\n\nSection 59. Mathematicial Optimization\n\n581 Simplex Method\nThe Simplex Method is a cornerstone of mathematical optimization, a geometric algorithm for solving linear programming (LP) problems efficiently. It walks along the vertices of the feasible region (a convex polytope) until it reaches the optimal vertex, where the objective function is maximized or minimized.\n\nWhat Problem Are We Solving?\nWe want to optimize a linear objective function subject to linear constraints:\n\\[\n\\text{maximize } z = c_1x_1 + c_2x_2 + \\dots + c_nx_n\n\\]\nsubject to\n\\[\n\\begin{aligned}\na_{11}x_1 + a_{12}x_2 + \\dots + a_{1n}x_n &\\le b_1, \\\na_{21}x_1 + a_{22}x_2 + \\dots + a_{2n}x_n &\\le b_2, \\\n&\\vdots \\\na_{m1}x_1 + a_{m2}x_2 + \\dots + a_{mn}x_n &\\le b_m, \\\nx_i &\\ge 0.\n\\end{aligned}\n\\]\nThis is the standard form of a linear program.\nThe goal is to find \\((x_1,\\dots,x_n)\\) that gives the maximum (or minimum) value of \\(z\\).\n\n\nHow Does It Work (Plain Language)\nImagine all constraints forming a polygon (in 2D) or polyhedron (in higher dimensions). The feasible region is convex, so the optimal point always lies on a vertex.\nThe Simplex method:\n\nStarts at one vertex (feasible basic solution).\nMoves along edges to an adjacent vertex that improves the objective function.\nRepeats until no further improvement is possible, that vertex is optimal.\n\n\n\nAlgebraic View\n\nConvert inequalities to equalities using slack variables. Example: \\(x_1 + x_2 \\le 4\\) → \\(x_1 + x_2 + s_1 = 4\\), where \\(s_1 \\ge 0\\).\nRepresent the system in tableau form.\nPerform pivot operations (like Gaussian elimination) to move from one basic feasible solution to another.\nStop when all reduced costs in the objective row are non-negative (for maximization).\n\n\n\nTiny Code (Simplified Demonstration)\nPython (educational version)\nimport numpy as np\n\ndef simplex(c, A, b):\n    m, n = A.shape\n    tableau = np.zeros((m+1, n+m+1))\n    tableau[:m, :n] = A\n    tableau[:m, n:n+m] = np.eye(m)\n    tableau[:m, -1] = b\n    tableau[-1, :n] = -c\n\n    while True:\n        col = np.argmin(tableau[-1, :-1])\n        if tableau[-1, col] &gt;= 0:\n            break  # optimal\n        ratios = [tableau[i, -1] / tableau[i, col] if tableau[i, col] &gt; 0 else np.inf for i in range(m)]\n        row = np.argmin(ratios)\n        pivot = tableau[row, col]\n        tableau[row, :] /= pivot\n        for i in range(m+1):\n            if i != row:\n                tableau[i, :] -= tableau[i, col] * tableau[row, :]\n    return tableau[-1, -1]\n\n# Example: maximize z = 3x1 + 2x2\n# subject to: x1 + x2 ≤ 4, x1 ≤ 2, x2 ≤ 3\nc = np.array([3, 2])\nA = np.array([[1, 1], [1, 0], [0, 1]])\nb = np.array([4, 2, 3])\nprint(\"Max z =\", simplex(c, A, b))\n\n\nWhy It Matters\n\nFoundation of operations research, economics, and optimization theory.\nUsed in logistics, finance, resource allocation, and machine learning (e.g., SVMs).\nDespite exponential worst-case complexity, it is extremely fast in practice.\n\n\n\nA Gentle Proof (Why It Works)\nBecause linear programs are convex, the optimum (if it exists) must occur at a vertex of the feasible region. The Simplex algorithm explores vertices in a way that strictly improves the objective function until reaching a vertex with no improving adjacent vertices.\nThis corresponds to the condition that all reduced costs in the tableau are non-negative, indicating optimality.\n\n\nTry It Yourself\n\nSolve the LP: maximize \\(z = 3x_1 + 5x_2\\) subject to: \\[\n\\begin{cases}\n2x_1 + x_2 \\le 8 \\\nx_1 + 2x_2 \\le 8 \\\nx_1, x_2 \\ge 0\n\\end{cases}\n\\]\nDraw the feasible region and verify the optimal vertex.\nModify constraints and observe how the solution moves.\nExperiment with minimization by negating the objective.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nObjective\nConstraints\nResult \\((x_1,x_2)\\)\nMax \\(z\\)\n\n\n\n\n\\(3x_1+2x_2\\)\n\\(x_1+x_2\\le4,\\ x_1\\le2,\\ x_2\\le3\\)\n(1,3)\n9\n\n\n\\(2x_1+3x_2\\)\n\\(2x_1+x_2\\le8,\\ x_1+2x_2\\le8\\)\n(2,3)\n13\n\n\n\\(x_1+x_2\\)\n\\(x_1,x_2\\le5\\)\n(5,5)\n10\n\n\n\n\n\nComplexity\n\nTime: Polynomial in practice (though worst-case exponential)\nSpace: \\(O(mn)\\) for tableau storage\n\nThe Simplex Method remains one of the most elegant algorithms ever created, a geometric dance over a convex landscape, always finding the corner of greatest value.\n\n\n\n582 Dual Simplex Method\nThe Dual Simplex Method is a close cousin of the Simplex algorithm. While the original Simplex keeps the solution feasible and moves toward optimality, the Dual Simplex does the opposite, it keeps the solution optimal and moves toward feasibility.\nIt’s especially useful when constraints change or when we start with an infeasible solution that already satisfies optimality conditions.\n\nWhat Problem Are We Solving?\nWe still solve a linear program (LP) in standard form, but we start from a tableau that is dual feasible (objective is optimal) but primal infeasible (some right-hand sides are negative).\nMaximize\n\\[\nz = c^T x\n\\]\nsubject to\n\\[\nA x \\le b, \\quad x \\ge 0.\n\\]\nThe dual simplex method works to restore feasibility while maintaining optimality of the reduced costs.\n\n\nHow Does It Work (Plain Language)\nThink of the Simplex and Dual Simplex as mirror images:\n\n\n\n\n\n\n\n\nStep\nSimplex\nDual Simplex\n\n\n\n\nKeeps\nFeasible solution\nOptimal reduced costs\n\n\nFixes\nOptimality\nFeasibility\n\n\nPivot choice\nMost negative reduced cost\nMost negative right-hand side\n\n\n\nIn the Dual Simplex, at each iteration:\n\nIdentify a row with a negative RHS (violating feasibility).\nAmong its coefficients, choose a pivot column that keeps reduced costs non-negative after pivoting.\nPerform a pivot to make that constraint feasible.\nRepeat until all RHS entries are non-negative (fully feasible).\n\n\n\nAlgebraic Formulation\nIf we maintain tableau:\n\\[\n\\begin{bmatrix}\nA & I & b \\\nc^T & 0 & z\n\\end{bmatrix}\n\\]\nthen the pivot condition becomes:\n\nChoose row \\(r\\) with \\(b_r &lt; 0\\).\nChoose column \\(s\\) where \\(a_{rs} &lt; 0\\) and \\(\\frac{c_s}{a_{rs}}\\) is minimal.\nPivot on \\((r,s)\\).\n\nThis restores primal feasibility step by step while maintaining dual optimality.\n\n\nTiny Code (Illustrative Example)\nPython\nimport numpy as np\n\ndef dual_simplex(A, b, c):\n    m, n = A.shape\n    tableau = np.zeros((m+1, n+m+1))\n    tableau[:m, :n] = A\n    tableau[:m, n:n+m] = np.eye(m)\n    tableau[:m, -1] = b\n    tableau[-1, :n] = -c\n\n    while np.any(tableau[:-1, -1] &lt; 0):\n        r = np.argmin(tableau[:-1, -1])\n        ratios = []\n        for j in range(n+m):\n            if tableau[r, j] &lt; 0:\n                ratios.append(tableau[-1, j] / tableau[r, j])\n            else:\n                ratios.append(np.inf)\n        s = np.argmin(ratios)\n        tableau[r, :] /= tableau[r, s]\n        for i in range(m+1):\n            if i != r:\n                tableau[i, :] -= tableau[i, s] * tableau[r, :]\n    return tableau[-1, -1]\n\n# Example\nA = np.array([[1, 1], [2, 1]])\nb = np.array([-2, 2])  # infeasible start\nc = np.array([3, 2])\nprint(\"Optimal value:\", dual_simplex(A, b, c))\n\n\nWhy It Matters\n\nEfficient for re-optimizing problems after small changes in constraints or RHS.\nCommonly used in branch-and-bound and cutting-plane algorithms for integer programming.\nAvoids recomputation when previous Simplex solutions become infeasible.\n\n\n\nA Gentle Proof (Why It Works)\nAt each iteration, the pivot preserves dual feasibility, meaning the reduced costs remain non-negative, and decreases the objective value (for a maximization problem). When all basic variables become feasible (RHS non-negative), the solution is both feasible and optimal.\nThus, convergence is guaranteed in a finite number of steps for non-degenerate cases.\n\n\nTry It Yourself\n\nStart with a Simplex tableau that has a negative RHS.\nApply the Dual Simplex pivot rule to fix feasibility.\nObserve that objective value never increases (for maximization).\nCompare the path taken with the standard Simplex method.\nUse it to re-solve a modified LP without starting from scratch.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nObjective\nConstraints\nMethod Start\nResult\n\n\n\n\n\\(3x_1+2x_2\\)\n\\(x_1+x_2\\le2\\), \\(x_1-x_2\\ge1\\)\ninfeasible primal\n\\(x_1=1.5, x_2=0.5, z=5.5\\)\n\n\n\\(2x_1+x_2\\)\n\\(x_1-x_2\\ge2\\), \\(x_1+x_2\\le6\\)\ninfeasible primal\n\\(x_1=2, x_2=4, z=8\\)\n\n\n\n\n\nComplexity\n\nTime: Similar to Simplex, efficient in practice\nSpace: \\(O(mn)\\) for tableau\n\nThe Dual Simplex is the mirror that balances infeasibility and optimality, a practical algorithm for when the landscape shifts but the solution must adapt gracefully.\n\n\n\n583 Interior-Point Method\nThe Interior-Point Method is a modern alternative to the Simplex algorithm for solving linear and convex optimization problems. Instead of moving along the edges of the feasible region, it moves through the interior, following smooth curves toward the optimal point.\n\nWhat Problem Are We Solving?\nWe want to solve a standard linear program:\n\\[\n\\text{minimize } c^T x\n\\]\nsubject to\n\\[\nA x = b, \\quad x \\ge 0.\n\\]\nThis defines a convex feasible region, the intersection of half-spaces and equality constraints. The interior-point method searches for the optimal point inside that region rather than on its boundary.\n\n\nHow Does It Work (Plain Language)\nThink of the feasible region as a polyhedron. Instead of “walking along edges” like the Simplex method, we “slide” through its interior along a smooth central path guided by both the objective and the constraints.\nThe method uses a barrier function to prevent the solution from crossing boundaries. For example, to keep \\(x_i \\ge 0\\), we add a penalty term \\(-\\mu \\sum_i \\ln(x_i)\\) to the objective.\nSo we solve:\n\\[\n\\text{minimize } c^T x - \\mu \\sum_i \\ln(x_i)\n\\]\nwhere \\(\\mu &gt; 0\\) controls how close we stay to the boundary. As \\(\\mu \\to 0\\), the solution approaches the true optimal vertex.\n\n\nMathematical Steps\n\nBarrier problem formulation: \\[\n\\min_x ; c^T x - \\mu \\sum_{i=1}^n \\ln(x_i)\n\\] subject to \\(A x = b\\).\nFirst-order condition: \\[\nA x = b, \\quad X s = \\mu e, \\quad s = c - A^T y,\n\\] where \\(X = \\text{diag}(x_1, \\dots, x_n)\\) and \\(s\\) are slack variables.\nNewton update: Solve the linearized KKT (Karush–Kuhn–Tucker) system for \\(\\Delta x, \\Delta y, \\Delta s\\).\nStep and update: \\[\nx \\leftarrow x + \\alpha \\Delta x, \\quad y \\leftarrow y + \\alpha \\Delta y, \\quad s \\leftarrow s + \\alpha \\Delta s,\n\\] with step size \\(\\alpha\\) chosen to maintain positivity.\nReduce \\(\\mu\\) and repeat until convergence.\n\n\n\nTiny Code (Conceptual Example)\nPython (illustrative version)\nimport numpy as np\n\ndef interior_point(A, b, c, mu=1.0, tol=1e-8, max_iter=50):\n    m, n = A.shape\n    x = np.ones(n)\n    y = np.zeros(m)\n    s = np.ones(n)\n\n    for _ in range(max_iter):\n        r1 = A @ x - b\n        r2 = A.T @ y + s - c\n        r3 = x * s - mu * np.ones(n)\n\n        if np.linalg.norm(r1) &lt; tol and np.linalg.norm(r2) &lt; tol and np.linalg.norm(r3) &lt; tol:\n            break\n\n        # Construct Newton system\n        diagX = np.diag(x)\n        diagS = np.diag(s)\n        M = A @ np.linalg.inv(diagS) @ diagX @ A.T\n        rhs = -r1 + A @ np.linalg.inv(diagS) @ (r3 - diagX @ r2)\n        dy = np.linalg.solve(M, rhs)\n        ds = -r2 - A.T @ dy\n        dx = (r3 - diagX @ ds) / s\n\n        # Step size\n        alpha = 0.99 * min(1, min(-x[dx &lt; 0] / dx[dx &lt; 0], default=1))\n        x += alpha * dx\n        y += alpha * dy\n        s += alpha * ds\n        mu *= 0.5\n    return x, y, c @ x\n\n# Example\nA = np.array([[1, 1], [1, -1]])\nb = np.array([1, 0])\nc = np.array([1, 2])\nx, y, val = interior_point(A, b, c)\nprint(\"Optimal x:\", x, \"Objective:\", val)\n\n\nWhy It Matters\n\nCompetes with or outperforms Simplex for very large-scale LPs and QPs.\nSmooth and robust, avoids corner-by-corner traversal.\nForms the foundation of modern convex optimization and machine learning solvers (e.g., SVMs, logistic regression).\n\n\n\nA Gentle Proof (Why It Works)\nThe logarithmic barrier ensures that the iterates remain strictly positive. Each Newton step minimizes a local quadratic approximation of the barrier-augmented objective. As \\(\\mu \\to 0\\), the barrier term vanishes, and the solution converges to the true KKT optimal point of the original LP.\n\n\nTry It Yourself\n\nMinimize \\(x_1 + x_2\\) subject to \\[\n\\begin{cases}\nx_1 + 2x_2 \\ge 2, \\\n3x_1 + x_2 \\ge 3, \\\nx_1, x_2 \\ge 0.\n\\end{cases}\n\\]\nCompare convergence with the Simplex solution.\nExperiment with different \\(\\mu\\) reduction schedules.\nPlot trajectory, note the smooth curve through the interior.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nObjective\nConstraints\nOptimal \\((x_1,x_2)\\)\nMin \\(c^T x\\)\n\n\n\n\n\\(x_1+x_2\\)\n\\(x_1+x_2\\ge2\\)\n(1,1)\n2\n\n\n\\(x_1+2x_2\\)\n\\(x_1+2x_2\\ge4\\), \\(x_1,x_2\\ge0\\)\n(0,2)\n4\n\n\n\\(2x_1+x_2\\)\n\\(x_1+x_2\\ge3\\)\n(2,1)\n5\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^{3.5}L)\\) for linear programs (polynomial time)\nSpace: \\(O(n^2)\\) due to matrix factorizations\n\nThe Interior-Point Method is the elegant smooth traveler of optimization, gliding through the feasible space’s heart while homing in on the global optimum with mathematical grace.\n\n\n\n584 Gradient Descent (Unconstrained Optimization)\nGradient Descent is one of the simplest and most fundamental optimization algorithms. It is used to find the local minimum of a differentiable function by repeatedly moving in the opposite direction of the gradient, the direction of steepest descent.\n\nWhat Problem Are We Solving?\nWe want to minimize a smooth function \\(f(x)\\), where \\(x \\in \\mathbb{R}^n\\):\n\\[\n\\min_x f(x)\n\\]\nThe function \\(f(x)\\) could represent cost, loss, or error, and our goal is to find a point where the gradient (slope) is close to zero:\n\\[\n\\nabla f(x^*) = 0\n\\]\n\n\nHow Does It Work (Plain Language)\nAt each step, we update \\(x\\) by moving against the gradient, since that’s the direction in which \\(f(x)\\) decreases fastest.\n\\[\nx_{t+1} = x_t - \\eta \\nabla f(x_t)\n\\]\nHere:\n\n\\(\\eta &gt; 0\\) is the learning rate (step size).\n\\(\\nabla f(x_t)\\) is the gradient vector at the current point.\n\nThe algorithm continues until the gradient becomes very small or until changes in \\(x\\) or \\(f(x)\\) are negligible.\n\n\nStep-by-Step Example\nLet’s minimize \\(f(x) = x^2\\). Then \\(\\nabla f(x) = 2x\\).\n\\[\nx_{t+1} = x_t - \\eta (2x_t) = (1 - 2\\eta)x_t\n\\]\nIf \\(0 &lt; \\eta &lt; 1\\), the sequence converges to \\(x=0\\), the global minimum.\n\n\nTiny Code (Simple Implementation)\nPython\nimport numpy as np\n\ndef gradient_descent(fprime, x0, eta=0.1, tol=1e-6, max_iter=1000):\n    x = x0\n    for _ in range(max_iter):\n        grad = fprime(x)\n        if np.linalg.norm(grad) &lt; tol:\n            break\n        x -= eta * grad\n    return x\n\n# Example: minimize f(x) = x^2 + y^2\nfprime = lambda v: np.array([2*v[0], 2*v[1]])\nx_min = gradient_descent(fprime, np.array([5.0, -3.0]))\nprint(\"Minimum at:\", x_min)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nvoid grad(double x[], double g[]) {\n    g[0] = 2*x[0];\n    g[1] = 2*x[1];\n}\n\nvoid gradient_descent(double x[], double eta, double tol, int max_iter) {\n    double g[2];\n    for (int i = 0; i &lt; max_iter; i++) {\n        grad(x, g);\n        double norm = sqrt(g[0]*g[0] + g[1]*g[1]);\n        if (norm &lt; tol) break;\n        x[0] -= eta * g[0];\n        x[1] -= eta * g[1];\n    }\n}\n\nint main(void) {\n    double x[2] = {5.0, -3.0};\n    gradient_descent(x, 0.1, 1e-6, 1000);\n    printf(\"Minimum at (%.4f, %.4f)\\n\", x[0], x[1]);\n    return 0;\n}\n\n\nWhy It Matters\n\nFoundation for machine learning, deep learning, and optimization theory.\nWorks in high dimensions with simple computation per step.\nForms the basis of more advanced algorithms like SGD, Momentum, and Adam.\n\n\n\nA Gentle Proof (Why It Works)\nFor convex and differentiable \\(f(x)\\) with Lipschitz-continuous gradient (\\(L\\)-smooth), the update rule guarantees:\n\\[\nf(x_{t+1}) \\le f(x_t) - \\frac{\\eta}{2}|\\nabla f(x_t)|^2\n\\]\nif \\(0 &lt; \\eta \\le \\frac{1}{L}\\).\nThis means each step decreases \\(f(x)\\) by a quantity proportional to the squared gradient magnitude, leading to convergence.\n\n\nTry It Yourself\n\nMinimize \\(f(x)=x^2+y^2\\) starting from \\((5,-3)\\).\nTry \\(\\eta=0.1\\), \\(\\eta=0.5\\), and \\(\\eta=1.0\\), see which converges fastest.\nAdd a stopping condition based on \\(|f(x_{t+1}) - f(x_t)|\\).\nVisualize the path on a contour plot of \\(f(x,y)\\).\nExtend to non-convex functions like \\(f(x)=x^4 - 3x^3 + 2\\).\n\n\n\nTest Cases\n\n\n\nFunction\nGradient\nStart \\(x_0\\)\n\\(\\eta\\)\nResult\nTrue Minimum\n\n\n\n\n\\(x^2\\)\n\\(2x\\)\n5\n0.1\n0.0000\n0\n\n\n\\(x^2+y^2\\)\n\\((2x,2y)\\)\n(5,-3)\n0.1\n(0,0)\n(0,0)\n\n\n\\((x-2)^2\\)\n\\(2(x-2)\\)\n0\n0.1\n2.0000\n2\n\n\n\n\n\nComplexity\n\nTime: \\(O(k)\\) iterations (depends on learning rate and tolerance)\nSpace: \\(O(n)\\)\n\nGradient Descent is the universal descent path, a simple, elegant method that lies at the foundation of optimization and learning across all of modern computation.\n\n\n\n585 Stochastic Gradient Descent (SGD)\nStochastic Gradient Descent (SGD) is the workhorse of modern machine learning. It extends ordinary gradient descent by using random samples (or mini-batches) to estimate the gradient at each step, allowing it to scale efficiently to massive datasets.\n\nWhat Problem Are We Solving?\nWe aim to minimize a function defined as the average of many sample-based losses:\n\\[\nf(x) = \\frac{1}{N} \\sum_{i=1}^N f_i(x)\n\\]\nComputing the full gradient \\(\\nabla f(x) = \\frac{1}{N}\\sum_i \\nabla f_i(x)\\) at every iteration can be very expensive when \\(N\\) is large.\nSGD avoids that by using only one (or a few) random samples per step:\n\\[\nx_{t+1} = x_t - \\eta \\nabla f_{i_t}(x_t)\n\\]\nwhere \\(i_t\\) is randomly selected from \\({1,2,\\dots,N}\\).\n\n\nHow Does It Work (Plain Language)\nInstead of calculating the exact slope of the entire landscape, SGD takes a noisy but much cheaper estimate of the slope. It zigzags its way downhill, sometimes overshooting, sometimes correcting, but overall, it trends toward the minimum.\nThis randomness acts like “built-in exploration,” helping SGD escape shallow local minima in non-convex problems.\n\n\nAlgorithm Steps\n\nInitialize \\(x_0\\) (random or zero).\nFor each iteration \\(t\\):\n\nRandomly sample \\(i_t \\in {1,\\dots,N}\\)\nCompute gradient estimate \\(g_t = \\nabla f_{i_t}(x_t)\\)\nUpdate parameter: \\(x_{t+1} = x_t - \\eta g_t\\)\n\nOptionally decay the learning rate \\(\\eta_t\\) over time.\n\nCommon decay schedules: \\[\n\\eta_t = \\frac{\\eta_0}{1 + \\lambda t}\n\\]\n\n\nTiny Code (Simple Example)\nPython\nimport numpy as np\n\ndef sgd(fprime, x0, data, eta=0.1, epochs=1000):\n    x = x0\n    N = len(data)\n    for t in range(epochs):\n        i = np.random.randint(N)\n        grad = fprime(x, data[i])\n        x -= eta * grad\n    return x\n\n# Example: minimize average (x - y)^2 over samples\ndata = np.array([1.0, 2.0, 3.0, 4.0])\ndef grad(x, y): return 2*(x - y)\nx_min = sgd(grad, x0=0.0, data=data)\nprint(\"Estimated minimum:\", x_min)\nC\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;time.h&gt;\n\ndouble grad(double x, double y) {\n    return 2*(x - y);\n}\n\ndouble sgd(double *data, int N, double x0, double eta, int epochs) {\n    double x = x0;\n    srand(time(NULL));\n    for (int t = 0; t &lt; epochs; t++) {\n        int i = rand() % N;\n        double g = grad(x, data[i]);\n        x -= eta * g;\n    }\n    return x;\n}\n\nint main(void) {\n    double data[] = {1.0, 2.0, 3.0, 4.0};\n    int N = 4;\n    double xmin = sgd(data, N, 0.0, 0.1, 1000);\n    printf(\"Estimated minimum: %.4f\\n\", xmin);\n    return 0;\n}\n\n\nWhy It Matters\n\nCrucial for training neural networks and large-scale models.\nHandles billions of data points efficiently.\nNaturally fits streaming or online learning settings.\nRandomness helps avoid bad local minima in non-convex landscapes.\n\n\n\nA Gentle Proof (Why It Works)\nIf the learning rate \\(\\eta_t\\) satisfies \\[\n\\sum_t \\eta_t = \\infty \\quad \\text{and} \\quad \\sum_t \\eta_t^2 &lt; \\infty,\n\\] then under mild convexity and smoothness assumptions, SGD converges in expectation to the true minimum \\(x^*\\).\nThe randomness introduces variance, but averaging or decreasing \\(\\eta_t\\) controls it.\n\n\nTry It Yourself\n\nMinimize \\(f(x) = \\frac{1}{N}\\sum_i (x - y_i)^2\\) for random samples \\(y_i\\).\nCompare full gradient descent vs SGD convergence speed.\nAdd learning rate decay: \\(\\eta_t = \\eta_0/(1+0.01t)\\).\nTry mini-batch SGD (use several samples per step).\nPlot \\(f(x_t)\\) vs iteration, notice noisy but downward trend.\n\n\n\nTest Cases\n\n\n\nFunction\nGradient\nDataset\nResult\nTrue Minimum\n\n\n\n\n\\((x-y)^2\\)\n\\(2(x-y)\\)\n[1,2,3,4]\n≈ 2.5\n2.5\n\n\n\\((x-5)^2\\)\n\\(2(x-5)\\)\n[5]*100\n5.0\n5.0\n\n\n\\((x-y)^2\\)\n[1,1000]\nlarge variance\n~500\n500\n\n\n\n\n\nComplexity\n\nTime: \\(O(k)\\) iterations (each uses one or few samples)\nSpace: \\(O(1)\\) or \\(O(\\text{mini-batch size})\\)\nConvergence: Sublinear but scalable\n\nSGD is the heart of modern learning, a simple yet powerful idea that trades precision for speed, letting massive systems learn efficiently from a sea of data.\n\n\n\n586 Newton’s Method (Multivariate Optimization)\nNewton’s Method in multiple dimensions generalizes the one-dimensional root-finding approach to efficiently locate stationary points of smooth functions. It uses both the gradient (first derivative) and Hessian (second derivative matrix) to make quadratic steps toward the optimum.\n\nWhat Problem Are We Solving?\nWe want to minimize a smooth function \\(f(x)\\), where \\(x \\in \\mathbb{R}^n\\):\n\\[\n\\min_x f(x)\n\\]\nAt the minimum, the gradient vanishes:\n\\[\n\\nabla f(x^*) = 0.\n\\]\nNewton’s method refines the guess \\(x_t\\) by approximating \\(f(x)\\) locally with its second-order Taylor expansion:\n\\[\nf(x+\\Delta x) \\approx f(x) + \\nabla f(x)^T \\Delta x + \\frac{1}{2} \\Delta x^T H(x) \\Delta x,\n\\]\nwhere \\(H(x)\\) is the Hessian matrix.\nSetting the gradient of this approximation to zero gives:\n\\[\nH(x) \\Delta x = -\\nabla f(x),\n\\]\nwhich leads to the update rule:\n\\[\nx_{t+1} = x_t - H(x_t)^{-1} \\nabla f(x_t).\n\\]\n\n\nHow Does It Work (Plain Language)\nImagine standing on a curved surface representing \\(f(x)\\). The gradient tells you the slope, but the Hessian tells you how the slope itself bends. By combining them, Newton’s method jumps directly toward the local minimum of that curve’s quadratic approximation, often converging in just a few steps when the surface is well-behaved.\n\n\nTiny Code (Illustrative Example)\nPython\nimport numpy as np\n\ndef newton_multivariate(fprime, hessian, x0, tol=1e-6, max_iter=100):\n    x = x0\n    for _ in range(max_iter):\n        g = fprime(x)\n        H = hessian(x)\n        if np.linalg.norm(g) &lt; tol:\n            break\n        dx = np.linalg.solve(H, g)\n        x -= dx\n    return x\n\n# Example: f(x, y) = x^2 + y^2\nfprime = lambda v: np.array([2*v[0], 2*v[1]])\nhessian = lambda v: np.array([[2, 0], [0, 2]])\nx_min = newton_multivariate(fprime, hessian, np.array([5.0, -3.0]))\nprint(\"Minimum at:\", x_min)\nC (simplified)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nvoid gradient(double x[], double g[]) {\n    g[0] = 2*x[0];\n    g[1] = 2*x[1];\n}\n\nvoid hessian(double H[2][2]) {\n    H[0][0] = 2; H[0][1] = 0;\n    H[1][0] = 0; H[1][1] = 2;\n}\n\nvoid newton(double x[], double tol, int max_iter) {\n    double g[2], H[2][2];\n    for (int k = 0; k &lt; max_iter; k++) {\n        gradient(x, g);\n        if (sqrt(g[0]*g[0] + g[1]*g[1]) &lt; tol) break;\n        hessian(H);\n        x[0] -= g[0] / H[0][0];\n        x[1] -= g[1] / H[1][1];\n    }\n}\n\nint main(void) {\n    double x[2] = {5.0, -3.0};\n    newton(x, 1e-6, 100);\n    printf(\"Minimum at (%.4f, %.4f)\\n\", x[0], x[1]);\n    return 0;\n}\n\n\nWhy It Matters\n\nExtremely fast near the optimum (quadratic convergence).\nThe foundation for many advanced solvers, BFGS, Newton-CG, and trust-region methods.\nCentral to optimization, machine learning, and numerical analysis.\n\n\n\nA Gentle Proof (Why It Works)\nNear a true minimum \\(x^*\\), the function behaves almost quadratically:\n\\[\nf(x) \\approx f(x^*) + \\frac{1}{2}(x-x^*)^T H(x^*)(x-x^*).\n\\]\nThus, the Newton update \\(x_{t+1}=x_t-H^{-1}\\nabla f(x_t)\\) effectively solves this local quadratic model exactly. When \\(H\\) is positive definite and \\(x_t\\) is sufficiently close to \\(x^*\\), convergence is quadratic, the error shrinks roughly as the square of the previous error.\n\n\nTry It Yourself\n\nMinimize \\(f(x,y)=x^2+y^2\\) from \\((5,-3)\\).\nTry a non-diagonal Hessian: \\(f(x,y)=x^2+xy+y^2\\).\nCompare convergence speed with Gradient Descent.\nObserve behavior when the Hessian is not positive definite.\nAdd line search to improve robustness.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\n\nFunction\nGradient\nHessian\nStart\nResult\nTrue Minimum\n\n\n\n\n\\(x^2 + y^2\\)\n\\((2x,\\,2y)\\)\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}\\)\n\\((5,-3)\\)\n\\((0,0)\\)\n\\((0,0)\\)\n\n\n\\((x-2)^2 + (y+1)^2\\)\n\\((2(x-2),\\,2(y+1))\\)\n\\(\\operatorname{diag}(2,2)\\)\n\\((0,0)\\)\n\\((2,-1)\\)\n\\((2,-1)\\)\n\n\n\\(x^2 + xy + y^2\\)\n\\((2x+y,\\,x+2y)\\)\n\\(\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\)\n\\((3,-2)\\)\n\\((0,0)\\)\n\\((0,0)\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n^3)\\) per iteration (matrix inversion or linear solve)\nSpace: \\(O(n^2)\\) (store Hessian)\n\nNewton’s Method is the mathematician’s scalpel, precise, elegant, and fast when the terrain is smooth, cutting straight to the heart of an optimum in just a few careful steps.\n\n\n\n587 Conjugate Gradient Method\nThe Conjugate Gradient (CG) Method is an iterative algorithm for solving large systems of linear equations of the form\n\\[\nA x = b\n\\]\nwhere \\(A\\) is symmetric positive definite (SPD). It’s especially powerful because it doesn’t require matrix inversion or even explicit storage of \\(A\\), only the ability to compute matrix–vector products.\nCG can also be seen as a method for minimizing quadratic functions efficiently in high dimensions.\n\nWhat Problem Are We Solving?\nWe want to minimize the quadratic form\n\\[\nf(x) = \\frac{1}{2}x^T A x - b^T x,\n\\]\nwhich has the gradient\n\\[\n\\nabla f(x) = A x - b.\n\\]\nSetting \\(\\nabla f(x)=0\\) gives the same equation \\(A x = b\\).\nThus, solving \\(A x = b\\) and minimizing \\(f(x)\\) are equivalent.\n\n\nHow Does It Work (Plain Language)\nOrdinary gradient descent may zigzag and converge slowly when contours of \\(f(x)\\) are elongated. The Conjugate Gradient method fixes this by ensuring each search direction is A-orthogonal (conjugate) to all previous ones, meaning each step eliminates error in a new dimension without undoing progress from earlier steps.\nIt can find the exact solution in at most \\(n\\) steps (for \\(n\\) variables) in exact arithmetic.\n\n\nAlgorithm Steps\n\nInitialize \\(x_0\\) (e.g., zeros).\nCompute initial residual \\(r_0 = b - A x_0\\) and set direction \\(p_0 = r_0\\).\nFor each iteration \\(k=0,1,2,\\dots\\): \\[\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n\\] \\[\nx_{k+1} = x_k + \\alpha_k p_k\n\\] \\[\nr_{k+1} = r_k - \\alpha_k A p_k\n\\] If \\(|r_{k+1}|\\) is small enough, stop. Otherwise: \\[\n\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}, \\quad p_{k+1} = r_{k+1} + \\beta_k p_k.\n\\]\n\nEach new \\(p_k\\) is “conjugate” to all previous directions with respect to \\(A\\).\n\n\nTiny Code (Minimal Implementation)\nPython\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0=None, tol=1e-6, max_iter=1000):\n    n = len(b)\n    x = np.zeros(n) if x0 is None else x0\n    r = b - A @ x\n    p = r.copy()\n    for _ in range(max_iter):\n        Ap = A @ p\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x += alpha * p\n        r_new = r - alpha * Ap\n        if np.linalg.norm(r_new) &lt; tol:\n            break\n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        p = r_new + beta * p\n        r = r_new\n    return x\n\n# Example: Solve A x = b\nA = np.array([[4, 1], [1, 3]], dtype=float)\nb = np.array([1, 2], dtype=float)\nx = conjugate_gradient(A, b)\nprint(\"Solution:\", x)\nC\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nvoid matvec(double A[2][2], double x[2], double y[2]) {\n    y[0] = A[0][0]*x[0] + A[0][1]*x[1];\n    y[1] = A[1][0]*x[0] + A[1][1]*x[1];\n}\n\nvoid conjugate_gradient(double A[2][2], double b[2], double x[2], int max_iter, double tol) {\n    double r[2], p[2], Ap[2];\n    matvec(A, x, r);\n    for (int i = 0; i &lt; 2; i++) {\n        r[i] = b[i] - r[i];\n        p[i] = r[i];\n    }\n    for (int k = 0; k &lt; max_iter; k++) {\n        matvec(A, p, Ap);\n        double rr = r[0]*r[0] + r[1]*r[1];\n        double alpha = rr / (p[0]*Ap[0] + p[1]*Ap[1]);\n        x[0] += alpha * p[0];\n        x[1] += alpha * p[1];\n        for (int i = 0; i &lt; 2; i++) r[i] -= alpha * Ap[i];\n        double rr_new = r[0]*r[0] + r[1]*r[1];\n        if (sqrt(rr_new) &lt; tol) break;\n        double beta = rr_new / rr;\n        for (int i = 0; i &lt; 2; i++) p[i] = r[i] + beta * p[i];\n    }\n}\n\nint main(void) {\n    double A[2][2] = {{4, 1}, {1, 3}};\n    double b[2] = {1, 2};\n    double x[2] = {0, 0};\n    conjugate_gradient(A, b, x, 1000, 1e-6);\n    printf(\"Solution: (%.4f, %.4f)\\n\", x[0], x[1]);\n    return 0;\n}\n\n\nWhy It Matters\n\nIdeal for large sparse systems, especially those from numerical PDEs and finite element methods.\nAvoids explicit matrix inversion, only needs \\(A p\\) products.\nCore building block in machine learning, physics simulations, and scientific computing.\n\n\n\nA Gentle Proof (Why It Works)\nEach direction \\(p_k\\) is chosen so that\n\\[\np_i^T A p_j = 0 \\quad \\text{for } i \\ne j,\n\\]\nensuring orthogonality under the \\(A\\)-inner product. This means each step eliminates error along one conjugate direction, never revisiting it. For an \\(n\\)-dimensional system, all error components are eliminated after at most \\(n\\) steps.\n\n\nTry It Yourself\n\nSolve \\(A x = b\\) with \\[\nA =\n\\begin{bmatrix}\n4 & 1\\\\\n1 & 3\n\\end{bmatrix},\n\\quad\nb =\n\\begin{bmatrix}\n1\\\\\n2\n\\end{bmatrix}.\n\\]\nCompare the result with Gaussian elimination.\nModify \\(A\\) to be non-symmetric and observe failure or oscillation.\nAdd a preconditioner \\(M^{-1}\\) to improve convergence.\nPlot \\(|r_k|\\) versus iterations and note the geometric decay.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(b\\)\nSolution \\(x^*\\)\nIterations\n\n\n\n\n\\(\\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}\\)\n\\([1, 2]^T\\)\n\\((0.0909,\\, 0.6364)\\)\n3\n\n\n\\(\\operatorname{diag}(2, 5, 10)\\)\n\\([1, 1, 1]^T\\)\n\\((0.5,\\, 0.2,\\, 0.1)\\)\n3\n\n\nrandom SPD \\((5 \\times 5)\\)\nrandom \\(b\\)\naccurate\n\\(&lt; n\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(k n)\\) (each iteration needs one matrix–vector multiply)\nSpace: \\(O(n)\\)\nConvergence: Linear but very efficient for well-conditioned \\(A\\)\n\nThe Conjugate Gradient method is the quiet power of numerical optimization, using geometry and algebra hand in hand to solve vast systems with elegant efficiency.\n\n\n\n588 Lagrange Multipliers (Constrained Optimization)\nThe Lagrange Multiplier Method provides a systematic way to find extrema (minima or maxima) of a function subject to equality constraints. It introduces auxiliary variables, called Lagrange multipliers, that enforce the constraints algebraically, transforming a constrained problem into an unconstrained one.\n\nWhat Problem Are We Solving?\nWe want to minimize (or maximize) a function \\[\nf(x_1, x_2, \\dots, x_n)\n\\] subject to one or more equality constraints: \\[\ng_i(x_1, x_2, \\dots, x_n) = 0, \\quad i = 1, 2, \\dots, m.\n\\]\nFor simplicity, start with a single constraint \\(g(x) = 0\\).\n\n\nThe Core Idea\nAt an optimum, the gradient of \\(f\\) must lie in the same direction as the gradient of the constraint \\(g\\):\n\\[\n\\nabla f(x^*) = \\lambda \\nabla g(x^*),\n\\]\nwhere \\(\\lambda\\) is the Lagrange multiplier. This captures the idea that any small movement along the constraint surface cannot reduce \\(f\\) further.\nWe introduce the Lagrangian function:\n\\[\n\\mathcal{L}(x, \\lambda) = f(x) - \\lambda g(x),\n\\]\nand find stationary points by setting all derivatives to zero:\n\\[\n\\nabla_x \\mathcal{L} = 0, \\quad g(x) = 0.\n\\]\n\n\nExample: Classic Two-Variable Case\nMinimize \\[\nf(x, y) = x^2 + y^2\n\\] subject to \\[\nx + y = 1.\n\\]\n\nConstruct the Lagrangian: \\[\n\\mathcal{L}(x, y, \\lambda) = x^2 + y^2 - \\lambda(x + y - 1)\n\\]\nTake partial derivatives and set them to zero: \\[\n\\frac{\\partial \\mathcal{L}}{\\partial x} = 2x - \\lambda = 0\n\\] \\[\n\\frac{\\partial \\mathcal{L}}{\\partial y} = 2y - \\lambda = 0\n\\] \\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x + y - 1) = 0\n\\]\nSolve: from the first two equations, \\(2x = 2y = \\lambda\\) → \\(x = y\\). Substituting into the constraint \\(x + y = 1\\) → \\(x = y = 0.5\\).\n\nThus the minimum occurs at \\((x, y) = (0.5, 0.5)\\) with \\(\\lambda = 1\\).\n\n\nTiny Code (Simple Example)\nPython\nimport sympy as sp\n\nx, y, lam = sp.symbols('x y lam')\nf = x2 + y2\ng = x + y - 1\nL = f - lam * g\n\nsol = sp.solve([sp.diff(L, x), sp.diff(L, y), sp.diff(L, lam)], [x, y, lam])\nprint(sol)\nOutput:\n{x: 0.5, y: 0.5, lam: 1}\nC (conceptual numeric)\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    double x = 0.5, y = 0.5, lam = 1.0;\n    printf(\"Minimum at (%.2f, %.2f), lambda = %.2f\\n\", x, y, lam);\n    return 0;\n}\n\n\nWhy It Matters\n\nCore of constrained optimization in calculus, economics, and machine learning.\nGeneralizes easily to multiple constraints and higher dimensions.\nForms the foundation for KKT conditions (see next section) used in convex optimization and support vector machines.\n\n\n\nA Gentle Proof (Geometric View)\nAt the optimum point, any movement tangent to the constraint surface must not change \\(f(x)\\). Hence, \\(\\nabla f\\) must be perpendicular to that surface, that is, parallel to \\(\\nabla g\\). Introducing \\(\\lambda\\) allows us to equate these directions algebraically, creating solvable equations.\n\n\nTry It Yourself\n\nMinimize \\(f(x, y) = x^2 + y^2\\) subject to \\(x + y = 1\\).\nTry \\(f(x, y) = x^2 + 2y^2\\) subject to \\(x - y = 0\\).\nSolve with two constraints: \\[\ng_1(x, y) = x + y - 1 = 0, \\quad g_2(x, y) = x - 2y = 0.\n\\]\nObserve how \\(\\lambda_1, \\lambda_2\\) act as “weights” enforcing constraints.\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\nFunction\nConstraint\nResult \\((x^*, y^*)\\)\n\\(\\lambda\\)\n\n\n\n\n\\(x^2+y^2\\)\n\\(x+y=1\\)\n(0.5, 0.5)\n1\n\n\n\\(x^2+2y^2\\)\n\\(x-y=0\\)\n(0, 0)\n0\n\n\n\\(x^2+y^2\\)\n\\(x^2+y^2=4\\)\ncircle constraint → any point on circle\nvariable\n\n\n\n\n\nComplexity\n\nSymbolic solution: \\(O(n^3)\\) for solving equations.\nNumeric solution: iterative methods (Newton–Raphson, SQP) for large systems.\n\nThe method of Lagrange multipliers is the mathematical bridge between freedom and constraint, guiding optimization across boundaries defined by nature, design, or logic itself.\n\n\n\n589 Karush–Kuhn–Tucker (KKT) Conditions\nThe Karush–Kuhn–Tucker (KKT) conditions generalize the Lagrange multiplier method to handle inequality and equality constraints in nonlinear optimization. They form the cornerstone of modern constrained optimization, especially in convex optimization, machine learning (SVMs), and economics.\n\nWhat Problem Are We Solving?\nWe want to minimize\n\\[\nf(x)\n\\]\nsubject to\n\\[\ng_i(x) \\le 0 \\quad (i = 1, \\dots, m)\n\\]\nand\n\\[\nh_j(x) = 0 \\quad (j = 1, \\dots, p).\n\\]\nHere:\n\n\\(g_i(x)\\) are inequality constraints,\n\\(h_j(x)\\) are equality constraints.\n\n\n\nThe Lagrangian Function\nWe extend the idea of the Lagrange function:\n\\[\n\\mathcal{L}(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^p \\mu_j h_j(x),\n\\]\nwhere\n\n\\(\\lambda_i \\ge 0\\) are multipliers for inequality constraints,\n\\(\\mu_j\\) are multipliers for equality constraints.\n\n\n\nThe KKT Conditions\nFor an optimal point \\(x^*\\), there must exist \\(\\lambda_i\\) and \\(\\mu_j\\) satisfying the following four conditions:\n\nStationarity \\[\n\\nabla f(x^*) + \\sum_{i=1}^m \\lambda_i \\nabla g_i(x^*) + \\sum_{j=1}^p \\mu_j \\nabla h_j(x^*) = 0\n\\]\nPrimal Feasibility \\[\ng_i(x^*) \\le 0, \\quad h_j(x^*) = 0\n\\]\nDual Feasibility \\[\n\\lambda_i \\ge 0 \\quad \\text{for all } i\n\\]\nComplementary Slackness \\[\n\\lambda_i g_i(x^*) = 0 \\quad \\text{for all } i\n\\]\n\nComplementary slackness means that if a constraint is not “tight” (inactive), its corresponding \\(\\lambda_i\\) must be zero, it exerts no force on the solution.\n\n\nExample: Quadratic Optimization with Constraint\nMinimize \\[\nf(x) = x^2\n\\] subject to \\[\ng(x) = 1 - x \\le 0.\n\\]\nStep 1: Write the Lagrangian \\[\n\\mathcal{L}(x, \\lambda) = x^2 + \\lambda(1 - x)\n\\]\nStep 2: KKT conditions\n\nStationarity: \\[\n\\frac{d\\mathcal{L}}{dx} = 2x - \\lambda = 0\n\\]\nPrimal feasibility: \\[\n1 - x \\le 0\n\\]\nDual feasibility: \\[\n\\lambda \\ge 0\n\\]\nComplementary slackness: \\[\n\\lambda(1 - x) = 0\n\\]\n\nSolve: From stationarity, \\(\\lambda = 2x\\). From complementary slackness, either \\(\\lambda=0\\) or \\(1-x=0\\).\n\nIf \\(\\lambda=0\\), then \\(x=0\\). But \\(1-x=1&gt;0\\), violates feasibility.\nIf \\(1-x=0\\), then \\(x=1\\) and \\(\\lambda=2\\).\n\nSolution: \\(x^* = 1\\), \\(\\lambda^* = 2\\).\n\n\nTiny Code (Symbolic Example)\nPython\nimport sympy as sp\n\nx, lam = sp.symbols('x lam')\nf = x2\ng = 1 - x\nL = f + lam * g\n\nsol = sp.solve([sp.diff(L, x), g, lam &gt;= 0, lam * g], [x, lam], dict=True)\nprint(sol)\nOutput:\n$${x: 1, lam: 2}]\n\n\nWhy It Matters\n\nGeneralizes Lagrange multipliers to handle inequality constraints.\nFundamental to convex optimization, machine learning (SVMs), econometrics, and engineering design.\nProvides necessary conditions (and sufficient for convex problems) for optimality.\n\n\n\nA Gentle Proof (Intuition)\nAt an optimum, any feasible perturbation \\(\\Delta x\\) must not reduce \\(f(x)\\). This is only possible if the gradient of \\(f\\) lies within the cone formed by the gradients of the active constraints. KKT multipliers \\(\\lambda_i\\) express this combination mathematically.\n\n\nTry It Yourself\n\nMinimize \\(f(x)=x^2+y^2\\) subject to \\(x+y\\ge1\\).\nSolve \\(f(x)=x^2\\) with constraint \\(x\\ge1\\).\nCompare unconstrained and constrained solutions.\nImplement KKT solver using symbolic differentiation.\n\n\n\nTest Cases\n\n\n\nFunction\nConstraint\nResult \\((x^*)\\)\n\\(\\lambda^*\\)\n\n\n\n\n\\(x^2\\)\n\\(1-x\\le0\\)\n1\n2\n\n\n\\(x^2+y^2\\)\n\\(x+y-1=0\\)\n\\((0.5, 0.5)\\)\n1\n\n\n\\(x^2\\)\n\\(x\\ge0\\)\n0\n0\n\n\n\n\n\nComplexity\n\nSymbolic solving: \\(O(n^3)\\) for small systems.\nNumerical KKT solvers (e.g. Sequential Quadratic Programming): \\(O(n^3)\\) per iteration.\n\nThe KKT conditions are the grammar of optimization, expressing how objectives and constraints negotiate balance at the frontier of possibility.\n\n\n\n590 Coordinate Descent\nThe Coordinate Descent method is one of the simplest yet surprisingly powerful algorithms for optimization. Instead of adjusting all variables at once, it updates one coordinate at a time, cycling through variables until convergence. It’s widely used in LASSO regression, matrix factorization, and sparse optimization.\n\nWhat Problem Are We Solving?\nWe want to minimize a function\n\\[\nf(x_1, x_2, \\dots, x_n)\n\\]\npossibly subject to simple constraints (like \\(x_i \\ge 0\\)).\n\n\nThe Idea\nRather than tackling the full gradient \\(\\nabla f(x)\\) at once, we fix all variables except one and minimize with respect to that variable.\nFor example, in 2D:\n\\[\nf(x, y) \\rightarrow \\text{first fix } y, \\text{ minimize over } x; \\text{ then fix } x, \\text{ minimize over } y.\n\\]\nEach update step reduces the objective, leading to convergence for convex functions.\n\n\nAlgorithm Steps\nGiven an initial vector \\(x^{(0)}\\):\n\nFor each coordinate \\(i = 1, \\dots, n\\):\n\nDefine the partial function \\(f_i(x_i) = f(x_1, \\dots, x_i, \\dots, x_n)\\), holding other variables fixed.\nFind \\[\nx_i^{(k+1)} = \\arg \\min_{x_i} f_i(x_i)\n\\]\n\nRepeat until convergence (when \\(f(x)\\) changes negligibly or \\(|x^{(k+1)} - x^{(k)}| &lt; \\varepsilon\\)).\n\n\n\nExample: Simple Quadratic Function\nMinimize \\[\nf(x, y) = (x - 2)^2 + (y - 3)^2.\n\\]\n\nInitialize \\((x, y) = (0, 0)\\).\nFix \\(y=0\\): minimize \\((x-2)^2\\) → \\(x=2\\).\nFix \\(x=2\\): minimize \\((y-3)^2\\) → \\(y=3\\).\nDone, reached \\((2, 3)\\) in one sweep.\n\nFor convex quadratics, coordinate descent converges linearly to the global minimum.\n\n\nTiny Code\nPython\nimport numpy as np\n\ndef coordinate_descent(f, grad, x0, tol=1e-6, max_iter=1000):\n    x = x0.copy()\n    n = len(x)\n    for _ in range(max_iter):\n        x_old = x.copy()\n        for i in range(n):\n            g = grad(x)\n            x[i] -= 0.1 * g[i]  # simple step along coordinate gradient\n        if np.linalg.norm(x - x_old) &lt; tol:\n            break\n    return x\n\n# Example: f(x,y) = (x-2)^2 + (y-3)^2\nf = lambda v: (v[0]-2)2 + (v[1]-3)2\ngrad = lambda v: np.array([2*(v[0]-2), 2*(v[1]-3)])\nx = coordinate_descent(f, grad, np.array([0.0, 0.0]))\nprint(\"Minimum:\", x)\nC (simple version)\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nint main(void) {\n    double x = 0, y = 0;\n    double alpha = 0.1;\n    for (int iter = 0; iter &lt; 100; iter++) {\n        double x_old = x, y_old = y;\n        x -= alpha * 2 * (x - 2);\n        y -= alpha * 2 * (y - 3);\n        if (fabs(x - x_old) &lt; 1e-6 && fabs(y - y_old) &lt; 1e-6) break;\n    }\n    printf(\"Minimum at (%.6f, %.6f)\\n\", x, y);\n    return 0;\n}\n\n\nWhy It Matters\n\nExtremely simple to implement.\nWorks well when \\(f(x)\\) is separable or coordinate-wise convex.\nScales to very high dimensions, since each step only updates one variable.\nUsed in LASSO, ridge regression, support vector machines, and matrix factorization.\n\n\n\nA Gentle Proof (Intuition)\nAt each iteration, minimizing over one coordinate cannot increase \\(f(x)\\). Thus, \\(f(x^{(k)})\\) is non-increasing and converges. For convex \\(f\\), this limit is the global minimum.\n\n\nTry It Yourself\n\nMinimize \\(f(x, y) = (x-1)^2 + (2y-3)^2\\).\nAdd constraint \\(x, y \\ge 0\\).\nReplace gradient update with exact 1D minimization per coordinate.\nTry a non-convex example, observe convergence to local minima.\n\n\n\nTest Cases\n\n\n\nFunction\nStart\nResult\nNotes\n\n\n\n\n\\((x-2)^2+(y-3)^2\\)\n(0,0)\n(2,3)\nQuadratic, exact minimum\n\n\n\\((x-1)^2+(y-1)^2\\)\n(5,5)\n(1,1)\nConverges linearly\n\n\n\\(\\sin(x)+\\sin(y)\\)\n(2,2)\n(π/2, π/2)\nLocal minimum\n\n\n\n\n\nComplexity\n\nEach coordinate update: \\(O(1)\\) (if gradient is cheap).\nOne full cycle: \\(O(n)\\).\nTotal complexity: \\(O(nk)\\) for \\(k\\) iterations.\n\nCoordinate Descent embodies the minimalist spirit of optimization, improving one small piece at a time, yet always moving closer to the whole solution.\n\n\n\n\nSection 60. Algebraic Tricks and Transform Techniques\n\n591 Polynomial Multiplication (FFT)\nMultiply two polynomials fast by converting coefficient vectors to point values via the Fast Fourier Transform, multiplying pointwise, then transforming back. This computes convolution in \\(O(n\\log n)\\) instead of \\(O(n^2)\\).\n\nWhat Problem Are We Solving?\nGiven \\[\nA(x)=\\sum_{i=0}^{n-1}a_i x^i,\\quad B(x)=\\sum_{j=0}^{m-1}b_j x^j,\n\\] compute coefficients of \\[\nC(x)=A(x)B(x)=\\sum_{k=0}^{n+m-2}c_k x^k,\n\\quad c_k=\\sum_{i+j=k}a_i b_j.\n\\]\nNaive convolution is \\(O(nm)\\). FFT uses the Convolution Theorem to do it in \\(O(N\\log N)\\) where \\(N\\) is a power of two with \\(N\\ge n+m-1\\).\n\n\nHow Does It Work\n\nChoose size: \\(N=\\text{next power of two}\\ge n+m-1\\).\nZero pad \\(a\\) and \\(b\\) to length \\(N\\).\nFFT both sequences: \\(\\hat a=\\operatorname{FFT}(a)\\), \\(\\hat b=\\operatorname{FFT}(b)\\).\nPointwise multiply: \\(\\hat c_k=\\hat a_k\\hat b_k\\).\nInverse FFT: \\(c=\\operatorname{IFFT}(\\hat c)\\).\nRound real parts to nearest integers if inputs are integers.\n\nConvolution Theorem: \\[\n\\mathcal{F}(a*b)=\\mathcal{F}(a)\\odot \\mathcal{F}(b).\n\\]\n\n\nTiny Code\nPython (NumPy, real coefficients)\nimport numpy as np\n\ndef poly_mul_fft(a, b):\n    n = len(a) + len(b) - 1\n    N = 1 &lt;&lt; (n - 1).bit_length()\n    fa = np.fft.rfft(a, N)\n    fb = np.fft.rfft(b, N)\n    fc = fa * fb\n    c = np.fft.irfft(fc, N)[:n]\n    # If inputs are integers, round to nearest int\n    return np.rint(c).astype(int)\n\n# Example\nprint(poly_mul_fft([1,2,3], [4,5]))  # [4,13,22,15]\nC++17 (iterative Cooley–Tukey with std::complex)\n#include &lt;bits/stdc++.h&gt;\nusing namespace std;\n\nusing cd = complex&lt;double&gt;;\nconst double PI = acos(-1);\n\nvoid fft(vector&lt;cd&gt;& a, bool inv){\n    int n = (int)a.size();\n    static vector&lt;int&gt; rev;\n    static vector&lt;cd&gt; roots{0,1};\n    if ((int)rev.size() != n){\n        int k = __builtin_ctz(n);\n        rev.assign(n,0);\n        for (int i=0;i&lt;n;i++)\n            rev[i] = (rev[i&gt;&gt;1]&gt;&gt;1) | ((i&1)&lt;&lt;(k-1));\n    }\n    if ((int)roots.size() &lt; n){\n        int k = __builtin_ctz(roots.size());\n        roots.resize(n);\n        while ((1&lt;&lt;k) &lt; n){\n            double ang = 2*PI/(1&lt;&lt;(k+1));\n            for (int i=1&lt;&lt;(k-1); i&lt;(1&lt;&lt;k); i++){\n                roots[2*i]   = roots[i];\n                roots[2*i+1] = cd(cos(ang*(2*i+1-(1&lt;&lt;k))), sin(ang*(2*i+1-(1&lt;&lt;k))));\n            }\n            k++;\n        }\n    }\n    for (int i=0;i&lt;n;i++) if (i&lt;rev[i]) swap(a[i],a[rev[i]]);\n    for (int len=1; len&lt;n; len&lt;&lt;=1){\n        for (int i=0;i&lt;n;i+=2*len){\n            for (int j=0;j&lt;len;j++){\n                cd u=a[i+j];\n                cd v=a[i+j+len]*roots[len+j];\n                a[i+j]=u+v;\n                a[i+j+len]=u-v;\n            }\n        }\n    }\n    if (inv){\n        reverse(a.begin()+1, a.end());\n        for (auto& x:a) x/=n;\n    }\n}\n\nvector&lt;long long&gt; multiply(const vector&lt;long long&gt;& A, const vector&lt;long long&gt;& B){\n    int n = 1;\n    int need = (int)A.size() + (int)B.size() - 1;\n    while (n &lt; need) n &lt;&lt;= 1;\n    vector&lt;cd&gt; fa(n), fb(n);\n    for (size_t i=0;i&lt;A.size();i++) fa[i] = A[i];\n    for (size_t i=0;i&lt;B.size();i++) fb[i] = B[i];\n    fft(fa,false); fft(fb,false);\n    for (int i=0;i&lt;n;i++) fa[i] *= fb[i];\n    fft(fa,true);\n    vector&lt;long long&gt; C(need);\n    for (int i=0;i&lt;need;i++) C[i] = llround(fa[i].real());\n    return C;\n}\n\nint main(){\n    vector&lt;long long&gt; a={1,2,3}, b={4,5};\n    auto c = multiply(a,b); // 4 13 22 15\n    for (auto x:c) cout&lt;&lt;x&lt;&lt;\" \";\n    cout&lt;&lt;\"\\n\";\n}\n\n\nWhy It Matters\n\nReduces polynomial multiplication from quadratic to quasi-linear time.\nBackbone for big integer arithmetic, signal processing, string matching via convolution, and combinatorial counting.\nExtends to multidimensional convolutions.\n\n\n\nA Gentle Proof Idea\nChoose \\(N\\)-th roots of unity \\(\\omega_N^k\\). The DFT evaluates a degree less than \\(N\\) polynomial at \\(N\\) distinct points: \\[\n\\hat a_k=\\sum_{t=0}^{N-1} a_t \\omega_N^{kt}.\n\\] Pointwise multiplication gives values of \\(C(x)=A(x)B(x)\\) at the same points: \\(\\hat c_k=\\hat a_k\\hat b_k\\). Since evaluation points are distinct, inverse DFT uniquely reconstructs \\(c_0,\\dots,c_{N-1}\\).\n\n\nPractical Tips\n\nPick \\(N\\) as a power of two for fastest FFT.\nFor integer inputs, rounding after IFFT recovers exact coefficients.\nLarge coefficients risk floating error. Two fixes:\n\nsplit coefficients into chunks and use two or three FFTs with Chinese Remainder reconstruction,\nuse NTT over a prime modulus for exact modular convolution.\n\nTrim trailing zeros.\n\n\n\nTry It Yourself\n\nMultiply \\((1+2x+3x^2)\\) by \\((4+5x)\\) by hand and compare to FFT result.\nConvolve two random length 10 vectors and verify against naive \\(O(n^2)\\).\nMeasure runtime vs naive as sizes double.\nImplement circular convolution and compare with linear convolution via zero padding.\nExplore double-precision limits by scaling inputs.\n\n\n\nTest Cases\n\n\n\nInput A\nInput B\nOutput C\n\n\n\n\n\\([1,2,3]\\)\n\\([4,5]\\)\n\\([4,13,22,15]\\)\n\n\n\\([1,0,1]\\)\n\\([1,1]\\)\n\\([1,1,1,1]\\)\n\n\n\\([2,3,4]\\)\n\\([5,6,7]\\)\n\\([10,27,52,45,28]\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(N\\log N)\\) for \\(N\\ge n+m-1\\)\nSpace: \\(O(N)\\)\n\nFFT based multiplication is the standard fast path for large polynomial and integer products.\n\n\n\n592 Polynomial Inversion (Newton Iteration)\nPolynomial inversion finds a series \\(B(x)\\) such that\n\\[\nA(x)B(x)\\equiv1\\pmod{x^n}.\n\\]\nThat means multiplying \\(A\\) and \\(B\\) should yield \\(1\\) up to terms of degree \\(n-1\\). It’s the polynomial version of computing \\(1/a\\) using iterative refinement, built on the same Newton–Raphson principle.\n\nWhat Problem Are We Solving?\nGiven a formal power series\n\\[\nA(x)=a_0+a_1x+a_2x^2+\\dots,\n\\]\nwe want another series\n\\[\nB(x)=b_0+b_1x+b_2x^2+\\dots\n\\]\nsuch that\n\\[\nA(x)B(x)=1.\n\\]\nThis is only possible if \\(a_0\\ne0\\).\n\n\nNewton’s Method for Series\nWe use Newton iteration, similar to numerical inversion. Let \\(B_k(x)\\) be our current approximation modulo \\(x^k\\). Then we refine it using:\n\\[\nB_{2k}(x)=B_k(x),(2-A(x)B_k(x)) \\pmod{x^{2k}}.\n\\]\nEach iteration doubles the correct number of terms.\n\n\nAlgorithm Steps\n\nStart with \\(B_1(x)=1/a_0\\).\nFor \\(k=1,2,4,8,\\dots\\) until \\(k\\ge n\\):\n\nCompute \\(C(x)=A(x)B_k(x)\\pmod{x^{2k}}\\).\nUpdate \\(B_{2k}(x)=B_k(x),(2-C(x))\\pmod{x^{2k}}\\).\n\nTruncate \\(B(x)\\) to degree \\(n-1\\).\n\nAll polynomial multiplications use FFT or NTT for speed.\n\n\nExample\nSuppose\n\\[\nA(x)=1+x.\n\\]\nWe expect\n\\[\nB(x)=1-x+x^2-x^3+\\dots.\n\\]\nStep 1: \\(B_1=1\\). Step 2: \\(A(x)B_1=1+x\\), so\n\\[\nB_2=B_1(2-(1+x))=1-x.\n\\]\nStep 3: \\(A(x)B_2=(1+x)(1-x)=1-x^2\\),\n\\[\nB_4=B_2(2-(1-x^2))=(1-x)(1+x^2)=1-x+x^2-x^3.\n\\]\nAnd so on. Each step doubles precision.\n\n\nTiny Code\nPython (using NumPy FFT)\nimport numpy as np\n\ndef poly_mul(a, b):\n    n = len(a) + len(b) - 1\n    N = 1 &lt;&lt; (n - 1).bit_length()\n    fa = np.fft.rfft(a, N)\n    fb = np.fft.rfft(b, N)\n    fc = fa * fb\n    c = np.fft.irfft(fc, N)[:n]\n    return c\n\ndef poly_inv(a, n):\n    b = np.array([1 / a[0]])\n    k = 1\n    while k &lt; n:\n        k *= 2\n        ab = poly_mul(a[:k], b)[:k]\n        b = (b * 2 - poly_mul(b, ab)[:k])[:k]\n    return b[:n]\n\n# Example: invert 1 + x\na = np.array([1.0, 1.0])\nb = poly_inv(a, 8)\nprint(np.round(b, 3))\n# [1. -1. 1. -1. 1. -1. 1. -1.]\n\n\nWhy It Matters\n\nUsed for series division, modular inverses, and polynomial division in FFT-based arithmetic.\nCore primitive in formal power series computations.\nAppears in combinatorics, symbolic algebra, and computer algebra systems.\n\n\n\nIntuition Behind Newton Update\nIf we want \\(AB=1\\), define \\[\nF(B)=A(x)B(x)-1.\n\\] Newton iteration gives\n\\[\nB_{k+1}=B_k-F(B_k)/A(x)=B_k-(A(x)B_k-1)B_k=B_k(2-A(x)B_k),\n\\]\nwhich matches our polynomial update rule.\n\n\nTry It Yourself\n\nInvert \\(A(x)=1+x+x^2\\) up to degree 8.\nVerify by multiplying \\(A(x)B(x)\\) and confirming all terms above constant vanish.\nImplement with modular arithmetic under prime \\(p\\).\nCompare FFT-based vs naive performance.\n\n\n\nTest Cases\n\n\n\n\\(A(x)\\)\n\\(B(x)\\) (first terms)\nVerification\n\n\n\n\n\\(1+x\\)\n\\(1-x+x^2-x^3+\\dots\\)\n\\((1+x)(1-x+x^2-\\dots)=1\\)\n\n\n\\(1-2x\\)\n\\(1+2x+4x^2+8x^3+\\dots\\)\n\\((1-2x)(1+2x+4x^2+\\dots)=1\\)\n\n\n\\(2+x\\)\n\\(0.5-0.25x+0.125x^2-\\dots\\)\n\\((2+x)(B)=1\\)\n\n\n\n\n\nComplexity\n\nEach iteration doubles precision.\nUses FFT multiplication → \\(O(n\\log n)\\).\nMemory: \\(O(n)\\).\n\nPolynomial inversion by Newton iteration is a masterclass in algebraic efficiency, one simple update that doubles accuracy each time.\n\n\n\n593 Polynomial Derivative\nTaking the derivative of a polynomial is one of the simplest symbolic operations, yet it appears everywhere in algorithms for optimization, root finding, series manipulation, and numerical analysis.\nGiven a polynomial \\[\nA(x)=a_0+a_1x+a_2x^2+\\dots+a_nx^n,\n\\] its derivative is \\[\nA'(x)=a_1+2a_2x+3a_3x^2+\\dots+n a_nx^{n-1}.\n\\]\n\nWhat Problem Are We Solving?\nWe want to compute the derivative coefficients efficiently and represent \\(A'(x)\\) in the same coefficient form as \\(A(x)\\). If \\[\nA(x)=[a_0,a_1,\\dots,a_n],\n\\] then \\[\nA'(x)=[a_1,2a_2,3a_3,\\dots,n a_n].\n\\]\nThis operation runs in \\(O(n)\\) time and is a key subroutine in polynomial algebra and calculus of formal power series.\n\n\nAlgorithm Steps\n\nGiven coefficients \\(a_0,\\dots,a_n\\).\nFor each \\(i\\) from \\(1\\) to \\(n\\):\n\nCompute \\(b_{i-1}=i\\times a_i\\).\n\nReturn coefficients \\([b_0,b_1,\\dots,b_{n-1}]\\) of \\(A'(x)\\).\n\n\n\nExample\nLet \\[\nA(x)=3+2x+5x^2+4x^3.\n\\]\nThen \\[\nA'(x)=2+10x+12x^2.\n\\]\nIn coefficient form:\n\n\n\nTerm\nCoefficient\nNew Coefficient\n\n\n\n\n\\(x^0\\)\n\\(3\\)\n,\n\n\n\\(x^1\\)\n\\(2\\)\n\\(2\\times1=2\\)\n\n\n\\(x^2\\)\n\\(5\\)\n\\(5\\times2=10\\)\n\n\n\\(x^3\\)\n\\(4\\)\n\\(4\\times3=12\\)\n\n\n\n\n\nTiny Code\nPython\ndef poly_derivative(a):\n    n = len(a)\n    return [i * a[i] for i in range(1, n)]\n\n# Example\nA = [3, 2, 5, 4]\nprint(poly_derivative(A))  # [2, 10, 12]\nC\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    double a[] = {3, 2, 5, 4};\n    int n = 4;\n    double d[n - 1];\n    for (int i = 1; i &lt; n; i++)\n        d[i - 1] = i * a[i];\n    for (int i = 0; i &lt; n - 1; i++)\n        printf(\"%.0f \", d[i]);\n    printf(\"\\n\");\n    return 0;\n}\n\n\nWhy It Matters\n\nCentral in Newton–Raphson root-finding, gradient descent, and optimization.\nAppears in polynomial division, Taylor series, differential equations, and symbolic algebra.\nUsed in automatic differentiation and backpropagation as the algebraic foundation.\n\n\n\nA Gentle Proof\nFrom calculus: \\[\n\\frac{d}{dx}(x^i)=i x^{i-1}.\n\\]\nSince \\(A(x)=\\sum_i a_i x^i\\), linearity gives \\[\nA'(x)=\\sum_i a_i i x^{i-1}.\n\\] Thus the coefficient of \\(x^{i-1}\\) in \\(A'(x)\\) is \\(i a_i\\).\n\n\nTry It Yourself\n\nDifferentiate \\(A(x)=5x^4+2x^3-x^2+3x-7\\).\nCompute the derivative twice (second derivative).\nImplement derivative modulo \\(p\\) for large integer polynomials.\nUse derivative in Newton–Raphson root updates: \\(x_{k+1}=x_k-\\frac{A(x_k)}{A'(x_k)}.\\)\n\n\n\nTest Cases\n\n\n\nPolynomial\nDerivative\nResult\n\n\n\n\n\\(3+2x+5x^2+4x^3\\)\n\\(2+10x+12x^2\\)\n[2,10,12]\n\n\n\\(x^4\\)\n\\(4x^3\\)\n[0,0,0,4]\n\n\n\\(1+x+x^2+x^3\\)\n\\(1+2x+3x^2\\)\n[1,2,3]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nPolynomial differentiation is a one-line operation, but it underlies much of algorithmic calculus, the small step that powers big changes.\n\n\n\n594 Polynomial Integration\nPolynomial integration is the reverse of differentiation: we find a polynomial \\(B(x)\\) such that \\(B'(x)=A(x)\\). It’s a simple yet vital tool in symbolic algebra, numerical integration, and generating functions.\n\nWhat Problem Are We Solving?\nGiven \\[\nA(x)=a_0+a_1x+a_2x^2+\\dots+a_{n-1}x^{n-1},\n\\] we want \\[\nB(x)=\\int A(x),dx=C+b_1x+b_2x^2+\\dots+b_nx^n,\n\\] where each coefficient satisfies \\[\nb_{i+1}=\\frac{a_i}{i+1}.\n\\]\nUsually, we set the constant of integration \\(C=0\\) for computational purposes.\n\n\nHow It Works\nIf \\(A(x)=[a_0,a_1,a_2,\\dots,a_{n-1}]\\), then \\[\nB(x)=[0,\\frac{a_0}{1},\\frac{a_1}{2},\\frac{a_2}{3},\\dots,\\frac{a_{n-1}}{n}].\n\\]\nEach term is divided by its new exponent index.\n\n\nExample\nLet \\[\nA(x)=2+10x+12x^2.\n\\]\nThen \\[\nB(x)=2x+\\frac{10}{2}x^2+\\frac{12}{3}x^3=2x+5x^2+4x^3.\n\\]\nIn coefficient form:\n\n\n\nTerm\nCoefficient in A(x)\nIntegrated Term\nCoefficient in B(x)\n\n\n\n\n\\(x^0\\)\n2\n\\(2x\\)\n2\n\n\n\\(x^1\\)\n10\n\\(10x^2/2\\)\n5\n\n\n\\(x^2\\)\n12\n\\(12x^3/3\\)\n4\n\n\n\nSo \\(B(x)=[0,2,5,4]\\).\n\n\nTiny Code\nPython\ndef poly_integrate(a):\n    n = len(a)\n    return [0.0] + [a[i] / (i + 1) for i in range(n)]\n\n# Example\nA = [2, 10, 12]\nprint(poly_integrate(A))  # [0.0, 2.0, 5.0, 4.0]\nC\n#include &lt;stdio.h&gt;\n\nint main(void) {\n    double a[] = {2, 10, 12};\n    int n = 3;\n    double b[n + 1];\n    b[0] = 0.0;\n    for (int i = 0; i &lt; n; i++)\n        b[i + 1] = a[i] / (i + 1);\n    for (int i = 0; i &lt;= n; i++)\n        printf(\"%.2f \", b[i]);\n    printf(\"\\n\");\n    return 0;\n}\n\n\nWhy It Matters\n\nConverts differential equations into integral form.\nCore for symbolic calculus and automatic integration.\nUsed in Taylor series reconstruction, antiderivative computation, and formal power series analysis.\nIn computational contexts, enables integration modulo p for exact algebraic manipulation.\n\n\n\nA Gentle Proof\nWe know from calculus: \\[\n\\frac{d}{dx}(x^{i+1})=(i+1)x^i.\n\\]\nThus, to “undo” differentiation, each term’s coefficient is divided by \\((i+1)\\): \\[\n\\int a_i x^i dx = \\frac{a_i}{i+1}x^{i+1}.\n\\]\nLinearity of integration guarantees the full polynomial follows the same rule.\n\n\nTry It Yourself\n\nIntegrate \\(A(x)=3+4x+5x^2\\) with \\(C=0\\).\nAdd a nonzero constant of integration \\(C=7\\).\nVerify by differentiating your result.\nImplement integration under modulo arithmetic (\\(\\text{mod }p\\)).\nUse integration to compute area under a polynomial curve from \\(x=0\\) to \\(x=1\\).\n\n\n\nTest Cases\n\n\n\n\n\n\n\n\n\\(A(x)\\)\n\\(B(x)\\)\nVerification\n\n\n\n\n\\(2+10x+12x^2\\)\n\\(2x+5x^2+4x^3\\)\n\\(B'(x)=A(x)\\)\n\n\n\\(1+x+x^2\\)\n\\(x+\\frac{x^2}{2}+\\frac{x^3}{3}\\)\nderivative recovers \\(A(x)\\)\n\n\n\\(5x^2\\)\n\\(\\frac{5x^3}{3}\\)\nderivative gives \\(5x^2\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(n)\\)\nSpace: \\(O(n)\\)\n\nPolynomial integration is straightforward but fundamental, turning discrete coefficients into a continuous curve, one fraction at a time.\n\n\n\n595 Formal Power Series Composition\nFormal power series (FPS) composition is the operation of substituting one series into another, \\[\nC(x)=A(B(x)),\n\\] where both \\(A(x)\\) and \\(B(x)\\) are formal power series with no concern for convergence, we only care about coefficients up to a chosen degree \\(n\\).\nIt’s a fundamental operation in algebraic combinatorics, symbolic computation, and generating function analysis.\n\nWhat Problem Are We Solving?\nGiven \\[\nA(x)=a_0+a_1x+a_2x^2+\\dots,\n\\] and \\[\nB(x)=b_0+b_1x+b_2x^2+\\dots,\n\\] we want \\[\nC(x)=A(B(x))=a_0+a_1B(x)+a_2(B(x))^2+a_3(B(x))^3+\\dots.\n\\]\nWe compute only terms up to degree \\(n-1\\).\n\n\nKey Assumptions\n\nUsually \\(b_0=0\\) (so \\(B(x)\\) has no constant term), otherwise \\(A(B(x))\\) involves a constant shift.\nThe target degree \\(n\\) limits all computations, truncate at each step.\n\n\n\nHow It Works\n\nInitialize \\(C(x)=[a_0]\\).\nFor each \\(k\\ge1\\):\n\nCompute \\((B(x))^k\\) using repeated convolution or precomputed powers.\nMultiply by \\(a_k\\).\nAdd to \\(C(x)\\), truncating after degree \\(n-1\\).\n\n\nMathematically: \\[\nC(x)=\\sum_{k=0}^{n-1} a_k (B(x))^k \\pmod{x^n}.\n\\]\nEfficient algorithms use divide-and-conquer and FFT-based polynomial multiplications.\n\n\nExample\nLet \\[\nA(x)=1+x+x^2, \\quad B(x)=x+x^2.\n\\]\nThen: \\[\nA(B(x))=1+(x+x^2)+(x+x^2)^2=1+x+x^2+x^2+2x^3+x^4.\n\\] Simplify: \\[\nA(B(x))=1+x+2x^2+2x^3+x^4.\n\\]\nCoefficient form: \\([1,1,2,2,1]\\).\n\n\nTiny Code\nPython\nimport numpy as np\n\ndef poly_mul(a, b, n):\n    m = len(a) + len(b) - 1\n    N = 1 &lt;&lt; (m - 1).bit_length()\n    fa = np.fft.rfft(a, N)\n    fb = np.fft.rfft(b, N)\n    fc = fa * fb\n    c = np.fft.irfft(fc, N)[:n]\n    return np.rint(c).astype(int)\n\ndef poly_compose(a, b, n):\n    res = np.zeros(n, dtype=int)\n    term = np.ones(1, dtype=int)\n    for i in range(len(a)):\n        if i &gt; 0:\n            term = poly_mul(term, b, n)\n        res[:len(term)] += a[i] * term[:n]\n    return res[:n]\n\n# Example\nA = [1, 1, 1]\nB = [0, 1, 1]\nprint(poly_compose(A, B, 5))  # [1, 1, 2, 2, 1]\n\n\nWhy It Matters\n\nComposition is central in generating functions for combinatorial classes.\nAppears in power series reversion, functional iteration, and differential equations.\nUsed in Taylor expansion of composite functions and symbolic algebra systems.\nRequired for constructing exponential generating functions and series transformations.\n\n\n\nA Gentle Proof\nUsing the formal definition: \\[\nA(x)=\\sum_{k=0}^\\infty a_kx^k.\n\\]\nThen substituting \\(B(x)\\) gives \\[\nA(B(x))=\\sum_{k=0}^\\infty a_k (B(x))^k.\n\\]\nSince we only keep coefficients up to \\(x^{n-1}\\), higher-degree terms vanish modulo \\(x^n\\). Each \\((B(x))^k\\) contributes terms of degree \\(\\ge k\\), ensuring finite computation.\n\n\nTry It Yourself\n\nCompute \\(A(B(x))\\) for \\(A(x)=1+x+x^2\\), \\(B(x)=x+x^2\\).\nCompare against symbolic expansion.\nTest \\(A(x)=\\exp(x)\\) and \\(B(x)=\\sin(x)\\) up to degree 6.\nImplement truncated FFT-based composition for large \\(n\\).\nExplore reversion: find \\(B(x)\\) such that \\(A(B(x))=x\\).\n\n\n\nTest Cases\n\n\n\n\\(A(x)\\)\n\\(B(x)\\)\nResult up to \\(x^4\\)\n\n\n\n\n\\(1+x+x^2\\)\n\\(x+x^2\\)\n\\(1+x+2x^2+2x^3+x^4\\)\n\n\n\\(1+2x\\)\n\\(x+x^2\\)\n\\(1+2x+2x^2\\)\n\n\n\\(1+x^2\\)\n\\(x+x^2\\)\n\\(1+x^2+2x^3+x^4\\)\n\n\n\n\n\nComplexity\n\nNaive: \\(O(n^2)\\)\nFFT-based: \\(O(n\\log n)\\) for each multiplication\nDivide-and-conquer composition: \\(O(n^{1.5})\\) with advanced algorithms\n\nFormal power series composition is where algebra meets structure, substituting one infinite series into another to build new functions, patterns, and generating laws.\n\n\n\n596 Exponentiation by Squaring\nExponentiation by squaring is a fast method to compute powers efficiently. Instead of multiplying repeatedly, it halves the exponent at each step, a divide-and-conquer strategy that reduces the time from \\(O(n)\\) to \\(O(\\log n)\\) multiplications.\nIt’s the workhorse behind modular exponentiation, fast matrix powers, and polynomial powering.\n\nWhat Problem Are We Solving?\nWe want to compute \\[\na^n\n\\] for integer (or polynomial, matrix) \\(a\\) and nonnegative integer \\(n\\).\nNaively we’d multiply \\(a\\) by itself \\(n\\) times, which is slow. Exponentiation by squaring uses the binary expansion of \\(n\\) to skip unnecessary multiplications.\n\n\nKey Idea\nUse these rules:\n\\[\na^n =\n\\begin{cases}\n1, & n = 0, \\\\[6pt]\n\\left(a^{\\,n/2}\\right)^2, & n \\text{ even}, \\\\[6pt]\na \\cdot \\left(a^{\\,(n-1)/2}\\right)^2, & n \\text{ odd}.\n\\end{cases}\n\\]\nEach step halves \\(n\\), so only \\(\\log_2 n\\) levels of recursion are needed.\n\n\nExample\nCompute \\(a^9\\):\n\n\n\nStep\nExpression\nSimplified\n\n\n\n\n\\(a^9\\)\n\\(a\\cdot(a^4)^2\\)\nuses odd rule\n\n\n\\(a^4\\)\n\\((a^2)^2\\)\nuses even rule\n\n\n\\(a^2\\)\n\\((a^1)^2\\)\nbase recursion\n\n\n\\(a^1\\)\n\\(a\\)\nbase case\n\n\n\nTotal: 4 multiplications instead of 8.\n\n\nTiny Code\nPython\ndef power(a, n):\n    if n == 0:\n        return 1\n    if n % 2 == 0:\n        half = power(a, n // 2)\n        return half * half\n    else:\n        half = power(a, (n - 1) // 2)\n        return a * half * half\n\nprint(power(3, 9))  # 19683\nC\n#include &lt;stdio.h&gt;\n\nlong long power(long long a, long long n) {\n    if (n == 0) return 1;\n    if (n % 2 == 0) {\n        long long half = power(a, n / 2);\n        return half * half;\n    } else {\n        long long half = power(a, n / 2);\n        return a * half * half;\n    }\n}\n\nint main(void) {\n    printf(\"%lld\\n\", power(3, 9)); // 19683\n    return 0;\n}\nModular version (Python)\ndef modpow(a, n, m):\n    res = 1\n    a %= m\n    while n &gt; 0:\n        if n & 1:\n            res = (res * a) % m\n        a = (a * a) % m\n        n &gt;&gt;= 1\n    return res\n\nprint(modpow(3, 200, 1000000007))  # fast\n\n\nWhy It Matters\n\nUsed in modular arithmetic, cryptography, RSA, and discrete exponentiation.\nEssential for matrix exponentiation in dynamic programming and Fibonacci computation.\nBasis for polynomial powering, fast doubling, and fast modular inverse routines.\n\n\n\nA Gentle Proof\nEach step halves the exponent, maintaining equivalence:\nFor even \\(n\\): \\[\na^n=(a^{n/2})^2.\n\\] For odd \\(n\\): \\[\na^n=a\\cdot(a^{(n-1)/2})^2.\n\\] Since each recursion uses \\(n/2\\), total calls are \\(O(\\log n)\\).\n\n\nTry It Yourself\n\nCompute \\(2^{31}\\) manually using repeated squaring.\nCompare multiplication counts with naive \\(O(n)\\) method.\nModify algorithm to work on \\(2\\times2\\) matrices.\nExtend to polynomials using convolution for multiplication.\nImplement an iterative version and benchmark both.\n\n\n\nTest Cases\n\n\n\nBase \\(a\\)\nExponent \\(n\\)\nResult\n\n\n\n\n\\(3\\)\n\\(9\\)\n\\(19683\\)\n\n\n\\(2\\)\n\\(10\\)\n\\(1024\\)\n\n\n\\(5\\)\n\\(0\\)\n\\(1\\)\n\n\n\\(7\\)\n\\(13\\)\n\\(96889010407\\)\n\n\n\n\n\nComplexity\n\nTime: \\(O(\\log n)\\) multiplications\nSpace: \\(O(\\log n)\\) (recursive) or \\(O(1)\\) (iterative)\n\nExponentiation by squaring is the quintessential divide-and-conquer power trick, a perfect blend of simplicity, speed, and mathematical elegance.\n\n\n\n597 Modular Exponentiation\nModular exponentiation efficiently computes \\[\na^b \\bmod m\n\\] without overflowing intermediate results. It’s fundamental in cryptography, primality testing, and modular arithmetic systems such as RSA and Diffie–Hellman.\n\nWhat Problem Are We Solving?\nWe want to compute \\[\nr=(a^b)\\bmod m,\n\\] where \\(a\\), \\(b\\), and \\(m\\) are integers and \\(b\\) may be very large.\nDirectly computing \\(a^b\\) is impractical because the intermediate result grows exponentially. We instead apply modular reduction at each multiplication step, using the rule:\n\\[\n(xy)\\bmod m=((x\\bmod m)(y\\bmod m))\\bmod m.\n\\]\n\n\nKey Idea\nCombine modular arithmetic with exponentiation by squaring:\n\\[\na^b \\bmod m =\n\\begin{cases}\n1, & b = 0, \\\\[6pt]\n\\left((a^{\\,b/2} \\bmod m)^2\\right) \\bmod m, & b \\text{ even}, \\\\[6pt]\n\\left(a \\times (a^{\\,(b-1)/2} \\bmod m)^2\\right) \\bmod m, & b \\text{ odd}.\n\\end{cases}\n\\]\nEach step reduces both \\(b\\) and intermediate values modulo \\(m\\).\n\n\nExample\nCompute \\(3^{13}\\bmod 17\\):\n\n\n\n\n\n\n\n\n\nStep\n\\(b\\)\nOperation\nResult\n\n\n\n\n13\nodd\n\\(res=3\\)\n\\(res=3\\)\n\n\n6\neven\nsquare \\(3\\to9\\)\n\\(a=9\\)\n\n\n3\nodd\n\\(res=res*a=3*9=27\\Rightarrow27\\bmod17=10\\)\n\\(res=10\\)\n\n\n1\nodd\n\\(res=res*a=10*13=130\\Rightarrow130\\bmod17=11\\)\nFinal\n\n\n\nResult: \\(3^{13}\\bmod17=11\\)\n\n\nTiny Code\nPython\ndef modexp(a, b, m):\n    res = 1\n    a %= m\n    while b &gt; 0:\n        if b & 1:\n            res = (res * a) % m\n        a = (a * a) % m\n        b &gt;&gt;= 1\n    return res\n\nprint(modexp(3, 13, 17))  # 11\nC\n#include &lt;stdio.h&gt;\n\nlong long modexp(long long a, long long b, long long m) {\n    long long res = 1;\n    a %= m;\n    while (b &gt; 0) {\n        if (b & 1)\n            res = (res * a) % m;\n        a = (a * a) % m;\n        b &gt;&gt;= 1;\n    }\n    return res;\n}\n\nint main(void) {\n    printf(\"%lld\\n\", modexp(3, 13, 17)); // 11\n    return 0;\n}\n\n\nWhy It Matters\n\nCore of RSA encryption/decryption, Diffie–Hellman key exchange, and ElGamal systems.\nEssential in modular inverses, hashing, and primitive root computations.\nEnables large computations like \\(a^{10^{18}}\\bmod m\\) to run in microseconds.\nUsed in Fermat, Miller–Rabin, and Carmichael tests for primality.\n\n\n\nA Gentle Proof\nFor any modulus \\(m\\), \\[\n(a\\times b)\\bmod m=((a\\bmod m)(b\\bmod m))\\bmod m.\n\\] This property lets us reduce after every multiplication.\nUsing binary exponentiation, \\[\na^b=\\prod_{i=0}^{k-1} a^{2^i\\cdot d_i},\n\\] where \\(b=\\sum_i d_i2^i\\). We only multiply terms where \\(d_i=1\\), keeping everything modulo \\(m\\) to stay bounded.\n\n\nTry It Yourself\n\nCompute \\(5^{117}\\bmod19\\).\nCompare against built-in pow(5, 117, 19) in Python.\nModify to handle \\(b&lt;0\\) using modular inverses.\nExtend to matrices modulo \\(m\\).\nProve that \\(a^{p-1}\\bmod p=1\\) for prime \\(p\\) (Fermat’s Little Theorem).\n\n\n\nTest Cases\n\n\n\n\\(a\\)\n\\(b\\)\n\\(m\\)\n\\(a^b\\bmod m\\)\n\n\n\n\n3\n13\n17\n11\n\n\n2\n10\n1000\n24\n\n\n7\n256\n13\n9\n\n\n5\n117\n19\n1\n\n\n\n\n\nComplexity\n\nTime: \\(O(\\log b)\\) multiplications\nSpace: \\(O(1)\\)\n\nModular exponentiation is one of the most elegant and essential routines in computational number theory, small, exact, and powerful enough to secure the internet.\n\n\n\n598 Fast Walsh–Hadamard Transform (FWHT)\nThe Fast Walsh–Hadamard Transform (FWHT) is a divide-and-conquer algorithm for computing pairwise XOR-based convolutions efficiently. It is the discrete analog of the Fast Fourier Transform, but instead of multiplication under addition, it works under the bitwise XOR operation.\n\nWhat Problem Are We Solving?\nGiven two sequences \\[\nA=(a_0,a_1,\\dots,a_{n-1}), \\quad B=(b_0,b_1,\\dots,b_{n-1}),\n\\] we want their XOR convolution defined as\n\\[\nC[k]=\\sum_{i\\oplus j=k}a_i b_j,\n\\] where \\(\\oplus\\) is the bitwise XOR.\nNaively, this requires \\(O(n^2)\\) work. FWHT reduces it to \\(O(n\\log n)\\).\n\n\nKey Idea\nThe Walsh–Hadamard Transform (WHT) maps a vector to its XOR-domain form. We can compute XOR convolutions by transforming both sequences, multiplying pointwise, and then inverting the transform.\nLet \\(\\text{FWT}\\) denote the transform. Then\n\\[\nC=\\text{FWT}^{-1}(\\text{FWT}(A)\\circ\\text{FWT}(B)),\n\\] where \\(\\circ\\) is element-wise multiplication.\n\n\nTransform Definition\nFor \\(n=2^k\\), define the fast Walsh–Hadamard transform (FWT) recursively.\nBase case: \\[\n\\text{FWT}([a_0]) = [a_0].\n\\]\nRecursive step: split \\(A\\) into \\(A_1\\) (first half) and \\(A_2\\) (second half). After computing \\(\\text{FWT}(A_1)\\) and \\(\\text{FWT}(A_2)\\), combine as \\[\n\\forall\\, i=0,\\dots,\\tfrac{n}{2}-1:\\quad\nA'[i] = A_1[i] + A_2[i],\\qquad\nA'[i+\\tfrac{n}{2}] = A_1[i] - A_2[i].\n\\]\nInverse transform: \\[\n\\text{IFWT}(A') = \\frac{1}{n}\\,\\text{FWT}(A').\n\\]\n\n\nExample\nLet \\(A=[1,2,3,4]\\). Split into \\(A_1=[1,2]\\) and \\(A_2=[3,4]\\), then combine \\[\n[\\,1+3,\\ 2+4,\\ 1-3,\\ 2-4\\,] = [\\,4,6,-2,-2\\,].\n\\] For longer vectors, apply the same split–recurse–combine pattern until length \\(1\\).\n\n\nTiny Code\nPython\ndef fwht(a, inverse=False):\n    n = len(a)\n    h = 1\n    while h &lt; n:\n        for i in range(0, n, h * 2):\n            for j in range(h):\n                x = a[i + j]\n                y = a[i + j + h]\n                a[i + j] = x + y\n                a[i + j + h] = x - y\n        h *= 2\n    if inverse:\n        for i in range(n):\n            a[i] //= n\n    return a\n\ndef xor_convolution(a, b):\n    n = 1\n    while n &lt; max(len(a), len(b)):\n        n *= 2\n    a = a + [0] * (n - len(a))\n    b = b + [0] * (n - len(b))\n    A = fwht(a[:])\n    B = fwht(b[:])\n    C = [A[i] * B[i] for i in range(n)]\n    C = fwht(C, inverse=True)\n    return C\n\nprint(xor_convolution([1,2,3,4],[4,3,2,1]))\n\n\nWhy It Matters\n\nXOR convolution appears in subset transforms, bitmask dynamic programming, and Boolean algebra problems.\nUsed in signal processing, error-correcting codes, and polynomial transforms over GF(2).\nKey to fast computations on hypercubes and bitwise domains.\n\n\n\nA Gentle Proof\nEach level of recursion computes pairwise sums and differences — a linear transformation using the Hadamard matrix \\(H_n\\):\n\\[\nH_n=\n\\begin{bmatrix}\nH_{n/2} & H_{n/2}\\\nH_{n/2} & -H_{n/2}\n\\end{bmatrix}.\n\\]\nSince \\(H_nH_n^T=nI\\), its inverse is \\(\\frac{1}{n}H_n^T\\), giving correctness and invertibility.\n\n\nTry It Yourself\n\nCompute the FWHT of \\([1,1,0,0]\\).\nPerform XOR convolution of \\([1,2,3,4]\\) and \\([4,3,2,1]\\).\nModify to handle floating-point or modular arithmetic.\nImplement inverse transform explicitly and verify recovery.\nCompare timing with naive \\(O(n^2)\\) approach.\n\n\n\nTest Cases\n\n\n\nInput A\nInput B\nXOR Convolution Result\n\n\n\n\n[1,2]\n[3,4]\n[10, -2]\n\n\n[1,1,0,0]\n[0,1,1,0]\n[2,0,0,2]\n\n\n[1,2,3,4]\n[4,3,2,1]\n[20, 0, 0, 0]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe Fast Walsh–Hadamard Transform is the XOR-world twin of FFT, smaller, sharper, and just as elegant.\n\n\n\n598 Fast Walsh–Hadamard Transform (FWHT)\nThe Fast Walsh–Hadamard Transform (FWHT) is a divide-and-conquer algorithm for computing pairwise XOR-based convolutions efficiently. It is the discrete analog of the Fast Fourier Transform, but instead of multiplication under addition, it works under the bitwise XOR operation.\n\nWhat Problem Are We Solving?\nGiven two sequences \\[\nA=(a_0,a_1,\\dots,a_{n-1}), \\quad B=(b_0,b_1,\\dots,b_{n-1}),\n\\] we want their XOR convolution defined as\n\\[\nC[k]=\\sum_{i\\oplus j=k}a_i b_j,\n\\] where \\(\\oplus\\) is the bitwise XOR.\nNaively, this requires \\(O(n^2)\\) work. FWHT reduces it to \\(O(n\\log n)\\).\n\n\nKey Idea\nThe Walsh–Hadamard Transform (WHT) maps a vector to its XOR-domain form. We can compute XOR convolutions by transforming both sequences, multiplying pointwise, and then inverting the transform.\nLet \\(\\text{FWT}\\) denote the transform. Then\n\\[\nC=\\text{FWT}^{-1}(\\text{FWT}(A)\\circ\\text{FWT}(B)),\n\\] where \\(\\circ\\) is element-wise multiplication.\n\n\nTransform Definition\nFor \\(n = 2^k\\), recursively define:\n\\[\n\\text{FWT}(A) =\n\\begin{cases}\n[a_0], & n = 1, \\\\[6pt]\n\\text{combine } \\text{FWT}(A_1) \\text{ and } \\text{FWT}(A_2)\n& \\text{for } n &gt; 1.\n\\end{cases}\n\\]\nThe combination step: \\[\nA'[i] = A_1[i] + A_2[i], \\qquad\nA'[i + n/2] = A_1[i] - A_2[i],\n\\quad i = 0, \\dots, n/2 - 1.\n\\]\nTo invert, divide all results by \\(n\\) after applying the same process.\n\n\nExample\nLet \\(A = [1, 2, 3, 4]\\).\n\nSplit into \\([1, 2]\\) and \\([3, 4]\\)\nCombine: \\[\n[1 + 3,\\ 2 + 4,\\ 1 - 3,\\ 2 - 4] = [4, 6, -2, -2].\n\\]\nApply recursively for longer vectors.\n\n\n\nTiny Code\nPython\ndef fwht(a, inverse=False):\n    n = len(a)\n    h = 1\n    while h &lt; n:\n        for i in range(0, n, h * 2):\n            for j in range(h):\n                x = a[i + j]\n                y = a[i + j + h]\n                a[i + j] = x + y\n                a[i + j + h] = x - y\n        h *= 2\n    if inverse:\n        for i in range(n):\n            a[i] //= n\n    return a\n\ndef xor_convolution(a, b):\n    n = 1\n    while n &lt; max(len(a), len(b)):\n        n *= 2\n    a = a + [0] * (n - len(a))\n    b = b + [0] * (n - len(b))\n    A = fwht(a[:])\n    B = fwht(b[:])\n    C = [A[i] * B[i] for i in range(n)]\n    C = fwht(C, inverse=True)\n    return C\n\nprint(xor_convolution([1,2,3,4],[4,3,2,1]))\n\n\nWhy It Matters\n\nXOR convolution appears in subset transforms, bitmask dynamic programming, and Boolean algebra problems.\nUsed in signal processing, error-correcting codes, and polynomial transforms over GF(2).\nKey to fast computations on hypercubes and bitwise domains.\n\n\n\nA Gentle Proof\nEach level of recursion computes pairwise sums and differences — a linear transformation using the Hadamard matrix \\(H_n\\):\n\\[\nH_n=\n\\begin{bmatrix}\nH_{n/2} & H_{n/2}\\\nH_{n/2} & -H_{n/2}\n\\end{bmatrix}.\n\\]\nSince \\(H_nH_n^T=nI\\), its inverse is \\(\\frac{1}{n}H_n^T\\), giving correctness and invertibility.\n\n\nTry It Yourself\n\nCompute the FWHT of \\([1,1,0,0]\\).\nPerform XOR convolution of \\([1,2,3,4]\\) and \\([4,3,2,1]\\).\nModify to handle floating-point or modular arithmetic.\nImplement inverse transform explicitly and verify recovery.\nCompare timing with naive \\(O(n^2)\\) approach.\n\n\n\nTest Cases\n\n\n\nInput A\nInput B\nXOR Convolution Result\n\n\n\n\n[1,2]\n[3,4]\n[10, -2]\n\n\n[1,1,0,0]\n[0,1,1,0]\n[2,0,0,2]\n\n\n[1,2,3,4]\n[4,3,2,1]\n[20, 0, 0, 0]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n\\log n)\\)\nSpace: \\(O(n)\\)\n\nThe Fast Walsh–Hadamard Transform is the XOR-world twin of FFT, smaller, sharper, and just as elegant.\n\n\n\n599 Zeta Transform\nThe Zeta Transform is a combinatorial transform that accumulates values over subsets or supersets. It is especially useful in subset dynamic programming (DP over bitmasks), inclusion–exclusion, and fast subset convolutions.\n\nWhat Problem Are We Solving?\nGiven a function \\(f(S)\\) defined over subsets \\(S\\) of a universe \\(U\\) (size \\(n\\)), the Zeta Transform produces a new function \\(F(S)\\) that sums over all subsets (or supersets) of \\(S\\):\n\nSubset version \\[\nF(S)=\\sum_{T\\subseteq S}f(T)\n\\]\nSuperset version \\[\nF(S)=\\sum_{T\\supseteq S}f(T)\n\\]\n\nThe transform runs in \\(O(n2^n)\\) naively, but can be computed in \\(O(n2^n)\\) efficiently using bit DP.\n\n\nKey Idea\nFor subset zeta transform over bitmasks of size \\(n\\):\nFor each bit \\(i\\) from \\(0\\) to \\(n-1\\):\n\nFor each mask \\(m\\) from \\(0\\) to \\(2^n-1\\):\n\nIf bit \\(i\\) is set in \\(m\\), then add \\(f[m\\text{ without bit }i]\\) to \\(f[m]\\).\n\n\nIn code: \\[\nf[m]+=f[m\\setminus{i}].\n\\]\nThis efficiently accumulates all contributions of subsets.\n\n\nExample\nLet \\(f\\) over 3-bit subsets be:\n\n\n\nMask\nSubset\n\\(f(S)\\)\n\n\n\n\n000\n∅\n1\n\n\n001\n{0}\n2\n\n\n010\n{1}\n3\n\n\n011\n{0,1}\n4\n\n\n100\n{2}\n5\n\n\n101\n{0,2}\n6\n\n\n110\n{1,2}\n7\n\n\n111\n{0,1,2}\n8\n\n\n\nAfter subset zeta transform, \\(F(S)=\\sum_{T\\subseteq S}f(T)\\).\nFor \\(S={0,1}\\) (mask 011): \\[\nF(011)=f(000)+f(001)+f(010)+f(011)=1+2+3+4=10.\n\\]\n\n\nTiny Code\nPython\ndef subset_zeta_transform(f):\n    n = len(f).bit_length() - 1\n    F = f[:]\n    for i in range(n):\n        for mask in range(1 &lt;&lt; n):\n            if mask & (1 &lt;&lt; i):\n                F[mask] += F[mask ^ (1 &lt;&lt; i)]\n    return F\n\n# Example: f for subsets of size 3\nf = [1,2,3,4,5,6,7,8]\nprint(subset_zeta_transform(f))\nC\n#include &lt;stdio.h&gt;\n\nvoid subset_zeta_transform(int *f, int n) {\n    for (int i = 0; i &lt; n; i++) {\n        for (int mask = 0; mask &lt; (1 &lt;&lt; n); mask++) {\n            if (mask & (1 &lt;&lt; i))\n                f[mask] += f[mask ^ (1 &lt;&lt; i)];\n        }\n    }\n}\n\nint main(void) {\n    int f[8] = {1,2,3,4,5,6,7,8};\n    subset_zeta_transform(f, 3);\n    for (int i = 0; i &lt; 8; i++) printf(\"%d \", f[i]);\n    return 0;\n}\n\n\nWhy It Matters\n\nCentral in subset DP, SOS DP, and bitmask convolutions.\nUsed for fast inclusion–exclusion and Möbius inversion.\nAppears in counting problems, graph subset enumeration, and Boolean function transforms.\nWorks hand-in-hand with the Möbius Transform to invert accumulated results.\n\n\n\nA Gentle Proof\nEach iteration over bit \\(i\\) ensures every subset without bit \\(i\\) contributes to the superset with bit \\(i\\). By induction, all subsets \\(T\\subseteq S\\) accumulate into \\(F(S)\\).\n\n\nTry It Yourself\n\nCompute Zeta transform manually for \\(n=3\\).\nVerify inversion using Möbius inversion (next section).\nModify code for superset version (flip the condition).\nImplement modulo arithmetic (e.g., \\(\\bmod 10^9+7\\)).\nUse it to compute number of subsets with a given property.\n\n\n\nTest Cases\n\n\n\n\\(f(S)\\)\nSubset\n\\(F(S)\\)\n\n\n\n\n[1,2,3,4]\n2 bits\n[1,3,4,10]\n\n\n[0,1,1,0,1,0,0,0]\n3 bits\n[0,1,1,2,1,2,2,4]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n2^n)\\)\nSpace: \\(O(2^n)\\)\n\nThe Zeta Transform is the summation heart of subset DP, a way to see all parts of a set at once, elegantly and efficiently.\n\n\n\n600 Möbius Inversion\nThe Möbius Inversion is the mathematical inverse of the Zeta Transform. It allows us to recover a function \\(f(S)\\) from its accumulated form \\(F(S)\\) where \\[F(S)=\\sum_{T\\subseteq S}f(T).\\] It is a cornerstone in combinatorics, number theory, and subset dynamic programming.\n\nWhat Problem Are We Solving?\nSuppose we know \\(F(S)\\), the total contribution from all subsets \\(T\\subseteq S\\). We want to invert this accumulation and get back the original \\(f(S)\\).\nThe inversion formula is:\n\\[\nf(S)=\\sum_{T\\subseteq S}(-1)^{|S\\setminus T|}F(T).\n\\]\nThis is the combinatorial analog of differentiation for sums over subsets.\n\n\nKey Idea\nMöbius inversion reverses the cumulative effect of the subset Zeta transform. In iterative (bitwise) form, we can perform it efficiently by “subtracting contributions” instead of adding them.\nFor each bit \\(i\\) from \\(0\\) to \\(n-1\\):\nIf bit \\(i\\) is set in \\(S\\), subtract \\(f[S\\setminus{i}]\\) from \\(f[S]\\):\n\\[\nf[S]-=f[S\\setminus{i}].\n\\]\nThis undoes the inclusion steps done in the Zeta transform.\n\n\nExample\nLet’s start from \\[\nF(S)=\\sum_{T\\subseteq S}f(T)\n\\] and we know:\n\n\n\nMask\nSubset\n\\(F(S)\\)\n\n\n\n\n00\n∅\n1\n\n\n01\n{0}\n3\n\n\n10\n{1}\n4\n\n\n11\n{0,1}\n10\n\n\n\nWe apply Möbius inversion:\n\n\n\n\n\n\n\n\n\nStep\nSubset\nComputation\nResult\n\n\n\n\n∅\n,\n\\(f(∅)=F(∅)=1\\)\n1\n\n\n{0}\n\\(f(01)=F(01)-F(00)=3-1\\)\n2\n\n\n\n{1}\n\\(f(10)=F(10)-F(00)=4-1\\)\n3\n\n\n\n{0,1}\n\\(f(11)=F(11)-F(01)-F(10)+F(00)=10-3-4+1\\)\n4\n\n\n\n\nSo we recovered \\(f=[1,2,3,4]\\).\n\n\nTiny Code\nPython\ndef mobius_inversion(F):\n    n = len(F).bit_length() - 1\n    f = F[:]\n    for i in range(n):\n        for mask in range(1 &lt;&lt; n):\n            if mask & (1 &lt;&lt; i):\n                f[mask] -= f[mask ^ (1 &lt;&lt; i)]\n    return f\n\n# Example\nF = [1,3,4,10]\nprint(mobius_inversion(F))  # [1,2,3,4]\nC\n#include &lt;stdio.h&gt;\n\nvoid mobius_inversion(int *F, int n) {\n    for (int i = 0; i &lt; n; i++) {\n        for (int mask = 0; mask &lt; (1 &lt;&lt; n); mask++) {\n            if (mask & (1 &lt;&lt; i))\n                F[mask] -= F[mask ^ (1 &lt;&lt; i)];\n        }\n    }\n}\n\nint main(void) {\n    int F[4] = {1,3,4,10};\n    mobius_inversion(F, 2);\n    for (int i = 0; i &lt; 4; i++) printf(\"%d \", F[i]);\n    return 0;\n}\n\n\nWhy It Matters\n\nThe backbone of inclusion–exclusion principle and subset DPs.\nUsed in number-theoretic inversions such as the classic integer Möbius function μ(n).\nPairs perfectly with Zeta transform to toggle between cumulative and pointwise representations.\nAppears in fast subset convolution, polynomial transforms, and combinatorial counting.\n\n\n\nA Gentle Proof\nFrom \\[\nF(S)=\\sum_{T\\subseteq S}f(T),\n\\] we can see the system as a triangular matrix with \\(1\\)s for \\(T\\subseteq S\\). Its inverse has entries \\((-1)^{|S\\setminus T|}\\) for \\(T\\subseteq S\\), giving\n\\[\nf(S)=\\sum_{T\\subseteq S}(-1)^{|S\\setminus T|}F(T).\n\\]\nThus Möbius inversion exactly cancels the overcounting in the Zeta transform.\n\n\nTry It Yourself\n\nStart with \\(f=[1,2,3,4]\\) and compute its \\(F\\) using the Zeta transform.\nApply Möbius inversion to get \\(f\\) back.\nExtend to 3-bit subsets (\\(n=3\\)).\nUse it to compute inclusion–exclusion counts.\nModify to work modulo \\(10^9+7\\).\n\n\n\nTest Cases\n\n\n\n\\(F(S)\\)\nExpected \\(f(S)\\)\n\n\n\n\n[1,3,4,10]\n[1,2,3,4]\n\n\n[0,1,1,2]\n[0,1,1,0]\n\n\n[2,4,6,12,8,16,20,40]\n[2,2,2,4,2,4,4,8]\n\n\n\n\n\nComplexity\n\nTime: \\(O(n2^n)\\)\nSpace: \\(O(2^n)\\)\n\nMöbius inversion closes the circle of combinatorial transforms, the mirror image of the Zeta transform, turning sums back into their sources.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 6. Mathematics for Algorithms</span>"
    ]
  }
]