[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Algorithms",
    "section": "",
    "text": "Roadmap\nThe Little Book of Algorithms is a multi-volume project. Each volume has a clear sequence of chapters, and each chapter has three levels of depth (L0 beginner intuition, L1 practical techniques, L2 advanced systems/theory). This roadmap outlines the plan for development and publication.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "The Little Book of Algorithms",
    "section": "Goals",
    "text": "Goals\n\nEstablish a consistent layered structure across all chapters.\nProvide runnable implementations in Python, C, Go, Erlang, and Lean.\nEnsure Quarto build supports HTML, PDF, EPUB, and LaTeX.\nDeliver both pedagogy (L0) and production insights (L2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#volumes",
    "href": "index.html#volumes",
    "title": "The Little Book of Algorithms",
    "section": "Volumes",
    "text": "Volumes\n\nVolume I - Structures Linéaires\n\nChapter 0 - Foundations\nChapter 1 - Numbers\nChapter 2 - Arrays\nChapter 3 - Strings\nChapter 4 - Linked Lists\nChapter 5 - Stacks & Queues\n\n\n\nVolume II - Algorithmes Fondamentaux\n\nChapter 6 - Searching\nChapter 7 - Selection\nChapter 8 - Sorting\nChapter 9 - Amortized Analysis\n\n\n\nVolume III - Structures Hiérarchiques\n\nChapter 10 - Tree Fundamentals\nChapter 11 - Heaps & Priority Queues\nChapter 12 - Binary Search Trees\nChapter 13 - Balanced Trees & Ordered Maps\nChapter 14 - Range Queries\nChapter 15 - Vector Databases\n\n\n\nVolume IV - Paradigmes Algorithmiques\n\nChapter 16 - Divide-and-Conquer\nChapter 17 - Greedy\nChapter 18 - Dynamic Programming\nChapter 19 - Backtracking & Search\n\n\n\nVolume V - Graphes et Complexité\n\nChapter 20 - Graph Basics\nChapter 21 - DAGs & SCC\nChapter 22 - Shortest Paths\nChapter 23 - Flows & Matchings\nChapter 24 - Tree Algorithms\nChapter 25 - Complexity & Limits\nChapter 26 - External & Cache-Oblivious\nChapter 27 - Probabilistic & Streaming\nChapter 28 - Engineering",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#milestones",
    "href": "index.html#milestones",
    "title": "The Little Book of Algorithms",
    "section": "Milestones",
    "text": "Milestones\n\nComplete detailed outlines for all chapters (L0, L1, L2).\nWrite draft text for all L0 sections (intuition, analogies, simple examples).\nExpand each chapter with L1 content (implementations, correctness arguments, exercises).\nAdd L2 content (systems insights, proofs, optimizations, advanced references).\nDevelop and test runnable code in src/ across Python, C, Go, Erlang, and Lean.\nIntegrate diagrams, figures, and visual explanations.\nFinalize Quarto build setup for HTML, PDF, and EPUB.\nRelease first public edition (HTML + PDF).\nAdd LaTeX build, refine EPUB, and polish cross-references.\nPublish on GitHub Pages and archive DOI.\nGather feedback, refine explanations, and expand exercises/problem sets.\nLong-term: maintain as a living reference with continuous updates and companion volumes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#deliverables",
    "href": "index.html#deliverables",
    "title": "The Little Book of Algorithms",
    "section": "Deliverables",
    "text": "Deliverables\n\nQuarto project with 29 chapters (00–28).\nMulti-language reference implementations.\nLearning matrix in README for navigation.\nROADMAP.md (this file) to track progress.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#long-term-vision",
    "href": "index.html#long-term-vision",
    "title": "The Little Book of Algorithms",
    "section": "Long-term Vision",
    "text": "Long-term Vision\n\nMaintain the repository as a living reference.\nExtend with exercises, problem sets, and quizzes.\nBuild a dependency map across volumes for prerequisites.\nConnect to companion “Little Book” series (linear algebra, calculus, probability).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "Chapter 1. Numbers",
    "section": "",
    "text": "1.1 Representation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#representation",
    "href": "chapter_1.html#representation",
    "title": "Chapter 1. Numbers",
    "section": "",
    "text": "1.1 L0. Decimal and Binary Basics\nA number representation is a way of writing numbers using symbols and positional rules. Humans typically use decimal notation, while computers rely on binary because it aligns with the two-state nature of electronic circuits. Understanding both systems is the first step in connecting mathematical intuition with machine computation.\n\nNumbers in Everyday Life\nHumans work with the decimal system (base 10), which uses digits 0 through 9. Each position in a number has a place value that is a power of 10.\n\\[\n427 = 4 \\times 10^2 + 2 \\times 10^1 + 7 \\times 10^0\n\\]\nThis principle of positional notation is the same idea used in other bases.\n\n\nNumbers in Computers\nComputers, however, operate in binary (base 2). A binary digit (bit) can only be 0 or 1, matching the two stable states of electronic circuits (off/on). Each binary place value represents a power of 2.\n\\[\n1011_2 = 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0 = 11_{10}\n\\]\nJust like in decimal where \\(9 + 1 = 10\\), in binary \\(1 + 1 = 10_2\\).\n\n\nConversion Between Decimal and Binary\nTo convert from decimal to binary, repeatedly divide the number by 2 and record the remainders. Then read the remainders from bottom to top.\nExample: Convert \\(42_{10}\\) into binary.\n\n42 ÷ 2 = 21 remainder 0\n21 ÷ 2 = 10 remainder 1\n10 ÷ 2 = 5 remainder 0\n5 ÷ 2 = 2 remainder 1\n2 ÷ 2 = 1 remainder 0\n1 ÷ 2 = 0 remainder 1\n\nReading upward: \\(101010_2\\).\nTo convert from binary to decimal, expand into powers of 2 and sum:\n\\[\n101010_2 = 1 \\times 2^5 + 0 \\times 2^4 + 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 0 \\times 2^0 = 42_{10}\n\\]\n\n\nWorked Example (Python)\nn = 42\nprint(\"Decimal:\", n)\nprint(\"Binary :\", bin(n))   # 0b101010\n\n# binary literal in Python\nb = 0b101010\nprint(\"Binary literal:\", b)\n\n# converting binary string to decimal\nprint(\"From binary '1011':\", int(\"1011\", 2))\nOutput:\nDecimal: 42\nBinary : 0b101010\nBinary literal: 42\nFrom binary '1011': 11\n\n\nWhy It Matters\n\nAll information inside a computer — numbers, text, images, programs — reduces to binary representation.\nDecimal and binary conversions are the first bridge between human-friendly math and machine-level data.\nUnderstanding binary is essential for debugging, low-level programming, and algorithms that depend on bit operations.\n\n\n\nExercises\n\nWrite the decimal number 19 in binary.\nConvert the binary number 10101₂ into decimal.\nShow the repeated division steps to convert 27 into binary.\nVerify in Python that 0b111111 equals 63.\nExplain why computers use binary instead of decimal.\n\n\n\n\n1.1 L1. Beyond Binary: Octal, Hex, and Two’s Complement\nNumbers are not always written in base-10 or even in base-2. For efficiency and compactness, programmers often use octal (base-8) and hexadecimal (base-16). At the same time, negative numbers must be represented reliably; modern computers use two’s complement for this purpose.\n\nOctal and Hexadecimal\nOctal and hex are simply alternate numeral systems.\n\nOctal (base 8): digits 0–7.\nHexadecimal (base 16): digits 0–9 plus A–F.\n\nWhy they matter:\n\nHex is concise: one hex digit = 4 binary bits.\nOctal was historically convenient: one octal digit = 3 binary bits (useful on early 12-, 24-, or 36-bit machines).\n\nFor example, the number 42 is written as:\n\n\n\nDecimal\nBinary\nOctal\nHex\n\n\n\n\n42\n101010\n52\n2A\n\n\n\n\n\nTwo’s Complement\nTo represent negative numbers, we cannot just “stick a minus sign” in memory. Instead, binary uses two’s complement:\n\nChoose a fixed bit-width (say 8 bits).\nFor a negative number -x, compute 2^bits - x.\nStore the result as an ordinary binary integer.\n\nExample with 8 bits:\n\n+5 → 00000101\n-5 → 11111011\n-1 → 11111111\n\nWhy two’s complement is powerful:\n\nAddition and subtraction “just work” with the same circuitry for signed and unsigned.\nThere is only one representation of zero.\n\n\n\nWorking Example (Python)\n# Decimal 42 in different bases\nn = 42\nprint(\"Decimal:\", n)\nprint(\"Binary :\", bin(n))\nprint(\"Octal  :\", oct(n))\nprint(\"Hex    :\", hex(n))\n\n# Two's complement for -5 in 8 bits\ndef to_twos_complement(x: int, bits: int = 8) -&gt; str:\n    if x &gt;= 0:\n        return format(x, f\"0{bits}b\")\n    return format((1 &lt;&lt; bits) + x, f\"0{bits}b\")\n\nprint(\"+5:\", to_twos_complement(5, 8))\nprint(\"-5:\", to_twos_complement(-5, 8))\nOutput:\nDecimal: 42\nBinary : 0b101010\nOctal  : 0o52\nHex    : 0x2a\n+5: 00000101\n-5: 11111011\n\n\nWhy It Matters\n\nProgrammer convenience: Hex makes binary compact and human-readable.\nHardware design: Two’s complement ensures arithmetic circuits are simple and unified.\nDebugging: Memory dumps, CPU registers, and network packets are usually shown in hex.\n\n\n\nExercises\n\nConvert 100 into binary, octal, and hex.\nWrite -7 in 8-bit two’s complement.\nVerify that 0xFF is equal to 255.\nParse the bitstring \"11111001\" as an 8-bit two’s complement number.\nExplain why engineers prefer two’s complement over “sign-magnitude” representation.\n\n\n\n\n1.1 L2. Floating-Point and Precision Issues\nNot all numbers are integers. To approximate fractions, scientific notation, and very large or very small values, computers use floating-point representation. The de-facto standard is IEEE-754, which defines how real numbers are encoded, how special values are handled, and what precision guarantees exist.\n\nStructure of Floating-Point Numbers\nA floating-point value is composed of three fields:\n\nSign bit (s) — indicates positive (0) or negative (1).\nExponent (e) — determines the scale or “magnitude.”\nMantissa / significand (m) — contains the significant digits.\n\nThe value is interpreted as:\n\\[\n(-1)^s \\times 1.m \\times 2^{(e - \\text{bias})}\n\\]\nExample: IEEE-754 single precision (32 bits)\n\n1 sign bit\n8 exponent bits (bias = 127)\n23 mantissa bits\n\n\n\nExact vs Approximate Representation\nSome numbers are represented exactly:\n\n1.0 has a clean binary form.\n\nOthers cannot be represented precisely:\n\n0.1 in decimal is a repeating fraction in binary, so the closest approximation is stored.\n\nPython example:\na = 0.1 + 0.2\nprint(\"0.1 + 0.2 =\", a)\nprint(\"Equal to 0.3?\", a == 0.3)\nOutput:\n0.1 + 0.2 = 0.30000000000000004\nEqual to 0.3? False\n\n\nSpecial Values\nIEEE-754 reserves encodings for special cases:\n\n\n\nSign\nExponent\nMantissa\nMeaning\n\n\n\n\n0/1\nall 1s\n0\n+∞ / −∞\n\n\n0/1\nall 1s\nnonzero\nNaN (Not a Number)\n\n\n0/1\nall 0s\nnonzero\nDenormals (gradual underflow)\n\n\n\nExamples:\n\nDivision by zero produces infinity: 1.0 / 0.0 = inf.\n0.0 / 0.0 yields NaN, which propagates in computations.\nDenormals allow gradual precision near zero.\n\n\n\nArbitrary Precision\nLanguages like Python and libraries like GMP provide arbitrary-precision arithmetic:\n\nIntegers (int) can grow as large as memory allows.\nDecimal libraries (decimal.Decimal in Python) allow exact decimal arithmetic.\nThese are slower, but essential for cryptography, symbolic computation, and finance.\n\n\n\nWorked Example (Python)\nimport math\n\nprint(\"Infinity:\", 1.0 / 0.0)\nprint(\"NaN:\", 0.0 / 0.0)\n\nprint(\"Is NaN?\", math.isnan(float('nan')))\nprint(\"Is Inf?\", math.isinf(float('inf')))\n\n# Arbitrary precision integer\nbig = 2200\nprint(\"2200 =\", big)\n\n\nWhy It Matters\n\nRounding surprises: Many decimal fractions cannot be represented exactly.\nError propagation: Repeated arithmetic may accumulate tiny inaccuracies.\nSpecial values: NaN and infinity must be handled carefully.\nDomain correctness: Cryptography, finance, and symbolic algebra require exact precision.\n\n\n\nExercises\n\nWrite down the IEEE-754 representation (sign, exponent, mantissa) of 1.0.\nExplain why 0.1 is not exactly representable in binary.\nTest in Python whether float('nan') == float('nan'). What happens, and why?\nFind the smallest positive number you can add to 1.0 before it changes (machine epsilon).\nWhy is arbitrary precision slower but critical in some applications?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#basic-operations",
    "href": "chapter_1.html#basic-operations",
    "title": "Chapter 1. Numbers",
    "section": "1.2 Basic Operations",
    "text": "1.2 Basic Operations\n\n1.2 L0. Addition, Subtraction, Multiplication, Division\nAn arithmetic operation combines numbers to produce a new number. At this level we focus on four basics: addition, subtraction, multiplication, and division—first with decimal intuition, then a peek at how the same ideas look in binary. Mastering these is essential before moving to algorithms that build on them.\n\nIntuition: place value + carrying/borrowing\nAll four operations are versions of combining place values (ones, tens, hundreds …; or in binary: ones, twos, fours …).\n\nAddition: add column by column; if a column exceeds the base, carry 1 to the next column.\nSubtraction: subtract column by column; if a column is too small, borrow 1 from the next column.\nMultiplication: repeated addition; multiply by each digit and shift (place value), then add partial results.\nDivision: repeated subtraction or sharing; find how many times a number “fits,” track the remainder.\n\nThese rules are identical in any base. Only the place values change.\n\n\nDecimal examples (by hand)\n\nAddition (carry)\n\n   478\n + 259\n ----\n   737    (8+9=17 → write 7, carry 1; 7+5+1=13 → write 3, carry 1; 4+2+1=7)\n\nSubtraction (borrow)\n\n   503\n -  78\n ----\n   425    (3-8 borrow → 13-8=5; 0 became -1 so borrow from 5 → 9-7=2; 4 stays 4)\n\nMultiplication (partial sums)\n\n   214\n ×   3\n ----\n   642    (214*3 = 642)\n\nLong division (quotient + remainder)\n\n  47 ÷ 5 → 9 remainder 2   (because 5*9 = 45, leftover 2)\n\n\nBinary peek (same rules, base 2)\n\nAdd rules: 0+0=0, 0+1=1, 1+0=1, 1+1=10₂ (write 0, carry 1)\nSubtract rules: 0−0=0, 1−0=1, 1−1=0, 0−1 → borrow (becomes 10₂−1=1, borrow 1)\n\nExample: \\(1011₂ + 0110₂\\)\n   1011\n + 0110\n ------\n  10001   (1+0=1; 1+1=0 carry1; 0+1+carry=0 carry1; 1+0+carry=0 carry1 → carry out)\n\n\nWorked examples (Python)\n# Basic arithmetic with integers\na, b = 478, 259\nprint(\"a+b =\", a + b)      # 737\nprint(\"a-b =\", a - b)      # 219\nprint(\"a*b =\", a * b)      # 123,  478*259 = 123,  ... actually compute:\nprint(\"47//5 =\", 47 // 5)  # integer division -&gt; 9\nprint(\"47%5  =\", 47 % 5)   # remainder -&gt; 2\n\n# Show carry/borrow intuition using binary strings\nx, y = 0b1011, 0b0110\ns = x + y\nprint(\"x+y (binary):\", bin(x), \"+\", bin(y), \"=\", bin(s))\n\n# Small helper: manual long division that returns (quotient, remainder)\ndef long_divide(n: int, d: int):\n    if d == 0:\n        raise ZeroDivisionError(\"division by zero\")\n    q = n // d\n    r = n % d\n    return q, r\n\nprint(\"long_divide(47,5):\", long_divide(47, 5))  # (9, 2)\n\nNote: // is integer division in Python; % is the remainder. For now we focus on integers (no decimals).\n\n\n\nWhy it matters\n\nEvery higher-level algorithm (searching, hashing, cryptography, numeric methods) relies on these operations.\nUnderstanding carry/borrow makes binary arithmetic and bit-level reasoning feel natural.\nKnowing integer division and remainder is vital for base conversions, hashing (mod), and many algorithmic patterns.\n\n\n\nExercises\n\nCompute by hand, then verify in Python:\n\n\\(326 + 589\\)\n\\(704 - 259\\)\n\\(38 \\times 12\\)\n\\(123 \\div 7\\) (give quotient and remainder)\n\nIn binary, add \\(10101₂ + 111₍₂₎\\). Show carries.\nWrite a short Python snippet that prints the quotient and remainder for n=200 divided by d=23.\nConvert your remainder into a sentence: “200 = 23 × (quotient) + (remainder)”.\nChallenge: Multiply \\(19 \\times 23\\) by hand using partial sums; then check with Python.\n\n\n\n\n1.2 L1. Division, Modulo, and Efficiency\nBeyond the simple four arithmetic operations, programmers need to think about division with remainder, the modulo operator, and how efficient these operations are on real machines. Addition and subtraction are almost always “constant time,” but division can be slower, and understanding modulo is essential for algorithms like hashing, cryptography, and scheduling.\n\nInteger Division and Modulo\nFor integers, division produces both a quotient and a remainder.\n\nMathematical definition: for integers \\(n, d\\) with \\(d \\neq 0\\),\n\\[\nn = d \\times q + r, \\quad 0 \\leq r &lt; |d|\n\\]\nwhere \\(q\\) is the quotient, \\(r\\) the remainder.\nProgramming notation (Python):\n\nn // d → quotient\nn % d → remainder\n\n\nExamples:\n\n47 // 5 = 9, 47 % 5 = 2 because \\(47 = 5 \\times 9 + 2\\).\n23 // 7 = 3, 23 % 7 = 2 because \\(23 = 7 \\times 3 + 2\\).\n\n\n\n\nn\nd\nn // d\nn % d\n\n\n\n\n47\n5\n9\n2\n\n\n23\n7\n3\n2\n\n\n100\n9\n11\n1\n\n\n\n\n\nModulo in Algorithms\nThe modulo operation is a workhorse in programming:\n\nHashing: To map a large integer into a table of size m, use key % m.\nCyclic behavior: To loop back after 7 days in a week: (day + shift) % 7.\nCryptography: Modular arithmetic underlies RSA, Diffie–Hellman, and many number-theoretic algorithms.\n\n\n\nEfficiency Considerations\n\nAddition and subtraction: generally 1 CPU cycle.\nMultiplication: slightly more expensive, but still fast on modern hardware.\nDivision and modulo: slower, often an order of magnitude more costly than multiplication.\n\nPractical tricks:\n\nIf d is a power of two, n % d can be computed by a bitmask.\n\nExample: n % 8 == n & 7 (since 8 = 2³).\n\nSome compilers automatically optimize modulo when the divisor is constant.\n\n\n\nWorked Example (Python)\n# Quotient and remainder\nn, d = 47, 5\nprint(\"Quotient:\", n // d)  # 9\nprint(\"Remainder:\", n % d)  # 2\n\n# Identity check: n == d*q + r\nq, r = divmod(n, d)  # built-in tuple return\nprint(\"Check:\", d*q + r == n)\n\n# Modulo for cyclic behavior: days of week\ndays = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nstart = 5  # Saturday\nshift = 4\nfuture_day = days[(start + shift) % 7]\nprint(\"Start Saturday + 4 days =\", future_day)\n\n# Optimization: power-of-two modulo with bitmask\nfor n in [5, 12, 20]:\n    print(f\"{n} % 8 = {n % 8}, bitmask {n & 7}\")\nOutput:\nQuotient: 9\nRemainder: 2\nCheck: True\nStart Saturday + 4 days = Wed\n5 % 8 = 5, bitmask 5\n12 % 8 = 4, bitmask 4\n20 % 8 = 4, bitmask 4\n\n\nWhy It Matters\n\nReal programs rely heavily on modulo for indexing, hashing, and wrap-around logic.\nDivision is computationally more expensive; knowing when to replace it with bit-level operations improves performance.\nModular arithmetic introduces a new “world” where numbers wrap around — the foundation of many advanced algorithms.\n\n\n\nExercises\n\nCompute by hand and confirm in Python:\n\n100 // 9 and 100 % 9\n123 // 11 and 123 % 11\n\nWrite a function that simulates a clock: given hour and shift, return the new hour (24-hour cycle).\nProve the identity: for any integers n and d,\nn == d * (n // d) + (n % d)\nby trying with random values.\nShow how to replace n % 16 with a bitwise operation. Why does it work?\nChallenge: Write a short Python function to check if a number is divisible by 7 using only % and //.\n\n\n\n\n1.2 L2. Fast Arithmetic Algorithms\nWhen numbers grow large, the naïve methods for multiplication and division become too slow. On paper, long multiplication takes \\(O(n^2)\\) steps for \\(n\\)-digit numbers. Computers face the same issue: multiplying two very large integers digit by digit can be expensive. Fast arithmetic algorithms reduce this cost, using clever divide-and-conquer techniques or transformations into other domains.\n\nMultiplication Beyond the School Method\nNaïve long multiplication\n\nTreats an \\(n\\)-digit number as a sequence of digits.\nEach digit of one number multiplies every digit of the other.\nComplexity: \\(O(n^2)\\).\nWorks fine for small integers, but too slow for cryptography or big-number libraries.\n\nKaratsuba’s Algorithm\n\nDiscovered in 1960 by Anatoly Karatsuba.\nIdea: split numbers into halves and reduce multiplications.\nComplexity: \\(O(n^{\\log_2 3}) \\approx O(n^{1.585})\\).\nRecursive strategy:\n\nFor numbers \\(x = x_1 \\cdot B^m + x_0\\), \\(y = y_1 \\cdot B^m + y_0\\).\nCompute 3 multiplications instead of 4:\n\n\\(z_0 = x_0 y_0\\)\n\\(z_2 = x_1 y_1\\)\n\\(z_1 = (x_0+x_1)(y_0+y_1) - z_0 - z_2\\)\n\nResult: \\(z_2 \\cdot B^{2m} + z_1 \\cdot B^m + z_0\\).\n\n\nFFT-based Multiplication (Schönhage–Strassen and successors)\n\nRepresent numbers as polynomials of their digits.\nMultiply polynomials efficiently using Fast Fourier Transform.\nComplexity: near \\(O(n \\log n)\\).\nUsed in modern big-integer libraries (e.g. GNU MP, Java’s BigInteger).\n\n\n\nDivision Beyond Long Division\n\nNaïve long division: \\(O(n^2)\\) for \\(n\\)-digit dividend.\nNewton’s method for reciprocal: approximate \\(1/d\\) using Newton–Raphson iterations, then multiply by \\(n\\).\nComplexity: tied to multiplication — if multiplication is fast, so is division.\n\n\n\nModular Exponentiation\nFast arithmetic also matters in modular contexts (cryptography).\n\nCompute \\(a^b \\bmod m\\) efficiently.\nSquare-and-multiply (binary exponentiation):\n\nWrite \\(b\\) in binary.\nFor each bit: square result, multiply if bit=1.\nComplexity: \\(O(\\log b)\\) multiplications.\n\n\n\n\nWorked Example (Python)\n# Naïve multiplication\ndef naive_mul(x: int, y: int) -&gt; int:\n    return x * y  # Python already uses fast methods internally\n\n# Karatsuba multiplication (recursive, simplified)\ndef karatsuba(x: int, y: int) -&gt; int:\n    # base case\n    if x &lt; 10 or y &lt; 10:\n        return x * y\n    # split numbers\n    n = max(x.bit_length(), y.bit_length())\n    m = n // 2\n    high1, low1 = divmod(x, 1 &lt;&lt; m)\n    high2, low2 = divmod(y, 1 &lt;&lt; m)\n    z0 = karatsuba(low1, low2)\n    z2 = karatsuba(high1, high2)\n    z1 = karatsuba(low1 + high1, low2 + high2) - z0 - z2\n    return (z2 &lt;&lt; (2*m)) + (z1 &lt;&lt; m) + z0\n\n# Modular exponentiation (square-and-multiply)\ndef modexp(a: int, b: int, m: int) -&gt; int:\n    result = 1\n    base = a % m\n    exp = b\n    while exp &gt; 0:\n        if exp & 1:\n            result = (result * base) % m\n        base = (base * base) % m\n        exp &gt;&gt;= 1\n    return result\n\n# Demo\nprint(\"Karatsuba(1234, 5678) =\", karatsuba(1234, 5678))\nprint(\"pow(7, 128, 13) =\", modexp(7, 128, 13))  # fast modular exponentiation\nOutput:\nKaratsuba(1234, 5678) = 7006652\npow(7, 128, 13) = 3\n\n\nWhy It Matters\n\nCryptography: RSA requires multiplying and dividing integers with thousands of digits.\nComputer algebra systems: symbolic computation depends on fast polynomial/integer arithmetic.\nBig data / simulation: arbitrary precision needed when floats are not exact.\n\n\n\nExercises\n\nMultiply 31415926 × 27182818 using:\n\nPython’s *\nYour Karatsuba implementation. Compare results.\n\nImplement modexp(a, b, m) for \\(a=5, b=117, m=19\\). Confirm with Python’s built-in pow(a, b, m).\nExplain why Newton’s method for division depends on fast multiplication.\nResearch: what is the current fastest known multiplication algorithm for large integers?\nChallenge: Modify Karatsuba to print intermediate z0, z1, z2 values for small inputs to visualize the recursion.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#properties",
    "href": "chapter_1.html#properties",
    "title": "Chapter 1. Numbers",
    "section": "1.3 Properties",
    "text": "1.3 Properties\n\n1.3 L0 — Simple Number Properties\nNumbers have patterns that help us reason about algorithms without heavy mathematics. At this level we focus on basic properties: even vs odd, divisibility, and remainders. These ideas show up everywhere—from loop counters to data structure layouts.\n\nEven and Odd\nA number is even if it ends with digit 0, 2, 4, 6, or 8 in decimal, and odd otherwise.\n\nIn binary, checking parity is even easier: the last bit tells the story.\n\n…0 → even\n…1 → odd\n\n\nExample in Python:\ndef is_even(n: int) -&gt; bool:\n    return n % 2 == 0\n\nprint(is_even(10))  # True\nprint(is_even(7))   # False\n\n\nDivisibility\nWe often ask: does one number divide another?\n\na is divisible by b if there exists some integer k with a = b * k.\nIn code: a % b == 0.\n\nExamples:\n\n12 is divisible by 3 → 12 % 3 == 0.\n14 is not divisible by 5 → 14 % 5 == 4.\n\n\n\nRemainders and Modular Thinking\nWhen dividing, the remainder is what’s left over.\n\nExample: 17 // 5 = 3, remainder 2.\nModular arithmetic wraps around like a clock:\n\n(17 % 5) = 2 → same as “2 o’clock after going 17 steps around a 5-hour clock.”\n\n\nThis “wrap-around” view is central in array indexing, hashing, and cryptography later on.\n\n\nWhy It Matters\n\nAlgorithms: Parity checks decide branching (e.g., even-odd optimizations).\nData structures: Array indices often wrap around using %.\nEveryday: Calendars cycle days of the week; remainders formalize that.\n\n\n\nExercises\n\nWrite a function that returns \"even\" or \"odd\" for a given number.\nCheck if 91 is divisible by 7.\nCompute the remainder of 100 divided by 9.\nUse % to simulate a 7-day week: if today is day 5 (Saturday) and you add 10 days, what day is it?\nFind the last digit of 2^15 without computing the full number (hint: check the remainder mod 10).\n\n\n\n\n1.3 L1 — Classical Number Theory Tools\nBeyond simple parity and divisibility, algorithms often need deeper number properties. At this level we introduce a few “toolkit” ideas from elementary number theory: greatest common divisor (GCD), least common multiple (LCM), and modular arithmetic identities. These are lightweight but powerful concepts that show up in algorithm design, cryptography, and optimization.\n\nGreatest Common Divisor (GCD)\nThe GCD of two numbers is the largest number that divides both.\n\nExample: gcd(20, 14) = 2.\nWhy useful: GCD simplifies fractions, ensures ratios are reduced, and appears in algorithm correctness proofs.\n\nEuclid’s Algorithm: Instead of trial division, we can compute GCD quickly:\ngcd(a, b) = gcd(b, a % b)\nThis repeats until b = 0, at which point a is the answer.\nPython example:\ndef gcd(a: int, b: int) -&gt; int:\n    while b:\n        a, b = b, a % b\n    return a\n\nprint(gcd(20, 14))  # 2\n\n\nLeast Common Multiple (LCM)\nThe LCM of two numbers is the smallest positive number divisible by both.\n\nExample: lcm(12, 18) = 36.\nConnection to GCD:\nlcm(a, b) = (a * b) // gcd(a, b)\n\nThis is useful in scheduling, periodic tasks, and synchronization problems.\n\n\nModular Arithmetic Identities\nRemainders behave predictably under operations:\n\nAddition: (a + b) % m = ((a % m) + (b % m)) % m\nMultiplication: (a * b) % m = ((a % m) * (b % m)) % m\n\nExample:\n\n(123 + 456) % 7 = (123 % 7 + 456 % 7) % 7\nThis property lets us work with small remainders instead of huge numbers, key in cryptography and hashing.\n\n\n\nWhy It Matters\n\nAlgorithms: GCD ensures efficiency in fraction reduction, graph algorithms, and number-theoretic algorithms.\nSystems: LCM models periodicity, e.g., aligning CPU scheduling intervals.\nCryptography: Modular arithmetic underpins secure communication (RSA, Diffie-Hellman).\nPractical programming: Modular identities simplify computations with limited ranges (hash tables, cyclic arrays).\n\n\n\nExercises\n\nCompute gcd(252, 198) by hand using Euclid’s algorithm.\nWrite a function that returns the LCM of two numbers. Test it on (12, 18).\nShow that (37 + 85) % 12 equals ((37 % 12) + (85 % 12)) % 12.\nReduce the fraction 84/126 using GCD.\nFind the smallest day d such that d is a multiple of both 12 and 18 (hint: LCM).\n\n\n\n\n1.3 L2 — Advanced Number Theory in Algorithms\nAt this level, we move beyond everyday divisibility and Euclid’s algorithm. Modern algorithms frequently rely on deep number theory to achieve efficiency. Topics such as modular inverses, Euler’s totient function, and primality tests are crucial foundations for cryptography, randomized algorithms, and competitive programming.\n\nModular Inverses\nThe modular inverse of a number a (mod m) is an integer x such that:\n(a * x) % m = 1\n\nExample: the inverse of 3 modulo 7 is 5, because (3*5) % 7 = 15 % 7 = 1.\nExistence: an inverse exists if and only if gcd(a, m) = 1.\nComputation: using the Extended Euclidean Algorithm.\n\nThis is the backbone of modular division and is heavily used in cryptography (RSA), hash functions, and matrix inverses mod p.\n\n\nEuler’s Totient Function (φ)\nThe function φ(n) counts the number of integers between 1 and n that are coprime to n.\n\nExample: φ(9) = 6 because {1, 2, 4, 5, 7, 8} are coprime to 9.\nKey property (Euler’s theorem):\na^φ(n) ≡ 1 (mod n)     if gcd(a, n) = 1\nSpecial case: Fermat’s Little Theorem — for prime p,\na^(p-1) ≡ 1 (mod p)\n\nThis result is central in modular exponentiation and cryptosystems like RSA.\n\n\nPrimality Testing\nDetermining if a number is prime is easy for small inputs but hard for large ones. Efficient algorithms are essential:\n\nTrial division: works only for small n.\nFermat primality test: uses Fermat’s Little Theorem to detect composites, but can be fooled by Carmichael numbers.\nMiller–Rabin test: a probabilistic algorithm widely used in practice (cryptographic key generation).\nAKS primality test: a deterministic polynomial-time method (theoretical importance).\n\nExample intuition:\n\nFor large n, we don’t check all divisors; we test properties of a^k mod n for random bases a.\n\n\n\nWhy It Matters\n\nCryptography: Public-key systems depend on modular inverses, Euler’s theorem, and large primes.\nAlgorithms: Modular inverses simplify solving equations in modular arithmetic (e.g., Chinese Remainder Theorem applications).\nPractical Computing: Randomized primality tests (like Miller–Rabin) balance correctness and efficiency in real-world systems.\n\n\n\nExercises\n\nFind the modular inverse of 7 modulo 13.\nCompute φ(10) and verify Euler’s theorem for a = 3.\nUse Fermat’s test to check whether 341 is prime. (Hint: try a = 2.)\nImplement modular inverse using the Extended Euclidean Algorithm.\nResearch: why do cryptographic protocols prefer Miller–Rabin over AKS, even though AKS is deterministic?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#overflow-precision",
    "href": "chapter_1.html#overflow-precision",
    "title": "Chapter 1. Numbers",
    "section": "1.4 Overflow & Precision",
    "text": "1.4 Overflow & Precision\n\n1.4 L0 - When Numbers Get Too Big or Too Small\nNumbers inside a computer are stored with a fixed number of bits. This means they can only represent values up to a certain limit. If a calculation produces a result larger than this limit, the value “wraps around,” much like the digits on an odometer rolling over after 999 to 000. This phenomenon is called overflow. Similarly, computers often cannot represent all decimal fractions exactly, leading to tiny errors called precision loss.\n\nDeep Dive\n\nInteger Overflow\n\nA computer uses a fixed number of bits (commonly 8, 16, 32, or 64) to store integers.\nAn 8-bit unsigned integer can represent values from 0 to 255. Adding 1 to 255 causes the value to wrap back to 0.\nSigned integers use two’s complement representation. For an 8-bit signed integer, the range is −128 to +127. Adding 1 to 127 makes it overflow to −128.\n\nExample in binary:\n11111111₂ (255) + 1 = 00000000₂ (0)\n01111111₂ (+127) + 1 = 10000000₂ (−128)\nFloating-Point Precision\n\nDecimal fractions like 0.1 cannot always be represented exactly in binary.\nAs a result, calculations may accumulate tiny errors.\nFor example, repeatedly adding 0.1 may not exactly equal 1.0 due to precision limits.\n\n\n\n\nExample\n# Integer overflow simulation with 8-bit values\ndef add_8bit(a, b):\n    result = (a + b) % 256  # simulate wraparound\n    return result\n\nprint(add_8bit(250, 10))   # 260 wraps to 4\nprint(add_8bit(255, 1))    # wraps to 0\n\n# Floating-point precision issue\nx = 0.1 + 0.2\nprint(x)           # Expected 0.3, but gives 0.30000000000000004\nprint(x == 0.3)    # False\n\n\nWhy It Matters\n\nUnexpected results: A calculation may suddenly produce a negative number or wrap around to zero.\nReal-world impact:\n\nVideo games may show scores jumping strangely if counters overflow.\nBanking or financial systems must avoid losing cents due to floating-point errors.\nEngineers and scientists rely on careful handling of precision to ensure correct simulations.\n\nFoundation for algorithms: Understanding overflow and precision prepares you for later topics like hashing, cryptography, and numerical analysis.\n\n\n\nExercises\n\nSimulate a 4-bit unsigned integer system. What happens if you start at 14 and keep adding 1?\nIn Python, try adding 0.1 to itself ten times. Does it equal exactly 1.0? Why or why not?\nWrite a function that checks if an 8-bit signed integer addition would overflow.\nImagine you are programming a digital clock that uses 2 digits for minutes (00–59). What happens when the value goes from 59 to 60? How would you handle this?\n\n\n\n\n1.4 L1 - Detecting and Managing Overflow in Real Programs\nComputers don’t do math in the abstract. Integers live in fixed-width registers; floats follow IEEE-754. Robust software accounts for these limits up front: choose the right representation, detect overflow, and compare floats safely. The following sections explain how these issues show up in practice and how to design around them.\n\nDeep Dive\n\n1) Integer arithmetic in practice\nFixed width means wraparound at \\(2^{n}\\). Unsigned wrap is modular arithmetic; signed overflow (two’s complement) can flip signs. Developers often discover this the hard way when a counter suddenly goes negative or wraps back to zero in production logs.\nBit width & ranges This table reminds us of the hard limits baked into hardware. Once the range is exceeded, the value doesn’t “grow bigger”—it wraps.\n\n\n\n\n\n\n\n\nWidth\nSigned range\nUnsigned range\n\n\n\n\n32\n−2,147,483,648 … 2,147,483,647\n0 … 4,294,967,295\n\n\n64\n−9,223,372,036,854,775,808 … 9,223,372,036,854,775,807\n0 … 18,446,744,073,709,551,615\n\n\n\nOverflow semantics by language Each language makes slightly different promises. This matters if you’re writing cross-language services or reading binary data across APIs.\n\n\n\n\n\n\n\n\n\nLanguage\nSigned overflow\nUnsigned overflow\nNotes\n\n\n\n\nC/C++\nUB (undefined)\nModular wrap\nUse UBSan/-fsanitize=undefined; widen types or check before add.\n\n\nRust\nTraps in debug; defined APIs\nwrapping_add, checked_add, saturating_add\nMake intent explicit.\n\n\nJava/Kotlin\nWraps (two’s complement)\nN/A (only signed types)\nUse Math.addExact to trap.\n\n\nC#\nWraps by default; checked to trap\nchecked/unchecked blocks\ndecimal type for money.\n\n\nPython\nArbitrary precision\nArbitrary precision\nSimulates fixed width if needed.\n\n\n\nA quick lesson: “wrap” may be safe in crypto or hashing, but it’s usually a bug in counters or indices. Always decide what you want up front.\n\n\n2) Floating-point you can depend on\nIEEE-754 doubles have ~15–16 decimal digits and huge dynamic range, but not exact decimal fractions. Think of floats as convenient approximations. They are perfect for physics simulations, but brittle when used to represent cents in a bank account.\nWhere precision is lost These examples show why “0.1 + 0.2 != 0.3” isn’t a joke—it’s a direct consequence of binary storage.\n\nScale mismatch: \\(1\\text{e}16 + 1 = 1\\text{e}16\\). The tiny +1 gets lost.\nCancellation: subtracting nearly equal numbers deletes significant digits.\nDecimal fractions (0.1) are repeating in binary.\n\nComparing floats Never compare with ==. Instead, use a mixed relative + absolute check:\n\\[\n|x-y| \\le \\max(\\text{rel}\\cdot\\max(|x|,|y|),\\ \\text{abs})\n\\]\nThis makes comparisons robust whether you’re near zero or far away.\nRounding modes (when you explicitly care) Most of the time you don’t think about rounding—hardware defaults to “round to nearest, ties to even.” But when writing financial systems or interval arithmetic, you want to control it.\n\n\n\n\n\n\n\nMode\nTypical use\n\n\n\n\nRound to nearest, ties to even (default)\nGeneral numeric work; minimizes bias\n\n\nToward 0 / ±∞\nBounds, interval arithmetic, conservative estimates\n\n\n\nHaving explicit rounding modes is like having a steering wheel—you don’t always turn, but you’re glad it’s there when the road curves.\nSummation strategies The order of addition matters for floats. These options give you a menu of accuracy vs. speed.\n\n\n\n\n\n\n\n\n\nMethod\nError\nCost\nWhen to use\n\n\n\n\nNaïve left-to-right\nWorst\nLow\nNever for sensitive sums\n\n\nPairwise / tree\nBetter\nMed\nParallel reductions, “good default”\n\n\nKahan (compensated)\nBest\nHigher\nFinancial-ish aggregates, small vectors\n\n\n\nYou don’t need Kahan everywhere, but knowing it exists keeps you from blaming “mystery bugs” on hardware.\nRepresentation choices Sometimes the best answer is to avoid floats entirely. Money is the classic example.\n\n\n\n\n\n\n\nUse case\nRecommended representation\n\n\n\n\nCurrency, invoicing\nFixed-point (e.g., cents as int64) or decimal/BigDecimal\n\n\nScientific compute\nfloat64, compensated sums, stable algorithms\n\n\nIDs, counters\nuint64/int64, detect overflow on boundaries\n\n\n\n\n\n\nCode (Python—portable patterns)\n# 32-bit checked add (raises on overflow)\ndef add_i32_checked(a: int, b: int) -&gt; int:\n    s = a + b\n    if s &lt; -2_147_483_648 or s &gt; 2_147_483_647:\n        raise OverflowError(\"int32 overflow\")\n    return s\n\n# Simulate 32-bit wrap (intentional modular arithmetic)\ndef add_i32_wrapping(a: int, b: int) -&gt; int:\n    s = (a + b) & 0xFFFFFFFF\n    return s - 0x100000000 if s & 0x80000000 else s\n\n# Relative+absolute epsilon float compare\ndef almost_equal(x: float, y: float, rel=1e-12, abs_=1e-12) -&gt; bool:\n    return abs(x - y) &lt;= max(rel * max(abs(x), abs(y)), abs_)\n\n# Kahan (compensated) summation\ndef kahan_sum(xs):\n    s = 0.0\n    c = 0.0\n    for x in xs:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\n# Fixed-point cents (safe for ~±9e16 cents with int64)\ndef dollars_to_cents(d: str) -&gt; int:\n    whole, _, frac = d.partition(\".\")\n    frac = (frac + \"00\")[:2]\n    return int(whole) * 100 + int(frac)\n\ndef cents_to_dollars(c: int) -&gt; str:\n    sign = \"-\" if c &lt; 0 else \"\"\n    c = abs(c)\n    return f\"{sign}{c//100}.{c%100:02d}\"\nThese examples are in Python for clarity, but the same ideas exist in every major language.\n\n\nWhy it matters\n\nReliability: Silent wrap or float drift becomes data corruption under load or over time.\nInteroperability: Services in different languages disagree on overflow; define and document your contracts.\nReproducibility: Deterministic numerics (same inputs → same bits) depend on summation order, rounding, and libraries.\nSecurity: UB-triggered overflows can turn into exploitable states.\n\nThis is why “it worked on my laptop” is not enough. You want to be sure it works on every platform, every time.\n\n\nExercises\n\nOverflow policy: For a metrics pipeline, decide where to use checked, wrapping, and saturating addition—and justify each with failure modes.\nULP probe: Find the smallest \\(\\epsilon\\) such that 1.0 + ε != 1.0 in your language; explain how it relates to machine epsilon.\nSummation bake-off: Sum the first 1M terms of the harmonic series with naïve, pairwise, and Kahan methods; compare results and timings.\nFixed-point ledger: Implement deposit/transfer/withdraw using int64 cents; prove no rounding loss for two-decimal currencies.\nBoundary tests: Write property tests that add_i32_checked raises on {INT_MAX,1} and {INT_MIN,-1}, and equals modular add where documented.\nCross-lang contract: Specify a JSON schema for monetary amounts and counters that avoids float types; include examples and edge cases.\n\nGreat — let’s rework 1.4 Overflow & Precision (L2) into a friendlier deep dive, using the same pattern: structured sections, tables, and added “bridge” sentences that guide the reader through complex, low-level material. This version should be dense enough to teach internals, but smooth enough to read without feeling like a spec sheet.\n\n\n\n1.4 L2. Under the Hood\nAt the lowest level, overflow and precision aren’t abstract concepts—they are consequences of how CPUs, compilers, and libraries actually implement arithmetic. Understanding these details makes debugging easier and gives you control over performance, reproducibility, and correctness.\n\nDeep Dive\n\n1) Hardware semantics\nCPUs implement integer and floating-point arithmetic directly in silicon. When the result doesn’t fit, different flags or traps are triggered.\n\nStatus flags (integers): Most architectures (x86, ARM, RISC-V) set overflow, carry, and zero flags after arithmetic. These flags drive branch instructions like jo (“jump if overflow”).\nFloating-point control: The FPU or SIMD unit maintains exception flags (inexact, overflow, underflow, invalid, divide-by-zero). These rarely trap by default; they silently set flags until explicitly checked.\n\nArchitectural view\n\n\n\n\n\n\n\n\n\nArch\nInteger overflow\nFP behavior\nDeveloper hooks\n\n\n\n\nx86-64\nWraparound in 2’s complement; OF/CF bits set\nIEEE-754; flags in MXCSR\njo/jno, fenv.h\n\n\nARM64\nWraparound; NZCV flags\nIEEE-754; exception bits\ncondition codes, feset*\n\n\nRISC-V\nWraparound; OV/CF optional\nIEEE-754; status regs\nCSRs, trap handlers\n\n\n\nKnowing what the CPU does lets you choose: rely on hardware wrap, trap explicitly, or add software checks.\n\n\n2) Compiler and language layers\nEven if hardware sets flags, your language may ignore them. Compilers often optimize based on the language spec.\n\nC/C++: Signed overflow is undefined behavior—the optimizer assumes it never happens, which can remove safety checks you thought were there.\nRust: Catches overflow in debug builds, then forces you to pick: checked_add, wrapping_add, or saturating_add.\nJVM languages (Java, Kotlin, Scala): Always wrap, hiding UB but forcing you to detect overflow yourself.\n.NET (C#, F#): Defaults to wrapping; you can enable checked contexts to trap.\nPython: Emulates unbounded integers, but sometimes simulates C-like behavior for low-level modules.\n\nThese choices aren’t arbitrary—they reflect trade-offs between speed, safety, and backward compatibility.\n\n\n3) Precision management in floating point\nFloating-point has more than just rounding errors. Engineers deal with gradual underflow, denormals, and fused operations.\n\nSubnormals: Numbers smaller than ~2.2e-308 in double precision become “denormalized,” losing precision but extending the range toward zero. Many CPUs handle these slowly.\nFlush-to-zero: Some systems skip subnormals entirely, treating them as zero to boost speed. Great for graphics; risky for scientific code.\nFMA (fused multiply-add): Computes (a*b + c) with one rounding, often improving precision and speed. However, it can break reproducibility across machines that do/don’t use FMA.\n\nPrecision events\n\n\n\n\n\n\n\n\nEvent\nWhat happens\nWhy it matters\n\n\n\n\nOverflow\nBecomes ±Inf\nDetectable via isinf, often safe\n\n\nUnderflow\nBecomes 0 or subnormal\nPerformance hit, possible accuracy loss\n\n\nInexact\nResult rounded\nHappens constantly; only matters if flagged\n\n\nInvalid\nNaN produced\nDivision 0/0, sqrt(-1), etc.\n\n\n\nWhen performance bugs show up in HPC or ML code, denormals and FMAs are often the hidden cause.\n\n\n4) Debugging and testing tools\nLow-level correctness requires instrumentation. Fortunately, toolchains give you options.\n\nSanitizers: -fsanitize=undefined (Clang/GCC) traps on signed overflow.\nValgrind / perf counters: Can catch denormal slowdowns.\nUnit-test utilities: Rust’s assert_eq!(checked_add(…)), Python’s math.isclose, Java’s BigDecimal reference checks.\nReproducibility flags: -ffast-math (fast but non-deterministic), vs. -frounding-math (strict).\n\nTesting with multiple compilers and settings reveals assumptions you didn’t know you had.\n\n\n5) Strategies in production systems\nWhen deploying real systems, you pick policies that match domain needs.\n\nDatabases: Use DECIMAL(p,s) to store fixed-point, preventing float drift in sums.\nFinancial systems: Explicit fixed-point types (cents as int64) + saturating logic on overflow.\nGraphics / ML: Accept float32 imprecision; gain throughput with fused ops and flush-to-zero.\nLow-level kernels: Exploit modular wraparound deliberately for hash tables, checksums, and crypto.\n\nPolicy menu\n\n\n\n\n\n\n\nScenario\nStrategy\n\n\n\n\nMoney transfers\nFixed-point, saturating arithmetic\n\n\nPhysics sim\nfloat64, stable integrators, compensated summation\n\n\nHashing / RNG\nEmbrace wraparound modular math\n\n\nCritical counters\nuint64 with explicit overflow trap\n\n\n\nThinking in policies avoids one-off hacks. Document “why” once, then apply consistently.\n\n\n\nCode Examples\nC (wrap vs check)\n#include &lt;stdint.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;limits.h&gt;\n\nbool add_checked_i32(int32_t a, int32_t b, int32_t *out) {\n    if ((b &gt; 0 && a &gt; INT32_MAX - b) ||\n        (b &lt; 0 && a &lt; INT32_MIN - b)) {\n        return false; // overflow\n    }\n    *out = a + b;\n    return true;\n}\nRust (explicit intent)\nfn demo() {\n    let x: i32 = i32::MAX;\n    println!(\"{:?}\", x.wrapping_add(1));   // wrap\n    println!(\"{:?}\", x.checked_add(1));    // None\n    println!(\"{:?}\", x.saturating_add(1)); // clamp\n}\nPython (reproducibility check)\nimport math\n\ndef ulp_diff(a: float, b: float) -&gt; int:\n    # Compares floats in terms of ULPs\n    import struct\n    ai = struct.unpack('!q', struct.pack('!d', a))[0]\n    bi = struct.unpack('!q', struct.pack('!d', b))[0]\n    return abs(ai - bi)\n\nprint(ulp_diff(1.0, math.nextafter(1.0, 2.0)))  # 1\nThese snippets show how different languages force you to state your policy, rather than relying on “whatever the hardware does.”\n\n\nWhy it matters\n\nPerformance: Understanding denormals and FMAs can save orders of magnitude in compute-heavy workloads.\nCorrectness: Database money columns or counters in billing systems can silently corrupt without fixed-point or overflow checks.\nPortability: Code that relies on UB may “work” on GCC Linux but fail on Clang macOS.\nSecurity: Integer overflow bugs (e.g., buffer length miscalculation) remain a classic vulnerability class.\n\nIn short, overflow and precision are not “just math”—they are systems-level contracts that must be understood and enforced.\n\n\nExercises\n\nCompiler behavior: Write a C function that overflows int32_t. Compile with and without -fsanitize=undefined. What changes?\nFMA investigation: Run a dot-product with and without -ffast-math. Measure result differences across compilers.\nDenormal trap: Construct a loop multiplying by 1e-308. Time it with flush-to-zero enabled vs disabled.\nPolicy design: For an in-memory database, define rules for counters, timestamps, and currency columns. Which use wrapping, which use fixed-point, which trap?\nCross-language test: Implement add_checked_i32 in C, Rust, and Python. Run edge-case inputs (INT_MAX, INT_MIN). Compare semantics.\nULP meter: Write a function in your language to compute ULP distance between two floats. Use it to compare rounding differences between platforms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "Chapter 2. Arrays",
    "section": "",
    "text": "2.1 Static Arrays",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#static-arrays",
    "href": "chapter_2.html#static-arrays",
    "title": "Chapter 2. Arrays",
    "section": "",
    "text": "2.2 L0 — Arrays That Grow\nA dynamic array is like a container that can expand and shrink as needed. Unlike static arrays, which must know their size in advance, a dynamic array adapts as elements are added or removed. You can think of it as a bookshelf where new shelves appear automatically when space runs out. The underlying idea is simple: keep the benefits of fast index-based access, while adding flexibility to change the size.\n\nDeep Dive\nA dynamic array begins with a fixed amount of space called its capacity. When the number of elements (the length) exceeds this capacity, the array grows. This is usually done by allocating a new, larger block of memory and copying the old elements into it. After this, new elements can be added until the new capacity is filled, at which point the process repeats.\nDespite this resizing process, the key properties remain:\n\nFast access and update: Elements can still be reached instantly using an index.\nAppend flexibility: New elements can be added at the end without worrying about fixed size.\nOccasional resizing cost: Most appends are quick, but when resizing happens, it takes longer because all elements must be copied.\n\nThe performance picture is intuitive:\n\n\n\n\n\n\n\n\nOperation\nTime Complexity (Typical)\nNotes\n\n\n\n\nAccess element\nO(1)\nIndex maps directly to position\n\n\nUpdate element\nO(1)\nReplace value in place\n\n\nAppend element\nO(1) amortized\nOccasionally O(n) when resizing occurs\n\n\nPop element\nO(1)\nRemove from end\n\n\nInsert/Delete\nO(n)\nElements must be shifted\n\n\n\nDynamic arrays therefore trade predictability for flexibility. The occasional slow operation is outweighed by the ability to grow and shrink on demand, which makes them useful for most real-world tasks where the number of elements is not known in advance.\n\n\nWorked Example\n# Create a dynamic array using Python's built-in list\narr = []\n\n# Append elements (array grows automatically)\nfor i in range(5):\n    arr.append((i + 1) * 10)\n\nprint(\"Array after appending:\", arr)\n\n# Access and update elements\nprint(\"Element at index 2:\", arr[2])\narr[2] = 99\nprint(\"Updated array:\", arr)\n\n# Remove last element\nlast = arr.pop()\nprint(\"Removed element:\", last)\nprint(\"Array after pop:\", arr)\n\n# Traverse array\nfor i in range(len(arr)):\n    print(f\"Index {i}: {arr[i]}\")\nThis short program shows how a dynamic array in Python resizes automatically with append and shrinks with pop. Access and updates remain instant, while resizing happens invisibly when more space is needed.\n\n\nWhy it matters\nDynamic arrays combine efficiency and flexibility. They allow programs to handle unknown or changing amounts of data without predefining sizes. They form the backbone of lists in high-level languages, balancing performance with usability. They also illustrate the idea of amortized cost: most operations are fast, but occasional expensive operations are averaged out over time.\n\n\nExercises\n\nCreate an array and append numbers 1 through 10. Print the final array.\nReplace the 3rd element with a new value.\nRemove the last two elements and print the result.\nWrite a procedure that traverses a dynamic array and computes the average of its elements.\nExplain why appending one element might sometimes be much slower than appending another, even though both look the same in code.\n\n\n\n\n2.1 L1 — Static Arrays in Practice\nStatic arrays are one of the simplest and most reliable ways of storing data. They are defined as collections of elements laid out in a fixed-size, contiguous block of memory. Unlike dynamic arrays, their size is determined at creation and cannot be changed later. This property makes them predictable, efficient, and easy to reason about, but also less flexible when dealing with varying amounts of data.\n\nDeep Dive\nAt the heart of static arrays is their memory layout. When an array is created, the program reserves a continuous region of memory large enough to hold all its elements. Each element is stored right next to the previous one. This design allows very fast access because the position of any element can be computed directly:\naddress_of(arr[i]) = base_address + (i × element_size)\nNo searching or scanning is required, only simple arithmetic. This is why reading or writing to an element at a given index is considered O(1) — constant time regardless of the array size.\nThe trade-offs emerge when considering insertion or deletion. Because elements are tightly packed, inserting a new element in the middle requires shifting all the subsequent elements by one position. Deleting works the same way in reverse. These operations are therefore O(n), linear in the size of the array.\nThe cost summary is straightforward:\n\n\n\nOperation\nTime Complexity\nNotes\n\n\n\n\nAccess element\nO(1)\nDirect index calculation\n\n\nUpdate element\nO(1)\nReplace in place\n\n\nTraverse\nO(n)\nVisit each element once\n\n\nInsert/Delete\nO(n)\nShifting elements required\n\n\n\n\nTrade-offs.\nStatic arrays excel when you know the size in advance. They guarantee fast access and compact memory usage because there is no overhead for resizing or metadata. However, they lack flexibility. If the array is too small, you must allocate a larger one and copy all elements over. If it is too large, memory is wasted. This is why languages like Python provide dynamic lists by default, while static arrays are used in performance-critical or resource-constrained contexts.\n\n\nUse cases.\n\nBuffers: Fixed-size areas for network packets or hardware input.\nLookup tables: Precomputed constants or small ranges of values (e.g., ASCII character tables).\nStatic configuration data: Tables known at compile-time, where resizing is unnecessary.\n\n\n\nPitfalls.\nProgrammers must be careful of two common issues:\n\nOut-of-bounds errors: Trying to access an index outside the valid range, leading to exceptions (in safe languages) or undefined behavior (in low-level languages).\nSizing problems: Underestimating leads to crashes, overestimating leads to wasted memory.\n\nStatic arrays are common in many programming environments. In Python, the array module provides a fixed-type sequence that behaves more like a C-style array. Libraries like NumPy also provide fixed-shape arrays that offer efficient memory usage and fast computations. In C and C++, arrays are part of the language itself, and they form the foundation of higher-level containers like std::vector.\n\n\n\nWorked Example\nimport array\n\n# Create a static array of integers (type 'i' = signed int)\narr = array.array('i', [0] * 5)\n\n# Fill the array with values\nfor i in range(len(arr)):\n    arr[i] = (i + 1) * 10\n\n# Access and update elements\nprint(\"Element at index 2:\", arr[2])\narr[2] = 99\nprint(\"Updated element at index 2:\", arr[2])\n\n# Traverse the array\nprint(\"All elements:\")\nfor i in range(len(arr)):\n    print(f\"Index {i}: {arr[i]}\")\n\n# Demonstrating the limitation: trying to insert beyond capacity\ntry:\n    arr.insert(5, 60)  # This technically works in Python's array, but resizes internally\n    print(\"Inserted new element:\", arr)\nexcept Exception as e:\n    print(\"Error inserting into static array:\", e)\nThis code illustrates the strengths and weaknesses of static arrays. Access and updates are immediate, and traversal is simple. But the notion of a “fixed size” means that insertion and deletion are costly or, in some languages, unsupported.\n\n\nWhy it matters\nStatic arrays are the building blocks of data structures. They teach the trade-off between speed and flexibility. They remind us that memory is finite and that how data is laid out in memory directly impacts performance. Whether writing Python code, using NumPy, or implementing algorithms in C, understanding static arrays makes it easier to reason about cost, predict behavior, and avoid common errors.\n\n\nExercises\n\nCreate an array of size 8 and fill it with even numbers from 2 to 16. Then access the 4th element directly.\nUpdate the middle element of a fixed-size array with a new value.\nWrite a procedure to traverse an array and find the maximum element.\nExplain why inserting a new value into the beginning of a static array requires shifting every other element.\nGive two examples of real-world systems where fixed-size arrays are a natural fit.\n\n\n\n\n2.1 L2 — Static Arrays and the System Beneath\nStatic arrays are more than just a collection of values; they are a direct window into how computers store and access data. At the advanced level, understanding static arrays means looking at memory models, cache behavior, compiler optimizations, and the role of arrays in operating systems and production libraries. This perspective is critical for building high-performance software and for avoiding subtle, system-level bugs.\n\nDeep Dive\nAt the lowest level, a static array is a contiguous block of memory. When an array is declared, the compiler calculates the required size as length × element_size and reserves that many bytes. Each element is addressed by simple arithmetic:\naddress_of(arr[i]) = base_address + (i × element_size)\nThis is why access and updates are constant time. The difference between static arrays and dynamically allocated ones often comes down to where the memory lives. Arrays declared inside a function may live on the stack, offering fast allocation and automatic cleanup. Larger arrays or arrays whose size isn’t known at compile time are allocated on the heap, requiring runtime management via calls such as malloc and free.\nThe cache hierarchy makes arrays especially efficient. Because elements are contiguous, accessing arr[i] loads not just one element but also its neighbors into a cache line (often 64 bytes). This property, known as spatial locality, means that scanning through an array is very fast. Prefetchers in modern CPUs exploit this by pulling in upcoming cache lines before they are needed. However, irregular access patterns (e.g., striding by 17) can defeat prefetching and lead to performance drops.\nAlignment and padding further influence performance. On most systems, integers must start at addresses divisible by 4, and doubles at addresses divisible by 8. If the compiler cannot guarantee alignment, it may add padding bytes to enforce it. Misaligned accesses can cause slowdowns or even hardware faults on strict architectures.\nDifferent programming languages expose these behaviors differently. In C, a declaration like int arr[10]; on the stack creates exactly 40 bytes on a 32-bit system. In contrast, malloc(10 * sizeof(int)) allocates memory on the heap. In C++, std::array&lt;int, 10&gt; is a safer wrapper around C arrays, while std::vector&lt;int&gt; adds resizing at the cost of indirection and metadata. In Fortran and NumPy, multidimensional arrays can be stored in column-major order rather than row-major, which changes how indices map to addresses and affects iteration performance.\nThe operating system kernel makes heavy use of static arrays. For example, Linux defines fixed-size arrays in structures like task_struct for file descriptors, and uses arrays in page tables for managing memory mappings. Static arrays provide predictability and remove the need for runtime memory allocation in performance-critical or security-sensitive code.\nFrom a performance profiling standpoint, arrays reveal fundamental trade-offs. Shifting elements during insertion or deletion requires copying bytes across memory, and the cost grows linearly with the number of elements. Compilers attempt to optimize loops over arrays with vectorization, turning element-wise operations into SIMD instructions. They may also apply loop unrolling or bounds-check elimination (BCE) when it can be proven that indices remain safe.\nStatic arrays also carry risks. In C and C++, accessing out-of-bounds memory leads to undefined behavior, often exploited in buffer overflow attacks. Languages like Java or Python mitigate this with runtime bounds checks, but at the expense of some performance.\nAt this level, static arrays should be seen not only as a data structure but as a fundamental contract between code, compiler, and hardware.\nWorked Example (C)\n#include &lt;stdio.h&gt;\n\nint main() {\n    // Static array of 8 integers allocated on the stack\n    int arr[8];\n\n    // Initialize array\n    for (int i = 0; i &lt; 8; i++) {\n        arr[i] = (i + 1) * 10;\n    }\n\n    // Access and update element\n    printf(\"Element at index 3: %d\\n\", arr[3]);\n    arr[3] = 99;\n    printf(\"Updated element at index 3: %d\\n\", arr[3]);\n\n    // Traverse with cache-friendly pattern\n    int sum = 0;\n    for (int i = 0; i &lt; 8; i++) {\n        sum += arr[i];\n    }\n    printf(\"Sum of array: %d\\n\", sum);\n\n    // Dangerous: Uncommenting would cause undefined behavior\n    // printf(\"%d\\n\", arr[10]);\n\n    return 0;\n}\nThis C program demonstrates how static arrays live on the stack, how indexing works, and why out-of-bounds access is dangerous. On real hardware, iterating sequentially benefits from spatial locality, making the traversal very fast compared to random access.\n\n\nWhy it matters\nStatic arrays are the substrate upon which much of computing is built. They are simple in abstraction but complex in practice, touching compilers, operating systems, and hardware. Understanding them is essential for:\n\nWriting cache-friendly and high-performance code.\nAvoiding security vulnerabilities like buffer overflows.\nAppreciating why higher-level data structures behave the way they do.\nBuilding intuition for memory layout, alignment, and the interaction between code and the CPU.\n\nArrays are not just “collections of values” — they are the foundation of efficient data processing.\n\n\nExercises\n\nIn C, declare a static array of size 16 and measure how long it takes to sum its elements sequentially versus accessing them in steps of 4. Explain the performance difference.\nExplain why iterating over a 2D array row by row is faster in C than column by column.\nConsider a struct with mixed types (e.g., char, int, double). Predict where padding bytes will be inserted if placed inside an array.\nResearch and describe how the Linux kernel uses static arrays in managing processes or memory.\nDemonstrate with code how accessing beyond the end of a static array in C can cause undefined behavior, and explain why this is a serious risk in system programming.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#dynamic-arrays",
    "href": "chapter_2.html#dynamic-arrays",
    "title": "Chapter 2. Arrays",
    "section": "2.2 Dynamic Arrays",
    "text": "2.2 Dynamic Arrays\n\n2.2 L0 — Arrays That Grow\nA dynamic array is like a container that can expand and shrink as needed. Unlike static arrays, which must know their size in advance, a dynamic array adapts as elements are added or removed. You can think of it as a bookshelf where new shelves appear automatically when space runs out. The underlying idea is simple: keep the benefits of fast index-based access, while adding flexibility to change the size.\n\nDeep Dive\nA dynamic array begins with a fixed amount of space called its capacity. When the number of elements (the length) exceeds this capacity, the array grows. This is usually done by allocating a new, larger block of memory and copying the old elements into it. After this, new elements can be added until the new capacity is filled, at which point the process repeats.\nDespite this resizing process, the key properties remain:\n\nFast access and update: Elements can still be reached instantly using an index.\nAppend flexibility: New elements can be added at the end without worrying about fixed size.\nOccasional resizing cost: Most appends are quick, but when resizing happens, it takes longer because all elements must be copied.\n\nThe performance picture is intuitive:\n\n\n\n\n\n\n\n\nOperation\nTime Complexity (Typical)\nNotes\n\n\n\n\nAccess element\nO(1)\nIndex maps directly to position\n\n\nUpdate element\nO(1)\nReplace value in place\n\n\nAppend element\nO(1) amortized\nOccasionally O(n) when resizing occurs\n\n\nPop element\nO(1)\nRemove from end\n\n\nInsert/Delete\nO(n)\nElements must be shifted\n\n\n\nDynamic arrays therefore trade predictability for flexibility. The occasional slow operation is outweighed by the ability to grow and shrink on demand, which makes them useful for most real-world tasks where the number of elements is not known in advance.\n\n\nWorked Example\n# Create a dynamic array using Python's built-in list\narr = []\n\n# Append elements (array grows automatically)\nfor i in range(5):\n    arr.append((i + 1) * 10)\n\nprint(\"Array after appending:\", arr)\n\n# Access and update elements\nprint(\"Element at index 2:\", arr[2])\narr[2] = 99\nprint(\"Updated array:\", arr)\n\n# Remove last element\nlast = arr.pop()\nprint(\"Removed element:\", last)\nprint(\"Array after pop:\", arr)\n\n# Traverse array\nfor i in range(len(arr)):\n    print(f\"Index {i}: {arr[i]}\")\nThis short program shows how a dynamic array in Python resizes automatically with append and shrinks with pop. Access and updates remain instant, while resizing happens invisibly when more space is needed.\n\n\nWhy it matters\nDynamic arrays combine efficiency and flexibility. They allow programs to handle unknown or changing amounts of data without predefining sizes. They form the backbone of lists in high-level languages, balancing performance with usability. They also illustrate the idea of amortized cost: most operations are fast, but occasional expensive operations are averaged out over time.\n\n\nExercises\n\nCreate an array and append numbers 1 through 10. Print the final array.\nReplace the 3rd element with a new value.\nRemove the last two elements and print the result.\nWrite a procedure that traverses a dynamic array and computes the average of its elements.\nExplain why appending one element might sometimes be much slower than appending another, even though both look the same in code.\n\n\n\n\n2.2 L1 — Dynamic Arrays in Practice\nDynamic arrays extend the idea of static arrays by making size flexible. They allow adding or removing elements without knowing the total number in advance. Under the hood, this flexibility is achieved through careful memory management: the array is stored in a contiguous block, but when more space is needed, a larger block is allocated, and all elements are copied over. This mechanism balances speed with adaptability and is the reason why dynamic arrays are the default sequence type in many languages.\n\nDeep Dive\nA dynamic array starts with a certain capacity, often larger than the initial number of elements. When the number of stored elements exceeds capacity, the array is resized. The common strategy is to double the capacity. For example, an array of capacity 4 that becomes full will reallocate to capacity 8. All existing elements are copied into the new block, and the old memory is freed.\nThis strategy makes appending efficient on average. While an individual resize costs O(n) because of the copying, most appends are O(1). Across a long sequence of operations, the total cost averages out — this is called amortized analysis.\nDynamic arrays retain the key advantages of static arrays:\n\nContiguous storage means fast random access with O(1) time.\nUpdates are also O(1) because they overwrite existing slots.\n\nThe challenges appear with other operations:\n\nInsertions or deletions in the middle require shifting elements, making them O(n).\nResizing events create temporary latency spikes, especially when arrays are large.\n\nA clear summary:\n\n\n\nOperation\nTime Complexity\nNotes\n\n\n\n\nAccess element\nO(1)\nDirect index calculation\n\n\nUpdate element\nO(1)\nReplace value in place\n\n\nAppend element\nO(1) amortized\nOccasional O(n) when resizing\n\n\nPop element\nO(1)\nRemove from end\n\n\nInsert/Delete\nO(n)\nShifting elements required\n\n\n\n\nTrade-offs.\nDynamic arrays sacrifice predictability for convenience. Resizing causes performance spikes, but the doubling strategy keeps the average cost low. Over-allocation wastes some memory, but it reduces the frequency of resizes. The key is that this trade-off is usually favorable in practice.\n\n\nUse cases.\nDynamic arrays are well-suited for:\n\nLists whose size is not known in advance.\nWorkloads dominated by appending and reading values.\nGeneral-purpose data structures in high-level programming languages.\n\n\n\nLanguage implementations.\n\nPython: list is a dynamic array, using an over-allocation strategy to reduce frequent resizes.\nC++: std::vector doubles its capacity when needed, invalidating pointers/references after reallocation.\nJava: ArrayList grows by about 1.5× when full, trading memory efficiency for fewer copies.\n\n\n\nPitfalls.\n\nIn languages with pointers or references, resizes can invalidate existing references.\nLarge arrays may cause noticeable latency during reallocation.\nMiddle insertions and deletions remain inefficient compared to linked structures.\n\n\n\n\nWorked Example\n# Demonstrate dynamic array behavior using Python's list\narr = []\n\n# Append elements to trigger resizing internally\nfor i in range(12):\n    arr.append(i)\n    print(f\"Appended {i}, length = {len(arr)}\")\n\n# Access and update\nprint(\"Element at index 5:\", arr[5])\narr[5] = 99\nprint(\"Updated element at index 5:\", arr[5])\n\n# Insert in the middle (expensive operation)\narr.insert(6, 123)\nprint(\"Array after middle insert:\", arr)\n\n# Pop elements\narr.pop()\nprint(\"Array after pop:\", arr)\nThis example illustrates appending, updating, inserting, and popping. While Python hides the resizing, the cost is there: occasionally the list must allocate more space and copy its contents.\n\n\nWhy it matters\nDynamic arrays balance flexibility and performance. They demonstrate the principle of amortized complexity, showing how expensive operations can be smoothed out over time. They also highlight trade-offs between memory usage and speed. Understanding them explains why high-level lists perform well in everyday coding but also where they can fail under stress.\n\n\nExercises\n\nCreate a dynamic array and append the numbers 1 to 20. Measure how many times resizing would have occurred if the growth factor were 2.\nInsert an element into the middle of a large array and explain why this operation is slower than appending at the end.\nWrite a procedure to remove all odd numbers from a dynamic array.\nCompare Python’s list, Java’s ArrayList, and C++’s std::vector in terms of growth strategy.\nExplain why references to elements of a std::vector may become invalid after resizing.\n\n\n\n\n2.2 L2 — Dynamic Arrays Under the Hood\nDynamic arrays reveal how high-level flexibility is built on top of low-level memory management. While they appear as resizable containers, underneath they are carefully engineered to balance performance, memory efficiency, and safety. Understanding their internals sheds light on allocators, cache behavior, and the risks of pointer invalidation.\n\nDeep Dive\nDynamic arrays rely on heap allocation. When first created, they reserve a contiguous memory block with some capacity. As elements are appended and the array fills, the implementation must allocate a new, larger block, copy all existing elements, and free the old block.\nMost implementations use a geometric growth strategy, often doubling the capacity when space runs out. Some use a factor smaller than two, such as 1.5×, to reduce memory waste. The trade-off is between speed and efficiency:\n\nLarger growth factors reduce the number of costly reallocations.\nSmaller growth factors waste less memory but increase resize frequency.\n\nThis leads to an amortized O(1) cost for append. Each resize is expensive, but they happen infrequently enough that the average cost remains constant across many operations.\nHowever, resizes have side effects:\n\nPointer invalidation: In C++ std::vector, any reference, pointer, or iterator into the old memory becomes invalid after reallocation.\nLatency spikes: Copying thousands or millions of elements in one step can stall a program, especially in real-time or low-latency systems.\nAllocator fragmentation: Repeated growth and shrink cycles can fragment the heap, reducing performance in long-running systems.\n\nCache efficiency is one of the strengths of dynamic arrays. Because elements are stored contiguously, traversals are cache-friendly, and prefetchers can load entire blocks into cache lines. But reallocations can disrupt locality temporarily, as the array may move to a new region of memory.\nDifferent languages implement dynamic arrays with variations:\n\nPython lists use over-allocation with a small growth factor (~12.5% to 25% extra). This minimizes wasted memory while keeping amortized costs stable.\nC++ std::vector typically doubles its capacity when needed. Developers can call reserve() to preallocate memory and avoid repeated reallocations.\nJava ArrayList grows by ~1.5×, balancing heap usage with resize frequency.\n\nDynamic arrays also face risks:\n\nIf resizing logic is incorrect, buffer overflows may occur.\nAttackers can exploit repeated growth/shrink cycles to cause denial-of-service via frequent allocations.\nVery large allocations can fail outright if memory is exhausted.\n\nFrom a profiling perspective, workloads matter. Append-heavy patterns perform extremely well due to amortization. Insert-heavy or middle-delete workloads perform poorly because of element shifting. Allocator-aware optimizations, like pre-reserving capacity, can dramatically improve performance.\n\n\nWorked Example (C++)\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nint main() {\n    std::vector&lt;int&gt; v;\n    v.reserve(4);  // reserve space for 4 elements to reduce reallocations\n\n    for (int i = 0; i &lt; 12; i++) {\n        v.push_back(i * 10);\n        std::cout &lt;&lt; \"Appended \" &lt;&lt; i*10\n                  &lt;&lt; \", size = \" &lt;&lt; v.size()\n                  &lt;&lt; \", capacity = \" &lt;&lt; v.capacity() &lt;&lt; std::endl;\n    }\n\n    // Access and update\n    std::cout &lt;&lt; \"Element at index 5: \" &lt;&lt; v[5] &lt;&lt; std::endl;\n    v[5] = 99;\n    std::cout &lt;&lt; \"Updated element at index 5: \" &lt;&lt; v[5] &lt;&lt; std::endl;\n\n    // Demonstrate invalidation risk\n    int* ptr = &v[0];\n    v.push_back(12345); // may reallocate and move data\n    std::cout &lt;&lt; \"Old pointer may now be invalid: \" &lt;&lt; *ptr &lt;&lt; std::endl; // UB if reallocated\n}\nThis program shows how std::vector manages capacity. The output reveals how capacity grows as more elements are appended. The pointer invalidation example highlights a subtle but critical risk: after a resize, old addresses into the array are no longer safe.\n\n\nWhy it matters\nDynamic arrays expose the tension between abstraction and reality. They appear simple, but internally they touch almost every layer of the system: heap allocators, caches, compiler optimizations, and safety checks. They are essential for understanding how high-level languages achieve both usability and performance, and they illustrate real-world engineering trade-offs between speed, memory, and safety.\n\n\nExercises\n\nIn C++, measure the capacity growth of a std::vector&lt;int&gt; as you append 1,000 elements. Plot size vs capacity.\nExplain why a program that repeatedly appends and deletes elements might fragment the heap over time.\nCompare the growth strategies of Python list, C++ std::vector, and Java ArrayList. Which wastes more memory? Which minimizes resize cost?\nWrite a program that appends 1 million integers to a dynamic array and then times the traversal. Compare it with inserting 1 million integers at the beginning.\nShow how reserve() in std::vector or ensureCapacity() in Java ArrayList can eliminate costly reallocation spikes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#slices-views",
    "href": "chapter_2.html#slices-views",
    "title": "Chapter 2. Arrays",
    "section": "2.3 Slices & Views",
    "text": "2.3 Slices & Views\n\n2.3 L0 — Looking Through a Window\nA slice or view is a way to look at part of an array without creating a new one. Instead of copying data, a slice points to the same underlying elements, just with its own start and end boundaries. This makes working with subarrays fast and memory-efficient. You can think of a slice as a window into a longer row of boxes, showing only the portion you care about.\n\nDeep Dive\nWhen you take a slice, you don’t get a new array filled with copied elements. Instead, you get a new “view” that remembers where in the original array it starts and stops. This is useful because:\n\nNo copying means creating a slice is very fast.\nShared storage means changes in the slice also affect the original array (in languages like Go, Rust, or NumPy).\nReduced scope means you can focus on a part of the array without carrying the entire structure.\n\nKey properties of slices:\n\nThey refer to the same memory as the original array.\nThey have their own length (number of elements visible).\nThey may also carry a capacity, which limits how far they can expand into the original array.\n\nIn Python, list slicing (arr[2:5]) creates a new list with copies of the elements. This is not a true view. By contrast, NumPy arrays, Go slices, and Rust slices provide real views — updates to the slice affect the original array.\nA summary:\n\n\n\nFeature\nSlice/View\nNew Array (Copy)\n\n\n\n\nMemory usage\nShares existing storage\nAllocates new storage\n\n\nCreation cost\nO(1)\nO(n) for copied elements\n\n\nUpdates\nAffect original array\nIndependent\n\n\nSafety\nRisk of aliasing issues\nNo shared changes\n\n\n\nSlices are especially valuable when working with large datasets, where copying would be too expensive.\n\n\nWorked Example\n# Python slicing creates a copy, but useful to illustrate concept\narr = [10, 20, 30, 40, 50]\n\n# Slice of middle part\nsub = arr[1:4]\nprint(\"Original array:\", arr)\nprint(\"Slice (copy in Python):\", sub)\n\n# Modifying the slice does not affect the original (Python behavior)\nsub[0] = 99\nprint(\"Modified slice:\", sub)\nprint(\"Original array unchanged:\", arr)\n\n# In contrast, NumPy arrays behave like true views\nimport numpy as np\narr_np = np.array([10, 20, 30, 40, 50])\nsub_np = arr_np[1:4]\nsub_np[0] = 99\nprint(\"NumPy slice reflects back:\", arr_np)\nThis example shows the difference: Python lists create a copy, while NumPy slices act as views and affect the original.\n\n\nWhy it matters\nSlices let you work with subsets of data without wasting memory or time copying. They are critical in systems and scientific computing where performance matters. They also highlight the idea of aliasing: when two names refer to the same data. Understanding slices teaches you when changes propagate and when they don’t, which helps avoid surprising bugs.\n\n\nExercises\n\nCreate an array of 10 numbers. Take a slice of the middle 5 elements and print them.\nUpdate the first element in your slice and describe what happens to the original array in your chosen language.\nCompare slicing behavior in Python and NumPy: which one copies, which one shares?\nExplain why slicing a very large dataset is more efficient than copying it.\nThink of a real-world analogy where two people share the same resource but only see part of it. How does this relate to slices?\n\n\n\n\n2.3 L1 — Slices in Practice\nSlices provide a practical way to work with subarrays efficiently. Instead of copying data into a new structure, a slice acts as a lightweight reference to part of an existing array. This gives programmers flexibility to manipulate sections of data without paying the cost of duplication, while still preserving the familiar indexing model of arrays.\n\nDeep Dive\nAt the implementation level, a slice is typically represented by a small structure that stores:\n\nA pointer to the first element in the slice.\nThe slice’s length (how many elements it can access).\nOptionally, its capacity (how far the slice can grow into the backing array).\n\nIndexing into a slice works just like indexing into an array:\nslice[i] → base_address + i × element_size\nThe complexity model stays consistent:\n\nSlice creation: O(1) when implemented as a view, O(n) if the language copies elements.\nAccess/update: O(1), just like arrays.\nTraversal: O(k), proportional to the slice’s length.\n\nThis design makes slices efficient but introduces trade-offs. With true views, the slice and the original array share memory. Updates made through one are visible through the other. This can be extremely useful but also dangerous, as it introduces the possibility of unintended side effects. Languages that prioritize safety (like Python lists) avoid this by returning a copy instead of a view.\nThe balance is clear:\n\nViews (Go, Rust, NumPy): fast and memory-efficient, but require discipline to avoid aliasing bugs.\nCopies (Python lists): safer, but slower and more memory-intensive for large arrays.\n\nA summary of behaviors:\n\n\n\n\n\n\n\n\n\nLanguage/Library\nSlice Behavior\nShared Updates\nNotes\n\n\n\n\nGo\nView\nYes\nBacked by (ptr, len, cap) triple\n\n\nRust\nView\nYes\nSafe with borrow checker (mutable/immutable)\n\n\nPython list\nCopy\nNo\nSafer but memory-expensive\n\n\nNumPy array\nView\nYes\nBasis of efficient scientific computing\n\n\nC/C++\nManual pointer\nYes\nNo built-in slice type; must manage manually\n\n\n\n\nUse cases.\n\nProcessing large datasets in segments without copying.\nImplementing algorithms like sliding windows, partitions, or block-based iteration.\nSharing views of arrays across functions for modular design without allocating new memory.\n\n\n\nPitfalls.\n\nIn languages with views, careless updates can corrupt the original array unexpectedly.\nIn Go and C++, extending a slice/view beyond its capacity causes runtime errors or undefined behavior.\nIn Python, forgetting that slices are copies can lead to performance issues in large-scale workloads.\n\n\n\n\nWorked Example\n# Demonstrating copy slices vs view slices in Python and NumPy\n\n# Python list slicing creates a copy\narr = [1, 2, 3, 4, 5]\nsub = arr[1:4]\nsub[0] = 99\nprint(\"Python original:\", arr)  # unchanged\nprint(\"Python slice (copy):\", sub)\n\n# NumPy slicing creates a view\nimport numpy as np\narr_np = np.array([1, 2, 3, 4, 5])\nsub_np = arr_np[1:4]\nsub_np[0] = 99\nprint(\"NumPy original (affected):\", arr_np)\nprint(\"NumPy slice (view):\", sub_np)\nThis example shows the key difference: Python lists copy, while NumPy provides true views. The choice reflects different design priorities: safety in Python’s core data structures versus performance in numerical computing.\n\n\nWhy it matters\nSlices make programs more efficient and expressive. They eliminate unnecessary copying, speed up algorithms that work on subranges, and support modular programming by passing references instead of duplicating data. At the same time, they expose important design trade-offs between safety and performance. Understanding slices provides insight into how modern languages manage memory efficiently while protecting against common errors.\n\n\nExercises\n\nIn Go, create an array of 10 elements and take a slice of the middle 5. Update the slice and observe the effect on the array.\nIn Python, slice a list of 1 million numbers and explain the performance cost compared to slicing a NumPy array of the same size.\nWrite a procedure that accepts a slice and doubles each element. Test with both a copy-based language (Python lists) and a view-based language (NumPy or Go).\nExplain why passing slices to functions is more memory-efficient than passing entire arrays.\nDiscuss a scenario where slice aliasing could lead to unintended bugs in a large program.\n\n\n\n\n2.3 L2 — Slices and Views in Systems\nSlices are not just convenient programming shortcuts; they represent a powerful abstraction that ties language semantics to hardware realities. At this level, slices expose details about memory layout, lifetime, and compiler optimizations. They are central to performance-critical systems because they allow efficient access to subsets of data without copying, while also demanding careful handling to avoid aliasing bugs and unsafe memory access.\n\nDeep Dive\nA slice is typically represented internally as a triple:\n\nA pointer to the first element,\nA length describing how many elements are visible,\nA capacity showing how far the slice may extend into the backing array.\n\nIndexing into a slice is still O(1), but the compiler inserts bounds checks to prevent invalid access. In performance-sensitive code, compilers often apply bounds-check elimination (BCE) when they can prove that loop indices remain within safe limits. This allows slices to combine safety with near-native performance.\nSlices are non-owning references. They do not manage memory themselves but instead depend on the underlying array. In languages like Rust, the borrow checker enforces lifetimes to prevent dangling slices. In C and C++, however, programmers must manually ensure that the backing array outlives the slice, or risk undefined behavior.\nBecause slices share memory, they introduce aliasing. Multiple slices can point to overlapping regions of the same array. This can lead to subtle bugs if two parts of a program update the same region concurrently. In multithreaded contexts, mutable aliasing without synchronization can cause data races. Some systems adopt copy-on-write strategies to reduce risks, but this adds overhead.\nFrom a performance perspective, slices preserve contiguity, which is ideal for cache locality and prefetching. Sequential traversal is cache-friendly, but strided access (e.g., every 3rd element) can defeat hardware prefetchers, reducing efficiency. Languages like NumPy exploit strides explicitly, enabling both dense and sparse-like views without copying.\nLanguage designs differ in how they handle slices:\n\nGo uses (ptr, len, cap). Appending to a slice may allocate a new array if capacity is exceeded, silently detaching it from the original backing storage.\nRust distinguishes &[T] for immutable and &mut [T] for mutable slices, with the compiler enforcing safe borrowing rules.\nC/C++ provide no built-in slice type, so developers rely on raw pointers and manual length tracking. This is flexible but error-prone.\nNumPy supports advanced slicing: views with strides, broadcasting rules, and multidimensional slices for scientific computing.\n\nCompilers also optimize slice-heavy code:\n\nVectorization transforms element-wise loops into SIMD instructions when slices are contiguous.\nEscape analysis determines whether slices can stay stack-allocated or must be promoted to the heap.\n\nSystem-level use cases highlight the importance of slices:\n\nZero-copy I/O: network and file system buffers are exposed as slices into larger memory regions.\nMemory-mapped files: slices map directly to disk pages, enabling efficient processing of large datasets.\nGPU programming: CUDA and OpenCL kernels operate on slices of device memory, avoiding transfers.\n\nThese applications show why slices are not just a programming convenience but a core tool for bridging high-level logic with low-level performance.\n\n\nWorked Example (Go)\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    arr := [6]int{10, 20, 30, 40, 50, 60}\n    s := arr[1:4] // slice referencing elements 20, 30, 40\n\n    fmt.Println(\"Original array:\", arr)\n    fmt.Println(\"Slice view:\", s)\n\n    // Update through slice\n    s[0] = 99\n    fmt.Println(\"After update via slice, array:\", arr)\n\n    // Demonstrate capacity\n    fmt.Println(\"Slice length:\", len(s), \"capacity:\", cap(s))\n\n    // Appending beyond slice capacity reallocates\n    s = append(s, 70, 80)\n    fmt.Println(\"Slice after append:\", s)\n    fmt.Println(\"Array after append (unchanged):\", arr)\n}\nThis example illustrates Go’s slice model. The slice s initially shares storage with arr. Updates propagate to the array. However, when appending exceeds the slice’s capacity, Go allocates a new backing array, breaking the link with the original. This behavior is efficient but can surprise developers if not understood.\n\n\nWhy it matters\nSlices embody key system concepts: pointer arithmetic, memory ownership, cache locality, and aliasing. They explain how languages achieve zero-copy abstractions while balancing safety and performance. They also highlight risks such as dangling references and silent reallocations. Mastery of slices is essential for building efficient algorithms, avoiding memory errors, and reasoning about system-level performance.\n\n\nExercises\n\nIn Go, create an array of 8 integers and take overlapping slices. Modify one slice and observe effects on the other. Explain why this happens.\nIn Rust, attempt to create two mutable slices of the same array region. Explain why the borrow checker rejects it.\nIn C, simulate a slice using a pointer and a length. Show what happens if the backing array is freed while the slice is still in use.\nIn NumPy, create a 2D array and take a strided slice (every second row). Explain why performance is worse than contiguous slicing.\nCompare how Python, Go, and Rust enforce (or fail to enforce) safety when working with slices.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#multidimensional-arrays",
    "href": "chapter_2.html#multidimensional-arrays",
    "title": "Chapter 2. Arrays",
    "section": "2.4 Multidimensional Arrays",
    "text": "2.4 Multidimensional Arrays\n\n2.4 L0 — Tables and Grids\nA multidimensional array is an extension of the simple array idea. Instead of storing data in a single row, a multidimensional array organizes elements in a grid, table, or cube. The most common example is a two-dimensional array, which looks like a table with rows and columns. Each position in the grid is identified by two coordinates: one for the row and one for the column. This structure is useful for representing spreadsheets, images, game boards, and mathematical matrices.\n\nDeep Dive\nYou can think of a multidimensional array as an array of arrays. A two-dimensional array is a list where each element is itself another list. For example, a 3×3 table contains 3 rows, each of which has 3 columns. Accessing an element requires specifying both coordinates: arr[row][col].\nEven though we visualize multidimensional arrays as grids, in memory they are still stored as a single continuous sequence. To find an element, the program computes its position using a formula. In a 2D array with n columns, the element at (row, col) is located at:\nindex = row × n + col\nThis mapping allows direct access in constant time, just like with 1D arrays.\nCommon operations are:\n\nCreation: decide dimensions and initialize with values.\nAccess: specify row and column to retrieve an element.\nUpdate: change the value at a given coordinate.\nTraversal: visit elements row by row or column by column.\n\nA quick summary:\n\n\n\nOperation\nDescription\nCost\n\n\n\n\nAccess element\nGet value at (row, col)\nO(1)\n\n\nUpdate element\nReplace value at (row, col)\nO(1)\n\n\nTraverse array\nVisit all elements\nO(n×m)\n\n\n\nMultidimensional arrays introduce an important detail: traversal order. In many languages (like C and Python’s NumPy), arrays are stored in row-major order, which means all elements of the first row are laid out contiguously, then the second row, and so on. Others, like Fortran, use column-major order. This difference affects performance in more advanced topics, but at this level, the key idea is that access is still fast and predictable.\n\n\nWorked Example\n# Create a 2D array (3x3 table) using list of lists\ntable = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Access element in second row, third column\nprint(\"Element at (1, 2):\", table[1][2])  # prints 6\n\n# Update element\ntable[0][0] = 99\nprint(\"Updated table:\", table)\n\n# Traverse row by row\nprint(\"Row traversal:\")\nfor row in table:\n    for val in row:\n        print(val, end=\" \")\n    print()\nThis example shows how to build and use a 2D array in Python. It looks like a table, with easy access via coordinates.\n\n\nWhy it matters\nMultidimensional arrays provide a natural way to represent structured data like matrices, grids, and images. They allow algorithms to work directly with two-dimensional or higher-dimensional information without flattening everything into one long row. This makes programs easier to write, read, and reason about.\n\n\nExercises\n\nCreate a 3×3 array with numbers 1 through 9 and print it in a table format.\nAccess the element at row 2, column 3 and describe how you found it.\nChange the center element of a 3×3 array to 0.\nWrite a loop to compute the sum of all values in a 4×4 array.\nExplain why accessing (row, col) in a 2D array is still O(1) even though the data is stored in a single sequence in memory.\n\n\n\n\n2.4 L1 — Multidimensional Arrays in Practice\nMultidimensional arrays are powerful because they extend the linear model of arrays into grids, tables, and higher dimensions. At a practical level, they are still stored in memory as a flattened linear block. What changes is the indexing formula: instead of a single index, we use multiple coordinates that the system translates into one offset.\n\nDeep Dive\nThe most common form is a 2D array. In memory, the elements are laid out row by row (row-major) or column by column (column-major).\n\nRow-major (C, NumPy default): elements of each row are contiguous.\nColumn-major (Fortran, MATLAB): elements of each column are contiguous.\n\nFor a 2D array with num_cols columns, the element at (row, col) in row-major order is located at:\nindex = row × num_cols + col\nFor column-major order with num_rows rows, the formula is:\nindex = col × num_rows + row\nThis distinction matters when traversing. Accessing elements in the memory’s natural order (row by row for row-major, column by column for column-major) is cache-friendly. Traversing in the opposite order forces the program to jump around in memory, leading to slower performance.\nExtending to 3D and higher is straightforward. For a 3D array with (layers, rows, cols) in row-major order:\nindex = layer × (rows × cols) + row × cols + col\nComplexity remains consistent:\n\nAccess/update: O(1) using index calculation.\nTraversal: O(n × m) for 2D, O(n × m × k) for 3D.\n\nTrade-offs:\n\nContiguous multidimensional arrays provide excellent performance for predictable workloads (e.g., matrix operations).\nResizing is costly because the entire block must be reallocated.\nJagged arrays (arrays of arrays) provide flexibility but lose memory contiguity, reducing cache performance.\n\nUse cases:\n\nStoring images (pixels as grids).\nMathematical matrices in scientific computing.\nGame boards and maps.\nTables in database-like structures.\n\nDifferent languages implement multidimensional arrays differently:\n\nPython lists: nested lists simulate 2D arrays but are jagged and fragmented in memory.\nNumPy: provides true multidimensional arrays stored contiguously in row-major (default) or column-major order.\nC/C++: support both contiguous multidimensional arrays (int arr[rows][cols];) and pointer-based arrays of arrays.\nJava: uses arrays of arrays (jagged by default).\n\n\n\nWorked Example\n# Comparing list of lists vs NumPy arrays\n# List of lists (jagged)\ntable = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nprint(\"Element at (2, 1):\", table[2][1])  # 8\n\n# NumPy array (true contiguous 2D array)\nimport numpy as np\nmatrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(\"Element at (2, 1):\", matrix[2,1])  # 8\n\n# Traversal in row-major order\nfor row in range(matrix.shape[0]):\n    for col in range(matrix.shape[1]):\n        val = matrix[row, col]  # efficient in NumPy\nThe Python list-of-lists behaves like a table, but each row may live separately in memory. NumPy, on the other hand, stores data contiguously, enabling much faster iteration and vectorized operations.\n\n\nWhy it matters\nMultidimensional arrays are central to real-world applications, from graphics and simulations to data science and machine learning. They highlight how physical memory layout (row-major vs column-major) interacts with algorithm design. Understanding them allows developers to choose between safety, flexibility, and performance, depending on the problem.\n\n\nExercises\n\nWrite a procedure to sum all values in a 5×5 array by traversing row by row.\nFor a 3×3 NumPy array, access element (2,1) and explain how its memory index is calculated in row-major order.\nCreate a jagged array (rows of different lengths) in Python. Show how traversal differs from a true 2D array.\nExplain why traversing a NumPy array by rows is faster than by columns.\nWrite a formula for computing the linear index of (i,j,k) in a 3D array stored in row-major order.\n\n\n\n\n2.4 L2 — Multidimensional Arrays and System Realities\nMultidimensional arrays are not only a logical abstraction but also a system-level structure that interacts with memory layout, caches, and compilers. At this level, understanding how they are stored, accessed, and optimized is essential for building high-performance code in scientific computing, graphics, and data-intensive systems.\n\nDeep Dive\nA multidimensional array is stored either as a contiguous linear block or as an array of pointers (jagged array). In the contiguous layout, elements follow one another in memory according to a linearization formula. In row-major order (C, NumPy), a 2D element at (row, col) is:\nindex = row × num_cols + col\nIn column-major order (Fortran, MATLAB), the formula is:\nindex = col × num_rows + row\nThis difference has deep performance consequences. In row-major layout, traversing row by row is cache-friendly because consecutive elements are contiguous. Traversing column by column introduces large strides, which can cause cache and TLB misses. In column-major arrays, the reverse holds true.\n\nCache and performance.\nWhen an array is traversed sequentially in its natural memory order, cache lines are used efficiently and hardware prefetchers work well. Strided access, such as reading every k-th column in a row-major layout, prevents prefetchers from predicting the access pattern and leads to performance drops. For large arrays, this can mean the difference between processing gigabytes per second and megabytes per second.\n\n\nAlignment and padding.\nCompilers and libraries often align rows to cache line or SIMD vector boundaries. For example, a 64-byte cache line may cause padding to be inserted so that each row begins on a boundary. In parallel systems, this prevents false sharing when multiple threads process different rows. However, padding increases memory footprint.\n\n\nLanguage-level differences.\n\nC/C++: contiguous 2D arrays (int arr[rows][cols]) guarantee row-major layout. Jagged arrays (array of pointers) sacrifice locality but allow uneven row sizes.\nFortran/MATLAB: column-major ordering dominates scientific computing, influencing algorithms in BLAS and LAPACK.\nNumPy: stores strides explicitly, enabling flexible slicing and arbitrary views. Strided slices can represent transposed matrices without copying.\n\n\n\nOptimizations.\n\nLoop tiling/blocking: partition loops into smaller blocks that fit into cache, maximizing reuse.\nSIMD-friendly layouts: structure-of-arrays (SoA) improves vectorization compared to array-of-structures (AoS).\nMatrix multiplication kernels: carefully designed to exploit cache hierarchy, prefetching, and SIMD registers.\n\n\n\nSystem-level use cases.\n\nImage processing: images stored as row-major arrays, with pixels in contiguous scanlines. Efficient filters process them row by row.\nGPU computing: memory coalescing requires threads in a warp to access contiguous memory regions; array layout directly affects throughput.\nDatabases: columnar storage uses column-major arrays, enabling fast scans and aggregation queries.\n\n\n\nPitfalls.\n\nTraversing in the “wrong” order can cause performance cliffs.\nLarge index calculations may overflow if not handled carefully.\nPorting algorithms between row-major and column-major languages can introduce subtle bugs.\n\nProfiling. Practical analysis involves comparing traversal patterns, cache miss rates, and vectorization efficiency. Modern compilers can eliminate redundant bounds checks and auto-vectorize well-structured loops, but poor layout or order can block these optimizations.\n\n\n\nWorked Example (C)\n#include &lt;stdio.h&gt;\n#define ROWS 4\n#define COLS 4\n\nint main() {\n    int arr[ROWS][COLS];\n\n    // Fill the array\n    for (int r = 0; r &lt; ROWS; r++) {\n        for (int c = 0; c &lt; COLS; c++) {\n            arr[r][c] = r * COLS + c;\n        }\n    }\n\n    // Row-major traversal (cache-friendly in C)\n    int sum_row = 0;\n    for (int r = 0; r &lt; ROWS; r++) {\n        for (int c = 0; c &lt; COLS; c++) {\n            sum_row += arr[r][c];\n        }\n    }\n\n    // Column traversal (less efficient in row-major)\n    int sum_col = 0;\n    for (int c = 0; c &lt; COLS; c++) {\n        for (int r = 0; r &lt; ROWS; r++) {\n            sum_col += arr[r][c];\n        }\n    }\n\n    printf(\"Row traversal sum: %d\\n\", sum_row);\n    printf(\"Column traversal sum: %d\\n\", sum_col);\n    return 0;\n}\nThis program highlights traversal order. On large arrays, row-major traversal is much faster in C because of cache-friendly memory access, while column traversal may cause frequent cache misses.\n\n\nWhy it matters\nMultidimensional arrays sit at the heart of performance-critical applications. Their memory layout determines how well algorithms interact with CPU caches, vector units, and GPUs. Understanding row-major vs column-major, stride penalties, and cache-aware traversal allows developers to write software that scales from toy programs to high-performance computing systems.\n\n\nExercises\n\nIn C, create a 1000×1000 matrix and measure the time difference between row-major and column traversal. Explain the results.\nIn NumPy, take a 2D array and transpose it. Use .strides to confirm that the transposed array is a view, not a copy.\nWrite the linear index formula for a 4D array (a,b,c,d) in row-major order.\nExplain how false sharing could occur when two threads update adjacent rows of a large array.\nCompare the impact of row-major vs column-major layout in matrix multiplication performance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#sparse-arrays",
    "href": "chapter_2.html#sparse-arrays",
    "title": "Chapter 2. Arrays",
    "section": "2.5 Sparse Arrays",
    "text": "2.5 Sparse Arrays\n\n2.5 L0 — Sparse Arrays as Empty Parking Lots\nA sparse array is a way of storing data when most of the positions are empty. Instead of recording every slot like in a dense array, a sparse array only remembers the places that hold actual values. You can think of a huge parking lot with only a few cars parked: a dense array writes down every spot, empty or not, while a sparse array just writes down the locations of the cars.\n\nDeep Dive\nDense arrays are straightforward: every position has a value, even if it is zero or unused. This makes access simple and fast, but wastes memory if most positions are empty. Sparse arrays solve this by storing only the useful entries.\nThere are many ways to represent a sparse array:\n\nDictionary/Map: store index → value pairs, ignoring empty slots.\nCoordinate list (COO): keep two lists, one for indices and one for values.\nRun-length encoding: store stretches of empty values as counts, followed by the next filled value.\n\nThe key idea is to save memory at the cost of more complex indexing. Access is no longer just arithmetic (arr[i]) but requires looking up in the chosen structure.\nComparison:\n\n\n\n\n\n\n\n\n\nRepresentation\nMemory Use\nAccess Speed\nGood For\n\n\n\n\nDense array\nHigh\nO(1)\nData with many filled elements\n\n\nSparse (map)\nLow\nO(1) average\nFew filled elements, random access\n\n\nSparse (list)\nVery low\nO(n)\nVery small number of entries\n\n\n\n\n\nWorked Example\n# Dense representation: wastes memory for mostly empty data\ndense = [0] * 20\ndense[3] = 10\ndense[15] = 25\nprint(\"Dense array:\", dense)\n\n# Sparse representation using dictionary\nsparse = {3: 10, 15: 25}\nprint(\"Sparse array:\", sparse)\n\n# Access value\nprint(\"Value at index 3:\", sparse.get(3, 0))\nprint(\"Value at index 7:\", sparse.get(7, 0))  # default to 0 for missing\nThis shows how a sparse dictionary only records the positions that matter, while the dense version allocates space for all 20 slots.\n\n\nWhy it matters\nSparse arrays are crucial when working with large data where most entries are empty. They save memory and make it possible to process huge datasets that would not fit into memory as dense arrays. They also appear in real-world systems like machine learning (feature vectors), scientific computing (matrices with few non-zero entries), and search engines (posting lists).\n\n\nExercises\n\nRepresent a sparse array of size 1000 with only 3 non-zero values at indices 2, 500, and 999.\nWrite a procedure to count the number of non-empty values in a sparse array.\nAccess an index that does not exist in the sparse array and explain what should be returned.\nCompare the memory used by a dense array of 1000 zeros and a sparse representation with 3 values.\nThink of a real-world example (outside programming) where recording only the “non-empty” spots is more efficient than listing everything.\n\n\n\n\n2.5 L1 — Sparse Arrays in Practice\nSparse arrays become important when dealing with very large datasets where only a few positions hold non-zero values. Instead of allocating memory for every element, practical implementations use compact structures to track only the occupied indices. This saves memory, but requires trade-offs in access speed and update complexity.\n\nDeep Dive\nThere are several practical ways to represent sparse arrays:\n\nDictionary/Hash Map\n\nStore index → value pairs.\nVery fast random access and updates (average O(1)).\nMemory overhead is higher because of hash structures.\n\nCoordinate List (COO)\n\nKeep two parallel arrays: one for indices, one for values.\nCompact, easy to construct, but access is O(n).\nGood for static data with few updates.\n\nCompressed Sparse Row (CSR) / Compressed Sparse Column (CSC)\n\nWidely used for sparse matrices.\nUse three arrays: values, column indices, and row pointers (or vice versa).\nExtremely efficient for matrix-vector operations.\nPoor at dynamic updates, since compression must be rebuilt.\n\nRun-Length Encoding (RLE)\n\nStore runs of zeros as counts, followed by non-zero entries.\nBest for sequences with long stretches of emptiness.\n\n\nA comparison:\n\n\n\n\n\n\n\n\n\nFormat\nMemory Use\nAccess Speed\nBest For\n\n\n\n\nDictionary\nHigher per-entry\nO(1) avg\nDynamic updates, unpredictable indices\n\n\nCOO\nVery low\nO(n)\nStatic, small sparse sets\n\n\nCSR/CSC\nCompact\nO(1) row scan, O(log n) col lookup\nLinear algebra, scientific computing\n\n\nRLE\nVery compact\nSequential O(n), random slower\nTime-series with long zero runs\n\n\n\n\nTrade-offs:\n\nDense arrays are fast but waste memory.\nSparse arrays save memory but access/update complexity varies.\nChoice of structure depends on workload (frequent random access vs batch computation).\n\n\n\nUse cases:\n\nMachine learning: sparse feature vectors in text classification or recommender systems.\nGraph algorithms: adjacency matrices for sparse graphs.\nSearch engines: inverted index posting lists.\nScientific computing: storing large sparse matrices for simulations.\n\n\n\n\nWorked Example\n# Sparse array using Python dictionary\nsparse = {2: 10, 100: 50, 999: 7}\n\n# Accessing\nprint(\"Value at 100:\", sparse.get(100, 0))\nprint(\"Value at 3 (missing):\", sparse.get(3, 0))\n\n# Inserting new value\nsparse[500] = 42\n\n# Traversing non-empty values\nfor idx, val in sparse.items():\n    print(f\"Index {idx} → {val}\")\nFor dense vs sparse comparison:\ndense = [0] * 1000\ndense[2], dense[100], dense[999] = 10, 50, 7\nprint(\"Dense uses 1000 slots, sparse uses\", len(sparse), \"entries\")\n\n\nWhy it matters\nSparse arrays strike a balance between memory efficiency and performance. They let you work with massive datasets that would otherwise be impossible to store in memory. They also demonstrate the importance of choosing the right representation for the problem: a dictionary for dynamic updates, CSR for scientific kernels, or RLE for compressed logs.\n\n\nExercises\n\nRepresent a sparse array of length 1000 with values at indices 2, 100, and 999 using:\n\na dictionary, and\ntwo parallel lists (indices, values).\n\nWrite a procedure that traverses only non-empty entries and prints them.\nExplain why inserting a value in CSR format is more expensive than in a dictionary-based representation.\nCompare memory usage of a dense array of length 1000 with only 5 non-zero entries against its sparse dictionary form.\nGive two real-world scenarios where CSR is preferable to dictionary-based sparse arrays.\n\n\n\n\n2.5 L2 — Sparse Arrays and Compressed Layouts in Systems\nSparse arrays are not only about saving memory; they embody deep design choices about compression, cache use, and hardware acceleration. At this level, the question is not “should I store zeros or not,” but “which representation balances memory, access speed, and computational efficiency for the workload?”\n\nDeep Dive\nSeveral compressed storage formats exist, each tuned to different needs:\n\nCOO (Coordinate List): Store parallel arrays for row indices, column indices, and values. Flexible and simple, but inefficient for repeated access because lookups require scanning.\nCSR (Compressed Sparse Row): Use three arrays: values, col_indices, and row_ptr to mark boundaries. Accessing all elements of a row is O(1), while finding a specific column in a row is O(log n) or linear. Excellent for sparse matrix-vector multiplication (SpMV).\nCSC (Compressed Sparse Column): Similar to CSR, but optimized for column operations.\nDIA (Diagonal): Only store diagonals in banded matrices. Extremely memory-efficient for PDE solvers.\nELL (Ellpack/Itpack): Store each row padded to the same length, enabling SIMD and GPU vectorization. Works well when rows have similar numbers of nonzeros.\nHYB (Hybrid, CUDA): Combines ELL for regular rows and COO for irregular cases. Used in GPU-accelerated sparse libraries.\n\n\nPerformance and Complexity.\n\nDictionaries/maps: O(1) average access, but higher overhead per entry.\nCOO: O(n) lookups, better for incremental construction.\nCSR/CSC: excellent for batch operations, poor for insertions.\nELL/DIA: high throughput on SIMD/GPU hardware but inflexible.\n\nSparse matrix-vector multiplication (SpMV) illustrates trade-offs. With CSR:\ny[row] = Σ values[k] * x[col_indices[k]]  \nwhere row_ptr guides which elements belong to each row. The cost is proportional to the number of nonzeros, but performance is limited by memory bandwidth and irregular access to x.\n\n\nCache and alignment.\nCompressed formats improve locality for sequential access but introduce irregular memory access patterns when multiplying or searching. Strided iteration can align with cache lines, but pointer-heavy layouts fragment memory. Padding (in ELL) improves SIMD alignment but wastes space.\n\n\nLanguage and library implementations.\n\nPython SciPy: csr_matrix, csc_matrix, coo_matrix, dia_matrix.\nC++: Eigen and Armadillo expose CSR and CSC; Intel MKL provides highly optimized kernels.\nCUDA/cuSPARSE: Hybrid ELL + COO kernels tuned for GPUs.\n\n\n\nSystem-level use cases.\n\nLarge-scale PDE solvers and finite element methods.\nGraph algorithms (PageRank, shortest paths) using sparse adjacency matrices.\nInverted indices in search engines (postings lists).\nFeature vectors in machine learning (bag-of-words, recommender systems).\n\n\n\nPitfalls.\n\nInsertion is expensive in compressed formats (requires shifting or rebuilding).\nConverting between formats (e.g., COO ↔︎ CSR) can dominate runtime if done repeatedly.\nA poor choice of format (e.g., using ELL for irregular sparsity) can waste memory or block vectorization.\n\n\n\nOptimization and profiling.\n\nBenchmark SpMV across formats and measure achieved bandwidth.\nProfile cache misses and TLB behavior in irregular workloads.\nOn GPUs, measure coalesced vs scattered memory access to judge format suitability.\n\n\n\nWorked Example (Python with SciPy)\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\n# Dense 5x5 with many zeros\ndense = np.array([\n    [1, 0, 0, 0, 2],\n    [0, 0, 3, 0, 0],\n    [4, 0, 0, 5, 0],\n    [0, 6, 0, 0, 0],\n    [0, 0, 0, 7, 8]\n])\n\n# Convert to CSR\nsparse = csr_matrix(dense)\n\nprint(\"CSR data array:\", sparse.data)\nprint(\"CSR indices:\", sparse.indices)\nprint(\"CSR indptr:\", sparse.indptr)\n\n# Sparse matrix-vector multiplication\nx = np.array([1, 2, 3, 4, 5])\ny = sparse @ x\nprint(\"Result of SpMV:\", y)\nThis example shows how a dense matrix with many zeros can be stored efficiently in CSR. Only nonzeros are stored, and SpMV avoids unnecessary multiplications.\n\n\n\nWhy it matters\nSparse array formats are the backbone of scientific computing, machine learning, and search engines. Choosing the right format determines whether a computation runs in seconds or hours. At scale, cache efficiency, memory bandwidth, and vectorization potential matter as much as algorithmic complexity. Sparse arrays teach the critical lesson that representation is performance.\n\n\nExercises\n\nImplement COO and CSR representations of the same sparse matrix and compare memory usage.\nWrite a small CSR-based SpMV routine and measure its speed against a dense implementation.\nExplain why ELL format is efficient on GPUs but wasteful on highly irregular graphs.\nIn SciPy, convert a csr_matrix to csc_matrix and back. Measure the cost for large matrices.\nGiven a graph with 1M nodes and 10M edges, explain why adjacency lists and CSR are more practical than dense matrices.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#prefix-sums-scans",
    "href": "chapter_2.html#prefix-sums-scans",
    "title": "Chapter 2. Arrays",
    "section": "2.6 Prefix Sums & Scans",
    "text": "2.6 Prefix Sums & Scans\n\n2.6 L0 — Running Totals\nA prefix sum, also called a scan, is a way of turning a sequence into running totals. Instead of just producing one final sum, we produce an array where each position shows the sum of all earlier elements. It is like keeping a receipt tape at the checkout: each item is added in order, and you see the growing total after each step.\n\nDeep Dive\nPrefix sums are simple but powerful. Given an array [a0, a1, a2, …, an-1], the prefix sum array [p0, p1, p2, …, pn-1] is defined as:\n\nInclusive scan:\npi = a0 + a1 + … + ai\nExclusive scan:\npi = a0 + a1 + … + ai-1\n(with p0 = 0 by convention).\n\nExample with array [1, 2, 3, 4]:\n\n\n\nIndex\nOriginal\nInclusive\nExclusive\n\n\n\n\n0\n1\n1\n0\n\n\n1\n2\n3\n1\n\n\n2\n3\n6\n3\n\n\n3\n4\n10\n6\n\n\n\nPrefix sums are built in a single pass, left to right. This is O(n) in time, requiring an extra array of length n to store results.\nOnce constructed, prefix sums allow fast range queries. For any subarray between indices i and j, the sum is:\nsum(i..j) = prefix[j] - prefix[i-1]\nThis reduces what would be O(n) work into O(1) time per query.\nPrefix sums also generalize beyond addition: they can be built with multiplication, min, max, or any associative operation.\n\n\nWorked Example\narr = [1, 2, 3, 4, 5]\n\n# Inclusive prefix sum\ninclusive = []\nrunning = 0\nfor x in arr:\n    running += x\n    inclusive.append(running)\nprint(\"Inclusive scan:\", inclusive)\n\n# Exclusive prefix sum\nexclusive = [0]\nrunning = 0\nfor x in arr[:-1]:\n    running += x\n    exclusive.append(running)\nprint(\"Exclusive scan:\", exclusive)\n\n# Range query using prefix sums\ni, j = 1, 3  # sum from index 1 to 3 (2+3+4)\nrange_sum = inclusive[j] - (inclusive[i-1] if i &gt; 0 else 0)\nprint(\"Range sum (1..3):\", range_sum)\nThis program shows inclusive and exclusive scans, and how to use them to answer range queries quickly.\n\n\nWhy it matters\nPrefix sums transform repeated work into reusable results. They make range queries efficient, reduce algorithmic complexity, and appear in countless applications: histograms, text processing, probability distributions, and parallel computing. They also introduce the idea of trading extra storage for faster queries, a common algorithmic technique.\n\n\nExercises\n\nCompute the prefix sum of [1, 2, 3, 4, 5] by hand.\nShow the difference between inclusive and exclusive prefix sums for [5, 10, 15].\nUse a prefix sum to find the sum of elements from index 2 to 4 in [3, 6, 9, 12, 15].\nGiven a prefix sum array [2, 5, 9, 14], reconstruct the original array.\nExplain why prefix sums are more efficient than computing each subarray sum from scratch when handling many queries.\n\n\n\n\n2.6 L1 — Prefix Sums in Practice\nPrefix sums are a versatile tool for speeding up algorithms that involve repeated range queries. Instead of recalculating sums over and over, we preprocess the array once to create cumulative totals. This preprocessing costs O(n), but it allows each query to be answered in O(1).\n\nDeep Dive\nA prefix sum array is built by scanning the original array from left to right:\nprefix[i] = prefix[i-1] + arr[i]\nThis produces the inclusive scan. The exclusive scan shifts everything rightward, leaving prefix[0] = 0 and excluding the current element.\nThe choice between inclusive and exclusive depends on application:\n\nInclusive is easier for direct cumulative totals.\nExclusive is more natural when answering range queries.\n\nOnce built, prefix sums enable efficient operations:\n\nRange queries: sum(i..j) = prefix[j] - prefix[i-1].\nReconstruction: the original array can be recovered with arr[i] = prefix[i] - prefix[i-1].\nGeneralization: the same idea works for multiplication (cumulative product), logical OR/AND, or even min/max. The key requirement is that the operation is associative.\n\n\nTrade-offs:\n\nBuilding prefix sums requires O(n) extra memory.\nIf only a few queries are needed, recomputing directly may be simpler.\nFor many queries, the preprocessing overhead is worthwhile.\n\n\n\nUse cases:\n\nFast range-sum queries in databases or competitive programming.\nCumulative frequencies in histograms.\nSubstring analysis in text algorithms (e.g., number of vowels in a range).\nProbability and statistics: cumulative distribution functions.\n\n\n\nLanguage implementations:\n\nPython: itertools.accumulate, numpy.cumsum.\nC++: std::partial_sum from &lt;numeric&gt;.\nJava: custom loop, or stream reductions.\n\n\n\nPitfalls:\n\nConfusing inclusive vs exclusive scans often leads to off-by-one errors.\nFor large datasets, cumulative sums may overflow fixed-width integers.\n\n\n\n\nWorked Example\nimport itertools\nimport numpy as np\n\narr = [2, 4, 6, 8, 10]\n\n# Inclusive prefix sum using Python loop\ninclusive = []\nrunning = 0\nfor x in arr:\n    running += x\n    inclusive.append(running)\nprint(\"Inclusive prefix sum:\", inclusive)\n\n# Exclusive prefix sum\nexclusive = [0]\nrunning = 0\nfor x in arr[:-1]:\n    running += x\n    exclusive.append(running)\nprint(\"Exclusive prefix sum:\", exclusive)\n\n# NumPy cumsum (inclusive)\nnp_inclusive = np.cumsum(arr)\nprint(\"NumPy inclusive scan:\", np_inclusive)\n\n# Range query using prefix sums\ni, j = 1, 3  # indices 1..3 → 4+6+8\nrange_sum = inclusive[j] - (inclusive[i-1] if i &gt; 0 else 0)\nprint(\"Range sum (1..3):\", range_sum)\n\n# Recover original array from prefix sums\nreconstructed = [inclusive[0]] + [inclusive[i] - inclusive[i-1] for i in range(1, len(inclusive))]\nprint(\"Reconstructed array:\", reconstructed)\nThis example demonstrates building prefix sums by hand, using built-in libraries, answering queries, and reconstructing the original array.\n\n\nWhy it matters\nPrefix sums reduce repeated work into reusable results. They transform O(n) queries into O(1), making algorithms faster and more scalable. They are a foundational idea in algorithm design, connecting to histograms, distributions, and dynamic programming.\n\n\nExercises\n\nBuild both inclusive and exclusive prefix sums for [5, 10, 15, 20].\nUse prefix sums to compute the sum of elements from index 2 to 4 in [1, 3, 5, 7, 9].\nGiven a prefix sum array [3, 8, 15, 24], reconstruct the original array.\nWrite a procedure that computes cumulative products (scan with multiplication).\nExplain why prefix sums are more useful when answering hundreds of queries instead of just one.\n\n\n\n\n2.6 L2 — Prefix Sums and Parallel Scans\nPrefix sums seem simple, but at scale they become a central systems primitive. They serve as the backbone of parallel algorithms, GPU kernels, and high-performance libraries. At this level, the focus shifts from “what is a prefix sum” to “how can we compute it efficiently across thousands of cores, with minimal synchronization and maximal throughput?”\n\nDeep Dive\nSequential algorithm. The simple prefix sum is O(n):\nprefix[0] = arr[0]\nfor i in 1..n-1:\n    prefix[i] = prefix[i-1] + arr[i]\nEfficient for single-threaded contexts, but inherently sequential because each value depends on the one before it.\nParallel algorithms. Two key approaches dominate:\n\nHillis–Steele scan (1986):\n\nIterative doubling method.\nAt step k, each thread adds the value from 2^k positions behind.\nO(n log n) work, O(log n) depth. Simple but not work-efficient.\n\nBlelloch scan (1990):\n\nWork-efficient, O(n) total operations, O(log n) depth.\nTwo phases:\n\nUp-sweep (reduce): build a tree of partial sums.\nDown-sweep: propagate sums back down to compute prefix results.\n\nWidely used in GPU libraries.\n\n\n\nHardware performance.\n\nCache-aware scans: memory locality matters for large arrays. Blocking and tiling reduce cache misses.\nSIMD vectorization: multiple prefix elements are computed in parallel inside CPU vector registers.\nGPUs: scans are implemented at warp and block levels, with CUDA providing primitives like thrust::inclusive_scan. Warp shuffles (__shfl_up_sync) allow efficient intra-warp scans without shared memory.\n\n\n\nMemory and synchronization.\n\nIn-place scans reduce memory use but complicate parallelization.\nExclusive vs inclusive variants require careful handling of initial values.\nSynchronization overhead and false sharing are common risks in multithreaded CPU scans.\nDistributed scans (MPI) require combining partial results from each node, then adjusting local scans with offsets.\n\n\n\nLibraries and implementations.\n\nC++ TBB: parallel_scan supports both exclusive and inclusive.\nCUDA Thrust: inclusive_scan, exclusive_scan for GPU workloads.\nOpenMP: provides #pragma omp parallel for reduction but true scans require more explicit handling.\nMPI: MPI_Scan and MPI_Exscan provide distributed prefix sums.\n\n\n\nSystem-level use cases.\n\nParallel histogramming: count frequencies in parallel, prefix sums to compute cumulative counts.\nRadix sort: scans partition data into buckets efficiently.\nStream compaction: filter elements while maintaining order.\nGPU memory allocation: prefix sums assign disjoint output positions to threads.\nDatabase indexing: scans help build offsets for columnar data storage.\n\n\n\nPitfalls.\n\nRace conditions when threads update overlapping memory.\nLoad imbalance in irregular workloads (e.g., skewed distributions).\nWrong handling of inclusive vs exclusive leads to subtle bugs in partitioning algorithms.\n\n\n\nProfiling and optimization.\n\nBenchmark sequential vs parallel scan on arrays of size 10^6 or 10^9.\nCompare scalability with 2, 4, 8, … cores.\nMeasure GPU kernel efficiency at warp, block, and grid levels.\n\n\n\n\nWorked Example (CUDA Thrust)\n#include &lt;thrust/device_vector.h&gt;\n#include &lt;thrust/scan.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    thrust::device_vector&lt;int&gt; data{1, 2, 3, 4, 5};\n\n    // Inclusive scan\n    thrust::inclusive_scan(data.begin(), data.end(), data.begin());\n    std::cout &lt;&lt; \"Inclusive scan: \";\n    for (int x : data) std::cout &lt;&lt; x &lt;&lt; \" \";\n    std::cout &lt;&lt; std::endl;\n\n    // Exclusive scan\n    thrust::device_vector&lt;int&gt; data2{1, 2, 3, 4, 5};\n    thrust::exclusive_scan(data2.begin(), data2.end(), data2.begin(), 0);\n    std::cout &lt;&lt; \"Exclusive scan: \";\n    for (int x : data2) std::cout &lt;&lt; x &lt;&lt; \" \";\n    std::cout &lt;&lt; std::endl;\n}\nThis program offloads prefix sum computation to the GPU. With thousands of threads, even huge arrays can be scanned in milliseconds.\n\n\nWhy it matters\nPrefix sums are a textbook example of how a simple algorithm scales into a building block of parallel computing. They are used in compilers, graphics, search engines, and machine learning systems. They show how rethinking algorithms for hardware (CPU caches, SIMD, GPUs, distributed clusters) leads to new designs.\n\n\nExercises\n\nImplement the Hillis–Steele scan for an array of length 16 and show each step.\nImplement the Blelloch scan in pseudocode and explain how the up-sweep and down-sweep phases work.\nBenchmark a sequential prefix sum vs an OpenMP parallel scan on 10^7 elements.\nIn CUDA, implement an exclusive scan at the warp level using shuffle instructions.\nExplain how prefix sums are used in stream compaction (removing zeros from an array while preserving order).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#deep-dive-18",
    "href": "chapter_2.html#deep-dive-18",
    "title": "Chapter 2. Arrays",
    "section": "Deep Dive",
    "text": "Deep Dive\n\n2.1 Static Arrays\n\nMemory alignment and padding in C and assembly.\nArray indexing formulas compiled into machine code.\nPage tables and kernel use of fixed-size arrays (task_struct, inode).\nVectorization of loops over static arrays (SSE/AVX).\nBounds checking elimination in high-level languages.\n\n\n\n2.2 Dynamic Arrays\n\nGrowth factor experiments: doubling vs 1.5× vs incremental.\nProfiling Python’s list growth strategy (measure capacity jumps).\nAmortized vs worst-case complexity: proofs with actual benchmarks.\nReallocation latency spikes in low-latency systems.\nComparing std::vector::reserve vs default growth.\nMemory fragmentation in long-running programs.\n\n\n\n2.3 Slices & Views\n\nSlice metadata structure in Go (ptr, len, cap).\nRust borrow checker rules for &[T] vs &mut [T].\nNumPy stride tricks: transpose as a view, not a copy.\nPerformance gap: traversing contiguous vs strided slices.\nCache/TLB impact of strided access (e.g., step=16).\nFalse sharing when two threads use overlapping slices.\n\n\n\n2.4 Multidimensional Arrays\n\nRow-major vs column-major benchmarks: traverse order timing.\nLinear index formulas for N-dimensional arrays.\nLoop tiling/blocking for matrix multiplication.\nStructure of Arrays (SoA) vs Array of Structures (AoS).\nFalse sharing and padding in multi-threaded traversal.\nBLAS/LAPACK optimizations and cache-aware kernels.\nGPU coalesced memory access in 2D/3D arrays.\n\n\n\n2.5 Sparse Arrays & Compressed Layouts\n\nCOO, CSR, CSC: hands-on with memory footprint and iteration cost.\nComparing dictionary-based vs CSR-based sparse vectors.\nParallel SpMV benchmarks on CPU vs GPU.\nDIA and ELL formats: why they shine in structured sparsity.\nHybrid GPU formats (HYB: ELL + COO).\nSearch engine inverted indices as sparse structures.\nSparse arrays in ML: bag-of-words and embeddings.\n\n\n\n2.6 Prefix Sums & Scans\n\nInclusive vs exclusive scans: correctness pitfalls.\nHillis–Steele vs Blelloch scans: step count vs work efficiency.\nCache-friendly prefix sums on CPUs (blocked scans).\nSIMD prefix sum using AVX intrinsics.\nCUDA warp shuffle scans (__shfl_up_sync).\nMPI distributed scans across clusters.\nStream compaction via prefix sums (remove zeros in O(n)).\nRadix sort built from parallel scans.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  },
  {
    "objectID": "chapter_2.html#lab",
    "href": "chapter_2.html#lab",
    "title": "Chapter 2. Arrays",
    "section": "LAB",
    "text": "LAB\n\n2.1 Static Arrays\n\nLAB 1: Implement fixed-size arrays in C and Python, compare access/update speeds.\nLAB 2: Explore how static arrays are used in Linux kernel (task_struct, page tables).\nLAB 3: Disassemble a simple loop over a static array and inspect the generated assembly.\nLAB 4: Benchmark cache effects: sequential vs random access in a large static array.\n\n\n\n2.2 Dynamic Arrays\n\nLAB 1: Implement your own dynamic array in C (with doubling strategy).\nLAB 2: Benchmark Python’s list growth by tracking capacity changes while appending.\nLAB 3: Compare growth factors: doubling vs 1.5× vs fixed increments.\nLAB 4: Stress test reallocation cost by appending millions of elements, measure latency spikes.\nLAB 5: Use std::vector::reserve in C++ and compare performance vs default growth.\n\n\n\n2.3 Slices & Views\n\nLAB 1: In Go, experiment with slice creation, capacity, and append — observe when new arrays are allocated.\nLAB 2: In Rust, create overlapping slices and see how the borrow checker enforces safety.\nLAB 3: In Python, compare slicing a list vs slicing a NumPy array — demonstrate copy vs view behavior.\nLAB 4: Benchmark stride slicing in NumPy (arr[::16]) and explain performance drop.\nLAB 5: Demonstrate aliasing bugs when two slices share the same underlying array.\n\n\n\n2.4 Multidimensional Arrays\n\nLAB 1: Write code to traverse a 1000×1000 array row by row vs column by column, measure performance.\nLAB 2: Implement your own 2D array in C using both contiguous memory and array-of-pointers, compare speed.\nLAB 3: Use NumPy to confirm row-major order with .strides, then create a column-major array and compare.\nLAB 4: Implement a tiled matrix multiplication in C/NumPy and measure cache improvement.\nLAB 5: Experiment with SoA vs AoS layouts for a struct of 3 floats (x,y,z). Measure iteration performance.\n\n\n\n2.5 Sparse Arrays & Compressed Layouts\n\nLAB 1: Implement sparse arrays with Python dict vs dense lists, compare memory usage.\nLAB 2: Build COO and CSR representations for the same matrix, print memory layout.\nLAB 3: Benchmark dense vs CSR matrix-vector multiplication.\nLAB 4: Use SciPy’s csr_matrix and csc_matrix, run queries, compare performance.\nLAB 5: Implement a simple search engine inverted index as a sparse array of word→docID list.\n\n\n\n2.6 Prefix Sums & Scans\n\nLAB 1: Write inclusive and exclusive prefix sums in Python.\nLAB 2: Benchmark prefix sums for answering 1000 range queries vs naive summation.\nLAB 3: Implement Blelloch scan in C/NumPy and visualize the up-sweep/down-sweep steps.\nLAB 4: Implement prefix sums on GPU (CUDA/Thrust), compare speed to CPU.\nLAB 5: Use prefix sums for stream compaction: remove zeros from an array while preserving order.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2. Arrays</span>"
    ]
  }
]