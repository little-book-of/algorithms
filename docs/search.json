[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of Algorithms",
    "section": "",
    "text": "Roadmap\nThe Little Book of Algorithms is a multi-volume project. Each volume has a clear sequence of chapters, and each chapter has three levels of depth (L0 beginner intuition, L1 practical techniques, L2 advanced systems/theory). This roadmap outlines the plan for development and publication.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "The Little Book of Algorithms",
    "section": "Goals",
    "text": "Goals\n\nEstablish a consistent layered structure across all chapters.\nProvide runnable implementations in Python, C, Go, Erlang, and Lean.\nEnsure Quarto build supports HTML, PDF, EPUB, and LaTeX.\nDeliver both pedagogy (L0) and production insights (L2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#volumes",
    "href": "index.html#volumes",
    "title": "The Little Book of Algorithms",
    "section": "Volumes",
    "text": "Volumes\n\nVolume I - Structures Linéaires\n\nChapter 0 - Foundations\nChapter 1 - Numbers\nChapter 2 - Arrays\nChapter 3 - Strings\nChapter 4 - Linked Lists\nChapter 5 - Stacks & Queues\n\n\n\nVolume II - Algorithmes Fondamentaux\n\nChapter 6 - Searching\nChapter 7 - Selection\nChapter 8 - Sorting\nChapter 9 - Amortized Analysis\n\n\n\nVolume III - Structures Hiérarchiques\n\nChapter 10 - Tree Fundamentals\nChapter 11 - Heaps & Priority Queues\nChapter 12 - Binary Search Trees\nChapter 13 - Balanced Trees & Ordered Maps\nChapter 14 - Range Queries\nChapter 15 - Vector Databases\n\n\n\nVolume IV - Paradigmes Algorithmiques\n\nChapter 16 - Divide-and-Conquer\nChapter 17 - Greedy\nChapter 18 - Dynamic Programming\nChapter 19 - Backtracking & Search\n\n\n\nVolume V - Graphes et Complexité\n\nChapter 20 - Graph Basics\nChapter 21 - DAGs & SCC\nChapter 22 - Shortest Paths\nChapter 23 - Flows & Matchings\nChapter 24 - Tree Algorithms\nChapter 25 - Complexity & Limits\nChapter 26 - External & Cache-Oblivious\nChapter 27 - Probabilistic & Streaming\nChapter 28 - Engineering",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#milestones",
    "href": "index.html#milestones",
    "title": "The Little Book of Algorithms",
    "section": "Milestones",
    "text": "Milestones\n\nComplete detailed outlines for all chapters (L0, L1, L2).\nWrite draft text for all L0 sections (intuition, analogies, simple examples).\nExpand each chapter with L1 content (implementations, correctness arguments, exercises).\nAdd L2 content (systems insights, proofs, optimizations, advanced references).\nDevelop and test runnable code in src/ across Python, C, Go, Erlang, and Lean.\nIntegrate diagrams, figures, and visual explanations.\nFinalize Quarto build setup for HTML, PDF, and EPUB.\nRelease first public edition (HTML + PDF).\nAdd LaTeX build, refine EPUB, and polish cross-references.\nPublish on GitHub Pages and archive DOI.\nGather feedback, refine explanations, and expand exercises/problem sets.\nLong-term: maintain as a living reference with continuous updates and companion volumes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#deliverables",
    "href": "index.html#deliverables",
    "title": "The Little Book of Algorithms",
    "section": "Deliverables",
    "text": "Deliverables\n\nQuarto project with 29 chapters (00–28).\nMulti-language reference implementations.\nLearning matrix in README for navigation.\nROADMAP.md (this file) to track progress.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "index.html#long-term-vision",
    "href": "index.html#long-term-vision",
    "title": "The Little Book of Algorithms",
    "section": "Long-term Vision",
    "text": "Long-term Vision\n\nMaintain the repository as a living reference.\nExtend with exercises, problem sets, and quizzes.\nBuild a dependency map across volumes for prerequisites.\nConnect to companion “Little Book” series (linear algebra, calculus, probability).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Roadmap</span>"
    ]
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "Chapter 1. Numbers",
    "section": "",
    "text": "1.1 Representation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#representation",
    "href": "chapter_1.html#representation",
    "title": "Chapter 1. Numbers",
    "section": "",
    "text": "1.1 L0. Decimal and Binary Basics\nA number representation is a way of writing numbers using symbols and positional rules. Humans typically use decimal notation, while computers rely on binary because it aligns with the two-state nature of electronic circuits. Understanding both systems is the first step in connecting mathematical intuition with machine computation.\n\nNumbers in Everyday Life\nHumans work with the decimal system (base 10), which uses digits 0 through 9. Each position in a number has a place value that is a power of 10.\n\\[\n427 = 4 \\times 10^2 + 2 \\times 10^1 + 7 \\times 10^0\n\\]\nThis principle of positional notation is the same idea used in other bases.\n\n\nNumbers in Computers\nComputers, however, operate in binary (base 2). A binary digit (bit) can only be 0 or 1, matching the two stable states of electronic circuits (off/on). Each binary place value represents a power of 2.\n\\[\n1011_2 = 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^0 = 11_{10}\n\\]\nJust like in decimal where \\(9 + 1 = 10\\), in binary \\(1 + 1 = 10_2\\).\n\n\nConversion Between Decimal and Binary\nTo convert from decimal to binary, repeatedly divide the number by 2 and record the remainders. Then read the remainders from bottom to top.\nExample: Convert \\(42_{10}\\) into binary.\n\n42 ÷ 2 = 21 remainder 0\n21 ÷ 2 = 10 remainder 1\n10 ÷ 2 = 5 remainder 0\n5 ÷ 2 = 2 remainder 1\n2 ÷ 2 = 1 remainder 0\n1 ÷ 2 = 0 remainder 1\n\nReading upward: \\(101010_2\\).\nTo convert from binary to decimal, expand into powers of 2 and sum:\n\\[\n101010_2 = 1 \\times 2^5 + 0 \\times 2^4 + 1 \\times 2^3 + 0 \\times 2^2 + 1 \\times 2^1 + 0 \\times 2^0 = 42_{10}\n\\]\n\n\nWorked Example (Python)\nn = 42\nprint(\"Decimal:\", n)\nprint(\"Binary :\", bin(n))   # 0b101010\n\n# binary literal in Python\nb = 0b101010\nprint(\"Binary literal:\", b)\n\n# converting binary string to decimal\nprint(\"From binary '1011':\", int(\"1011\", 2))\nOutput:\nDecimal: 42\nBinary : 0b101010\nBinary literal: 42\nFrom binary '1011': 11\n\n\nWhy It Matters\n\nAll information inside a computer — numbers, text, images, programs — reduces to binary representation.\nDecimal and binary conversions are the first bridge between human-friendly math and machine-level data.\nUnderstanding binary is essential for debugging, low-level programming, and algorithms that depend on bit operations.\n\n\n\nExercises\n\nWrite the decimal number 19 in binary.\nConvert the binary number 10101₂ into decimal.\nShow the repeated division steps to convert 27 into binary.\nVerify in Python that 0b111111 equals 63.\nExplain why computers use binary instead of decimal.\n\n\n\n\n1.1 L1. Beyond Binary: Octal, Hex, and Two’s Complement\nNumbers are not always written in base-10 or even in base-2. For efficiency and compactness, programmers often use octal (base-8) and hexadecimal (base-16). At the same time, negative numbers must be represented reliably; modern computers use two’s complement for this purpose.\n\nOctal and Hexadecimal\nOctal and hex are simply alternate numeral systems.\n\nOctal (base 8): digits 0–7.\nHexadecimal (base 16): digits 0–9 plus A–F.\n\nWhy they matter:\n\nHex is concise: one hex digit = 4 binary bits.\nOctal was historically convenient: one octal digit = 3 binary bits (useful on early 12-, 24-, or 36-bit machines).\n\nFor example, the number 42 is written as:\n\n\n\nDecimal\nBinary\nOctal\nHex\n\n\n\n\n42\n101010\n52\n2A\n\n\n\n\n\nTwo’s Complement\nTo represent negative numbers, we cannot just “stick a minus sign” in memory. Instead, binary uses two’s complement:\n\nChoose a fixed bit-width (say 8 bits).\nFor a negative number -x, compute 2^bits - x.\nStore the result as an ordinary binary integer.\n\nExample with 8 bits:\n\n+5 → 00000101\n-5 → 11111011\n-1 → 11111111\n\nWhy two’s complement is powerful:\n\nAddition and subtraction “just work” with the same circuitry for signed and unsigned.\nThere is only one representation of zero.\n\n\n\nWorking Example (Python)\n# Decimal 42 in different bases\nn = 42\nprint(\"Decimal:\", n)\nprint(\"Binary :\", bin(n))\nprint(\"Octal  :\", oct(n))\nprint(\"Hex    :\", hex(n))\n\n# Two's complement for -5 in 8 bits\ndef to_twos_complement(x: int, bits: int = 8) -&gt; str:\n    if x &gt;= 0:\n        return format(x, f\"0{bits}b\")\n    return format((1 &lt;&lt; bits) + x, f\"0{bits}b\")\n\nprint(\"+5:\", to_twos_complement(5, 8))\nprint(\"-5:\", to_twos_complement(-5, 8))\nOutput:\nDecimal: 42\nBinary : 0b101010\nOctal  : 0o52\nHex    : 0x2a\n+5: 00000101\n-5: 11111011\n\n\nWhy It Matters\n\nProgrammer convenience: Hex makes binary compact and human-readable.\nHardware design: Two’s complement ensures arithmetic circuits are simple and unified.\nDebugging: Memory dumps, CPU registers, and network packets are usually shown in hex.\n\n\n\nExercises\n\nConvert 100 into binary, octal, and hex.\nWrite -7 in 8-bit two’s complement.\nVerify that 0xFF is equal to 255.\nParse the bitstring \"11111001\" as an 8-bit two’s complement number.\nExplain why engineers prefer two’s complement over “sign-magnitude” representation.\n\n\n\n\n1.1 L2. Floating-Point and Precision Issues\nNot all numbers are integers. To approximate fractions, scientific notation, and very large or very small values, computers use floating-point representation. The de-facto standard is IEEE-754, which defines how real numbers are encoded, how special values are handled, and what precision guarantees exist.\n\nStructure of Floating-Point Numbers\nA floating-point value is composed of three fields:\n\nSign bit (s) — indicates positive (0) or negative (1).\nExponent (e) — determines the scale or “magnitude.”\nMantissa / significand (m) — contains the significant digits.\n\nThe value is interpreted as:\n\\[\n(-1)^s \\times 1.m \\times 2^{(e - \\text{bias})}\n\\]\nExample: IEEE-754 single precision (32 bits)\n\n1 sign bit\n8 exponent bits (bias = 127)\n23 mantissa bits\n\n\n\nExact vs Approximate Representation\nSome numbers are represented exactly:\n\n1.0 has a clean binary form.\n\nOthers cannot be represented precisely:\n\n0.1 in decimal is a repeating fraction in binary, so the closest approximation is stored.\n\nPython example:\na = 0.1 + 0.2\nprint(\"0.1 + 0.2 =\", a)\nprint(\"Equal to 0.3?\", a == 0.3)\nOutput:\n0.1 + 0.2 = 0.30000000000000004\nEqual to 0.3? False\n\n\nSpecial Values\nIEEE-754 reserves encodings for special cases:\n\n\n\nSign\nExponent\nMantissa\nMeaning\n\n\n\n\n0/1\nall 1s\n0\n+∞ / −∞\n\n\n0/1\nall 1s\nnonzero\nNaN (Not a Number)\n\n\n0/1\nall 0s\nnonzero\nDenormals (gradual underflow)\n\n\n\nExamples:\n\nDivision by zero produces infinity: 1.0 / 0.0 = inf.\n0.0 / 0.0 yields NaN, which propagates in computations.\nDenormals allow gradual precision near zero.\n\n\n\nArbitrary Precision\nLanguages like Python and libraries like GMP provide arbitrary-precision arithmetic:\n\nIntegers (int) can grow as large as memory allows.\nDecimal libraries (decimal.Decimal in Python) allow exact decimal arithmetic.\nThese are slower, but essential for cryptography, symbolic computation, and finance.\n\n\n\nWorked Example (Python)\nimport math\n\nprint(\"Infinity:\", 1.0 / 0.0)\nprint(\"NaN:\", 0.0 / 0.0)\n\nprint(\"Is NaN?\", math.isnan(float('nan')))\nprint(\"Is Inf?\", math.isinf(float('inf')))\n\n# Arbitrary precision integer\nbig = 2200\nprint(\"2200 =\", big)\n\n\nWhy It Matters\n\nRounding surprises: Many decimal fractions cannot be represented exactly.\nError propagation: Repeated arithmetic may accumulate tiny inaccuracies.\nSpecial values: NaN and infinity must be handled carefully.\nDomain correctness: Cryptography, finance, and symbolic algebra require exact precision.\n\n\n\nExercises\n\nWrite down the IEEE-754 representation (sign, exponent, mantissa) of 1.0.\nExplain why 0.1 is not exactly representable in binary.\nTest in Python whether float('nan') == float('nan'). What happens, and why?\nFind the smallest positive number you can add to 1.0 before it changes (machine epsilon).\nWhy is arbitrary precision slower but critical in some applications?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#basic-operations",
    "href": "chapter_1.html#basic-operations",
    "title": "Chapter 1. Numbers",
    "section": "1.2 Basic Operations",
    "text": "1.2 Basic Operations\n\n1.2 L0. Addition, Subtraction, Multiplication, Division\nAn arithmetic operation combines numbers to produce a new number. At this level we focus on four basics: addition, subtraction, multiplication, and division—first with decimal intuition, then a peek at how the same ideas look in binary. Mastering these is essential before moving to algorithms that build on them.\n\nIntuition: place value + carrying/borrowing\nAll four operations are versions of combining place values (ones, tens, hundreds …; or in binary: ones, twos, fours …).\n\nAddition: add column by column; if a column exceeds the base, carry 1 to the next column.\nSubtraction: subtract column by column; if a column is too small, borrow 1 from the next column.\nMultiplication: repeated addition; multiply by each digit and shift (place value), then add partial results.\nDivision: repeated subtraction or sharing; find how many times a number “fits,” track the remainder.\n\nThese rules are identical in any base. Only the place values change.\n\n\nDecimal examples (by hand)\n\nAddition (carry)\n\n   478\n + 259\n ----\n   737    (8+9=17 → write 7, carry 1; 7+5+1=13 → write 3, carry 1; 4+2+1=7)\n\nSubtraction (borrow)\n\n   503\n -  78\n ----\n   425    (3-8 borrow → 13-8=5; 0 became -1 so borrow from 5 → 9-7=2; 4 stays 4)\n\nMultiplication (partial sums)\n\n   214\n ×   3\n ----\n   642    (214*3 = 642)\n\nLong division (quotient + remainder)\n\n  47 ÷ 5 → 9 remainder 2   (because 5*9 = 45, leftover 2)\n\n\nBinary peek (same rules, base 2)\n\nAdd rules: 0+0=0, 0+1=1, 1+0=1, 1+1=10₂ (write 0, carry 1)\nSubtract rules: 0−0=0, 1−0=1, 1−1=0, 0−1 → borrow (becomes 10₂−1=1, borrow 1)\n\nExample: \\(1011₂ + 0110₂\\)\n   1011\n + 0110\n ------\n  10001   (1+0=1; 1+1=0 carry1; 0+1+carry=0 carry1; 1+0+carry=0 carry1 → carry out)\n\n\nWorked examples (Python)\n# Basic arithmetic with integers\na, b = 478, 259\nprint(\"a+b =\", a + b)      # 737\nprint(\"a-b =\", a - b)      # 219\nprint(\"a*b =\", a * b)      # 123,  478*259 = 123,  ... actually compute:\nprint(\"47//5 =\", 47 // 5)  # integer division -&gt; 9\nprint(\"47%5  =\", 47 % 5)   # remainder -&gt; 2\n\n# Show carry/borrow intuition using binary strings\nx, y = 0b1011, 0b0110\ns = x + y\nprint(\"x+y (binary):\", bin(x), \"+\", bin(y), \"=\", bin(s))\n\n# Small helper: manual long division that returns (quotient, remainder)\ndef long_divide(n: int, d: int):\n    if d == 0:\n        raise ZeroDivisionError(\"division by zero\")\n    q = n // d\n    r = n % d\n    return q, r\n\nprint(\"long_divide(47,5):\", long_divide(47, 5))  # (9, 2)\n\nNote: // is integer division in Python; % is the remainder. For now we focus on integers (no decimals).\n\n\n\nWhy it matters\n\nEvery higher-level algorithm (searching, hashing, cryptography, numeric methods) relies on these operations.\nUnderstanding carry/borrow makes binary arithmetic and bit-level reasoning feel natural.\nKnowing integer division and remainder is vital for base conversions, hashing (mod), and many algorithmic patterns.\n\n\n\nExercises\n\nCompute by hand, then verify in Python:\n\n\\(326 + 589\\)\n\\(704 - 259\\)\n\\(38 \\times 12\\)\n\\(123 \\div 7\\) (give quotient and remainder)\n\nIn binary, add \\(10101₂ + 111₍₂₎\\). Show carries.\nWrite a short Python snippet that prints the quotient and remainder for n=200 divided by d=23.\nConvert your remainder into a sentence: “200 = 23 × (quotient) + (remainder)”.\nChallenge: Multiply \\(19 \\times 23\\) by hand using partial sums; then check with Python.\n\n\n\n\n1.2 L1. Division, Modulo, and Efficiency\nBeyond the simple four arithmetic operations, programmers need to think about division with remainder, the modulo operator, and how efficient these operations are on real machines. Addition and subtraction are almost always “constant time,” but division can be slower, and understanding modulo is essential for algorithms like hashing, cryptography, and scheduling.\n\nInteger Division and Modulo\nFor integers, division produces both a quotient and a remainder.\n\nMathematical definition: for integers \\(n, d\\) with \\(d \\neq 0\\),\n\\[\nn = d \\times q + r, \\quad 0 \\leq r &lt; |d|\n\\]\nwhere \\(q\\) is the quotient, \\(r\\) the remainder.\nProgramming notation (Python):\n\nn // d → quotient\nn % d → remainder\n\n\nExamples:\n\n47 // 5 = 9, 47 % 5 = 2 because \\(47 = 5 \\times 9 + 2\\).\n23 // 7 = 3, 23 % 7 = 2 because \\(23 = 7 \\times 3 + 2\\).\n\n\n\n\nn\nd\nn // d\nn % d\n\n\n\n\n47\n5\n9\n2\n\n\n23\n7\n3\n2\n\n\n100\n9\n11\n1\n\n\n\n\n\nModulo in Algorithms\nThe modulo operation is a workhorse in programming:\n\nHashing: To map a large integer into a table of size m, use key % m.\nCyclic behavior: To loop back after 7 days in a week: (day + shift) % 7.\nCryptography: Modular arithmetic underlies RSA, Diffie–Hellman, and many number-theoretic algorithms.\n\n\n\nEfficiency Considerations\n\nAddition and subtraction: generally 1 CPU cycle.\nMultiplication: slightly more expensive, but still fast on modern hardware.\nDivision and modulo: slower, often an order of magnitude more costly than multiplication.\n\nPractical tricks:\n\nIf d is a power of two, n % d can be computed by a bitmask.\n\nExample: n % 8 == n & 7 (since 8 = 2³).\n\nSome compilers automatically optimize modulo when the divisor is constant.\n\n\n\nWorked Example (Python)\n# Quotient and remainder\nn, d = 47, 5\nprint(\"Quotient:\", n // d)  # 9\nprint(\"Remainder:\", n % d)  # 2\n\n# Identity check: n == d*q + r\nq, r = divmod(n, d)  # built-in tuple return\nprint(\"Check:\", d*q + r == n)\n\n# Modulo for cyclic behavior: days of week\ndays = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nstart = 5  # Saturday\nshift = 4\nfuture_day = days[(start + shift) % 7]\nprint(\"Start Saturday + 4 days =\", future_day)\n\n# Optimization: power-of-two modulo with bitmask\nfor n in [5, 12, 20]:\n    print(f\"{n} % 8 = {n % 8}, bitmask {n & 7}\")\nOutput:\nQuotient: 9\nRemainder: 2\nCheck: True\nStart Saturday + 4 days = Wed\n5 % 8 = 5, bitmask 5\n12 % 8 = 4, bitmask 4\n20 % 8 = 4, bitmask 4\n\n\nWhy It Matters\n\nReal programs rely heavily on modulo for indexing, hashing, and wrap-around logic.\nDivision is computationally more expensive; knowing when to replace it with bit-level operations improves performance.\nModular arithmetic introduces a new “world” where numbers wrap around — the foundation of many advanced algorithms.\n\n\n\nExercises\n\nCompute by hand and confirm in Python:\n\n100 // 9 and 100 % 9\n123 // 11 and 123 % 11\n\nWrite a function that simulates a clock: given hour and shift, return the new hour (24-hour cycle).\nProve the identity: for any integers n and d,\nn == d * (n // d) + (n % d)\nby trying with random values.\nShow how to replace n % 16 with a bitwise operation. Why does it work?\nChallenge: Write a short Python function to check if a number is divisible by 7 using only % and //.\n\n\n\n\n1.2 L2. Fast Arithmetic Algorithms\nWhen numbers grow large, the naïve methods for multiplication and division become too slow. On paper, long multiplication takes \\(O(n^2)\\) steps for \\(n\\)-digit numbers. Computers face the same issue: multiplying two very large integers digit by digit can be expensive. Fast arithmetic algorithms reduce this cost, using clever divide-and-conquer techniques or transformations into other domains.\n\nMultiplication Beyond the School Method\nNaïve long multiplication\n\nTreats an \\(n\\)-digit number as a sequence of digits.\nEach digit of one number multiplies every digit of the other.\nComplexity: \\(O(n^2)\\).\nWorks fine for small integers, but too slow for cryptography or big-number libraries.\n\nKaratsuba’s Algorithm\n\nDiscovered in 1960 by Anatoly Karatsuba.\nIdea: split numbers into halves and reduce multiplications.\nComplexity: \\(O(n^{\\log_2 3}) \\approx O(n^{1.585})\\).\nRecursive strategy:\n\nFor numbers \\(x = x_1 \\cdot B^m + x_0\\), \\(y = y_1 \\cdot B^m + y_0\\).\nCompute 3 multiplications instead of 4:\n\n\\(z_0 = x_0 y_0\\)\n\\(z_2 = x_1 y_1\\)\n\\(z_1 = (x_0+x_1)(y_0+y_1) - z_0 - z_2\\)\n\nResult: \\(z_2 \\cdot B^{2m} + z_1 \\cdot B^m + z_0\\).\n\n\nFFT-based Multiplication (Schönhage–Strassen and successors)\n\nRepresent numbers as polynomials of their digits.\nMultiply polynomials efficiently using Fast Fourier Transform.\nComplexity: near \\(O(n \\log n)\\).\nUsed in modern big-integer libraries (e.g. GNU MP, Java’s BigInteger).\n\n\n\nDivision Beyond Long Division\n\nNaïve long division: \\(O(n^2)\\) for \\(n\\)-digit dividend.\nNewton’s method for reciprocal: approximate \\(1/d\\) using Newton–Raphson iterations, then multiply by \\(n\\).\nComplexity: tied to multiplication — if multiplication is fast, so is division.\n\n\n\nModular Exponentiation\nFast arithmetic also matters in modular contexts (cryptography).\n\nCompute \\(a^b \\bmod m\\) efficiently.\nSquare-and-multiply (binary exponentiation):\n\nWrite \\(b\\) in binary.\nFor each bit: square result, multiply if bit=1.\nComplexity: \\(O(\\log b)\\) multiplications.\n\n\n\n\nWorked Example (Python)\n# Naïve multiplication\ndef naive_mul(x: int, y: int) -&gt; int:\n    return x * y  # Python already uses fast methods internally\n\n# Karatsuba multiplication (recursive, simplified)\ndef karatsuba(x: int, y: int) -&gt; int:\n    # base case\n    if x &lt; 10 or y &lt; 10:\n        return x * y\n    # split numbers\n    n = max(x.bit_length(), y.bit_length())\n    m = n // 2\n    high1, low1 = divmod(x, 1 &lt;&lt; m)\n    high2, low2 = divmod(y, 1 &lt;&lt; m)\n    z0 = karatsuba(low1, low2)\n    z2 = karatsuba(high1, high2)\n    z1 = karatsuba(low1 + high1, low2 + high2) - z0 - z2\n    return (z2 &lt;&lt; (2*m)) + (z1 &lt;&lt; m) + z0\n\n# Modular exponentiation (square-and-multiply)\ndef modexp(a: int, b: int, m: int) -&gt; int:\n    result = 1\n    base = a % m\n    exp = b\n    while exp &gt; 0:\n        if exp & 1:\n            result = (result * base) % m\n        base = (base * base) % m\n        exp &gt;&gt;= 1\n    return result\n\n# Demo\nprint(\"Karatsuba(1234, 5678) =\", karatsuba(1234, 5678))\nprint(\"pow(7, 128, 13) =\", modexp(7, 128, 13))  # fast modular exponentiation\nOutput:\nKaratsuba(1234, 5678) = 7006652\npow(7, 128, 13) = 3\n\n\nWhy It Matters\n\nCryptography: RSA requires multiplying and dividing integers with thousands of digits.\nComputer algebra systems: symbolic computation depends on fast polynomial/integer arithmetic.\nBig data / simulation: arbitrary precision needed when floats are not exact.\n\n\n\nExercises\n\nMultiply 31415926 × 27182818 using:\n\nPython’s *\nYour Karatsuba implementation. Compare results.\n\nImplement modexp(a, b, m) for \\(a=5, b=117, m=19\\). Confirm with Python’s built-in pow(a, b, m).\nExplain why Newton’s method for division depends on fast multiplication.\nResearch: what is the current fastest known multiplication algorithm for large integers?\nChallenge: Modify Karatsuba to print intermediate z0, z1, z2 values for small inputs to visualize the recursion.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#properties",
    "href": "chapter_1.html#properties",
    "title": "Chapter 1. Numbers",
    "section": "1.3 Properties",
    "text": "1.3 Properties\n\n1.3 L0 — Simple Number Properties\nNumbers have patterns that help us reason about algorithms without heavy mathematics. At this level we focus on basic properties: even vs odd, divisibility, and remainders. These ideas show up everywhere—from loop counters to data structure layouts.\n\nEven and Odd\nA number is even if it ends with digit 0, 2, 4, 6, or 8 in decimal, and odd otherwise.\n\nIn binary, checking parity is even easier: the last bit tells the story.\n\n…0 → even\n…1 → odd\n\n\nExample in Python:\ndef is_even(n: int) -&gt; bool:\n    return n % 2 == 0\n\nprint(is_even(10))  # True\nprint(is_even(7))   # False\n\n\nDivisibility\nWe often ask: does one number divide another?\n\na is divisible by b if there exists some integer k with a = b * k.\nIn code: a % b == 0.\n\nExamples:\n\n12 is divisible by 3 → 12 % 3 == 0.\n14 is not divisible by 5 → 14 % 5 == 4.\n\n\n\nRemainders and Modular Thinking\nWhen dividing, the remainder is what’s left over.\n\nExample: 17 // 5 = 3, remainder 2.\nModular arithmetic wraps around like a clock:\n\n(17 % 5) = 2 → same as “2 o’clock after going 17 steps around a 5-hour clock.”\n\n\nThis “wrap-around” view is central in array indexing, hashing, and cryptography later on.\n\n\nWhy It Matters\n\nAlgorithms: Parity checks decide branching (e.g., even-odd optimizations).\nData structures: Array indices often wrap around using %.\nEveryday: Calendars cycle days of the week; remainders formalize that.\n\n\n\nExercises\n\nWrite a function that returns \"even\" or \"odd\" for a given number.\nCheck if 91 is divisible by 7.\nCompute the remainder of 100 divided by 9.\nUse % to simulate a 7-day week: if today is day 5 (Saturday) and you add 10 days, what day is it?\nFind the last digit of 2^15 without computing the full number (hint: check the remainder mod 10).\n\n\n\n\n1.3 L1 — Classical Number Theory Tools\nBeyond simple parity and divisibility, algorithms often need deeper number properties. At this level we introduce a few “toolkit” ideas from elementary number theory: greatest common divisor (GCD), least common multiple (LCM), and modular arithmetic identities. These are lightweight but powerful concepts that show up in algorithm design, cryptography, and optimization.\n\nGreatest Common Divisor (GCD)\nThe GCD of two numbers is the largest number that divides both.\n\nExample: gcd(20, 14) = 2.\nWhy useful: GCD simplifies fractions, ensures ratios are reduced, and appears in algorithm correctness proofs.\n\nEuclid’s Algorithm: Instead of trial division, we can compute GCD quickly:\ngcd(a, b) = gcd(b, a % b)\nThis repeats until b = 0, at which point a is the answer.\nPython example:\ndef gcd(a: int, b: int) -&gt; int:\n    while b:\n        a, b = b, a % b\n    return a\n\nprint(gcd(20, 14))  # 2\n\n\nLeast Common Multiple (LCM)\nThe LCM of two numbers is the smallest positive number divisible by both.\n\nExample: lcm(12, 18) = 36.\nConnection to GCD:\nlcm(a, b) = (a * b) // gcd(a, b)\n\nThis is useful in scheduling, periodic tasks, and synchronization problems.\n\n\nModular Arithmetic Identities\nRemainders behave predictably under operations:\n\nAddition: (a + b) % m = ((a % m) + (b % m)) % m\nMultiplication: (a * b) % m = ((a % m) * (b % m)) % m\n\nExample:\n\n(123 + 456) % 7 = (123 % 7 + 456 % 7) % 7\nThis property lets us work with small remainders instead of huge numbers, key in cryptography and hashing.\n\n\n\nWhy It Matters\n\nAlgorithms: GCD ensures efficiency in fraction reduction, graph algorithms, and number-theoretic algorithms.\nSystems: LCM models periodicity, e.g., aligning CPU scheduling intervals.\nCryptography: Modular arithmetic underpins secure communication (RSA, Diffie-Hellman).\nPractical programming: Modular identities simplify computations with limited ranges (hash tables, cyclic arrays).\n\n\n\nExercises\n\nCompute gcd(252, 198) by hand using Euclid’s algorithm.\nWrite a function that returns the LCM of two numbers. Test it on (12, 18).\nShow that (37 + 85) % 12 equals ((37 % 12) + (85 % 12)) % 12.\nReduce the fraction 84/126 using GCD.\nFind the smallest day d such that d is a multiple of both 12 and 18 (hint: LCM).\n\n\n\n\n1.3 L2 — Advanced Number Theory in Algorithms\nAt this level, we move beyond everyday divisibility and Euclid’s algorithm. Modern algorithms frequently rely on deep number theory to achieve efficiency. Topics such as modular inverses, Euler’s totient function, and primality tests are crucial foundations for cryptography, randomized algorithms, and competitive programming.\n\nModular Inverses\nThe modular inverse of a number a (mod m) is an integer x such that:\n(a * x) % m = 1\n\nExample: the inverse of 3 modulo 7 is 5, because (3*5) % 7 = 15 % 7 = 1.\nExistence: an inverse exists if and only if gcd(a, m) = 1.\nComputation: using the Extended Euclidean Algorithm.\n\nThis is the backbone of modular division and is heavily used in cryptography (RSA), hash functions, and matrix inverses mod p.\n\n\nEuler’s Totient Function (φ)\nThe function φ(n) counts the number of integers between 1 and n that are coprime to n.\n\nExample: φ(9) = 6 because {1, 2, 4, 5, 7, 8} are coprime to 9.\nKey property (Euler’s theorem):\na^φ(n) ≡ 1 (mod n)     if gcd(a, n) = 1\nSpecial case: Fermat’s Little Theorem — for prime p,\na^(p-1) ≡ 1 (mod p)\n\nThis result is central in modular exponentiation and cryptosystems like RSA.\n\n\nPrimality Testing\nDetermining if a number is prime is easy for small inputs but hard for large ones. Efficient algorithms are essential:\n\nTrial division: works only for small n.\nFermat primality test: uses Fermat’s Little Theorem to detect composites, but can be fooled by Carmichael numbers.\nMiller–Rabin test: a probabilistic algorithm widely used in practice (cryptographic key generation).\nAKS primality test: a deterministic polynomial-time method (theoretical importance).\n\nExample intuition:\n\nFor large n, we don’t check all divisors; we test properties of a^k mod n for random bases a.\n\n\n\nWhy It Matters\n\nCryptography: Public-key systems depend on modular inverses, Euler’s theorem, and large primes.\nAlgorithms: Modular inverses simplify solving equations in modular arithmetic (e.g., Chinese Remainder Theorem applications).\nPractical Computing: Randomized primality tests (like Miller–Rabin) balance correctness and efficiency in real-world systems.\n\n\n\nExercises\n\nFind the modular inverse of 7 modulo 13.\nCompute φ(10) and verify Euler’s theorem for a = 3.\nUse Fermat’s test to check whether 341 is prime. (Hint: try a = 2.)\nImplement modular inverse using the Extended Euclidean Algorithm.\nResearch: why do cryptographic protocols prefer Miller–Rabin over AKS, even though AKS is deterministic?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  },
  {
    "objectID": "chapter_1.html#overflow-precision",
    "href": "chapter_1.html#overflow-precision",
    "title": "Chapter 1. Numbers",
    "section": "1.4 Overflow & Precision",
    "text": "1.4 Overflow & Precision\n\n1.4 L0 - When Numbers Get Too Big or Too Small\nNumbers inside a computer are stored with a fixed number of bits. This means they can only represent values up to a certain limit. If a calculation produces a result larger than this limit, the value “wraps around,” much like the digits on an odometer rolling over after 999 to 000. This phenomenon is called overflow. Similarly, computers often cannot represent all decimal fractions exactly, leading to tiny errors called precision loss.\n\nDeep Dive\n\nInteger Overflow\n\nA computer uses a fixed number of bits (commonly 8, 16, 32, or 64) to store integers.\nAn 8-bit unsigned integer can represent values from 0 to 255. Adding 1 to 255 causes the value to wrap back to 0.\nSigned integers use two’s complement representation. For an 8-bit signed integer, the range is −128 to +127. Adding 1 to 127 makes it overflow to −128.\n\nExample in binary:\n11111111₂ (255) + 1 = 00000000₂ (0)\n01111111₂ (+127) + 1 = 10000000₂ (−128)\nFloating-Point Precision\n\nDecimal fractions like 0.1 cannot always be represented exactly in binary.\nAs a result, calculations may accumulate tiny errors.\nFor example, repeatedly adding 0.1 may not exactly equal 1.0 due to precision limits.\n\n\n\n\nExample\n# Integer overflow simulation with 8-bit values\ndef add_8bit(a, b):\n    result = (a + b) % 256  # simulate wraparound\n    return result\n\nprint(add_8bit(250, 10))   # 260 wraps to 4\nprint(add_8bit(255, 1))    # wraps to 0\n\n# Floating-point precision issue\nx = 0.1 + 0.2\nprint(x)           # Expected 0.3, but gives 0.30000000000000004\nprint(x == 0.3)    # False\n\n\nWhy It Matters\n\nUnexpected results: A calculation may suddenly produce a negative number or wrap around to zero.\nReal-world impact:\n\nVideo games may show scores jumping strangely if counters overflow.\nBanking or financial systems must avoid losing cents due to floating-point errors.\nEngineers and scientists rely on careful handling of precision to ensure correct simulations.\n\nFoundation for algorithms: Understanding overflow and precision prepares you for later topics like hashing, cryptography, and numerical analysis.\n\n\n\nExercises\n\nSimulate a 4-bit unsigned integer system. What happens if you start at 14 and keep adding 1?\nIn Python, try adding 0.1 to itself ten times. Does it equal exactly 1.0? Why or why not?\nWrite a function that checks if an 8-bit signed integer addition would overflow.\nImagine you are programming a digital clock that uses 2 digits for minutes (00–59). What happens when the value goes from 59 to 60? How would you handle this?\n\n\n\n\n1.4 L1 - Detecting and Managing Overflow in Real Programs\nComputers don’t do math in the abstract. Integers live in fixed-width registers; floats follow IEEE-754. Robust software accounts for these limits up front: choose the right representation, detect overflow, and compare floats safely. The following sections explain how these issues show up in practice and how to design around them.\n\nDeep Dive\n\n1) Integer arithmetic in practice\nFixed width means wraparound at \\(2^{n}\\). Unsigned wrap is modular arithmetic; signed overflow (two’s complement) can flip signs. Developers often discover this the hard way when a counter suddenly goes negative or wraps back to zero in production logs.\nBit width & ranges This table reminds us of the hard limits baked into hardware. Once the range is exceeded, the value doesn’t “grow bigger”—it wraps.\n\n\n\n\n\n\n\n\nWidth\nSigned range\nUnsigned range\n\n\n\n\n32\n−2,147,483,648 … 2,147,483,647\n0 … 4,294,967,295\n\n\n64\n−9,223,372,036,854,775,808 … 9,223,372,036,854,775,807\n0 … 18,446,744,073,709,551,615\n\n\n\nOverflow semantics by language Each language makes slightly different promises. This matters if you’re writing cross-language services or reading binary data across APIs.\n\n\n\n\n\n\n\n\n\nLanguage\nSigned overflow\nUnsigned overflow\nNotes\n\n\n\n\nC/C++\nUB (undefined)\nModular wrap\nUse UBSan/-fsanitize=undefined; widen types or check before add.\n\n\nRust\nTraps in debug; defined APIs\nwrapping_add, checked_add, saturating_add\nMake intent explicit.\n\n\nJava/Kotlin\nWraps (two’s complement)\nN/A (only signed types)\nUse Math.addExact to trap.\n\n\nC#\nWraps by default; checked to trap\nchecked/unchecked blocks\ndecimal type for money.\n\n\nPython\nArbitrary precision\nArbitrary precision\nSimulates fixed width if needed.\n\n\n\nA quick lesson: “wrap” may be safe in crypto or hashing, but it’s usually a bug in counters or indices. Always decide what you want up front.\n\n\n2) Floating-point you can depend on\nIEEE-754 doubles have ~15–16 decimal digits and huge dynamic range, but not exact decimal fractions. Think of floats as convenient approximations. They are perfect for physics simulations, but brittle when used to represent cents in a bank account.\nWhere precision is lost These examples show why “0.1 + 0.2 != 0.3” isn’t a joke—it’s a direct consequence of binary storage.\n\nScale mismatch: \\(1\\text{e}16 + 1 = 1\\text{e}16\\). The tiny +1 gets lost.\nCancellation: subtracting nearly equal numbers deletes significant digits.\nDecimal fractions (0.1) are repeating in binary.\n\nComparing floats Never compare with ==. Instead, use a mixed relative + absolute check:\n\\[\n|x-y| \\le \\max(\\text{rel}\\cdot\\max(|x|,|y|),\\ \\text{abs})\n\\]\nThis makes comparisons robust whether you’re near zero or far away.\nRounding modes (when you explicitly care) Most of the time you don’t think about rounding—hardware defaults to “round to nearest, ties to even.” But when writing financial systems or interval arithmetic, you want to control it.\n\n\n\n\n\n\n\nMode\nTypical use\n\n\n\n\nRound to nearest, ties to even (default)\nGeneral numeric work; minimizes bias\n\n\nToward 0 / ±∞\nBounds, interval arithmetic, conservative estimates\n\n\n\nHaving explicit rounding modes is like having a steering wheel—you don’t always turn, but you’re glad it’s there when the road curves.\nSummation strategies The order of addition matters for floats. These options give you a menu of accuracy vs. speed.\n\n\n\n\n\n\n\n\n\nMethod\nError\nCost\nWhen to use\n\n\n\n\nNaïve left-to-right\nWorst\nLow\nNever for sensitive sums\n\n\nPairwise / tree\nBetter\nMed\nParallel reductions, “good default”\n\n\nKahan (compensated)\nBest\nHigher\nFinancial-ish aggregates, small vectors\n\n\n\nYou don’t need Kahan everywhere, but knowing it exists keeps you from blaming “mystery bugs” on hardware.\nRepresentation choices Sometimes the best answer is to avoid floats entirely. Money is the classic example.\n\n\n\n\n\n\n\nUse case\nRecommended representation\n\n\n\n\nCurrency, invoicing\nFixed-point (e.g., cents as int64) or decimal/BigDecimal\n\n\nScientific compute\nfloat64, compensated sums, stable algorithms\n\n\nIDs, counters\nuint64/int64, detect overflow on boundaries\n\n\n\n\n\n\nCode (Python—portable patterns)\n# 32-bit checked add (raises on overflow)\ndef add_i32_checked(a: int, b: int) -&gt; int:\n    s = a + b\n    if s &lt; -2_147_483_648 or s &gt; 2_147_483_647:\n        raise OverflowError(\"int32 overflow\")\n    return s\n\n# Simulate 32-bit wrap (intentional modular arithmetic)\ndef add_i32_wrapping(a: int, b: int) -&gt; int:\n    s = (a + b) & 0xFFFFFFFF\n    return s - 0x100000000 if s & 0x80000000 else s\n\n# Relative+absolute epsilon float compare\ndef almost_equal(x: float, y: float, rel=1e-12, abs_=1e-12) -&gt; bool:\n    return abs(x - y) &lt;= max(rel * max(abs(x), abs(y)), abs_)\n\n# Kahan (compensated) summation\ndef kahan_sum(xs):\n    s = 0.0\n    c = 0.0\n    for x in xs:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\n# Fixed-point cents (safe for ~±9e16 cents with int64)\ndef dollars_to_cents(d: str) -&gt; int:\n    whole, _, frac = d.partition(\".\")\n    frac = (frac + \"00\")[:2]\n    return int(whole) * 100 + int(frac)\n\ndef cents_to_dollars(c: int) -&gt; str:\n    sign = \"-\" if c &lt; 0 else \"\"\n    c = abs(c)\n    return f\"{sign}{c//100}.{c%100:02d}\"\nThese examples are in Python for clarity, but the same ideas exist in every major language.\n\n\nWhy it matters\n\nReliability: Silent wrap or float drift becomes data corruption under load or over time.\nInteroperability: Services in different languages disagree on overflow; define and document your contracts.\nReproducibility: Deterministic numerics (same inputs → same bits) depend on summation order, rounding, and libraries.\nSecurity: UB-triggered overflows can turn into exploitable states.\n\nThis is why “it worked on my laptop” is not enough. You want to be sure it works on every platform, every time.\n\n\nExercises\n\nOverflow policy: For a metrics pipeline, decide where to use checked, wrapping, and saturating addition—and justify each with failure modes.\nULP probe: Find the smallest \\(\\epsilon\\) such that 1.0 + ε != 1.0 in your language; explain how it relates to machine epsilon.\nSummation bake-off: Sum the first 1M terms of the harmonic series with naïve, pairwise, and Kahan methods; compare results and timings.\nFixed-point ledger: Implement deposit/transfer/withdraw using int64 cents; prove no rounding loss for two-decimal currencies.\nBoundary tests: Write property tests that add_i32_checked raises on {INT_MAX,1} and {INT_MIN,-1}, and equals modular add where documented.\nCross-lang contract: Specify a JSON schema for monetary amounts and counters that avoids float types; include examples and edge cases.\n\nGreat — let’s rework 1.4 Overflow & Precision (L2) into a friendlier deep dive, using the same pattern: structured sections, tables, and added “bridge” sentences that guide the reader through complex, low-level material. This version should be dense enough to teach internals, but smooth enough to read without feeling like a spec sheet.\n\n\n\n1.4 L2. Under the Hood\nAt the lowest level, overflow and precision aren’t abstract concepts—they are consequences of how CPUs, compilers, and libraries actually implement arithmetic. Understanding these details makes debugging easier and gives you control over performance, reproducibility, and correctness.\n\nDeep Dive\n\n1) Hardware semantics\nCPUs implement integer and floating-point arithmetic directly in silicon. When the result doesn’t fit, different flags or traps are triggered.\n\nStatus flags (integers): Most architectures (x86, ARM, RISC-V) set overflow, carry, and zero flags after arithmetic. These flags drive branch instructions like jo (“jump if overflow”).\nFloating-point control: The FPU or SIMD unit maintains exception flags (inexact, overflow, underflow, invalid, divide-by-zero). These rarely trap by default; they silently set flags until explicitly checked.\n\nArchitectural view\n\n\n\n\n\n\n\n\n\nArch\nInteger overflow\nFP behavior\nDeveloper hooks\n\n\n\n\nx86-64\nWraparound in 2’s complement; OF/CF bits set\nIEEE-754; flags in MXCSR\njo/jno, fenv.h\n\n\nARM64\nWraparound; NZCV flags\nIEEE-754; exception bits\ncondition codes, feset*\n\n\nRISC-V\nWraparound; OV/CF optional\nIEEE-754; status regs\nCSRs, trap handlers\n\n\n\nKnowing what the CPU does lets you choose: rely on hardware wrap, trap explicitly, or add software checks.\n\n\n2) Compiler and language layers\nEven if hardware sets flags, your language may ignore them. Compilers often optimize based on the language spec.\n\nC/C++: Signed overflow is undefined behavior—the optimizer assumes it never happens, which can remove safety checks you thought were there.\nRust: Catches overflow in debug builds, then forces you to pick: checked_add, wrapping_add, or saturating_add.\nJVM languages (Java, Kotlin, Scala): Always wrap, hiding UB but forcing you to detect overflow yourself.\n.NET (C#, F#): Defaults to wrapping; you can enable checked contexts to trap.\nPython: Emulates unbounded integers, but sometimes simulates C-like behavior for low-level modules.\n\nThese choices aren’t arbitrary—they reflect trade-offs between speed, safety, and backward compatibility.\n\n\n3) Precision management in floating point\nFloating-point has more than just rounding errors. Engineers deal with gradual underflow, denormals, and fused operations.\n\nSubnormals: Numbers smaller than ~2.2e-308 in double precision become “denormalized,” losing precision but extending the range toward zero. Many CPUs handle these slowly.\nFlush-to-zero: Some systems skip subnormals entirely, treating them as zero to boost speed. Great for graphics; risky for scientific code.\nFMA (fused multiply-add): Computes (a*b + c) with one rounding, often improving precision and speed. However, it can break reproducibility across machines that do/don’t use FMA.\n\nPrecision events\n\n\n\n\n\n\n\n\nEvent\nWhat happens\nWhy it matters\n\n\n\n\nOverflow\nBecomes ±Inf\nDetectable via isinf, often safe\n\n\nUnderflow\nBecomes 0 or subnormal\nPerformance hit, possible accuracy loss\n\n\nInexact\nResult rounded\nHappens constantly; only matters if flagged\n\n\nInvalid\nNaN produced\nDivision 0/0, sqrt(-1), etc.\n\n\n\nWhen performance bugs show up in HPC or ML code, denormals and FMAs are often the hidden cause.\n\n\n4) Debugging and testing tools\nLow-level correctness requires instrumentation. Fortunately, toolchains give you options.\n\nSanitizers: -fsanitize=undefined (Clang/GCC) traps on signed overflow.\nValgrind / perf counters: Can catch denormal slowdowns.\nUnit-test utilities: Rust’s assert_eq!(checked_add(…)), Python’s math.isclose, Java’s BigDecimal reference checks.\nReproducibility flags: -ffast-math (fast but non-deterministic), vs. -frounding-math (strict).\n\nTesting with multiple compilers and settings reveals assumptions you didn’t know you had.\n\n\n5) Strategies in production systems\nWhen deploying real systems, you pick policies that match domain needs.\n\nDatabases: Use DECIMAL(p,s) to store fixed-point, preventing float drift in sums.\nFinancial systems: Explicit fixed-point types (cents as int64) + saturating logic on overflow.\nGraphics / ML: Accept float32 imprecision; gain throughput with fused ops and flush-to-zero.\nLow-level kernels: Exploit modular wraparound deliberately for hash tables, checksums, and crypto.\n\nPolicy menu\n\n\n\n\n\n\n\nScenario\nStrategy\n\n\n\n\nMoney transfers\nFixed-point, saturating arithmetic\n\n\nPhysics sim\nfloat64, stable integrators, compensated summation\n\n\nHashing / RNG\nEmbrace wraparound modular math\n\n\nCritical counters\nuint64 with explicit overflow trap\n\n\n\nThinking in policies avoids one-off hacks. Document “why” once, then apply consistently.\n\n\n\nCode Examples\nC (wrap vs check)\n#include &lt;stdint.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;limits.h&gt;\n\nbool add_checked_i32(int32_t a, int32_t b, int32_t *out) {\n    if ((b &gt; 0 && a &gt; INT32_MAX - b) ||\n        (b &lt; 0 && a &lt; INT32_MIN - b)) {\n        return false; // overflow\n    }\n    *out = a + b;\n    return true;\n}\nRust (explicit intent)\nfn demo() {\n    let x: i32 = i32::MAX;\n    println!(\"{:?}\", x.wrapping_add(1));   // wrap\n    println!(\"{:?}\", x.checked_add(1));    // None\n    println!(\"{:?}\", x.saturating_add(1)); // clamp\n}\nPython (reproducibility check)\nimport math\n\ndef ulp_diff(a: float, b: float) -&gt; int:\n    # Compares floats in terms of ULPs\n    import struct\n    ai = struct.unpack('!q', struct.pack('!d', a))[0]\n    bi = struct.unpack('!q', struct.pack('!d', b))[0]\n    return abs(ai - bi)\n\nprint(ulp_diff(1.0, math.nextafter(1.0, 2.0)))  # 1\nThese snippets show how different languages force you to state your policy, rather than relying on “whatever the hardware does.”\n\n\nWhy it matters\n\nPerformance: Understanding denormals and FMAs can save orders of magnitude in compute-heavy workloads.\nCorrectness: Database money columns or counters in billing systems can silently corrupt without fixed-point or overflow checks.\nPortability: Code that relies on UB may “work” on GCC Linux but fail on Clang macOS.\nSecurity: Integer overflow bugs (e.g., buffer length miscalculation) remain a classic vulnerability class.\n\nIn short, overflow and precision are not “just math”—they are systems-level contracts that must be understood and enforced.\n\n\nExercises\n\nCompiler behavior: Write a C function that overflows int32_t. Compile with and without -fsanitize=undefined. What changes?\nFMA investigation: Run a dot-product with and without -ffast-math. Measure result differences across compilers.\nDenormal trap: Construct a loop multiplying by 1e-308. Time it with flush-to-zero enabled vs disabled.\nPolicy design: For an in-memory database, define rules for counters, timestamps, and currency columns. Which use wrapping, which use fixed-point, which trap?\nCross-language test: Implement add_checked_i32 in C, Rust, and Python. Run edge-case inputs (INT_MAX, INT_MIN). Compare semantics.\nULP meter: Write a function in your language to compute ULP distance between two floats. Use it to compare rounding differences between platforms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1. Numbers</span>"
    ]
  }
]